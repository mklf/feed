{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"GreaseLM: Graph REASoning Enhanced Language Models for Question Answering. (arXiv:2201.08860v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08860","description":"<p>Answering complex questions about textual narratives requires reasoning over\nboth stated context and the world knowledge that underlies it. However,\npretrained language models (LM), the foundation of most modern QA systems, do\nnot robustly represent latent relationships between concepts, which is\nnecessary for reasoning. While knowledge graphs (KG) are often used to augment\nLMs with structured representations of world knowledge, it remains an open\nquestion how to effectively fuse and reason over the KG representations and the\nlanguage context, which provides situational constraints and nuances. In this\nwork, we propose GreaseLM, a new model that fuses encoded representations from\npretrained LMs and graph neural networks over multiple layers of modality\ninteraction operations. Information from both modalities propagates to the\nother, allowing language context representations to be grounded by structured\nworld knowledge, and allowing linguistic nuances (e.g., negation, hedging) in\nthe context to inform the graph representations of knowledge. Our results on\nthree benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA)\nand medical question answering (i.e., MedQA-USMLE) domains demonstrate that\nGreaseLM can more reliably answer questions that require reasoning over both\nsituational constraints and structured knowledge, even outperforming models 8x\nlarger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Description-Driven Task-Oriented Dialog Modeling. (arXiv:2201.08904v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08904","description":"<p>Task-oriented dialogue (TOD) systems are required to identify key information\nfrom conversations for the completion of given tasks. Such information is\nconventionally specified in terms of intents and slots contained in\ntask-specific ontology or schemata. Since these schemata are designed by system\ndevelopers, the naming convention for slots and intents is not uniform across\ntasks, and may not convey their semantics effectively. This can lead to models\nmemorizing arbitrary patterns in data, resulting in suboptimal performance and\ngeneralization. In this paper, we propose that schemata should be modified by\nreplacing names or notations entirely with natural language descriptions. We\nshow that a language description-driven system exhibits better understanding of\ntask specifications, higher performance on state tracking, improved data\nefficiency, and effective zero-shot transfer to unseen tasks. Following this\nparadigm, we present a simple yet effective Description-Driven Dialog State\nTracking (D3ST) model, which relies purely on schema descriptions and an\n\"index-picking\" mechanism. We demonstrate the superiority in quality, data\nefficiency and robustness of our approach as measured on the MultiWOZ\n(Budzianowski et al.,2018), SGD (Rastogi et al., 2020), and the recent SGD-X\n(Lee et al., 2021) benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raghav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harrison Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Neural Networks with Mixed Hierarchical Structures and EM Algorithm for Natural Language Processing. (arXiv:2201.08919v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08919","description":"<p>How to obtain hierarchical representations with an increasing level of\nabstraction becomes one of the key issues of learning with deep neural\nnetworks. A variety of RNN models have recently been proposed to incorporate\nboth explicit and implicit hierarchical information in modeling languages in\nthe literature. In this paper, we propose a novel approach called the latent\nindicator layer to identify and learn implicit hierarchical information (e.g.,\nphrases), and further develop an EM algorithm to handle the latent indicator\nlayer in training. The latent indicator layer further simplifies a text's\nhierarchical structure, which allows us to seamlessly integrate different\nlevels of attention mechanisms into the structure. We called the resulting\narchitecture as the EM-HRNN model. Furthermore, we develop two bootstrap\nstrategies to effectively and efficiently train the EM-HRNN model on long text\ndocuments. Simulation studies and real data applications demonstrate that the\nEM-HRNN model with bootstrap training outperforms other RNN-based models in\ndocument classification tasks. The performance of the EM-HRNN model is\ncomparable to a Transformer-based method called Bert-base, though the former is\nmuch smaller model and does not require pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhaoxin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Word Segmentation with Heterogeneous Graph Neural Network. (arXiv:2201.08975v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08975","description":"<p>In recent years, deep learning has achieved significant success in the\nChinese word segmentation (CWS) task. Most of these methods improve the\nperformance of CWS by leveraging external information, e.g., words, sub-words,\nsyntax. However, existing approaches fail to effectively integrate the\nmulti-level linguistic information and also ignore the structural feature of\nthe external information. Therefore, in this paper, we proposed a framework to\nimprove CWS, named HGNSeg. It exploits multi-level external information\nsufficiently with the pre-trained language model and heterogeneous graph neural\nnetwork. The experimental results on six benchmark datasets (e.g., Bakeoff\n2005, Bakeoff 2008) validate that our approach can effectively improve the\nperformance of Chinese word segmentation. Importantly, in cross-domain\nscenarios, our method also shows a strong ability to alleviate the\nout-of-vocabulary (OOV) problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuemei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qi Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leaf: Multiple-Choice Question Generation. (arXiv:2201.09012v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09012","description":"<p>Testing with quiz questions has proven to be an effective way to assess and\nimprove the educational process. However, manually creating quizzes is tedious\nand time-consuming. To address this challenge, we present Leaf, a system for\ngenerating multiple-choice questions from factual text. In addition to being\nvery well suited for the classroom, Leaf could also be used in an industrial\nsetting, e.g., to facilitate onboarding and knowledge sharing, or as a\ncomponent of chatbots, question answering systems, or Massive Open Online\nCourses (MOOCs). The code and the demo are available on\nhttps://github.com/KristiyanVachev/Leaf-Question-Generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vachev_K/0/1/0/all/0/1\">Kristiyan Vachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solvability of orbit-finite systems of linear equations. (arXiv:2201.09060v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09060","description":"<p>We study orbit-finite systems of linear equations, in the setting of sets\nwith atoms. Our principal contribution is a decision procedure for solvability\nof such systems. The procedure works for every field (and even commutative\nring) under mild effectiveness assumptions, and reduces a given orbit-finite\nsystem to a number of finite ones: exponentially many in general, but\npolynomially many when atom dimension of input systems is fixed. Towards\nobtaining the procedure we push further the theory of vector spaces generated\nby orbit-finite sets, and show that each such vector space admits an\norbit-finite basis. This fundamental property is a key tool in our development,\nbut should be also of wider interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Arka Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_P/0/1/0/all/0/1\">Piotr Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasota_S/0/1/0/all/0/1\">S&#x142;awomir Lasota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Information Guided Zero-Shot Paraphrase Generation. (arXiv:2201.09107v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09107","description":"<p>Zero-shot paraphrase generation has drawn much attention as the large-scale\nhigh-quality paraphrase corpus is limited. Back-translation, also known as the\npivot-based method, is typical to this end. Several works leverage different\ninformation as \"pivot\" such as language, semantic representation and so on. In\nthis paper, we explore using visual information such as image as the \"pivot\" of\nback-translation. Different with the pipeline back-translation method, we\npropose visual information guided zero-shot paraphrase generation (ViPG) based\nonly on paired image-caption data. It jointly trains an image captioning model\nand a paraphrasing model and leverage the image captioning model to guide the\ntraining of the paraphrasing model. Both automatic evaluation and human\nevaluation show our model can generate paraphrase with good relevancy, fluency\nand diversity, and image is a promising kind of pivot for zero-shot paraphrase\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Causal Lens for Controllable Text Generation. (arXiv:2201.09119v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09119","description":"<p>Controllable text generation concerns two fundamental tasks of wide\napplications, namely generating text of given attributes (i.e.,\nattribute-conditional generation), and minimally editing existing text to\npossess desired attributes (i.e., text attribute transfer). Extensive prior\nwork has largely studied the two problems separately, and developed different\nconditional models which, however, are prone to producing biased text (e.g.,\nvarious gender stereotypes). This paper proposes to formulate controllable text\ngeneration from a principled causal perspective which models the two tasks with\na unified framework. A direct advantage of the causal formulation is the use of\nrich causality tools to mitigate generation biases and improve control. We\ntreat the two tasks as interventional and counterfactual causal inference based\non a structural causal model, respectively. We then apply the framework to the\nchallenging practical setting where confounding factors (that induce spurious\ncorrelations) are observable only on a small fraction of data. Experiments show\nsignificant superiority of the causal approach over previous conditional models\nfor improved control accuracy and reduced bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Erran Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question rewriting? Assessing its importance for conversational question answering. (arXiv:2201.09146v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09146","description":"<p>In conversational question answering, systems must correctly interpret the\ninterconnected interactions and generate knowledgeable answers, which may\nrequire the retrieval of relevant information from a background repository.\nRecent approaches to this problem leverage neural language models, although\ndifferent alternatives can be considered in terms of modules for (a)\nrepresenting user questions in context, (b) retrieving the relevant background\ninformation, and (c) generating the answer. This work presents a conversational\nquestion answering system designed specifically for the Search-Oriented\nConversational AI (SCAI) shared task, and reports on a detailed analysis of its\nquestion rewriting module. In particular, we considered different variations of\nthe question rewriting module to evaluate the influence on the subsequent\ncomponents, and performed a careful analysis of the results obtained with the\nbest system configuration. Our system achieved the best performance in the\nshared task and our analysis emphasizes the importance of the conversation\ncontext representation for the overall system performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raposo_G/0/1/0/all/0/1\">Gon&#xe7;alo Raposo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_R/0/1/0/all/0/1\">Rui Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_B/0/1/0/all/0/1\">Bruno Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Lu&#xed;sa Coheur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large and Diverse Arabic Corpus for Language Modeling. (arXiv:2201.09227v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09227","description":"<p>Language models (LMs) have introduced a major paradigm shift in Natural\nLanguage Processing (NLP) modeling where large pre-trained LMs became integral\nto most of the NLP tasks. The LMs are intelligent enough to find useful and\nrelevant representations of the language without any supervision. Perhaps,\nthese models are used to fine-tune typical NLP tasks with significantly high\naccuracy as compared to the traditional approaches. Conversely, the training of\nthese models requires a massively large corpus that is a good representation of\nthe language. English LMs generally perform better than their other language\ncounterparts, due to the availability of massive English corpora.\n</p>\n<p>This work elaborates on the design and development of a large Arabic corpus.\nIt consists of over 500 GB of Arabic cleaned text targeted at improving\ncross-domain knowledge and downstream generalization capability of large-scale\nlanguage models. Moreover, the corpus is utilized in the training of a large\nArabic LM. In order to evaluate the effectiveness of the LM, a number of\ntypical NLP tasks are fine-tuned. The tasks demonstrate a significant boost\nfrom 4.5 to 8.5% when compared to tasks fine-tuned on multi-lingual BERT\n(mBERT). To the best of my knowledge, this is currently the largest clean and\ndiverse Arabic corpus ever collected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Abbas Raza Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WIDAR -- Weighted Input Document Augmented ROUGE. (arXiv:2201.09282v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09282","description":"<p>The task of automatic text summarization has gained a lot of traction due to\nthe recent advancements in machine learning techniques. However, evaluating the\nquality of a generated summary remains to be an open problem. The literature\nhas widely adopted Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as\nthe standard evaluation metric for summarization. However, ROUGE has some\nlong-established limitations; a major one being its dependence on the\navailability of good quality reference summary. In this work, we propose the\nmetric WIDAR which in addition to utilizing the reference summary uses also the\ninput document in order to evaluate the quality of the generated summary. The\nproposed metric is versatile, since it is designed to adapt the evaluation\nscore according to the quality of the reference summary. The proposed metric\ncorrelates better than ROUGE by 26%, 76%, 82%, and 15%, respectively, in\ncoherence, consistency, fluency, and relevance on human judgement scores\nprovided in the SummEval dataset. The proposed metric is able to obtain\ncomparable results with other state-of-the-art metrics while requiring a\nrelatively short computational time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Raghav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavi_V/0/1/0/all/0/1\">Vaibhav Mavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jangra_A/0/1/0/all/0/1\">Anubhav Jangra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Visual Attention for Simultaneous Multimodal Machine Translation. (arXiv:2201.09324v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09324","description":"<p>Recently, there has been a surge in research in multimodal machine\ntranslation (MMT), where additional modalities such as images are used to\nimprove translation quality of textual systems. A particular use for such\nmultimodal systems is the task of simultaneous machine translation, where\nvisual context has been shown to complement the partial information provided by\nthe source sentence, especially in the early phases of translation (Caglayanet\nal., 2020a; Imankulova et al., 2020). In this paper, we propose the first\nTransformer-based simultaneous MMT architecture, which has not been previously\nexplored in the field. Additionally, we extend this model with an auxiliary\nsupervision signal that guides its visual attention mechanism using labelled\nphrase-region alignments. We perform comprehensive experiments on three\nlanguage directions and conduct thorough quantitative and qualitative analyses\nusing both automatic metrics and manual inspection. Our results show that (i)\nsupervised visual attention consistently improves the translation quality of\nthe MMT models, and (ii) fine-tuning the MMT with supervision loss enabled\nleads to better performance than training the MMT from scratch. Compared to the\nstate-of-the-art, our proposed model achieves improvements of up to 2.3 BLEU\nand 3.5 METEOR points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haralampieva_V/0/1/0/all/0/1\">Veneta Haralampieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caglayan_O/0/1/0/all/0/1\">Ozan Caglayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Application of Pseudo-Log-Likelihoods to Natural Language Scoring. (arXiv:2201.09377v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09377","description":"<p>Language models built using semi-supervised machine learning on large corpora\nof natural language have very quickly enveloped the fields of natural language\ngeneration and understanding. In this paper we apply a zero-shot approach\nindependently developed by a number of researchers now gaining recognition as a\nsignificant alternative to fine-tuning for evaluation on common sense tasks. A\nlanguage model with relatively few parameters and training steps compared to a\nmore recent language model (T5) can outperform it on a recent large data set\n(TimeDial), while displaying robustness in its performance across a similar\nclass of language tasks. Surprisingly, this result is achieved by using a\nhyperparameter-free zero-shot method with the smaller model, compared to\nfine-tuning to the larger model. We argue that robustness of the smaller model\nought to be understood in terms of compositionality, in a sense that we draw\nfrom recent literature on a class of similar models. We identify a practical\ncost for our method and model: high GPU-time for natural language evaluation.\nThe zero-shot measurement technique that produces remarkable stability, both\nfor ALBERT and other BERT variants, is an application of pseudo-log-likelihoods\nto masked language models for the relative measurement of probability for\nsubstitution alternatives in forced choice language tasks such as the Winograd\nSchema Challenge, Winogrande, and others. One contribution of this paper is to\nbring together a number of similar, but independent strands of research. We\nproduce some absolute state-of-the-art results for common sense reasoning in\nbinary choice tasks, performing better than any published result in the\nliterature, including fine-tuned efforts. We show a remarkable consistency of\nthe model's performance under adversarial settings, which we argue is best\nexplained by the model's compositionality of representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abramson_D/0/1/0/all/0/1\">Darren Abramson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emami_A/0/1/0/all/0/1\">Ali Emami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion-based Modeling of Mental Disorders on Social Media. (arXiv:2201.09451v1 [cs.SI])","link":"http://arxiv.org/abs/2201.09451","description":"<p>According to the World Health Organization (WHO), one in four people will be\naffected by mental disorders at some point in their lives. However, in many\nparts of the world, patients do not actively seek professional diagnosis\nbecause of stigma attached to mental illness, ignorance of mental health and\nits associated symptoms. In this paper, we propose a model for passively\ndetecting mental disorders using conversations on Reddit. Specifically, we\nfocus on a subset of mental disorders that are characterized by distinct\nemotional patterns (henceforth called emotional disorders): major depressive,\nanxiety, and bipolar disorders. Through passive (i.e., unprompted) detection,\nwe can encourage patients to seek diagnosis and treatment for mental disorders.\nOur proposed model is different from other work in this area in that our model\nis based entirely on the emotional states, and the transition between these\nstates of users on Reddit, whereas prior work is typically based on\ncontent-based representations (e.g., n-grams, language model embeddings, etc).\nWe show that content-based representation is affected by domain and topic bias\nand thus does not generalize, while our model, on the other hand, suppresses\ntopic-specific information and thus generalizes well across different topics\nand times. We conduct experiments on our model's ability to detect different\nemotional disorders and on the generalizability of our model. Our experiments\nshow that while our model performs comparably to content-based models, such as\nBERT, it generalizes much better across time and topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaobo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yaojia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias in Automated Speaker Recognition. (arXiv:2201.09486v1 [cs.SD])","link":"http://arxiv.org/abs/2201.09486","description":"<p>Automated speaker recognition uses data processing to identify speakers by\ntheir voice. Today, automated speaker recognition technologies are deployed on\nbillions of smart devices and in services such as call centres. Despite their\nwide-scale deployment and known sources of bias in face recognition and natural\nlanguage processing, bias in automated speaker recognition has not been studied\nsystematically. We present an in-depth empirical and analytical study of bias\nin the machine learning development workflow of speaker verification, a voice\nbiometric and core task in automated speaker recognition. Drawing on an\nestablished framework for understanding sources of harm in machine learning, we\nshow that bias exists at every development stage in the well-known VoxCeleb\nSpeaker Recognition Challenge, including model building, implementation, and\ndata generation. Most affected are female speakers and non-US nationalities,\nwho experience significant performance degradation. Leveraging the insights\nfrom our findings, we make practical recommendations for mitigating bias in\nautomated speaker recognition, and outline future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_W/0/1/0/all/0/1\">Wiebke Toussaint</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_A/0/1/0/all/0/1\">Aaron Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data and knowledge-driven approaches for multilingual training to improve the performance of speech recognition systems of Indian languages. (arXiv:2201.09494v1 [eess.AS])","link":"http://arxiv.org/abs/2201.09494","description":"<p>We propose data and knowledge-driven approaches for multilingual training of\nthe automated speech recognition (ASR) system for a target language by pooling\nspeech data from multiple source languages. Exploiting the acoustic\nsimilarities between Indian languages, we implement two approaches. In\nphone/senone mapping, deep neural network (DNN) learns to map senones or phones\nfrom one language to the others, and the transcriptions of the source languages\nare modified such that they can be used along with the target language data to\ntrain and fine-tune the target language ASR system. In the other approach, we\nmodel the acoustic information for all the languages simultaneously by training\na multitask DNN (MTDNN) to predict the senones of each language in different\noutput layers. The cross-entropy loss and the weight update procedure are\nmodified such that only the shared layers and the output layer responsible for\npredicting the senone classes of a language are updated during training, if the\nfeature vector belongs to that particular language. In the low-resource setting\n(LRS), 40 hours of transcribed data each for Tamil, Telugu and Gujarati\nlanguages are used for training. The DNN based senone mapping technique gives\nrelative improvements in word error rates (WER) of 9.66%, 7.2% and 15.21% over\nthe baseline system for Tamil, Gujarati and Telugu languages, respectively. In\nmedium-resourced setting (MRS), 160, 275 and 135 hours of data for Tamil,\nKannada and Hindi languages are used, where, the same technique gives better\nrelative improvements of 13.94%, 10.28% and 27.24% for Tamil, Kannada and\nHindi, respectively. The MTDNN with senone mapping based training in LRS, gives\nhigher relative WER improvements of 15.0%, 17.54% and 16.06%, respectively for\nTamil, Gujarati and Telugu, whereas in MRS, we see improvements of 21.24%\n21.05% and 30.17% for Tamil, Kannada and Hindi languages, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Madhavaraj_A/0/1/0/all/0/1\">A. Madhavaraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ganesan_R/0/1/0/all/0/1\">Ramakrishnan Angarai Ganesan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Books. (arXiv:2201.09518v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09518","description":"<p>The article explores new ways of written language aided by AI technologies,\nlike GPT-2 and GPT-3. The question that is stated in the paper is not about\nwhether these novel technologies will eventually replace authored books, but\nhow to relate to and contextualize such publications and what kind of new\ntools, processes, and ideas are behind them. For that purpose, a new concept of\nsynthetic books is introduced in the article. It stands for the publications\ncreated by deploying AI technology, more precisely autoregressive language\nmodels that are able to generate human-like text. Supported by the case\nstudies, the value and reasoning of the synthetic books are discussed. The\npaper emphasizes that artistic quality is an issue when it comes to\nAI-generated content. The article introduces projects that demonstrate an\ninteractive input by an artist and/or audience combined with the\ndeep-learning-based language models. In the end, the paper focuses on\nunderstanding the neural aesthetics of written language in the art context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guljajeva_V/0/1/0/all/0/1\">Varvara Guljajeva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BTPK-based learning: An Interpretable Method for Named Entity Recognition. (arXiv:2201.09523v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09523","description":"<p>Named entity recognition (NER) is an essential task in natural language\nprocessing, but the internal mechanism of most NER models is a black box for\nusers. In some high-stake decision-making areas, improving the interpretability\nof an NER method is crucial but challenging. In this paper, based on the\nexisting Deterministic Talmudic Public announcement logic (TPK) model, we\npropose a novel binary tree model (called BTPK) and apply it to two widely used\nBi-RNNs to obtain BTPK-based interpretable ones. Then, we design a\ncounterfactual verification module to verify the BTPK-based learning method.\nExperimental results on three public datasets show that the BTPK-based learning\noutperform two classical Bi-RNNs with self-attention, especially on small,\nsimple data and relatively large, complex data. Moreover, the counterfactual\nverification demonstrates that the explanations provided by the BTPK-based\nlearning method are reasonable and accurate in NER tasks. Besides, the logical\nreasoning based on BTPK shows how Bi-RNNs handle NER tasks, with different\ndistance of public announcements on long and complex sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zelai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1\">Haixiao Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_D/0/1/0/all/0/1\">Dov Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentzen_B/0/1/0/all/0/1\">Bruno Bentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Beishui Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals. (arXiv:2201.09555v1 [cs.AI])","link":"http://arxiv.org/abs/2201.09555","description":"<p>Scholarly data is growing continuously containing information about the\narticles from plethora of venues including conferences, journals, etc. Many\ninitiatives have been taken to make scholarly data available in the for of\nKnowledge Graphs (KGs). These efforts to standardize these data and make them\naccessible have also lead to many challenges such as exploration of scholarly\narticles, ambiguous authors, etc. This study more specifically targets the\nproblem of Author Name Disambiguation (AND) on Scholarly KGs and presents a\nnovel framework, Literally Author Name Disambiguation (LAND), which utilizes\nKnowledge Graph Embeddings (KGEs) using multimodal literal information\ngenerated from these KGs. This framework is based on three components: 1)\nMultimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical\nAgglomerative Clustering. Extensive experiments have been conducted on two\nnewly created KGs: (i) KG containing information from Scientometrics Journal\nfrom 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known\nbenchmark for AND provided by AMiner (AMiner-534K). The results show that our\nproposed architecture outperforms our baselines of 8-14\\% in terms of F$_1$\nscore and shows competitive performances on a challenging benchmark such as\nAMiner. The code and the datasets are publicly available through Github\n(https://github.com/sntcristian/and-kge) and Zenodo\n(https://zenodo.org/record/5675787\\#.YcCJzL3MJTY) respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santini_C/0/1/0/all/0/1\">Cristian Santini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gesese_G/0/1/0/all/0/1\">Genet Asefa Gesese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1\">Silvio Peroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1\">Aldo Gangemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1\">Harald Sack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEPOR: An Augmented Machine Translation Evaluation Metric. (arXiv:1703.08748v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1703.08748","description":"<p>Machine translation (MT) was developed as one of the hottest research topics\nin the natural language processing (NLP) literature. One important issue in MT\nis that how to evaluate the MT system reasonably and tell us whether the\ntranslation system makes an improvement or not. The traditional manual judgment\nmethods are expensive, time-consuming, unrepeatable, and sometimes with low\nagreement. On the other hand, the popular automatic MT evaluation methods have\nsome weaknesses. Firstly, they tend to perform well on the language pairs with\nEnglish as the target language, but weak when English is used as source.\nSecondly, some methods rely on many additional linguistic features to achieve\ngood performance, which makes the metric unable to replicate and apply to other\nlanguage pairs easily. Thirdly, some popular metrics utilize incomprehensive\nfactors, which result in low performance on some practical tasks. In this\nthesis, to address the existing problems, we design novel MT evaluation methods\nand investigate their performances on different languages. Firstly, we design\naugmented factors to yield highly accurate evaluation. Secondly, we design a\ntunable evaluation model where weighting of factors can be optimized according\nto the characteristics of languages. Thirdly, in the enhanced version of our\nmethods, we design concise linguistic feature using part-of-speech (POS) to\nshow that our methods can yield even higher performance when using some\nexternal linguistic resources. Finally, we introduce the practical performance\nof our metrics in the ACL-WMT workshop shared tasks, which show that the\nproposed methods are robust across different languages. In addition, we also\npresent some novel work on quality estimation of MT without using reference\ntranslations including the usage of probability models of Na\\\"ive Bayes (NB),\nsupport vector machine (SVM) classification algorithms, and CRFs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v11 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge-Enhanced Text Generation. (arXiv:2010.04389v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.04389","description":"<p>The goal of text generation is to make machines express in human language. It\nis one of the most important yet challenging tasks in natural language\nprocessing (NLP). Since 2014, various neural encoder-decoder models pioneered\nby Seq2Seq have been proposed to achieve the goal by learning to map input text\nto output text. However, the input text alone often provides limited knowledge\nto generate the desired output, so the performance of text generation is still\nfar from satisfaction in many real-world scenarios. To address this issue,\nresearchers have considered incorporating various forms of knowledge beyond the\ninput text into the generation models. This research direction is known as\nknowledge-enhanced text generation. In this survey, we present a comprehensive\nreview of the research on knowledge enhanced text generation over the past five\nyears. The main content includes two parts: (i) general methods and\narchitectures for integrating knowledge into text generation; (ii) specific\ntechniques and applications according to different forms of knowledge data.\nThis survey can have broad audiences, researchers and practitioners, in\nacademia and industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaitang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Sparse Transformer for Multilingual Translation. (arXiv:2104.07358v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07358","description":"<p>Multilingual machine translation has attracted much attention recently due to\nits support of knowledge transfer among languages and the low cost of training\nand deployment compared with numerous bilingual models. A known challenge of\nmultilingual models is the negative language interference. In order to enhance\nthe translation quality, deeper and wider architectures are applied to\nmultilingual modeling for larger model capacity, which suffers from the\nincreased inference cost at the same time. It has been pointed out in recent\nstudies that parameters shared among languages are the cause of interference\nwhile they may also enable positive transfer. Based on these insights, we\npropose an adaptive and sparse architecture for multilingual modeling, and\ntrain the model to learn shared and language-specific parameters to improve the\npositive transfer and mitigate the interference. The sparse architecture only\nactivates a sub-network which preserves inference efficiency, and the adaptive\ndesign selects different sub-networks based on the input languages. Our model\noutperforms strong baselines across multiple benchmarks. On the large-scale\nOPUS dataset with $100$ languages, we achieve $+2.1$, $+1.3$ and $+6.2$ BLEU\nimprovements in one-to-many, many-to-one and zero-shot tasks respectively\ncompared to standard Transformer without increasing the inference cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genzel_D/0/1/0/all/0/1\">Dmitriy Genzel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results for specific few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces\n(i.e., templates) into the input and transform a classification task into a\nmasked language modeling problem. However, for relation extraction, determining\nan appropriate prompt template requires domain expertise, and it is cumbersome\nand time-consuming to obtain a suitable label word. Furthermore, there exists\nabundant semantic and prior knowledge among the relation labels that cannot be\nignored. To this end, we focus on incorporating knowledge among relation labels\ninto prompt-tuning for relation extraction and propose a Knowledge-aware\nPrompt-tuning approach with synergistic optimization (KnowPrompt).\nSpecifically, we inject latent knowledge contained in relation labels into\nprompt construction with learnable virtual type words and answer words. Then,\nwe synergistically optimize their representation with structured constraints.\nExtensive experimental results on five datasets with standard and low-resource\nsettings demonstrate the effectiveness of our approach. Our code and datasets\nare available in https://github.com/zjunlp/KnowPrompt for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duplex Sequence-to-Sequence Learning for Reversible Machine Translation. (arXiv:2105.03458v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03458","description":"<p>Sequence-to-sequence learning naturally has two directions. How to\neffectively utilize supervision signals from both directions? Existing\napproaches either require two separate models, or a multitask-learned model but\nwith inferior performance. In this paper, we propose REDER (Reversible Duplex\nTransformer), a parameter-efficient model and apply it to machine translation.\nEither end of REDER can simultaneously input and output a distinct language.\nThus REDER enables reversible machine translation by simply flipping the input\nand output ends. Experiments verify that REDER achieves the first success of\nreversible machine translation, which helps outperform its multitask-trained\nbaselines by up to 1.3 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13948","description":"<p>Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation. (arXiv:2109.06379v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06379","description":"<p>Natural language generation (NLG) spans a broad range of tasks, each of which\nserves for specific objectives and desires different properties of generated\ntext. The complexity makes automatic evaluation of NLG particularly\nchallenging. Previous work has typically focused on a single task and developed\nindividual evaluation metrics based on specific intuitions. In this paper, we\npropose a unifying perspective that facilitates the design of metrics for a\nwide range of language generation tasks and quality aspects. Based on the\nnature of information change from input to output, we classify NLG tasks into\ncompression (e.g., summarization), transduction (e.g., text rewriting), and\ncreation (e.g., dialog). The information alignment, or overlap, between input,\ncontext, and output text plays a common central role in characterizing the\ngeneration. Using the uniform concept of information alignment, we develop a\nfamily of interpretable metrics for various NLG tasks and aspects, often\nwithout need of gold reference data. To operationalize the metrics, we train\nself-supervised models to approximate information alignment as a prediction\ntask. Experiments show the uniformly designed metrics achieve stronger or\ncomparable correlations with human judgement compared to state-of-the-art\nmetrics in each of diverse tasks, including text summarization, style transfer,\nand knowledge-grounded dialog. With information alignment as the intermediate\nrepresentation, we deliver a composable library for easy NLG evaluation and\nfuture metric design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Mingkai Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14927","description":"<p>Temporal expressions in text play a significant role in language\nunderstanding and correctly identifying them is fundamental to various\nretrieval and natural language processing systems. Previous works have slowly\nshifted from rule-based to neural architectures, capable of tagging expressions\nwith higher accuracy. However, neural models can not yet distinguish between\ndifferent expression types at the same level as their rule-based counterparts.\nIn this work, we aim to identify the most suitable transformer architecture for\njoint temporal tagging and type classification, as well as, investigating the\neffect of semi-supervised training on the performance of these systems. Based\non our study of token classification variants and encoder-decoder\narchitectures, we present a transformer encoder-decoder model using the RoBERTa\nlanguage model as our best performing system. By supplementing training\nresources with weakly labeled data from rule-based systems, our model surpasses\nprevious works in temporal tagging and type classification, especially on rare\nclasses. Our code and pre-trained experiments are available at:\nhttps://github.com/satya77/Transformer_Temporal_Tagger\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almasian_S/0/1/0/all/0/1\">Satya Almasian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aumiller_D/0/1/0/all/0/1\">Dennis Aumiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_M/0/1/0/all/0/1\">Michael Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01900","description":"<p>Self-supervised speech representation learning methods like wav2vec 2.0 and\nHidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and\noffer good representations for numerous speech processing tasks. Despite the\nsuccess of these methods, they require large memory and high pre-training\ncosts, making them inaccessible for researchers in academia and small\ncompanies. Therefore, this paper introduces DistilHuBERT, a novel multi-task\nlearning framework to distill hidden representations from a HuBERT model\ndirectly. This method reduces HuBERT's size by 75% and 73% faster while\nretaining most performance in ten different tasks. Moreover, DistilHuBERT\nrequired little training time and data, opening the possibilities of\npre-training personal and on-device SSL models for speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR. (arXiv:2110.03151v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.03151","description":"<p>This paper presents Transcribe-to-Diarize, a new approach for neural speaker\ndiarization that uses an end-to-end (E2E) speaker-attributed automatic speech\nrecognition (SA-ASR). The E2E SA-ASR is a joint model that was recently\nproposed for speaker counting, multi-talker speech recognition, and speaker\nidentification from monaural audio that contains overlapping speech. Although\nthe E2E SA-ASR model originally does not estimate any time-related information,\nwe show that the start and end times of each word can be estimated with\nsufficient accuracy from the internal state of the E2E SA-ASR by adding a small\nnumber of learnable parameters. Similar to the target-speaker voice activity\ndetection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to\nestimate speech activity of each speaker while it has the advantages of (i)\nhandling unlimited number of speakers, (ii) leveraging linguistic information\nfor speaker diarization, and (iii) simultaneously generating speaker-attributed\ntranscriptions. Experimental results on the LibriCSS and AMI corpora show that\nthe proposed method achieves significantly better diarization error rate than\nvarious existing speaker diarization methods when the number of speakers is\nunknown, and achieves a comparable performance to TS-VAD when the number of\nspeakers is given in advance. The proposed method simultaneously generates\nspeaker-attributed transcription with state-of-the-art accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Pitfalls of Analyzing Individual Neurons in Language Models. (arXiv:2110.07483v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07483","description":"<p>While many studies have shown that linguistic information is encoded in\nhidden word representations, few have studied individual neurons, to show how\nand in which neurons it is encoded. Among these, the common approach is to use\nan external probe to rank neurons according to their relevance to some\nlinguistic attribute, and to evaluate the obtained ranking using the same probe\nthat produced it. We show two pitfalls in this methodology: 1. It confounds\ndistinct factors: probe quality and ranking quality. We separate them and draw\nconclusions on each. 2. It focuses on encoded information, rather than\ninformation that is used by the model. We show that these are not the same. We\ncompare two recent ranking methods and a simple one we introduce, and evaluate\nthem with regard to both of these aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antverg_O/0/1/0/all/0/1\">Omer Antverg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNN-LM: Language Modeling based on Global Contexts via GNN. (arXiv:2110.08743v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08743","description":"<p>Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in\nthis work, we introduce GNN-LM, which extends the vanilla neural language model\n(LM) by allowing to reference similar contexts in the entire training corpus.\nWe build a directed heterogeneous graph between an input context and its\nsemantically related neighbors selected from the training corpus, where nodes\nare tokens in the input context and retrieved neighbor contexts, and edges\nrepresent connections between nodes. Graph neural networks (GNNs) are\nconstructed upon the graph to aggregate information from similar contexts to\ndecode the token. This learning paradigm provides direct access to the\nreference contexts and helps improve a model's generalization ability. We\nconduct comprehensive experiments to validate the effectiveness of the GNN-LM:\nGNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a\n3.9 point improvement over its counterpart of the vanilla LM model), and shows\nsubstantial improvement on One Billion Word and Enwiki8 datasets against strong\nbaselines. In-depth ablation studies are performed to understand the mechanics\nof GNN-LM. \\footnote{The code can be found at\n\\url{https://github.com/ShannonAI/GNN-LM}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensembling Graph Predictions for AMR Parsing. (arXiv:2110.09131v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.09131","description":"<p>In many machine learning tasks, models are trained to predict structure data\nsuch as graphs. For example, in natural language processing, it is very common\nto parse texts into dependency trees or abstract meaning representation (AMR)\ngraphs. On the other hand, ensemble methods combine predictions from multiple\nmodels to create a new one that is more robust and accurate than individual\npredictions. In the literature, there are many ensembling techniques proposed\nfor classification or regression problems, however, ensemble graph prediction\nhas not been studied thoroughly. In this work, we formalize this problem as\nmining the largest graph that is the most supported by a collection of graph\npredictions. As the problem is NP-Hard, we propose an efficient heuristic\nalgorithm to approximate the optimal solution. To validate our approach, we\ncarried out experiments in AMR parsing problems. The experimental results\ndemonstrate that the proposed approach can combine the strength of\nstate-of-the-art AMR parsers to create new predictions that are more accurate\nthan any individual models in five standard benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1\">Hoang Thanh Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picco_G/0/1/0/all/0/1\">Gabriele Picco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Lam M. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_D/0/1/0/all/0/1\">Dzung T. Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_V/0/1/0/all/0/1\">Vanessa L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Ramon Fernandez Astudillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Embedded Knowledge Graph Multi-hop Question Answering by introducing Relational Chain Reasoning. (arXiv:2110.12679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.12679","description":"<p>Knowledge Base Question Answering (KBQA) aims to answer userquestions from a\nknowledge base (KB) by identifying the reasoningrelations between topic entity\nand answer. As a complex branchtask of KBQA, multi-hop KGQA requires reasoning\nover multi-hop relational chains preserved in KG to arrive at the right\nanswer.Despite the successes made in recent years, the existing works\nonanswering multi-hop complex question face the following challenges: i)\nsuffering from poor performances due to the neglect of explicit relational\nchain order and its relational types reflected inuser questions; ii) failing to\nconsider implicit relations between thetopic entity and the answer implied in\nstructured KG because oflimited neighborhood size constraints in subgraph\nretrieval based algorithms. To address these issues in multi-hop KGQA, we\nproposea novel model in this paper, namely Relational Chain-based Embed-ded\nKGQA (Rce-KGQA), which simultaneously utilizes the explicitrelational chain\ndescribed in natural language questions and the implicit relational chain\nstored in structured KG. Our extensiveempirical study on two open-domain\nbenchmarks proves that ourmethod significantly outperforms the state-of-the-art\ncounterpartslike GraftNet, PullNet and EmbedKGQA. Comprehensive ablation\nexperiments also verify the effectiveness of our method for multi-hop KGQA\ntasks. We have made our model's source code availableat Github:\nhttps://github.com/albert-jin/Rce-KGQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weiqiang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xi Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Ruiping Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. (arXiv:2110.13900v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.13900","description":"<p>Self-supervised learning (SSL) achieves great success in speech recognition,\nwhile limited exploration has been attempted for other speech processing tasks.\nAs speech signal contains multi-faceted information including speaker identity,\nparalinguistics, spoken content, etc., learning universal representations for\nall speech tasks is challenging. To tackle the problem, we propose a new\npre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM\njointly learns masked speech prediction and denoising in pre-training. By this\nmeans, WavLM does not only keep the speech content modeling capability by the\nmasked speech prediction, but also improves the potential to non-ASR tasks by\nthe speech denoising. In addition, WavLM employs gated relative position bias\nfor the Transformer structure to better capture sequence ordering of input\nspeech, and scale up the training dataset from 60k hours to 94k hours. WavLM\nLarge achieves state-of-the-art performance on the SUPERB benchmark, and brings\nsignificant improvements for various speech processing tasks on their\nrepresentative benchmarks. The code and pre-trained models are available at\nhttps://aka.ms/wavlm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiangzhan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrast and Generation Make BART a Good Dialogue Emotion Recognizer. (arXiv:2112.11202v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11202","description":"<p>In dialogue systems, utterances with similar semantics may have distinctive\nemotions under different contexts. Therefore, modeling long-range contextual\nemotional relationships with speaker dependency plays a crucial part in\ndialogue emotion recognition. Meanwhile, distinguishing the different emotion\ncategories is non-trivial since they usually have semantically similar\nsentiments. To this end, we adopt supervised contrastive learning to make\ndifferent emotions mutually exclusive to identify similar emotions better.\nMeanwhile, we utilize an auxiliary response generation task to enhance the\nmodel's ability of handling context information, thereby forcing the model to\nrecognize emotions with similar semantics in diverse contexts. To achieve these\nobjectives, we use the pre-trained encoder-decoder model BART as our backbone\nmodel since it is very suitable for both understanding and generation tasks.\nThe experiments on four datasets demonstrate that our proposed model obtains\nsignificantly more favorable results than the state-of-the-art model in\ndialogue emotion recognition. The ablation study further demonstrates the\neffectiveness of supervised contrastive loss and generative loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations. (arXiv:2201.00318v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.00318","description":"<p>Text classification is a fundamental Natural Language Processing task that\nhas a wide variety of applications, where deep learning approaches have\nproduced state-of-the-art results. While these models have been heavily\ncriticized for their black-box nature, their robustness to slight perturbations\nin input text has been a matter of concern. In this work, we carry out a\ndata-focused study evaluating the impact of systematic practical perturbations\non the performance of the deep learning based text classification models like\nCNN, LSTM, and BERT-based algorithms. The perturbations are induced by the\naddition and removal of unwanted tokens like punctuation and stop-words that\nare minimally associated with the final performance of the model. We show that\nthese deep learning approaches including BERT are sensitive to such legitimate\ninput perturbations on four standard benchmark datasets SST2, TREC-6, BBC News,\nand tweet_eval. We observe that BERT is more susceptible to the removal of\ntokens as compared to the addition of tokens. Moreover, LSTM is slightly more\nsensitive to input perturbations as compared to CNN based model. The work also\nserves as a practical guide to assessing the impact of discrepancies in\ntrain-test conditions on the final performance of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miyajiwala_A/0/1/0/all/0/1\">Aamir Miyajiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladkat_A/0/1/0/all/0/1\">Arnav Ladkat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadale_S/0/1/0/all/0/1\">Samiksha Jagadale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03335","description":"<p>We present a new open-source and extensible knowledge extraction toolkit,\ncalled DeepKE (Deep learning based Knowledge Extraction), supporting standard\nfully supervised, low-resource few-shot and document-level scenarios. DeepKE\nimplements various information extraction tasks, including named entity\nrecognition, relation extraction and attribute extraction. With a unified\nframework, DeepKE allows developers and researchers to customize datasets and\nmodels to extract information from unstructured texts according to their\nrequirements. Specifically, DeepKE not only provides various functional modules\nand model implementation for different tasks and scenarios but also organizes\nall components by consistent frameworks to maintain sufficient modularity and\nextensibility. Besides, we present an online platform in\n<a href=\"http://deepke.zjukg.cn/\">this http URL</a> for real-time extraction of various tasks. DeepKE has\nbeen equipped with Google Colab tutorials and comprehensive documents for\nbeginners. We release the source code at https://github.com/zjunlp/DeepKE, with\na demo video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Liankuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guozhou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Lexical Simplification for Turkish. (arXiv:2201.05878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05878","description":"<p>In this paper, we present the first automatic lexical simplification system\nfor the Turkish language. Recent text simplification efforts rely on manually\ncrafted simplified corpora and comprehensive NLP tools that can analyse the\ntarget text both in word and sentence levels. Turkish is a morphologically rich\nagglutinative language that requires unique considerations such as the proper\nhandling of inflectional cases. Being a low-resource language in terms of\navailable resources and industrial-strength tools, it makes the text\nsimplification task harder to approach. We present a new text simplification\npipeline based on pretrained representation model BERT together with\nmorphological features to generate grammatically correct and semantically\nappropriate word-level simplifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaMDA: Language Models for Dialog Applications. (arXiv:2201.08239v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08239","description":"<p>We present LaMDA: Language Models for Dialog Applications. LaMDA is a family\nof Transformer-based neural language models specialized for dialog, which have\nup to 137B parameters and are pre-trained on 1.56T words of public dialog data\nand web text. While model scaling alone can improve quality, it shows less\nimprovements on safety and factual grounding. We demonstrate that fine-tuning\nwith annotated data and enabling the model to consult external knowledge\nsources can lead to significant improvements towards the two key challenges of\nsafety and factual grounding. The first challenge, safety, involves ensuring\nthat the model's responses are consistent with a set of human values, such as\npreventing harmful suggestions and unfair bias. We quantify safety using a\nmetric based on an illustrative set of human values, and we find that filtering\ncandidate responses using a LaMDA classifier fine-tuned with a small amount of\ncrowdworker-annotated data offers a promising approach to improving model\nsafety. The second challenge, factual grounding, involves enabling the model to\nconsult external knowledge sources, such as an information retrieval system, a\nlanguage translator, and a calculator. We quantify factuality using a\ngroundedness metric, and we find that our approach enables the model to\ngenerate responses grounded in known sources, rather than responses that merely\nsound plausible. Finally, we explore the use of LaMDA in the domains of\neducation and content recommendations, and analyze their helpfulness and role\nconsistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thoppilan_R/0/1/0/all/0/1\">Romal Thoppilan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_D/0/1/0/all/0/1\">Daniel De Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jamie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_A/0/1/0/all/0/1\">Apoorv Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1\">Alicia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_T/0/1/0/all/0/1\">Taylor Bos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_L/0/1/0/all/0/1\">Leslie Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">YaGuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hongrae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghafouri_A/0/1/0/all/0/1\">Amin Ghafouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menegali_M/0/1/0/all/0/1\">Marcelo Menegali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepikhin_D/0/1/0/all/0/1\">Dmitry Lepikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dehao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokon_I/0/1/0/all/0/1\">Igor Krivokon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusch_W/0/1/0/all/0/1\">Will Rusch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pickett_M/0/1/0/all/0/1\">Marc Pickett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_Hellstern_K/0/1/0/all/0/1\">Kathleen Meier-Hellstern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Meredith Ringel Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_T/0/1/0/all/0/1\">Tulsee Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1\">Renelito Delos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_T/0/1/0/all/0/1\">Toju Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soraker_J/0/1/0/all/0/1\">Johnny Soraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zevenbergen_B/0/1/0/all/0/1\">Ben Zevenbergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1\">Ben Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olson_K/0/1/0/all/0/1\">Kristen Olson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1\">Alejandra Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_John_E/0/1/0/all/0/1\">Erin Hoffman-John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Josh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1\">Lora Aroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajakumar_R/0/1/0/all/0/1\">Ravi Rajakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butryna_A/0/1/0/all/0/1\">Alena Butryna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamm_M/0/1/0/all/0/1\">Matthew Lamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmina_V/0/1/0/all/0/1\">Viktoriya Kuzmina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenton_J/0/1/0/all/0/1\">Joe Fenton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Aaron Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_R/0/1/0/all/0/1\">Rachel Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurzweil_R/0/1/0/all/0/1\">Ray Kurzweil</a>, et al. (5 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"On the in vivo recognition of kidney stones using machine learning. (arXiv:2201.08865v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08865","description":"<p>Determining the type of kidney stones allows urologists to prescribe a\ntreatment to avoid recurrence of renal lithiasis. An automated in-vivo\nimage-based classification method would be an important step towards an\nimmediate identification of the kidney stone type required as a first phase of\nthe diagnosis. In the literature it was shown on ex-vivo data (i.e., in very\ncontrolled scene and image acquisition conditions) that an automated kidney\nstone classification is indeed feasible. This pilot study compares the kidney\nstone recognition performances of six shallow machine learning methods and\nthree deep-learning architectures which were tested with in-vivo images of the\nfour most frequent urinary calculi types acquired with an endoscope during\nstandard ureteroscopies. This contribution details the database construction\nand the design of the tested kidney stones classifiers. Even if the best\nresults were obtained by the Inception v3 architecture (weighted precision,\nrecall and F1-score of 0.97, 0.98 and 0.97, respectively), it is also shown\nthat choosing an appropriate colour space and texture features allows a shallow\nmachine learning method to approach closely the performances of the most\npromising deep-learning methods (the XGBoost classifier led to weighted\nprecision, recall and F1-score values of 0.96). This paper is the first one\nthat explores the most discriminant features to be extracted from images\nacquired during ureteroscopies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Estrade_V/0/1/0/all/0/1\">Vincent Estrade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lopez_F/0/1/0/all/0/1\">Francisco Lopez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Flores_Araiza_D/0/1/0/all/0/1\">Daniel Flores-Araiza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beze_J/0/1/0/all/0/1\">Jonathan El Beze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trinh_D/0/1/0/all/0/1\">Dinh-Hoan Trinh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eschwege_P/0/1/0/all/0/1\">Pascal Eschw&#xe8;ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hubert_J/0/1/0/all/0/1\">Jacques Hubert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daul_C/0/1/0/all/0/1\">Christian Daul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-to-Video Re-Identification via Mutual Discriminative Knowledge Transfer. (arXiv:2201.08887v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08887","description":"<p>The gap in representations between image and video makes Image-to-Video\nRe-identification (I2V Re-ID) challenging, and recent works formulate this\nproblem as a knowledge distillation (KD) process. In this paper, we propose a\nmutual discriminative knowledge distillation framework to transfer a\nvideo-based richer representation to an image based representation more\neffectively. Specifically, we propose the triplet contrast loss (TCL), a novel\nloss designed for KD. During the KD process, the TCL loss transfers the local\nstructure, exploits the higher order information, and mitigates the\nmisalignment of the heterogeneous output of teacher and student networks.\nCompared with other losses for KD, the proposed TCL loss selectively transfers\nthe local discriminative features from teacher to student, making it effective\nin the ReID. Besides the TCL loss, we adopt mutual learning to regularize both\nthe teacher and student networks training. Extensive experiments demonstrate\nthe effectiveness of our method on the MARS, DukeMTMC-VideoReID and VeRi-776\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signal Strength and Noise Drive Feature Preference in CNN Image Classifiers. (arXiv:2201.08893v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08893","description":"<p>Feature preference in Convolutional Neural Network (CNN) image classifiers is\nintegral to their decision making process, and while the topic has been well\nstudied, it is still not understood at a fundamental level. We test a range of\ntask relevant feature attributes (including shape, texture, and color) with\nvarying degrees of signal and noise in highly controlled CNN image\nclassification experiments using synthetic datasets to determine feature\npreferences. We find that CNNs will prefer features with stronger signal\nstrength and lower noise irrespective of whether the feature is texture, shape,\nor color. This provides guidance for a predictive model for task relevant\nfeature preferences, demonstrates pathways for bias in machine models that can\nbe avoided with careful controls on experimental setup, and suggests that\ncomparisons between how humans and machines prefer task relevant features in\nvision classification tasks should be revisited. Code to reproduce experiments\nin this paper can be found at\n\\url{https://github.com/mwolff31/signal_preference}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolff_M/0/1/0/all/0/1\">Max Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_S/0/1/0/all/0/1\">Stuart Wolff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ensemble Model for Face Liveness Detection. (arXiv:2201.08901v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08901","description":"<p>In this paper, we present a passive method to detect face presentation attack\na.k.a face liveness detection using an ensemble deep learning technique. Face\nliveness detection is one of the key steps involved in user identity\nverification of customers during the online onboarding/transaction processes.\nDuring identity verification, an unauthenticated user tries to bypass the\nverification system by several means, for example, they can capture a user\nphoto from social media and do an imposter attack using printouts of users\nfaces or using a digital photo from a mobile device and even create a more\nsophisticated attack like video replay attack. We have tried to understand the\ndifferent methods of attack and created an in-house large-scale dataset\ncovering all the kinds of attacks to train a robust deep learning model. We\npropose an ensemble method where multiple features of the face and background\nregions are learned to predict whether the user is a bonafide or an attacker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shashank Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Avinash Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haloi_M/0/1/0/all/0/1\">Mrinal Haloi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_A/0/1/0/all/0/1\">Asif Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAR Image Change Detection Based on Multiscale Capsule Network. (arXiv:2201.08935v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08935","description":"<p>Traditional synthetic aperture radar image change detection methods based on\nconvolutional neural networks (CNNs) face the challenges of speckle noise and\ndeformation sensitivity. To mitigate these issues, we proposed a Multiscale\nCapsule Network (Ms-CapsNet) to extract the discriminative information between\nthe changed and unchanged pixels. On the one hand, the multiscale capsule\nmodule is employed to exploit the spatial relationship of features. Therefore,\nequivariant properties can be achieved by aggregating the features from\ndifferent positions. On the other hand, an adaptive fusion convolution (AFC)\nmodule is designed for the proposed Ms-CapsNet. Higher semantic features can be\ncaptured for the primary capsules. Feature extracted by the AFC module\nsignificantly improves the robustness to speckle noise. The effectiveness of\nthe proposed Ms-CapsNet is verified on three real SAR datasets. The comparison\nexperiments with four state-of-the-art methods demonstrate the efficiency of\nthe proposed method. Our codes are available at\nhttps://github.com/summitgao/SAR_CD_MS_CapsNet .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Heng-Chao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive DropBlock Enhanced Generative Adversarial Networks for Hyperspectral Image Classification. (arXiv:2201.08938v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08938","description":"<p>In recent years, hyperspectral image (HSI) classification based on generative\nadversarial networks (GAN) has achieved great progress. GAN-based\nclassification methods can mitigate the limited training sample dilemma to some\nextent. However, several studies have pointed out that existing GAN-based HSI\nclassification methods are heavily affected by the imbalanced training data\nproblem. The discriminator in GAN always contradicts itself and tries to\nassociate fake labels to the minority-class samples, and thus impair the\nclassification performance. Another critical issue is the mode collapse in\nGAN-based methods. The generator is only capable of producing samples within a\nnarrow scope of the data space, which severely hinders the advancement of\nGAN-based HSI classification methods. In this paper, we proposed an Adaptive\nDropBlock-enhanced Generative Adversarial Networks (ADGAN) for HSI\nclassification. First, to solve the imbalanced training data problem, we adjust\nthe discriminator to be a single classifier, and it will not contradict itself.\nSecond, an adaptive DropBlock (AdapDrop) is proposed as a regularization method\nemployed in the generator and discriminator to alleviate the mode collapse\nissue. The AdapDrop generated drop masks with adaptive shapes instead of a\nfixed size region, and it alleviates the limitations of DropBlock in dealing\nwith ground objects with various shapes. Experimental results on three HSI\ndatasets demonstrated that the proposed ADGAN achieved superior performance\nover state-of-the-art GAN-based methods. Our codes are available at\nhttps://github.com/summitgao/HC_ADGAN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCNGAN: A Deformable Convolutional-Based GAN with QP Adaptation for Perceptual Quality Enhancement of Compressed Video. (arXiv:2201.08944v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08944","description":"<p>In this paper, we propose a deformable convolution-based generative\nadversarial network (DCNGAN) for perceptual quality enhancement of compressed\nvideos. DCNGAN is also adaptive to the quantization parameters (QPs). Compared\nwith optical flows, deformable convolutions are more effective and efficient to\nalign frames. Deformable convolutions can operate on multiple frames, thus\nleveraging more temporal information, which is beneficial for enhancing the\nperceptual quality of compressed videos. Instead of aligning frames in a\npairwise manner, the deformable convolution can process multiple frames\nsimultaneously, which leads to lower computational complexity. Experimental\nresults demonstrate that the proposed DCNGAN outperforms other state-of-the-art\ncompressed video quality enhancement algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Saiping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1\">Marc Gorriz Blanch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_S/0/1/0/all/0/1\">Shuai Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fuzheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Aggregation for Adaptive RGBT Tracking. (arXiv:2201.08949v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08949","description":"<p>Visual object tracking with RGB and thermal infrared (TIR) spectra available,\nshorted in RGBT tracking, is a novel and challenging research topic which draws\nincreasing attention nowadays. In this paper, we propose an RGBT tracker which\ntakes spatio-temporal clues into account for robust appearance model learning,\nand simultaneously, constructs an adaptive fusion sub-network for cross-modal\ninteractions. Unlike most existing RGBT trackers that implement object tracking\ntasks with only spatial information included, temporal information is further\nconsidered in this method. Specifically, different from traditional Siamese\ntrackers, which only obtain one search image during the process of picking up\ntemplate-search image pairs, an extra search sample adjacent to the original\none is selected to predict the temporal transformation, resulting in improved\nrobustness of tracking performance.As for multi-modal tracking, constrained to\nthe limited RGBT datasets, the adaptive fusion sub-network is appended to our\nmethod at the decision level to reflect the complementary characteristics\ncontained in two modalities. To design a thermal infrared assisted RGB tracker,\nthe outputs of the classification head from the TIR modality are taken into\nconsideration before the residual connection from the RGB modality. Extensive\nexperimental results on three challenging datasets, i.e. VOT-RGBT2019, GTOT and\nRGBT210, verify the effectiveness of our method. Code will be shared at\n\\textcolor{blue}{\\emph{https://github.com/Zhangyong-Tang/TAAT}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhangyong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Representation Learning with Self-Supervised Attention for Low-Label High-data Regime. (arXiv:2201.08951v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08951","description":"<p>Self-supervision has shown outstanding results for natural language\nprocessing, and more recently, for image recognition. Simultaneously, vision\ntransformers and its variants have emerged as a promising and scalable\nalternative to convolutions on various computer vision tasks. In this paper, we\nare the first to question if self-supervised vision transformers (SSL-ViTs) can\nbe adapted to two important computer vision tasks in the low-label, high-data\nregime: few-shot image classification and zero-shot image retrieval. The\nmotivation is to reduce the number of manual annotations required to train a\nvisual embedder, and to produce generalizable, semantically meaningful and\nrobust embeddings. For few-shot image classification we train SSL-ViTs without\nany supervision, on external data, and use this trained embedder to adapt\nquickly to novel classes with limited number of labels. For zero-shot image\nretrieval, we use SSL-ViTs pre-trained on a large dataset without any labels\nand fine-tune them with several metric learning objectives. Our self-supervised\nattention representations outperforms the state-of-the-art on several public\nbenchmarks for both tasks, namely miniImageNet and CUB200 for few-shot image\nclassification by up-to 6%-10%, and Stanford Online Products, Cars196 and\nCUB200 for zero-shot image retrieval by up-to 4%-11%. Code is available at\n\\url{https://github.com/AutoVision-cloud/SSL-ViT-lowlabel-highdata}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Prarthana Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaonan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fehervari_I/0/1/0/all/0/1\">Istv&#xe1;n Feh&#xe9;rv&#xe1;ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jason Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedMed-GAN: Federated Multi-Modal Unsupervised Brain Image Synthesis. (arXiv:2201.08953v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08953","description":"<p>Utilizing the paired multi-modal neuroimaging data has been proved to be\neffective to investigate human cognitive activities and certain pathologies.\nHowever, it is not practical to obtain the full set of paired neuroimaging data\ncentrally since the collection faces several constraints, e.g., high\nexamination costs, long acquisition time, and even image corruption. In\naddition, most of the paired neuroimaging data are dispersed into different\nmedical institutions and cannot group together for centralized training\nconsidering the privacy issues. Under the circumstance, there is a clear need\nto launch federated learning and facilitate the integration of other unpaired\ndata from different hospitals or data owners. In this paper, we build up a new\nbenchmark for federated multi-modal unsupervised brain image synthesis (termed\nas FedMed-GAN) to bridge the gap between federated learning and medical GAN.\nMoreover, based on the similarity of edge information across multi-modal\nneuroimaging data, we propose a novel edge loss to solve the generative mode\ncollapse issue of FedMed-GAN and mitigate the performance drop resulting from\ndifferential privacy. Compared with the state-of-the-art method shown in our\nbuilt benchmark, our novel edge loss could significantly speed up the generator\nconvergence rate without sacrificing performance under different unpaired data\ndistribution settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network. (arXiv:2201.08954v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08954","description":"<p>Synthetic aperture radar (SAR) image change detection is a vital yet\nchallenging task in the field of remote sensing image analysis. Most previous\nworks adopt a self-supervised method which uses pseudo-labeled samples to guide\nsubsequent training and testing. However, deep networks commonly require many\nhigh-quality samples for parameter optimization. The noise in pseudo-labels\ninevitably affects the final change detection performance. To solve the\nproblem, we propose a Graph-based Knowledge Supplement Network (GKSNet). To be\nmore specific, we extract discriminative information from the existing labeled\ndataset as additional knowledge, to suppress the adverse effects of noisy\nsamples to some extent. Afterwards, we design a graph transfer module to\ndistill contextual information attentively from the labeled dataset to the\ntarget dataset, which bridges feature correlation between datasets. To validate\nthe proposed method, we conducted extensive experiments on four SAR datasets,\nwhich demonstrated the superiority of the proposed GKSNet as compared to\nseveral state-of-the-art baselines. Our codes are available at\nhttps://github.com/summitgao/SAR_CD_GKSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality Bank: Learn multi-modality images across data centers without sharing medical data. (arXiv:2201.08955v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08955","description":"<p>Multi-modality images have been widely used and provide comprehensive\ninformation for medical image analysis. However, acquiring all modalities among\nall institutes is costly and often impossible in clinical settings. To leverage\nmore comprehensive multi-modality information, we propose a privacy secured\ndecentralized multi-modality adaptive learning architecture named ModalityBank.\nOur method could learn a set of effective domain-specific modulation parameters\nplugged into a common domain-agnostic network. We demonstrate by switching\ndifferent sets of configurations, the generator could output high-quality\nimages for a specific modality. Our method could also complete the missing\nmodalities across all data centers, thus could be used for modality completion\npurposes. The downstream task trained from the synthesized multi-modality\nsamples could achieve higher performance than learning from one real data\ncenter and achieve close-to-real performance compare with all real images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chang_Q/0/1/0/all/0/1\">Qi Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qu_H/0/1/0/all/0/1\">Hui Qu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zhennan Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baskaran_L/0/1/0/all/0/1\">Lohendran Baskaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Efficient Representations for Enhanced Object Detection on Large-scene SAR Images. (arXiv:2201.08958v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08958","description":"<p>It is a challenging problem to detect and recognize targets on complex\nlarge-scene Synthetic Aperture Radar (SAR) images. Recently developed deep\nlearning algorithms can automatically learn the intrinsic features of SAR\nimages, but still have much room for improvement on large-scene SAR images with\nlimited data. In this paper, based on learning representations and multi-scale\nfeatures of SAR images, we propose an efficient and robust deep learning based\ntarget detection method. Especially, by leveraging the effectiveness of\nadversarial autoencoder (AAE) which influences the distribution of the\ninvestigated data explicitly, the raw SAR dataset is augmented into an enhanced\nversion with a large quantity and diversity. Besides, an auto-labeling scheme\nis proposed to improve labeling efficiency. Finally, with jointly training\nsmall target chips and large-scene images, an integrated YOLO network combining\nnon-maximum suppression on sub-images is used to realize multiple targets\ndetection of high resolution images. The numerical experimental results on the\nMSTAR dataset show that our method can realize target detection and recognition\non large-scene images accurately and efficiently. The superior anti-noise\nperformance is also confirmed by experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yue Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Robert C. Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Correlation-based Feature Refinement for Few-shot Counting. (arXiv:2201.08959v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08959","description":"<p>Few-shot counting aims to count objects of any class in an image given only a\nfew exemplars of the same class. Existing correlation-based few-shot counting\napproaches suffer from the coarseness and low semantic level of the\ncorrelation. To solve these problems, we propose an iterative framework to\nprogressively refine the exemplar-related features based on the correlation\nbetween the image and exemplars. Then the density map is predicted from the\nfinal refined feature map. The iterative framework includes a Correlation\nDistillation module and a Feature Refinement module. During the iterations, the\nexemplar-related features are gradually refined, while the exemplar-unrelated\nfeatures are suppressed, benefiting few-shot counting where the\nexemplar-related features are more important. Our approach surpasses all\nbaselines significantly on few-shot counting benchmark FSC-147. Surprisingly,\nthough designed for general class-agnostic counting, our approach still\nachieves state-of-the-art performance on car counting benchmarks CARPK and\nPUCPR+, and crowd counting benchmarks UCSD and Mall. We also achieve\ncompetitive performance on crowd counting benchmark ShanghaiTech. The code will\nbe released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhiyuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xinyi Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Representation for SPD Matrices with Application to Image-Set Classification. (arXiv:2201.08962v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08962","description":"<p>Collaborative representation-based classification (CRC) has demonstrated\nremarkable progress in the past few years because of its closed-form analytical\nsolutions. However, the existing CRC methods are incapable of processing the\nnonlinear variational information directly. Recent advances illustrate that how\nto effectively model these nonlinear variational information and learn\ninvariant representations is an open challenge in the community of computer\nvision and pattern recognition To this end, we try to design a new algorithm to\nhandle this problem. Firstly, the second-order statistic, i.e., covariance\nmatrix is applied to model the original image sets. Due to the space formed by\na set of nonsingular covariance matrices is a well-known Symmetric Positive\nDefinite (SPD) manifold, generalising the Euclidean collaborative\nrepresentation to the SPD manifold is not an easy task. Then, we devise two\nstrategies to cope with this issue. One attempts to embed the SPD\nmanifold-valued data representations into an associated tangent space via the\nmatrix logarithm map. Another is to embed them into a Reproducing Kernel\nHilbert Space (RKHS) by utilizing the Riemannian kernel function. After these\ntwo treatments, CRC is applicable to the SPD manifold-valued features. The\nevaluations on four banchmarking datasets justify its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Li Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffractive all-optical computing for quantitative phase imaging. (arXiv:2201.08964v1 [physics.optics])","link":"http://arxiv.org/abs/2201.08964","description":"<p>Quantitative phase imaging (QPI) is a label-free computational imaging\ntechnique that provides optical path length information of specimens. In modern\nimplementations, the quantitative phase image of an object is reconstructed\ndigitally through numerical methods running in a computer, often using\niterative algorithms. Here, we demonstrate a diffractive QPI network that can\nsynthesize the quantitative phase image of an object by converting the input\nphase information of a scene into intensity variations at the output plane. A\ndiffractive QPI network is a specialized all-optical processor designed to\nperform a quantitative phase-to-intensity transformation through passive\ndiffractive surfaces that are spatially engineered using deep learning and\nimage data. Forming a compact, all-optical network that axially extends only\n~200-300 times the illumination wavelength, this framework can replace\ntraditional QPI systems and related digital computational burden with a set of\npassive transmissive layers. All-optical diffractive QPI networks can\npotentially enable power-efficient, high frame-rate and compact phase imaging\nsystems that might be useful for various applications, including, e.g., on-chip\nmicroscopy and sensing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Mengu_D/0/1/0/all/0/1\">Deniz Mengu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Rectangle Flip Attack: A Query-based Black-box Attack against Object Detection. (arXiv:2201.08970v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08970","description":"<p>Object detection has been widely used in many safety-critical tasks, such as\nautonomous driving. However, its vulnerability to adversarial examples has not\nbeen sufficiently studied, especially under the practical scenario of black-box\nattacks, where the attacker can only access the query feedback of predicted\nbounding-boxes and top-1 scores returned by the attacked model. Compared with\nblack-box attack to image classification, there are two main challenges in\nblack-box attack to detection. Firstly, even if one bounding-box is\nsuccessfully attacked, another sub-optimal bounding-box may be detected near\nthe attacked bounding-box. Secondly, there are multiple bounding-boxes, leading\nto very high attack cost. To address these challenges, we propose a Parallel\nRectangle Flip Attack (PRFA) via random search. We explain the difference\nbetween our method with other attacks in Fig.~\\ref{fig1}. Specifically, we\ngenerate perturbations in each rectangle patch to avoid sub-optimal detection\nnear the attacked region. Besides, utilizing the observation that adversarial\nperturbations mainly locate around objects' contours and critical points under\nwhite-box attacks, the search space of attacked rectangles is reduced to\nimprove the attack efficiency. Moreover, we develop a parallel mechanism of\nattacking multiple rectangles simultaneously to further accelerate the attack\nprocess. Extensive experiments demonstrate that our method can effectively and\nefficiently attack various popular object detectors, including anchor-based and\nanchor-free, and generate transferable adversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Siyuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Adversarial Recognition of Refined Window Structures for Inverse Procedural Fa\\c{c}ade Modeling. (arXiv:2201.08977v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08977","description":"<p>Deep learning methods are notoriously data-hungry, which requires a large\nnumber of labeled samples. Unfortunately, the large amount of interactive\nsample labeling efforts has dramatically hindered the application of deep\nlearning methods, especially for 3D modeling tasks, which require heterogeneous\nsamples. To alleviate the work of data annotation for learned 3D modeling of\nfa\\c{c}ades, this paper proposed a semi-supervised adversarial recognition\nstrategy embedded in inverse procedural modeling. Beginning with textured LOD-2\n(Level-of-Details) models, we use the classical convolutional neural networks\nto recognize the types and estimate the parameters of windows from image\npatches. The window types and parameters are then assembled into procedural\ngrammar. A simple procedural engine is built inside an existing 3D modeling\nsoftware, producing fine-grained window geometries. To obtain a useful model\nfrom a few labeled samples, we leverage the generative adversarial network to\ntrain the feature extractor in a semi-supervised manner. The adversarial\ntraining strategy can also exploit unlabeled data to make the training phase\nmore stable. Experiments using publicly available fa\\c{c}ade image datasets\nreveal that the proposed training strategy can obtain about 10% improvement in\nclassification accuracy and 50% improvement in parameter estimation under the\nsame network structure. In addition, performance gains are more pronounced when\ntesting against unseen data featuring different fa\\c{c}ade styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinrong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yulin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Q/0/1/0/all/0/1\">Qisen Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuming Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruofei Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BBA-net: A bi-branch attention network for crowd counting. (arXiv:2201.08983v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08983","description":"<p>In the field of crowd counting, the current mainstream CNN-based regression\nmethods simply extract the density information of pedestrians without finding\nthe position of each person. This makes the output of the network often found\nto contain incorrect responses, which may erroneously estimate the total number\nand not conducive to the interpretation of the algorithm. To this end, we\npropose a Bi-Branch Attention Network (BBA-NET) for crowd counting, which has\nthree innovation points. i) A two-branch architecture is used to estimate the\ndensity information and location information separately. ii) Attention\nmechanism is used to facilitate feature extraction, which can reduce false\nresponses. iii) A new density map generation method combining geometric\nadaptation and Voronoi split is introduced. Our method can integrate the\npedestrian's head and body information to enhance the feature expression\nability of the density map. Extensive experiments performed on two public\ndatasets show that our method achieves a lower crowd counting error compared to\nother state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liping Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Huizhu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaodong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing and Dissecting Crowd Counting By Synthetic Data. (arXiv:2201.08992v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08992","description":"<p>In this article, we propose a simulated crowd counting dataset CrowdX, which\nhas a large scale, accurate labeling, parameterized realization, and high\nfidelity. The experimental results of using this dataset as data enhancement\nshow that the performance of the proposed streamlined and efficient benchmark\nnetwork ESA-Net can be improved by 8.4\\%. The other two classic heterogeneous\narchitectures MCNN and CSRNet pre-trained on CrowdX also show significant\nperformance improvements. Considering many influencing factors determine\nperformance, such as background, camera angle, human density, and resolution.\nAlthough these factors are important, there is still a lack of research on how\nthey affect crowd counting. Thanks to the CrowdX dataset with rich annotation\ninformation, we conduct a large number of data-driven comparative experiments\nto analyze these factors. Our research provides a reference for a deeper\nunderstanding of the crowd counting problem and puts forward some useful\nsuggestions in the actual deployment of the algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liping Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Huizhu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaodong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Array Network for Low-light Image Enhancement. (arXiv:2201.08996v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08996","description":"<p>Convolution neural networks (CNNs) based methods have dominated the low-light\nimage enhancement tasks due to their outstanding performance. However, the\nconvolution operation is based on a local sliding window mechanism, which is\ndifficult to construct the long-range dependencies of the feature maps.\nMeanwhile, the self-attention based global relationship aggregation methods\nhave been widely used in computer vision, but these methods are difficult to\nhandle high-resolution images because of the high computational complexity. To\nsolve this problem, this paper proposes a Linear Array Self-attention (LASA)\nmechanism, which uses only two 2-D feature encodings to construct 3-D global\nweights and then refines feature maps generated by convolution layers. Based on\nLASA, Linear Array Network (LAN) is proposed, which is superior to the existing\nstate-of-the-art (SOTA) methods in both RGB and RAW based low-light enhancement\ntasks with a smaller amount of parameters. The code is released in\n\\url{https://github.com/cuiziteng/LASA_enhancement}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Ziteng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Ge Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuhua Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-aware Warping for View Synthesis. (arXiv:2201.09023v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09023","description":"<p>Existing image-based rendering methods usually adopt depth-based image\nwarping operation to synthesize novel views. In this paper, we reason the\nessential limitations of the traditional warping operation to be the limited\nneighborhood and only distance-based interpolation weights. To this end, we\npropose content-aware warping, which adaptively learns the interpolation\nweights for pixels of a relatively large neighborhood from their contextual\ninformation via a lightweight neural network. Based on this learnable warping\nmodule, we propose a new end-to-end learning-based framework for novel view\nsynthesis from two input source views, in which two additional modules, namely\nconfidence-based blending and feature-assistant spatial refinement, are\nnaturally proposed to handle the occlusion issue and capture the spatial\ncorrelation among pixels of the synthesized view, respectively. Besides, we\nalso propose a weight-smoothness loss term to regularize the network.\nExperimental results on structured light field datasets with wide baselines and\nunstructured multi-view datasets show that the proposed method significantly\noutperforms state-of-the-art methods both quantitatively and visually. The\nsource code will be publicly available at https://github.com/MantangGuo/CW4VS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mantang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huanqiang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inter-Semantic Domain Adversarial in Histopathological Images. (arXiv:2201.09041v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09041","description":"<p>In computer vision, data shift has proven to be a major barrier for safe and\nrobust deep learning applications. In medical applications, histopathological\nimages are often associated with data shift and they are hardly available. It\nis important to understand to what extent a model can be made robust against\ndata shift using all available data. Here, we first show that domain\nadversarial methods can be very deleterious if they are wrongly used. We then\nuse domain adversarial methods to transfer data shift invariance from one\ndataset to another dataset with different semantics and show that domain\nadversarial methods are efficient inter-semantically with similar performance\nthan intra-semantical domain adversarial methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dumas_N/0/1/0/all/0/1\">Nicolas Dumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derangere_V/0/1/0/all/0/1\">Valentin Derang&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnould_L/0/1/0/all/0/1\">Laurent Arnould</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladoire_S/0/1/0/all/0/1\">Sylvain Ladoire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morel_L/0/1/0/all/0/1\">Louis-Oscar Morel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincon_N/0/1/0/all/0/1\">Nathan Vin&#xe7;on</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware deep learning methods for robust diabetic retinopathy classification. (arXiv:2201.09042v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09042","description":"<p>Automatic classification of diabetic retinopathy from retinal images has been\nwidely studied using deep neural networks with impressive results. However,\nthere is a clinical need for estimation of the uncertainty in the\nclassifications, a shortcoming of modern neural networks. Recently, approximate\nBayesian deep learning methods have been proposed for the task but the studies\nhave only considered the binary referable/non-referable diabetic retinopathy\nclassification applied to benchmark datasets. We present novel results by\nsystematically investigating a clinical dataset and a clinically relevant\n5-class classification scheme, in addition to benchmark datasets and the binary\nclassification scheme. Moreover, we derive a connection between uncertainty\nmeasures and classifier risk, from which we develop a new uncertainty measure.\nWe observe that the previously proposed entropy-based uncertainty measure\ngeneralizes to the clinical dataset on the binary classification scheme but not\non the 5-class scheme, whereas our new uncertainty measure generalizes to the\nlatter case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaskari_J/0/1/0/all/0/1\">Joel Jaskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlsten_J/0/1/0/all/0/1\">Jaakko Sahlsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damoulas_T/0/1/0/all/0/1\">Theodoros Damoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoblauch_J/0/1/0/all/0/1\">Jeremias Knoblauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkka_S/0/1/0/all/0/1\">Simo S&#xe4;rkk&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karkkainen_L/0/1/0/all/0/1\">Leo K&#xe4;rkk&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hietala_K/0/1/0/all/0/1\">Kustaa Hietala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phase-SLAM: Phase Based Simultaneous Localization and Mapping for Mobile Structured Light Illumination Systems. (arXiv:2201.09048v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09048","description":"<p>Structured Light Illumination (SLI) systems have been used for reliable\nindoor dense 3D scanning via phase triangulation. However, mobile SLI systems\nfor 360 degree 3D reconstruction demand 3D point cloud registration, involving\nhigh computational complexity. In this paper, we propose a phase based\nSimultaneous Localization and Mapping (Phase-SLAM) framework for fast and\naccurate SLI sensor pose estimation and 3D object reconstruction. The novelty\nof this work is threefold: (1) developing a reprojection model from 3D points\nto 2D phase data towards phase registration with low computational complexity;\n(2) developing a local optimizer to achieve SLI sensor pose estimation\n(odometry) using the derived Jacobian matrix for the 6 DoF variables; (3)\ndeveloping a compressive phase comparison method to achieve high-efficiency\nloop closure detection. The whole Phase-SLAM pipeline is then exploited using\nexisting global pose graph optimization techniques. We build datasets from both\nthe unreal simulation platform and a robotic arm based SLI system in real-world\nto verify the proposed approach. The experiment results demonstrate that the\nproposed Phase-SLAM outperforms other state-of-the-art methods in terms of the\nefficiency and accuracy of pose estimation and 3D reconstruction. The\nopen-source code is available at https://github.com/ZHENGXi-git/Phase-SLAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1\">Qi Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LTC-SUM: Lightweight Client-driven Personalized Video Summarization Framework Using 2D CNN. (arXiv:2201.09049v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09049","description":"<p>This paper proposes a novel lightweight thumbnail container-based\nsummarization (LTC-SUM) framework for full feature-length videos. This\nframework generates a personalized keyshot summary for concurrent users by\nusing the computational resource of the end-user device. State-of-the-art\nmethods that acquire and process entire video data to generate video summaries\nare highly computationally intensive. In this regard, the proposed LTC-SUM\nmethod uses lightweight thumbnails to handle the complex process of detecting\nevents. This significantly reduces computational complexity and improves\ncommunication and storage efficiency by resolving computational and privacy\nbottlenecks in resource-constrained end-user devices. These improvements were\nachieved by designing a lightweight 2D CNN model to extract features from\nthumbnails, which helped select and retrieve only a handful of specific\nsegments. Extensive quantitative experiments on a set of full 18 feature-length\nvideos (approximately 32.9 h in duration) showed that the proposed method is\nsignificantly computationally efficient than state-of-the-art methods on the\nsame end-user device configurations. Joint qualitative assessments of the\nresults of 56 participants showed that participants gave higher ratings to the\nsummaries generated using the proposed method. To the best of our knowledge,\nthis is the first attempt in designing a fully client-driven personalized\nkeyshot video summarization framework using thumbnail containers for\nfeature-length videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1\">Ghulam Mujtaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1\">Adeel Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_E/0/1/0/all/0/1\">Eun-Seok Ryu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore the Expression: Facial Expression Generation using Auxiliary Classifier Generative Adversarial Network. (arXiv:2201.09061v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09061","description":"<p>Facial expressions are a form of non-verbal communication that humans perform\nseamlessly for meaningful transfer of information. Most of the literature\naddresses the facial expression recognition aspect however, with the advent of\nGenerative Models, it has become possible to explore the affect space in\naddition to mere classification of a set of expressions. In this article, we\npropose a generative model architecture which robustly generates a set of\nfacial expressions for multiple character identities and explores the\npossibilities of generating complex expressions by combining the simple ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_J/0/1/0/all/0/1\">J. Rafid Siddiqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LTC-GIF: Attracting More Clicks on Feature-length Sports Videos. (arXiv:2201.09077v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09077","description":"<p>This paper proposes a lightweight method to attract users and increase views\nof the video by presenting personalized artistic media -- i.e, static\nthumbnails and animated GIFs. This method analyzes lightweight thumbnail\ncontainers (LTC) using computational resources of the client device to\nrecognize personalized events from full-length sports videos. In addition,\ninstead of processing the entire video, small video segments are processed to\ngenerate artistic media. This makes the proposed approach more computationally\nefficient compared to the baseline approaches that create artistic media using\nthe entire video. The proposed method retrieves and uses thumbnail containers\nand video segments, which reduces the required transmission bandwidth as well\nas the amount of locally stored data used during artistic media generation.\nWhen extensive experiments were conducted on the Nvidia Jetson TX2, the\ncomputational complexity of the proposed method was 3.57 times lower than that\nof the SoA method. In the qualitative assessment, GIFs generated using the\nproposed method received 1.02 higher overall ratings compared to the SoA\nmethod. To the best of our knowledge, this is the first technique that uses LTC\nto generate artistic media while providing lightweight and high-performance\nservices even on resource-constrained devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1\">Ghulam Mujtaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaehyuk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_E/0/1/0/all/0/1\">Eun-Seok Ryu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension. (arXiv:2201.09079v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09079","description":"<p>Robust subspace recovery (RSR) is a fundamental problem in robust\nrepresentation learning. Here we focus on a recently proposed RSR method termed\nDual Principal Component Pursuit (DPCP) approach, which aims to recover a basis\nof the orthogonal complement of the subspace and is amenable to handling\nsubspaces of high relative dimension. Prior work has shown that DPCP can\nprovably recover the correct subspace in the presence of outliers, as long as\nthe true dimension of the subspace is known. We show that DPCP can provably\nsolve RSR problems in the {\\it unknown} subspace dimension regime, as long as\northogonality constraints -- adopted in previous DPCP formulations -- are\nrelaxed and random initialization is used instead of spectral one. Namely, we\npropose a very simple algorithm based on running multiple instances of a\nprojected sub-gradient descent method (PSGM), with each problem instance\nseeking to find one vector in the null space of the subspace. We theoretically\nprove that under mild conditions this approach will succeed with high\nprobability. In particular, we show that 1) all of the problem instances will\nconverge to a vector in the nullspace of the subspace and 2) the ensemble of\nproblem instance solutions will be sufficiently diverse to fully span the\nnullspace of the subspace thus also revealing its true unknown codimension. We\nprovide empirical results that corroborate our theoretical results and showcase\nthe remarkable implicit rank regularization behavior of PSGM algorithm that\nallows us to perform RSR without being aware of the subspace dimension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giampouras_P/0/1/0/all/0/1\">Paris V. Giampouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1\">Benjamin D. Haeffele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Ren&#xe9; Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Study on Occlusion Invariant Face Recognition under Face Mask Occlusion. (arXiv:2201.09089v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09089","description":"<p>The face mask is an essential sanitaryware in daily lives growing during the\npandemic period and is a big threat to current face recognition systems. The\nmasks destroy a lot of details in a large area of face, and it makes it\ndifficult to recognize them even for humans. The evaluation report shows the\ndifficulty well when recognizing masked faces. Rapid development and\nbreakthrough of deep learning in the recent past have witnessed most promising\nresults from face recognition algorithms. But they fail to perform far from\nsatisfactory levels in the unconstrained environment during the challenges such\nas varying lighting conditions, low resolution, facial expressions, pose\nvariation and occlusions. Facial occlusions are considered one of the most\nintractable problems. Especially when the occlusion occupies a large region of\nthe face because it destroys lots of official features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hemathilaka_S/0/1/0/all/0/1\">Susith Hemathilaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aponso_A/0/1/0/all/0/1\">Achala Aponso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Unpaired Single Image Super-Resolution of Faces. (arXiv:2201.09109v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09109","description":"<p>We propose an adversarial attack for facial class-specific Single Image\nSuper-Resolution (SISR) methods. Existing attacks, such as the Fast Gradient\nSign Method (FGSM) or the Projected Gradient Descent (PGD) method, are either\nfast but ineffective, or effective but prohibitively slow on these networks. By\nclosely inspecting the surface that the MSE loss, used to train such networks,\ntraces under varying degradations, we were able to identify its parameterizable\nproperty. We leverage this property to propose an adverasrial attack that is\nable to locate the optimum degradation (effective) without needing multiple\ngradient-ascent steps (fast). Our experiments show that the proposed method is\nable to achieve a better speed vs effectiveness trade-off than the\nstate-of-theart adversarial attacks, such as FGSM and PGD, for the task of\nunpaired facial as well as class-specific SISR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1\">Saurabh Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_R/0/1/0/all/0/1\">Rajagopalan A. N</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Potential of Auxiliary-Classifier GANs for Image Classification in Low Data Regimes. (arXiv:2201.09120v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09120","description":"<p>Generative Adversarial Networks (GANs) have shown promise in augmenting\ndatasets and boosting convolutional neural networks' (CNN) performance on image\nclassification tasks. But they introduce more hyperparameters to tune as well\nas the need for additional time and computational power to train supplementary\nto the CNN. In this work, we examine the potential for Auxiliary-Classifier\nGANs (AC-GANs) as a 'one-stop-shop' architecture for image classification,\nparticularly in low data regimes. Additionally, we explore modifications to the\ntypical AC-GAN framework, changing the generator's latent space sampling scheme\nand employing a Wasserstein loss with gradient penalty to stabilize the\nsimultaneous training of image synthesis and classification. Through\nexperiments on images of varying resolutions and complexity, we demonstrate\nthat AC-GANs show promise in image classification, achieving competitive\nperformance with standard CNNs. These methods can be employed as an\n'all-in-one' framework with particular utility in the absence of large amounts\nof training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dravid_A/0/1/0/all/0/1\">Amil Dravid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiffers_F/0/1/0/all/0/1\">Florian Schiffers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1\">Oliver Cossairt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A Review. (arXiv:2201.09130v1 [cs.AI])","link":"http://arxiv.org/abs/2201.09130","description":"<p>Death by suicide is the seventh of the leading death cause worldwide. The\nrecent advancement in Artificial Intelligence (AI), specifically AI application\nin image and voice processing, has created a promising opportunity to\nrevolutionize suicide risk assessment. Subsequently, we have witnessed\nfast-growing literature of researches that applies AI to extract audiovisual\nnon-verbal cues for mental illness assessment. However, the majority of the\nrecent works focus on depression, despite the evident difference between\ndepression signs and suicidal behavior non-verbal cues. In this paper, we\nreview the recent works that study suicide ideation and suicide behavior\ndetection through audiovisual feature analysis, mainly suicidal voice/speech\nacoustic features analysis and suicidal visual cues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhelim_S/0/1/0/all/0/1\">Sahraoui Dhelim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Huansheng Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nugent_C/0/1/0/all/0/1\">Chris Nugent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIDAS: Deep learning human action intention prediction from natural eye movement patterns. (arXiv:2201.09135v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09135","description":"<p>Eye movements have long been studied as a window into the attentional\nmechanisms of the human brain and made accessible as novelty style\nhuman-machine interfaces. However, not everything that we gaze upon, is\nsomething we want to interact with; this is known as the Midas Touch problem\nfor gaze interfaces. To overcome the Midas Touch problem, present interfaces\ntend not to rely on natural gaze cues, but rather use dwell time or gaze\ngestures. Here we present an entirely data-driven approach to decode human\nintention for object manipulation tasks based solely on natural gaze cues. We\nrun data collection experiments where 16 participants are given manipulation\nand inspection tasks to be performed on various objects on a table in front of\nthem. The subjects' eye movements are recorded using wearable eye-trackers\nallowing the participants to freely move their head and gaze upon the scene. We\nuse our Semantic Fovea, a convolutional neural network model to obtain the\nobjects in the scene and their relation to gaze traces at every frame. We then\nevaluate the data and examine several ways to model the classification task for\nintention prediction. Our evaluation shows that intention prediction is not a\nnaive result of the data, but rather relies on non-linear temporal processing\nof gaze cues. We model the task as a time series classification problem and\ndesign a bidirectional Long-Short-Term-Memory (LSTM) network architecture to\ndecode intentions. Our results show that we can decode human intention of\nmotion purely from natural gaze cues and object relative position, with\n$91.9\\%$ accuracy. Our work demonstrates the feasibility of natural gaze as a\nZero-UI interface for human-machine interaction, i.e., users will only need to\nact naturally, and do not need to interact with the interface itself or deviate\nfrom their natural eye movement patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Festor_P/0/1/0/all/0/1\">Paul Festor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafti_A/0/1/0/all/0/1\">Ali Shafti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harston_A/0/1/0/all/0/1\">Alex Harston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Michey Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_P/0/1/0/all/0/1\">Pavel Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisal_A/0/1/0/all/0/1\">A. Aldo Faisal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Flattening Transformers through Decomposed Row and Column Queries for Semantic Segmentation. (arXiv:2201.09139v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09139","description":"<p>It is critical to obtain high resolution features with long range dependency\nfor dense prediction tasks such as semantic segmentation. To generate\nhigh-resolution output of size $H\\times W$ from a low-resolution feature map of\nsize $h\\times w$ ($hw\\ll HW$), a naive dense transformer incurs an intractable\ncomplexity of $\\mathcal{O}(hwHW)$, limiting its application on high-resolution\ndense prediction. We propose a Dual-Flattening Transformer (DFlatFormer) to\nenable high-resolution output by reducing complexity to $\\mathcal{O}(hw(H+W))$\nthat is multiple orders of magnitude smaller than the naive dense transformer.\nDecomposed queries are presented to retrieve row and column attentions\ntractably through separate transformers, and their outputs are combined to form\na dense feature map at high resolution. To this end, the input sequence fed\nfrom an encoder is row-wise and column-wise flattened to align with decomposed\nqueries by preserving their row and column structures, respectively. Row and\ncolumn transformers also interact with each other to capture their mutual\nattentions with the spatial crossings between rows and columns. We also propose\nto perform attentions through efficient grouping and pooling to further reduce\nthe model complexity. Extensive experiments on ADE20K and Cityscapes datasets\ndemonstrate the superiority of the proposed dual-flattening transformer\narchitecture with higher mIoUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Chiuman Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenju Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Z/0/1/0/all/0/1\">Ziwei Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Background Invariant Classification on Infrared Imagery by Data Efficient Training and Reducing Bias in CNNs. (arXiv:2201.09144v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09144","description":"<p>Even though convolutional neural networks can classify objects in images very\naccurately, it is well known that the attention of the network may not always\nbe on the semantically important regions of the scene. It has been observed\nthat networks often learn background textures which are not relevant to the\nobject of interest. In turn this makes the networks susceptible to variations\nand changes in the background which negatively affect their performance. We\npropose a new two-step training procedure called \\textit{split training} to\nreduce this bias in CNNs on both Infrared imagery and RGB data. Our split\ntraining procedure has two steps: using MSE loss first train the layers of the\nnetwork on images with background to match the activations of the same network\nwhen it is trained using images without background; then with these layers\nfrozen, train the rest of the network with cross-entropy loss to classify the\nobjects. Our training method outperforms the traditional training procedure in\nboth a simple CNN architecture, and deep CNNs like VGG and Densenet which use\nlots of hardware resources, and learns to mimic human vision which focuses more\non shape and structure than background with higher accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arif_M/0/1/0/all/0/1\">Maliha Arif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_C/0/1/0/all/0/1\">Calvin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahalanobis_A/0/1/0/all/0/1\">Abhijit Mahalanobis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Network Applications in Creating a Meta-Universe. (arXiv:2201.09152v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09152","description":"<p>Generative Adversarial Networks (GANs) are machine learning methods that are\nused in many important and novel applications. For example, in imaging science,\nGANs are effectively utilized in generating image datasets, photographs of\nhuman faces, image and video captioning, image-to-image translation,\ntext-to-image translation, video prediction, and 3D object generation to name a\nfew. In this paper, we discuss how GANs can be used to create an artificial\nworld. More specifically, we discuss how GANs help to describe an image\nutilizing image/video captioning methods and how to translate the image to a\nnew image using image-to-image translation frameworks in a theme we desire. We\narticulate how GANs impact creating a customized world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amirian_S/0/1/0/all/0/1\">Soheyla Amirian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taha_T/0/1/0/all/0/1\">Thiab R. Taha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Integrated Approach for Video Captioning and Applications. (arXiv:2201.09153v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09153","description":"<p>Physical computing infrastructure, data gathering, and algorithms have\nrecently had significant advances to extract information from images and\nvideos. The growth has been especially outstanding in image captioning and\nvideo captioning. However, most of the advancements in video captioning still\ntake place in short videos. In this research, we caption longer videos only by\nusing the keyframes, which are a small subset of the total video frames.\nInstead of processing thousands of frames, only a few frames are processed\ndepending on the number of keyframes. There is a trade-off between the\ncomputation of many frames and the speed of the captioning process. The\napproach in this research is to allow the user to specify the trade-off between\nexecution time and accuracy. In addition, we argue that linking images, videos,\nand natural language offers many practical benefits and immediate practical\napplications. From the modeling perspective, instead of designing and staging\nexplicit algorithms to process videos and generate captions in complex\nprocessing pipelines, our contribution lies in designing hybrid deep learning\narchitectures to apply in long videos by captioning video keyframes. We\nconsider the technology and the methodology that we have developed as steps\ntoward the applications discussed in this research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amirian_S/0/1/0/all/0/1\">Soheyla Amirian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taha_T/0/1/0/all/0/1\">Thiab R. Taha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSNet: Extremely Light-Weight Siamese Network For Change Detection in Remote Sensing Image. (arXiv:2201.09156v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09156","description":"<p>The Siamese network is becoming the mainstream in change detection of remote\nsensing images (RSI). However, in recent years, the development of more\ncomplicated structure, module and training processe has resulted in the\ncumbersome model, which hampers their application in large-scale RSI\nprocessing. To this end, this paper proposes an extremely lightweight Siamese\nnetwork (LSNet) for RSI change detection, which replaces standard convolution\nwith depthwise separable atrous convolution, and removes redundant dense\nconnections, retaining only valid feature flows while performing Siamese\nfeature fusion, greatly compressing parameters and computation amount. Compared\nwith the first-place model on the CCD dataset, the parameters and the\ncomputation amount of LSNet is greatly reduced by 90.35\\% and 91.34\\%\nrespectively, with only a 1.5\\% drops in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Biyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhixi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pulmonary Fissure Segmentation in CT Images Based on ODoS Filter and Shape Features. (arXiv:2201.09163v1 [eess.IV])","link":"http://arxiv.org/abs/2201.09163","description":"<p>Priori knowledge of pulmonary anatomy plays a vital role in diagnosis of lung\ndiseases. In CT images, pulmonary fissure segmentation is a formidable mission\ndue to various of factors. To address the challenge, an useful approach based\non ODoS filter and shape features is presented for pulmonary fissure\nsegmentation. Here, we adopt an ODoS filter by merging the orientation\ninformation and magnitude information to highlight structure features for\nfissure enhancement, which can effectively distinguish between pulmonary\nfissures and clutters. Motivated by the fact that pulmonary fissures appear as\nlinear structures in 2D space and planar structures in 3D space in orientation\nfield, an orientation curvature criterion and an orientation partition scheme\nare fused to separate fissure patches and other structures in different\norientation partition, which can suppress parts of clutters. Considering the\nshape difference between pulmonary fissures and tubular structures in magnitude\nfield, a shape measure approach and a 3D skeletonization model are combined to\nsegment pulmonary fissures for clutters removal. When applying our scheme to 55\nchest CT scans which acquired from a publicly available LOLA11 datasets, the\nmedian F1-score, False Discovery Rate (FDR), and False Negative Rate (FNR)\nrespectively are 0.896, 0.109, and 0.100, which indicates that the presented\nmethod has a satisfactory pulmonary fissure segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yuanyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luan_P/0/1/0/all/0/1\">Pengpeng Luan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_H/0/1/0/all/0/1\">Hongbin Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1\">Ping Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pre-trained Audio-Visual Transformer for Emotion Recognition. (arXiv:2201.09165v1 [cs.MM])","link":"http://arxiv.org/abs/2201.09165","description":"<p>In this paper, we introduce a pretrained audio-visual Transformer trained on\nmore than 500k utterances from nearly 4000 celebrities from the VoxCeleb2\ndataset for human behavior understanding. The model aims to capture and extract\nuseful information from the interactions between human facial and auditory\nbehaviors, with application in emotion recognition. We evaluate the model\nperformance on two datasets, namely CREMAD-D (emotion classification) and\nMSP-IMPROV (continuous emotion regression). Experimental results show that\nfine-tuning the pre-trained model helps improving emotion classification\naccuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous\nemotion recognition by 0.03-0.09 compared to the same model trained from\nscratch. We also demonstrate the robustness of finetuning the pre-trained model\nin a low-resource setting. With only 10% of the original training set provided,\nfine-tuning the pre-trained model can lead to at least 10% better emotion\nrecognition accuracy and a CCC score improvement by at least 0.1 for continuous\nemotion recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1\">Mohammad Soleymani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed X-Ray Image Separation for Artworks with Concealed Designs. (arXiv:2201.09167v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09167","description":"<p>In this paper, we focus on X-ray images of paintings with concealed\nsub-surface designs (e.g., deriving from reuse of the painting support or\nrevision of a composition by the artist), which include contributions from both\nthe surface painting and the concealed features. In particular, we propose a\nself-supervised deep learning-based image separation approach that can be\napplied to the X-ray images from such paintings to separate them into two\nhypothetical X-ray images. One of these reconstructed images is related to the\nX-ray image of the concealed painting, while the second one contains only\ninformation related to the X-ray of the visible painting. The proposed\nseparation network consists of two components: the analysis and the synthesis\nsub-networks. The analysis sub-network is based on learned coupled iterative\nshrinkage thresholding algorithms (LCISTA) designed using algorithm unrolling\ntechniques, and the synthesis sub-network consists of several linear mappings.\nThe learning algorithm operates in a totally self-supervised fashion without\nrequiring a sample set that contains both the mixed X-ray images and the\nseparated ones. The proposed method is demonstrated on a real painting with\nconcealed content, Do\\~na Isabel de Porcel by Francisco de Goya, to show its\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_W/0/1/0/all/0/1\">Wei Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun-Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_N/0/1/0/all/0/1\">Nathan Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higgitt_C/0/1/0/all/0/1\">Catherine Higgitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daubechies_I/0/1/0/all/0/1\">Ingrid Daubechies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragotti_P/0/1/0/all/0/1\">Pier Luigi Dragotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodigues_M/0/1/0/all/0/1\">Miguel Rodigues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reading-strategy Inspired Visual Representation Learning for Text-to-Video Retrieval. (arXiv:2201.09168v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09168","description":"<p>This paper aims for the task of text-to-video retrieval, where given a query\nin the form of a natural-language sentence, it is asked to retrieve videos\nwhich are semantically relevant to the given query, from a great number of\nunlabeled videos. The success of this task depends on cross-modal\nrepresentation learning that projects both videos and sentences into common\nspaces for semantic similarity computation. In this work, we concentrate on\nvideo representation learning, an essential component for text-to-video\nretrieval. Inspired by the reading strategy of humans, we propose a\nReading-strategy Inspired Visual Representation Learning (RIVRL) to represent\nvideos, which consists of two branches: a previewing branch and an\nintensive-reading branch. The previewing branch is designed to briefly capture\nthe overview information of videos, while the intensive-reading branch is\ndesigned to obtain more in-depth information. Moreover, the intensive-reading\nbranch is aware of the video overview captured by the previewing branch. Such\nholistic information is found to be useful for the intensive-reading branch to\nextract more fine-grained features. Extensive experiments on three datasets are\nconducted, where our model RIVRL achieves a new state-of-the-art on TGIF and\nVATEX. Moreover, on MSR-VTT, our model using two video features shows\ncomparable performance to the state-of-the-art using seven video features and\neven outperforms models pre-trained on the large-scale HowTo100M dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASCNet: Action Semantic Consistent Learning of Arbitrary Progress Levels for Early Action Prediction. (arXiv:2201.09169v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09169","description":"<p>Early action prediction aims to recognize human actions from only a part of\naction execution, which is an important video analysis task for many practical\napplications. Most prior works treat partial or full videos as a whole, which\nneglects the semantic consistencies among partial videos of various progress\nlevels due to their large intra-class variances. In contrast, we partition\noriginal partial or full videos to form a series of new partial videos and mine\nthe Action Semantic Consistent Knowledge (ASCK) among these new partial videos\nevolving in arbitrary progress levels. Moreover, a novel Action Semantic\nConsistent learning network (ASCNet) under the teacher-student framework is\nproposed for early action prediction. Specifically, we treat partial videos as\nnodes and their action semantic consistencies as edges. Then we build a\nbi-directional fully connected graph for the teacher network and a\nsingle-directional fully connected graph for the student network to model ASCK\namong partial videos. The MSE and MMD losses are incorporated as our\ndistillation loss to further transfer the ASCK from the teacher to the student\nnetwork. Extensive experiments and ablative studies have been conducted,\ndemonstrating the effectiveness of modeling ASCK for early action prediction.\nWith the proposed ASCNet, we have achieved state-of-the-art performance on two\nbenchmarks. The code will be released if the paper is accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Di Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Minimize the Remainder in Supervised Learning. (arXiv:2201.09193v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09193","description":"<p>The learning process of deep learning methods usually updates the model's\nparameters in multiple iterations. Each iteration can be viewed as the\nfirst-order approximation of Taylor's series expansion. The remainder, which\nconsists of higher-order terms, is usually ignored in the learning process for\nsimplicity. This learning scheme empowers various multimedia based\napplications, such as image retrieval, recommendation system, and video search.\nGenerally, multimedia data (e.g., images) are semantics-rich and\nhigh-dimensional, hence the remainders of approximations are possibly non-zero.\nIn this work, we consider the remainder to be informative and study how it\naffects the learning process. To this end, we propose a new learning approach,\nnamely gradient adjustment learning (GAL), to leverage the knowledge learned\nfrom the past training iterations to adjust vanilla gradients, such that the\nremainders are minimized and the approximations are improved. The proposed GAL\nis model- and optimizer-agnostic, and is easy to adapt to the standard learning\nframework. It is evaluated on three tasks, i.e., image classification, object\ndetection, and regression, with state-of-the-art models and optimizers. The\nexperiments show that the proposed GAL consistently enhances the evaluated\nmodels, whereas the ablation studies validate various aspects of the proposed\nGAL. The code is available at\n\\url{https://github.com/luoyan407/gradient_adjustment.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Predict Gradients for Semi-Supervised Continual Learning. (arXiv:2201.09196v1 [cs.LG])","link":"http://arxiv.org/abs/2201.09196","description":"<p>A key challenge for machine intelligence is to learn new visual concepts\nwithout forgetting the previously acquired knowledge. Continual learning is\naimed towards addressing this challenge. However, there is a gap between\nexisting supervised continual learning and human-like intelligence, where human\nis able to learn from both labeled and unlabeled data. How unlabeled data\naffects learning and catastrophic forgetting in the continual learning process\nremains unknown. To explore these issues, we formulate a new semi-supervised\ncontinual learning method, which can be generically applied to existing\ncontinual learning models. Specifically, a novel gradient learner learns from\nlabeled data to predict gradients on unlabeled data. Hence, the unlabeled data\ncould fit into the supervised continual learning method. Different from\nconventional semi-supervised settings, we do not hypothesize that the\nunderlying classes, which are associated to the unlabeled data, are known to\nthe learning process. In other words, the unlabeled data could be very distinct\nfrom the labeled data. We evaluate the proposed method on mainstream continual\nlearning, adversarial continual learning, and semi-supervised learning tasks.\nThe proposed method achieves state-of-the-art performance on classification\naccuracy and backward transfer in the continual learning setting while\nachieving desired performance on classification accuracy in the semi-supervised\nlearning setting. This implies that the unlabeled images can enhance the\ngeneralizability of continual learning models on the predictive ability on\nunseen data and significantly alleviate catastrophic forgetting. The code is\navailable at \\url{https://github.com/luoyan407/grad_prediction.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Based UAV Localization System in Denial Environments. (arXiv:2201.09201v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09201","description":"<p>Unmanned Aerial Vehicle (UAV) localization capability is critical in a Global\nNavigation Satellite System (GNSS) denial environment. The aim of this paper is\nto investigate the problem of locating the UAV itself through a purely visual\napproach. This task mainly refers to: matching the corresponding geo-tagged\nsatellite images through the images acquired by the camera when the UAV does\nnot acquire GNSS signals, where the satellite images are the bridge between the\nUAV images and the location information. However, the sampling points of\nprevious cross-view datasets based on UAVs are discrete in spatial distribution\nand the inter-class relationships are not established. In the actual process of\nUAV-localization, the inter-class feature similarity of the proximity position\ndistribution should be small due to the continuity of UAV movement in space. In\nview of this, this paper has reformulated an intensive dataset for UAV\npositioning tasks, which is named DenseUAV, aiming to solve the problems caused\nby spatial distance and scale transformation in practical application\nscenarios, so as to achieve high-precision UAV-localization in GNSS denial\nenvironment. In addition, a new continuum-type evaluation metric named SDM is\ndesigned to evaluate the accuracy of model matching by exploiting the continuum\nof UAVs in space. Specifically, with the ideas of siamese networks and metric\nlearning, a transformer-based baseline was constructed to enhance the capture\nof spatially subtle features. Ultimately, a neighbor-search post-processing\nstrategy was proposed to solve the problem of large distance localisation bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Ming Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinglin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jiedong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Wenbo Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yongheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_E/0/1/0/all/0/1\">Enhui Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeply Explain CNN via Hierarchical Decomposition. (arXiv:2201.09205v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09205","description":"<p>In computer vision, some attribution methods for explaining CNNs attempt to\nstudy how the intermediate features affect the network prediction. However,\nthey usually ignore the feature hierarchies among the intermediate features.\nThis paper introduces a hierarchical decomposition framework to explain CNN's\ndecision-making process in a top-down manner. Specifically, we propose a\ngradient-based activation propagation (gAP) module that can decompose any\nintermediate CNN decision to its lower layers and find the supporting features.\nThen we utilize the gAP module to iteratively decompose the network decision to\nthe supporting evidence from different CNN layers. The proposed framework can\ngenerate a deep hierarchy of strongly associated supporting evidence for the\nnetwork decision, which provides insight into the decision-making process.\nMoreover, gAP is effort-free for understanding CNN-based models without network\narchitecture modification and extra training process. Experiments show the\neffectiveness of the proposed method. The code and interactive demo website\nwill be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Ling-Hao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization. (arXiv:2201.09206v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09206","description":"<p>Cross-view geo-localization is a task of matching the same geographic image\nfrom different views, e.g., unmanned aerial vehicle (UAV) and satellite. The\nmost difficult challenges are the position shift and the uncertainty of\ndistance and scale. Existing methods are mainly aimed at digging for more\ncomprehensive fine-grained information. However, it underestimates the\nimportance of extracting robust feature representation and the impact of\nfeature alignment. The CNN-based methods have achieved great success in\ncross-view geo-localization. However it still has some limitations, e.g., it\ncan only extract part of the information in the neighborhood and some scale\nreduction operations will make some fine-grained information lost. In\nparticular, we introduce a simple and efficient transformer-based structure\ncalled Feature Segmentation and Region Alignment (FSRA) to enhance the model's\nability to understand contextual information as well as to understand the\ndistribution of instances. Without using additional supervisory information,\nFSRA divides regions based on the heat distribution of the transformer's\nfeature map, and then aligns multiple specific regions in different views one\non one. Finally, FSRA integrates each region into a set of feature\nrepresentations. The difference is that FSRA does not divide regions manually,\nbut automatically based on the heat distribution of the feature map. So that\nspecific instances can still be divided and aligned when there are significant\nshifts and scale changes in the image. In addition, a multiple sampling\nstrategy is proposed to overcome the disparity in the number of satellite\nimages and that of images from other sources. Experiments show that the\nproposed method has superior performance and achieves the state-of-the-art in\nboth tasks of drone view target localization and drone navigation. Code will be\nreleased at https://github.com/Dmmm1997/FSRA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Ming Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianhong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jiedong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_E/0/1/0/all/0/1\">Enhui Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Object Tracking on Multi-modal RGB-D Videos: A Review. (arXiv:2201.09207v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09207","description":"<p>The development of visual object tracking has continued for decades. Recent\nyears, as the wide accessibility of the low-cost RGBD sensors, the task of\nvisual object tracking on RGB-D videos has drawn much attention. Compared to\nconventional RGB-only tracking, the RGB-D videos can provide more information\nthat facilitates objecting tracking in some complicated scenarios. The goal of\nthis review is to summarize the relative knowledge of the research filed of\nRGB-D tracking. To be specific, we will generalize the related RGB-D tracking\nbenchmarking datasets as well as the corresponding performance measurements.\nBesides, the existing RGB-D tracking methods are summarized in the paper.\nMoreover, we discuss the possible future direction in the field of RGB-D\ntracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xue-Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design of Sensor Fusion Driver Assistance System for Active Pedestrian Safety. (arXiv:2201.09208v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09208","description":"<p>In this paper, we present a parallel architecture for a sensor fusion\ndetection system that combines a camera and 1D light detection and ranging\n(lidar) sensor for object detection. The system contains two object detection\nmethods, one based on an optical flow, and the other using lidar. The two\nsensors can effectively complement the defects of the other. The accurate\nlongitudinal accuracy of the object's location and its lateral movement\ninformation can be achieved simultaneously. Using a spatio-temporal alignment\nand a policy of sensor fusion, we completed the development of a fusion\ndetection system with high reliability at distances of up to 20 m. Test results\nshow that the proposed system achieves a high level of accuracy for pedestrian\nor object detection in front of a vehicle, and has high robustness to special\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kao_I/0/1/0/all/0/1\">I-Hsi Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yian_Y/0/1/0/all/0/1\">Ya-Zhu Yian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jian-An Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-Horng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perng_J/0/1/0/all/0/1\">Jau-Woei Perng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1\">Tung-Li Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Shueh Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">Min-Shiu Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FN-Net:Remove the Outliers by Filtering the Noise. (arXiv:2201.09213v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09213","description":"<p>Establishing the correspondence between two images is an important research\ndirection of computer vision. When estimating the relationship between two\nimages, it is often disturbed by outliers. In this paper, we propose a\nconvolutional neural network that can filter the noise of outliers. It can\noutput the probability that the pair of feature points is an inlier and regress\nthe essential matrix representing the relative pose of the camera. The outliers\nare mainly caused by the noise introduced by the previous processing. The\noutliers rejection can be treated as a problem of noise elimination, and the\nsoft threshold function has a very good effect on noise reduction. Therefore,\nwe designed an adaptive denoising module based on soft threshold function to\nremove noise components in the outliers, to reduce the probability that the\noutlier is predicted to be an inlier. Experimental results on the YFCC100M\ndataset show that our method exceeds the state-of-the-art in relative pose\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kai Lv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-Driven Lossy Image Compression; A Comprehensive Survey. (arXiv:2201.09240v1 [eess.IV])","link":"http://arxiv.org/abs/2201.09240","description":"<p>In the realm of image processing and computer vision (CV), machine learning\n(ML) architectures are widely applied. Convolutional neural networks (CNNs)\nsolve a wide range of image processing issues and can solve image compression\nproblem. Compression of images is necessary due to bandwidth and memory\nconstraints. Helpful, redundant, and irrelevant information are three different\nforms of information found in images. This paper aims to survey recent\ntechniques utilizing mostly lossy image compression using ML architectures\nincluding different auto-encoders (AEs) such as convolutional auto-encoders\n(CAEs), variational auto-encoders (VAEs), and AEs with hyper-prior models,\nrecurrent neural networks (RNNs), CNNs, generative adversarial networks (GANs),\nprincipal component analysis (PCA) and fuzzy means clustering. We divide all of\nthe algorithms into several groups based on architecture. We cover still image\ncompression in this survey. Various discoveries for the researchers are\nemphasized and possible future directions for researchers. The open research\nproblems such as out of memory (OOM), striped region distortion (SRD),\naliasing, and compatibility of the frameworks with central processing unit\n(CPU) and graphics processing unit (GPU) simultaneously are explained. The\nmajority of the publications in the compression domain surveyed are from the\nprevious five years and use a variety of approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jamil_S/0/1/0/all/0/1\">Sonain Jamil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Piran_M/0/1/0/all/0/1\">Md. Jalil Piran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+MuhibUrRahman/0/1/0/all/0/1\">MuhibUrRahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing the Cost of Model Extraction with Calibrated Proof of Work. (arXiv:2201.09243v1 [cs.CR])","link":"http://arxiv.org/abs/2201.09243","description":"<p>In model extraction attacks, adversaries can steal a machine learning model\nexposed via a public API by repeatedly querying it and adjusting their own\nmodel based on obtained predictions. To prevent model stealing, existing\ndefenses focus on detecting malicious queries, truncating, or distorting\noutputs, thus necessarily introducing a tradeoff between robustness and model\nutility for legitimate users. Instead, we propose to impede model extraction by\nrequiring users to complete a proof-of-work before they can read the model's\npredictions. This deters attackers by greatly increasing (even up to 100x) the\ncomputational effort needed to leverage query access for model extraction.\nSince we calibrate the effort required to complete the proof-of-work to each\nquery, this only introduces a slight overhead for regular users (up to 2x). To\nachieve this, our calibration applies tools from differential privacy to\nmeasure the information revealed by a query. Our method requires no\nmodification of the victim model and can be applied by machine learning\npractitioners to guard their publicly exposed models against being easily\nstolen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1\">Adam Dziedzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaleem_M/0/1/0/all/0/1\">Muhammad Ahmad Kaleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Shen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face recognition via compact second order image gradient orientations. (arXiv:2201.09246v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09246","description":"<p>Conventional subspace learning approaches based on image gradient\norientations only employ the first-order gradient information. However, recent\nresearches on human vision system (HVS) uncover that the neural image is a\nlandscape or a surface whose geometric properties can be captured through the\nsecond order gradient information. The second order image gradient orientations\n(SOIGO) can mitigate the adverse effect of noises in face images. To reduce the\nredundancy of SOIGO, we propose compact SOIGO (CSOIGO) by applying linear\ncomplex principal component analysis (PCA) in SOIGO. Combined with\ncollaborative representation based classification (CRC) algorithm, the\nclassification performance of CSOIGO is further enhanced. CSOIGO is evaluated\nunder real-world disguise, synthesized occlusion and mixed variations.\nExperimental results indicate that the proposed method is superior to its\ncompeting approaches with few training samples, and even outperforms some\nprevailing deep neural network based approaches. The source code of CSOIGO is\navailable at https://github.com/yinhefeng/SOIGO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">He-Feng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaoning Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey. (arXiv:2201.09267v1 [stat.ML])","link":"http://arxiv.org/abs/2201.09267","description":"<p>This is a tutorial and survey paper on metric learning. Algorithms are\ndivided into spectral, probabilistic, and deep metric learning. We first start\nwith the definition of distance metric, Mahalanobis distance, and generalized\nMahalanobis distance. In spectral methods, we start with methods using scatters\nof data, including the first spectral metric learning, relevant methods to\nFisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant\nComponent Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric\nlearning, imbalanced metric learning, locally linear metric adaptation, and\nadversarial metric learning are covered. We also explain several kernel\nspectral methods for metric learning in the feature space. We also introduce\ngeometric metric learning methods on the Riemannian manifolds. In probabilistic\nmethods, we start with collapsing classes in both input and feature spaces and\nthen explain the neighborhood component analysis methods, Bayesian metric\nlearning, information theoretic methods, and empirical risk minimization in\nmetric learning. In deep learning methods, we first introduce reconstruction\nautoencoders and supervised loss functions for metric learning. Then, Siamese\nnetworks and its various loss functions, triplet mining, and triplet sampling\nare explained. Deep discriminant analysis methods, based on Fisher discriminant\nanalysis, are also reviewed. Finally, we introduce multi-modal deep metric\nlearning, geometric metric learning by neural networks, and few-shot metric\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet-Attention CNN for Image Classification. (arXiv:2201.09271v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09271","description":"<p>The feature learning methods based on convolutional neural network (CNN) have\nsuccessfully produced tremendous achievements in image classification tasks.\nHowever, the inherent noise and some other factors may weaken the effectiveness\nof the convolutional feature statistics. In this paper, we investigate Discrete\nWavelet Transform (DWT) in the frequency domain and design a new\nWavelet-Attention (WA) block to only implement attention in the high-frequency\ndomain. Based on this, we propose a Wavelet-Attention convolutional neural\nnetwork (WA-CNN) for image classification. Specifically, WA-CNN decomposes the\nfeature maps into low-frequency and high-frequency components for storing the\nstructures of the basic objects, as well as the detailed information and noise,\nrespectively. Then, the WA block is leveraged to capture the detailed\ninformation in the high-frequency domain with different attention factors but\nreserves the basic object structures in the low-frequency domain. Experimental\nresults on CIFAR-10 and CIFAR-100 datasets show that our proposed WA-CNN\nachieves significant improvements in classification accuracy compared to other\nrelated networks. Specifically, based on MobileNetV2 backbones, WA-CNN achieves\n1.26% Top-1 accuracy improvement on the CIFAR-10 benchmark and 1.54% Top-1\naccuracy improvement on the CIFAR-100 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiangyu_Z/0/1/0/all/0/1\">Zhao Xiangyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to scale hyperparameters for quickshift image segmentation. (arXiv:2201.09286v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09286","description":"<p>Quickshift is a popular algorithm for image segmentation, used as a\npreprocessing step in many applications. Unfortunately, it is quite challenging\nto understand the hyperparameters' influence on the number and shape of\nsuperpixels produced by the method. In this paper, we study theoretically a\nslightly modified version of the quickshift algorithm, with a particular\nemphasis on homogeneous image patches with i.i.d. pixel noise and sharp\nboundaries between such patches. Leveraging this analysis, we derive a simple\nheuristic to scale quickshift hyperparameters when dealing with real images,\nwhich we check empirically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey for Deep RGBT Tracking. (arXiv:2201.09296v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09296","description":"<p>Visual object tracking with the visible (RGB) and thermal infrared (TIR)\nelectromagnetic waves, shorted in RGBT tracking, recently draws increasing\nattention in the tracking community. Considering the rapid development of deep\nlearning, a survey for the recent deep neural network based RGBT trackers is\npresented in this paper. Firstly, we give brief introduction for the RGBT\ntrackers concluded into this category. Then, a comparison among the existing\nRGBT trackers on several challenging benchmarks is given statistically.\nSpecifically, MDNet and Siamese architectures are the two mainstream frameworks\nin the RGBT community, especially the former. Trackers based on MDNet achieve\nhigher performance while Siamese-based trackers satisfy the real-time\nrequirement. In summary, since the large-scale dataset LasHeR is published, the\nintegration of end-to-end framework, e.g., Siamese and Transformer, should be\nfurther considered to fulfil the real-time as well as more robust performance.\nFurthermore, the mathematical meaning should be more considered during\ndesigning the network. This survey can be treated as a look-up-table for\nresearchers who are concerned about RGBT tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhangyong Tang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a> (1) ((1) Jiangnan University, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"1000x Faster Camera and Machine Vision with Ordinary Devices. (arXiv:2201.09302v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09302","description":"<p>In digital cameras, we find a major limitation: the image and video form\ninherited from a film camera obstructs it from capturing the rapidly changing\nphotonic world. Here, we present vidar, a bit sequence array where each bit\nrepresents whether the accumulation of photons has reached a threshold, to\nrecord and reconstruct the scene radiance at any moment. By employing only\nconsumer-level CMOS sensors and integrated circuits, we have developed a vidar\ncamera that is 1,000x faster than conventional cameras. By treating vidar as\nspike trains in biological vision, we have further developed a spiking neural\nnetwork-based machine vision system that combines the speed of the machine and\nthe mechanism of biological vision, achieving high-speed object detection and\ntracking 1,000x faster than human vision. We demonstrate the utility of the\nvidar camera and the super vision system in an assistant referee and target\npointing system. Our study is expected to fundamentally revolutionize the image\nand video concepts and related industries, including photography, movies, and\nvisual media, and to unseal a new spiking neural network-enabled speed-free\nmachine vision era.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yajing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siwei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Shanshan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yihua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Si Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Basket-based Softmax. (arXiv:2201.09308v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09308","description":"<p>Softmax-based losses have achieved state-of-the-art performances on various\ntasks such as face recognition and re-identification. However, these methods\nhighly relied on clean datasets with global labels, which limits their usage in\nmany real-world applications. An important reason is that merging and\norganizing datasets from various temporal and spatial scenarios is usually not\nrealistic, as noisy labels can be introduced and exponential-increasing\nresources are required. To address this issue, we propose a novel\nmining-during-training strategy called Basket-based Softmax (BBS) as well as\nits parallel version to effectively train models on multiple datasets in an\nend-to-end fashion. Specifically, for each training sample, we simultaneously\nadopt similarity scores as the clue to mining negative classes from other\ndatasets, and dynamically add them to assist the learning of discriminative\nfeatures. Experimentally, we demonstrate the efficiency and superiority of the\nBBS on the tasks of face recognition and re-identification, with both simulated\nand real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qiang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xinqian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual cGAN for MRI Super-resolution. (arXiv:2201.09314v1 [eess.IV])","link":"http://arxiv.org/abs/2201.09314","description":"<p>Capturing high-resolution magnetic resonance (MR) images is a time consuming\nprocess, which makes it unsuitable for medical emergencies and pediatric\npatients. Low-resolution MR imaging, by contrast, is faster than its\nhigh-resolution counterpart, but it compromises on fine details necessary for a\nmore precise diagnosis. Super-resolution (SR), when applied to low-resolution\nMR images, can help increase their utility by synthetically generating\nhigh-resolution images with little additional time. In this paper, we present a\nSR technique for MR images that is based on generative adversarial networks\n(GANs), which have proven to be quite useful in generating sharp-looking\ndetails in SR. We introduce a conditional GAN with perceptual loss, which is\nconditioned upon the input low-resolution image, which improves the performance\nfor isotropic and anisotropic MRI super-resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nasser_S/0/1/0/all/0/1\">Sahar Almahfouz Nasser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shamsi_S/0/1/0/all/0/1\">Saqib Shamsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bundele_V/0/1/0/all/0/1\">Valay Bundele</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_B/0/1/0/all/0/1\">Bhavesh Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse-view Cone Beam CT Reconstruction using Data-consistent Supervised and Adversarial Learning from Scarce Training Data. (arXiv:2201.09318v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09318","description":"<p>Reconstruction of CT images from a limited set of projections through an\nobject is important in several applications ranging from medical imaging to\nindustrial settings. As the number of available projections decreases,\ntraditional reconstruction techniques such as the FDK algorithm and model-based\niterative reconstruction methods perform poorly. Recently, data-driven methods\nsuch as deep learning-based reconstruction have garnered a lot of attention in\napplications because they yield better performance when enough training data is\navailable. However, even these methods have their limitations when there is a\nscarcity of available training data. This work focuses on image reconstruction\nin such settings, i.e., when both the number of available CT projections and\nthe training data is extremely limited. We adopt a sequential reconstruction\napproach over several stages using an adversarially trained shallow network for\n'destreaking' followed by a data-consistency update in each stage. To deal with\nthe challenge of limited data, we use image subvolumes to train our method, and\npatch aggregation during testing. To deal with the computational challenge of\nlearning on 3D datasets for 3D reconstruction, we use a hybrid 3D-to-2D mapping\nnetwork for the 'destreaking' part. Comparisons to other methods over several\ntest examples indicate that the proposed method has much potential, when both\nthe number of projections and available training data are highly limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1\">Anish Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasky_M/0/1/0/all/0/1\">Marc Klasky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fessler_J/0/1/0/all/0/1\">Jeffrey A. Fessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1\">Saiprasad Ravishankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out of Distribution Detection on ImageNet-O. (arXiv:2201.09352v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09352","description":"<p>Out of distribution (OOD) detection is a crucial part of making machine\nlearning systems robust. The ImageNet-O dataset is an important tool in testing\nthe robustness of ImageNet trained deep neural networks that are widely used\nacross a variety of systems and applications. We aim to perform a comparative\nanalysis of OOD detection methods on ImageNet-O, a first of its kind dataset\nwith a label distribution different than that of ImageNet, that has been\ncreated to aid research in OOD detection for ImageNet models. As this dataset\nis fairly new, we aim to provide a comprehensive benchmarking of some of the\ncurrent state of the art OOD detection methods on this novel dataset. This\nbenchmarking covers a variety of model architectures, settings where we haves\nprior access to the OOD data versus when we don't, predictive score based\napproaches, deep generative approaches to OOD detection, and more.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Anugya Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shriya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thigle_M/0/1/0/all/0/1\">Mugdha Thigle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey and Systematization of 3D Object Detection Models and Methods. (arXiv:2201.09354v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09354","description":"<p>This paper offers a comprehensive survey of recent developments in 3D object\ndetection covering the full pipeline from input data, over data representation\nand feature extraction to the actual detection modules. We include basic\nconcepts, focus our survey on a broad spectrum of different approaches arising\nin the last ten years and propose a systematization which offers a practical\nframework to compare those approaches on the methods level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drobnitzky_M/0/1/0/all/0/1\">Moritz Drobnitzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friederich_J/0/1/0/all/0/1\">Jonas Friederich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zschech_P/0/1/0/all/0/1\">Patrick Zschech</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based SAR Image Despeckling. (arXiv:2201.09355v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09355","description":"<p>Synthetic Aperture Radar (SAR) images are usually degraded by a\nmultiplicative noise known as speckle which makes processing and interpretation\nof SAR images difficult. In this paper, we introduce a transformer-based\nnetwork for SAR image despeckling. The proposed despeckling network comprises\nof a transformer-based encoder which allows the network to learn global\ndependencies between different image regions - aiding in better despeckling.\nThe network is trained end-to-end with synthetically generated speckled images\nusing a composite loss function. Experiments show that the proposed method\nachieves significant improvements over traditional and convolutional neural\nnetwork-based despeckling methods on both synthetic and real SAR images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Malsha V. Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for COVID-19 Detection. (arXiv:2201.09360v1 [eess.IV])","link":"http://arxiv.org/abs/2201.09360","description":"<p>A critical step in the fight against COVID-19, which continues to have a\ncatastrophic impact on peoples lives, is the effective screening of patients\npresented in the clinics with severe COVID-19 symptoms. Chest radiography is\none of the promising screening approaches. Many studies reported detecting\nCOVID-19 in chest X-rays accurately using deep learning. A serious limitation\nof many published approaches is insufficient attention paid to explaining\ndecisions made by deep learning models. Using explainable artificial\nintelligence methods, we demonstrate that model decisions may rely on\nconfounding factors rather than medical pathology. After an analysis of\npotential confounding factors found on chest X-ray images, we propose a novel\nmethod to minimise their negative impact. We show that our proposed method is\nmore robust than previous attempts to counter confounding factors such as ECG\nleads in chest X-rays that often influence model classification decisions. In\naddition to being robust, our method achieves results comparable to the\nstate-of-the-art. The source code and pre-trained weights are publicly\navailable (https://github.com/tomek1911/POTHER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Szczepanski_T/0/1/0/all/0/1\">Tomasz Szczepa&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v1 [cs.GR])","link":"http://arxiv.org/abs/2201.09367","description":"<p>The freeform architectural modeling process often involves two important\nstages: concept design and digital modeling. In the first stage, architects\nusually sketch the overall 3D shape and the panel layout on a physical or\ndigital paper briefly. In the second stage, a digital 3D model is created using\nthe sketching as the reference. The digital model needs to incorporate\ngeometric requirements for its components, such as planarity of panels due to\nconsideration of construction costs, which can make the modeling process more\nchallenging. In this work, we present a novel sketch-based system to bridge the\nconcept design and digital modeling of freeform roof-like shapes represented as\nplanar quadrilateral (PQ) meshes. Our system allows the user to sketch the\nsurface boundary and contour lines under axonometric projection and supports\nthe sketching of occluded regions. In addition, the user can sketch feature\nlines to provide directional guidance to the PQ mesh layout. Given the 2D\nsketch input, we propose a deep neural network to infer in real-time the\nunderlying surface shape along with a dense conjugate direction field, both of\nwhich are used to extract the final PQ mesh. To train and validate our network,\nwe generate a large synthetic dataset that mimics architect sketching of\nfreeform quadrilateral patches. The effectiveness and usability of our system\nare demonstrated with quantitative and qualitative evaluation as well as user\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabi_W/0/1/0/all/0/1\">Wassim Jabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Severely Deformed Mesh Reconstruction (DMR) from a Single-View Image. (arXiv:2201.09373v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09373","description":"<p>Much progress has been made in the supervised learning of 3D reconstruction\nof rigid objects from multi-view images or a video. However, it is more\nchallenging to reconstruct severely deformed objects from a single-view RGB\nimage in an unsupervised manner. Although training-based methods, such as\nspecific category-level training, have been shown to successfully reconstruct\nrigid objects and slightly deformed objects like birds from a single-view\nimage, they cannot effectively handle severely deformed objects and neither can\nbe applied to some downstream tasks in the real world due to the inconsistent\nsemantic meaning of vertices, which are crucial in defining the adopted 3D\ntemplates of objects to be reconstructed. In this work, we introduce a\ntemplate-based method to infer 3D shapes from a single-view image and apply the\nreconstructed mesh to a downstream task, i.e., absolute length measurement.\nWithout using 3D ground truth, our method faithfully reconstructs 3D meshes and\nachieves state-of-the-art accuracy in a length measurement task on a severely\ndeformed fish dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingxi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romain_S/0/1/0/all/0/1\">Suzanne Romain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Craig Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magrane_K/0/1/0/all/0/1\">Kelsey Magrane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeeSon_G/0/1/0/all/0/1\">Graeme LeeSon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReconFormer: Accelerated MRI Reconstruction Using Recurrent Transformer. (arXiv:2201.09376v1 [eess.IV])","link":"http://arxiv.org/abs/2201.09376","description":"<p>Accelerating magnetic resonance image (MRI) reconstruction process is a\nchallenging ill-posed inverse problem due to the excessive under-sampling\noperation in k-space. In this paper, we propose a recurrent transformer model,\nnamely \\textbf{ReconFormer}, for MRI reconstruction which can iteratively\nreconstruct high fertility magnetic resonance images from highly under-sampled\nk-space data. In particular, the proposed architecture is built upon Recurrent\nPyramid Transformer Layers (RPTL), which jointly exploits intrinsic multi-scale\ninformation at every architecture unit as well as the dependencies of the deep\nfeature correlation through recurrent states. Moreover, the proposed\nReconFormer is lightweight since it employs the recurrent structure for its\nparameter efficiency. We validate the effectiveness of ReconFormer on multiple\ndatasets with different magnetic resonance sequences and show that it achieves\nsignificant improvements over the state-of-the-art methods with better\nparameter efficiency. Implementation code will be available in\nhttps://github.com/guopengf/ReconFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_Y/0/1/0/all/0/1\">Yiqun Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jinyuan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1\">Shanshan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"vCLIMB: A Novel Video Class Incremental Learning Benchmark. (arXiv:2201.09381v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09381","description":"<p>Continual learning (CL) is under-explored in the video domain. The few\nexisting works contain splits with imbalanced class distributions over the\ntasks, or study the problem in unsuitable datasets. We introduce vCLIMB, a\nnovel video continual learning benchmark. vCLIMB is a standardized test-bed to\nanalyze catastrophic forgetting of deep models in video continual learning. In\ncontrast to previous work, we focus on class incremental continual learning\nwith models trained on a sequence of disjoint tasks, and distribute the number\nof classes uniformly across the tasks. We perform in-depth evaluations of\nexisting CL methods in vCLIMB, and observe two unique challenges in video data.\nThe selection of instances to store in episodic memory is performed at the\nframe level. Second, untrimmed training data influences the effectiveness of\nframe sampling strategies. We address these two challenges by proposing a\ntemporal consistency regularization that can be applied on top of memory-based\ncontinual learning methods. Our approach significantly improves the baseline,\nby up to 24% on the untrimmed continual learning task. To streamline and foster\nfuture research in video continual learning, we will publicly release the code\nfor our benchmark and method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villa_A/0/1/0/all/0/1\">Andr&#xe9;s Villa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhamoud_K/0/1/0/all/0/1\">Kumail Alhamoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escorcia_V/0/1/0/all/0/1\">Victor Escorcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Federated Learning: Concept and Applications. (arXiv:2201.09384v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09384","description":"<p>This paper provides a comprehensive study of Federated Learning (FL) with an\nemphasis on components, challenges, applications and FL environment. FL can be\napplicable in multiple fields and domains in real-life models. in the medical\nsystem, the privacy of patients records and their medical condition is critical\ndata, therefore collaborative learning or federated learning comes into the\npicture. On other hand build an intelligent system assist the medical staff\nwithout sharing the data lead into the FL concept and one of the applications\nthat are used is a brain tumor diagnosis intelligent system based on AI methods\nthat can efficiently work in a collaborative environment.this paper will\nintroduce some of the applications and related work in the medical field and\nwork under the FL concept then summarize them to introduce the main limitations\nof their work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahlool_D/0/1/0/all/0/1\">Dhurgham Hassan Mahlool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abed_M/0/1/0/all/0/1\">Mohammed Hamzah Abed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Patients Privacy Protection with Stganography and Visual Encryption. (arXiv:2201.09388v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09388","description":"<p>In this survey, thirty models for steganography and visual encryption methods\nhave been discussed to provide patients privacy protection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alzubaidy_H/0/1/0/all/0/1\">Hussein K. Alzubaidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shammary_D/0/1/0/all/0/1\">Dhiah Al-Shammary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abed_M/0/1/0/all/0/1\">Mohammed Hamzah Abed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks. (arXiv:2201.09390v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09390","description":"<p>This work proposes an attention-based sequence-to-sequence model for\nhandwritten word recognition and explores transfer learning for data-efficient\ntraining of HTR systems. To overcome training data scarcity, this work\nleverages models pre-trained on scene text images as a starting point towards\ntailoring the handwriting recognition models. ResNet feature extraction and\nbidirectional LSTM-based sequence modeling stages together form an encoder. The\nprediction stage consists of a decoder and a content-based attention mechanism.\nThe effectiveness of the proposed end-to-end HTR system has been empirically\nevaluated on a novel multi-writer dataset Imgur5K and the IAM dataset. The\nexperimental results evaluate the performance of the HTR framework, further\nsupported by an in-depth analysis of the error cases. Source code and\npre-trained models are available at https://github.com/dmitrijsk/AttentionHTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kass_D/0/1/0/all/0/1\">Dmitrijs Kass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_E/0/1/0/all/0/1\">Ekta Vats</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISeval: a Metric Library for Medical Image Segmentation Evaluation. (arXiv:2201.09395v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09395","description":"<p>Correct performance assessment is crucial for evaluating modern artificial\nintelligence algorithms in medicine like deep-learning based medical image\nsegmentation models. However, there is no universal metric library in Python\nfor standardized and reproducible evaluation. Thus, we propose our open-source\npublicly available Python package MISeval: a metric library for Medical Image\nSegmentation Evaluation. The implemented metrics can be intuitively used and\neasily integrated into any performance assessment pipeline. The package\nutilizes modern CI/CD strategies to ensure functionality and stability. MISeval\nis available from PyPI (miseval) and GitHub:\nhttps://github.com/frankkramer-lab/miseval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1\">Dominik M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_D/0/1/0/all/0/1\">Dennis Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_P/0/1/0/all/0/1\">Philip Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_F/0/1/0/all/0/1\">Florian Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_Rey_I/0/1/0/all/0/1\">I&#xf1;aki Soto-Rey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Label Assignment for Object Detection by Combining Predicted and Anchor IoUs. (arXiv:2201.09396v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09396","description":"<p>Label assignment plays a significant role in modern object detection models.\nDetection models may yield totally different performances with different label\nassignment strategies. For anchor-based detection models, the IoU threshold\nbetween the anchors and their corresponding ground truth bounding boxes is the\nkey element since the positive samples and negative samples are divided by the\nIoU threshold. Early object detectors simply utilize a fixed threshold for all\ntraining samples, while recent detection algorithms focus on adaptive\nthresholds based on the distribution of the IoUs to the ground truth boxes. In\nthis paper, we introduce a simple and effective approach to perform label\nassignment dynamically based on the training status with predictions. By\nintroducing the predictions in label assignment, more high-quality samples with\nhigher IoUs to the ground truth objects are selected as the positive samples,\nwhich could reduce the discrepancy between the classification scores and the\nIoU scores, and generate more high-quality boundary boxes. Our approach shows\nimprovements in the performance of the detection models with the adaptive label\nassignment algorithm and lower bounding box losses for those positive samples,\nindicating more samples with higher quality predicted boxes are selected as\npositives. The source code will be available at\nhttps://github.com/ZTX-100/DLA-Combined-IoUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharda_A/0/1/0/all/0/1\">Ajay Sharda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v1 [eess.IV])","link":"http://arxiv.org/abs/2201.09400","description":"<p>Magnetic resonance imaging (MRI) is a widely used non-radiative and\nnon-invasive method for clinical interrogation of organ structures and\nmetabolism, with an inherently long scanning time. Methods by k-space\nundersampling and deep learning based reconstruction have been popularised to\naccelerate the scanning process. This work focuses on investigating how\npowerful transformers are for fast MRI by exploiting and comparing different\nnovel network architectures. In particular, a generative adversarial network\n(GAN) based Swin transformer (ST-GAN) was introduced for the fast MRI\nreconstruction. To further preserve the edge and texture information, edge\nenhanced GAN based Swin transformer (EESGAN) and texture enhanced GAN based\nSwin transformer (TES-GAN) were also developed, where a dual-discriminator GAN\nstructure was applied. We compared our proposed GAN based transformers,\nstandalone Swin transformer and other convolutional neural networks based based\nGAN model in terms of the evaluation metrics PSNR, SSIM and FID. We showed that\ntransformers work well for the MRI reconstruction from different undersampling\nconditions. The utilisation of GAN's adversarial structure improves the quality\nof images reconstructed when undersampled for 30% or higher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huanjun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Chest X-Ray Report Generation by Leveraging Warm-Starting. (arXiv:2201.09405v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09405","description":"<p>Automatically generating a report from a patient's Chest X-Rays (CXRs) is a\npromising solution to reducing clinical workload and improving patient care.\nHowever, current CXR report generators, which are predominantly\nencoder-to-decoder models, lack the diagnostic accuracy to be deployed in a\nclinical setting. To improve CXR report generation, we investigate\nwarm-starting the encoder and decoder with recent open-source computer vision\nand natural language processing checkpoints, such as the Vision Transformer\n(ViT) and PubMedBERT. To this end, each checkpoint is evaluated on the\nMIMIC-CXR and IU X-Ray datasets using natural language generation and Clinical\nEfficacy (CE) metrics. Our experimental investigation demonstrates that the\nConvolutional vision Transformer (CvT) ImageNet-21K and the Distilled\nGenerative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for\nwarm-starting the encoder and decoder, respectively. Compared to the\nstate-of-the-art (M2 Transformer Progressive), CvT2DistilGPT2 attained an\nimprovement of 8.3% for CE F-1, 1.8% for BLEU-4, 1.6% for ROUGE-L, and 1.0% for\nMETEOR. The reports generated by CvT2DistilGPT2 are more diagnostically\naccurate and have a higher similarity to radiologist reports than previous\napproaches. By leveraging warm-starting, CvT2DistilGPT2 brings automatic CXR\nreport generation one step closer to the clinical setting. CvT2DistilGPT2 and\nits MIMIC-CXR checkpoint are available at\nhttps://github.com/aehrc/cvt2distilgpt2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nicolson_A/0/1/0/all/0/1\">Aaron Nicolson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowling_J/0/1/0/all/0/1\">Jason Dowling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koopman_B/0/1/0/all/0/1\">Bevan Koopman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Document Layout Analysis via Unsupervised Document Style Guide. (arXiv:2201.09407v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09407","description":"<p>The document layout analysis (DLA) aims to decompose document images into\nhigh-level semantic areas (i.e., figures, tables, texts, and background).\nCreating a DLA framework with strong generalization capabilities is a challenge\ndue to document objects are diversity in layout, size, aspect ratio, texture,\netc. Many researchers devoted this challenge by synthesizing data to build\nlarge training sets. However, the synthetic training data has different styles\nand erratic quality. Besides, there is a large gap between the source data and\nthe target data. In this paper, we propose an unsupervised cross-domain DLA\nframework based on document style guidance. We integrated the document quality\nassessment and the document cross-domain analysis into a unified framework. Our\nframework is composed of three components, Document Layout Generator (GLD),\nDocument Elements Decorator(GED), and Document Style Discriminator(DSD). The\nGLD is used to document layout generates, the GED is used to document layout\nelements fill, and the DSD is used to document quality assessment and\ncross-domain guidance. First, we apply GLD to predict the positions of the\ngenerated document. Then, we design a novel algorithm based on aesthetic\nguidance to fill the document positions. Finally, we use contrastive learning\nto evaluate the quality assessment of the document. Besides, we design a new\nstrategy to change the document quality assessment component into a document\ncross-domain style guide component. Our framework is an unsupervised document\nlayout analysis framework. We have proved through numerous experiments that our\nproposed method has achieved remarkable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingjiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Luwei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiangcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yingbin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tianlong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Attention-based Hybrid Dimensional Network for Multimodal Imaging Computer-aided Diagnosis. (arXiv:2201.09421v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09421","description":"<p>Recent works on Multimodal 3D Computer-aided diagnosis have demonstrated that\nobtaining a competitive automatic diagnosis model when a 3D convolution neural\nnetwork (CNN) brings more parameters and medical images are scarce remains\nnontrivial and challenging. Considering both consistencies of regions of\ninterest in multimodal images and diagnostic accuracy, we propose a novel\nmutual attention-based hybrid dimensional network for MultiModal 3D medical\nimage classification (MMNet). The hybrid dimensional network integrates 2D CNN\nwith 3D convolution modules to generate deeper and more informative feature\nmaps, and reduce the training complexity of 3D fusion. Besides, the pre-trained\nmodel of ImageNet can be used in 2D CNN, which improves the performance of the\nmodel. The stereoscopic attention is focused on building rich contextual\ninterdependencies of the region in 3D medical images. To improve the regional\ncorrelation of pathological tissues in multimodal medical images, we further\ndesign a mutual attention framework in the network to build the region-wise\nconsistency in similar stereoscopic regions of different image modalities,\nproviding an implicit manner to instruct the network to focus on pathological\ntissues. MMNet outperforms many previous solutions and achieves results\ncompetitive to the state-of-the-art on three multimodal imaging datasets, i.e.,\nParotid Gland Tumor (PGT) dataset, the MRNet dataset, and the PROSTATEx\ndataset, and its advantages are validated by extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniFormer: Unifying Convolution and Self-attention for Visual Recognition. (arXiv:2201.09450v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09450","description":"<p>It is a challenging task to learn discriminative representation from images\nand videos, due to large local redundancy and complex global dependency in\nthese visual data. Convolution neural networks (CNNs) and vision transformers\n(ViTs) have been two dominant frameworks in the past few years. Though CNNs can\nefficiently decrease local redundancy by convolution within a small\nneighborhood, the limited receptive field makes it hard to capture global\ndependency. Alternatively, ViTs can effectively capture long-range dependency\nvia self-attention, while blind similarity comparisons among all the tokens\nlead to high redundancy. To resolve these problems, we propose a novel Unified\ntransFormer (UniFormer), which can seamlessly integrate the merits of\nconvolution and self-attention in a concise transformer format. Different from\nthe typical transformer blocks, the relation aggregators in our UniFormer block\nare equipped with local and global token affinity respectively in shallow and\ndeep layers, allowing to tackle both redundancy and dependency for efficient\nand effective representation learning. Finally, we flexibly stack our UniFormer\nblocks into a new powerful backbone, and adopt it for various vision tasks from\nimage to video domain, from classification to dense prediction. Without any\nextra training data, our UniFormer achieves 86.3 top-1 accuracy on ImageNet-1K\nclassification. With only ImageNet-1K pre-training, it can simply achieve\nstate-of-the-art performance in a broad range of downstream tasks, e.g., it\nobtains 82.9/84.8 top-1 accuracy on Kinetics-400/600, 60.9/71.2 top-1 accuracy\non Something-Something V1/V2 video classification tasks, 53.8 box AP and 46.4\nmask AP on COCO object detection task, 50.8 mIoU on ADE20K semantic\nsegmentation task, and 77.4 AP on COCO pose estimation task. Code is available\nat https://github.com/Sense-X/UniFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyber Mobility Mirror for Enabling Cooperative Driving Automation: A Co-Simulation Platform. (arXiv:2201.09463v1 [cs.SE])","link":"http://arxiv.org/abs/2201.09463","description":"<p>Endowed with automation and connectivity, Connected and Automated Vehicles\n(CAVs) are meant to be a revolutionary promoter for Cooperative Driving\nAutomation (CDA). Nevertheless, CAVs need high-fidelity perception information\non their surroundings, which is available but costly to collect from various\non-board sensors, such as radar, camera, and LiDAR, as well as\nvehicle-to-everything (V2X) communications. Therefore, precisely simulating the\nsensing process with high-fidelity sensor inputs and timely retrieving the\nperception information via a cost-effective platform are of increasing\nsignificance for enabling CDA-related research, e.g., development of\ndecision-making or control module. Most state-of-the-art traffic simulation\nstudies for CAVs rely on the situation-awareness information by directly\ncalling on intrinsic attributes of the objects, which impedes the reliability\nand fidelity for testing and validation of CDA algorithms. In this study, a\nco-simulation platform is developed, which can simulate both the real world\nwith a high-fidelity sensor perception system and the cyber world (or \"mirror\"\nworld) with a real-time 3D reconstruction system. Specifically, the real-world\nsimulator is mainly in charge of simulating the road-users (such as vehicles,\nbicyclists, and pedestrians), infrastructure (e.g., traffic signals and\nroadside sensors) as well as the object detection process. The mirror-world\nsimulator is responsible for reconstructing 3D objects and their trajectories\nfrom the perceived information (provided by those roadside sensors in the\nreal-world simulator) to support the development and evaluation of CDA\nalgorithms. To illustrate the efficacy of this co-simulation platform, a\nroadside LiDAR-based real-time vehicle detection and 3D reconstruction system\nis prototyped as a study case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xuewei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew J. Barth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forgery Attack Detection in Surveillance Video Streams Using Wi-Fi Channel State Information. (arXiv:2201.09487v1 [cs.CR])","link":"http://arxiv.org/abs/2201.09487","description":"<p>The cybersecurity breaches expose surveillance video streams to forgery\nattacks, under which authentic streams are falsified to hide unauthorized\nactivities. Traditional video forensics approaches can localize forgery traces\nusing spatial-temporal analysis on relatively long video clips, while falling\nshort in real-time forgery detection. The recent work correlates time-series\ncamera and wireless signals to detect looped videos but cannot realize\nfine-grained forgery localization. To overcome these limitations, we propose\nSecure-Pose, which exploits the pervasive coexistence of surveillance and Wi-Fi\ninfrastructures to defend against video forgery attacks in a real-time and\nfine-grained manner. We observe that coexisting camera and Wi-Fi signals convey\ncommon human semantic information and forgery attacks on video streams will\ndecouple such information correspondence. Particularly, retrievable human pose\nfeatures are first extracted from concurrent video and Wi-Fi channel state\ninformation (CSI) streams. Then, a lightweight detection network is developed\nto accurately discover forgery attacks and an efficient localization algorithm\nis devised to seamlessly track forgery traces in video streams. We implement\nSecure-Pose using one Logitech camera and two Intel 5300 NICs and evaluate it\nin different environments. Secure-Pose achieves a high detection accuracy of\n98.7% and localizes abnormal objects under playback and tampering attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerated Intravascular Ultrasound Imaging using Deep Reinforcement Learning. (arXiv:2201.09522v1 [eess.SP])","link":"http://arxiv.org/abs/2201.09522","description":"<p>Intravascular ultrasound (IVUS) offers a unique perspective in the treatment\nof vascular diseases by creating a sequence of ultrasound-slices acquired from\nwithin the vessel. However, unlike conventional hand-held ultrasound, the thin\ncatheter only provides room for a small number of physical channels for signal\ntransfer from a transducer-array at the tip. For continued improvement of image\nquality and frame rate, we present the use of deep reinforcement learning to\ndeal with the current physical information bottleneck. Valuable inspiration has\ncome from the field of magnetic resonance imaging (MRI), where learned\nacquisition schemes have brought significant acceleration in image acquisition\nat competing image quality. To efficiently accelerate IVUS imaging, we propose\na framework that utilizes deep reinforcement learning for an optimal adaptive\nacquisition policy on a per-frame basis enabled by actor-critic methods and\nGumbel top-$K$ sampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Stevens_T/0/1/0/all/0/1\">Tristan S.W. Stevens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chennakeshava_N/0/1/0/all/0/1\">Nishith Chennakeshava</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruijn_F/0/1/0/all/0/1\">Frederik J. de Bruijn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pekar_M/0/1/0/all/0/1\">Martin Peka&#x159;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sloun_R/0/1/0/all/0/1\">Ruud J.G. van Sloun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent 3D Hand Reconstruction in Video via self-supervised Learning. (arXiv:2201.09548v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09548","description":"<p>We present a method for reconstructing accurate and consistent 3D hands from\na monocular video. We observe that detected 2D hand keypoints and the image\ntexture provide important cues about the geometry and texture of the 3D hand,\nwhich can reduce or even eliminate the requirement on 3D hand annotation. Thus\nwe propose ${\\rm {S}^{2}HAND}$, a self-supervised 3D hand reconstruction model,\nthat can jointly estimate pose, shape, texture, and the camera viewpoint from a\nsingle RGB input through the supervision of easily accessible 2D detected\nkeypoints. We leverage the continuous hand motion information contained in the\nunlabeled video data and propose ${\\rm {S}^{2}HAND(V)}$, which uses a set of\nweights shared ${\\rm {S}^{2}HAND}$ to process each frame and exploits\nadditional motion, texture, and shape consistency constrains to promote more\naccurate hand poses and more consistent shapes and textures. Experiments on\nbenchmark datasets demonstrate that our self-supervised approach produces\ncomparable hand reconstruction performance compared with the recent\nfull-supervised methods in single-frame as input setup, and notably improves\nthe reconstruction accuracy and consistency when using video training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhisheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bisheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Average Biased ReLU Based CNN Descriptor for Improved Face Retrieval. (arXiv:1804.02051v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.02051","description":"<p>The convolutional neural networks (CNN), including AlexNet, GoogleNet,\nVGGNet, etc. extract features for many computer vision problems which are very\ndiscriminative. The trained CNN model over one dataset performs reasonably well\nwhereas on another dataset of similar type the hand-designed feature descriptor\noutperforms the same trained CNN model. The Rectified Linear Unit (ReLU) layer\ndiscards some values in order to introduce the non-linearity. In this paper, it\nis proposed that the discriminative ability of deep image representation using\ntrained model can be improved by Average Biased ReLU (AB-ReLU) at the last few\nlayers. Basically, AB-ReLU improves the discriminative ability in two ways: 1)\nit exploits some of the discriminative and discarded negative information of\nReLU and 2) it also neglects the irrelevant and positive information used in\nReLU. The VGGFace model trained in MatConvNet over the VGG-Face dataset is used\nas the feature descriptor for face retrieval over other face datasets. The\nproposed approach is tested over six challenging, unconstrained and robust face\ndatasets (PubFig, LFW, PaSC, AR, FERET and ExtYale) and also on a large scale\nface dataset (PolyUNIR) in retrieval framework. It is observed that the AB-ReLU\noutperforms the ReLU when used with a pre-trained VGGFace model over the face\ndatasets. The validation error by training the network after replacing all\nReLUs with AB-ReLUs is also observed to be favorable over each dataset. The\nAB-ReLU even outperforms the state-of-the-art activation functions, such as\nSigmoid, ReLU, Leaky ReLU and Flexible ReLU over all seven face datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Soumendu Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grassmannian Discriminant Maps (GDM) for Manifold Dimensionality Reduction with Application to Image Set Classification. (arXiv:1806.10830v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1806.10830","description":"<p>In image set classification, a considerable progress has been made by\nrepresenting original image sets on Grassmann manifolds. In order to extend the\nadvantages of the Euclidean based dimensionality reduction methods to the\nGrassmann Manifold, several methods have been suggested recently which jointly\nperform dimensionality reduction and metric learning on Grassmann manifold to\nimprove performance. Nevertheless, when applied to complex datasets, the\nlearned features do not exhibit enough discriminatory power. To overcome this\nproblem, we propose a new method named Grassmannian Discriminant Maps (GDM) for\nmanifold dimensionality reduction problems. The core of the method is a new\ndiscriminant function for metric learning and dimensionality reduction. For\ncomparison and better understanding, we also study a simple variations to GDM.\nThe key difference between them is the discriminant function. We experiment on\ndata sets corresponding to three tasks: face recognition, object\ncategorization, and hand gesture recognition to evaluate the proposed method\nand its simple extensions. Compared with the state of the art, the results\nachieved show the effectiveness of the proposed algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai-Xuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Parsing in Videos: A Multi-modal Appraoch. (arXiv:1903.02252v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1903.02252","description":"<p>Text-level discourse parsing aims to unmask how two sentences in the text are\nrelated to each other. We propose the task of Visual Discourse Parsing, which\nrequires understanding discourse relations among scenes in a video. Here we use\nthe term scene to refer to a subset of video frames that can better summarize\nthe video. In order to collect a dataset for learning discourse cues from\nvideos, one needs to manually identify the scenes from a large pool of video\nframes and then annotate the discourse relations between them. This is clearly\na time consuming, expensive and tedious task. In this work, we propose an\napproach to identify discourse cues from the videos without the need to\nexplicitly identify and annotate the scenes. We also present a novel dataset\ncontaining 310 videos and the corresponding discourse cues to evaluate our\napproach. We believe that many of the multi-discipline AI problems such as\nVisual Dialog and Visual Storytelling would greatly benefit from the use of\nvisual discourse cues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun R. Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Localization of Mixed Image Tampering Techniques. (arXiv:1904.08484v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.08484","description":"<p>With technological advances leading to an increase in mechanisms for image\ntampering, fraud detection methods must continue to be upgraded to match their\nsophistication. One problem with current methods is that they require prior\nknowledge of the method of forgery in order to determine which features to\nextract from the image to localize the region of interest. When a machine\nlearning algorithm is used to learn different types of tampering from a large\nset of various image types, with a large enough database we can easily classify\nwhich images are tampered. However, we still are left with the question of\nwhich features to train on, and how to localize the manipulation. In this work,\ndeep learning for object detection is adapted to tampering detection to solve\nthese two problems, while fusing features from multiple classic techniques for\nimproved accuracy. A Multi-stream version of the Faster RCNN network will be\nemployed with the second stream having an input of the element-wise sum of the\nELA and BAG error maps to provide even higher accuracy than a single stream\nalone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yancey_R/0/1/0/all/0/1\">Robin Elizabeth Yancey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey of Object Classification and Detection based on 2D/3D data. (arXiv:1905.12683v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1905.12683","description":"<p>Recently, by using deep neural network based algorithms, object\nclassification, detection and semantic segmentation solutions are significantly\nimproved. However, one challenge for 2D image-based systems is that they cannot\nprovide accurate 3D location information. This is critical for location\nsensitive applications such as autonomous driving and robot navigation. On the\nother hand, 3D methods, such as RGB-D and RGB-LiDAR based systems, can provide\nsolutions that significantly improve the RGB only approaches. That is why this\nis an interesting research area for both industry and academia. Compared with\n2D image-based systems, 3D-based systems are more complicated due to the\nfollowing five reasons: 1) Data representation itself is more complicated. 3D\nimages can be represented by point clouds, meshes, volumes. 2D images have\npixel grid representations. 2) The computation and memory resource requirement\nis higher as an extra dimension is added. 3) Different distribution of the\nobjects and difference in scene areas between indoor and outdoor make one\nunified framework hard to achieve. 4) 3D data, especially for the outdoor\nscenario, is sparse compared with the dense 2D images which makes the detection\ntask more challenging. Finally, large size labelled datasets, which are\nextremely important for supervised based algorithms, are still under\nconstruction compared with well-built 2D datasets such as ImageNet. Based on\nchallenges listed above, the described systems are organized by application\nscenarios, data representation methods and main tasks addressed. At the same\ntime, critical 2D based systems which greatly influence the 3D ones are also\nintroduced to show the connection between them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoke Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Parametric Activation Functions. (arXiv:2006.03179v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.03179","description":"<p>Recent studies have shown that the choice of activation function can\nsignificantly affect the performance of deep learning networks. However, the\nbenefits of novel activation functions have been inconsistent and task\ndependent, and therefore the rectified linear unit (ReLU) is still the most\ncommonly used. This paper proposes a technique for customizing activation\nfunctions automatically, resulting in reliable improvements in performance.\nEvolutionary search is used to discover the general form of the function, and\ngradient descent to optimize its parameters for different parts of the network\nand over the learning process. Experiments with four different neural network\narchitectures on the CIFAR-10 and CIFAR-100 image classification datasets show\nthat this approach is effective. It discovers both general activation functions\nand specialized functions for different architectures, consistently improving\naccuracy over ReLU and other activation functions by significant margins. The\napproach can therefore be used as an automated optimization step in applying\ndeep learning to new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bingham_G/0/1/0/all/0/1\">Garrett Bingham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1\">Risto Miikkulainen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary Neural Networks for Memory-Efficient and Effective Visual Place Recognition in Changing Environments. (arXiv:2010.00716v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.00716","description":"<p>Visual place recognition (VPR) is a robot's ability to determine whether a\nplace was visited before using visual data. While conventional hand-crafted\nmethods for VPR fail under extreme environmental appearance changes, those\nbased on convolutional neural networks (CNNs) achieve state-of-the-art\nperformance but result in heavy runtime processes and model sizes that demand a\nlarge amount of memory. Hence, CNN-based approaches are unsuitable for\nresource-constrained platforms, such as small robots and drones. In this paper,\nwe take a multi-step approach of decreasing the precision of model parameters,\ncombining it with network depth reduction and fewer neurons in the classifier\nstage to propose a new class of highly compact models that drastically reduces\nthe memory requirements and computational effort while maintaining\nstate-of-the-art VPR performance. To the best of our knowledge, this is the\nfirst attempt to propose binary neural networks for solving the visual place\nrecognition problem effectively under changing conditions and with\nsignificantly reduced resource requirements. Our best-performing binary neural\nnetwork, dubbed FloppyNet, achieves comparable VPR performance when considered\nagainst its full-precision and deeper counterparts while consuming 99% less\nmemory and increasing the inference speed seven times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrarini_B/0/1/0/all/0/1\">Bruno Ferrarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_Maier_K/0/1/0/all/0/1\">Klaus D. McDonald-Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1\">Shoaib Ehsan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Local Robust Quaternion Matrix Completion for Large-Scale Color Images and Videos Inpainting. (arXiv:2011.08675v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08675","description":"<p>The image nonlocal self-similarity (NSS) prior refers to the fact that a\nlocal patch often has many nonlocal similar patches to it across the image and\nhas been widely applied in many recently proposed machining learning algorithms\nfor image processing. However, there is no theoretical analysis on its working\nprinciple in the literature. In this paper, we discover a potential causality\nbetween NSS and low-rank property of color images, which is also available to\ngrey images. A new patch group based NSS prior learning scheme is proposed to\nlearn explicit NSS models of natural color images. The numerical low-rank\nproperty of patched matrices is also rigorously proved. The NSS-based QMC\nalgorithm computes an optimal low-rank approximation to the high-rank color\nimage, resulting in high PSNR and SSIM measures and particularly the better\nvisual quality. A new tensor NSS-based QMC method is also presented to solve\nthe color video inpainting problem based on quaternion tensor representation.\nThe numerical experiments on large-scale color images and videos indicate the\nadvantages of NSS-based QMC over the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhigang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xile Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation. (arXiv:2011.14589v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14589","description":"<p>Monocular 3D object detection, with the aim of predicting the geometric\nproperties of on-road objects, is a promising research topic for the\nintelligent perception systems of autonomous driving. Most state-of-the-art\nmethods follow a keypoint-based paradigm, where the keypoints of objects are\npredicted and employed as the basis for regressing the other geometric\nproperties. In this work, a unified network named as FADNet is presented to\naddress the task of monocular 3D object detection. In contrast to previous\nkeypoint-based methods, we propose to divide the output modalities into\ndifferent groups according to the estimation difficulty of object properties.\nDifferent groups are treated differently and sequentially associated by a\nconvolutional Gated Recurrent Unit. Another contribution of this work is the\nstrategy of depth hint augmentation. To provide characterized depth patterns as\nhints for depth estimation, a dedicated depth hint module is designed to\ngenerate row-wise features named as depth hints, which are explicitly\nsupervised in a bin-wise manner. The contributions of this work are validated\nby conducting experiments and ablation study on the KITTI benchmark. Without\nutilizing depth priors, post optimization, or other refinement modules, our\nnetwork performs competitively against state-of-the-art methods while\nmaintaining a decent running speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianze Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Huihui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Huijun Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CM-Net: Concentric Mask based Arbitrary-Shaped Text Detection. (arXiv:2011.14714v9 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14714","description":"<p>Recently fast arbitrary-shaped text detection has become an attractive\nresearch topic. However, most existing methods are non-real-time, which may\nfall short in intelligent systems. Although a few real-time text methods are\nproposed, the detection accuracy is far behind non-real-time methods. To\nimprove the detection accuracy and speed simultaneously, we propose a novel\nfast and accurate text detection framework, namely CM-Net, which is constructed\nbased on a new text representation method and a multi-perspective feature (MPF)\nmodule. The former can fit arbitrary-shaped text contours by concentric mask\n(CM) in an efficient and robust way. The latter encourages the network to learn\nmore CM-related discriminative features from multiple perspectives and brings\nno extra computational cost. Benefiting the advantages of CM and MPF, the\nproposed CM-Net only needs to predict one CM of the text instance to rebuild\nthe text contour and achieves the best balance between detection accuracy and\nspeed compared with previous works. Moreover, to ensure that multi-perspective\nfeatures are effectively learned, the multi-factor constraints loss is\nproposed. Extensive experiments demonstrate the proposed CM is efficient and\nrobust to fit arbitrary-shaped text instances, and also validate the\neffectiveness of MPF and constraints loss for discriminative text features\nrecognition. Furthermore, experimental results show that the proposed CM-Net is\nsuperior to existing state-of-the-art (SOTA) real-time text detection methods\nin both detection speed and accuracy on MSRA-TD500, CTW1500, Total-Text, and\nICDAR2015 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhitong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Iterative Neural Network for High-Quality Image-Domain Material Decomposition in Dual-Energy CT. (arXiv:2012.01986v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.01986","description":"<p>Dual-energy computed tomography (DECT) has been widely used in many\napplications that need material decomposition. Image-domain methods directly\ndecompose material images from high- and low-energy attenuation images, and\nthus, are susceptible to noise and artifacts on attenuation images. The purpose\nof this study is to develop an improved iterative neural network (INN) for\nhigh-quality image-domain material decomposition in DECT, and to study its\nproperties. We propose a new INN architecture for DECT material decomposition.\nThe proposed INN architecture uses distinct cross-material convolutional neural\nnetwork (CNN) in image refining modules, and uses image decomposition physics\nin image reconstruction modules. The distinct cross-material CNN refiners\nincorporate distinct encoding-decoding filters and cross-material model that\ncaptures correlations between different materials. We study the distinct\ncross-material CNN refiner with patch-based reformulation and tight-frame\ncondition. Numerical experiments with extended cardiactorso (XCAT) phantom and\nclinical data show that the proposed INN significantly improves the image\nquality over several image-domain material decomposition methods, including a\nconventional model-based image decomposition (MBID) method using an\nedge-preserving regularizer, a recent MBID method using pre-learned\nmaterial-wise sparsifying transforms, and a noniterative deep CNN method. Our\nstudy with patch-based reformulations reveals that learned filters of distinct\ncross-material CNN refiners can approximately satisfy the tight-frame\ncondition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhipeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1\">Yong Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chun_I/0/1/0/all/0/1\">Il Yong Chun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERANet: Facial Micro-Expression Recognition using 3D Residual Attention Network. (arXiv:2012.04581v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04581","description":"<p>Micro-expression has emerged as a promising modality in affective computing\ndue to its high objectivity in emotion detection. Despite the higher\nrecognition accuracy provided by the deep learning models, there are still\nsignificant scope for improvements in micro-expression recognition techniques.\nThe presence of micro-expressions in small-local regions of the face, as well\nas the limited size of available databases, continue to limit the accuracy in\nrecognizing micro-expressions. In this work, we propose a facial\nmicro-expression recognition model using 3D residual attention network named\nMERANet to tackle such challenges. The proposed model takes advantage of\nspatial-temporal attention and channel attention together, to learn deeper\nfine-grained subtle features for classification of emotions. Further, the\nproposed model encompasses both spatial and temporal information simultaneously\nusing the 3D kernels and residual connections. Moreover, the channel features\nand spatio-temporal features are re-calibrated using the channel and\nspatio-temporal attentions, respectively in each residual module. Our attention\nmechanism enables the model to learn to focus on different facial areas of\ninterest. The experiments are conducted on benchmark facial micro-expression\ndatasets. A superior performance is observed as compared to the\nstate-of-the-art for facial micro-expression recognition on benchmark data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gajjala_V/0/1/0/all/0/1\">Viswanatha Reddy Gajjala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Sai Prasanna Teja Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Snehasis Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-Based Human Pose Estimation: A Survey. (arXiv:2012.13392v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13392","description":"<p>Human pose estimation aims to locate the human body parts and build human\nbody representation (e.g., body skeleton) from input data such as images and\nvideos. It has drawn increasing attention during the past decade and has been\nutilized in a wide range of applications including human-computer interaction,\nmotion analysis, augmented reality, and virtual reality. Although the recently\ndeveloped deep learning-based solutions have achieved high performance in human\npose estimation, there still remain challenges due to insufficient training\ndata, depth ambiguities, and occlusion. The goal of this survey paper is to\nprovide a comprehensive review of recent deep learning-based solutions for both\n2D and 3D pose estimation via a systematic analysis and comparison of these\nsolutions based on their input data and inference procedures. More than 250\nresearch papers since 2014 are covered in this survey. Furthermore, 2D and 3D\nhuman pose estimation datasets and evaluation metrics are included.\nQuantitative performance comparisons of the reviewed methods on popular\ndatasets are summarized and discussed. Finally, the challenges involved,\napplications, and future research directions are concluded. A regularly updated\nproject page is provided: \\url{https://github.com/zczcwh/DL-HPE}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Ce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Taojiannan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Ju Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kehtarnavaz_N/0/1/0/all/0/1\">Nasser Kehtarnavaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Image Super-Resolution via Neural Differential Equation. (arXiv:2101.08987v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.08987","description":"<p>We propose a new approach for the image super-resolution (SR) task that\nprogressively restores a high-resolution (HR) image from an input\nlow-resolution (LR) image on the basis of a neural ordinary differential\nequation. In particular, we newly formulate the SR problem as an initial value\nproblem, where the initial value is the input LR image. Unlike conventional\nprogressive SR methods that perform gradual updates using straightforward\niterative mechanisms, our SR process is formulated in a concrete manner based\non explicit modeling with a much clearer understanding. Our method can be\neasily implemented using conventional neural networks for image restoration.\nMoreover, the proposed method can super-resolve an image with arbitrary scale\nfactors on continuous domain, and achieves superior SR performance over\nstate-of-the-art SR methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Seobin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_T/0/1/0/all/0/1\">Tae Hyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs. (arXiv:2102.02828v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.02828","description":"<p>Convolutional neural networks (CNNs) constructed natively on the sphere have\nbeen developed recently and shown to be highly effective for the analysis of\nspherical data. While an efficient framework has been formulated, spherical\nCNNs are nevertheless highly computationally demanding; typically they cannot\nscale beyond spherical signals of thousands of pixels. We develop scattering\nnetworks constructed natively on the sphere that provide a powerful\nrepresentational space for spherical data. Spherical scattering networks are\ncomputationally scalable and exhibit rotational equivariance, while their\nrepresentational space is invariant to isometries and provides efficient and\nstable signal representations. By integrating scattering networks as an\nadditional type of layer in the generalized spherical CNN framework, we show\nhow they can be leveraged to scale spherical CNNs to the high-resolution data\ntypical of many practical applications, with spherical signals of many tens of\nmegapixels and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McEwen_J/0/1/0/all/0/1\">Jason D. McEwen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_C/0/1/0/all/0/1\">Christopher G. R. Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavor_Parker_A/0/1/0/all/0/1\">Augustine N. Mavor-Parker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Multisensor Change Detection. (arXiv:2103.05102v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05102","description":"<p>Most change detection methods assume that pre-change and post-change images\nare acquired by the same sensor. However, in many real-life scenarios, e.g.,\nnatural disaster, it is more practical to use the latest available images\nbefore and after the occurrence of incidence, which may be acquired using\ndifferent sensors. In particular, we are interested in the combination of the\nimages acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR\nimages appear vastly different from the optical images even when capturing the\nsame scene. Adding to this, change detection methods are often constrained to\nuse only target image-pair, no labeled data, and no additional unlabeled data.\nSuch constraints limit the scope of traditional supervised machine learning and\nunsupervised generative approaches for multi-sensor change detection. Recent\nrapid development of self-supervised learning methods has shown that some of\nthem can even work with only few images. Motivated by this, in this work we\npropose a method for multi-sensor change detection using only the unlabeled\ntarget bi-temporal images that are used for training a network in\nself-supervised fashion by using deep clustering and contrastive learning. The\nproposed method is evaluated on four multi-modal bi-temporal scenes showing\nchange and the benefits of our self-supervised approach are demonstrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebel_P/0/1/0/all/0/1\">Patrick Ebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Contrastive Optimization of Siamese Networks for Place Recognition. (arXiv:2103.06638v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.06638","description":"<p>Visual place recognition is a challenging task in computer vision and a key\ncomponent of camera-based localization and navigation systems. Recently,\nConvolutional Neural Networks (CNNs) achieved high results and good\ngeneralization capabilities. They are usually trained using pairs or triplets\nof images labeled as either similar or dissimilar, in a binary fashion. In\npractice, the similarity between two images is not binary, but continuous.\nFurthermore, training these CNNs is computationally complex and involves costly\npair and triplet mining strategies.\n</p>\n<p>We propose a Generalized Contrastive loss (GCL) function that relies on image\nsimilarity as a continuous measure, and use it to train a siamese CNN.\nFurthermore, we present three techniques for automatic annotation of image\npairs with labels indicating their degree of similarity, and deploy them to\nre-annotate the MSLS, TB-Places, and 7Scenes datasets.\n</p>\n<p>We demonstrate that siamese CNNs trained using the GCL function and the\nimproved annotations consistently outperform their binary counterparts. Our\nmodels trained on MSLS outperform the state-of-the-art methods, including\nNetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the\nPittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons\ndatasets. Furthermore, training a siamese network using the GCL function does\nnot require complex pair mining. We release the source code at\nhttps://github.com/marialeyvallina/generalized_contrastive_loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leyva_Vallina_M/0/1/0/all/0/1\">Mar&#xed;a Leyva-Vallina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strisciuglio_N/0/1/0/all/0/1\">Nicola Strisciuglio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkov_N/0/1/0/all/0/1\">Nicolai Petkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration under Noise Domain Shifts. (arXiv:2104.01231v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.01231","description":"<p>Deep neural networks achieve high prediction accuracy when the train and test\ndistributions coincide. In practice though, various types of corruptions occur\nwhich deviate from this setup and cause severe performance degradations. Few\nmethods have been proposed to address generalization in the presence of\nunforeseen domain shifts. In particular, digital noise corruptions arise\ncommonly in practice during the image acquisition stage and present a\nsignificant challenge for current robustness approaches. In this paper, we\npropose a diverse Gaussian noise consistency regularization method for\nimproving robustness of image classifiers under a variety of noise corruptions\nwhile still maintaining high clean accuracy. We derive bounds to motivate our\nGaussian noise consistency regularization using a local loss landscape\nanalysis. We show that this simple approach improves robustness against various\nunforeseen noise corruptions over standard and adversarial training and other\nstrong baselines. Furthermore, when combined with diverse data augmentation\ntechniques we empirically show this type of consistency regularization further\nimproves robustness and uncertainty calibration for common corruptions upon the\nstate-of-the-art for several image classification benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_A/0/1/0/all/0/1\">Athanasios Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme-Pose Dictionary. (arXiv:2104.14631v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14631","description":"<p>With the advance of deep learning technology, automatic video generation from\naudio or text has become an emerging and promising research topic. In this\npaper, we present a novel approach to synthesize video from the text. The\nmethod builds a phoneme-pose dictionary and trains a generative adversarial\nnetwork (GAN) to generate video from interpolated phoneme poses. Compared to\naudio-driven video generation algorithms, our approach has a number of\nadvantages: 1) It only needs a fraction of the training data used by an\naudio-driven approach; 2) It is more flexible and not subject to vulnerability\ndue to speaker variation; 3) It significantly reduces the preprocessing,\ntraining and inference time. We perform extensive experiments to compare the\nproposed method with state-of-the-art talking face generation methods on a\nbenchmark dataset and datasets of our own. The results demonstrate the\neffectiveness and superiority of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiahong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Miao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Agent Semi-Siamese Training for Long-tail and Shallow Face Learning. (arXiv:2105.04113v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04113","description":"<p>With the recent development of deep convolutional neural networks and\nlarge-scale datasets, deep face recognition has made remarkable progress and\nbeen widely used in various applications. However, unlike the existing public\nface datasets, in many real-world scenarios of face recognition, the depth of\ntraining dataset is shallow, which means only two face images are available for\neach ID. With the non-uniform increase of samples, such issue is converted to a\nmore general case, a.k.a long-tail face learning, which suffers from data\nimbalance and intra-class diversity dearth simultaneously. These adverse\nconditions damage the training and result in the decline of model performance.\nBased on the Semi-Siamese Training (SST), we introduce an advanced solution,\nnamed Multi-Agent Semi-Siamese Training (MASST), to address these problems.\nMASST includes a probe network and multiple gallery agents, the former aims to\nencode the probe features, and the latter constitutes a stack of networks that\nencode the prototypes (gallery features). For each training iteration, the\ngallery network, which is sequentially rotated from the stack, and the probe\nnetwork form a pair of semi-siamese networks. We give the theoretical and\nempirical analysis that, given the long-tail (or shallow) data and training\nloss, MASST smooths the loss landscape and satisfies the Lipschitz continuity\nwith the help of multiple agents and the updating gallery queue. The proposed\nmethod is out of extra-dependency, thus can be easily integrated with the\nexisting loss functions and network architectures. It is worth noting that,\nalthough multiple gallery agents are employed for training, only the probe\nnetwork is needed for inference, without increasing the inference cost.\nExtensive experiments and comparisons demonstrate the advantages of MASST for\nlong-tail and shallow face learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yichun Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yibo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forensic Analysis of Video Files Using Metadata. (arXiv:2105.06361v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2105.06361","description":"<p>The unprecedented ease and ability to manipulate video content has led to a\nrapid spread of manipulated media. The availability of video editing tools\ngreatly increased in recent years, allowing one to easily generate\nphoto-realistic alterations. Such manipulations can leave traces in the\nmetadata embedded in video files. This metadata information can be used to\ndetermine video manipulations, brand of video recording device, the type of\nvideo editing tool, and other important evidence. In this paper, we focus on\nthe metadata contained in the popular MP4 video wrapper/container. We describe\nour method for metadata extractor that uses the MP4's tree structure. Our\napproach for analyzing the video metadata produces a more compact\nrepresentation. We will describe how we construct features from the metadata\nand then use dimensionality reduction and nearest neighbor classification for\nforensic analysis of a video file. Our approach allows one to visually inspect\nthe distribution of metadata features and make decisions. The experimental\nresults confirm that the performance of our approach surpasses other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Ziyue Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_J/0/1/0/all/0/1\">J&#xe1;nos Horv&#xe1;th</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baireddy_S/0/1/0/all/0/1\">Sriram Baireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images. (arXiv:2105.10239v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.10239","description":"<p>Covid-19 global pandemic continues to devastate health care systems across\nthe world. At present, the Covid-19 testing is costly and time-consuming. Chest\nX-Ray (CXR) testing can be a fast, scalable, and non-invasive method. The\nexisting methods suffer due to the limited CXR samples available from Covid-19.\nThus, inspired by the limitations of the open-source work in this field, we\npropose attention guided contrastive CNN architecture (AC-CovidNet) for\nCovid-19 detection in CXR images. The proposed method learns the robust and\ndiscriminative features with the help of contrastive loss. Moreover, the\nproposed method gives more importance to the infected regions as guided by the\nattention mechanism. We compute the sensitivity of the proposed method over the\npublicly available Covid-19 dataset. It is observed that the proposed\nAC-CovidNet exhibits very promising performance as compared to the existing\nmethods even with limited training data. It can tackle the bottleneck of CXR\nCovid-19 datasets being faced by the researchers. The code used in this paper\nis released publicly at \\url{https://github.com/shivram1987/AC-CovidNet/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ambati_A/0/1/0/all/0/1\">Anirudh Ambati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data. (arXiv:2105.10837v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10837","description":"<p>The ultimate goal for an inference model is to be robust and functional in\nreal life applications. However, training vs. test data domain gaps often\nnegatively affect model performance. This issue is especially critical for the\nmonocular 3D human pose estimation problem, in which 3D human data is often\ncollected in a controlled lab setting. In this paper, we focus on alleviating\nthe negative effect of domain shift in both appearance and pose space for 3D\nhuman pose estimation by presenting our adapted human pose (AHuP) approach.\nAHuP is built upon two key components: (1) semantically aware adaptation (SAA)\nfor the cross-domain feature space adaptation, and (2) skeletal pose adaptation\n(SPA) for the pose space adaptation which takes only limited information from\nthe target domain. By using zero real 3D human pose data, one of our adapted\nsynthetic models shows comparable performance with the SOTA pose estimation\nmodels trained with large scale real 3D human datasets. The proposed SPA can be\nalso employed independently as a light-weighted head to improve existing SOTA\nmodels in a novel context. A new 3D scan-based synthetic human dataset called\nScanAva+ is also going to be publicly released with this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sehgal_N/0/1/0/all/0/1\">Naveen Sehgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Contextual Deep Network Based Multimodal Pedestrian Detection For Autonomous Driving. (arXiv:2105.12713v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12713","description":"<p>Pedestrian Detection is the most critical module of an Autonomous Driving\nsystem. Although a camera is commonly used for this purpose, its quality\ndegrades severely in low-light night time driving scenarios. On the other hand,\nthe quality of a thermal camera image remains unaffected in similar conditions.\nThis paper proposes an end-to-end multimodal fusion model for pedestrian\ndetection using RGB and thermal images. Its novel spatio-contextual deep\nnetwork architecture is capable of exploiting the multimodal input efficiently.\nIt consists of two distinct deformable ResNeXt-50 encoders for feature\nextraction from the two modalities. Fusion of these two encoded features takes\nplace inside a multimodal feature embedding module (MuFEm) consisting of\nseveral groups of a pair of Graph Attention Network and a feature fusion unit.\nThe output of the last feature fusion unit of MuFEm is subsequently passed to\ntwo CRFs for their spatial refinement. Further enhancement of the features is\nachieved by applying channel-wise attention and extraction of contextual\ninformation with the help of four RNNs traversing in four different directions.\nFinally, these feature maps are used by a single-stage decoder to generate the\nbounding box of each pedestrian and the score map. We have performed extensive\nexperiments of the proposed framework on three publicly available multimodal\npedestrian detection benchmark datasets, namely KAIST, CVC-14, and UTokyo. The\nresults on each of them improved the respective state-of-the-art performance. A\nshort video giving an overview of this work along with its qualitative results\ncan be seen at https://youtu.be/FDJdSifuuCs. Our source code will be released\nupon publication of the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_K/0/1/0/all/0/1\">Kinjal Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Arindam Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sudip Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Ujjwal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Graph Reasoning for Neural Network Regularization. (arXiv:2106.01805v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01805","description":"<p>Regularizers help deep neural networks prevent feature co-adaptations.\nDropout, as a commonly used regularization technique, stochastically disables\nneuron activations during network optimization. However, such complete feature\ndisposal can affect the feature representation and network understanding.\nToward better descriptions of latent representations, we present DropGraph that\nlearns a regularization function by constructing a stand-alone graph from the\nbackbone features. DropGraph first samples stochastic spatial feature vectors\nand then incorporates graph reasoning methods to generate feature map\ndistortions. This add-on graph regularizes the network during training and can\nbe completely skipped during inference. We provide intuitions on the linkage\nbetween graph reasoning and Dropout with further discussions on how partial\ngraph reasoning method reduces feature correlations. To this end, we\nextensively study the modeling of graph vertex dependencies and the utilization\nof the graph for distorting backbone feature maps. DropGraph was validated on 4\ntasks with a total of 8 different datasets. The experimental results show that\nour method outperforms other state-of-the-art regularizers while leaving the\nbase model structure unmodified during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongliang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion-aware Unsupervised Learning of Depth from 4-D Light Fields. (arXiv:2106.03043v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03043","description":"<p>Depth estimation is a fundamental issue in 4-D light field processing and\nanalysis. Although recent supervised learning-based light field depth\nestimation methods have significantly improved the accuracy and efficiency of\ntraditional optimization-based ones, these methods rely on the training over\nlight field data with ground-truth depth maps which are challenging to obtain\nor even unavailable for real-world light field data. Besides, due to the\ninevitable gap (or domain difference) between real-world and synthetic data,\nthey may suffer from serious performance degradation when generalizing the\nmodels trained with synthetic data to real-world data. By contrast, we propose\nan unsupervised learning-based method, which does not require ground-truth\ndepth as supervision during training. Specifically, based on the basic\nknowledge of the unique geometry structure of light field data, we present an\nocclusion-aware strategy to improve the accuracy on occlusion areas, in which\nwe explore the angular coherence among subsets of the light field views to\nestimate initial depth maps, and utilize a constrained unsupervised loss to\nlearn their corresponding reliability for final depth prediction. Additionally,\nwe adopt a multi-scale network with a weighted smoothness loss to handle the\ntextureless areas. Experimental results on synthetic data show that our method\ncan significantly shrink the performance gap between the previous unsupervised\nmethod and supervised ones, and produce depth maps with comparable accuracy to\ntraditional methods with obviously reduced computational cost. Moreover,\nexperiments on real-world datasets show that our method can avoid the domain\nshift problem presented in supervised methods, demonstrating the great\npotential of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13948","description":"<p>Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Margin-Aware Intra-Class Novelty Identification for Medical Images. (arXiv:2108.00117v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00117","description":"<p>Traditional anomaly detection methods focus on detecting inter-class\nvariations while medical image novelty identification is inherently an\nintra-class detection problem. For example, a machine learning model trained\nwith normal chest X-ray and common lung abnormalities, is expected to discover\nand flag idiopathic pulmonary fibrosis which a rare lung disease and unseen by\nthe model during training. The nuances from intra-class variations and lack of\nrelevant training data in medical image analysis pose great challenges for\nexisting anomaly detection methods. To tackle the challenges, we propose a\nhybrid model - Transformation-based Embedding learning for Novelty Detection\n(TEND) which without any out-of-distribution training data, performs novelty\nidentification by combining both autoencoder-based and classifier-based method.\nWith a pre-trained autoencoder as image feature extractor, TEND learns to\ndiscriminate the feature embeddings of in-distribution data from the\ntransformed counterparts as fake out-of-distribution inputs. To enhance the\nseparation, a distance objective is optimized to enforce a margin between the\ntwo classes. Extensive experimental results on both natural image datasets and\nmedical image datasets are presented and our method out-performs\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport for Unsupervised Denoising Learning. (arXiv:2108.02574v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02574","description":"<p>Recently, much progress has been made in unsupervised denoising learning.\nHowever, existing methods more or less rely on some assumptions on the signal\nand/or degradation model, which limits their practical performance. How to\nconstruct an optimal criterion for unsupervised denoising learning without any\nprior knowledge on the degradation model is still an open question. Toward\nanswering this question, this work proposes a criterion for unsupervised\ndenoising learning based on the optimal transport theory. This criterion has\nfavorable properties, e.g., approximately maximal preservation of the\ninformation of the signal, whilst achieving perceptual reconstruction.\nFurthermore, though a relaxed unconstrained formulation is used in practical\nimplementation, we prove that the relaxed formulation in theory has the same\nsolution as the original constrained formulation. Experiments on synthetic and\nreal-world data, including realistic photographic, microscopy, depth, and raw\ndepth images, demonstrate that the proposed method even compares favorably with\nsupervised methods, e.g., approaching the PSNR of supervised methods while\nhaving better perceptual quality. Particularly, for spatially correlated noise\nand realistic microscopy images, the proposed method not only achieves better\nperceptual quality but also has higher PSNR than supervised methods. Besides,\nit shows remarkable superiority in harsh practical conditions with complex\nnoise, e.g., raw depth images. Code is available at\nhttps://github.com/wangweiSJTU/OTUR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zeyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Field Image Super-Resolution with Transformers. (arXiv:2108.07597v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07597","description":"<p>Light field (LF) image super-resolution (SR) aims at reconstructing\nhigh-resolution LF images from their low-resolution counterparts. Although\nCNN-based methods have achieved remarkable performance in LF image SR, these\nmethods cannot fully model the non-local properties of the 4D LF data. In this\npaper, we propose a simple but effective Transformer-based method for LF image\nSR. In our method, an angular Transformer is designed to incorporate\ncomplementary information among different views, and a spatial Transformer is\ndeveloped to capture both local and long-range dependencies within each\nsub-aperture image. With the proposed angular and spatial Transformers, the\nbeneficial information in an LF can be fully exploited and the SR performance\nis boosted. We validate the effectiveness of our angular and spatial\nTransformers through extensive ablation studies, and compare our method to\nrecent state-of-the-art methods on five public LF datasets. Our method achieves\nsuperior SR performance with a small model size and low computational cost.\nCode is available at https://github.com/ZhengyuLiang24/LFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARCH++: Animation-Ready Clothed Human Reconstruction Revisited. (arXiv:2108.07845v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07845","description":"<p>We present ARCH++, an image-based method to reconstruct 3D avatars with\narbitrary clothing styles. Our reconstructed avatars are animation-ready and\nhighly realistic, in both the visible regions from input views and the unseen\nregions. While prior work shows great promise of reconstructing animatable\nclothed humans with various topologies, we observe that there exist fundamental\nlimitations resulting in sub-optimal reconstruction quality. In this paper, we\nrevisit the major steps of image-based avatar reconstruction and address the\nlimitations with ARCH++. First, we introduce an end-to-end point based geometry\nencoder to better describe the semantics of the underlying 3D human body, in\nreplacement of previous hand-crafted features. Second, in order to address the\noccupancy ambiguity caused by topological changes of clothed humans in the\ncanonical pose, we propose a co-supervising framework with cross-space\nconsistency to jointly estimate the occupancy in both the posed and canonical\nspaces. Last, we use image-to-image translation networks to further refine\ndetailed geometry and texture on the reconstructed surface, which improves the\nfidelity and consistency across arbitrary viewpoints. In the experiments, we\ndemonstrate improvements over the state of the art on both public benchmarks\nand user studies in reconstruction quality and realism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11845","description":"<p>This letter is concerned with image classification with deep convolutional\nneural networks (CNNs). The focus is on the following question: given a set of\ncandidate CNN models, how to select the right one with the best generalization\nproperty for the current task? Present model selection methods require access\nto a batch of labeled data for computing a pre-specified performance metric,\nsuch as the cross-entropy loss, the classification error rate, the negative\nlog-likelihood. In many practical cases, labels are not available in time as\nlabeling itself is a time-consuming and expensive task. To this end, this\nletter presents an approach to CNN model selection using only unlabeled data.\nThis method is developed based on a principle termed consistent relative\nconfidence. The effectiveness and efficiency of the proposed method are\ndemonstrated by experiments using benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Tensor Completion via Element-wise Weighted Low-rank Tensor Train with Overlapping Ket Augmentation. (arXiv:2109.05736v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05736","description":"<p>In recent years, there have been an increasing number of applications of\ntensor completion based on the tensor train (TT) format because of its\nefficiency and effectiveness in dealing with higher-order tensor data. However,\nexisting tensor completion methods using TT decomposition have two obvious\ndrawbacks. One is that they only consider mode weights according to the degree\nof mode balance, even though some elements are recovered better in an\nunbalanced mode. The other is that serious blocking artifacts appear when the\nmissing element rate is relatively large. To remedy such two issues, in this\nwork, we propose a novel tensor completion approach via the element-wise\nweighted technique. Accordingly, a novel formulation for tensor completion and\nan effective optimization algorithm, called as tensor completion by parallel\nweighted matrix factorization via tensor train (TWMac-TT), is proposed. In\naddition, we specifically consider the recovery quality of edge elements from\nadjacent blocks. Different from traditional reshaping and ket augmentation, we\nutilize a new tensor augmentation technique called overlapping ket\naugmentation, which can further avoid blocking artifacts. We then conduct\nextensive performance evaluations on synthetic data and several real image data\nsets. Our experimental results demonstrate that the proposed algorithm TWMac-TT\noutperforms several other competing tensor completion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi&#x27;ai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yandong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigation-Oriented Scene Understanding for Robotic Autonomy: Learning to Segment Driveability in Egocentric Images. (arXiv:2109.07245v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.07245","description":"<p>This work tackles scene understanding for outdoor robotic navigation, solely\nrelying on images captured by an on-board camera. Conventional visual scene\nunderstanding interprets the environment based on specific descriptive\ncategories. However, such a representation is not directly interpretable for\ndecision-making and constrains robot operation to a specific domain. Thus, we\npropose to segment egocentric images directly in terms of how a robot can\nnavigate in them, and tailor the learning problem to an autonomous navigation\ntask. Building around an image segmentation network, we present a generic\naffordance consisting of 3 driveability levels which can broadly apply to both\nurban and off-road scenes. By encoding these levels with soft ordinal labels,\nwe incorporate inter-class distances during learning which improves\nsegmentation compared to standard \"hard\" one-hot labelling. In addition, we\npropose a navigation-oriented pixel-wise loss weighting method which assigns\nhigher importance to safety-critical areas. We evaluate our approach on\nlarge-scale public image segmentation datasets ranging from sunny city streets\nto snowy forest trails. In a cross-dataset generalization experiment, we show\nthat our affordance learning scheme can be applied across a diverse mix of\ndatasets and improves driveability estimation in unseen environments compared\nto general-purpose, single-dataset segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humblot_Renaux_G/0/1/0/all/0/1\">Galadrielle Humblot-Renaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchegiani_L/0/1/0/all/0/1\">Letizia Marchegiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gade_R/0/1/0/all/0/1\">Rikke Gade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Optimal Control Framework for Joint-channel Parallel MRI Reconstruction without Coil Sensitivities. (arXiv:2109.09738v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.09738","description":"<p>Goal: This work aims at developing a novel calibration-free fast parallel MRI\n(pMRI) reconstruction method incorporate with discrete-time optimal control\nframework. The reconstruction model is designed to learn a regularization that\ncombines channels and extracts features by leveraging the information sharing\namong channels of multi-coil images. We propose to recover both magnitude and\nphase information by taking advantage of structured convolutional networks in\nimage and Fourier spaces. Methods: We develop a novel variational model with a\nlearnable objective function that integrates an adaptive multi-coil image\ncombination operator and effective image regularization in the image and\nFourier spaces. We cast the reconstruction network as a structured\ndiscrete-time optimal control system, resulting in an optimal control\nformulation of parameter training where the parameters of the objective\nfunction play the role of control variables. We demonstrate that the Lagrangian\nmethod for solving the control problem is equivalent to back-propagation,\nensuring the local convergence of the training algorithm. Results: We conduct a\nlarge number of numerical experiments of the proposed method with comparisons\nto several state-of-the-art pMRI reconstruction networks on real pMRI datasets.\nThe numerical results demonstrate the promising performance of the proposed\nmethod evidently. Conclusion: We conduct a large number of numerical\nexperiments of the proposed method with comparisons to several state-of-the-art\npMRI reconstruction networks on real pMRI datasets. The numerical results\ndemonstrate the promising performance of the proposed method evidently.\nSignificance: By learning multi-coil image combination operator and performing\nregularizations in both image domain and k-space domain, the proposed method\nachieves a highly efficient image reconstruction network for pMRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bian_W/0/1/0/all/0/1\">Wanyu Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojing Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing. (arXiv:2109.10737v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10737","description":"<p>The semantic controllability of StyleGAN is enhanced by unremitting research.\nAlthough the existing weak supervision methods work well in manipulating the\nstyle codes along one attribute, the accuracy of manipulating multiple\nattributes is neglected. Sequential editing leads to error accumulation and\nattribute entanglement. To address these limitations, we design a Dynamic Style\nManipulation Network (DyStyle) whose structure and parameters vary by input\nsamples, to perform nonlinear and adaptive manipulation of latent codes for\nflexible and precise attribute control. In order to efficient and stable\noptimization of the DyStyle network, we propose a Dynamic Multi-Attribute\nContrastive Learning (DmaCL) method: including dynamic multi-attribute\ncontrastor and dynamic multi-attribute contrastive loss, which simultaneously\ndisentangle a variety of attributes from the generative image and latent space\nof model. As a result, our approach demonstrates fine-grained disentangled\nedits along multiple numeric and binary attributes. Qualitative and\nquantitative comparisons with existing style manipulation methods verify the\nsuperiority of our method in terms of the multi-attribute control accuracy and\nidentity preservation without compromising photorealism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingchuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1\">Miao Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zili Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong 3D Object Recognition and Grasp Synthesis Using Dual Memory Recurrent Self-Organization Networks. (arXiv:2109.11544v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.11544","description":"<p>Humans learn to recognize and manipulate new objects in lifelong settings\nwithout forgetting the previously gained knowledge under non-stationary and\nsequential conditions. In autonomous systems, the agents also need to mitigate\nsimilar behavior to continually learn the new object categories and adapt to\nnew environments. In most conventional deep neural networks, this is not\npossible due to the problem of catastrophic forgetting, where the newly gained\nknowledge overwrites existing representations. Furthermore, most\nstate-of-the-art models excel either in recognizing the objects or in grasp\nprediction, while both tasks use visual input. The combined architecture to\ntackle both tasks is very limited. In this paper, we proposed a hybrid model\narchitecture consists of a dynamically growing dual-memory recurrent neural\nnetwork (GDM) and an autoencoder to tackle object recognition and grasping\nsimultaneously. The autoencoder network is responsible to extract a compact\nrepresentation for a given object, which serves as input for the GDM learning,\nand is responsible to predict pixel-wise antipodal grasp configurations. The\nGDM part is designed to recognize the object in both instances and categories\nlevels. We address the problem of catastrophic forgetting using the intrinsic\nmemory replay, where the episodic memory periodically replays the neural\nactivation trajectories in the absence of external sensory information. To\nextensively evaluate the proposed model in a lifelong setting, we generate a\nsynthetic dataset due to lack of sequential 3D objects dataset. Experiment\nresults demonstrated that the proposed model can learn both object\nrepresentation and grasping simultaneously in continual learning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santhakumar_K/0/1/0/all/0/1\">Krishnakumar Santhakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1\">Hamidreza Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Affinity with Maximum Bipartite Matching in Few-Shot Learning. (arXiv:2110.02399v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.02399","description":"<p>We propose an asymmetric affinity score for representing the complexity of\nutilizing the knowledge of one task for learning another one. Our method is\nbased on the maximum bipartite matching algorithm and utilizes the Fisher\nInformation matrix. We provide theoretical analyses demonstrating that the\nproposed score is mathematically well-defined, and subsequently use the\naffinity score to propose a novel algorithm for the few-shot learning problem.\nIn particular, using this score, we find relevant training data labels to the\ntest data and leverage the discovered relevant data for episodically\nfine-tuning a few-shot model. Results on various few-shot benchmark datasets\ndemonstrate the efficacy of the proposed approach by improving the\nclassification accuracy over the state-of-the-art methods even when using\nsmaller models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Cat P. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Juncheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera Calibration through Camera Projection Loss. (arXiv:2110.03479v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03479","description":"<p>Camera calibration is a necessity in various tasks including 3D\nreconstruction, hand-eye coordination for a robotic interaction, autonomous\ndriving, etc. In this work we propose a novel method to predict extrinsic\n(baseline, pitch, and translation), intrinsic (focal length and principal point\noffset) parameters using an image pair. Unlike existing methods, instead of\ndesigning an end-to-end solution, we proposed a new representation that\nincorporates camera model equations as a neural network in multi-task learning\nframework. We estimate the desired parameters via novel camera projection loss\n(CPL) that uses the camera model neural network to reconstruct the 3D points\nand uses the reconstruction loss to estimate the camera parameters. To the best\nof our knowledge, ours is the first method to jointly estimate both the\nintrinsic and extrinsic parameters via a multi-task learning methodology that\ncombines analytical equations in learning framework for the estimation of\ncamera parameters. We also proposed a novel dataset using CARLA Simulator.\nEmpirically, we demonstrate that our proposed approach achieves better\nperformance with respect to both deep learning-based and traditional methods on\n8 out of 10 parameters evaluated using both synthetic and real data. Our code\nand generated dataset are available at\nhttps://github.com/thanif/Camera-Calibration-through-Camera-Projection-Loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butt_T/0/1/0/all/0/1\">Talha Hanif Butt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1\">Murtaza Taj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Unlearning of Backdoors via Implicit Hypergradient. (arXiv:2110.03735v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03735","description":"<p>We propose a minimax formulation for removing backdoors from a given poisoned\nmodel based on a small set of clean data. This formulation encompasses much of\nprior work on backdoor removal. We propose the Implicit Bacdoor Adversarial\nUnlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which\nbreaks down the minimax into separate inner and outer problems, our algorithm\nutilizes the implicit hypergradient to account for the interdependence between\ninner and outer optimization. We theoretically analyze its convergence and the\ngeneralizability of the robustness gained by solving minimax on clean data to\nunseen test data. In our evaluation, we compare I-BAU with six state-of-art\nbackdoor defenses on seven backdoor attacks over two datasets and various\nattack settings, including the common setting where the attacker targets one\nclass as well as important but underexplored settings where multiple classes\nare targeted. I-BAU's performance is comparable to and most often significantly\nbetter than the best baseline. Particularly, its performance is more robust to\nthe variation on triggers, attack settings, poison ratio, and clean data size.\nMoreover, I-BAU requires less computation to take effect; particularly, it is\nmore than $13\\times$ faster than the most efficient baseline in the\nsingle-target attack setting. Furthermore, it can remain effective in the\nextreme case where the defender can only access 100 clean samples -- a setting\nwhere all the baselines fail to produce acceptable results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Won Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Z. Morley Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03825","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to adversarial\nattacks. A range of defense methods have been proposed to train adversarially\nrobust DNNs, among which adversarial training has demonstrated promising\nresults. However, despite preliminary understandings developed for adversarial\ntraining, it is still not clear, from the architectural perspective, what\nconfigurations can lead to more robust DNNs. In this paper, we address this gap\nvia a comprehensive investigation on the impact of network width and depth on\nthe robustness of adversarially trained DNNs. Specifically, we make the\nfollowing key observations: 1) more parameters (higher model capacity) does not\nnecessarily help adversarial robustness; 2) reducing capacity at the last stage\n(the last group of blocks) of the network can actually improve adversarial\nrobustness; and 3) under the same parameter budget, there exists an optimal\narchitectural configuration for adversarial robustness. We also provide a\ntheoretical analysis explaning why such network configuration can help\nrobustness. These architectural insights can help design adversarially robust\nDNNs. Code is available at \\url{https://github.com/HanxunH/RobustWRN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanxun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Monazam Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Is Graph: Structured Graph Module for Video Action Recognition. (arXiv:2110.05904v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05904","description":"<p>In the field of action recognition, video clips are always treated as ordered\nframes for subsequent processing. To achieve spatio-temporal perception,\nexisting approaches propose to embed adjacent temporal interaction in the\nconvolutional layer. The global semantic information can therefore be obtained\nby stacking multiple local layers hierarchically. However, such global temporal\naccumulation can only reflect the high-level semantics in deep layers,\nneglecting the potential low-level holistic clues in shallow layers. In this\npaper, we first propose to transform a video sequence into a graph to obtain\ndirect long-term dependencies among temporal frames. To preserve sequential\ninformation during transformation, we devise a structured graph module (SGM),\nachieving fine-grained temporal interactions throughout the entire network. In\nparticular, SGM divides the neighbors of each node into several temporal\nregions so as to extract global structural information with diverse sequential\nflows. Extensive experiments are performed on standard benchmark datasets,\ni.e., Something-Something V1 &amp; V2, Diving48, Kinetics-400, UCF101, and HMDB51.\nThe reported performance and analysis demonstrate that SGM can achieve\noutstanding precision with less computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEMO: Test Time Robustness via Adaptation and Augmentation. (arXiv:2110.09506v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09506","description":"<p>While deep neural networks can attain good accuracy on in-distribution test\npoints, many applications require robustness even in the face of unexpected\nperturbations in the input, changes in the domain, or other sources of\ndistribution shift. We study the problem of test time robustification, i.e.,\nusing the test input to improve model robustness. Recent prior works have\nproposed methods for test time adaptation, however, they each introduce\nadditional assumptions, such as access to multiple test points, that prevent\nwidespread adoption. In this work, we aim to study and devise methods that make\nno assumptions about the model training process and are broadly applicable at\ntest time. We propose a simple approach that can be used in any test setting\nwhere the model is probabilistic and adaptable: when presented with a test\nexample, perform different data augmentations on the data point, and then adapt\n(all of) the model parameters by minimizing the entropy of the model's average,\nor marginal, output distribution across the augmentations. Intuitively, this\nobjective encourages the model to make the same prediction across different\naugmentations, thus enforcing the invariances encoded in these augmentations,\nwhile also maintaining confidence in its predictions. In our experiments, we\nevaluate two baseline ResNet models, two robust ResNet-50 models, and a robust\nvision transformer model, and we demonstrate that this approach achieves\naccuracy gains of 1-8\\% over standard model evaluation and also generally\noutperforms prior augmentation and adaptation strategies. For the setting in\nwhich only one test point is available, we achieve state-of-the-art results on\nthe ImageNet-C, ImageNet-R, and, among ResNet-50 models, ImageNet-A\ndistribution shift benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Marvin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERQA: Edge-Restoration Quality Assessment for Video Super-Resolution. (arXiv:2110.09992v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09992","description":"<p>Despite the growing popularity of video super-resolution (VSR), there is\nstill no good way to assess the quality of the restored details in upscaled\nframes. Some SR methods may produce the wrong digit or an entirely different\nface. Whether a method's results are trustworthy depends on how well it\nrestores truthful details. Image super-resolution can use natural distributions\nto produce a high-resolution image that is only somewhat similar to the real\none. VSR enables exploration of additional information in neighboring frames to\nrestore details from the original scene. The ERQA metric, which we propose in\nthis paper, aims to estimate a model's ability to restore real details using\nVSR. On the assumption that edges are significant for detail and character\nrecognition, we chose edge fidelity as the foundation for this metric.\nExperimental validation of our work is based on the MSU Video Super-Resolution\nBenchmark, which includes the most difficult patterns for detail restoration\nand verifies the fidelity of details from the original frame. Code for the\nproposed metric is publicly available at\nhttps://github.com/msu-video-group/ERQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kirillova_A/0/1/0/all/0/1\">Anastasia Kirillova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyapustin_E/0/1/0/all/0/1\">Eugene Lyapustin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitry Vatolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alpha-IoU: A Family of Power Intersection over Union Losses for Bounding Box Regression. (arXiv:2110.13675v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13675","description":"<p>Bounding box (bbox) regression is a fundamental task in computer vision. So\nfar, the most commonly used loss functions for bbox regression are the\nIntersection over Union (IoU) loss and its variants. In this paper, we\ngeneralize existing IoU-based losses to a new family of power IoU losses that\nhave a power IoU term and an additional power regularization term with a single\npower parameter $\\alpha$. We call this new family of losses the $\\alpha$-IoU\nlosses and analyze properties such as order preservingness and loss/gradient\nreweighting. Experiments on multiple object detection benchmarks and models\ndemonstrate that $\\alpha$-IoU losses, 1) can surpass existing IoU-based losses\nby a noticeable performance margin; 2) offer detectors more flexibility in\nachieving different levels of bbox regression accuracy by modulating $\\alpha$;\nand 3) are more robust to small datasets and noisy bboxes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Ying Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiMoSeg: Real-time Bird's Eye View based LiDAR Motion Segmentation. (arXiv:2111.04875v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.04875","description":"<p>Moving object detection and segmentation is an essential task in the\nAutonomous Driving pipeline. Detecting and isolating static and moving\ncomponents of a vehicle's surroundings are particularly crucial in path\nplanning and localization tasks. This paper proposes a novel real-time\narchitecture for motion segmentation of Light Detection and Ranging (LiDAR)\ndata. We use three successive scans of LiDAR data in 2D Bird's Eye View (BEV)\nrepresentation to perform pixel-wise classification as static or moving.\nFurthermore, we propose a novel data augmentation technique to reduce the\nsignificant class imbalance between static and moving objects. We achieve this\nby artificially synthesizing moving objects by cutting and pasting static\nvehicles. We demonstrate a low latency of 8 ms on a commonly used automotive\nembedded platform, namely Nvidia Jetson Xavier. To the best of our knowledge,\nthis is the first work directly performing motion segmentation in LiDAR BEV\nspace. We provide quantitative results on the challenging SemanticKITTI\ndataset, and qualitative results are provided in https://youtu.be/2aJ-cL8b0LI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Sambit Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodaei_M/0/1/0/all/0/1\">Mona Hodaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1\">Stefan Milz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotzig_H/0/1/0/all/0/1\">Heinrich Gotzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_M/0/1/0/all/0/1\">Martin Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maeder_P/0/1/0/all/0/1\">Patrick Maeder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Myocardial Infarction Detection over Multi-view Echocardiography. (arXiv:2111.05790v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.05790","description":"<p>Myocardial infarction (MI) is the leading cause of mortality in the world\nthat occurs due to a blockage of the coronary arteries feeding the myocardium.\nAn early diagnosis of MI and its localization can mitigate the extent of\nmyocardial damage by facilitating early therapeutic interventions. Following\nthe blockage of a coronary artery, the regional wall motion abnormality (RWMA)\nof the ischemic myocardial segments is the earliest change to set in.\nEchocardiography is the fundamental tool to assess any RWMA. Assessing the\nmotion of the left ventricle (LV) wall only from a single echocardiography view\nmay lead to missing the diagnosis of MI as the RWMA may not be visible on that\nspecific view. Therefore, in this study, we propose to fuse apical 4-chamber\n(A4C) and apical 2-chamber (A2C) views in which a total of 12 myocardial\nsegments can be analyzed for MI detection. The proposed method first estimates\nthe motion of the LV wall by Active Polynomials (APs), which extract and track\nthe endocardial boundary to compute myocardial segment displacements. The\nfeatures are extracted from the A4C and A2C view displacements, which are\nconcatenated and fed into the classifiers to detect MI. The main contributions\nof this study are 1) creation of a new benchmark dataset by including both A4C\nand A2C views in a total of 260 echocardiography recordings, which is publicly\nshared with the research community, 2) improving the performance of the prior\nwork of threshold-based APs by a Machine Learning based approach, and 3) a\npioneer MI detection approach via multi-view echocardiography by fusing the\ninformation of A4C and A2C views. Experimental results show that the proposed\nmethod achieves 90.91% sensitivity and 86.36% precision for MI detection over\nmulti-view echocardiography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Degerli_A/0/1/0/all/0/1\">Aysen Degerli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamid_T/0/1/0/all/0/1\">Tahir Hamid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazhar_R/0/1/0/all/0/1\">Rashid Mazhar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pansharpening by convolutional neural networks in the full resolution framework. (arXiv:2111.08334v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08334","description":"<p>In recent years, there has been a growing interest in deep learning-based\npansharpening. Thus far, research has mainly focused on architectures.\nNonetheless, model training is an equally important issue. A first problem is\nthe absence of ground truths, unavoidable in pansharpening. This is often\naddressed by training networks in a reduced resolution domain and using the\noriginal data as ground truth, relying on an implicit scale invariance\nassumption. However, on full resolution images results are often disappointing,\nsuggesting such invariance not to hold. A further problem is the scarcity of\ntraining data, which causes a limited generalization ability and a poor\nperformance on off-training test images. In this paper, we propose a\nfull-resolution training framework for deep learning-based pansharpening. The\nframework is fully general and can be used for any deep learning-based\npansharpening model. Training takes place in the high-resolution domain,\nrelying only on the original data, thus avoiding any loss of information. To\nensure spectral and spatial fidelity, a suitable two-component loss is defined.\nThe spectral component enforces consistency between the pansharpened output and\nthe low-resolution multispectral input. The spatial component, computed at\nhigh-resolution, maximizes the local correlation between each pansharpened band\nand the panchromatic input. At testing time, the target-adaptive operating\nmodality is adopted, achieving good generalization with a limited computational\noverhead. Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1\nimages show that methods trained with the proposed framework guarantee a pretty\ngood performance in terms of both full-resolution numerical indexes and visual\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciotola_M/0/1/0/all/0/1\">Matteo Ciotola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitale_S/0/1/0/all/0/1\">Sergio Vitale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazza_A/0/1/0/all/0/1\">Antonio Mazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_G/0/1/0/all/0/1\">Giovanni Poggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1\">Giuseppe Scarpa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransMorph: Transformer for unsupervised medical image registration. (arXiv:2111.10480v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.10480","description":"<p>In the last decade, convolutional neural networks (ConvNets) have been a\nmajor focus of research in medical image analysis. However, the performances of\nConvNets may be limited by a lack of explicit consideration of the long-range\nspatial relationships in an image. Recently Vision Transformer architectures\nhave been proposed to address the shortcomings of ConvNets and have produced\nstate-of-the-art performances in many medical imaging applications.\nTransformers may be a strong candidate for image registration because their\nunlimited receptive field enables a more precise comprehension of the spatial\ncorrespondence between moving and fixed images. Here, we present TransMorph, a\nhybrid Transformer-ConvNet model for volumetric medical image registration.\nThis paper also presents diffeomorphic and Bayesian variants of TransMorph: the\ndiffeomorphic variants ensure the topology-preserving deformations, and the\nBayesian variant produces a well-calibrated registration uncertainty estimate.\nWe extensively validated the proposed models using 3D medical images from three\napplications: inter-patient and atlas-to-patient brain MRI registration and\nphantom-to-CT registration. The proposed models are evaluated in comparison to\na variety of existing registration methods and Transformer architectures.\nQualitative and quantitative results demonstrate that the proposed\nTransformer-based model leads to a substantial performance improvement over the\nbaseline methods, confirming the effectiveness of Transformers for medical\nimage registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Junyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frey_E/0/1/0/all/0/1\">Eric C. Frey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yufan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Segars_W/0/1/0/all/0/1\">William P. Segars</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Ye Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Y/0/1/0/all/0/1\">Yong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Bed Human Pose Estimation from Unseen and Privacy-Preserving Image Domains. (arXiv:2111.15124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15124","description":"<p>Medical applications have benefited greatly from the rapid advancement in\ncomputer vision. Considering patient monitoring in particular, in-bed human\nposture estimation offers important health-related metrics with potential value\nin medical condition assessments. Despite great progress in this domain, it\nremains challenging due to substantial ambiguity during occlusions, and the\nlack of large corpora of manually labeled data for model training, particularly\nwith domains such as thermal infrared imaging which are privacy-preserving, and\nthus of great interest. Motivated by the effectiveness of self-supervised\nmethods in learning features directly from data, we propose a multi-modal\nconditional variational autoencoder (MC-VAE) capable of reconstructing features\nfrom missing modalities seen during training. This approach is used with HRNet\nto enable single modality inference for in-bed pose estimation. Through\nextensive evaluations, we demonstrate that body positions can be effectively\nrecognized from the available modality, achieving on par results with baseline\nmodels that are highly dependent on having access to multiple modes at\ninference time. The proposed framework supports future research towards\nself-supervised learning that generates a robust model from a single source,\nand expects it to generalize over many unknown distributions in clinical\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Ting Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features. (arXiv:2111.15263v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15263","description":"<p>Linguistic knowledge has brought great benefits to scene text recognition by\nproviding semantics to refine character sequences. However, since linguistic\nknowledge has been applied individually on the output sequence, previous\nmethods have not fully utilized the semantics to understand visual clues for\ntext recognition. This paper introduces a novel method, called Multi-modAl Text\nRecognition Network (MATRN), that enables interactions between visual and\nsemantic features for better recognition performances. Specifically, MATRN\nidentifies visual and semantic feature pairs and encodes spatial information\ninto semantic features. Based on the spatial encoding, visual and semantic\nfeatures are enhanced by referring to related features in the other modality.\nFurthermore, MATRN stimulates combining semantic features into visual features\nby hiding visual clues related to the character in the training phase. Our\nexperiments demonstrate that MATRN achieves state-of-the-art performances on\nseven benchmarks with large margins, while naive combinations of two modalities\nshow marginal improvements. Further ablative studies prove the effectiveness of\nour proposed components. Our implementation is publicly available at\nhttps://github.com/wp03052/MATRN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1\">Byeonghu Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Deep Learning for Low-Shot Object Detection. (arXiv:2112.02814v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02814","description":"<p>Object detection has achieved a huge breakthrough with deep neural networks\nand massive annotated data. However, current detection methods cannot be\ndirectly transferred to the scenario where the annotated data is scarce due to\nthe severe overfitting problem. Although few-shot learning and zero-shot\nlearning have been extensively explored in the field of image classification,\nit is indispensable to design new methods for object detection in the\ndata-scarce scenario since object detection has an additional challenging\nlocalization task. Low-Shot Object Detection (LSOD) is an emerging research\ntopic of detecting objects from a few or even no annotated samples, consisting\nof One-Shot Object Detection (OSOD), Few-Shot Object Detection (FSOD) and\nZero-Shot Object Detection (ZSD). This survey provides a comprehensive review\nof LSOD methods. First, we propose a thorough taxonomy of LSOD methods and\nanalyze them systematically, comprising some extensional topics of LSOD\n(semi-supervised LSOD, weakly-supervised LSOD and incremental LSOD). Then, we\nindicate the pros and cons of current LSOD methods with a comparison of their\nperformance. Finally, we discuss the challenges and promising directions of\nLSOD to provide guidance for future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qihan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mengqi Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symmetry Perception by Deep Networks: Inadequacy of Feed-Forward Architectures and Improvements with Recurrent Connections. (arXiv:2112.04162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04162","description":"<p>Symmetry is omnipresent in nature and perceived by the visual system of many\nspecies, as it facilitates detecting ecologically important classes of objects\nin our environment. Symmetry perception requires abstraction of long-range\nspatial dependencies between image regions, and its underlying neural\nmechanisms remain elusive. In this paper, we evaluate Deep Neural Network (DNN)\narchitectures on the task of learning symmetry perception from examples. We\ndemonstrate that feed-forward DNNs that excel at modelling human performance on\nobject recognition tasks, are unable to acquire a general notion of symmetry.\nThis is the case even when the DNNs are architected to capture long-range\nspatial dependencies, such as through `dilated' convolutions and the recently\nintroduced `transformers' design. By contrast, we find that recurrent\narchitectures are capable of learning to perceive symmetry by decomposing the\nlong-range spatial dependencies into a sequence of local operations, that are\nreusable for novel images. These results suggest that recurrent connections\nlikely play an important role in symmetry perception in artificial systems, and\npossibly, biological ones too.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Shobhita Sundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_D/0/1/0/all/0/1\">Darius Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groth_M/0/1/0/all/0/1\">Matthew Groth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Robot Collaborative Perception with Graph Neural Networks. (arXiv:2201.01760v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2201.01760","description":"<p>Multi-robot systems such as swarms of aerial robots are naturally suited to\noffer additional flexibility, resilience, and robustness in several tasks\ncompared to a single robot by enabling cooperation among the agents. To enhance\nthe autonomous robot decision-making process and situational awareness,\nmulti-robot systems have to coordinate their perception capabilities to\ncollect, share, and fuse environment information among the agents in an\nefficient and meaningful way such to accurately obtain context-appropriate\ninformation or gain resilience to sensor noise or failures. In this paper, we\npropose a general-purpose Graph Neural Network (GNN) with the main goal to\nincrease, in multi-robot perception tasks, single robots' inference perception\naccuracy as well as resilience to sensor failures and disturbances. We show\nthat the proposed framework can address multi-view visual perception problems\nsuch as monocular depth estimation and semantic segmentation. Several\nexperiments both using photo-realistic and real data gathered from multiple\naerial robots' viewpoints show the effectiveness of the proposed approach in\nchallenging inference conditions including images corrupted by heavy noise and\ncamera occlusions or failures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiuhong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1\">Giuseppe Loianno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning. (arXiv:2201.04676v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04676","description":"<p>It is a challenging task to learn rich and multi-scale spatiotemporal\nsemantics from high-dimensional videos, due to large local redundancy and\ncomplex global dependency between video frames. The recent advances in this\nresearch have been mainly driven by 3D convolutional neural networks and vision\ntransformers. Although 3D convolution can efficiently aggregate local context\nto suppress local redundancy from a small 3D neighborhood, it lacks the\ncapability to capture global dependency because of the limited receptive field.\nAlternatively, vision transformers can effectively capture long-range\ndependency by self-attention mechanism, while having the limitation on reducing\nlocal redundancy with blind similarity comparison among all the tokens in each\nlayer. Based on these observations, we propose a novel Unified transFormer\n(UniFormer) which seamlessly integrates merits of 3D convolution and\nspatiotemporal self-attention in a concise transformer format, and achieves a\npreferable balance between computation and accuracy. Different from traditional\ntransformers, our relation aggregator can tackle both spatiotemporal redundancy\nand dependency, by learning local and global token affinity respectively in\nshallow and deep layers. We conduct extensive experiments on the popular video\nbenchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&amp;V2.\nWith only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1\naccuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than\nother state-of-the-art methods. For Something-Something V1 and V2, our\nUniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1\naccuracy respectively. Code is available at\nhttps://github.com/Sense-X/UniFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking. (arXiv:2201.07425v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07425","description":"<p>In this work, we contribute a new million-scale Unmanned Aerial Vehicle (UAV)\ntracking benchmark, called WebUAV-3M. Firstly, we collect 4,485 videos with\nmore than 3M frames from the Internet. Then, an efficient and scalable\nSemi-Automatic Target Annotation (SATA) pipeline is devised to label the\ntremendous WebUAV-3M in every frame. To the best of our knowledge, the densely\nbounding box annotated WebUAV-3M is by far the largest public UAV tracking\nbenchmark. We expect to pave the way for the follow-up study in the UAV\ntracking by establishing a million-scale annotated benchmark covering a wide\nrange of target categories. Moreover, considering the close connections among\nvisual appearance, natural language and audio, we enrich WebUAV-3M by providing\nnatural language specification and audio description, encouraging the\nexploration of natural language features and audio cues for UAV tracking.\nEquipped with this benchmark, we delve into million-scale deep UAV tracking\nproblems, aiming to provide the community with a dedicated large-scale\nbenchmark for training deep UAV trackers and evaluating UAV tracking\napproaches. Extensive experiments on WebUAV-3M demonstrate that there is still\na big room for robust deep UAV tracking improvements. The dataset, toolkits and\nbaseline results will be available at\n\\url{https://github.<a href=\"/abs/com/9836328\">com/9836328</a>47/WebUAV-3M}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guanjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-by-Novel-View-Synthesis for Full-Face Appearance-based 3D Gaze Estimation. (arXiv:2201.07927v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07927","description":"<p>Despite recent advances in appearance-based gaze estimation techniques, the\nneed for training data that covers the target head pose and gaze distribution\nremains a crucial challenge for practical deployment. This work examines a\nnovel approach for synthesizing gaze estimation training data based on\nmonocular 3D face reconstruction. Unlike prior works using multi-view\nreconstruction, photo-realistic CG models, or generative neural networks, our\napproach can manipulate and extend the head pose range of existing training\ndata without any additional requirements. We introduce a projective matching\nprocedure to align the reconstructed 3D facial mesh to the camera coordinate\nsystem and synthesize face images with accurate gaze labels. We also propose a\nmask-guided gaze estimation model and data augmentation strategies to further\nimprove the estimation accuracy by taking advantage of the synthetic training\ndata. Experiments using multiple public datasets show that our approach can\nsignificantly improve the estimation performance on challenging cross-dataset\nsettings with non-overlapping gaze distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiawei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimoyama_T/0/1/0/all/0/1\">Takuru Shimoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1\">Yusuke Sugano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDhuman: High-quality Human Performance Capture with Sparse Views. (arXiv:2201.08158v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08158","description":"<p>In this paper, we introduce HDhuman, a method that addresses the challenge of\nnovel view rendering of human performers that wear clothes with complex texture\npatterns using a sparse set of camera views. Although some recent works have\nachieved remarkable rendering quality on humans with relatively uniform\ntextures using sparse views, the rendering quality remains limited when dealing\nwith complex texture patterns as they are unable to recover the high-frequency\ngeometry details that observed in the input views. To this end, the proposed\nHDhuman uses a human reconstruction network with a pixel-aligned spatial\ntransformer and a rendering network that uses geometry-guided pixel-wise\nfeature integration to achieve high-quality human reconstruction and rendering.\nThe designed pixel-aligned spatial transformer calculates the correlations\nbetween the input views, producing human reconstruction results with\nhigh-frequency details. Based on the surface reconstruction results, the\ngeometry-guided pixel-wise visibility reasoning provides guidance for\nmulti-view feature integration, enabling the rendering network to render\nhigh-quality images at 2k resolution on novel views. Unlike previous neural\nrendering works that always need to train or fine-tune an independent network\nfor a different scene, our method is a general framework that is able to\ngeneralize to novel subjects. Experiments show that our approach outperforms\nall the prior generic or specific methods on both synthetic data and real-world\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tiansong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruizhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization. (arXiv:2201.08613v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08613","description":"<p>Localizing keypoints of an object is a basic visual problem. However,\nsupervised learning of a keypoint localization network often requires a large\namount of data, which is expensive and time-consuming to obtain. To remedy\nthis, there is an ever-growing interest in semi-supervised learning (SSL),\nwhich leverages a small set of labeled data along with a large set of unlabeled\ndata. Among these SSL approaches, pseudo-labeling (PL) is one of the most\npopular. PL approaches apply pseudo-labels to unlabeled data, and then train\nthe model with a combination of the labeled and pseudo-labeled data\niteratively. The key to the success of PL is the selection of high-quality\npseudo-labeled samples. Previous works mostly select training samples by\nmanually setting a single confidence threshold. We propose to automatically\nselect reliable pseudo-labeled samples with a series of dynamic thresholds,\nwhich constitutes a learning curriculum. Extensive experiments on six keypoint\nlocalization benchmark datasets demonstrate that the proposed approach\nsignificantly outperforms the previous state-of-the-art SSL approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yingda Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}