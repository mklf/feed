{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Don't \"research fast and break things\": On the ethics of Computational Social Science. (arXiv:2206.06370v1 [cs.CY])","link":"http://arxiv.org/abs/2206.06370","description":"<p>This article is concerned with setting up practical guardrails within the\nresearch activities and environments of CSS. It aims to provide CSS scholars,\nas well as policymakers and other stakeholders who apply CSS methods, with the\ncritical and constructive means needed to ensure that their practices are\nethical, trustworthy, and responsible. It begins by providing a taxonomy of the\nethical challenges faced by researchers in the field of CSS. These are\nchallenges related to (1) the treatment of research subjects, (2) the impacts\nof CSS research on affected individuals and communities, (3) the quality of CSS\nresearch and to its epistemological status, (4) research integrity, and (5)\nresearch equity. Taking these challenges as a motivation for cultural\ntransformation, it then argues for the end-to-end incorporation of habits of\nresponsible research and innovation (RRI) into CSS practices, focusing on the\nrole that contextual considerations, anticipatory reflection, impact\nassessment, public engagement, and justifiable and well-documented action\nshould play across the research lifecycle. In proposing the inclusion of habits\nof RRI in CSS practices, the chapter lays out several practical steps needed\nfor ethical, trustworthy, and responsible CSS research activities. These\ninclude stakeholder engagement processes, research impact assessments, data\nlifecycle documentation, bias self-assessments, and transparent research\nreporting protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leslie_D/0/1/0/all/0/1\">David Leslie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Post-Editing Effectiveness in Text Summarization. (arXiv:2206.06383v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06383","description":"<p>Automatic summarization methods are efficient but can suffer from low\nquality. In comparison, manual summarization is expensive but produces higher\nquality. Can humans and AI collaborate to improve summarization performance? In\nsimilar text generation tasks (e.g., machine translation), human-AI\ncollaboration in the form of \"post-editing\" AI-generated text reduces human\nworkload and improves the quality of AI output. Therefore, we explored whether\npost-editing offers advantages in text summarization. Specifically, we\nconducted an experiment with 72 participants, comparing post-editing provided\nsummaries with manual summarization for summary quality, human efficiency, and\nuser experience on formal (XSum news) and informal (Reddit posts) text. This\nstudy sheds valuable insights on when post-editing is useful for text\nsummarization: it helped in some cases (e.g., when participants lacked domain\nknowledge) but not in others (e.g., when provided summaries include inaccurate\ninformation). Participants' different editing strategies and needs for\nassistance offer implications for future human-AI summarization systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Vivian Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_Renner_A/0/1/0/all/0/1\">Alison Smith-Renner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ruijia Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alejandro Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Speech and Counter Speech Detection: Conversational Context Does Matter. (arXiv:2206.06423v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06423","description":"<p>Hate speech is plaguing the cyberspace along with user-generated content.\nThis paper investigates the role of conversational context in the annotation\nand detection of online hate and counter speech, where context is defined as\nthe preceding comment in a conversation thread. We created a context-aware\ndataset for a 3-way classification task on Reddit comments: hate speech,\ncounter speech, or neutral. Our analyses indicate that context is critical to\nidentify hate and counter speech: human judgments change for most comments\ndepending on whether we show annotators the context. A linguistic analysis\ndraws insights into the language people use to express hate and counter speech.\nExperimental results show that neural networks obtain significantly better\nresults if context is taken into account. We also present qualitative error\nanalyses shedding light into (a) when and why context is beneficial and (b) the\nremaining errors made by our best model when context is taken into account.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_E/0/1/0/all/0/1\">Eduardo Blanco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lingzi Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Based Model Editing at Scale. (arXiv:2206.06520v1 [cs.AI])","link":"http://arxiv.org/abs/2206.06520","description":"<p>Even the largest neural networks make errors, and once-correct predictions\ncan become invalid as the world changes. Model editors make local updates to\nthe behavior of base (pre-trained) models to inject updated knowledge or\ncorrect undesirable behaviors. Existing model editors have shown promise, but\nalso suffer from insufficient expressiveness: they struggle to accurately model\nan edit's intended scope (examples affected by the edit), leading to inaccurate\npredictions for test inputs loosely related to the edit, and they often fail\naltogether after many edits. As a higher-capacity alternative, we propose\nSemi-Parametric Editing with a Retrieval-Augmented Counterfactual Model\n(SERAC), which stores edits in an explicit memory and learns to reason over\nthem to modulate the base model's predictions as needed. To enable more\nrigorous evaluation of model editors, we introduce three challenging language\nmodel editing problems based on question answering, fact-checking, and dialogue\ngeneration. We find that only SERAC achieves high performance on all three\nproblems, consistently outperforming existing approaches to model editing by a\nsignificant margin. Code, data, and additional project information will be made\navailable at https://sites.google.com/view/serac-editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Charles Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning. (arXiv:2206.06522v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06522","description":"<p>Fine-tuning large pre-trained models on downstream tasks has been adopted in\na variety of domains recently. However, it is costly to update the entire\nparameter set of large pre-trained models. Although recently proposed\nparameter-efficient transfer learning (PETL) techniques allow updating a small\nsubset of parameters (e.g. only using 2% of parameters) inside a pre-trained\nbackbone network for a new task, they only reduce the training memory\nrequirement by up to 30%. This is because the gradient computation for the\ntrainable parameters still requires backpropagation through the large\npre-trained backbone model. To address this, we propose Ladder Side-Tuning\n(LST), a new PETL technique that reduces training memory requirements by more\nsubstantial amounts. Unlike existing parameter-efficient methods that insert\nadditional parameters inside backbone networks, we train a ladder side network,\na small and separate network that takes intermediate activations as input via\nshortcut connections (ladders) from backbone networks and makes predictions.\nLST has significantly lower memory requirements than previous methods, because\nit does not require backpropagation through the backbone network, but instead\nonly through the side network and ladder connections. We evaluate our method\nwith various models (T5, CLIP-T5) on both NLP (GLUE) and vision-language (VQA,\nGQA, NLVR2, MSCOCO) tasks. LST saves 69% of the memory costs to fine-tune the\nwhole network, while other methods only save 26% of that in similar parameter\nusages (hence, 2.7x more memory savings). Moreover, LST achieves higher\naccuracy than Adapter and LoRA in a low-memory regime. To further show the\nadvantage of this better memory efficiency, we also apply LST to larger T5\nmodels (T5-large, T5-3B), attaining better GLUE performance than full\nfine-tuning and other PETL methods. The exact same trend also holds in our\nexperiments on VL tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks. (arXiv:2206.06565v1 [cs.LG])","link":"http://arxiv.org/abs/2206.06565","description":"<p>Fine-tuning pretrained language models (LMs) without making any architectural\nchanges has become a norm for learning various language downstream tasks.\nHowever, for non-language downstream tasks, a common practice is to employ\ntask-specific designs for input, output layers, and loss functions. For\ninstance, it is possible to fine-tune an LM into an MNIST classifier by\nreplacing the word embedding layer with an image patch embedding layer, the\nword token output layer with a 10-way output layer, and the word prediction\nloss with a 10-way classification loss, respectively. A natural question\narises: can LM fine-tuning solve non-language downstream tasks without changing\nthe model architecture or loss function? To answer this, we propose\nLanguage-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations\nby conducting an extensive empirical study on a suite of non-language\nclassification and regression tasks. LIFT does not make any changes to the\nmodel architecture or loss function, and it solely relies on the natural\nlanguage interface, enabling \"no-code machine learning with LMs.\" We find that\nLIFT performs relatively well across a wide range of low-dimensional\nclassification and regression tasks, matching the performances of the best\nbaselines in many cases, especially for the classification tasks. We report the\nexperimental results on the fundamental properties of LIFT, including its\ninductive bias, sample efficiency, ability to extrapolate, robustness to\noutliers and label noise, and generalization. We also analyze a few\nproperties/techniques specific to LIFT, e.g., context-aware learning via\nappropriate prompting, quantification of predictive uncertainty, and two-stage\nfine-tuning. Our code is available at\nhttps://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruisu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1\">Shashank Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gira_M/0/1/0/all/0/1\">Michael Gira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization. (arXiv:2206.06581v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06581","description":"<p>The quest for seeking health information has swamped the web with consumers'\nhealth-related questions. Generally, consumers use overly descriptive and\nperipheral information to express their medical condition or other healthcare\nneeds, contributing to the challenges of natural language understanding. One\nway to address this challenge is to summarize the questions and distill the key\ninformation of the original question. To address this issue, we introduce a new\ndataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health\nquestions and corresponding summaries. The dataset is derived from the\ncommunity question-answering forum and therefore provides a valuable resource\nfor understanding consumer health-related posts on social media. We benchmark\nthe dataset on multiple state-of-the-art summarization models to show the\neffectiveness of the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Shweta Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demner_Fushman_D/0/1/0/all/0/1\">Dina Demner-Fushman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreeTransfer-X: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models. (arXiv:2206.06586v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06586","description":"<p>Cross-lingual transfer (CLT) is of various applications. However, labeled\ncross-lingual corpus is expensive or even inaccessible, especially in the\nfields where labels are private, such as diagnostic results of symptoms in\nmedicine and user profiles in business. Nevertheless, there are off-the-shelf\nmodels in these sensitive fields. Instead of pursuing the original labels, a\nworkaround for CLT is to transfer knowledge from the off-the-shelf models\nwithout labels. To this end, we define a novel CLT problem named FreeTransfer-X\nthat aims to achieve knowledge transfer from the off-the-shelf models in\nrich-resource languages. To address the problem, we propose a 2-step knowledge\ndistillation (KD, Hinton et al., 2015) framework based on multilingual\npre-trained language models (mPLM). The significant improvement over strong\nneural machine translation (NMT) baselines demonstrates the effectiveness of\nthe proposed method. In addition to reducing annotation cost and protecting\nprivate labels, the proposed method is compatible with different networks and\neasy to be deployed. Finally, a range of analyses indicate the great potential\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yinpeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Astock: A New Dataset and Automated Stock Trading based on Stock-specific News Analyzing Model. (arXiv:2206.06606v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06606","description":"<p>Natural Language Processing(NLP) demonstrates a great potential to support\nfinancial decision-making by analyzing the text from social media or news\noutlets. In this work, we build a platform to study the NLP-aided stock\nauto-trading algorithms systematically. In contrast to the previous work, our\nplatform is characterized by three features: (1) We provide financial news for\neach specific stock. (2) We provide various stock factors for each stock. (3)\nWe evaluate performance from more financial-relevant metrics. Such a design\nallows us to develop and evaluate NLP-aided stock auto-trading algorithms in a\nmore realistic setting. In addition to designing an evaluation platform and\ndataset collection, we also made a technical contribution by proposing a system\nto automatically learn a good feature representation from various input\ninformation. The key to our algorithm is a method called semantic role labeling\nPooling (SRLP), which leverages Semantic Role Labeling (SRL) to create a\ncompact representation of each news paragraph. Based on SRLP, we further\nincorporate other stock factors to make the final prediction. In addition, we\npropose a self-supervised learning strategy based on SRLP to enhance the\nout-of-distribution generalization performance of our system. Through our\nexperimental study, we show that the proposed method achieves better\nperformance and outperforms all the baselines' annualized rate of return as\nwell as the maximum drawdown of the CSI300 index and XIN9 index on real\ntrading. Our Astock dataset and code are available at\nhttps://github.com/JinanZou/Astock.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jinan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haiyao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuhao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1\">Ehsan Abbasnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Javen Qinfeng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Transfer and Domain Adaptation for Zero-Shot Question Answering. (arXiv:2206.06705v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06705","description":"<p>Pretrained language models have shown success in various areas of natural\nlanguage processing, including reading comprehension tasks. However, when\napplying machine learning methods to new domains, labeled data may not always\nbe available. To address this, we use supervised pretraining on source-domain\ndata to reduce sample complexity on domain-specific downstream tasks. We\nevaluate zero-shot performance on domain-specific reading comprehension tasks\nby combining task transfer with domain adaptation to fine-tune a pretrained\nmodel with no labelled data from the target task. Our approach outperforms\nDomain-Adaptive Pretraining on downstream domain-specific reading comprehension\ntasks in 3 out of 4 domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_A/0/1/0/all/0/1\">Alex Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimshoni_D/0/1/0/all/0/1\">David Shimshoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_A/0/1/0/all/0/1\">Aditya Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenthal_S/0/1/0/all/0/1\">Sara Rosenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06807","description":"<p>Ambiguity is a natural language phenomenon occurring at different levels of\nsyntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,\nfor instance, we have a variety of competing studies for the human\ndisambiguation processes. These studies are empirical and based on eyetracking\nmeasurements. Here we take first steps towards formalizing these processes for\nsemantic ambiguities where we identified the presence of two features: (1)\njoint plausibility degrees of different possible interpretations, (2) causal\nstructures according to which certain words play a more substantial role in the\nprocesses. The novel sheaf-theoretic model of definite causality developed by\nGogioso and Pinzani in QPL 2021 offers tools to model and reason about these\nfeatures. We applied this theory to a dataset of ambiguous phrases extracted\nfrom Psycholinguistics literature and their human plausibility judgements\ncollected by us using the Amazon Mechanical Turk engine. We measured the causal\nfractions of different disambiguation orders within the phrases and discovered\ntwo prominent orders: from subject to verb in the subject-verb and from object\nto verb in the verb object phrases. We also found evidence for delay in the\ndisambiguation of polysemous vs homonymous verbs, again compatible with\nPsycholinguistic findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daphne Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"hasSignification()\": une nouvelle fonction de distance pour soutenir la d\\'etection de donn\\'ees personnelles. (arXiv:2206.06836v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06836","description":"<p>Today with Big Data and data lakes, we are faced of a mass of data that is\nvery difficult to manage it manually. The protection of personal data in this\ncontext requires an automatic analysis for data discovery. Storing the names of\nattributes already analyzed in a knowledge base could optimize this automatic\ndiscovery. To have a better knowledge base, we should not store any attributes\nwhose name does not make sense. In this article, to check if the name of an\nattribute has a meaning, we propose a solution that calculate the distances\nbetween this name and the words in a dictionary. Our studies on the distance\nfunctions like N-Gram, Jaro-Winkler and Levenshtein show limits to set an\nacceptance threshold for an attribute in the knowledge base. In order to\novercome these limitations, our solution aims to strengthen the score\ncalculation by using an exponential function based on the longest sequence. In\naddition, a double scan in dictionary is also proposed in order to process the\nattributes which have a compound name.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mrabet_A/0/1/0/all/0/1\">Amine Mrabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1\">Ali Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darmon_P/0/1/0/all/0/1\">Patrice Darmon</a> (Umanis)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Sentence Generation from API Specifications. (arXiv:2206.06868v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06868","description":"<p>APIs are everywhere; they provide access to automation solutions that could\nhelp businesses automate some of their tasks. Unfortunately, they may not be\naccessible to the business users who need them but are not equipped with the\nnecessary technical skills to leverage them. Wrapping these APIs with chatbot\ncapabilities is one solution to make these automation solutions interactive. In\nthis work, we propose a system to generate sentences to train intent\nrecognition models, a crucial component within chatbots to understand natural\nlanguage utterances from users. Evaluation of our approach based on deep\nlearning models showed promising and inspiring results, and the\nhuman-in-the-loop interaction will provide further improvement on the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_S/0/1/0/all/0/1\">Siyu Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_K/0/1/0/all/0/1\">Kushal Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandlamudi_J/0/1/0/all/0/1\">Jayachandu Bandlamudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isahagian_V/0/1/0/all/0/1\">Vatche Isahagian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthusamy_V/0/1/0/all/0/1\">Vinod Muthusamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizk_Y/0/1/0/all/0/1\">Yara Rizk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation. (arXiv:2206.06888v1 [cs.SE])","link":"http://arxiv.org/abs/2206.06888","description":"<p>Code generation is a longstanding challenge, aiming to generate a code\nsnippet based on a natural language description. Usually, expensive text-code\npaired data is essential for training a code generation model. Recently, thanks\nto the success of pre-training techniques, large language models are trained on\nlarge-scale unlabelled code corpora and perform well in code generation. In\nthis paper, we investigate how to leverage an unlabelled code corpus to train a\nmodel for library-oriented code generation. Since it is a common practice for\nprogrammers to reuse third-party libraries, in which case the text-code paired\ndata are harder to obtain due to the huge number of libraries. We observe that\nlibrary-oriented code snippets are more likely to share similar code sketches.\nHence, we present CERT with two steps: a sketcher generates the sketch, then a\ngenerator fills the details in the sketch. Both the sketcher and the generator\nare continually pre-trained upon a base model using unlabelled data.\nFurthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate\nlibrary-oriented code generation. Experimental results demonstrate the\nimpressive performance of CERT. For example, it surpasses the base model by an\nabsolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is\navailable at https://github.com/microsoft/PyCodeGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1\">Daoguang Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dejian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_B/0/1/0/all/0/1\">Bei Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RDU: A Region-based Approach to Form-style Document Understanding. (arXiv:2206.06890v1 [cs.AI])","link":"http://arxiv.org/abs/2206.06890","description":"<p>Key Information Extraction (KIE) is aimed at extracting structured\ninformation (e.g. key-value pairs) from form-style documents (e.g. invoices),\nwhich makes an important step towards intelligent document understanding.\nPrevious approaches generally tackle KIE by sequence tagging, which faces\ndifficulty to process non-flatten sequences, especially for table-text mixed\ndocuments. These approaches also suffer from the trouble of pre-defining a\nfixed set of labels for each type of documents, as well as the label imbalance\nissue. In this work, we assume Optical Character Recognition (OCR) has been\napplied to input documents, and reformulate the KIE task as a region prediction\nproblem in the two-dimensional (2D) space given a target field. Following this\nnew setup, we develop a new KIE model named Region-based Document Understanding\n(RDU) that takes as input the text content and corresponding coordinates of a\ndocument, and tries to predict the result by localizing a bounding-box-like\nregion. Our RDU first applies a layout-aware BERT equipped with a soft layout\nattention masking and bias mechanism to incorporate layout information into the\nrepresentations. Then, a list of candidate regions is generated from the\nrepresentations via a Region Proposal Module inspired by computer vision models\nwidely applied for object detection. Finally, a Region Categorization Module\nand a Region Selection Module are adopted to judge whether a proposed region is\nvalid and select the one with the largest probability from all proposed regions\nrespectively. Experiments on four types of form-style documents show that our\nproposed method can achieve impressive results. In addition, our RDU model can\nbe trained with different document types seamlessly, which is especially\nhelpful over low-resource documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengbin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Maximum Linear Arrangement for trees under projectivity and planarity. (arXiv:2206.06924v1 [cs.DS])","link":"http://arxiv.org/abs/2206.06924","description":"<p>The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping\n$\\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers\nthat maximizes $D_{\\pi}(G)=\\sum_{uv\\in E(G)}|\\pi(u) - \\pi(v)|$. In this\nsetting, vertices are considered to lie on a horizontal line and edges are\ndrawn as semicircles above the line. There exist variants of MaxLA in which the\narrangements are constrained. In the planar variant edge crossings are\nforbidden. In the projective variant for rooted trees arrangements are planar\nand the root cannot be covered by any edge. Here we present $O(n)$-time and\n$O(n)$-space algorithms that solve Planar and Projective MaxLA for trees. We\nalso prove several properties of maximum projective and planar arrangements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehending and Ordering Semantics for Image Captioning. (arXiv:2206.06930v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06930","description":"<p>Comprehending the rich semantics in an image and ordering them in linguistic\norder are essential to compose a visually-grounded and linguistically coherent\ndescription for image captioning. Modern techniques commonly capitalize on a\npre-trained object detector/classifier to mine the semantics in an image, while\nleaving the inherent linguistic ordering of semantics under-exploited. In this\npaper, we propose a new recipe of Transformer-style structure, namely\nComprehending and Ordering Semantics Networks (COS-Net), that novelly unifies\nan enriched semantic comprehending and a learnable semantic ordering processes\ninto a single architecture. Technically, we initially utilize a cross-modal\nretrieval model to search the relevant sentences of each image, and all words\nin the searched sentences are taken as primary semantic cues. Next, a novel\nsemantic comprehender is devised to filter out the irrelevant semantic words in\nprimary semantic cues, and meanwhile infer the missing relevant semantic words\nvisually grounded in the image. After that, we feed all the screened and\nenriched semantic words into a semantic ranker, which learns to allocate all\nsemantic words in linguistic order as humans. Such sequence of ordered semantic\nwords are further integrated with visual tokens of images to trigger sentence\ngeneration. Empirical evidences show that COS-Net clearly surpasses the\nstate-of-the-art approaches on COCO and achieves to-date the best CIDEr score\nof 141.1% on Karpathy test split. Source code is available at\n\\url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSN Dashboard Tool For Sentiment Analysis. (arXiv:2206.06935v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06935","description":"<p>The amount of opinionated data on the internet is rapidly increasing. More\nand more people are sharing their ideas and opinions in reviews, discussion\nforums, microblogs and general social media. As opinions are central in all\nhuman activities, sentiment analysis has been applied to gain insights in this\ntype of data. There are proposed several approaches for sentiment\nclassification. The major drawback is the lack of standardized solutions for\nclassification and high-level visualization. In this study, a sentiment\nanalyzer dashboard for online social networking analysis is proposed. This, to\nenable people gaining insights in topics interesting to them. The tool allows\nusers to run the desired sentiment analysis algorithm in the dashboard. In\naddition to providing several visualization types, the dashboard facilitates\nraw data results from the sentiment classification which can be downloaded for\nfurther analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lien_A/0/1/0/all/0/1\">Andreas Kilde Lien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randem_L/0/1/0/all/0/1\">Lars Martin Randem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taralrud_H/0/1/0/all/0/1\">Hans Petter Fauchald Taralrud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edalati_M/0/1/0/all/0/1\">Maryam Edalati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FETILDA: An Effective Framework For Fin-tuned Embeddings For Long Financial Text Documents. (arXiv:2206.06952v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06952","description":"<p>Unstructured data, especially text, continues to grow rapidly in various\ndomains. In particular, in the financial sphere, there is a wealth of\naccumulated unstructured financial data, such as the textual disclosure\ndocuments that companies submit on a regular basis to regulatory agencies, such\nas the Securities and Exchange Commission (SEC). These documents are typically\nvery long and tend to contain valuable soft information about a company's\nperformance. It is therefore of great interest to learn predictive models from\nthese long textual documents, especially for forecasting numerical key\nperformance indicators (KPIs). Whereas there has been a great progress in\npre-trained language models (LMs) that learn from tremendously large corpora of\ntextual data, they still struggle in terms of effective representations for\nlong documents. Our work fills this critical need, namely how to develop better\nmodels to extract useful information from long textual documents and learn\neffective features that can leverage the soft financial and risk information\nfor text regression (prediction) tasks. In this paper, we propose and implement\na deep learning framework that splits long documents into chunks and utilizes\npre-trained LMs to process and aggregate the chunks into vector\nrepresentations, followed by self-attention to extract valuable document-level\nfeatures. We evaluate our model on a collection of 10-K public disclosure\nreports from US banks, and another dataset of reports submitted by US\ncompanies. Overall, our framework outperforms strong baseline methods for\ntextual modeling as well as a baseline regression model using only numerical\ndata. Our work provides better insights into how utilizing pre-trained\ndomain-specific and fine-tuned long-input LMs in representing long documents\ncan improve the quality of representation of textual data, and therefore, help\nin improving predictive analyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bolun &quot;Namir&quot; Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawte_V/0/1/0/all/0/1\">Vipula D. Rawte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammed J. Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aparna Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Experimental Investigation of Part-Of-Speech Taggers for Vietnamese. (arXiv:2206.06992v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06992","description":"<p>Part-of-speech (POS) tagging plays an important role in Natural Language\nProcessing (NLP). Its applications can be found in many NLP tasks such as named\nentity recognition, syntactic parsing, dependency parsing and text chunking. In\nthe investigation conducted in this paper, we utilize the technologies of two\nwidely-used toolkits, ClearNLP and Stanford POS Tagger, as well as develop two\nnew POS taggers for Vietnamese, then compare them to three well-known\nVietnamese taggers, namely JVnTagger, vnTagger and RDRPOSTagger. We make a\nsystematic comparison to find out the tagger having the best performance. We\nalso design a new feature set to measure the performance of the statistical\ntaggers. Our new taggers built from Stanford Tagger and ClearNLP with the new\nfeature set can outperform all other current Vietnamese taggers in term of\ntagging accuracy. Moreover, we also analyze the affection of some features to\nthe performance of statistical taggers. Lastly, the experimental results also\nreveal that the transformation-based tagger, RDRPOSTagger, can run\nsignificantly faster than any other statistical tagger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_Q/0/1/0/all/0/1\">Quoc-Tuan Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1\">Anh-Cuong Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable AMR Meaning Features. (arXiv:2206.07023v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07023","description":"<p>Metrics for graph-based meaning representations (e.g., Abstract Meaning\nRepresentation, AMR) can help us uncover key semantic aspects in which two\nsentences are similar to each other. However, such metrics tend to be slow,\nrely on parsers, and do not reach state-of-the-art performance when rating\nsentence similarity. On the other hand, models based on large-pretrained\nlanguage models, such as S(entence)BERT, show high correlation to human\nsimilarity ratings, but lack interpretability.\n</p>\n<p>In this paper, we aim at the best of these two worlds, by creating similarity\nmetrics that are highly effective, while also providing an interpretable\nrationale for their rating. Our approach works in two steps: We first select\nAMR graph metrics that measure meaning similarity of sentences with respect to\nkey semantic facets, such as, i.a., semantic roles, negation, or\nquantification. Second, we employ these metrics to induce Semantically\nStructured Sentence BERT embeddings (S$^3$BERT), which are composed of\ndifferent meaning aspects captured in different sub-spaces. In our experimental\nstudies, we show that our approach offers a valuable balance between\nperformance and interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational linguistics and Natural Language Processing. (arXiv:2206.07026v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07026","description":"<p>This chapter provides an introduction to computational linguistics methods,\nwith focus on their applications to the practice and study of translation. It\ncovers computational models, methods and tools for collection, storage,\nindexing and analysis of linguistic data in the context of translation, and\ndiscusses the main methodological issues and challenges in this field. While an\nexhaustive review of existing computational linguistics methods and tools is\nbeyond the scope of this chapter, we describe the most representative\napproaches, and illustrate them with descriptions of typical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luz_S/0/1/0/all/0/1\">Saturnino Luz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Generation with Text-Editing Models. (arXiv:2206.07043v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07043","description":"<p>Text-editing models have recently become a prominent alternative to seq2seq\nmodels for monolingual text-generation tasks such as grammatical error\ncorrection, simplification, and style transfer. These tasks share a common\ntrait - they exhibit a large amount of textual overlap between the source and\ntarget texts. Text-editing models take advantage of this observation and learn\nto generate the output by predicting edit operations applied to the source\nsequence. In contrast, seq2seq models generate outputs word-by-word from\nscratch thus making them slow at inference time. Text-editing models provide\nseveral benefits over seq2seq models including faster inference speed, higher\nsample efficiency, and better control and interpretability of the outputs. This\ntutorial provides a comprehensive overview of text-editing models and current\nstate-of-the-art approaches, and analyzes their pros and cons. We discuss\nchallenges related to productionization and how these models can be used to\nmitigate hallucination and bias, both pressing challenges in the field of text\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1\">Eric Malmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallinson_J/0/1/0/all/0/1\">Jonathan Mallinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuklin_A/0/1/0/all/0/1\">Aleksandr Chuklin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamek_J/0/1/0/all/0/1\">Jakub Adamek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirylenka_D/0/1/0/all/0/1\">Daniil Mirylenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stahlberg_F/0/1/0/all/0/1\">Felix Stahlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_S/0/1/0/all/0/1\">Sebastian Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1\">Aliaksei Severyn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"fairseq S2T: Fast Speech-to-Text Modeling with fairseq. (arXiv:2010.05171v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.05171","description":"<p>We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T)\nmodeling tasks such as end-to-end speech recognition and speech-to-text\ntranslation. It follows fairseq's careful design for scalability and\nextensibility. We provide end-to-end workflows from data pre-processing, model\ntraining to offline (online) inference. We implement state-of-the-art\nRNN-based, Transformer-based as well as Conformer-based models and open-source\ndetailed training recipes. Fairseq's machine translation models and language\nmodels can be seamlessly integrated into S2T workflows for multi-task learning\nor transfer learning. Fairseq S2T documentation and examples are available at\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Model Editing at Scale. (arXiv:2110.11309v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11309","description":"<p>While large pre-trained models have enabled impressive results on a variety\nof downstream tasks, the largest existing models still make errors, and even\naccurate predictions may become outdated over time. Because detecting all such\nfailures at training time is impossible, enabling both developers and end users\nof such models to correct inaccurate outputs while leaving the model otherwise\nintact is desirable. However, the distributed, black-box nature of the\nrepresentations learned by large neural networks makes producing such targeted\nedits difficult. If presented with only a single problematic input and new\ndesired output, fine-tuning approaches tend to overfit; other editing\nalgorithms are either computationally infeasible or simply ineffective when\napplied to very large models. To enable easy post-hoc editing at scale, we\npropose Model Editor Networks using Gradient Decomposition (MEND), a collection\nof small auxiliary editing networks that use a single desired input-output pair\nto make fast, local edits to a pre-trained model's behavior. MEND learns to\ntransform the gradient obtained by standard fine-tuning, using a low-rank\ndecomposition of the gradient to make the parameterization of this\ntransformation tractable. MEND can be trained on a single GPU in less than a\nday even for 10 billion+ parameter models; once trained MEND enables rapid\napplication of new edits to the pre-trained model. Our experiments with T5,\nGPT, BERT, and BART models show that MEND is the only approach to model editing\nthat effectively edits the behavior of models with more than 10 billion\nparameters. Code and data available at\nhttps://sites.google.com/view/mend-editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Charles Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark. (arXiv:2112.13610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.13610","description":"<p>Realizing general-purpose language intelligence has been a longstanding goal\nfor natural language processing, where standard evaluation benchmarks play a\nfundamental and guiding role. We argue that for general-purpose language\nintelligence evaluation, the benchmark itself needs to be comprehensive and\nsystematic. To this end, we propose CUGE, a Chinese Language Understanding and\nGeneration Evaluation benchmark with the following features: (1) Hierarchical\nbenchmark framework, where datasets are principally selected and organized with\na language capability-task-dataset hierarchy. (2) Multi-level scoring strategy,\nwhere different levels of model performance are provided based on the\nhierarchical framework. To facilitate CUGE, we provide a public leaderboard\nthat can be customized to support flexible model judging criteria. Evaluation\nresults on representative pre-trained language models indicate ample room for\nimprovement towards general-purpose language intelligence. CUGE is publicly\navailable at cuge.baai.ac.cn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jinran Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zheni Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuancheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinliang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zile Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KIND: an Italian Multi-Domain Dataset for Named Entity Recognition. (arXiv:2112.15099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15099","description":"<p>In this paper we present KIND, an Italian dataset for Named-entity\nrecognition. It contains more than one million tokens with annotation covering\nthree classes: person, location, and organization. The dataset (around 600K\ntokens) mostly contains manual gold annotations in three different domains\n(news, literature, and political discourses) and a semi-automatically annotated\npart. The multi-domain feature is the main strength of the present work,\noffering a resource which covers different styles and language uses, as well as\nthe largest Italian NER dataset with manual gold annotations. It represents an\nimportant resource for the training of NER systems in Italian. Texts and\nannotations are freely downloadable from the Github repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paccosi_T/0/1/0/all/0/1\">Teresa Paccosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aprosio_A/0/1/0/all/0/1\">Alessio Palmero Aprosio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11903","description":"<p>We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection. (arXiv:2204.05515v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05515","description":"<p>Compared with unimodal data, multimodal data can provide more features to\nhelp the model analyze the sentiment of data. Previous research works rarely\nconsider token-level feature fusion, and few works explore learning the common\nfeatures related to sentiment in multimodal data to help the model fuse\nmultimodal features. In this paper, we propose a Contrastive Learning and\nMulti-Layer Fusion (CLMLF) method for multimodal sentiment detection.\nSpecifically, we first encode text and image to obtain hidden representations,\nand then use a multi-layer fusion module to align and fuse the token-level\nfeatures of text and image. In addition to the sentiment analysis task, we also\ndesigned two contrastive learning tasks, label based contrastive learning and\ndata based contrastive learning tasks, which will help the model learn common\nfeatures related to sentiment in multimodal data. Extensive experiments\nconducted on three publicly available multimodal datasets demonstrate the\neffectiveness of our approach for multimodal sentiment detection compared with\nexisting methods. The codes are available for use at\nhttps://github.com/Link-Li/CLMLF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Conghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems. (arXiv:2205.15060v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15060","description":"<p>In this paper, we present Duplex Conversation, a multi-turn, multimodal\nspoken dialogue system that enables telephone-based agents to interact with\ncustomers like a human. We use the concept of full-duplex in telecommunication\nto demonstrate what a human-like interactive experience should be and how to\nachieve smooth turn-taking through three subtasks: user state detection,\nbackchannel selection, and barge-in detection. Besides, we propose\nsemi-supervised learning with multimodal data augmentation to leverage\nunlabeled data to increase model generalization. Experimental results on three\nsub-tasks show that the proposed method achieves consistent improvements\ncompared with baselines. We deploy the Duplex Conversation to Alibaba\nintelligent customer service and share lessons learned in production. Online\nA/B experiments show that the proposed system can significantly reduce response\nlatency by 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ask to Know More: Generating Counterfactual Explanations for Fake Claims. (arXiv:2206.04869v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04869","description":"<p>Automated fact checking systems have been proposed that quickly provide\nveracity prediction at scale to mitigate the negative influence of fake news on\npeople and on public opinion. However, most studies focus on veracity\nclassifiers of those systems, which merely predict the truthfulness of news\narticles. We posit that effective fact checking also relies on people's\nunderstanding of the predictions. In this paper, we propose elucidating fact\nchecking predictions using counterfactual explanations to help people\nunderstand why a specific piece of news was identified as fake. In this work,\ngenerating counterfactual explanations for fake news involves three steps:\nasking good questions, finding contradictions, and reasoning appropriately. We\nframe this research question as contradicted entailment reasoning through\nquestion answering (QA). We first ask questions towards the false claim and\nretrieve potential answers from the relevant evidence documents. Then, we\nidentify the most contradictory answer to the false claim by use of an\nentailment classifier. Finally, a counterfactual explanation is created using a\nmatched QA pair with three different counterfactual explanation forms.\nExperiments are conducted on the FEVER dataset for both system and human\nevaluations. Results suggest that the proposed approach generates the most\nhelpful explanations compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shih-Chieh Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yi-Li Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_A/0/1/0/all/0/1\">Aiping Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task. (arXiv:2206.05777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05777","description":"<p>This paper describes the submission of our end-to-end YiTrans speech\ntranslation system for the IWSLT 2022 offline task, which translates from\nEnglish audio to German, Chinese, and Japanese. The YiTrans system is built on\nlarge-scale pre-trained encoder-decoder models. More specifically, we first\ndesign a multi-stage pre-training strategy to build a multi-modality model with\na large amount of labeled and unlabeled data. We then fine-tune the\ncorresponding components of the model for the downstream speech translation\ntasks. Moreover, we make various efforts to improve performance, such as data\nfiltering, data augmentation, speech segmentation, model ensemble, and so on.\nExperimental results show that our YiTrans system obtains a significant\nimprovement than the strong baseline on three translation directions, and it\nachieves +5.2 BLEU improvements over last year's optimal end-to-end system on\ntst2021 English-German. Our final submissions rank first on English-German and\nEnglish-Chinese end-to-end systems in terms of the automatic evaluation metric.\nWe make our code and models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1\">Junyi Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-critiquing models for assisting human evaluators. (arXiv:2206.05802v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05802","description":"<p>We fine-tune large language models to write natural language critiques\n(natural language critical comments) using behavioral cloning. On a topic-based\nsummarization task, critiques written by our models help humans find flaws in\nsummaries that they would have otherwise missed. Our models help find naturally\noccurring flaws in both model and human written summaries, and intentional\nflaws in summaries written by humans to be deliberately misleading. We study\nscaling properties of critiquing with both topic-based summarization and\nsynthetic tasks. Larger models write more helpful critiques, and on most tasks,\nare better at self-critiquing, despite having harder-to-critique outputs.\nLarger models can also integrate their own self-critiques as feedback, refining\ntheir own summaries into better ones. Finally, we motivate and introduce a\nframework for comparing critiquing ability to generation and discrimination\nability. Our measurements suggest that even large models may still have\nrelevant knowledge they cannot or do not articulate as critiques. These results\nare a proof of concept for using AI-assisted human feedback to scale the\nsupervision of machine learning systems to tasks that are difficult for humans\nto evaluate directly. We release our training datasets, as well as samples from\nour critique assistance experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Catherine Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bills_S/0/1/0/all/0/1\">Steven Bills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_J/0/1/0/all/0/1\">Jonathan Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.05895","description":"<p>Latent space Energy-Based Models (EBMs), also known as energy-based priors,\nhave drawn growing interests in generative modeling. Fueled by its flexibility\nin the formulation and strong modeling power of the latent space, recent works\nbuilt upon it have made interesting attempts aiming at the interpretability of\ntext modeling. However, latent space EBMs also inherit some flaws from EBMs in\ndata space; the degenerate MCMC sampling quality in practice can lead to poor\ngeneration quality and instability in training, especially on data with complex\nlatent structures. Inspired by the recent efforts that leverage diffusion\nrecovery likelihood learning as a cure for the sampling issue, we introduce a\nnovel symbiosis between the diffusion models and latent space EBMs in a\nvariational learning framework, coined as the latent diffusion energy-based\nmodel. We develop a geometric clustering-based regularization jointly with the\ninformation bottleneck to further improve the quality of the learned latent\nspace. Experiments on several challenging tasks demonstrate the superior\nperformance of our model on interpretable text modeling over strong\ncounterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peiyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sirui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1\">Baoxiong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Construction and Its Application in Automatic Radiology Report Generation from Radiologist's Dictation. (arXiv:2206.06308v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.06308","description":"<p>Conventionally, the radiologist prepares the diagnosis notes and shares them\nwith the transcriptionist. Then the transcriptionist prepares a preliminary\nformatted report referring to the notes, and finally, the radiologist reviews\nthe report, corrects the errors, and signs off. This workflow causes\nsignificant delays and errors in the report. In current research work, we focus\non applications of NLP techniques like Information Extraction (IE) and\ndomain-specific Knowledge Graph (KG) to automatically generate radiology\nreports from radiologist's dictation. This paper focuses on KG construction for\neach organ by extracting information from an existing large corpus of free-text\nradiology reports. We develop an information extraction pipeline that combines\nrule-based, pattern-based, and dictionary-based techniques with\nlexical-semantic features to extract entities and relations. Missing\ninformation in short dictation can be accessed from the KGs to generate\npathological descriptions and hence the radiology report. Generated\npathological descriptions evaluated using semantic similarity metrics, which\nshows 97% similarity with gold standard pathological descriptions. Also, our\nanalysis shows that our IE module is performing better than the OpenIE tool for\nthe radiology domain. Furthermore, we include a manual qualitative analysis\nfrom radiologists, which shows that 80-85% of the generated reports are\ncorrectly written, and the remaining are partially correct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kale_K/0/1/0/all/0/1\">Kaveri Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1\">Aditya Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gune_M/0/1/0/all/0/1\">Milind Gune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_K/0/1/0/all/0/1\">Kush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawyer_R/0/1/0/all/0/1\">Rustom Lawyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Spriha Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Compositional Mixture Representations for Vision and Text. (arXiv:2206.06404v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06404","description":"<p>Learning a common representation space between vision and language allows\ndeep networks to relate objects in the image to the corresponding semantic\nmeaning. We present a model that learns a shared Gaussian mixture\nrepresentation imposing the compositionality of the text onto the visual domain\nwithout having explicit location supervision. By combining the spatial\ntransformer with a representation learning approach we learn to split images\ninto separately encoded patches to associate visual and textual representations\nin an interpretable manner. On variations of MNIST and CIFAR10, our model is\nable to perform weakly supervised object detection and demonstrates its ability\nto extrapolate to unseen combination of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alaniz_S/0/1/0/all/0/1\">Stephan Alaniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federici_M/0/1/0/all/0/1\">Marco Federici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06420","description":"<p>Modern multi-layer perceptron (MLP) models have shown competitive results in\nlearning visual representations without self-attention. However, existing MLP\nmodels are not good at capturing local details and lack prior knowledge of\nhuman configurations, which limits their modeling power for skeletal\nrepresentation learning. To address these issues, we propose a simple yet\neffective graph-reinforced MLP-Like architecture, named GraphMLP, that combines\nMLPs and graph convolutional networks (GCNs) in a global-local-graphical\nunified architecture for 3D human pose estimation. GraphMLP incorporates the\ngraph structure of human bodies into an MLP model to meet the domain-specific\ndemand while also allowing for both local and global spatial interactions.\nExtensive experiments show that the proposed GraphMLP achieves state-of-the-art\nperformance on two datasets, i.e., Human3.6M and MPI-INF-3DHP. Our source code\nand pretrained models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tianyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runwei Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-purpose Real Haze Benchmark with Quantifiable Haze Levels and Ground Truth. (arXiv:2206.06427v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06427","description":"<p>Imagery collected from outdoor visual environments is often degraded due to\nthe presence of dense smoke or haze. A key challenge for research in scene\nunderstanding in these degraded visual environments (DVE) is the lack of\nrepresentative benchmark datasets. These datasets are required to evaluate\nstate-of-the-art object recognition and other computer vision algorithms in\ndegraded settings. In this paper, we address some of these limitations by\nintroducing the first paired real image benchmark dataset with hazy and\nhaze-free images, and in-situ haze density measurements. This dataset was\nproduced in a controlled environment with professional smoke generating\nmachines that covered the entire scene, and consists of images captured from\nthe perspective of both an unmanned aerial vehicle (UAV) and an unmanned ground\nvehicle (UGV). We also evaluate a set of representative state-of-the-art\ndehazing approaches as well as object detectors on the dataset. The full\ndataset presented in this paper, including the ground truth object\nclassification bounding boxes and haze density measurements, is provided for\nthe community to evaluate their algorithms at: https://a2i2-archangel.vision. A\nsubset of this dataset has been used for the Object Detection in Haze Track of\nCVPR UG2 2022 challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1\">Priya Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thielke_M/0/1/0/all/0/1\">Matthew D Thielke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_J/0/1/0/all/0/1\">John G Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_A/0/1/0/all/0/1\">Andre V Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAgostino_J/0/1/0/all/0/1\">John A D&#x27;Agostino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">James D Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quang_L/0/1/0/all/0/1\">Long P Quang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uplinger_J/0/1/0/all/0/1\">James R Uplinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heesung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Training Method For VideoPose3D With Ideology of Action Recognition. (arXiv:2206.06430v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06430","description":"<p>Action recognition and pose estimation from videos are closely related to\nunderstand human motions, but more literature focuses on how to solve pose\nestimation tasks alone from action recognition. This research shows a faster\nand more flexible training method for VideoPose3D which is based on action\nrecognition. This model is fed with the same type of action as the type that\nwill be estimated, and different types of actions can be trained separately.\nEvidence has shown that, for common pose-estimation tasks, this model requires\na relatively small amount of data to carry out similar results with the\noriginal research, and for action-oriented tasks, it outperforms the original\nresearch by 4.5% with a limited receptive field size and training epoch on\nVelocity Error of MPJPE. This model can handle both action-oriented and common\npose-estimation problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Hao Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICP Algorithm: Theory, Practice And Its SLAM-oriented Taxonomy. (arXiv:2206.06435v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06435","description":"<p>The Iterative Closest Point (ICP) algorithm is one of the most important\nalgorithms for geometric alignment of three-dimensional surface registration,\nwhich is frequently used in computer vision tasks, including the Simultaneous\nLocalization And Mapping (SLAM) tasks. In this paper, we illustrate the\ntheoretical principles of the ICP algorithm, how it can be used in surface\nregistration tasks, and the traditional taxonomy of the variants of the ICP\nalgorithm. As SLAM is becoming a popular topic, we also introduce a\nSLAM-oriented taxonomy of the ICP algorithm, based on the characteristics of\neach type of SLAM task, including whether the SLAM task is online or not and\nwhether the landmarks are present as features in the SLAM task. We make a\nsynthesis of each type of SLAM task by comparing several up-to-date research\npapers and analyzing their implementation details.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Hao Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fitting Segmentation Networks on Varying Image Resolutions using Splatting. (arXiv:2206.06445v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06445","description":"<p>Data used in image segmentation are not always defined on the same grid. This\nis particularly true for medical images, where the resolution, field-of-view\nand orientation can differ across channels and subjects. Images and labels are\ntherefore commonly resampled onto the same grid, as a pre-processing step.\nHowever, the resampling operation introduces partial volume effects and\nblurring, thereby changing the effective resolution and reducing the contrast\nbetween structures. In this paper we propose a splat layer, which automatically\nhandles resolution mismatches in the input data. This layer pushes each image\nonto a mean space where the forward pass is performed. As the splat operator is\nthe adjoint to the resampling operator, the mean-space prediction can be pulled\nback to the native label space, where the loss function is computed. Thus, the\nneed for explicit resolution adjustment using interpolation is removed. We show\non two publicly available datasets, with simulated and real multi-modal\nmagnetic resonance images, that this model improves segmentation results\ncompared to resampling as a pre-processing step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Brudfors_M/0/1/0/all/0/1\">Mikael Brudfors</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balbastre_Y/0/1/0/all/0/1\">Yael Balbastre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashburner_J/0/1/0/all/0/1\">John Ashburner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rees_G/0/1/0/all/0/1\">Geraint Rees</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN. (arXiv:2206.06448v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06448","description":"<p>Training computer-vision related algorithms on medical images for disease\ndiagnosis or image segmentation is difficult in large part due to privacy\nconcerns. For this reason, generative image models are highly sought after to\nfacilitate data sharing. However, 3-D generative models are understudied, and\ninvestigation of their privacy leakage is needed. We introduce our 3-D\ngenerative model, Transversal GAN (TrGAN), using head &amp; neck PET images which\nare conditioned on tumour masks as a case study. We define quantitative\nmeasures of image fidelity, utility and privacy for our model. These metrics\nare evaluated in the course of training to identify ideal fidelity, utility and\nprivacy trade-offs and establish the relationships between these parameters. We\nshow that the discriminator of the TrGAN is vulnerable to attack, and that an\nattacker can identify which samples were used in training with almost perfect\naccuracy (AUC = 0.99). We also show that an attacker with access to only the\ngenerator cannot reliably classify whether a sample had been used for training\n(AUC = 0.51). This suggests that TrGAN generators, but not discriminators, may\nbe used for sharing synthetic 3-D PET data with minimal privacy risk while\nmaintaining good utility and fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bergen_R/0/1/0/all/0/1\">Robert V. Bergen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajotte_J/0/1/0/all/0/1\">Jean-Francois Rajotte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yousefirizi_F/0/1/0/all/0/1\">Fereshteh Yousefirizi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_R/0/1/0/all/0/1\">Raymond T. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning With MUlti-Segmental Informational Coding (MUSIC). (arXiv:2206.06461v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06461","description":"<p>Self-supervised representation learning maps high-dimensional data into a\nmeaningful embedding space, where samples of similar semantic contents are\nclose to each other. Most of the recent representation learning methods\nmaximize cosine similarity or minimize the distance between the embedding\nfeatures of different views from the same sample usually on the $l2$ normalized\nunit hypersphere. To prevent the trivial solutions that all samples have the\nsame embedding feature, various techniques have been developed, such as\ncontrastive learning, stop gradient, variance and covariance regularization,\netc. In this study, we propose MUlti-Segmental Informational Coding (MUSIC) for\nself-supervised representation learning. MUSIC divides the embedding feature\ninto multiple segments that discriminatively partition samples into different\nsemantic clusters and different segments focus on different partition\nprinciples. Information theory measurements are directly used to optimize MUSIC\nand theoretically guarantee trivial solutions are avoided. MUSIC does not\ndepend on commonly used techniques, such as memory bank or large batches,\nasymmetry networks, gradient stopping, momentum weight updating, etc, making\nthe training framework flexible. Our experiments demonstrate that MUSIC\nachieves better results than most related Barlow Twins and VICReg methods on\nImageNet classification with linear probing, and requires neither deep\nprojectors nor large feature dimensions. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Shape-Bias of Deep Learning for Dermoscopic Skin Lesion Classification. (arXiv:2206.06466v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06466","description":"<p>It is generally believed that the human visual system is biased towards the\nrecognition of shapes rather than textures. This assumption has led to a\ngrowing body of work aiming to align deep models' decision-making processes\nwith the fundamental properties of human vision. The reliance on shape features\nis primarily expected to improve the robustness of these models under covariate\nshift. In this paper, we revisit the significance of shape-biases for the\nclassification of skin lesion images. Our analysis shows that different skin\nlesion datasets exhibit varying biases towards individual image features.\nInterestingly, despite deep feature extractors being inclined towards learning\nentangled features for skin lesion classification, individual features can\nstill be decoded from this entangled representation. This indicates that these\nfeatures are still represented in the learnt embedding spaces of the models,\nbut not used for classification. In addition, the spectral analysis of\ndifferent datasets shows that in contrast to common visual recognition,\ndermoscopic skin lesion classification, by nature, is reliant on complex\nfeature combinations beyond shape-bias. As a natural consequence, shifting away\nfrom the prevalent desire of shape-biasing models can even improve skin lesion\nclassifiers in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucieri_A/0/1/0/all/0/1\">Adriano Lucieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeisser_F/0/1/0/all/0/1\">Fabian Schmeisser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balada_C/0/1/0/all/0/1\">Christoph Peter Balada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shoaib Ahmed Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sheraz Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RigNeRF: Fully Controllable Neural 3D Portraits. (arXiv:2206.06481v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06481","description":"<p>Volumetric neural rendering methods, such as neural radiance fields (NeRFs),\nhave enabled photo-realistic novel view synthesis. However, in their standard\nform, NeRFs do not support the editing of objects, such as a human head, within\na scene. In this work, we propose RigNeRF, a system that goes beyond just novel\nview synthesis and enables full control of head pose and facial expressions\nlearned from a single portrait video. We model changes in head pose and facial\nexpressions using a deformation field that is guided by a 3D morphable face\nmodel (3DMM). The 3DMM effectively acts as a prior for RigNeRF that learns to\npredict only residuals to the 3DMM deformations and allows us to render novel\n(rigid) poses and (non-rigid) expressions that were not present in the input\nsequence. Using only a smartphone-captured short video of a subject for\ntraining, we demonstrate the effectiveness of our method on free view synthesis\nof a portrait scene with explicit head pose and expression controls. The\nproject page can be found here:\n<a href=\"http://shahrukhathar.github.io/2022/06/06/RigNeRF.html\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athar_S/0/1/0/all/0/1\">ShahRukh Athar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice. (arXiv:2206.06484v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06484","description":"<p>We study two of the most popular performance metrics in medical image\nsegmentation, Accuracy and Dice, when the target labels are noisy. For both\nmetrics, several statements related to characterization and volume properties\nof the set of optimal segmentations are proved, and associated experiments are\nprovided. Our main insights are: (i) the volume of the solutions to both\nmetrics may deviate significantly from the expected volume of the target, (ii)\nthe volume of a solution to Accuracy is always less than or equal to the volume\nof a solution to Dice and (iii) the optimal solutions to both of these metrics\ncoincide when the set of feasible segmentations is constrained to the set of\nsegmentations with the volume equal to the expected volume of the target.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nordstrom_M/0/1/0/all/0/1\">Marcus Nordstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hult_H/0/1/0/all/0/1\">Henrik Hult</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soderberg_J/0/1/0/all/0/1\">Jonas S&#xf6;derberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lofman_F/0/1/0/all/0/1\">Fredrik L&#xf6;fman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Modality Focusing Hypothesis: On the Blink of Multimodal Knowledge Distillation. (arXiv:2206.06487v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06487","description":"<p>Multimodal knowledge distillation (KD) extends traditional knowledge\ndistillation to the area of multimodal learning. One common practice is to\nadopt a well-performed multimodal network as the teacher in the hope that it\ncan transfer its full knowledge to a unimodal student for performance\nimprovement. In this paper, we investigate the efficacy of multimodal KD. We\nbegin by providing two failure cases of it and demonstrate that KD is not a\nuniversal cure in multimodal knowledge transfer. We present the modality Venn\ndiagram to understand modality relationships and the modality focusing\nhypothesis revealing the decisive factor in the efficacy of multimodal KD.\nExperimental results on 6 multimodal datasets help justify our hypothesis,\ndiagnose failure cases, and point directions to improve distillation\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Learning with Transformers: A Survey. (arXiv:2206.06488v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06488","description":"<p>Transformer is a promising neural network learner, and has achieved great\nsuccess in various machine learning tasks. Thanks to the recent prevalence of\nmultimodal applications and big data, Transformer-based multimodal learning has\nbecome a hot topic in AI research. This paper presents a comprehensive survey\nof Transformer techniques oriented at multimodal data. The main contents of\nthis survey include: (1) a background of multimodal learning, Transformer\necosystem, and the multimodal big data era, (2) a theoretical review of Vanilla\nTransformer, Vision Transformer, and multimodal Transformers, from a\ngeometrically topological perspective, (3) a review of multimodal Transformer\napplications, via two important paradigms, i.e., for multimodal pretraining and\nfor specific multimodal tasks, (4) a summary of the common challenges and\ndesigns shared by the multimodal Transformer models and applications, and (5) a\ndiscussion of open problems and potential research directions for the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEHAVIOR in Habitat 2.0: Simulator-Independent Logical Task Description for Benchmarking Embodied AI Agents. (arXiv:2206.06489v1 [cs.AI])","link":"http://arxiv.org/abs/2206.06489","description":"<p>Robots excel in performing repetitive and precision-sensitive tasks in\ncontrolled environments such as warehouses and factories, but have not been yet\nextended to embodied AI agents providing assistance in household tasks.\nInspired by the catalyzing effect that benchmarks have played in the AI fields\nsuch as computer vision and natural language processing, the community is\nlooking for new benchmarks for embodied AI. Prior work in embodied AI benchmark\ndefines tasks using a different formalism, often specific to one environment,\nsimulator or domain, making it hard to develop general and comparable\nsolutions. In this work, we bring a subset of BEHAVIOR activities into Habitat\n2.0 to benefit from its fast simulation speed, as a first step towards\ndemonstrating the ease of adapting activities defined in the logic space into\ndifferent simulators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Task-Independent Game State Representations from Unlabeled Images. (arXiv:2206.06490v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06490","description":"<p>Self-supervised learning (SSL) techniques have been widely used to learn\ncompact and informative representations from high-dimensional complex data. In\nmany computer vision tasks, such as image classification, such methods achieve\nstate-of-the-art results that surpass supervised learning approaches. In this\npaper, we investigate whether SSL methods can be leveraged for the task of\nlearning accurate state representations of games, and if so, to what extent.\nFor this purpose, we collect game footage frames and corresponding sequences of\ngames' internal state from three different 3D games: VizDoom, the CARLA racing\nsimulator and the Google Research Football Environment. We train an image\nencoder with three widely used SSL algorithms using solely the raw frames, and\nthen attempt to recover the internal state variables from the learned\nrepresentations. Our results across all three games showcase significantly\nhigher correlation between SSL representations and the game's internal state\ncompared to pre-trained baseline models such as ImageNet. Such findings suggest\nthat SSL-based visual encoders can yield general -- not tailored to a specific\ntask -- yet informative game representations solely from game pixel\ninformation. Such representations can, in turn, form the basis for boosting the\nperformance of downstream learning tasks in games, including gameplaying,\ncontent generation and player modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_C/0/1/0/all/0/1\">Chintan Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makantasis_K/0/1/0/all/0/1\">Konstantinos Makantasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1\">Antonios Liapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spiking Neural Networks for Frame-based and Event-based Single Object Localization. (arXiv:2206.06506v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06506","description":"<p>Spiking neural networks have shown much promise as an energy-efficient\nalternative to artificial neural networks. However, understanding the impacts\nof sensor noises and input encodings on the network activity and performance\nremains difficult with common neuromorphic vision baselines like\nclassification. Therefore, we propose a spiking neural network approach for\nsingle object localization trained using surrogate gradient descent, for frame-\nand event-based sensors. We compare our method with similar artificial neural\nnetworks and show that our model has competitive/better performance in\naccuracy, robustness against various corruptions, and has lower energy\nconsumption. Moreover, we study the impact of neural coding schemes for static\nimages in accuracy, robustness, and energy efficiency. Our observations differ\nimportantly from previous studies on bio-plausible learning rules, which helps\nin the design of surrogate gradient trained architectures, and offers insight\nto design priorities in future neuromorphic technologies in terms of noise\ncharacteristics and data encoding methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barchid_S/0/1/0/all/0/1\">Sami Barchid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mennesson_J/0/1/0/all/0/1\">Jos&#xe9; Mennesson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshraghian_J/0/1/0/all/0/1\">Jason Eshraghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Dj&#xe9;raba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Method for Face Anti-Spoofing with Semi-Supervised Learning. (arXiv:2206.06510v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06510","description":"<p>Face anti-spoofing has drawn a lot of attention due to the high security\nrequirements in biometric authentication systems. Bringing face biometric to\ncommercial hardware became mostly dependent on developing reliable methods for\ndetecting fake login sessions without specialized sensors. Current CNN-based\nmethod perform well on the domains they were trained for, but often show poor\ngeneralization on previously unseen datasets. In this paper we describe a\nmethod for utilizing unsupervised pretraining for improving performance across\nmultiple datasets without any adaptation, introduce the Entry Antispoofing\nDataset for supervised fine-tuning, and propose a multi-class auxiliary\nclassification layer for augmenting the binary classification task of detecting\nspoofing attempts with explicit interpretable signals. We demonstrate the\nefficiency of our model by achieving state-of-the-art results on cross-dataset\ntesting on MSU-MFSD, Replay-Attack, and OULU-NPU datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sergievskiy_N/0/1/0/all/0/1\">Nikolay Sergievskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlasov_R/0/1/0/all/0/1\">Roman Vlasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trusov_R/0/1/0/all/0/1\">Roman Trusov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Pose from Pressure Data for Smart Beds with Deep Image-based Pose Estimators. (arXiv:2206.06518v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06518","description":"<p>In-bed pose estimation has shown value in fields such as hospital patient\nmonitoring, sleep studies, and smart homes. In this paper, we explore different\nstrategies for detecting body pose from highly ambiguous pressure data, with\nthe aid of pre-existing pose estimators. We examine the performance of\npre-trained pose estimators by using them either directly or by re-training\nthem on two pressure datasets. We also explore other strategies utilizing a\nlearnable pre-processing domain adaptation step, which transforms the vague\npressure maps to a representation closer to the expected input space of common\npurpose pose estimation modules. Accordingly, we used a fully convolutional\nnetwork with multiple scales to provide the pose-specific characteristics of\nthe pressure maps to the pre-trained pose estimation module. Our complete\nanalysis of different approaches shows that the combination of learnable\npre-processing module along with re-training pre-existing image-based pose\nestimators on the pressure data is able to overcome issues such as highly vague\npressure points to achieve very high pose estimation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davoodnia_V/0/1/0/all/0/1\">Vandad Davoodnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_S/0/1/0/all/0/1\">Saeed Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning. (arXiv:2206.06522v1 [cs.CL])","link":"http://arxiv.org/abs/2206.06522","description":"<p>Fine-tuning large pre-trained models on downstream tasks has been adopted in\na variety of domains recently. However, it is costly to update the entire\nparameter set of large pre-trained models. Although recently proposed\nparameter-efficient transfer learning (PETL) techniques allow updating a small\nsubset of parameters (e.g. only using 2% of parameters) inside a pre-trained\nbackbone network for a new task, they only reduce the training memory\nrequirement by up to 30%. This is because the gradient computation for the\ntrainable parameters still requires backpropagation through the large\npre-trained backbone model. To address this, we propose Ladder Side-Tuning\n(LST), a new PETL technique that reduces training memory requirements by more\nsubstantial amounts. Unlike existing parameter-efficient methods that insert\nadditional parameters inside backbone networks, we train a ladder side network,\na small and separate network that takes intermediate activations as input via\nshortcut connections (ladders) from backbone networks and makes predictions.\nLST has significantly lower memory requirements than previous methods, because\nit does not require backpropagation through the backbone network, but instead\nonly through the side network and ladder connections. We evaluate our method\nwith various models (T5, CLIP-T5) on both NLP (GLUE) and vision-language (VQA,\nGQA, NLVR2, MSCOCO) tasks. LST saves 69% of the memory costs to fine-tune the\nwhole network, while other methods only save 26% of that in similar parameter\nusages (hence, 2.7x more memory savings). Moreover, LST achieves higher\naccuracy than Adapter and LoRA in a low-memory regime. To further show the\nadvantage of this better memory efficiency, we also apply LST to larger T5\nmodels (T5-large, T5-3B), attaining better GLUE performance than full\nfine-tuning and other PETL methods. The exact same trend also holds in our\nexperiments on VL tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D scene reconstruction from monocular spherical video with motion parallax. (arXiv:2206.06533v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06533","description":"<p>In this paper, we describe a method to capture nearly entirely spherical (360\ndegree) depth information using two adjacent frames from a single spherical\nvideo with motion parallax. After illustrating a spherical depth information\nretrieval using two spherical cameras, we demonstrate monocular spherical\nstereo by using stabilized first-person video footage. Experiments demonstrated\nthat the depth information was retrieved on up to 97% of the entire sphere in\nsolid angle. At a speed of 30 km/h, we were able to estimate the depth of an\nobject located over 30 m from the camera. We also reconstructed the 3D\nstructures (point cloud) using the obtained depth data and confirmed the\nstructures can be clearly observed. We can apply this method to 3D structure\nretrieval of surrounding environments such as 1) previsualization, location\nhunting/planning of a film, 2) real scene/computer graphics synthesis and 3)\nmotion capture. Thanks to its simplicity, this method can be applied to various\nvideos. As there is no pre-condition other than to be a 360 video with motion\nparallax, we can use any 360 videos including those on the Internet to\nreconstruct the surrounding environments. The cameras can be lightweight enough\nto be mounted on a drone. We also demonstrated such applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1\">Kenji Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel-by-pixel Mean Opinion Score (pMOS) for No-Reference Image Quality Assessment. (arXiv:2206.06541v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06541","description":"<p>Deep-learning based techniques have contributed to the remarkable progress in\nthe field of automatic image quality assessment (IQA). Existing IQA methods are\ndesigned to measure the quality of an image in terms of Mean Opinion Score\n(MOS) at the image-level (i.e. the whole image) or at the patch-level (dividing\nthe image into multiple units and measuring quality of each patch). Some\napplications may require assessing the quality at the pixel-level (i.e. MOS\nvalue for each pixel), however, this is not possible in case of existing\ntechniques as the spatial information is lost owing to their network\nstructures. This paper proposes an IQA algorithm that can measure the MOS at\nthe pixel-level, in addition to the image-level MOS. The proposed algorithm\nconsists of three core parts, namely: i) Local IQA; ii) Region of Interest\n(ROI) prediction; iii) High-level feature embedding. The Local IQA part outputs\nthe MOS at the pixel-level, or pixel-by-pixel MOS - we term it 'pMOS'. The ROI\nprediction part outputs weights that characterize the relative importance of\nregion when calculating the image-level IQA. The high-level feature embedding\npart extracts high-level image features which are then embedded into the Local\nIQA part. In other words, the proposed algorithm yields three outputs: the pMOS\nwhich represents MOS for each pixel, the weights from the ROI indicating the\nrelative importance of region, and finally the image-level MOS that is obtained\nby the weighted sum of pMOS and ROI values. The image-level MOS thus obtained\nby utilizing pMOS and ROI weights shows superior performance compared to the\nexisting popular IQA techniques. In addition, visualization results indicate\nthat predicted pMOS and ROI outputs are reasonably aligned with the general\nprinciples of the human visual system (HVS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_W/0/1/0/all/0/1\">Wook-Hyung Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hahm_C/0/1/0/all/0/1\">Cheul-hee Hahm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baijal_A/0/1/0/all/0/1\">Anant Baijal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_N/0/1/0/all/0/1\">Namuk Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_I/0/1/0/all/0/1\">Ilhyun Cho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koo_J/0/1/0/all/0/1\">Jayoon Koo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Automated Data Augmentation Algorithms for Deep Learning-based Image Classication Tasks. (arXiv:2206.06544v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06544","description":"<p>In recent years, one of the most popular techniques in the computer vision\ncommunity has been the deep learning technique. As a data-driven technique,\ndeep model requires enormous amounts of accurately labelled training data,\nwhich is often inaccessible in many real-world applications. A data-space\nsolution is Data Augmentation (DA), that can artificially generate new images\nout of original samples. Image augmentation strategies can vary by dataset, as\ndifferent data types might require different augmentations to facilitate model\ntraining. However, the design of DA policies has been largely decided by the\nhuman experts with domain knowledge, which is considered to be highly\nsubjective and error-prone. To mitigate such problem, a novel direction is to\nautomatically learn the image augmentation policies from the given dataset\nusing Automated Data Augmentation (AutoDA) techniques. The goal of AutoDA\nmodels is to find the optimal DA policies that can maximize the model\nperformance gains. This survey discusses the underlying reasons of the\nemergence of AutoDA technology from the perspective of image classification. We\nidentify three key components of a standard AutoDA model: a search space, a\nsearch algorithm and an evaluation function. Based on their architecture, we\nprovide a systematic taxonomy of existing image AutoDA approaches. This paper\npresents the major works in AutoDA field, discussing their pros and cons, and\nproposing several potential directions for future improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinnott_R/0/1/0/all/0/1\">Richard O. Sinnott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safe Output Feedback Motion Planning from Images via Learned Perception Modules and Contraction Theory. (arXiv:2206.06553v1 [cs.RO])","link":"http://arxiv.org/abs/2206.06553","description":"<p>We present a motion planning algorithm for a class of uncertain\ncontrol-affine nonlinear systems which guarantees runtime safety and goal\nreachability when using high-dimensional sensor measurements (e.g., RGB-D\nimages) and a learned perception module in the feedback control loop. First,\ngiven a dataset of states and observations, we train a perception system that\nseeks to invert a subset of the state from an observation, and estimate an\nupper bound on the perception error which is valid with high probability in a\ntrusted domain near the data. Next, we use contraction theory to design a\nstabilizing state feedback controller and a convergent dynamic state observer\nwhich uses the learned perception system to update its state estimate. We\nderive a bound on the trajectory tracking error when this controller is\nsubjected to errors in the dynamics and incorrect state estimates. Finally, we\nintegrate this bound into a sampling-based motion planner, guiding it to return\ntrajectories that can be safely tracked at runtime using sensor data. We\ndemonstrate our approach in simulation on a 4D car, a 6D planar quadrotor, and\na 17D manipulation task with RGB(-D) sensor measurements, demonstrating that\nour method safely and reliably steers the system to the goal, while baselines\nthat fail to consider the trusted domain or state estimation errors can be\nunsafe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chou_G/0/1/0/all/0/1\">Glen Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozay_N/0/1/0/all/0/1\">Necmiye Ozay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berenson_D/0/1/0/all/0/1\">Dmitry Berenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Med-DANet: Dynamic Architecture Network for Efficient Medical Volumetric Segmentation. (arXiv:2206.06575v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06575","description":"<p>For 3D medical image (e.g. CT and MRI) segmentation, the difficulty of\nsegmenting each slice in a clinical case varies greatly. Previous research on\nvolumetric medical image segmentation in a slice-by-slice manner conventionally\nuse the identical 2D deep neural network to segment all the slices of the same\ncase, ignoring the data heterogeneity among image slices. In this paper, we\nfocus on multi-modal 3D MRI brain tumor segmentation and propose a dynamic\narchitecture network named Med-DANet based on adaptive model selection to\nachieve effective accuracy and efficiency trade-off. For each slice of the\ninput 3D MRI volume, our proposed method learns a slice-specific decision by\nthe Decision Network to dynamically select a suitable model from the predefined\nModel Bank for the subsequent 2D segmentation task. Extensive experimental\nresults on both BraTS 2019 and 2020 datasets show that our proposed method\nachieves comparable or better results than previous state-of-the-art methods\nfor 3D MRI brain tumor segmentation with much less model complexity. Compared\nwith the state-of-the-art 3D method TransBTS, the proposed framework improves\nthe model efficiency by up to 3.5x without sacrificing the accuracy. Our code\nwill be publicly available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics Informed Neural Fields for Smoke Reconstruction with Sparse Data. (arXiv:2206.06577v1 [cs.GR])","link":"http://arxiv.org/abs/2206.06577","description":"<p>High-fidelity reconstruction of fluids from sparse multiview RGB videos\nremains a formidable challenge due to the complexity of the underlying physics\nas well as complex occlusion and lighting in captures. Existing solutions\neither assume knowledge of obstacles and lighting, or only focus on simple\nfluid scenes without obstacles or complex lighting, and thus are unsuitable for\nreal-world scenes with unknown lighting or arbitrary obstacles. We present the\nfirst method to reconstruct dynamic fluid by leveraging the governing physics\n(ie, Navier -Stokes equations) in an end-to-end optimization from sparse videos\nwithout taking lighting conditions, geometry information, or boundary\nconditions as input. We provide a continuous spatio-temporal scene\nrepresentation using neural networks as the ansatz of density and velocity\nsolution functions for fluids as well as the radiance field for static objects.\nWith a hybrid architecture that separates static and dynamic contents, fluid\ninteractions with static obstacles are reconstructed for the first time without\nadditional geometry input or human labeling. By augmenting time-varying neural\nradiance fields with physics-informed deep learning, our method benefits from\nthe supervision of images and physical priors. To achieve robust optimization\nfrom sparse views, we introduced a layer-by-layer growing strategy to\nprogressively increase the network capacity. Using progressively growing models\nwith a new regularization term, we manage to disentangle density-color\nambiguity in radiance fields without overfitting. A pretrained\ndensity-to-velocity fluid model is leveraged in addition as the data prior to\navoid suboptimal velocity which underestimates vorticity but trivially fulfills\nphysical equations. Our method exhibits high-quality results with relaxed\nconstraints and strong flexibility on a representative set of synthetic and\nreal flow captures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_M/0/1/0/all/0/1\">Mengyu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Quan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franz_E/0/1/0/all/0/1\">Erik Franz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1\">Hans-Peter Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayer_R/0/1/0/all/0/1\">Rhaleb Zayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CorticalFlow$^{++}$: Boosting Cortical Surface Reconstruction Accuracy, Regularity, and Interoperability. (arXiv:2206.06598v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06598","description":"<p>The problem of Cortical Surface Reconstruction from magnetic resonance\nimaging has been traditionally addressed using lengthy pipelines of image\nprocessing techniques like FreeSurfer, CAT, or CIVET. These frameworks require\nvery long runtimes deemed unfeasible for real-time applications and unpractical\nfor large-scale studies. Recently, supervised deep learning approaches have\nbeen introduced to speed up this task cutting down the reconstruction time from\nhours to seconds. Using the state-of-the-art CorticalFlow model as a blueprint,\nthis paper proposes three modifications to improve its accuracy and\ninteroperability with existing surface analysis tools, while not sacrificing\nits fast inference time and low GPU memory consumption. First, we employ a more\naccurate ODE solver to reduce the diffeomorphic mapping approximation error.\nSecond, we devise a routine to produce smoother template meshes avoiding mesh\nartifacts caused by sharp edges in CorticalFlow's convex-hull based template.\nLast, we recast pial surface prediction as the deformation of the predicted\nwhite surface leading to a one-to-one mapping between white and pial surface\nvertices. This mapping is essential to many existing surface analysis tools for\ncortical morphometry. We name the resulting method CorticalFlow$^{++}$. Using\nlarge-scale datasets, we demonstrate the proposed changes provide more\ngeometric accuracy and surface regularity while keeping the reconstruction time\nand GPU memory requirements almost unchanged.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cruz_R/0/1/0/all/0/1\">Rodrigo Santa Cruz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebrat_L/0/1/0/all/0/1\">L&#xe9;o Lebrat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_D/0/1/0/all/0/1\">Darren Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bourgeat_P/0/1/0/all/0/1\">Pierrick Bourgeat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fripp_J/0/1/0/all/0/1\">Jurgen Fripp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salvado_O/0/1/0/all/0/1\">Olivier Salvado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug-and-Play Pseudo Label Correction Network for Unsupervised Person Re-identification. (arXiv:2206.06607v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06607","description":"<p>Clustering-based methods, which alternate between the generation of pseudo\nlabels and the optimization of the feature extraction network, play a dominant\nrole in both unsupervised learning (USL) and unsupervised domain adaptive (UDA)\nperson re-identification (Re-ID). To alleviate the adverse effect of noisy\npseudo labels, the existing methods either abandon unreliable labels or refine\nthe pseudo labels via mutual learning or label propagation. However, a great\nmany erroneous labels are still accumulated because these methods mostly adopt\ntraditional unsupervised clustering algorithms which rely on certain\nassumptions on data distribution and fail to capture the distribution of\ncomplex real-world data. In this paper, we propose the plug-and-play\ngraph-based pseudo label correction network (GLC) to refine the pseudo labels\nin the manner of supervised clustering. GLC is trained to perceive the varying\ndata distribution at each epoch of the self-training with the supervision of\ninitial pseudo labels generated by any clustering method. It can learn to\nrectify the initial noisy labels by means of the relationship constraints\nbetween samples on the k Nearest Neighbor (kNN) graph and early-stop training\nstrategy. Specifically, GLC learns to aggregate node features from neighbors\nand predict whether the nodes should be linked on the graph. Besides, GLC is\noptimized with 'early stop' before the noisy labels are severely memorized to\nprevent overfitting to noisy pseudo labels. Consequently, GLC improves the\nquality of pseudo labels though the supervision signals contain some noise,\nleading to better Re-ID performance. Extensive experiments in USL and UDA\nperson Re-ID on Market-1501 and MSMT17 show that our method is widely\ncompatible with various clustering-based methods and promotes the\nstate-of-the-art performance consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tianyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+guo_H/0/1/0/all/0/1\">Haiyun guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Matching Semi-Supervised Object Detection. (arXiv:2206.06608v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06608","description":"<p>Semi-supervised object detection has made significant progress with the\ndevelopment of mean teacher driven self-training. Despite the promising\nresults, the label mismatch problem is not yet fully explored in the previous\nworks, leading to severe confirmation bias during self-training. In this paper,\nwe delve into this problem and propose a simple yet effective LabelMatch\nframework from two different yet complementary perspectives, i.e.,\ndistribution-level and instance-level. For the former one, it is reasonable to\napproximate the class distribution of the unlabeled data from that of the\nlabeled data according to Monte Carlo Sampling. Guided by this weakly\nsupervision cue, we introduce a re-distribution mean teacher, which leverages\nadaptive label-distribution-aware confidence thresholds to generate unbiased\npseudo labels to drive student learning. For the latter one, there exists an\noverlooked label assignment ambiguity problem across teacher-student models. To\nremedy this issue, we present a novel label assignment mechanism for\nself-training framework, namely proposal self-assignment, which injects the\nproposals from student into teacher and generates accurate pseudo labels to\nmatch each proposal in the student model accordingly. Experiments on both\nMS-COCO and PASCAL-VOC datasets demonstrate the considerable superiority of our\nproposed framework to other state-of-the-arts. Code will be available at\nhttps://github.com/hikvision-research/SSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Binbin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shicai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Y/0/1/0/all/0/1\">Yunyi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer. (arXiv:2206.06619v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06619","description":"<p>In this work, we explore neat yet effective Transformer-based frameworks for\nvisual grounding. The previous methods generally address the core problem of\nvisual grounding, i.e., multi-modal fusion and reasoning, with\nmanually-designed mechanisms. Such heuristic designs are not only complicated\nbut also make models easily overfit specific data distributions. To avoid this,\nwe first propose TransVG, which establishes multi-modal correspondences by\nTransformers and localizes referred regions by directly regressing box\ncoordinates. We empirically show that complicated fusion modules can be\nreplaced by a simple stack of Transformer encoder layers with higher\nperformance. However, the core fusion Transformer in TransVG is stand-alone\nagainst uni-modal encoders, and thus should be trained from scratch on limited\nvisual grounding data, which makes it hard to be optimized and leads to\nsub-optimal performance. To this end, we further introduce TransVG++ to make\ntwo-fold improvements. For one thing, we upgrade our framework to a purely\nTransformer-based one by leveraging Vision Transformer (ViT) for vision feature\nencoding. For another, we devise Language Conditioned Vision Transformer that\nremoves external fusion modules and reuses the uni-modal ViT for\nvision-language fusion at the intermediate layers. We conduct extensive\nexperiments on five prevalent datasets, and report a series of state-of-the-art\nrecords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slimmable Domain Adaptation. (arXiv:2206.06620v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06620","description":"<p>Vanilla unsupervised domain adaptation methods tend to optimize the model\nwith fixed neural architecture, which is not very practical in real-world\nscenarios since the target data is usually processed by different\nresource-limited devices. It is therefore of great necessity to facilitate\narchitecture adaptation across various devices. In this paper, we introduce a\nsimple framework, Slimmable Domain Adaptation, to improve cross-domain\ngeneralization with a weight-sharing model bank, from which models of different\ncapacities can be sampled to accommodate different accuracy-efficiency\ntrade-offs. The main challenge in this framework lies in simultaneously\nboosting the adaptation performance of numerous models in the model bank. To\ntackle this problem, we develop a Stochastic EnsEmble Distillation method to\nfully exploit the complementary knowledge in the model bank for inter-model\ninteraction. Nevertheless, considering the optimization conflict between\ninter-model interaction and intra-model adaptation, we augment the existing\nbi-classifier domain confusion architecture into an Optimization-Separated\nTri-Classifier counterpart. After optimizing the model bank, architecture\nadaptation is leveraged via our proposed Unsupervised Performance Evaluation\nMetric. Under various resource constraints, our framework surpasses other\ncompeting approaches by a very large margin on multiple benchmarks. It is also\nworth emphasizing that our framework can preserve the performance improvement\nagainst the source-only model even when the computing complexity is reduced to\n$1/64$. Code will be available at https://github.com/hikvision-research/SlimDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shicai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Luojun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ULTRA: Uncertainty-aware Label Distribution Learning for Breast Tumor Cellularity Assessment. (arXiv:2206.06623v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06623","description":"<p>Neoadjuvant therapy (NAT) for breast cancer is a common treatment option in\nclinical practice. Tumor cellularity (TC), which represents the percentage of\ninvasive tumors in the tumor bed, has been widely used to quantify the response\nof breast cancer to NAT. Therefore, automatic TC estimation is significant in\nclinical practice. However, existing state-of-the-art methods usually take it\nas a TC score regression problem, which ignores the ambiguity of TC labels\ncaused by subjective assessment or multiple raters. In this paper, to\nefficiently leverage the label ambiguities, we proposed an Uncertainty-aware\nLabel disTRibution leArning (ULTRA) framework for automatic TC estimation. The\nproposed ULTRA first converted the single-value TC labels to discrete label\ndistributions, which effectively models the ambiguity among all possible TC\nlabels. Furthermore, the network learned TC label distributions by minimizing\nthe Kullback-Leibler (KL) divergence between the predicted and ground-truth TC\nlabel distributions, which better supervised the model to leverage the\nambiguity of TC labels. Moreover, the ULTRA mimicked the multi-rater fusion\nprocess in clinical practice with a multi-branch feature fusion module to\nfurther explore the uncertainties of TC labels. We evaluated the ULTRA on the\npublic BreastPathQ dataset. The experimental results demonstrate that the ULTRA\noutperformed the regression-based methods for a large margin and achieved\nstate-of-the-art results. The code will be available from\nhttps://github.com/PerceptionComputingLab/ULTRA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiangyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_X/0/1/0/all/0/1\">Xinjie Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gongning Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuanquan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks. (arXiv:2206.06637v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06637","description":"<p>Temporal/spatial receptive fields of models play an important role in\nsequential/spatial tasks. Large receptive fields facilitate long-term\nrelations, while small receptive fields help to capture the local details.\nExisting methods construct models with hand-designed receptive fields in\nlayers. Can we effectively search for receptive field combinations to replace\nhand-designed patterns? To answer this question, we propose to find better\nreceptive field combinations through a global-to-local search scheme. Our\nsearch scheme exploits both global search to find the coarse combinations and\nlocal search to get the refined receptive field combinations further. The\nglobal search finds possible coarse combinations other than human-designed\npatterns. On top of the global search, we propose an expectation-guided\niterative local search scheme to refine combinations effectively. Our RF-Next\nmodels, plugging receptive field search to various models, boost the\nperformance on many tasks, e.g., temporal action segmentation, object\ndetection, instance segmentation, and speech synthesis. The source code is\npublicly available on <a href=\"http://mmcheng.net/rfnext.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shanghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong-Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Score for Source-Free Unsupervised Domain Adaptation. (arXiv:2206.06640v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06640","description":"<p>Source-free unsupervised domain adaptation (SFUDA) aims to obtain high\nperformance in the unlabeled target domain using the pre-trained source model,\nnot the source data. Existing SFUDA methods assign the same importance to all\ntarget samples, which is vulnerable to incorrect pseudo-labels. To\ndifferentiate between sample importance, in this study, we propose a novel\nsample-wise confidence score, the Joint Model-Data Structure (JMDS) score for\nSFUDA. Unlike existing confidence scores that use only one of the source or\ntarget domain knowledge, the JMDS score uses both knowledge. We then propose a\nConfidence score Weighting Adaptation using the JMDS (CoWA-JMDS) framework for\nSFUDA. CoWA-JMDS consists of the JMDS scores as sample weights and weight Mixup\nthat is our proposed variant of Mixup. Weight Mixup promotes the model make\nmore use of the target domain knowledge. The experimental results show that the\nJMDS score outperforms the existing confidence scores. Moreover, CoWA-JMDS\nachieves state-of-the-art performance on various SFUDA scenarios: closed, open,\nand partial-set scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jonghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dahuin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Junho Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Kidneys Are Not All Normal: Investigating the Speckle Distributions of Transplanted Kidneys. (arXiv:2206.06654v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06654","description":"<p>Modelling ultrasound speckle has generated considerable interest for its\nability to characterize tissue properties. As speckle is dependent on the\nunderlying tissue architecture, modelling it may aid in tasks like segmentation\nor disease detection. However, for the transplanted kidney where ultrasound is\ncommonly used to investigate dysfunction, it is currently unknown which\nstatistical distribution best characterises such speckle. This is especially\ntrue for the regions of the transplanted kidney: the cortex, the medulla and\nthe central echogenic complex. Furthermore, it is unclear how these\ndistributions vary by patient variables such as age, sex, body mass index,\nprimary disease, or donor type. These traits may influence speckle modelling\ngiven their influence on kidney anatomy. We are the first to investigate these\ntwo aims. N=821 kidney transplant recipient B-mode images were automatically\nsegmented into the cortex, medulla, and central echogenic complex using a\nneural network. Seven distinct probability distributions were fitted to each\nregion. The Rayleigh and Nakagami distributions had model parameters that\ndiffered significantly between the three regions (p &lt;= 0.05). While both had\nexcellent goodness of fit, the Nakagami had higher Kullbeck-Leibler divergence.\nRecipient age correlated weakly with scale in the cortex (Omega: rho = 0.11, p\n= 0.004), while body mass index correlated weakly with shape in the medulla (m:\nrho = 0.08, p = 0.04). Neither sex, primary disease, nor donor type\ndemonstrated any correlation. We propose the Nakagami distribution be used to\ncharacterize transplanted kidneys regionally independent of disease etiology\nand most patient characteristics based on our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_R/0/1/0/all/0/1\">Rohit Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_R/0/1/0/all/0/1\">Ricky Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ringstrom_C/0/1/0/all/0/1\">Cailin Ringstrom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lessoway_V/0/1/0/all/0/1\">Victoria Lessoway</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reid_J/0/1/0/all/0/1\">Janice Reid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguan_C/0/1/0/all/0/1\">Christopher Nguan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohling_R/0/1/0/all/0/1\">Robert Rohling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Open Kidney Ultrasound Data Set. (arXiv:2206.06657v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06657","description":"<p>Ultrasound use is because of its low cost, non-ionizing, and non-invasive\ncharacteristics, and has established itself as a cornerstone radiological\nexamination. Research on ultrasound applications has also expanded, especially\nwith image analysis with machine learning. However, ultrasound data are\nfrequently restricted to closed data sets, with only a few openly available.\nDespite being a frequently examined organ, the kidney lacks a publicly\navailable ultrasonography data set. The proposed Open Kidney Ultrasound Data\nSet is the first publicly available set of kidney B-mode ultrasound data that\nincludes annotations for multi-class semantic segmentation. It is based on data\nretrospectively collected in a 5-year period from over 500 patients with a mean\nage of 53.2 +/- 14.7 years, body mass index of 27.0 +/- 5.4 kg/m2, and most\ncommon primary diseases being diabetes mellitus, IgA nephropathy, and\nhypertension. There are labels for the view and fine-grained manual annotations\nfrom two expert sonographers. Notably, this data includes native and\ntransplanted kidneys. Initial benchmarking measurements are performed,\ndemonstrating a state-of-the-art algorithm achieving a Dice Sorenson\nCoefficient of 0.74 for the kidney capsule. This data set is a high-quality\ndata set, including two sets of expert annotations, with a larger breadth of\nimages than previously available. In increasing access to kidney ultrasound\ndata, future researchers may be able to create novel image analysis techniques\nfor tissue characterization, disease detection, and prognostication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_R/0/1/0/all/0/1\">Rohit Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ringstrom_C/0/1/0/all/0/1\">Cailin Ringstrom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_G/0/1/0/all/0/1\">Grace Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lessoway_V/0/1/0/all/0/1\">Victoria Lessoway</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reid_J/0/1/0/all/0/1\">Janice Reid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguan_C/0/1/0/all/0/1\">Christopher Nguan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohling_R/0/1/0/all/0/1\">Robert Rohling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Best Combination for Efficient N:M Sparsity. (arXiv:2206.06662v1 [cs.LG])","link":"http://arxiv.org/abs/2206.06662","description":"<p>By forcing at most N out of M consecutive weights to be non-zero, the recent\nN:M network sparsity has received increasing attention for its two attractive\nadvantages: 1) Promising performance at a high sparsity. 2) Significant\nspeedups on NVIDIA A100 GPUs. Recent studies require an expensive pre-training\nphase or a heavy dense-gradient computation. In this paper, we show that the\nN:M learning can be naturally characterized as a combinatorial problem which\nsearches for the best combination candidate within a finite collection.\nMotivated by this characteristic, we solve N:M sparsity in an efficient\ndivide-and-conquer manner. First, we divide the weight vector into\n$C_{\\text{M}}^{\\text{N}}$ combination subsets of a fixed size N. Then, we\nconquer the combinatorial problem by assigning each combination a learnable\nscore that is jointly optimized with its associate weights. We prove that the\nintroduced scoring mechanism can well model the relative importance between\ncombination subsets. And by gradually removing low-scored subsets, N:M\nfine-grained sparsity can be efficiently optimized during the normal training\nphase. Comprehensive experiments demonstrate that our learning best combination\n(LBC) performs consistently better than off-the-shelf N:M sparsity methods\nacross various networks. Our code is released at\n\\url{https://github.com/zyxxmu/LBC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhihang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yiting Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantitative Imaging Principles Improves Medical Image Learning. (arXiv:2206.06663v1 [q-bio.QM])","link":"http://arxiv.org/abs/2206.06663","description":"<p>Fundamental differences between natural and medical images have recently\nfavored the use of self-supervised learning (SSL) over ImageNet transfer\nlearning for medical image applications. Differences between image types are\nprimarily due to the imaging modality and medical images utilize a wide range\nof physics based techniques while natural images are captured using only\nvisible light. While many have demonstrated that SSL on medical images has\nresulted in better downstream task performance, our work suggests that more\nperformance can be gained. The scientific principles which are used to acquire\nmedical images are not often considered when constructing learning problems.\nFor this reason, we propose incorporating quantitative imaging principles\nduring generative SSL to improve image quality and quantitative biological\naccuracy. We show that this training schema results in better starting states\nfor downstream supervised training on limited data. Our model also generates\nimages that validate on clinical quantitative analysis software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Leong_L/0/1/0/all/0/1\">Lambert T. Leong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wong_M/0/1/0/all/0/1\">Michael C. Wong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Glaser_Y/0/1/0/all/0/1\">Yannik Glaser</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wolfgruber_T/0/1/0/all/0/1\">Thomas Wolfgruber</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Heymsfield_S/0/1/0/all/0/1\">Steven B. Heymsfield</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sadwoski_P/0/1/0/all/0/1\">Peter Sadwoski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shepherd_J/0/1/0/all/0/1\">John A. Shepherd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images. (arXiv:2206.06665v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06665","description":"<p>Developing an AI-assisted gland segmentation method from histology images is\ncritical for automatic cancer diagnosis and prognosis; however, the high cost\nof pixel-level annotations hinders its applications to broader diseases.\nExisting weakly-supervised semantic segmentation methods in computer vision\nachieve degenerative results for gland segmentation, since the characteristics\nand problems of glandular datasets are different from general object datasets.\nWe observe that, unlike natural images, the key problem with histology images\nis the confusion of classes owning to morphological homogeneity and low color\ncontrast among different tissues. To this end, we propose a novel method Online\nEasy Example Mining (OEEM) that encourages the network to focus on credible\nsupervision signals rather than noisy signals, therefore mitigating the\ninfluence of inevitable false predictions in pseudo-masks. According to the\ncharacteristics of glandular datasets, we design a strong framework for gland\nsegmentation. Our results exceed many fully-supervised methods and\nweakly-supervised methods for gland segmentation over 4.4% and 6.04% at mIoU,\nrespectively. Code is available at https://github.com/xmed-lab/OEEM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yiduo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yiwen Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tianqi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISLES 2022: A multi-center magnetic resonance imaging stroke lesion segmentation dataset. (arXiv:2206.06694v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06694","description":"<p>Magnetic resonance imaging (MRI) is a central modality for stroke imaging. It\nis used upon patient admission to make treatment decisions such as selecting\npatients for intravenous thrombolysis or endovascular therapy. MRI is later\nused in the duration of hospital stay to predict outcome by visualizing infarct\ncore size and location. Furthermore, it may be used to characterize stroke\netiology, e.g. differentiation between (cardio)-embolic and non-embolic stroke.\nComputer based automated medical image processing is increasingly finding its\nway into clinical routine. Previous iterations of the Ischemic Stroke Lesion\nSegmentation (ISLES) challenge have aided in the generation of identifying\nbenchmark methods for acute and sub-acute ischemic stroke lesion segmentation.\nHere we introduce an expert-annotated, multicenter MRI dataset for segmentation\nof acute to subacute stroke lesions. This dataset comprises 400 multi-vendor\nMRI cases with high variability in stroke lesion size, quantity and location.\nIt is split into a training dataset of n=250 and a test dataset of n=150. All\ntraining data will be made publicly available. The test dataset will be used\nfor model validation only and will not be released to the public. This dataset\nserves as the foundation of the ISLES 2022 challenge with the goal of finding\nalgorithmic methods to enable the development and benchmarking of robust and\naccurate segmentation algorithms for ischemic stroke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petzsche_M/0/1/0/all/0/1\">Moritz Roman Hernandez Petzsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1\">Ezequiel de la Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanning_U/0/1/0/all/0/1\">Uta Hanning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiest_R/0/1/0/all/0/1\">Roland Wiest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinilla_W/0/1/0/all/0/1\">Waldo Enrique Valenzuela Pinilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">Mauricio Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_M/0/1/0/all/0/1\">Maria Ines Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1\">Sook-Lei Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kofler_F/0/1/0/all/0/1\">Florian Kofler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robben_D/0/1/0/all/0/1\">David Robben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutton_A/0/1/0/all/0/1\">Alexander Hutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_T/0/1/0/all/0/1\">Tassilo Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarth_T/0/1/0/all/0/1\">Teresa Zarth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burkle_J/0/1/0/all/0/1\">Johannes B&#xfc;rkle</a>, The <a href=\"http://arxiv.org/find/cs/1/au:+Baran_A/0/1/0/all/0/1\">Anh Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broocks_G/0/1/0/all/0/1\">Gabriel Broocks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_L/0/1/0/all/0/1\">Lukas Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_C/0/1/0/all/0/1\">Claus Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boeckh_Behrens_T/0/1/0/all/0/1\">Tobias Boeckh-Behrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berndt_M/0/1/0/all/0/1\">Maria Berndt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikenberg_B/0/1/0/all/0/1\">Benno Ikenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1\">Jan S. Kirschke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-based Classification Framework for Tissues of Lung with Additional Information. (arXiv:2206.06701v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06701","description":"<p>Interstitial lung diseases are a large group of heterogeneous diseases\ncharacterized by different degrees of alveolitis and pulmonary fibrosis.\nAccurately diagnosing these diseases has significant guiding value for\nformulating treatment plans. Although previous work has produced impressive\nresults in classifying interstitial lung diseases, there is still room for\nimproving the accuracy of these techniques, mainly to enhance automated\ndecision-making. In order to improve the classification precision, our study\nproposes a convolutional neural networks-based framework with additional\ninformation. Firstly, ILD images are added with their medical information by\nre-scaling the original image in Hounsfield Units. Secondly, a modified CNN\nmodel is used to produce a vector of classification probability for each\ntissue. Thirdly, location information of the input image, consisting of the\noccurrence frequencies of different diseases in the CT scans on certain\nlocations, is used to calculate a location weight vector. Finally, the Hadamard\nproduct between two vectors is used to produce a decision vector for the\nprediction. Compared to the state-of-the-art methods, the results using a\npublicly available ILD database show the potential of predicting these using\ndifferent additional information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Huafeng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_R/0/1/0/all/0/1\">Ruijie Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiyagalingam_J/0/1/0/all/0/1\">Jeyarajan Thiyagalingam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coenen_F/0/1/0/all/0/1\">Frans Coenen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Radial Basis Q-Network. (arXiv:2206.06712v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06712","description":"<p>While reinforcement learning (RL) from raw images has been largely\ninvestigated in the last decade, existing approaches still suffer from a number\nof constraints. The high input dimension is often handled using either expert\nknowledge to extract handcrafted features or environment encoding through\nconvolutional networks. Both solutions require numerous parameters to be\noptimized. In contrast, we propose a generic method to extract sparse features\nfrom raw images with few trainable parameters. We achieved this using a Radial\nBasis Function Network (RBFN) directly on raw image. We evaluate the\nperformance of the proposed approach for visual extraction in Q-learning tasks\nin the Vizdoom environment. Then, we compare our results with two Deep\nQ-Network, one trained directly on images and another one trained on feature\nextracted by a pretrained auto-encoder. We show that the proposed approach\nprovides similar or, in some cases, even better performances with fewer\ntrainable parameters while being conceptually simpler.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hautot_J/0/1/0/all/0/1\">Julien Hautot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teuliere_C/0/1/0/all/0/1\">C&#xe9;line Teuliere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azzaoui_N/0/1/0/all/0/1\">Nourddine Azzaoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Gait Recognition by Granger Causality. (arXiv:2206.06714v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06714","description":"<p>Which joint interactions in the human gait cycle can be used as biometric\ncharacteristics? Most current methods on gait recognition suffer from the lack\nof interpretability. We propose an interpretable feature representation of gait\nsequences by the graphical Granger causal inference. Gait sequence of a person\nin the standardized motion capture format, constituting a set of 3D joint\nspatial trajectories, is envisaged as a causal system of joints interacting in\ntime. We apply the graphical Granger model (GGM) to obtain the so-called\nGranger causal graph among joints as a discriminative and visually\ninterpretable representation of a person's gait. We evaluate eleven distance\nfunctions in the GGM feature space by established classification and\nclass-separability evaluation metrics. Our experiments indicate that, depending\non the metric, the most appropriate distance functions for the GGM are the\ntotal norm distance and the Ky-Fan 1-norm distance. Experiments also show that\nthe GGM is able to detect the most discriminative joint interactions and that\nit outperforms five related interpretable models in correct classification rate\nand in Davies-Bouldin index. The proposed GGM model can serve as a\ncomplementary tool for gait analysis in kinesiology or for gait recognition in\nvideo surveillance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balazia_M/0/1/0/all/0/1\">Michal Balazia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hlavackova_Schindler_K/0/1/0/all/0/1\">Katerina Hlavackova-Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plant_C/0/1/0/all/0/1\">Claudia Plant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-signed neural fitting for surface reconstruction from unoriented point clouds. (arXiv:2206.06715v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06715","description":"<p>Reconstructing 3D geometry from \\emph{unoriented} point clouds can benefit\nmany downstream tasks. Recent methods mostly adopt a neural shape\nrepresentation with a neural network to represent a signed distance field and\nfit the point cloud with an unsigned supervision. However, we observe that\nusing unsigned supervision may cause severe ambiguities and often leads to\n\\emph{unexpected} failures such as generating undesired surfaces in free space\nwhen reconstructing complex structures and struggle with reconstructing\naccurate surfaces. To reconstruct a better signed distance field, we propose\nsemi-signed neural fitting (SSN-Fitting), which consists of a semi-signed\nsupervision and a loss-based region sampling strategy. Our key insight is that\nsigned supervision is more informative and regions that are obviously outside\nthe object can be easily determined. Meanwhile, a novel importance sampling is\nproposed to accelerate the optimization and better reconstruct the fine\ndetails. Specifically, we voxelize and partition the object space into\n\\emph{sign-known} and \\emph{sign-uncertain} regions, in which different\nsupervisions are applied. Also, we adaptively adjust the sampling rate of each\nvoxel according to the tracked reconstruction loss, so that the network can\nfocus more on the complex under-fitting regions. We conduct extensive\nexperiments to demonstrate that SSN-Fitting achieves state-of-the-art\nperformance under different settings on multiple datasets, including clean,\ndensity-varying, and noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Runsong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Ka-Hei Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yue Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated SSIM Regression for Detection and Quantification of Motion Artefacts in Brain MR Images. (arXiv:2206.06725v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06725","description":"<p>Motion artefacts in magnetic resonance brain images are a crucial issue. The\nassessment of MR image quality is fundamental before proceeding with the\nclinical diagnosis. If the motion artefacts alter a correct delineation of\nstructure and substructures of the brain, lesions, tumours and so on, the\npatients need to be re-scanned. Otherwise, neuro-radiologists could report an\ninaccurate or incorrect diagnosis. The first step right after scanning a\npatient is the \"\\textit{image quality assessment}\" in order to decide if the\nacquired images are diagnostically acceptable. An automated image quality\nassessment based on the structural similarity index (SSIM) regression through a\nresidual neural network has been proposed here, with the possibility to perform\nalso the classification in different groups - by subdividing with SSIM ranges.\nThis method predicts SSIM values of an input image in the absence of a\nreference ground truth image. The networks were able to detect motion\nartefacts, and the best performance for the regression and classification task\nhas always been achieved with ResNet-18 with contrast augmentation. Mean and\nstandard deviation of residuals' distribution were $\\mu=-0.0009$ and\n$\\sigma=0.0139$, respectively. Whilst for the classification task in 3, 5 and\n10 classes, the best accuracies were 97, 95 and 89\\%, respectively. The\nobtained results show that the proposed method could be a tool in supporting\nneuro-radiologists and radiographers in evaluating the image quality before the\ndiagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sciarra_A/0/1/0/all/0/1\">Alessandro Sciarra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dunnwald_M/0/1/0/all/0/1\">Max D&#xfc;nnwald</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Placidi_G/0/1/0/all/0/1\">Giuseppe Placidi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oeltze_Jafra_S/0/1/0/all/0/1\">Steffen Oeltze-Jafra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Precision Localization of Peripherally Inserted Central Catheter Tip through Model-Agnostic Multi-Stage Networks. (arXiv:2206.06730v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06730","description":"<p>Peripherally inserted central catheters (PICCs) have been widely used as one\nof the representative central venous lines (CVCs) due to their long-term\nintravascular access with low infectivity. However, PICCs have a fatal drawback\nof a high frequency of tip mispositions, increasing the risk of puncture,\nembolism, and complications such as cardiac arrhythmias. To automatically and\nprecisely detect it, various attempts have been made by using the latest deep\nlearning (DL) technologies. However, even with these approaches, it is still\npractically difficult to determine the tip location because the multiple\nfragments phenomenon (MFP) occurs in the process of predicting and extracting\nthe PICC line required before predicting the tip. This study aimed to develop a\nsystem generally applied to existing models and to restore the PICC line more\nexactly by removing the MFs of the model output, thereby precisely localizing\nthe actual tip position for detecting its disposition. To achieve this, we\nproposed a multi-stage DL-based framework post-processing the PICC line\nextraction result of the existing technology. The performance was compared by\neach root mean squared error (RMSE) and MFP incidence rate according to whether\nor not MFCN is applied to five conventional models. In internal validation,\nwhen MFCN was applied to the existing single model, MFP was improved by an\naverage of 45%. The RMSE was improved by over 63% from an average of 26.85mm\n(17.16 to 35.80mm) to 9.72mm (9.37 to 10.98mm). In external validation, when\nMFCN was applied, the MFP incidence rate decreased by an average of 32% and the\nRMSE decreased by an average of 65\\%. Therefore, by applying the proposed MFCN,\nwe observed the significant/consistent detection performance improvement of\nPICC tip location compared to the existing model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Subin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cha_Y/0/1/0/all/0/1\">Yoon Ki Cha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Soyoung Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Su Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_M/0/1/0/all/0/1\">Myung Jin Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dense Features for Point Cloud Registration Using Graph Attention Network. (arXiv:2206.06731v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06731","description":"<p>Point cloud registration is a fundamental task in many applications such as\nlocalization, mapping, tracking, and reconstruction. The successful\nregistration relies on extracting robust and discriminative geometric features.\nExisting learning-based methods require high computing capacity for processing\na large number of raw points at the same time. Although these approaches\nachieve convincing results, they are difficult to apply in real-world\nsituations due to high computational costs. In this paper, we introduce a\nframework that efficiently and economically extracts dense features using graph\nattention network for point cloud matching and registration (DFGAT). The\ndetector of the DFGAT is responsible for finding highly reliable key points in\nlarge raw data sets. The descriptor of the DFGAT takes these key points\ncombined with their neighbors to extract invariant density features in\npreparation for the matching. The graph attention network uses the attention\nmechanism that enriches the relationships between point clouds. Finally, we\nconsider this as an optimal transport problem and use the Sinkhorn algorithm to\nfind positive and negative matches. We perform thorough tests on the KITTI\ndataset and evaluate the effectiveness of this approach. The results show that\nthis method with the efficiently compact keypoint selection and description can\nachieve the best performance matching metrics and reach highest success ratio\nof 99.88% registration in comparison with other state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vinh_L/0/1/0/all/0/1\">Lai Dang Quoc Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nengroo_S/0/1/0/all/0/1\">Sarvar Hussain Nengroo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hojun Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Vulnerability of Randomized Ensembles. (arXiv:2206.06737v1 [cs.LG])","link":"http://arxiv.org/abs/2206.06737","description":"<p>Despite the tremendous success of deep neural networks across various tasks,\ntheir vulnerability to imperceptible adversarial perturbations has hindered\ntheir deployment in the real world. Recently, works on randomized ensembles\nhave empirically demonstrated significant improvements in adversarial\nrobustness over standard adversarially trained (AT) models with minimal\ncomputational overhead, making them a promising solution for safety-critical\nresource-constrained applications. However, this impressive performance raises\nthe question: Are these robustness gains provided by randomized ensembles real?\nIn this work we address this question both theoretically and empirically. We\nfirst establish theoretically that commonly employed robustness evaluation\nmethods such as adaptive PGD provide a false sense of security in this setting.\nSubsequently, we propose a theoretically-sound and efficient adversarial attack\nalgorithm (ARC) capable of compromising random ensembles even in cases where\nadaptive PGD fails to do so. We conduct comprehensive experiments across a\nvariety of network architectures, training schemes, datasets, and norms to\nsupport our claims, and empirically establish that randomized ensembles are in\nfact more vulnerable to $\\ell_p$-bounded adversarial perturbations than even\nstandard AT models. Our code can be found at https://github.com/hsndbk4/ARC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dbouk_H/0/1/0/all/0/1\">Hassan Dbouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanbhag_N/0/1/0/all/0/1\">Naresh R. Shanbhag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Transformer Variational Autoencoders for Multi-Action Motion Synthesis. (arXiv:2206.06741v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06741","description":"<p>We consider the problem of synthesizing multi-action human motion sequences\nof arbitrary lengths. Existing approaches have mastered motion sequence\ngeneration in single-action scenarios, but fail to generalize to multi-action\nand arbitrary-length sequences. We fill this gap by proposing a novel efficient\napproach that leverages the expressiveness of Recurrent Transformers and\ngenerative richness of conditional Variational Autoencoders. The proposed\niterative approach is able to generate smooth and realistic human motion\nsequences with an arbitrary number of actions and frames while doing so in\nlinear space and time. We train and evaluate the proposed approach on PROX\ndataset which we augment with ground-truth action labels. Experimental\nevaluation shows significant improvements in FID score and semantic consistency\nmetrics compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Briq_R/0/1/0/all/0/1\">Rania Briq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1\">Chuhang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pishchulin_L/0/1/0/all/0/1\">Leonid Pishchulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broaddus_C/0/1/0/all/0/1\">Chris Broaddus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Crack Detection. (arXiv:2206.06743v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06743","description":"<p>Pixel-level crack segmentation is widely studied due to its high impact on\nbuilding and road inspections. Recent studies have made significant\nimprovements in accuracy, but overlooked the annotation cost bottleneck. To\nresolve this issue, we reformulate the crack segmentation problem as a\nweakly-supervised problem, and propose a two-branched inference framework and\nan annotation refinement module that requires no additional data, in order to\ncounteract the loss in annotation quality. Experimental results confirm the\neffectiveness of the proposed method in crack segmentation as well as other\ntarget domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_Y/0/1/0/all/0/1\">Yuki Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagayoshi_H/0/1/0/all/0/1\">Hiroto Nagayoshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO. (arXiv:2206.06761v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06761","description":"<p>This work conducts the first analysis on the robustness against adversarial\nattacks on self-supervised Vision Transformers trained using DINO. First, we\nevaluate whether features learned through self-supervision are more robust to\nadversarial attacks than those emerging from supervised learning. Then, we\npresent properties arising for attacks in the latent space. Finally, we\nevaluate whether three well-known defense strategies can increase adversarial\nrobustness in downstream tasks by only fine-tuning the classification head to\nprovide robustness even in view of limited compute resources. These defense\nstrategies are: Adversarial Training, Ensemble Adversarial Training and\nEnsemble of Specialized Networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1\">Javier Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naimi_N/0/1/0/all/0/1\">Nasib Naimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumann_T/0/1/0/all/0/1\">Thomas Baumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathys_M/0/1/0/all/0/1\">Max Mathys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Peripheral Vision Transformer. (arXiv:2206.06801v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06801","description":"<p>Human vision possesses a special type of visual processing systems called\nperipheral vision. Partitioning the entire visual field into multiple contour\nregions based on the distance to the center of our gaze, the peripheral vision\nprovides us the ability to perceive various visual features at different\nregions. In this work, we take a biologically inspired approach and explore to\nmodel peripheral vision in deep neural networks for visual recognition. We\npropose to incorporate peripheral position encoding to the multi-head\nself-attention layers to let the network learn to partition the visual field\ninto diverse peripheral regions given training data. We evaluate the proposed\nnetwork, dubbed PerViT, on the large-scale ImageNet dataset and systematically\ninvestigate the inner workings of the model for machine perception, showing\nthat the network learns to perceive visual data similarly to the way that human\nvision does. The state-of-the-art performance in image classification task\nacross various model sizes demonstrates the efficacy of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1\">Juhong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yucheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal. (arXiv:2206.06803v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06803","description":"<p>This work studies the joint rain and haze removal problem. In real-life\nscenarios, rain and haze, two often co-occurring common weather phenomena, can\ngreatly degrade the clarity and quality of the scene images, leading to a\nperformance drop in the visual applications, such as autonomous driving.\nHowever, jointly removing the rain and haze in scene images is ill-posed and\nchallenging, where the existence of haze and rain and the change of atmosphere\nlight, can both degrade the scene information. Current methods focus on the\ncontamination removal part, thus ignoring the restoration of the scene\ninformation affected by the change of atmospheric light. We propose a novel\ndeep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address\nthe aforementioned challenge. The ADU-Net produces both the contamination\nresidual and the scene residual to efficiently remove the rain and haze while\npreserving the fidelity of the scene information. Extensive experiments show\nour work outperforms the existing state-of-the-art methods by a considerable\nmargin in both synthetic data and real-world data benchmarks, including\nRainCityscapes, BID Rain, and SPA-Data. For instance, we improve the\nstate-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data,\nrespectively.\n</p>\n<p>Codes will be made available freely to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaojun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shengyong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites. (arXiv:2206.06813v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06813","description":"<p>In clinical practice, a segmentation network is often required to continually\nlearn on a sequential data stream from multiple sites rather than a\nconsolidated set, due to the storage cost and privacy restriction. However,\nduring the continual learning process, existing methods are usually restricted\nin either network memorizability on previous sites or generalizability on\nunseen sites. This paper aims to tackle the challenging problem of Synchronous\nMemorizability and Generalizability (SMG) and to simultaneously improve\nperformance on both previous and unseen sites, with a novel proposed\nSMG-learning framework. First, we propose a Synchronous Gradient Alignment\n(SGA) objective, which \\emph{not only} promotes the network memorizability by\nenforcing coordinated optimization for a small exemplar set from previous sites\n(called replay buffer), \\emph{but also} enhances the generalizability by\nfacilitating site-invariance under simulated domain shift. Second, to simplify\nthe optimization of SGA objective, we design a Dual-Meta algorithm that\napproximates the SGA objective as dual meta-objectives for optimization without\nexpensive computation overhead. Third, for efficient rehearsal, we configure\nthe replay buffer comprehensively considering additional inter-site diversity\nto reduce redundancy. Experiments on prostate MRI data sequentially acquired\nfrom six institutes demonstrate that our method can simultaneously achieve\nhigher memorizability and generalizability over state-of-the-art methods. Code\nis available at https://github.com/jingyzhang/SMG-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_P/0/1/0/all/0/1\">Peng Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_R/0/1/0/all/0/1\">Ran Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yuning Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1\">Mianxin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yongsheng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiming Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiawei Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06829","description":"<p>Vision transformers (ViTs) are changing the landscape of object detection\napproaches. A natural usage of ViTs in detection is to replace the CNN-based\nbackbone with a transformer-based backbone, which is straightforward and\neffective, with the price of bringing considerable computation burden for\ninference. More subtle usage is the DETR family, which eliminates the need for\nmany hand-designed components in object detection but introduces a decoder\ndemanding an extra-long time to converge. As a result, transformer-based object\ndetection can not prevail in large-scale applications. To overcome these\nissues, we propose a novel decoder-free fully transformer-based (DFFT) object\ndetector, achieving high efficiency in both training and inference stages, for\nthe first time. We simplify objection detection into an encoder-only\nsingle-level anchor-based dense prediction problem by centering around two\nentry points: 1) Eliminate the training-inefficient decoder and leverage two\nstrong encoders to preserve the accuracy of single-level feature map\nprediction; 2) Explore low-level semantic features for the detection task with\nlimited computational resources. In particular, we design a novel lightweight\ndetection-oriented transformer backbone that efficiently captures low-level\nfeatures with rich semantics based on a well-conceived ablation study.\nExtensive experiments on the MS COCO benchmark demonstrate that DFFT_SMALL\noutperforms DETR by 2.5% AP with 28% computation cost reduction and more than\n$10\\times$ fewer training epochs. Compared with the cutting-edge anchor-based\ndetector RetinaNet, DFFT_SMALL obtains over 5.5% AP gain while cutting down 70%\ncomputation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peixian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a> (Tencent Youtu Lab)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When adversarial attacks become interpretable counterfactual explanations. (arXiv:2206.06854v1 [cs.AI])","link":"http://arxiv.org/abs/2206.06854","description":"<p>We argue that, when learning a 1-Lipschitz neural network with the dual loss\nof an optimal transportation problem, the gradient of the model is both the\ndirection of the transportation plan and the direction to the closest\nadversarial attack. Traveling along the gradient to the decision boundary is no\nmore an adversarial attack but becomes a counterfactual explanation, explicitly\ntransporting from one class to the other. Through extensive experiments on XAI\nmetrics, we find that the simple saliency map method, applied on such networks,\nbecomes a reliable explanation, and outperforms the state-of-the-art\nexplanation approaches on unconstrained models. The proposed networks were\nalready known to be certifiably robust, and we prove that they are also\nexplainable with a fast and simple method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serrurier_M/0/1/0/all/0/1\">Mathieu Serrurier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamalet_F/0/1/0/all/0/1\">Franck Mamalet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethune_L/0/1/0/all/0/1\">Louis B&#xe9;thune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boissin_T/0/1/0/all/0/1\">Thibaut Boissin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating histopathology transfer learning with ChampKit. (arXiv:2206.06862v1 [q-bio.QM])","link":"http://arxiv.org/abs/2206.06862","description":"<p>Histopathology remains the gold standard for diagnosis of various cancers.\nRecent advances in computer vision, specifically deep learning, have\nfacilitated the analysis of histopathology images for various tasks, including\nimmune cell detection and microsatellite instability classification. The\nstate-of-the-art for each task often employs base architectures that have been\npretrained for image classification on ImageNet. The standard approach to\ndevelop classifiers in histopathology tends to focus narrowly on optimizing\nmodels for a single task, not considering the aspects of modeling innovations\nthat improve generalization across tasks. Here we present ChampKit\n(Comprehensive Histopathology Assessment of Model Predictions toolKit): an\nextensible, fully reproducible benchmarking toolkit that consists of a broad\ncollection of patch-level image classification tasks across different cancers.\nChampKit enables a way to systematically document the performance impact of\nproposed improvements in models and methodology. ChampKit source code and data\nare freely accessible at https://github.com/kaczmarj/champkit .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Kaczmarzyk_J/0/1/0/all/0/1\">Jakub R. Kaczmarzyk</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin M. Kurc</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Abousamra_S/0/1/0/all/0/1\">Shahira Abousamra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_R/0/1/0/all/0/1\">Rajarsi Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saltz_J/0/1/0/all/0/1\">Joel H. Saltz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Koo_P/0/1/0/all/0/1\">Peter K. Koo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Scene Representation Transformer. (arXiv:2206.06922v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06922","description":"<p>A compositional understanding of the world in terms of objects and their\ngeometry in 3D space is considered a cornerstone of human cognition.\nFacilitating the learning of such a representation in neural networks holds\npromise for substantially improving labeled data efficiency. As a key step in\nthis direction, we make progress on the problem of learning 3D-consistent\ndecompositions of complex scenes into individual objects in an unsupervised\nfashion. We introduce Object Scene Representation Transformer (OSRT), a\n3D-centric model in which individual object representations naturally emerge\nthrough novel view synthesis. OSRT scales to significantly more complex scenes\nwith larger diversity of objects and backgrounds than existing methods. At the\nsame time, it is multiple orders of magnitude faster at compositional rendering\nthanks to its light field parametrization and the novel Slot Mixer decoder. We\nbelieve this work will not only accelerate future architecture exploration and\nscaling efforts, but it will also serve as a useful tool for both\nobject-centric as well as neural scene representation learning communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1\">Mehdi S. M. Sajjadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duckworth_D/0/1/0/all/0/1\">Daniel Duckworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendran_A/0/1/0/all/0/1\">Aravindh Mahendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenkiste_S/0/1/0/all/0/1\">Sjoerd van Steenkiste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavetic_F/0/1/0/all/0/1\">Filip Paveti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lu&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greff_K/0/1/0/all/0/1\">Klaus Greff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-task Framework for Infrared Small Target Detection and Segmentation. (arXiv:2206.06923v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06923","description":"<p>Due to the complicated background and noise of infrared images, infrared\nsmall target detection is one of the most difficult problems in the field of\ncomputer vision. In most existing studies, semantic segmentation methods are\ntypically used to achieve better results. The centroid of each target is\ncalculated from the segmentation map as the detection result. In contrast, we\npropose a novel end-to-end framework for infrared small target detection and\nsegmentation in this paper. First, with the use of UNet as the backbone to\nmaintain resolution and semantic information, our model can achieve a higher\ndetection accuracy than other state-of-the-art methods by attaching a simple\nanchor-free head. Then, a pyramid pool module is used to further extract\nfeatures and improve the precision of target segmentation. Next, we use\nsemantic segmentation tasks that pay more attention to pixel-level features to\nassist in the training process of object detection, which increases the average\nprecision and allows the model to detect some targets that were previously not\ndetectable. Furthermore, we develop a multi-task framework for infrared small\ntarget detection and segmentation. Our multi-task learning model reduces\ncomplexity by nearly half and speeds up inference by nearly twice compared to\nthe composite single-task model, while maintaining accuracy. The code and\nmodels are publicly available at https://github.com/Chenastron/MTUNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiaofeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fansheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehending and Ordering Semantics for Image Captioning. (arXiv:2206.06930v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06930","description":"<p>Comprehending the rich semantics in an image and ordering them in linguistic\norder are essential to compose a visually-grounded and linguistically coherent\ndescription for image captioning. Modern techniques commonly capitalize on a\npre-trained object detector/classifier to mine the semantics in an image, while\nleaving the inherent linguistic ordering of semantics under-exploited. In this\npaper, we propose a new recipe of Transformer-style structure, namely\nComprehending and Ordering Semantics Networks (COS-Net), that novelly unifies\nan enriched semantic comprehending and a learnable semantic ordering processes\ninto a single architecture. Technically, we initially utilize a cross-modal\nretrieval model to search the relevant sentences of each image, and all words\nin the searched sentences are taken as primary semantic cues. Next, a novel\nsemantic comprehender is devised to filter out the irrelevant semantic words in\nprimary semantic cues, and meanwhile infer the missing relevant semantic words\nvisually grounded in the image. After that, we feed all the screened and\nenriched semantic words into a semantic ranker, which learns to allocate all\nsemantic words in linguistic order as humans. Such sequence of ordered semantic\nwords are further integrated with visual tokens of images to trigger sentence\ngeneration. Empirical evidences show that COS-Net clearly surpasses the\nstate-of-the-art approaches on COCO and achieves to-date the best CIDEr score\nof 141.1% on Karpathy test split. Source code is available at\n\\url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stand-Alone Inter-Frame Attention in Video Models. (arXiv:2206.06931v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06931","description":"<p>Motion, as the uniqueness of a video, has been critical to the development of\nvideo understanding models. Modern deep learning models leverage motion by\neither executing spatio-temporal 3D convolutions, factorizing 3D convolutions\ninto spatial and temporal convolutions separately, or computing self-attention\nalong temporal dimension. The implicit assumption behind such successes is that\nthe feature maps across consecutive frames can be nicely aggregated.\nNevertheless, the assumption may not always hold especially for the regions\nwith large deformation. In this paper, we present a new recipe of inter-frame\nattention block, namely Stand-alone Inter-Frame Attention (SIFA), that novelly\ndelves into the deformation across frames to estimate local self-attention on\neach spatial location. Technically, SIFA remoulds the deformable design via\nre-scaling the offset predictions by the difference between two frames. Taking\neach spatial location in the current frame as the query, the locally deformable\nneighbors in the next frame are regarded as the keys/values. Then, SIFA\nmeasures the similarity between query and keys as stand-alone attention to\nweighted average the values for temporal aggregation. We further plug SIFA\nblock into ConvNets and Vision Transformer, respectively, to devise SIFA-Net\nand SIFA-Transformer. Extensive experiments conducted on four video datasets\ndemonstrate the superiority of SIFA-Net and SIFA-Transformer as stronger\nbackbones. More remarkably, SIFA-Transformer achieves an accuracy of 83.1% on\nKinetics-400 dataset. Source code is available at\n\\url{https://github.com/FuchenUSTC/SIFA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1\">Fuchen Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaofan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Space Transformer for Fast MRIReconstruction with Implicit Representation. (arXiv:2206.06947v1 [eess.IV])","link":"http://arxiv.org/abs/2206.06947","description":"<p>This paper considers the problem of fast MRI reconstruction. We propose a\nnovel Transformer-based framework for directly processing the sparsely sampled\nsignals in k-space, going beyond the limitation of regular grids as ConvNets\ndo. We adopt an implicit representation of spectrogram, treating spatial\ncoordinates as inputs, and dynamically query the partially observed\nmeasurements to complete the spectrogram, i.e. learning the inductive bias in\nk-space. To strive a balance between computational cost and reconstruction\nquality, we build an hierarchical structure with low-resolution and\nhigh-resolution decoders respectively. To validate the necessity of our\nproposed modules, we have conducted extensive experiments on two public\ndatasets, and demonstrate superior or comparable performance over\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring Urban Forests from Auto-Generated Segmentation Maps. (arXiv:2206.06948v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06948","description":"<p>We present and evaluate a weakly-supervised methodology to quantify the\nspatio-temporal distribution of urban forests based on remotely sensed data\nwith close-to-zero human interaction. Successfully training machine learning\nmodels for semantic segmentation typically depends on the availability of\nhigh-quality labels. We evaluate the benefit of high-resolution,\nthree-dimensional point cloud data (LiDAR) as source of noisy labels in order\nto train models for the localization of trees in orthophotos. As proof of\nconcept we sense Hurricane Sandy's impact on urban forests in Coney Island, New\nYork City (NYC) and reference it to less impacted urban space in Brooklyn, NYC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1\">Conrad M Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1\">Levente Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AuxMix: Semi-Supervised Learning with Unconstrained Unlabeled Data. (arXiv:2206.06959v1 [cs.CV])","link":"http://arxiv.org/abs/2206.06959","description":"<p>Semi-supervised learning (SSL) has seen great strides when labeled data is\nscarce but unlabeled data is abundant. Critically, most recent work assume that\nsuch unlabeled data is drawn from the same distribution as the labeled data. In\nthis work, we show that state-of-the-art SSL algorithms suffer a degradation in\nperformance in the presence of unlabeled auxiliary data that does not\nnecessarily possess the same class distribution as the labeled set. We term\nthis problem as Auxiliary-SSL and propose AuxMix, an algorithm that leverages\nself-supervised learning tasks to learn generic features in order to mask\nauxiliary data that are not semantically similar to the labeled set. We also\npropose to regularize learning by maximizing the predicted entropy for\ndissimilar auxiliary samples. We show an improvement of 5% over existing\nbaselines on a ResNet-50 model when trained on CIFAR10 dataset with 4k labeled\nsamples and all unlabeled data is drawn from the Tiny-ImageNet dataset. We\nreport competitive results on several datasets and conduct ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujjar_P/0/1/0/all/0/1\">Pratik Gujjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. (arXiv:2206.06994v1 [cs.AI])","link":"http://arxiv.org/abs/2206.06994","description":"<p>Massive datasets and high-capacity models have driven many recent\nadvancements in computer vision and natural language understanding. This work\npresents a platform to enable similar success stories in Embodied AI. We\npropose ProcTHOR, a framework for procedural generation of Embodied AI\nenvironments. ProcTHOR enables us to sample arbitrarily large datasets of\ndiverse, interactive, customizable, and performant virtual environments to\ntrain and evaluate embodied agents across navigation, interaction, and\nmanipulation tasks. We demonstrate the power and potential of ProcTHOR via a\nsample of 10,000 generated houses and a simple neural model. Models trained\nusing only RGB images on ProcTHOR, with no explicit mapping and no human task\nsupervision produce state-of-the-art results across 6 embodied AI benchmarks\nfor navigation, rearrangement, and arm manipulation, including the presently\nrunning Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges. We\nalso demonstrate strong 0-shot results on these benchmarks, via pre-training on\nProcTHOR with no fine-tuning on the downstream benchmark, often beating\nprevious state-of-the-art systems that access the downstream training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deitke_M/0/1/0/all/0/1\">Matt Deitke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanderBilt_E/0/1/0/all/0/1\">Eli VanderBilt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrasti_A/0/1/0/all/0/1\">Alvaro Herrasti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1\">Luca Weihs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvador_J/0/1/0/all/0/1\">Jordi Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Winson Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolve_E/0/1/0/all/0/1\">Eric Kolve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Video Instance Segmentation with Inter-Frame Recurrent Attention. (arXiv:2206.07011v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07011","description":"<p>Video instance segmentation aims at predicting object segmentation masks for\neach frame, as well as associating the instances across multiple frames. Recent\nend-to-end video instance segmentation methods are capable of performing object\nsegmentation and instance association together in a direct parallel sequence\ndecoding/prediction framework. Although these methods generally predict higher\nquality object segmentation masks, they can fail to associate instances in\nchallenging cases because they do not explicitly model the temporal instance\nconsistency for adjacent frames. We propose a consistent end-to-end video\ninstance segmentation framework with Inter-Frame Recurrent Attention to model\nboth the temporal instance consistency for adjacent frames and the global\ntemporal context. Our extensive experiments demonstrate that the Inter-Frame\nRecurrent Attention significantly improves temporal instance consistency while\nmaintaining the quality of the object segmentation masks. Our model achieves\nstate-of-the-art accuracy on both YouTubeVIS-2019 (62.1\\%) and YouTubeVIS-2021\n(54.7\\%) datasets. In addition, quantitative and qualitative results show that\nthe proposed methods predict more temporally consistent instance segmentation\nmasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1\">Quanzeng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_P/0/1/0/all/0/1\">Peng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrantes_A/0/1/0/all/0/1\">Andre Abrantes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning a Curse Into a Blessing: Enabling Clean-Data-Free Defenses by Model Inversion. (arXiv:2206.07018v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07018","description":"<p>It is becoming increasingly common to utilize pre-trained models provided by\nthird parties due to their convenience. At the same time, however, these models\nmay be vulnerable to both poisoning and evasion attacks. We introduce an\nalgorithmic framework that can mitigate potential security vulnerabilities in a\npre-trained model when clean data from its training distribution is unavailable\nto the defender. The framework reverse-engineers samples from a given\npre-trained model. The resulting synthetic samples can then be used as a\nsubstitute for clean data to perform various defenses. We consider two\nimportant attack scenarios -- backdoor attacks and evasion attacks -- to\nshowcase the utility of synthesized samples. For both attacks, we show that\nwhen supplied with our synthetic data, the state-of-the-art defenses perform\ncomparably or sometimes even better than the case when it's supplied with the\nsame amount of clean data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Won Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning 3D Object Shape and Layout without 3D Supervision. (arXiv:2206.07028v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07028","description":"<p>A 3D scene consists of a set of objects, each with a shape and a layout\ngiving their position in space. Understanding 3D scenes from 2D images is an\nimportant goal, with applications in robotics and graphics. While there have\nbeen recent advances in predicting 3D shape and layout from a single image,\nmost approaches rely on 3D ground truth for training which is expensive to\ncollect at scale. We overcome these limitations and propose a method that\nlearns to predict 3D shape and layout for objects without any ground truth\nshape or layout information: instead we rely on multi-view images with 2D\nsupervision which can more easily be collected at scale. Through extensive\nexperiments on 3D Warehouse, Hypersim, and ScanNet we demonstrate that our\napproach scales to large datasets of realistic images, and compares favorably\nto methods relying on 3D ground truth. On Hypersim and ScanNet where reliable\n3D ground truth is not available, our approach outperforms supervised\napproaches trained on smaller and less diverse datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gkioxari_G/0/1/0/all/0/1\">Georgia Gkioxari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1\">Nikhila Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate 3D Body Shape Regression using Metric and Semantic Attributes. (arXiv:2206.07036v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07036","description":"<p>While methods that regress 3D human meshes from images have progressed\nrapidly, the estimated body shapes often do not capture the true human shape.\nThis is problematic since, for many applications, accurate body shape is as\nimportant as pose. The key reason that body shape accuracy lags pose accuracy\nis the lack of data. While humans can label 2D joints, and these constrain 3D\npose, it is not so easy to \"label\" 3D body shape. Since paired data with images\nand 3D body shape are rare, we exploit two sources of information: (1) we\ncollect internet images of diverse \"fashion\" models together with a small set\nof anthropometric measurements; (2) we collect linguistic shape attributes for\na wide range of 3D body meshes and the model images. Taken together, these\ndatasets provide sufficient constraints to infer dense 3D shape. We exploit the\nanthropometric measurements and linguistic shape attributes in several novel\nways to train a neural network, called SHAPY, that regresses 3D human pose and\nshape from an RGB image. We evaluate SHAPY on public benchmarks, but note that\nthey either lack significant body shape variation, ground-truth shape, or\nclothing variation. Thus, we collect a new dataset for evaluating 3D human\nshape estimation, called HBW, containing photos of \"Human Bodies in the Wild\"\nfor which we have ground-truth 3D body scans. On this new benchmark, SHAPY\nsignificantly outperforms state-of-the-art methods on the task of 3D body shape\nestimation. This is the first demonstration that 3D body shape regression from\nimages can be trained from easy-to-obtain anthropometric measurements and\nlinguistic shape attributes. Our model and data are available at:\nshapy.is.tue.mpg.de\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choutas_V/0/1/0/all/0/1\">Vasileios Choutas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lea Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chun-Hao P. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzionas_D/0/1/0/all/0/1\">Dimitrios Tzionas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnimeSR: Learning Real-World Super-Resolution Models for Animation Videos. (arXiv:2206.07038v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07038","description":"<p>This paper studies the problem of real-world video super-resolution (VSR) for\nanimation videos, and reveals three key improvements for practical animation\nVSR. First, recent real-world super-resolution approaches typically rely on\ndegradation simulation using basic operators without any learning capability,\nsuch as blur, noise, and compression. In this work, we propose to learn such\nbasic operators from real low-quality animation videos, and incorporate the\nlearned ones into the degradation generation pipeline. Such\nneural-network-based basic operators could help to better capture the\ndistribution of real degradations. Second, a large-scale high-quality animation\nvideo dataset, AVC, is built to facilitate comprehensive training and\nevaluations for animation VSR. Third, we further investigate an efficient\nmulti-scale network structure. It takes advantage of the efficiency of\nunidirectional recurrent networks and the effectiveness of sliding-window-based\nmethods. Thanks to the above delicate designs, our method, AnimeSR, is capable\nof restoring real-world low-quality animation videos effectively and\nefficiently, achieving superior performance to previous state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReCo: Retrieve and Co-segment for Zero-shot Transfer. (arXiv:2206.07045v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07045","description":"<p>Semantic segmentation has a broad range of applications, but its real-world\nimpact has been significantly limited by the prohibitive annotation costs\nnecessary to enable deployment. Segmentation methods that forgo supervision can\nside-step these costs, but exhibit the inconvenient requirement to provide\nlabelled examples from the target distribution to assign concept names to\npredictions. An alternative line of work in language-image pre-training has\nrecently demonstrated the potential to produce models that can both assign\nnames across large vocabularies of concepts and enable zero-shot transfer for\nclassification, but do not demonstrate commensurate segmentation abilities. In\nthis work, we strive to achieve a synthesis of these two approaches that\ncombines their strengths. We leverage the retrieval abilities of one such\nlanguage-image pre-trained model, CLIP, to dynamically curate training sets\nfrom unlabelled images for arbitrary collections of concept names, and leverage\nthe robust correspondences offered by modern image representations to\nco-segment entities among the resulting collections. The synthetic segment\ncollections are then employed to construct a segmentation model (without\nrequiring pixel labels) whose knowledge of concepts is inherited from the\nscalable pre-training process of CLIP. We demonstrate that our approach, termed\nRetrieve and Co-segment (ReCo) performs favourably to unsupervised segmentation\napproaches while inheriting the convenience of nameable predictions and\nzero-shot transfer. We also demonstrate ReCo's ability to generate specialist\nsegmenters for extremely rare objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_G/0/1/0/all/0/1\">Gyungin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB-Multispectral Matching: Dataset, Learning Methodology, Evaluation. (arXiv:2206.07047v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07047","description":"<p>We address the problem of registering synchronized color (RGB) and\nmulti-spectral (MS) images featuring very different resolution by solving\nstereo matching correspondences. Purposely, we introduce a novel RGB-MS dataset\nframing 13 different scenes in indoor environments and providing a total of 34\nimage pairs annotated with semi-dense, high-resolution ground-truth labels in\nthe form of disparity maps. To tackle the task, we propose a deep learning\narchitecture trained in a self-supervised manner by exploiting a further RGB\ncamera, required only during training data acquisition. In this setup, we can\nconveniently learn cross-modal matching in the absence of ground-truth labels\nby distilling knowledge from an easier RGB-RGB matching task based on a\ncollection of about 11K unlabeled image triplets. Experiments show that the\nproposed pipeline sets a good performance bar (1.16 pixels average registration\nerror) for future research on this novel, challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tosi_F/0/1/0/all/0/1\">Fabio Tosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_P/0/1/0/all/0/1\">Pierluigi Zama Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1\">Matteo Poggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salti_S/0/1/0/all/0/1\">Samuele Salti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1\">Stefano Mattoccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1\">Luigi Di Stefano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-encoders for Track Reconstruction in Drift Chambers for CLAS12. (arXiv:2009.05144v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.05144","description":"<p>In this article we describe the development of machine learning models to\nassist the CLAS12 tracking algorithm by identifying tracks through inferring\nmissing segments in the drift chambers. Auto encoders are used to reconstruct\nmissing segments from track trajectory. Implemented neural network was able to\nreliably reconstruct missing segment positions with accuracy of $\\approx 0.35$\nwires, and lead to recovery of missing tracks with accuracy of $&gt;99.8\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gavalian_G/0/1/0/all/0/1\">Gagik Gavalian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Unique Is a Face: An Investigative Study. (arXiv:2102.04965v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.04965","description":"<p>Face recognition has been widely accepted as a means of identification in\napplications ranging from border control to security in the banking sector.\nSurprisingly, while widely accepted, we still lack the understanding of\nuniqueness or distinctiveness of faces as biometric modality. In this work, we\nstudy the impact of factors such as image resolution, feature representation,\ndatabase size, age and gender on uniqueness denoted by the Kullback-Leibler\ndivergence between genuine and impostor distributions. Towards understanding\nthe impact, we present experimental results on the datasets AT&amp;T, LFW,\nIMDb-Face, as well as ND-TWINS, with the feature extraction algorithms VGGFace,\nVGG16, ResNet50, InceptionV3, MobileNet and DenseNet121, that reveal the\nquantitative impact of the named factors. While these are early results, our\nfindings indicate the need for a better understanding of the concept of\nbiometric uniqueness and its implication on face recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balazia_M/0/1/0/all/0/1\">Michal Balazia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Happy_S/0/1/0/all/0/1\">S L Happy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Biases and Preserving Privacy on Balanced Faces in the Wild. (arXiv:2103.09118v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09118","description":"<p>There are demographic biases in current models used for facial recognition\n(FR). Our Balanced Faces In the Wild (BFW) dataset serves as a proxy to measure\nbias across ethnicity and gender subgroups, allowing one to characterize FR\nperformances per subgroup. We show results are non-optimal when a single score\nthreshold determines whether sample pairs are genuine or imposters. Within\nsubgroups, performance often varies significantly from the global average.\nThus, claims of specific error rates only hold for populations matching the\nvalidation data. We mitigate the imbalanced performances using a novel domain\nadaptation learning scheme on the facial features extracted using\nstate-of-the-art neural networks. This technique balances performance, but it\nalso boosts the overall performance. A benefit of the proposed is to preserve\nidentity information in facial features while decreasing the demographic\ninformation they contain. The removal of demographic knowledge prevents\npotential future biases from being injected into decision-making. This removal\nimproves privacy since less information is available or inferred about an\nindividual. We explore this qualitatively; we also show quantitatively that\nsubgroup classifiers no longer learn from the features of the proposed domain\nadaptation scheme. For source code and data descriptions, see\nhttps://github.com/visionjo/facerec-bias-bfw.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joseph P Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henon_Y/0/1/0/all/0/1\">Yann Henon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timoner_S/0/1/0/all/0/1\">Samson Timoner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness via Fisher-Rao Regularization. (arXiv:2106.06685v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.06685","description":"<p>Adversarial robustness has become a topic of growing interest in machine\nlearning since it was observed that neural networks tend to be brittle. We\npropose an information-geometric formulation of adversarial defense and\nintroduce FIRE, a new Fisher-Rao regularization for the categorical\ncross-entropy loss, which is based on the geodesic distance between the softmax\noutputs corresponding to natural and perturbed input features. Based on the\ninformation-geometric properties of the class of softmax distributions, we\nderive an explicit characterization of the Fisher-Rao Distance (FRD) for the\nbinary and multiclass cases, and draw some interesting properties as well as\nconnections with standard regularization metrics. Furthermore, for a simple\nlinear and Gaussian model, we show that all Pareto-optimal points in the\naccuracy-robustness region can be reached by FIRE while other state-of-the-art\nmethods fail. Empirically, we evaluate the performance of various classifiers\ntrained with the proposed loss on standard datasets, showing up to a\nsimultaneous 1\\% of improvement in terms of clean and robust performances while\nreducing the training time by 20\\% over the best-performing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picot_M/0/1/0/all/0/1\">Marine Picot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_F/0/1/0/all/0/1\">Francisco Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_F/0/1/0/all/0/1\">Fabrice Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steerable 3D Spherical Neurons. (arXiv:2106.13863v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13863","description":"<p>Emerging from low-level vision theory, steerable filters found their\ncounterpart in prior work on steerable convolutional neural networks\nequivariant to rigid transformations. In our work, we propose a steerable\nfeed-forward learning-based approach that consists of neurons with spherical\ndecision surfaces and operates on point clouds. Such spherical neurons are\nobtained by conformal embedding of Euclidean space and have recently been\nrevisited in the context of learning representations of point sets. Focusing on\n3D geometry, we exploit the isometry property of spherical neurons and derive a\n3D steerability constraint. After training spherical neurons to classify point\nclouds in a canonical orientation, we use a tetrahedron basis to quadruplicate\nthe neurons and construct rotation-equivariant spherical filter banks. We then\napply the derived constraint to interpolate the filter bank outputs and, thus,\nobtain a rotation-invariant network. Finally, we use a synthetic point set and\nreal-world 3D skeleton data to verify our theoretical findings. The code is\navailable at https://github.com/pavlo-melnyk/steerable-3d-neurons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying High Accuracy Regions in Traffic Camera Images to Enhance the Estimation of Road Traffic Metrics: A Quadtree-Based Method. (arXiv:2106.14049v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14049","description":"<p>The growing number of real-time camera feeds in urban areas has made it\npossible to provide high-quality traffic data for effective transportation\nplanning, operations, and management. However, deriving reliable traffic\nmetrics from these camera feeds has been a challenge due to the limitations of\ncurrent vehicle detection techniques, as well as the various camera conditions\nsuch as height and resolution. In this work, a quadtree based algorithm is\ndeveloped to continuously partition the image extent until only regions with\nhigh detection accuracy are remained. These regions are referred to as the\nhigh-accuracy identification regions (HAIR) in this paper. We demonstrate how\nthe use of the HAIR can improve the accuracy of traffic density estimates using\nimages from traffic cameras at different heights and resolutions in Central\nOhio. Our experiments show that the proposed algorithm can be used to derive\nrobust HAIR where vehicle detection accuracy is 41 percent higher than that in\nthe original image extent. The use of the HAIR also significantly improves the\ntraffic density estimation with an overall decrease of 49 percent in root mean\nsquared error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Ningchuan Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMT: Convolutional Neural Networks Meet Vision Transformers. (arXiv:2107.06263v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06263","description":"<p>Vision transformers have been successfully applied to image recognition tasks\ndue to their ability to capture long-range dependencies within an image.\nHowever, there are still gaps in both performance and computational cost\nbetween transformers and existing convolutional neural networks (CNNs). In this\npaper, we aim to address this issue and develop a network that can outperform\nnot only the canonical transformers, but also the high-performance\nconvolutional models. We propose a new transformer based hybrid network by\ntaking advantage of transformers to capture long-range dependencies, and of\nCNNs to model local features. Furthermore, we scale it to obtain a family of\nmodels, called CMTs, obtaining much better accuracy and efficiency than\nprevious convolution and transformer based models. In particular, our CMT-S\nachieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on\nFLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S\nalso generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%),\nand other challenging vision datasets such as COCO (44.3% mAP), with\nconsiderably less computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02235","description":"<p>Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods adopting training way similar to meta-learning have achieved promising\nperformance, such as Meta R-CNN series. However, support data is only used as\nthe class attention to guide the detecting of query images each time. Their\nrelevance to each other remains unexploited. Moreover, a lot of recent works\ntreat the support data and query images as independent branch without\nconsidering the relationship between them. To address this issue, we propose a\ndynamic relevance learning model, which utilizes the relationship between all\nsupport images and Region of Interest (RoI) on the query images to construct a\ndynamic graph convolutional network (GCN). By adjusting the prediction\ndistribution of the base detector using the output of this GCN, the proposed\nmodel serves as a hard auxiliary classification task, which guides the detector\nto improve the class representation implicitly. Comprehensive experiments have\nbeen conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves\nthe best overall performance, which shows its effectiveness of learning more\ngeneralized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haohe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shenghao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jiangbo Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiafei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated Construction of Multimodal Atlases with Structural Connectomes in the Space of Riemannian Metrics. (arXiv:2109.09808v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09808","description":"<p>The structural network of the brain, or structural connectome, can be\nrepresented by fiber bundles generated by a variety of tractography methods.\nWhile such methods give qualitative insights into brain structure, there is\ncontroversy over whether they can provide quantitative information, especially\nat the population level. In order to enable population-level statistical\nanalysis of the structural connectome, we propose representing a connectome as\na Riemannian metric, which is a point on an infinite-dimensional manifold. We\nequip this manifold with the Ebin metric, a natural metric structure for this\nspace, to get a Riemannian manifold along with its associated geometric\nproperties. We then use this Riemannian framework to apply object-oriented\nstatistical analysis to define an atlas as the Fr\\'echet mean of a population\nof Riemannian metrics. This formulation ties into the existing framework for\ndiffeomorphic construction of image atlases, allowing us to construct a\nmultimodal atlas by simultaneously integrating complementary white matter\nstructure details from DWMRI and cortical details from T1-weighted MRI. We\nillustrate our framework with 2D data examples of connectome registration and\natlas formation. Finally, we build an example 3D multimodal atlas using T1\nimages and connectomes derived from diffusion tensors estimated from a subset\nof subjects from the Human Connectome Project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campbell_K/0/1/0/all/0/1\">Kristen M. Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haocheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhe Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1\">Martin Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_P/0/1/0/all/0/1\">P. Thomas Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sarang C. Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09818","description":"<p>Convolutional Neural Networks have demonstrated dermatologist-level\nperformance in the classification of melanoma from skin lesion images, but\nprediction irregularities due to biases seen within the training data are an\nissue that should be addressed before widespread deployment is possible. In\nthis work, we robustly remove bias and spurious variation from an automated\nmelanoma classification pipeline using two leading bias unlearning techniques.\nWe show that the biases introduced by surgical markings and rulers presented in\nprevious studies can be reasonably mitigated using these bias removal methods.\nWe also demonstrate the generalisation benefits of unlearning spurious\nvariation relating to the imaging instrument used to capture lesion images. Our\nexperimental results provide evidence that the effects of each of the\naforementioned biases are notably reduced, with different debiasing techniques\nexcelling at different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Stacking Ensemble Approach for Supervised Video Summarization. (arXiv:2109.12581v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12581","description":"<p>Video summarization methods are usually classified into shot-level or\nframe-level methods, which are individually used in a general way. This paper\ninvestigates the underlying complementarity between the frame-level and\nshot-level methods, and a stacking ensemble approach is proposed for supervised\nvideo summarization. Firstly, we build up a stacking model to predict both the\nkey frame probabilities and the temporal interest segments simultaneously. The\ntwo components are then combined via soft decision fusion to obtain the final\nscores of each frame in the video. A joint loss function is proposed here to\ntrain the model. The ablation experimental results show that the proposed\nmethod outperforms both the two corresponding individual method. Furthermore,\nextensive experiments and analysis on two benchmark datasets demonstrate the\neffectiveness of our method and its superior performance in comparison with the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yubo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shenghui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAME: Facial Landmark Heatmap Activated Multimodal Gaze Estimation. (arXiv:2110.04828v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04828","description":"<p>3D gaze estimation is about predicting the line of sight of a person in 3D\nspace. Person-independent models for the same lack precision due to anatomical\ndifferences of subjects, whereas person-specific calibrated techniques add\nstrict constraints on scalability. To overcome these issues, we propose a novel\ntechnique, Facial Landmark Heatmap Activated Multimodal Gaze Estimation\n(FLAME), as a way of combining eye anatomical information using eye landmark\nheatmaps to obtain precise gaze estimation without any person-specific\ncalibration. Our evaluation demonstrates a competitive performance of about 10%\nimprovement on benchmark datasets ColumbiaGaze and EYEDIAP. We also conduct an\nablation study to validate our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1\">Neelabh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazia_M/0/1/0/all/0/1\">Michal Balazia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Normalized Gaussian Wasserstein Distance for Tiny Object Detection. (arXiv:2110.13389v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13389","description":"<p>Detecting tiny objects is a very challenging problem since a tiny object only\ncontains a few pixels in size. We demonstrate that state-of-the-art detectors\ndo not produce satisfactory results on tiny objects due to the lack of\nappearance information. Our key observation is that Intersection over Union\n(IoU) based metrics such as IoU itself and its extensions are very sensitive to\nthe location deviation of the tiny objects, and drastically deteriorate the\ndetection performance when used in anchor-based detectors. To alleviate this,\nwe propose a new evaluation metric using Wasserstein distance for tiny object\ndetection. Specifically, we first model the bounding boxes as 2D Gaussian\ndistributions and then propose a new metric dubbed Normalized Wasserstein\nDistance (NWD) to compute the similarity between them by their corresponding\nGaussian distributions. The proposed NWD metric can be easily embedded into the\nassignment, non-maximum suppression, and loss function of any anchor-based\ndetector to replace the commonly used IoU metric. We evaluate our metric on a\nnew dataset for tiny object detection (AI-TOD) in which the average object size\nis much smaller than existing object detection datasets. Extensive experiments\nshow that, when equipped with NWD metric, our approach yields performance that\nis 6.7 AP points higher than a standard fine-tuning baseline, and 6.0 AP points\nhigher than state-of-the-art competitors. Codes are available at:\nhttps://github.com/jwwangchn/NWD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocScanner: Robust Document Image Rectification with Progressive Learning. (arXiv:2110.14968v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14968","description":"<p>Compared with flatbed scanners, portable smartphones are much more convenient\nfor physical documents digitizing. However, such digitized documents are often\ndistorted due to uncontrolled physical deformations, camera positions, and\nillumination variations. To this end, we present DocScanner, a novel framework\nfor document image rectification. Different from existing methods, DocScanner\naddresses this issue by introducing a progressive learning mechanism.\nSpecifically, DocScanner maintains a single estimate of the rectified image,\nwhich is progressively corrected with a recurrent architecture. The iterative\nrefinements make DocScanner converge to a robust and superior performance,\nwhile the lightweight recurrent architecture ensures the running efficiency. In\naddition, before the above rectification process, observing the corrupted\nrectified boundaries existing in prior works, DocScanner exploits a document\nlocalization module to explicitly segment the foreground document from the\ncluttered background environments. To further improve the rectification\nquality, based on the geometric priori between the distorted and the rectified\nimages, a geometric regularization is introduced during training to further\nimprove the performance. Extensive experiments are conducted on the Doc3D\ndataset and the DocUNet Benchmark dataset, and the quantitative and qualitative\nevaluation results verify the effectiveness of DocScanner, which outperforms\nprevious methods on OCR accuracy, image similarity, and our proposed distortion\nmetric by a considerable margin. Furthermore, our DocScanner shows the highest\nefficiency in runtime latency and model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation. (arXiv:2111.09515v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09515","description":"<p>3D object detection from LiDAR data for autonomous driving has been making\nremarkable strides in recent years. Among the state-of-the-art methodologies,\nencoding point clouds into a bird's eye view (BEV) has been demonstrated to be\nboth effective and efficient. Different from perspective views, BEV preserves\nrich spatial and distance information between objects. Yet, while farther\nobjects of the same type do not appear smaller in the BEV, they contain sparser\npoint cloud features. This fact weakens BEV feature extraction using\nshared-weight convolutional neural networks (CNNs). In order to address this\nchallenge, we propose Range-Aware Attention Network (RAANet), which extracts\neffective BEV features and generates superior 3D object detection outputs. The\nrange-aware attention (RAA) convolutions significantly improve feature\nextraction for near as well as far objects. Moreover, we propose a novel\nauxiliary loss for point density estimation to further enhance the detection\naccuracy of RAANet for occluded objects. It is worth to note that our proposed\nRAA convolution is lightweight and compatible to be integrated into any CNN\narchitecture used for detection from a BEV. Extensive experiments on the\nnuScenes and KITTI datasets demonstrate that our proposed approach outperforms\nthe state-of-the-art methods for LiDAR-based 3D object detection, with\nreal-time inference speed of 16 Hz for the full version and 22 Hz for the lite\nversion tested on nuScenes lidar frames. The code is publicly available at our\nGithub repository https://github.com/erbloo/RAAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yantao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xuetao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yilan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1\">Weiheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velipasalar_S/0/1/0/all/0/1\">Senem Velipasalar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Person Re-identification Method Based on Color Attack and Joint Defence. (arXiv:2111.09571v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09571","description":"<p>The main challenges of ReID is the intra-class variations caused by color\ndeviation under different camera conditions. Simultaneously, we find that most\nof the existing adversarial metric attacks are realized by interfering with the\ncolor characteristics of the sample. Based on this observation, we first\npropose a local transformation attack (LTA) based on color variation. It uses\nmore obvious color variation to randomly disturb the color of the retrieved\nimage, rather than adding random noise. Experiments show that the performance\nof the proposed LTA method is better than the advanced attack methods.\nFurthermore, considering that the contour feature is the main factor of the\nrobustness of adversarial training, and the color feature will directly affect\nthe success rate of attack. Therefore, we further propose joint adversarial\ndefense (JAD) method, which include proactive defense and passive defense.\nProactive defense fuse multi-modality images to enhance the contour feature and\ncolor feature, and considers local homomorphic transformation to solve the\nover-fitting problem. Passive defense exploits the invariance of contour\nfeature during image scaling to mitigate the adversarial disturbance on contour\nfeature. Finally, a series of experimental results show that the proposed joint\nadversarial defense method is more competitive than a state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yunpeng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lifei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoding Hierarchical Information in Neural Networks helps in Subpopulation Shift. (arXiv:2112.10844v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10844","description":"<p>Over the past decade, deep neural networks have proven to be adept in image\nclassification tasks, often surpassing humans in terms of accuracy. However,\nstandard neural networks often fail to understand the concept of hierarchical\nstructures and dependencies among different classes for vision related tasks.\nHumans on the other hand, seem to intuitively learn categories conceptually,\nprogressively growing from understanding high-level concepts down to granular\nlevels of categories. One of the issues arising from the inability of neural\nnetworks to encode such dependencies within its learned structure is that of\nsubpopulation shift -- where models are queried with novel unseen classes taken\nfrom a shifted population of the training set categories. Since the neural\nnetwork treats each class as independent from all others, it struggles to\ncategorize shifting populations that are dependent at higher levels of the\nhierarchy. In this work, we study the aforementioned problems through the lens\nof a novel conditional supervised training framework. We tackle subpopulation\nshift by a structured learning procedure that incorporates hierarchical\ninformation conditionally through labels. Furthermore, we introduce a notion of\ngraphical distance to model the catastrophic effect of mispredictions. We show\nthat learning in this structured hierarchical manner results in networks that\nare more robust against subpopulation shifts, with an improvement up to 3\\% in\nterms of accuracy and up to 11\\% in terms of graphical distance over standard\nmodels on subpopulation shift benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Amitangshu Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_I/0/1/0/all/0/1\">Isha Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PETS-SWINF: A regression method that considers images with metadata based Neural Network for pawpularity prediction on 2021 Kaggle Competition \"PetFinder.my\". (arXiv:2201.06061v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06061","description":"<p>Millions of stray animals suffer on the streets or are euthanized in shelters\nevery day around the world. In order to better adopt stray animals, scoring the\npawpularity (cuteness) of stray animals is very important, but evaluating the\npawpularity of animals is a very labor-intensive thing. Consequently, there has\nbeen an urgent surge of interest to develop an algorithm that scores\npawpularity of animals. However, the dataset in Kaggle not only has images, but\nalso metadata describing images. Most methods basically focus on the most\nadvanced image regression methods in recent years, but there is no good method\nto deal with the metadata of images. To address the above challenges, the paper\nproposes an image regression model called PETS-SWINF that considers metadata of\nthe images. Our results based on a dataset of Kaggle competition,\n\"PetFinder.my\", show that PETS-SWINF has an advantage over only based images\nmodels. Our results shows that the RMSE loss of the proposed model on the test\ndataset is 17.71876 but 17.76449 without metadata. The advantage of the\nproposed method is that PETS-SWINF can consider both low-order and high-order\nfeatures of metadata, and adaptively adjust the weights of the image model and\nthe metadata model. The performance is promising as our leadboard score is\nranked 15 out of 3545 teams (Gold medal) currently for 2021 Kaggle competition\non the challenge \"PetFinder.my\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinghua Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Consistent and Efficient Evaluation Strategy for Attribution Methods. (arXiv:2202.00449v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00449","description":"<p>With a variety of local feature attribution methods being proposed in recent\nyears, follow-up work suggested several evaluation strategies. To assess the\nattribution quality across different attribution techniques, the most popular\namong these evaluation strategies in the image domain use pixel perturbations.\nHowever, recent advances discovered that different evaluation strategies\nproduce conflicting rankings of attribution methods and can be prohibitively\nexpensive to compute. In this work, we present an information-theoretic\nanalysis of evaluation strategies based on pixel perturbations. Our findings\nreveal that the results are strongly affected by information leakage through\nthe shape of the removed pixels as opposed to their actual values. Using our\ntheoretical insights, we propose a novel evaluation framework termed Remove and\nDebias (ROAD) which offers two contributions: First, it mitigates the impact of\nthe confounders, which entails higher consistency among evaluation strategies.\nSecond, ROAD does not require the computationally expensive retraining step and\nsaves up to 99% in computational costs compared to the state-of-the-art. We\nrelease our source code at https://github.com/tleemann/road_evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leemann_T/0/1/0/all/0/1\">Tobias Leemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borisov_V/0/1/0/all/0/1\">Vadim Borisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1\">Gjergji Kasneci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking and Analyzing Point Cloud Classification under Corruptions. (arXiv:2202.03377v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03377","description":"<p>3D perception, especially point cloud classification, has achieved\nsubstantial progress. However, in real-world deployment, point cloud\ncorruptions are inevitable due to the scene complexity, sensor inaccuracy, and\nprocessing imprecision. In this work, we aim to rigorously benchmark and\nanalyze point cloud classification under corruptions. To conduct a systematic\ninvestigation, we first provide a taxonomy of common 3D corruptions and\nidentify the atomic corruptions. Then, we perform a comprehensive evaluation on\na wide range of representative point cloud models to understand their\nrobustness and generalizability. Our benchmark results show that although point\ncloud classification performance improves over time, the state-of-the-art\nmethods are on the verge of being less robust. Based on the obtained\nobservations, we propose several effective techniques to enhance point cloud\nclassifier robustness. We hope our comprehensive benchmark, in-depth analysis,\nand proposed techniques could spark future research in robust 3D perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiawei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Acoustic Matching. (arXiv:2202.06875v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06875","description":"<p>We introduce the visual acoustic matching task, in which an audio clip is\ntransformed to sound like it was recorded in a target environment. Given an\nimage of the target environment and a waveform for the source audio, the goal\nis to re-synthesize the audio to match the target room acoustics as suggested\nby its visible geometry and materials. To address this novel task, we propose a\ncross-modal transformer model that uses audio-visual attention to inject visual\nproperties into the audio and generate realistic audio output. In addition, we\ndevise a self-supervised training objective that can learn acoustic matching\nfrom in-the-wild Web videos, despite their lack of acoustically mismatched\naudio. We demonstrate that our approach successfully translates human speech to\na variety of real-world environments depicted in images, outperforming both\ntraditional acoustic matching and more heavily supervised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruohan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calamia_P/0/1/0/all/0/1\">Paul Calamia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General-purpose, long-context autoregressive modeling with Perceiver AR. (arXiv:2202.07765v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.07765","description":"<p>Real-world data is high-dimensional: a book, image, or musical performance\ncan easily contain hundreds of thousands of elements even after compression.\nHowever, the most commonly used autoregressive models, Transformers, are\nprohibitively expensive to scale to the number of inputs and layers needed to\ncapture this long-range structure. We develop Perceiver AR, an autoregressive,\nmodality-agnostic architecture which uses cross-attention to map long-range\ninputs to a small number of latents while also maintaining end-to-end causal\nmasking. Perceiver AR can directly attend to over a hundred thousand tokens,\nenabling practical long-context density estimation without the need for\nhand-crafted sparsity patterns or memory mechanisms. When trained on images or\nmusic, Perceiver AR generates outputs with clear long-term coherence and\nstructure. Our architecture also obtains state-of-the-art likelihood on\nlong-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1\">Curtis Hawthorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangea_C/0/1/0/all/0/1\">C&#x103;t&#x103;lina Cangea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_C/0/1/0/all/0/1\">Charlie Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dieleman_S/0/1/0/all/0/1\">Sander Dieleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_I/0/1/0/all/0/1\">Ian Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheahan_H/0/1/0/all/0/1\">Hannah Sheahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeghidour_N/0/1/0/all/0/1\">Neil Zeghidour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1\">Jesse Engel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding. (arXiv:2203.09175v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09175","description":"<p>Large-scale crop type classification is a task at the core of remote sensing\nefforts with applications of both economic and ecological importance. Current\nstate-of-the-art deep learning methods are based on self-attention and use\nsatellite image time series (SITS) to discriminate crop types based on their\nunique growth patterns. However, existing methods generalize poorly to regions\nnot seen during training mainly due to not being robust to temporal shifts of\nthe growing season caused by variations in climate. To this end, we propose\nThermal Positional Encoding (TPE) for attention-based crop classifiers. Unlike\nprevious positional encoding based on calendar time (e.g. day-of-year), TPE is\nbased on thermal time, which is obtained by accumulating daily average\ntemperatures over the growing season. Since crop growth is directly related to\nthermal time, but not calendar time, TPE addresses the temporal shifts between\ndifferent regions to improve generalization. We propose multiple TPE\nstrategies, including learnable methods, to further improve results compared to\nthe common fixed positional encodings. We demonstrate our approach on a crop\nclassification task across four different European regions, where we obtain\nstate-of-the-art generalization results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nyborg_J/0/1/0/all/0/1\">Joachim Nyborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1\">Charlotte Pelletier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assent_I/0/1/0/all/0/1\">Ira Assent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Vision Transformers for Joint SAR-optical Representation Learning. (arXiv:2204.05381v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05381","description":"<p>Self-supervised learning (SSL) has attracted much interest in remote sensing\nand earth observation due to its ability to learn task-agnostic representations\nwithout human annotation. While most of the existing SSL works in remote\nsensing utilize ConvNet backbones and focus on a single modality, we explore\nthe potential of vision transformers (ViTs) for joint SAR-optical\nrepresentation learning. Based on DINO, a state-of-the-art SSL algorithm that\ndistills knowledge from two augmented views of an input image, we combine SAR\nand optical imagery by concatenating all channels to a unified input.\nSubsequently, we randomly mask out channels of one modality as a data\naugmentation strategy. While training, the model gets fed optical-only,\nSAR-only, and SAR-optical image pairs learning both inner- and intra-modality\nrepresentations. Experimental results employing the BigEarthNet-MM dataset\ndemonstrate the benefits of both, the ViT backbones and the proposed multimodal\nSSL algorithm DINO-MM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1\">Conrad M Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Bias in Facial Analysis Systems by Incorporating Label Diversity. (arXiv:2204.06364v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06364","description":"<p>Facial analysis models are increasingly applied in real-world applications\nthat have significant impact on peoples' lives. However, as literature has\nshown, models that automatically classify facial attributes might exhibit\nalgorithmic discrimination behavior with respect to protected groups,\npotentially posing negative impacts on individuals and society. It is therefore\ncritical to develop techniques that can mitigate unintended biases in facial\nclassifiers. Hence, in this work, we introduce a novel learning method that\ncombines both subjective human-based labels and objective annotations based on\nmathematical definitions of facial traits. Specifically, we generate new\nobjective annotations from two large-scale human-annotated dataset, each\ncapturing a different perspective of the analyzed facial trait. We then propose\nan ensemble learning method, which combines individual models trained on\ndifferent types of annotations. We provide an in-depth analysis of the\nannotation procedure as well as the datasets distribution. Moreover, we\nempirically demonstrate that, by incorporating label diversity, our method\nsuccessfully mitigates unintended biases, while maintaining significant\naccuracy on the downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolling_C/0/1/0/all/0/1\">Camila Kolling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Victor Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_A/0/1/0/all/0/1\">Adriano Veloso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musse_S/0/1/0/all/0/1\">Soraia Raupp Musse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Missingness Bias in Model Debugging. (arXiv:2204.08945v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08945","description":"<p>Missingness, or the absence of features from an input, is a concept\nfundamental to many model debugging tools. However, in computer vision, pixels\ncannot simply be removed from an image. One thus tends to resort to heuristics\nsuch as blacking out pixels, which may in turn introduce bias into the\ndebugging process. We study such biases and, in particular, show how\ntransformer-based architectures can enable a more natural implementation of\nmissingness, which side-steps these issues and improves the reliability of\nmodel debugging in practice. Our code is available at\nhttps://github.com/madrylab/missingness\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1\">Hadi Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1\">Sai Vemprala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander Madry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keypoint based Sign Language Translation without Glosses. (arXiv:2204.10511v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10511","description":"<p>Sign Language Translation (SLT) is a task that has not been studied\nrelatively much compared to the study of Sign Language Recognition (SLR).\nHowever, the SLR is a study that recognizes the unique grammar of sign\nlanguage, which is different from the spoken language and has a problem that\nnon-disabled people cannot easily interpret. So, we're going to solve the\nproblem of translating directly spoken language in sign language video. To this\nend, we propose a new keypoint normalization method for performing translation\nbased on the skeleton point of the signer and robustly normalizing these points\nin sign language translation. It contributed to performance improvement by a\ncustomized normalization method depending on the body parts. In addition, we\npropose a stochastic frame selection method that enables frame augmentation and\nsampling at the same time. Finally, it is translated into the spoken language\nthrough an Attention-based translation model. Our method can be applied to\nvarious datasets in a way that can be applied to datasets without glosses. In\naddition, quantitative experimental evaluation proved the excellence of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_M/0/1/0/all/0/1\">Minji Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dain Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeongeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_H/0/1/0/all/0/1\">Hyeongboo Baek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCa: Contrastive Captioners are Image-Text Foundation Models. (arXiv:2205.01917v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01917","description":"<p>Exploring large-scale pretrained foundation models is of significant interest\nin computer vision because these models can be quickly transferred to many\ndownstream tasks. This paper presents Contrastive Captioner (CoCa), a\nminimalist design to pretrain an image-text encoder-decoder foundation model\njointly with contrastive loss and captioning loss, thereby subsuming model\ncapabilities from contrastive approaches like CLIP and generative methods like\nSimVLM. In contrast to standard encoder-decoder transformers where all decoder\nlayers attend to encoder outputs, CoCa omits cross-attention in the first half\nof decoder layers to encode unimodal text representations, and cascades the\nremaining decoder layers which cross-attend to the image encoder for multimodal\nimage-text representations. We apply a contrastive loss between unimodal image\nand text embeddings, in addition to a captioning loss on the multimodal decoder\noutputs which predicts text tokens autoregressively. By sharing the same\ncomputational graph, the two training objectives are computed efficiently with\nminimal overhead. CoCa is pretrained end-to-end and from scratch on both\nweb-scale alt-text data and annotated images by treating all labels simply as\ntext, seamlessly unifying natural language supervision for representation\nlearning. Empirically, CoCa achieves state-of-the-art performance with\nzero-shot transfer or minimal task-specific adaptation on a broad range of\ndownstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700,\nMoments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal\nunderstanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps).\nNotably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1\naccuracy, 90.6% with a frozen encoder and learned classification head, and new\nstate-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_L/0/1/0/all/0/1\">Legg Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyedhosseini_M/0/1/0/all/0/1\">Mojtaba Seyedhosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction. (arXiv:2205.03883v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.03883","description":"<p>Deep learning based parallel Imaging (PI) has made great progresses in recent\nyears to accelerate magnetic resonance imaging (MRI). Nevertheless, the\nperformanc-es and robustness of existing methods can still be im-proved. In\nthis work, we propose to explore the k-space domain learning via robust\ngenerative modeling for flexible PI reconstruction, coined weight-k-space\ngenera-tive model (WKGM). Specifically, WKGM is a general-ized k-space domain\nmodel, where the k-space weighting technology and high-dimensional space\naugmentation design are efficiently incorporated for score-based gen-erative\nmodel training, resulting in good and robust re-constructions. In addition,\nWKGM is flexible and thus can be synergistically combined with various\ntraditional k-space PI models, generating learning-based priors to produce\nhigh-fidelity reconstructions. Experimental re-sults on datasets with varying\nsampling patterns and ac-celeration factors demonstrate that WKGM can attain\nstate-of-the-art reconstruction results with the well-learned k-space\ngenerative prior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zongjiang Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Die Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_C/0/1/0/all/0/1\">Chen Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection. (arXiv:2205.07403v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07403","description":"<p>Real-time and high-performance 3D object detection is of critical importance\nfor autonomous driving. Recent top-performing 3D object detectors mainly rely\non point-based or 3D voxel-based convolutions, which are both computationally\ninefficient for onboard deployment. In contrast, pillar-based methods use\nsolely 2D convolutions, which consume less computation resources, but they lag\nfar behind their voxel-based counterparts in detection accuracy. In this paper,\nby examining the primary performance gap between pillar- and voxel-based\ndetectors, we develop a real-time and high-performance pillar-based detector,\ndubbed PillarNet. The proposed PillarNet consists of a powerful encoder network\nfor effective pillar feature learning, a neck network for spatial-semantic\nfeature fusion and the commonly used detect head. Using only 2D convolutions,\nPillarNet is flexible to an optional pillar size and compatible with classical\n2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits\nfrom our designed orientation-decoupled IoU regression loss along with the\nIoU-aware prediction branch. Extensive experimental results on large-scale\nnuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet\nperforms well over the state-of-the-art 3D detectors in terms of effectiveness\nand efficiency. The source code is available at\nhttps://github.com/agent-sgs/PillarNet.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guangsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfReformer: Self-Refined Network with Transformer for Salient Object Detection. (arXiv:2205.11283v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11283","description":"<p>The global and local contexts significantly contribute to the integrity of\npredictions in Salient Object Detection (SOD). Unfortunately, existing methods\nstill struggle to generate complete predictions with fine details. There are\ntwo major problems in conventional approaches: first, for global context,\nhigh-level CNN-based encoder features cannot effectively catch long-range\ndependencies, resulting in incomplete predictions. Second, downsampling the\nground truth to fit the size of predictions will introduce inaccuracy as the\nground truth details are lost during interpolation or pooling. Thus, in this\nwork, we developed a Transformer-based network and framed a supervised task for\na branch to learn the global context information explicitly. Besides, we adopt\nPixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the\nsize of ground truth instead of the reverse. Thus details in the ground truth\nare untouched. In addition, we developed a two-stage Context Refinement Module\n(CRM) to fuse global context and automatically locate and refine the local\ndetails in the predictions. The proposed network can guide and correct itself\nbased on the global and local context generated, thus is named, Self-Refined\nTransformer (SelfReformer). Extensive experiments and evaluation results on\nfive benchmark datasets demonstrate the outstanding performance of the network,\nand we achieved the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1\">Yi Ke Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CellCentroidFormer: Combining Self-attention and Convolution for Cell Detection. (arXiv:2206.00338v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.00338","description":"<p>Cell detection in microscopy images is important to study how cells move and\ninteract with their environment. Most recent deep learning-based methods for\ncell detection use convolutional neural networks (CNNs). However, inspired by\nthe success in other computer vision applications, vision transformers (ViTs)\nare also used for this purpose. We propose a novel hybrid CNN-ViT model for\ncell detection in microscopy images to exploit the advantages of both types of\ndeep learning models. We employ an efficient CNN, that was pre-trained on the\nImageNet dataset, to extract image features and utilize transfer learning to\nreduce the amount of required training data. Extracted image features are\nfurther processed by a combination of convolutional and transformer layers, so\nthat the convolutional layers can focus on local information and the\ntransformer layers on global information. Our centroid-based cell detection\nmethod represents cells as ellipses and is end-to-end trainable. Furthermore,\nwe show that our proposed model can outperform fully convolutional one-stage\ndetectors on four different 2D microscopy datasets. Code is available at:\nhttps://github.com/roydenwa/cell-centroid-former\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wagner_R/0/1/0/all/0/1\">Royden Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohr_K/0/1/0/all/0/1\">Karl Rohr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01191","description":"<p>Vision Transformers (ViT) have shown rapid progress in computer vision tasks,\nachieving promising results on various benchmarks. However, due to the massive\nnumber of parameters and model design, e.g., attention mechanism, ViT-based\nmodels are generally times slower than lightweight convolutional networks.\nTherefore, the deployment of ViT for real-time applications is particularly\nchallenging, especially on resource-constrained hardware such as mobile\ndevices. Recent efforts try to reduce the computation complexity of ViT through\nnetwork architecture search or hybrid design with MobileNet block, yet the\ninference speed is still unsatisfactory. This leads to an important question:\ncan transformers run as fast as MobileNet while obtaining high performance? To\nanswer this, we first revisit the network architecture and operators used in\nViT-based models and identify inefficient designs. Then we introduce a\ndimension-consistent pure transformer (without MobileNet blocks) as a design\nparadigm. Finally, we perform latency-driven slimming to get a series of final\nmodels dubbed EfficientFormer. Extensive experiments show the superiority of\nEfficientFormer in performance and speed on mobile devices. Our fastest model,\nEfficientFormer-L1, achieves $79.2\\%$ top-1 accuracy on ImageNet-1K with only\n$1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which { runs as\nfast as MobileNetV2$\\times 1.4$ ($1.6$ ms, $74.7\\%$ top-1),} and our largest\nmodel, EfficientFormer-L7, obtains $83.3\\%$ accuracy with only $7.0$ ms\nlatency. Our work proves that properly designed transformers can reach\nextremely low latency on mobile devices while maintaining high performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Eric Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evangelidis_G/0/1/0/all/0/1\">Georgios Evangelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling. (arXiv:2206.01319v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01319","description":"<p>Domain adaptation (DA) aims to transfer knowledge learned from a labeled\nsource domain to an unlabeled or a less labeled but related target domain.\nIdeally, the source and target distributions should be aligned to each other\nequally to achieve unbiased knowledge transfer. However, due to the significant\nimbalance between the amount of annotated data in the source and target\ndomains, usually only the target distribution is aligned to the source domain,\nleading to adapting unnecessary source specific knowledge to the target domain,\ni.e., biased domain adaptation. To resolve this problem, in this work, we delve\ninto the transferability estimation problem in domain adaptation and propose a\nnon-intrusive Unbiased Transferability Estimation Plug-in (UTEP) by modeling\nthe uncertainty of a discriminator in adversarial-based DA methods to optimize\nunbiased transfer. We theoretically analyze the effectiveness of the proposed\napproach to unbiased transferability learning in DA. Furthermore, to alleviate\nthe impact of imbalanced annotated data, we utilize the estimated uncertainty\nfor pseudo label selection of unlabeled samples in the target domain, which\nhelps achieve better marginal and conditional distribution alignments between\ndomains. Extensive experimental results on a high variety of DA benchmark\ndatasets show that the proposed approach can be readily incorporated into\nvarious adversarial-based DA methods, achieving state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Haowen Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guile Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-OCRv3: More Attempts for the Improvement of Ultra Lightweight OCR System. (arXiv:2206.03001v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03001","description":"<p>Optical character recognition (OCR) technology has been widely used in\nvarious scenes, as shown in Figure 1. Designing a practical OCR system is still\na meaningful but challenging task. In previous work, considering the efficiency\nand accuracy, we proposed a practical ultra lightweight OCR system (PP-OCR),\nand an optimized version PP-OCRv2. In order to further improve the performance\nof PP-OCRv2, a more robust OCR system PP-OCRv3 is proposed in this paper.\nPP-OCRv3 upgrades the text detection model and text recognition model in 9\naspects based on PP-OCRv2. For text detector, we introduce a PAN module with\nlarge receptive field named LK-PAN, a FPN module with residual attention\nmechanism named RSE-FPN, and DML distillation strategy. For text recognizer,\nthe base model is replaced from CRNN to SVTR, and we introduce lightweight text\nrecognition network SVTR LCNet, guided training of CTC by attention, data\naugmentation strategy TextConAug, better pre-trained model by self-supervised\nTextRotNet, UDML, and UIM to accelerate the model and improve the effect.\nExperiments on real data show that the hmean of PP-OCRv3 is 5% higher than\nPP-OCRv2 under comparable inference speed. All the above mentioned models are\nopen-sourced and the code is available in the GitHub repository PaddleOCR which\nis powered by PaddlePaddle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiaoting Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kaitao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yongkun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1\">Baohua Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MS-RNN: A Flexible Multi-Scale Framework for Spatiotemporal Predictive Learning. (arXiv:2206.03010v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03010","description":"<p>Spatiotemporal predictive learning is to predict future frame changes through\nhistorical prior knowledge. Previous work improves the performance by making\nthe network wider and deeper, but that also brings huge memory overhead, which\nseriously hinders the development and application of the technology. Scale is\nanother dimension to improve model performance in common computer vision tasks,\nwhich can decrease the computing requirements and better sense context. Such an\nimportant dimension has not been considered and explored by recent RNN models.\nIn this paper, learning from the benefit of multi-scale, we propose a general\nframework named Multi-Scale RNN (MS-RNN) to boost recent RNN models. We verify\nthe MS-RNN framework by exhaustive experiments with 6 popular RNN models\n(ConvLSTM, TrajGRU, PredRNN, PredRNN++, MIM, and MotionRNN) on 4 different\ndatasets (Moving MNIST, KTH, TaxiBJ, and HKO-7). The results show the\nefficiency that the RNN models incorporating our framework have much lower\nmemory cost but better performance than before. Our code is released at\n\\url{https://github.com/mazhf/MS-RNN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhifeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet Prior Attention Learning in Axial Inpainting Network. (arXiv:2206.03113v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03113","description":"<p>Image inpainting is the task of filling masked or unknown regions of an image\nwith visually realistic contents, which has been remarkably improved by Deep\nNeural Networks (DNNs) recently. Essentially, as an inverse problem, the\ninpainting has the underlying challenges of reconstructing semantically\ncoherent results without texture artifacts. Many previous efforts have been\nmade via exploiting attention mechanisms and prior knowledge, such as edges and\nsemantic segmentation. However, these works are still limited in practice by an\navalanche of learnable prior parameters and prohibitive computational burden.\nTo this end, we propose a novel model -- Wavelet prior attention learning in\nAxial Inpainting Network (WAIN), whose generator contains the encoder, decoder,\nas well as two key components of Wavelet image Prior Attention (WPA) and\nstacked multi-layer Axial-Transformers (ATs). Particularly, the WPA guides the\nhigh-level feature aggregation in the multi-scale frequency domain, alleviating\nthe textual artifacts. Stacked ATs employ unmasked clues to help model\nreasonable features along with low-level features of horizontal and vertical\naxes, improving the semantic coherence. Extensive quantitative and qualitative\nexperiments on Celeba-HQ and Places2 datasets are conducted to validate that\nour WAIN can achieve state-of-the-art performance over the competitors. The\ncodes and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chenjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuntao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.03826","description":"<p>For unsupervised pretraining, mask-reconstruction pretraining (MRP)\napproaches randomly mask input patches and then reconstruct pixels or semantic\nfeatures of these masked patches via an auto-encoder. Then for a downstream\ntask, supervised fine-tuning the pretrained encoder remarkably surpasses the\nconventional supervised learning (SL) trained from scratch. However, it is\nstill unclear 1) how MRP performs semantic learning in the pretraining phase\nand 2) why it helps in downstream tasks. To solve these problems, we\ntheoretically show that on an auto-encoder of a two/one-layered convolution\nencoder/decoder, MRP can capture all discriminative semantics in the\npretraining dataset, and accordingly show its provable improvement over SL on\nthe classification downstream task. Specifically, we assume that pretraining\ndataset contains multi-view samples of ratio $1-\\mu$ and single-view samples of\nratio $\\mu$, where multi/single-view samples has multiple/single discriminative\nsemantics. Then for pretraining, we prove that 1) the convolution kernels of\nthe MRP encoder captures all discriminative semantics in the pretraining data;\nand 2) a convolution kernel captures at most one semantic. Accordingly, in the\ndownstream supervised fine-tuning, most semantics would be captured and\ndifferent semantics would not be fused together. This helps the downstream\nfine-tuned network to easily establish the relation between kernels and\nsemantic class labels. In this way, the fine-tuned encoder in MRP provably\nachieves zero test error with high probability for both multi-view and\nsingle-view test data. In contrast, as proved by~[3], conventional SL can only\nobtain a test accuracy between around $0.5\\mu$ for single-view test data. These\nresults together explain the benefits of MRP in downstream tasks. Experimental\nresults testify to multi-view data assumptions and our theoretical\nimplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiachun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Prompt Search. (arXiv:2206.04673v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04673","description":"<p>The size of vision models has grown exponentially over the last few years,\nespecially after the emergence of Vision Transformer. This has motivated the\ndevelopment of parameter-efficient tuning methods, such as learning adapter\nlayers or visual prompt tokens, which allow a tiny portion of model parameters\nto be trained whereas the vast majority obtained from pre-training are frozen.\nHowever, designing a proper tuning method is non-trivial: one might need to try\nout a lengthy list of design choices, not to mention that each downstream\ndataset often requires custom designs. In this paper, we view the existing\nparameter-efficient tuning methods as \"prompt modules\" and propose Neural\nprOmpt seArcH (NOAH), a novel approach that learns, for large vision models,\nthe optimal design of prompt modules through a neural architecture search\nalgorithm, specifically for each downstream dataset. By conducting extensive\nexperiments on over 20 vision datasets, we demonstrate that NOAH (i) is\nsuperior to individual prompt modules, (ii) has a good few-shot learning\nability, and (iii) is domain-generalizable. The code and models are available\nat https://github.com/Davidzhangyuanhan/NOAH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning self-calibrated optic disc and cup segmentation from multi-rater annotations. (arXiv:2206.05092v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.05092","description":"<p>The segmentation of optic disc(OD) and optic cup(OC) from fundus images is an\nimportant fundamental task for glaucoma diagnosis. In the clinical practice, it\nis often necessary to collect opinions from multiple experts to obtain the\nfinal OD/OC annotation. This clinical routine helps to mitigate the individual\nbias. But when data is multiply annotated, standard deep learning models will\nbe inapplicable. In this paper, we propose a novel neural network framework to\nlearn OD/OC segmentation from multi-rater annotations. The segmentation results\nare self-calibrated through the iterative optimization of multi-rater\nexpertness estimation and calibrated OD/OC segmentation. In this way, the\nproposed method can realize a mutual improvement of both tasks and finally\nobtain a refined segmentation result. Specifically, we propose Diverging\nModel(DivM) and Converging Model(ConM) to process the two tasks respectively.\nConM segments the raw image based on the multi-rater expertness map provided by\nDivM. DivM generates multi-rater expertness map from the segmentation mask\nprovided by ConM. The experiment results show that by recurrently running ConM\nand DivM, the results can be self-calibrated so as to outperform a range of\nstate-of-the-art(SOTA) multi-rater segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shang_F/0/1/0/all/0/1\">Fangxin Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wenshuo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Occlusion of Adding New Categories in Objection Detection. (arXiv:2206.05730v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05730","description":"<p>Building instance detection models that are data efficient and can handle\nrare object categories is an important challenge in computer vision. But data\ncollection methods and metrics are lack of research towards real scenarios\napplication using neural network. Here, we perform a systematic study of the\nObject Occlusion data collection and augmentation methods where we imitate\nobject occlusion relationship in target scenarios. However, we find that the\nsimple mechanism of object occlusion is good enough and can provide acceptable\naccuracy in real scenarios adding new category. We illustate that only adding\n15 images of new category in a half million training dataset with hundreds\ncategories, can give this new category 95% accuracy in unseen test dataset\nincluding thousands of images of this category.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Boyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meiyan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Shoulun Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Ranking Correlation of Super-net through Training Scheme from One-shot NAS to Few-shot NAS. (arXiv:2206.05896v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05896","description":"<p>The algorithms of one-shot neural architecture search(NAS) have been widely\nused to reduce computation consumption. However, because of the interference\namong the subnets in which weights are shared, the subnets inherited from these\nsuper-net trained by those algorithms have poor consistency in precision\nranking. To address this problem, we propose a step-by-step training super-net\nscheme from one-shot NAS to few-shot NAS. In the training scheme, we firstly\ntrain super-net in a one-shot way, and then we disentangle the weights of\nsuper-net by splitting them into multi-subnets and training them gradually.\nFinally, our method ranks 4th place in the CVPR2022 3rd Lightweight NAS\nChallenge Track1. Our code is available at\nhttps://github.com/liujiawei2333/CVPR2022-NAS-competition-Track-1-4th-solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weitai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping. (arXiv:2206.05963v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05963","description":"<p>In this paper, a novel solution is introduced for visual Simultaneous\nLocalization and Mapping (vSLAM) that is built up of Deep Learning components.\nThe proposed architecture is a highly modular framework in which each component\noffers state of the art results in their respective fields of vision-based deep\nlearning solutions. The paper shows that with the synergic integration of these\nindividual building blocks, a functioning and efficient all-through deep neural\n(ATDN) vSLAM system can be created. The Embedding Distance Loss function is\nintroduced and using it the ATDN architecture is trained. The resulting system\nmanaged to achieve 4.4% translation and 0.0176 deg/m rotational error on a\nsubset of the KITTI dataset. The proposed architecture can be used for\nefficient and low-latency autonomous driving (AD) aiding database creation as\nwell as a basis for autonomous vehicle (AV) control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szanto_M/0/1/0/all/0/1\">M&#xe1;ty&#xe1;s Sz&#xe1;nt&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogar_G/0/1/0/all/0/1\">Gy&#xf6;rgy R. Bog&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajta_L/0/1/0/all/0/1\">L&#xe1;szl&#xf3; Vajta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Human-in-the-loop System for Guiding DNNs Attention. (arXiv:2206.05981v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05981","description":"<p>Attention guidance is an approach to addressing dataset bias in deep\nlearning, where the model relies on incorrect features to make decisions.\nFocusing on image classification tasks, we propose an efficient\nhuman-in-the-loop system to interactively direct the attention of classifiers\nto the regions specified by users, thereby reducing the influence of\nco-occurrence bias and improving the transferability and interpretability of a\nDNN. Previous approaches for attention guidance require the preparation of\npixel-level annotations and are not designed as interactive systems. We present\na new interactive method to allow users to annotate images with simple clicks,\nand study a novel active learning strategy to significantly reduce the number\nof annotations. We conducted both a numerical evaluation and a user study to\nevaluate the proposed system on multiple datasets. Compared to the existing\nnon-active-learning approach which usually relies on huge amounts of\npolygon-based segmentation masks to fine-tune or train the DNNs, our system can\nsave lots of labor and money and obtain a fine-tuned network that works better\neven when the dataset is biased. The experiment results indicate that the\nproposed system is efficient, reasonable, and reliable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chia-Ming Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_T/0/1/0/all/0/1\">Takeo Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge Distillation. (arXiv:2206.06067v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06067","description":"<p>Knowledge distillation (KD) has shown very promising capabilities in\ntransferring learning representations from large models (teachers) to small\nmodels (students). However, as the capacity gap between students and teachers\nbecomes larger, existing KD methods fail to achieve better results. Our work\nshows that the 'prior knowledge' is vital to KD, especially when applying large\nteachers. Particularly, we propose the dynamic prior knowledge (DPK), which\nintegrates part of the teacher's features as the prior knowledge before the\nfeature distillation. This means that our method also takes the teacher's\nfeature as `input', not just `target'. Besides, we dynamically adjust the ratio\nof the prior knowledge during the training phase according to the feature gap,\nthus guiding the student in an appropriate difficulty. To evaluate the proposed\nmethod, we conduct extensive experiments on two image classification benchmarks\n(i.e. CIFAR100 and ImageNet) and an object detection benchmark (i.e. MS COCO).\nThe results demonstrate the superiority of our method in performance under\nvarying settings. More importantly, our DPK makes the performance of the\nstudent model is positively correlated with that of the teacher model, which\nmeans that we can further boost the accuracy of students by applying larger\nteachers. Our codes will be publicly available for the reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zengyu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kunlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunya Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Featurized Query R-CNN. (arXiv:2206.06258v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06258","description":"<p>The query mechanism introduced in the DETR method is changing the paradigm of\nobject detection and recently there are many query-based methods have obtained\nstrong object detection performance. However, the current query-based detection\npipelines suffer from the following two issues. Firstly, multi-stage decoders\nare required to optimize the randomly initialized object queries, incurring a\nlarge computation burden. Secondly, the queries are fixed after training,\nleading to unsatisfying generalization capability. To remedy the above issues,\nwe present featurized object queries predicted by a query generation network in\nthe well-established Faster R-CNN framework and develop a Featurized Query\nR-CNN. Extensive experiments on the COCO dataset show that our Featurized Query\nR-CNN obtains the best speed-accuracy trade-off among all R-CNN detectors,\nincluding the recent state-of-the-art Sparse R-CNN detector. The code is\navailable at \\url{https://github.com/hustvl/Featurized-QueryRCNN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}