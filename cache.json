{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.4","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing. (arXiv:2110.11338v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11338","description":"<p>Vision-language transformers (VL transformers) have shown impressive accuracy\nin cross-modal retrieval. However, most of the existing VL transformers use\nearly-interaction dataflow that computes a joint representation for the\ntext-image input. In the retrieval stage, such models need to infer on all the\nmatched text-image combinations, which causes high computing costs. The goal of\nthis paper is to decompose the early-interaction dataflow inside the\npre-trained VL transformer to achieve acceleration while maintaining its\noutstanding accuracy. To achieve this, we propose a novel Vision-language\nTransformer Decomposing (VLDeformer) to modify the VL transformer as an\nindividual encoder for a single image or text through contrastive learning,\nwhich accelerates retrieval speed by thousands of times. Meanwhile, we propose\nto compose bi-modal hard negatives for the contrastive learning objective,\nwhich enables the VLDeformer to maintain the outstanding accuracy of the\nbackbone VL transformer. Extensive experiments on COCO and Flickr30k datasets\ndemonstrate the superior performance of the proposed method. Considering both\neffectiveness and efficiency, VLDeformer provides a superior selection for\ncross-modal retrieval in the similar pre-training datascale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lisai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongfa Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yimeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhonghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dejiang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Joanna Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yunpeng Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCENIC: A JAX Library for Computer Vision Research and Beyond. (arXiv:2110.11403v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11403","description":"<p>Scenic is an open-source JAX library with a focus on Transformer-based models\nfor computer vision research and beyond. The goal of this toolkit is to\nfacilitate rapid experimentation, prototyping, and research of new vision\narchitectures and models. Scenic supports a diverse range of vision tasks\n(e.g., classification, segmentation, detection)and facilitates working on\nmulti-modal problems, along with GPU/TPU support for multi-host, multi-device\nlarge-scale training. Scenic also offers optimized implementations of\nstate-of-the-art research models spanning a wide range of modalities. Scenic\nhas been successfully used for numerous projects and published papers and\ncontinues serving as the library of choice for quick prototyping and\npublication of new research ideas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1\">Matthias Minderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SYNERGY: Building Task Bots at Scale Using Symbolic Knowledge and Machine Teaching. (arXiv:2110.11514v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11514","description":"<p>In this paper we explore the use of symbolic knowledge and machine teaching\nto reduce human data labeling efforts in building neural task bots. We propose\nSYNERGY, a hybrid learning framework where a task bot is developed in two\nsteps: (i) Symbolic knowledge to neural networks: Large amounts of simulated\ndialog sessions are generated based on task-specific symbolic knowledge which\nis represented as a task schema consisting of dialog flows and task-oriented\ndatabases. Then a pre-trained neural dialog model, SOLOIST, is fine-tuned on\nthe simulated dialogs to build a bot for the task. (ii) Neural learning: The\nfine-tuned neural dialog model is continually refined with a handful of real\ntask-specific dialogs via machine teaching, where training samples are\ngenerated by human teachers interacting with the task bot. We validate SYNERGY\non four dialog tasks. Experimental results show that SYNERGY maps task-specific\nknowledge into neural dialog models achieving greater diversity and coverage of\ndialog flows, and continually improves model performance with machine teaching,\nthus demonstrating strong synergistic effects of symbolic knowledge and machine\nteaching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Bridge between Training and Inference for Dialogue. (arXiv:2110.11560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11560","description":"<p>Although exposure bias has been widely studied in some NLP tasks, it faces\nits unique challenges in dialogue response generation, the representative\none-to-various generation scenario. In real human dialogue, there are many\nappropriate responses for the same context, not only with different\nexpressions, but also with different topics. Therefore, due to the much bigger\ngap between various ground-truth responses and the generated synthetic\nresponse, exposure bias is more challenging in dialogue generation task. What's\nmore, as MLE encourages the model to only learn the common words among\ndifferent ground-truth responses, but ignores the interesting and specific\nparts, exposure bias may further lead to the common response generation\nproblem, such as \"I don't know\" and \"HaHa?\" In this paper, we propose a novel\nadaptive switching mechanism, which learns to automatically transit between\nground-truth learning and generated learning regarding the word-level matching\nscore, such as the cosine similarity. Experimental results on both Chinese STC\ndataset and English Reddit dataset, show that our adaptive method achieves a\nsignificant improvement in terms of metric-based evaluation and human\nevaluation, as compared with the state-of-the-art exposure bias approaches.\nFurther analysis on NMT task also shows that our model can achieve a\nsignificant improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Counterfactuals via Latent Optimization and Shapley-Guided Search. (arXiv:2110.11589v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11589","description":"<p>We study the problem of generating counterfactual text for a classifier as a\nmeans for understanding and debugging classification. Given a textual input and\na classification model, we aim to minimally alter the text to change the\nmodel's prediction. White-box approaches have been successfully applied to\nsimilar problems in vision where one can directly optimize the continuous\ninput. Optimization-based approaches become difficult in the language domain\ndue to the discrete nature of text. We bypass this issue by directly optimizing\nin the latent space and leveraging a language model to generate candidate\nmodifications from optimized latent representations. We additionally use\nShapley values to estimate the combinatoric effect of multiple changes. We then\nuse these estimates to guide a beam search for the final counterfactual text.\nWe achieve favorable performance compared to recent white-box and black-box\nbaselines using human and automatic evaluations. Ablation studies show that\nboth latent optimization and the use of Shapley values improve success rate and\nthe quality of the generated counterfactuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pope_Q/0/1/0/all/0/1\">Quintin Pope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fern_X/0/1/0/all/0/1\">Xiaoli Z. Fern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCICAP: Generating Captions for Scientific Figures. (arXiv:2110.11624v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11624","description":"<p>Researchers use figures to communicate rich, complex information in\nscientific papers. The captions of these figures are critical to conveying\neffective messages. However, low-quality figure captions commonly occur in\nscientific articles and may decrease understanding. In this paper, we propose\nan end-to-end neural framework to automatically generate informative,\nhigh-quality captions for scientific figures. To this end, we introduce SCICAP,\na large-scale figure-caption dataset based on computer science arXiv papers\npublished between 2010 and 2020. After pre-processing - including figure-type\nclassification, sub-figure identification, text normalization, and caption text\nselection - SCICAP contained more than two million figures extracted from over\n290,000 papers. We then established baseline models that caption graph plots,\nthe dominant (19.2%) figure type. The experimental results showed both\nopportunities and steep challenges of generating captions for scientific\nfigures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ting-Yao/0/1/0/all/0/1\">Ting-Yao</a> (Edward)Hsu, <a href=\"http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1\">C. Lee Giles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ListReader: Extracting List-form Answers for Opinion Questions. (arXiv:2110.11692v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11692","description":"<p>Question answering (QA) is a high-level ability of natural language\nprocessing. Most extractive ma-chine reading comprehension models focus on\nfactoid questions (e.g., who, when, where) and restrict the output answer as a\nshort and continuous span in the original passage. However, in real-world\nscenarios, many questions are non-factoid (e.g., how, why) and their answers\nare organized in the list format that contains multiple non-contiguous spans.\nNaturally, existing extractive models are by design unable to answer such\nquestions. To address this issue, this paper proposes ListReader, a neural\nex-tractive QA model for list-form answer. In addition to learning the\nalignment between the question and content, we introduce a heterogeneous graph\nneural network to explicitly capture the associations among candidate segments.\nMoreover, our model adopts a co-extraction setting that can extract either\nspan- or sentence-level answers, allowing better applicability. Two large-scale\ndatasets of different languages are constructed to support this study.\nExperimental results show that our model considerably outperforms various\nstrong baselines. Further discussions provide an intuitive understanding of how\nour model works and where the performance gain comes from.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dongyao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Le Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Decoding Strategies for Increasing Specificity. (arXiv:2110.11850v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11850","description":"<p>Language models are known to produce vague and generic outputs. We propose\ntwo unsupervised decoding strategies based on either word-frequency or\npoint-wise mutual information to increase the specificity of any model that\noutputs a probability distribution over its vocabulary at generation time. We\ntest the strategies in a prompt completion task; with human evaluations, we\nfind that both strategies increase the specificity of outputs with only modest\ndecreases in sensibility. We also briefly present a summarization use case,\nwhere these strategies can produce more specific summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gero_K/0/1/0/all/0/1\">Katy Ilonka Gero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kedzie_C/0/1/0/all/0/1\">Chris Kedzie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Savvas Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilton_L/0/1/0/all/0/1\">Lydia Chilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction. (arXiv:2110.11864v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11864","description":"<p>Scanned documents in electronic health records (EHR) have been a challenge\nfor decades, and are expected to stay in the foreseeable future. Current\napproaches for processing often include image preprocessing, optical character\nrecognition (OCR), and text mining. However, there is limited work that\nevaluates the choice of image preprocessing methods, the selection of NLP\nmodels, and the role of document layout. The impact of each element remains\nunknown. We evaluated this method on a use case of two key indicators for sleep\napnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from\nscanned sleep study reports. Our data that included 955 manually annotated\nreports was secondarily utilized from a previous study in the University of\nTexas Medical Branch. We performed image preprocessing: gray-scaling followed\nby 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was\nimplemented with the Tesseract OCR engine. A total of seven Bag-of-Words models\n(Logistic Regression, Ridge Regression, Lasso Regression, Support Vector\nMachine, k-Nearest Neighbor, Na\\\"ive Bayes, and Random Forest) and three deep\nlearning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also\nevaluated the combinations of image preprocessing methods (gray-scaling, dilate\n&amp; erode, increased contrast by 20%, increased contrast by 60%), and two deep\nlearning architectures (with and without structured input that provides\ndocument layout information). Our proposed method using Clinical BERT reached\nan AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of\n0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper\nuse of image preprocessing and document layout could be beneficial to scanned\ndocument processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_E/0/1/0/all/0/1\">Enshuo Hsu</a> (1, 3, and 4), <a href=\"http://arxiv.org/find/cs/1/au:+Malagaris_I/0/1/0/all/0/1\">Ioannis Malagaris</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1\">Yong-Fang Kuo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sultana_R/0/1/0/all/0/1\">Rizwana Sultana</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_K/0/1/0/all/0/1\">Kirk Roberts</a> (3) ((1) Office of Biostatistics, (2) Division of Pulmonary, Critical Care and Sleep Medicine, Department of Internal Medicine, University of Texas Medical Branch, Galveston, Texas, USA. (3) School of Biomedical Informatics, University of Texas Health Science Center at Houston, Houston, Texas, USA. (4) Center for Outcomes Research, Houston Methodist, Houston, TX, USA.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLiText: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks. (arXiv:2110.11869v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11869","description":"<p>In natural language processing (NLP), state-of-the-art (SOTA) semi-supervised\nlearning (SSL) frameworks have shown great performance on deep pre-trained\nlanguage models such as BERT, and are expected to significantly reduce the\ndemand for manual labeling. However, our empirical studies indicate that these\nframeworks are not suitable for lightweight models such as TextCNN, LSTM and\netc. In this work, we develop a new SSL framework called FLiText, which stands\nfor Faster and Lighter semi-supervised Text classification. FLiText introduces\nan inspirer network together with the consistency regularization framework,\nwhich leverages a generalized regular constraint on the lightweight models for\nefficient SSL. As a result, FLiText obtains new SOTA performance for\nlightweight models across multiple SSL benchmarks on text classification.\nCompared with existing SOTA SSL methods on TextCNN, FLiText improves the\naccuracy of lightweight model TextCNN from 51.00% to 90.49% on IMDb, 39.8% to\n58.06% on Yelp-5, and from 55.3% to 65.08% on Yahoo. In addition, compared with\nthe fully supervised method on the full dataset, FLiText just uses less than 1%\nof labeled data to improve the accuracy by 6.59%, 3.94%, and 3.22% on the\ndatasets of IMDb, Yelp-5, and Yahoo respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhibin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1\">Pan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical text summarization using Conditional Generative Adversarial Network(CGAN). (arXiv:2110.11870v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11870","description":"<p>Text summarization in medicine can help doctors for reducing the time to\naccess important information from countless documents. The paper offers a\nsupervised extractive summarization method based on conditional generative\nadversarial networks using convolutional neural networks. Unlike previous\nmodels, which often use greedy methods to select sentences, we use a new\napproach for selecting sentences. Moreover, we provide a network for biomedical\nword embedding, which improves summarization. An essential contribution of the\npaper is introducing a new loss function for the discriminator, making the\ndiscriminator perform better. The proposed model achieves results comparable to\nthe state-of-the-art approaches, as determined by the ROUGE metric. Experiments\non the medical dataset show that the proposed method works on average 5% better\nthan the competing models and is more similar to the reference summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moravvej_S/0/1/0/all/0/1\">Seyed Vahid Moravvej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1\">Abdolreza Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safayani_M/0/1/0/all/0/1\">Mehran Safayani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An N-gram based approach to auto-extracting topics from research articles. (arXiv:2110.11879v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11879","description":"<p>A lot of manual work goes into identifying a topic for an article. With a\nlarge volume of articles, the manual process can be exhausting. Our approach\naims to address this issue by automatically extracting topics from the text of\nlarge Numbers of articles. This approach takes into account the efficiency of\nthe process. Based on existing N-gram analysis, our research examines how often\ncertain words appear in documents in order to support automatic topic\nextraction. In order to improve efficiency, we apply custom filtering standards\nto our research. Additionally, delete as many noncritical or irrelevant phrases\nas possible. In this way, we can ensure we are selecting unique keyphrases for\neach article, which capture its core idea. For our research, we chose to center\non the autonomous vehicle domain, since the research is relevant to our daily\nlives. We have to convert the PDF versions of most of the research papers into\neditable types of files such as TXT. This is because most of the research\npapers are only in PDF format. To test our proposed idea of automating,\nnumerous articles on robotics have been selected. Next, we evaluate our\napproach by comparing the result with that obtained manually.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linkai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Maoyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maomao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wennan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Dialogue System with AUDITED. (arXiv:2110.11881v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11881","description":"<p>We devise a multimodal conversation system for dialogue utterances composed\nof text, image or both modalities. We leverage Auxiliary UnsuperviseD vIsual\nand TExtual Data (AUDITED). To improve the performance of text-based task, we\nutilize translations of target sentences from English to French to form the\nassisted supervision. For the image-based task, we employ the DeepFashion\ndataset in which we seek nearest neighbor images of positive and negative\ntarget images of the MMD data. These nearest neighbors form the nearest\nneighbor embedding providing an external context for target images. We form two\nmethods to create neighbor embedding vectors, namely Neighbor Embedding by Hard\nAssignment (NEHA) and Neighbor Embedding by Soft Assignment (NESA) which\ngenerate context subspaces per target image. Subsequently, these subspaces are\nlearnt by our pipeline as a context for the target data. We also propose a\ndiscriminator which switches between the image- and text-based tasks. We show\nimprovements over baselines on the large-scale Multimodal Dialogue Dataset\n(MMD) and SIMMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tas_Y/0/1/0/all/0/1\">Yusuf Tas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges in Procedural Multimodal Machine Comprehension:A Novel Way To Benchmark. (arXiv:2110.11899v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11899","description":"<p>We focus on Multimodal Machine Reading Comprehension (M3C) where a model is\nexpected to answer questions based on given passage (or context), and the\ncontext and the questions can be in different modalities. Previous works such\nas RecipeQA have proposed datasets and cloze-style tasks for evaluation.\nHowever, we identify three critical biases stemming from the question-answer\ngeneration process and memorization capabilities of large deep models. These\nbiases makes it easier for a model to overfit by relying on spurious\ncorrelations or naive data patterns. We propose a systematic framework to\naddress these biases through three Control-Knobs that enable us to generate a\ntest bed of datasets of progressive difficulty levels. We believe that our\nbenchmark (referred to as Meta-RecipeQA) will provide, for the first time, a\nfine grained estimate of a model's generalization capabilities. We also propose\na general M3C model that is used to realize several prior SOTA models and\nmotivate a novel hierarchical transformer based reasoning network (HTRN). We\nperform a detailed evaluation of these models with different language and\nvisual features on our benchmark. We observe a consistent improvement with HTRN\nover SOTA (~18% in Visual Cloze task and ~13% in average over all the tasks).\nWe also observe a drop in performance across all the models when testing on\nRecipeQA and proposed Meta-RecipeQA (e.g. 83.6% versus 67.1% for HTRN), which\nshows that the proposed dataset is relatively less biased. We conclude by\nhighlighting the impact of the control knobs with some quantitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1\">Pritish Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1\">Karan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1\">Ajay Divakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double Trouble: How to not explain a text classifier's decisions using counterfactuals synthesized by masked language models?. (arXiv:2110.11929v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11929","description":"<p>Explaining how important each input feature is to a classifier's decision is\ncritical in high-stake applications. An underlying principle behind dozens of\nexplanation methods is to take the prediction difference between\nbefore-and-after an input feature (here, a token) is removed as its attribution\n- the individual treatment effect in causal inference. A recent method called\nInput Marginalization (IM) (Kim et al., 2020) uses BERT to replace a token -\ni.e. simulating the do(.) operator - yielding more plausible counterfactuals.\nHowever, our rigorous evaluation using five metrics and on three datasets found\nIM explanations to be consistently more biased, less accurate, and less\nplausible than those derived from simply deleting a word.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts. (arXiv:2110.11934v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11934","description":"<p>Substantial amounts of work are required to clean large collections of\ndigitized books for NLP analysis, both because of the presence of errors in the\nscanned text and the presence of duplicate volumes in the corpora. In this\npaper, we consider the issue of deduplication in the presence of optical\ncharacter recognition (OCR) errors. We present methods to handle these errors,\nevaluated on a collection of 19,347 texts from the Project Gutenberg dataset\nand 96,635 texts from the HathiTrust Library. We demonstrate that improvements\nin language models now enable the detection and correction of OCR errors\nwithout consideration of the scanning image itself. The inconsistencies found\nby aligning pairs of scans of the same underlying work provides training data\nto build models for detecting and correcting errors. We identify the canonical\nversion for each of 17,136 repeatedly-scanned books from 58,808 scans. Finally,\nwe investigate methods to detect and correct errors in single-copy texts. We\nshow that on average, our method corrects over six times as many errors as it\nintroduces. We also provide interesting analysis on the relation between\nscanning quality and other factors such as location and publication year.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Allen Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pethe_C/0/1/0/all/0/1\">Charuta Pethe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1\">Naoya Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1\">Steve Skiena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Learning Assessment through Multimodal Analysis of Reading Behaviour and Language Comprehension. (arXiv:2110.11938v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11938","description":"<p>Reading comprehension, which has been defined as gaining an understanding of\nwritten text through a process of translating grapheme into meaning, is an\nimportant academic skill. Other language learning skills - writing, speaking\nand listening, all are connected to reading comprehension. There have been\nseveral measures proposed by researchers to automate the assessment of\ncomprehension skills for second language (L2) learners, especially English as\nSecond Language (ESL) and English as Foreign Language (EFL) learners. However,\ncurrent methods measure particular skills without analysing the impact of\nreading frequency on comprehension skills. In this dissertation, we show how\ndifferent skills could be measured and scored automatically. We also\ndemonstrate, using example experiments on multiple forms of learners'\nresponses, how frequent reading practices could impact on the variables of\nmultimodal skills (reading pattern, writing, and oral fluency).\n</p>\n<p>This thesis comprises of five studies. The first and second studies are based\non eye-tracking data collected from EFL readers in repeated reading (RR)\nsessions. The third and fourth studies are to evaluate free-text summary\nwritten by EFL readers in repeated reading sessions. The fifth and last study,\ndescribed in the sixth chapter of the thesis, is to evaluate recorded oral\nsummaries recited by EFL readers in repeated reading sessions.\n</p>\n<p>In a nutshell, through this dissertation, we show that multimodal skills of\nlearners could be assessed to measure their comprehension skills as well as to\nmeasure the effect of repeated readings on these skills in the course of time,\nby finding significant features and by applying machine learning techniques\nwith a combination of statistical models such as LMER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barnwal_S/0/1/0/all/0/1\">Santosh Kumar Barnwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving BERT with Self-Supervised Attention. (arXiv:2004.03808v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.03808","description":"<p>One of the most popular paradigms of applying large pre-trained NLP models\nsuch as BERT is to fine-tune it on a smaller dataset. However, one challenge\nremains as the fine-tuned model often overfits on smaller datasets. A symptom\nof this phenomenon is that irrelevant or misleading words in the sentence,\nwhich are easy to understand for human beings, can substantially degrade the\nperformance of these finetuned BERT models. In this paper, we propose a novel\ntechnique, called Self-Supervised Attention (SSA) to help facilitate this\ngeneralization challenge. Specifically, SSA automatically generates weak,\ntoken-level attention labels iteratively by probing the fine-tuned model from\nthe previous iteration. We investigate two different ways of integrating SSA\ninto BERT and propose a hybrid approach to combine their benefits. Empirically,\nthrough a variety of public datasets, we illustrate significant performance\nimprovement using our SSA-enhanced BERT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiren Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_X/0/1/0/all/0/1\">Xiaoyu Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiangang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions. (arXiv:2102.05379v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2102.05379","description":"<p>Generative flows and diffusion models have been predominantly trained on\nordinal data, for example natural images. This paper introduces two extensions\nof flows and diffusion for categorical data such as language or image\nsegmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined\nby a composition of a continuous distribution (such as a normalizing flow), and\nan argmax function. To optimize this model, we learn a probabilistic inverse\nfor the argmax that lifts the categorical data to a continuous space.\nMultinomial Diffusion gradually adds categorical noise in a diffusion process,\nfor which the generative denoising process is learned. We demonstrate that our\nmethod outperforms existing dequantization approaches on text modelling and\nmodelling on image segmentation maps in log-likelihood.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Hoogeboom_E/0/1/0/all/0/1\">Emiel Hoogeboom</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_D/0/1/0/all/0/1\">Didrik Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jaini_P/0/1/0/all/0/1\">Priyank Jaini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Forre_P/0/1/0/all/0/1\">Patrick Forr&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress. (arXiv:2104.07123v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07123","description":"<p>Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the\ntasks of sentiment and emotion, as well as physiological-emotion and\nemotion-based stress recognition through more comprehensively integrating the\naudio-visual, language, and biological signal modalities. The purpose of MuSe\n2021 is to bring together communities from different disciplines; mainly, the\naudio-visual emotion recognition community (signal-based), the sentiment\nanalysis community (symbol-based), and the health informatics community. We\npresent four distinct sub-challenges: MuSe-Wilder and MuSe-Stress which focus\non continuous emotion (valence and arousal) prediction; MuSe-Sent, in which\nparticipants recognise five classes each for valence and arousal; and\nMuSe-Physio, in which the novel aspect of `physiological-emotion' is to be\npredicted. For this years' challenge, we utilise the MuSe-CaR dataset focusing\non user-generated reviews and introduce the Ulm-TSST dataset, which displays\npeople in stressful depositions. This paper also provides detail on the\nstate-of-the-art feature sets extracted from these datasets for utilisation by\nour baseline model, a Long Short-Term Memory-Recurrent Neural Network. For each\nsub-challenge, a competitive baseline for participants is set; namely, on test,\nwe report a Concordance Correlation Coefficient (CCC) of .4616 CCC for\nMuSe-Wilder; .4717 CCC for MuSe-Stress, and .4606 CCC for MuSe-Physio. For\nMuSe-Sent an F1 score of 32.82 % is obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sertolli_B/0/1/0/all/0/1\">Benjamin Sertolli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Messner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphTMT: Unsupervised Graph-based Topic Modeling from Video Transcripts. (arXiv:2105.01466v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01466","description":"<p>To unfold the tremendous amount of multimedia data uploaded daily to social\nmedia platforms, effective topic modeling techniques are needed. Existing work\ntends to apply topic models on written text datasets. In this paper, we propose\na topic extractor on video transcripts. Exploiting neural word embeddings\nthrough graph-based clustering, we aim to improve usability and semantic\ncoherence. Unlike most topic models, this approach works without knowing the\ntrue number of topics, which is important when no such assumption can or should\nbe made. Experimental results on the real-life multimodal dataset MuSe-CaR\ndemonstrates that our approach GraphTMT extracts coherent and meaningful topics\nand outperforms baseline methods. Furthermore, we successfully demonstrate the\napplicability of our approach on the popular Citysearch corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Jason Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Hierarchical Attention for Answering Complex Questions over Long Documents. (arXiv:2106.00200v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00200","description":"<p>We propose a new model, DocHopper, that iteratively attends to different\nparts of long, hierarchically structured documents to answer complex questions.\nSimilar to multi-hop question-answering (QA) systems, at each step, DocHopper\nuses a query $q$ to attend to information from a document, combines this\n``retrieved'' information with $q$ to produce the next query. However, in\ncontrast to most previous multi-hop QA systems, DocHopper is able to\n``retrieve'' either short passages or long sections of the document, thus\nemulating a multi-step process of ``navigating'' through a long document to\nanswer a question. To enable this novel behavior, DocHopper does not combine\ndocument information with $q$ by concatenating text to the text of $q$, but by\ncombining a compact neural representation of $q$ with a compact neural\nrepresentation of a hierarchical part of the document, which can potentially be\nquite large. We experiment with DocHopper on four different QA tasks that\nrequire reading long and complex documents to answer multi-hop questions, and\nshow that DocHopper achieves state-of-the-art results on three of the datasets.\nAdditionally, DocHopper is efficient at inference time, being 3--10 times\nfaster than the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02636","description":"<p>As humans, we understand events in the visual world contextually, performing\nmultimodal reasoning across time to make inferences about the past, present,\nand future. We introduce MERLOT, a model that learns multimodal script\nknowledge by watching millions of YouTube videos with transcribed speech -- in\nan entirely label-free, self-supervised manner. By pretraining with a mix of\nboth frame-level (spatial) and video-level (temporal) objectives, our model not\nonly learns to match images to temporally corresponding words, but also to\ncontextualize what is happening globally over time. As a result, MERLOT\nexhibits strong out-of-the-box representations of temporal commonsense, and\nachieves state-of-the-art performance on 12 different video QA datasets when\nfinetuned. It also transfers well to the world of static images, allowing\nmodels to reason about the dynamic context behind visual scenes. On Visual\nCommonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,\noutperforming state-of-the-art models of similar size by over 3%, even those\nthat make heavy use of auxiliary supervised data (like object bounding boxes).\n</p>\n<p>Ablation analyses demonstrate the complementary importance of: 1) training on\nvideos versus static images; 2) scaling the magnitude and diversity of the\npretraining video corpus; and 3) using diverse objectives that encourage\nfull-stack multimodal reasoning, from the recognition to cognition level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jize Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Large Scale Molecular Language Representations Capture Important Structural Information?. (arXiv:2106.09553v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09553","description":"<p>Predicting the chemical properties of a molecule is of great importance in\nmany applications, including drug discovery and material design. Machine\nlearning based molecular property prediction holds the promise of enabling\naccurate predictions at much less computationally complex cost when compared\nto, for example, Density Functional Theory (DFT) calculations. Various\nrepresentation learning methods in a supervised setting, including the features\nextracted using graph neural nets, have emerged for such tasks. However, the\nvast chemical space and the limited availability of labels make supervised\nlearning challenging, calling for learning a general-purpose molecular\nrepresentation. Recently, pre-trained transformer-based language models on\nlarge unlabeled corpus have produced state-of-the-art results in many\ndownstream natural language processing tasks. Inspired by this development, we\npresent molecular embeddings obtained by training an efficient transformer\nencoder model, MoLFormer. This model employs a linear attention mechanism\ncoupled with highly parallelized training on SMILES sequences of 1.1 billion\nunlabeled molecules from the PubChem and ZINC datasets. Experiments show that\nthe learned molecular representation outperforms supervised and unsupervised\ngraph neural net baselines on several regression and classification tasks from\n10 benchmark datasets, while performing competitively on others. Further\nanalyses, specifically through the lens of attention, demonstrate that\nMoLFormer indeed learns a molecule's local and global structural aspects. These\nresults provide encouraging evidence that large-scale molecular language models\ncan capture sufficient structural information to be able to predict diverse\nmolecular properties, including quantum-chemical properties\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1\">Jerret Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1\">Brian Belgodere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages. (arXiv:2108.07127v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07127","description":"<p>We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal Text Classification. (arXiv:2109.07234v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07234","description":"<p>We aim to highlight an interesting trend to contribute to the ongoing debate\naround advances within legal Natural Language Processing. Recently, the focus\nfor most legal text classification tasks has shifted towards large pre-trained\ndeep learning models such as BERT. In this paper, we show that a more\ntraditional approach based on Support Vector Machine classifiers reaches\nsurprisingly competitive performance with BERT-based models on the\nclassification tasks in the LexGLUE benchmark. We also highlight that error\nreduction obtained by using specialised BERT-based models over baselines is\nnoticeably smaller in the legal domain when compared to general language tasks.\nWe present and discuss three hypotheses as potential explanations for these\nresults to support future discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonsus_M/0/1/0/all/0/1\">Marc Alphonsus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08722","description":"<p>Prerequisite chain learning helps people acquire new knowledge efficiently.\nWhile people may quickly determine learning paths over concepts in a domain,\nfinding such paths in other domains can be challenging. We introduce\nDomain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this\ncross-domain prerequisite chain learning task efficiently. Our novel model\nconsists of a variational graph autoencoder (VGAE) and a domain discriminator.\nThe VGAE is trained to predict concept relations through link prediction, while\nthe domain discriminator takes both source and target domain data as input and\nis trained to predict domain labels. Most importantly, this method only needs\nsimple homogeneous graphs as input, compared with the current state-of-the-art\nmodel. We evaluate our model on the LectureBankCD dataset, and results show\nthat our model outperforms recent graph-based benchmarks while using only 1/10\nof graph scale and 1/3 computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13238","description":"<p>The design of widespread vision-and-language datasets and pre-trained\nencoders directly adopts, or draws inspiration from, the concepts and images of\nImageNet. While one can hardly overestimate how much this benchmark contributed\nto progress in computer vision, it is mostly derived from lexical databases and\nimage queries in English, resulting in source material with a North American or\nWestern European bias. Therefore, we devise a new protocol to construct an\nImageNet-style hierarchy representative of more languages and cultures. In\nparticular, we let the selection of both concepts and images be entirely driven\nby native speakers, rather than scraping them automatically. Specifically, we\nfocus on a typologically diverse set of languages, namely, Indonesian, Mandarin\nChinese, Swahili, Tamil, and Turkish. On top of the concepts and images\nobtained through this new protocol, we create a multilingual dataset for\n{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting\nstatements from native speaker annotators about pairs of images. The task\nconsists of discriminating whether each grounded statement is true or false. We\nestablish a series of baselines using state-of-the-art models and find that\ntheir cross-lingual transfer performance lags dramatically behind supervised\nperformance in English. These results invite us to reassess the robustness and\naccuracy of current state-of-the-art models beyond a narrow domain, but also\nopen up new exciting challenges for the development of truly multilingual and\nmulticultural systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Key Point Analysis via Contrastive Learning and Extractive Argument Summarization. (arXiv:2109.15086v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.15086","description":"<p>Key point analysis is the task of extracting a set of concise and high-level\nstatements from a given collection of arguments, representing the gist of these\narguments. This paper presents our proposed approach to the Key Point Analysis\nshared task, collocated with the 8th Workshop on Argument Mining. The approach\nintegrates two complementary components. One component employs contrastive\nlearning via a siamese neural network for matching arguments to key points; the\nother is a graph-based extractive summarization model for generating key\npoints. In both automatic and manual evaluation, our approach was ranked best\namong all submissions to the shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1\">Milad Alshomary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurcke_T/0/1/0/all/0/1\">Timon Gurcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_P/0/1/0/all/0/1\">Philipp Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spliethover_M/0/1/0/all/0/1\">Maximilian Splieth&#xf6;ver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimiano_P/0/1/0/all/0/1\">Philipp Cimiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Personality Detection Tools for Software Engineering Research: How Far Can We Go?. (arXiv:2110.05035v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2110.05035","description":"<p>Assessing the personality of software engineers may help to match individual\ntraits with the characteristics of development activities such as code review\nand testing, as well as support managers in team composition. However,\nself-assessment questionnaires are not a practical solution for collecting\nmultiple observations on a large scale. Instead, automatic personality\ndetection, while overcoming these limitations, is based on off-the-shelf\nsolutions trained on non-technical corpora, which might not be readily\napplicable to technical domains like Software Engineering (SE). In this paper,\nwe first assess the performance of general-purpose personality detection tools\nwhen applied to a technical corpus of developers' emails retrieved from the\npublic archives of the Apache Software Foundation. We observe a general low\naccuracy of predictions and an overall disagreement among the tools. Second, we\nreplicate two previous research studies in SE by replacing the personality\ndetection tool used to infer developers' personalities from pull-request\ndiscussions and emails. We observe that the original results are not confirmed,\ni.e., changing the tool used in the original study leads to diverging\nconclusions. Our results suggest a need for personality detection tools\nspecially targeted for the software engineering domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calefato_F/0/1/0/all/0/1\">Fabio Calefato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanubile_F/0/1/0/all/0/1\">Filippo Lanubile</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Importance Estimation from Multiple Perspectives for Keyphrase Extraction. (arXiv:2110.09749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.09749","description":"<p>Keyphrase extraction is a fundamental task in Natural Language Processing,\nwhich usually contains two main parts: candidate keyphrase extraction and\nkeyphrase importance estimation. From the view of human understanding\ndocuments, we typically measure the importance of phrase according to its\nsyntactic accuracy, information saliency, and concept consistency\nsimultaneously. However, most existing keyphrase extraction approaches only\nfocus on the part of them, which leads to biased results. In this paper, we\npropose a new approach to estimate the importance of keyphrase from multiple\nperspectives (called as \\textit{KIEMP}) and further improve the performance of\nkeyphrase extraction. Specifically, \\textit{KIEMP} estimates the importance of\nphrase with three modules: a chunking module to measure its syntactic accuracy,\na ranking module to check its information saliency, and a matching module to\njudge the concept (i.e., topic) consistency between phrase and the whole\ndocument. These three modules are seamlessly jointed together via an end-to-end\nmulti-task learning model, which is helpful for three parts to enhance each\nother and balance the effects of three perspectives. Experimental results on\nsix benchmark datasets show that \\textit{KIEMP} outperforms the existing\nstate-of-the-art keyphrase extraction approaches in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lin Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing. (arXiv:2110.11338v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11338","description":"<p>Vision-language transformers (VL transformers) have shown impressive accuracy\nin cross-modal retrieval. However, most of the existing VL transformers use\nearly-interaction dataflow that computes a joint representation for the\ntext-image input. In the retrieval stage, such models need to infer on all the\nmatched text-image combinations, which causes high computing costs. The goal of\nthis paper is to decompose the early-interaction dataflow inside the\npre-trained VL transformer to achieve acceleration while maintaining its\noutstanding accuracy. To achieve this, we propose a novel Vision-language\nTransformer Decomposing (VLDeformer) to modify the VL transformer as an\nindividual encoder for a single image or text through contrastive learning,\nwhich accelerates retrieval speed by thousands of times. Meanwhile, we propose\nto compose bi-modal hard negatives for the contrastive learning objective,\nwhich enables the VLDeformer to maintain the outstanding accuracy of the\nbackbone VL transformer. Extensive experiments on COCO and Flickr30k datasets\ndemonstrate the superior performance of the proposed method. Considering both\neffectiveness and efficiency, VLDeformer provides a superior selection for\ncross-modal retrieval in the similar pre-training datascale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lisai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongfa Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yimeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhonghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dejiang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Joanna Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yunpeng Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESOD:Edge-based Task Scheduling for Object Detection. (arXiv:2110.11342v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11342","description":"<p>Object Detection on the mobile system is a challenge in terms of everything.\nNowadays, many object detection models have been designed, and most of them\nconcentrate on precision. However, the computation burden of those models on\nmobile systems is unacceptable. Researchers have designed some lightweight\nnetworks for mobiles by sacrificing precision. We present a novel edge-based\ntask scheduling framework for object detection (termed as ESOD). In detail, we\ntrain a DNN model (termed as pre-model) to predict which object detection model\nto use for the coming task and offloads to which edge servers by physical\ncharacteristics of the image task (e.g., brightness, saturation). The results\nshow that ESOD can reduce latency and energy consumption by an average of\n22.13% and 29.60% and improve the mAP to 45.8(with 0.9 mAP better),\nrespectively, compared with the SOTA DETR model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanli Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decentralised Person Re-Identification with Selective Knowledge Aggregation. (arXiv:2110.11384v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11384","description":"<p>Existing person re-identification (Re-ID) methods mostly follow a centralised\nlearning paradigm which shares all training data to a collection for model\nlearning. This paradigm is limited when data from different sources cannot be\nshared due to privacy concerns. To resolve this problem, two recent works have\nintroduced decentralised (federated) Re-ID learning for constructing a globally\ngeneralised model (server)without any direct access to local training data nor\nshared data across different source domains (clients). However, these methods\nare poor on how to adapt the generalised model to maximise its performance on\nindividual client domain Re-ID tasks having different Re-ID label spaces, due\nto a lack of understanding of data heterogeneity across domains. We call this\npoor 'model personalisation'. In this work, we present a new Selective\nKnowledge Aggregation approach to decentralised person Re-ID to optimise the\ntrade-off between model personalisation and generalisation. Specifically, we\nincorporate attentive normalisation into the normalisation layers in a deep\nReID model and propose to learn local normalisation layers specific to each\ndomain, which are decoupled from the global model aggregation in federated\nRe-ID learning. This helps to preserve model personalisation knowledge on each\nlocal client domain and learn instance-specific information. Further, we\nintroduce a dual local normalisation mechanism to learn generalised\nnormalisation layers in each local model, which are then transmitted to the\nglobal model for central aggregation. This facilitates selective knowledge\naggregation on the server to construct a global generalised model for\nout-of-the-box deployment on unseen novel domains. Extensive experiments on\neight person Re-ID datasets show that the proposed approach to decentralised\nRe-ID significantly outperforms the state-of-the-art decentralised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shitong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guile Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEX: Domain Embedding Expansion for Generalized Person Re-identification. (arXiv:2110.11391v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11391","description":"<p>In recent years, supervised Person Re-identification (Person ReID) approaches\nhave demonstrated excellent performance. However, when these methods are\napplied to inputs from a different camera network, they typically suffer from\nsignificant performance degradation. Different from most domain adaptation (DA)\napproaches addressing this issue, we focus on developing a domain\ngeneralization (DG) Person ReID model that can be deployed without additional\nfine-tuning or adaptation. In this paper, we propose the Domain Embedding\nExpansion (DEX) module. DEX dynamically manipulates and augments deep features\nbased on person and domain labels during training, significantly improving the\ngeneralization capability and robustness of Person ReID models to unseen\ndomains. We also developed a light version of DEX (DEXLite), applying negative\nsampling techniques to scale to larger datasets and reduce memory usage for\nmulti-branch networks. Our proposed DEX and DEXLite can be combined with many\nexisting methods, Bag-of-Tricks (BagTricks), the Multi-Granularity Network\n(MGN), and Part-Based Convolutional Baseline (PCB), in a plug-and-play manner.\nWith DEX and DEXLite, existing methods can gain significant improvements when\ntested on other unseen datasets, thereby demonstrating the general\napplicability of our method. Our solution outperforms the state-of-the-art DG\nPerson ReID methods in all large-scale benchmarks as well as in most the\nsmall-scale benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ang_E/0/1/0/all/0/1\">Eugene P.W. Ang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1\">Lin Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning. (arXiv:2110.11395v1 [cs.LG])","link":"http://arxiv.org/abs/2110.11395","description":"<p>Pruning neural networks reduces inference time and memory costs. On standard\nhardware, these benefits will be especially prominent if coarse-grained\nstructures, like feature maps, are pruned. We devise two novel saliency-based\nmethods for second-order structured pruning (SOSP) which include correlations\namong all structures and layers. Our main method SOSP-H employs an innovative\nsecond-order approximation, which enables saliency evaluations by fast\nHessian-vector products. SOSP-H thereby scales like a first-order method\ndespite taking into account the full Hessian. We validate SOSP-H by comparing\nit to our second method SOSP-I that uses a well-established Hessian\napproximation, and to numerous state-of-the-art methods. While SOSP-H performs\non par or better in terms of accuracy, it has clear advantages in terms of\nscalability and efficiency. This allowed us to scale SOSP-H to large-scale\nvision tasks, even though it captures correlations across all layers of the\nnetwork. To underscore the global nature of our pruning methods, we evaluate\ntheir performance not only by removing structures from a pretrained network,\nbut also by detecting architectural bottlenecks. We show that our algorithms\nallow to systematically reveal architectural bottlenecks, which we then remove\nto further increase the accuracy of the networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nonnenmacher_M/0/1/0/all/0/1\">Manuel Nonnenmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeil_T/0/1/0/all/0/1\">Thomas Pfeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinwart_I/0/1/0/all/0/1\">Ingo Steinwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reeb_D/0/1/0/all/0/1\">David Reeb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Driven Reconstruction Technique based on Newton's Method for Emission Tomography. (arXiv:2110.11396v1 [eess.IV])","link":"http://arxiv.org/abs/2110.11396","description":"<p>In this work, we present the Deep Newton Reconstruction Network (DNR-Net), a\nhybrid data-driven reconstruction technique for emission tomography inspired by\nNewton's method, a well-known iterative optimization algorithm. The DNR-Net\nemploys prior information about the tomographic problem provided by the\nprojection operator while utilizing deep learning approaches to a) imitate\nNewton's method by approximating the Newton descent direction and b) provide\ndata-driven regularisation. We demonstrate that DNR-Net is capable of providing\nhigh-quality image reconstructions using data from SPECT phantom simulations by\napplying it to reconstruct images from noisy sinograms, each one containing 24\nprojections. The Structural Similarity Index (SSIM) and the Contrast-to-Noise\nratio (CNR) were used to quantify the image quality. We also compare our\nresults to those obtained by the OSEM method. According to the quantitative\nresults, the DNR-Net produces reconstructions comparable to the ones produced\nby OSEM while featuring higher contrast and less noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Koutsantonis_L/0/1/0/all/0/1\">Loizos Koutsantonis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carneiro_T/0/1/0/all/0/1\">Tiago Carneiro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kieffer_E/0/1/0/all/0/1\">Emmanuel Kieffer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pinel_F/0/1/0/all/0/1\">Frederic Pinel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouvry_P/0/1/0/all/0/1\">Pascal Bouvry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel redundancy and overlap in convolutional neural networks with channel-wise NNK graphs. (arXiv:2110.11400v1 [cs.LG])","link":"http://arxiv.org/abs/2110.11400","description":"<p>Feature spaces in the deep layers of convolutional neural networks (CNNs) are\noften very high-dimensional and difficult to interpret. However, convolutional\nlayers consist of multiple channels that are activated by different types of\ninputs, which suggests that more insights may be gained by studying the\nchannels and how they relate to each other. In this paper, we first analyze\ntheoretically channel-wise non-negative kernel (CW-NNK) regression graphs,\nwhich allow us to quantify the overlap between channels and, indirectly, the\nintrinsic dimension of the data representation manifold. We find that\nredundancy between channels is significant and varies with the layer depth and\nthe level of regularization during training. Additionally, we observe that\nthere is a correlation between channel overlap in the last convolutional layer\nand generalization performance. Our experimental results demonstrate that these\ntechniques can lead to a better understanding of deep representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonet_D/0/1/0/all/0/1\">David Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1\">Antonio Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Hidalgo_J/0/1/0/all/0/1\">Javier Ruiz-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekkizhar_S/0/1/0/all/0/1\">Sarath Shekkizhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCENIC: A JAX Library for Computer Vision Research and Beyond. (arXiv:2110.11403v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11403","description":"<p>Scenic is an open-source JAX library with a focus on Transformer-based models\nfor computer vision research and beyond. The goal of this toolkit is to\nfacilitate rapid experimentation, prototyping, and research of new vision\narchitectures and models. Scenic supports a diverse range of vision tasks\n(e.g., classification, segmentation, detection)and facilitates working on\nmulti-modal problems, along with GPU/TPU support for multi-host, multi-device\nlarge-scale training. Scenic also offers optimized implementations of\nstate-of-the-art research models spanning a wide range of modalities. Scenic\nhas been successfully used for numerous projects and published papers and\ncontinues serving as the library of choice for quick prototyping and\npublication of new research ideas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1\">Matthias Minderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Illiterate DALL$\\cdot$E Learns to Compose. (arXiv:2110.11405v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11405","description":"<p>Although DALL$\\cdot$E has shown an impressive ability of composition-based\nsystematic generalization in image generation, it requires the dataset of\ntext-image pairs and the compositionality is provided by the text. In contrast,\nobject-centric representation models like the Slot Attention model learn\ncomposable representations without the text prompt. However, unlike\nDALL$\\cdot$E its ability to systematically generalize for zero-shot generation\nis significantly limited. In this paper, we propose a simple but novel\nslot-based autoencoding architecture, called SLATE, for combining the best of\nboth worlds: learning object-centric representations that allows systematic\ngeneralization in zero-shot image generation without text. As such, this model\ncan also be seen as an illiterate DALL$\\cdot$E model. Unlike the pixel-mixture\ndecoders of existing object-centric representation models, we propose to use\nthe Image GPT decoder conditioned on the slots for capturing complex\ninteractions among the slots and pixels. In experiments, we show that this\nsimple and easy-to-implement architecture not requiring a text prompt achieves\nsignificant improvement in in-distribution and out-of-distribution (zero-shot)\nimage generation and qualitatively comparable or better slot-attention\nstructure than the models based on mixture decoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gautam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Fei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-Data Pipelines for Machine Learning Applications. (arXiv:2110.11407v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11407","description":"<p>Data pipelines are an essential component for end-to-end solutions that take\nmachine learning algorithms to production. Engineering data pipelines for\nvideo-sequences poses several challenges including isolation of key-frames from\nvideo sequences that are high quality and represent significant variations in\nthe scene. Manual isolation of such quality key-frames can take hours of\nsifting through hours worth of video data. In this work, we present a data\npipeline framework that can automate this process of manual frame sifting in\nvideo sequences by controlling the fraction of frames that can be removed based\non image quality and content type. Additionally, the frames that are retained\ncan be automatically tagged per sequence, thereby simplifying the process of\nautomated data retrieval for future ML model deployments. We analyze the\nperformance of the proposed video-data pipeline for versioned deployment and\nmonitoring for object detection algorithms that are trained on outdoor\nautonomous driving video sequences. The proposed video-data pipeline can retain\nanywhere between 0.1-20% of the all input frames that are representative of\nhigh image quality and high variations in content. This frame selection,\nautomated scene tagging followed by model verification can be completed in\nunder 30 seconds for 22 video-sequences under analysis in this work. Thus, the\nproposed framework can be scaled to additional video-sequence data sets for\nautomating ML versioned deployments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sohini Roychowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_J/0/1/0/all/0/1\">James Y. Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PROVES: Establishing Image Provenance using Semantic Signatures. (arXiv:2110.11411v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11411","description":"<p>Modern AI tools, such as generative adversarial networks, have transformed\nour ability to create and modify visual data with photorealistic results.\nHowever, one of the deleterious side-effects of these advances is the emergence\nof nefarious uses in manipulating information in visual data, such as through\nthe use of deep fakes. We propose a novel architecture for preserving the\nprovenance of semantic information in images to make them less susceptible to\ndeep fake attacks. Our architecture includes semantic signing and verification\nsteps. We apply this architecture to verifying two types of semantic\ninformation: individual identities (faces) and whether the photo was taken\nindoors or outdoors. Verification accounts for a collection of common image\ntransformation, such as translation, scaling, cropping, and small rotations,\nand rejects adversarial transformations, such as adversarially perturbed or, in\nthe case of face verification, swapped faces. Experiments demonstrate that in\nthe case of provenance of faces in an image, our approach is robust to\nblack-box adversarial transformations (which are rejected) as well as benign\ntransformations (which are accepted), with few false negatives and false\npositives. Background verification, on the other hand, is susceptible to\nblack-box adversarial examples, but becomes significantly more robust after\nadversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mingyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulshrestha_M/0/1/0/all/0/1\">Manav Kulshrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinghan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1\">Ayan Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1\">Yevgeniy Vorobeychik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time, low-cost multi-person 3D pose estimation. (arXiv:2110.11414v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11414","description":"<p>The process of tracking human anatomy in computer vision is referred to pose\nestimation, and it is used in fields ranging from gaming to surveillance.\nThree-dimensional pose estimation traditionally requires advanced equipment,\nsuch as multiple linked intensity cameras or high-resolution time-of-flight\ncameras to produce depth images. However, there are applications, e.g.~consumer\nelectronics, where significant constraints are placed on the size, power\nconsumption, weight and cost of the usable technology. Here, we demonstrate\nthat computational imaging methods can achieve accurate pose estimation and\novercome the apparent limitations of time-of-flight sensors designed for much\nsimpler tasks. The sensor we use is already widely integrated in consumer-grade\nmobile devices, and despite its low spatial resolution, only 4$\\times$4 pixels,\nour proposed Pixels2Pose system transforms its data into accurate depth maps\nand 3D pose data of multiple people up to a distance of 3 m from the sensor. We\nare able to generate depth maps at a resolution of 32$\\times$32 and 3D\nlocalization of a body parts with an error of only $\\approx$10 cm at a frame\nrate of 7 fps. This work opens up promising real-life applications in scenarios\nthat were previously restricted by the advanced hardware requirements and cost\nof time-of-flight technology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruget_A/0/1/0/all/0/1\">Alice Ruget</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyler_M/0/1/0/all/0/1\">Max Tyler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_G/0/1/0/all/0/1\">Germ&#xe1;n Mora Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholes_S/0/1/0/all/0/1\">Stirling Scholes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyongy_I/0/1/0/all/0/1\">Istvan Gyongy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearn_B/0/1/0/all/0/1\">Brent Hearn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_S/0/1/0/all/0/1\">Steve McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halimi_A/0/1/0/all/0/1\">Abderrahim Halimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_J/0/1/0/all/0/1\">Jonathan Leach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training with Crafted Input Noise. (arXiv:2110.11417v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11417","description":"<p>Low-latency deep spiking neural networks (SNNs) have become a promising\nalternative to conventional artificial neural networks (ANNs) because of their\npotential for increased energy efficiency on event-driven neuromorphic\nhardware. Neural networks, including SNNs, however, are subject to various\nadversarial attacks and must be trained to remain resilient against such\nattacks for many applications. Nevertheless, due to prohibitively high training\ncosts associated with SNNs, analysis, and optimization of deep SNNs under\nvarious adversarial attacks have been largely overlooked. In this paper, we\nfirst present a detailed analysis of the inherent robustness of low-latency\nSNNs against popular gradient-based attacks, namely fast gradient sign method\n(FGSM) and projected gradient descent (PGD). Motivated by this analysis, to\nharness the model robustness against these attacks we present an SNN training\nalgorithm that uses crafted input noise and incurs no additional training time.\nTo evaluate the merits of our algorithm, we conducted extensive experiments\nwith variants of VGG and ResNet on both CIFAR-10 and CIFAR-100 datasets.\nCompared to standard trained direct input SNNs, our trained models yield\nimproved classification accuracy of up to 13.7% and 10.1% on FGSM and PGD\nattack-generated images, respectively, with negligible loss in clean image\naccuracy. Our models also outperform inherently robust SNNs trained on\nrate-coded inputs with improved or similar classification performance on\nattack-generated images while having up to 25x and 4.6x lower latency and\ncomputation energy, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1\">Massoud Pedram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Graph Sampling for Short Video Summarization using Gershgorin Disc Alignment. (arXiv:2110.11420v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11420","description":"<p>We study the problem of efficiently summarizing a short video into several\nkeyframes, leveraging recent progress in fast graph sampling. Specifically, we\nfirst construct a similarity path graph (SPG) $\\mathcal{G}$, represented by\ngraph Laplacian matrix $\\mathbf{L}$, where the similarities between adjacent\nframes are encoded as positive edge weights. We show that maximizing the\nsmallest eigenvalue $\\lambda_{\\min}(\\mathbf{B})$ of a coefficient matrix\n$\\mathbf{B} = \\text{diag}(\\mathbf{a}) + \\mu \\mathbf{L}$, where $\\mathbf{a}$ is\nthe binary keyframe selection vector, is equivalent to minimizing a worst-case\nsignal reconstruction error. We prove that, after partitioning $\\mathcal{G}$\ninto $Q$ sub-graphs $\\{\\mathcal{G}^q\\}^Q_{q=1}$, the smallest Gershgorin circle\ntheorem (GCT) lower bound of $Q$ corresponding coefficient matrices -- $\\min_q\n\\lambda^-_{\\min}(\\mathbf{B}^q)$ -- is a lower bound for\n$\\lambda_{\\min}(\\mathbf{B})$. This inspires a fast graph sampling algorithm to\niteratively partition $\\mathcal{G}$ into $Q$ sub-graphs using $Q$ samples\n(keyframes), while maximizing $\\lambda^-_{\\min}(\\mathbf{B}^q)$ for each\nsub-graph $\\mathcal{G}^q$. Experimental results show that our algorithm\nachieves comparable video summarization performance as state-of-the-art\nmethods, at a substantially reduced complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahami_S/0/1/0/all/0/1\">Sadid Sahami</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1\">Gene Cheung</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a> (1) ((1) National Tsing Hua University, (2) York University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUGL: Large Scale Multi Person Conditional Action Generation with Locomotion. (arXiv:2110.11460v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11460","description":"<p>We introduce MUGL, a novel deep neural model for large-scale, diverse\ngeneration of single and multi-person pose-based action sequences with\nlocomotion. Our controllable approach enables variable-length generations\ncustomizable by action category, across more than 100 categories. To enable\nintra/inter-category diversity, we model the latent generative space using a\nConditional Gaussian Mixture Variational Autoencoder. To enable realistic\ngeneration of actions involving locomotion, we decouple local pose and global\ntrajectory components of the action sequence. We incorporate duration-aware\nfeature representations to enable variable-length sequence generation. We use a\nhybrid pose sequence representation with 3D pose sequences sourced from videos\nand 3D Kinect-based sequences of NTU-RGBD-120. To enable principled comparison\nof generation quality, we employ suitably modified strong baselines during\nevaluation. Although smaller and simpler compared to baselines, MUGL provides\nbetter quality generations, paving the way for practical and controllable\nlarge-scale human action generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1\">Shubh Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Debtanu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation. (arXiv:2110.11474v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11474","description":"<p>Humans typically perceive the establishment of an action in a video through\nthe interaction between an actor and the surrounding environment. An action\nonly starts when the main actor in the video begins to interact with the\nenvironment, while it ends when the main actor stops the interaction. Despite\nthe great progress in temporal action proposal generation, most existing works\nignore the aforementioned fact and leave their model learning to propose\nactions as a black-box. In this paper, we make an attempt to simulate that\nability of a human by proposing Actor Environment Interaction (AEI) network to\nimprove the video representation for temporal action proposals generation. AEI\ncontains two modules, i.e., perception-based visual representation (PVR) and\nboundary-matching module (BMM). PVR represents each video snippet by taking\nhuman-human relations and humans-environment relations into consideration using\nthe proposed adaptive attention mechanism. Then, the video representation is\ntaken by BMM to generate action proposals. AEI is comprehensively evaluated in\nActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and\ndetection tasks, with two boundary-matching architectures (i.e., CNN-based and\nGCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly\noutperforms the state-of-the-art methods with remarkable performance and\ngeneralization for both temporal action proposal generation and temporal action\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1\">Khoa Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hyekang Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Sang Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixNorm: Test-Time Adaptation Through Online Normalization Estimation. (arXiv:2110.11478v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11478","description":"<p>We present a simple and effective way to estimate the batch-norm statistics\nduring test time, to fast adapt a source model to target test samples. Known as\nTest-Time Adaptation, most prior works studying this task follow two\nassumptions in their evaluation where (1) test samples come together as a large\nbatch, and (2) all from a single test distribution. However, in practice, these\ntwo assumptions may not stand, the reasons for which we propose two new\nevaluation settings where batch sizes are arbitrary and multiple distributions\nare considered. Unlike the previous methods that require a large batch of\nsingle distribution during test time to calculate stable batch-norm statistics,\nour method avoid any dependency on large online batches and is able to estimate\naccurate batch-norm statistics with a single sample. The proposed method\nsignificantly outperforms the State-Of-The-Art in the newly proposed settings\nin Test-Time Adaptation Task, and also demonstrates improvements in various\nother settings such as Source-Free Unsupervised Domain Adaptation and Zero-Shot\nClassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuefeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzunbas_G/0/1/0/all/0/1\">Gokhan Uzunbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sirius Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ashish Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1\">Ram Nevatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SymbioLCD: Ensemble-Based Loop Closure Detection using CNN-Extracted Objects and Visual Bag-of-Words. (arXiv:2110.11491v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11491","description":"<p>Loop closure detection is an essential tool of Simultaneous Localization and\nMapping (SLAM) to minimize drift in its localization. Many state-of-the-art\nloop closure detection (LCD) algorithms use visual Bag-of-Words (vBoW), which\nis robust against partial occlusions in a scene but cannot perceive the\nsemantics or spatial relationships between feature points. CNN object\nextraction can address those issues, by providing semantic labels and spatial\nrelationships between objects in a scene. Previous work has mainly focused on\nreplacing vBoW with CNN-derived features. In this paper, we propose SymbioLCD,\na novel ensemble-based LCD that utilizes both CNN-extracted objects and vBoW\nfeatures for LCD candidate prediction. When used in tandem, the added elements\nof object semantics and spatial-awareness create a more robust and symbiotic\nloop closure detection system. The proposed SymbioLCD uses scale-invariant\nspatial and semantic matching, Hausdorff distance with temporal constraints,\nand a Random Forest that utilizes combined information from both CNN-extracted\nobjects and vBoW features for predicting accurate loop closure candidates.\nEvaluation of the proposed method shows it outperforms other Machine Learning\n(ML) algorithms - such as SVM, Decision Tree and Neural Network, and\ndemonstrates that there is a strong symbiosis between CNN-extracted object\ninformation and vBoW features which assists accurate LCD candidate prediction.\nFurthermore, it is able to perceive loop closure candidates earlier than\nstate-of-the-art SLAM algorithms, utilizing added spatial and semantic\ninformation from CNN-extracted objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jonathan J.Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1\">Martin Urschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riddle_P/0/1/0/all/0/1\">Patricia J. Riddle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1\">J&#xf6;rg S. Wicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Decision-Making for Active Object Detection from Hand. (arXiv:2110.11524v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11524","description":"<p>A key component of understanding hand-object interactions is the ability to\nidentify the active object -- the object that is being manipulated by the human\nhand -- despite the occlusion induced by hand-object interactions. Based on the\nobservation that hand appearance is a strong indicator of the location and size\nof the active object, we set up our active object detection method as a\nsequential decision-making process that is conditioned on the location and\nappearance of the hands. The key innovation of our approach is the design of\nthe active object detection policy that uses an internal representation called\nthe Relational Box Field, which allows for every pixel to regress an improved\nlocation of an active object bounding box, essentially giving every pixel the\nability to vote for a better bounding box location. The policy is trained using\na hybrid imitation learning and reinforcement learning approach, and at test\ntime, the policy is used repeatedly to refine the bounding box location of the\nactive object. We perform experiments on two large-scale datasets: 100DOH and\nMECCANO, improving AP50 performance by 8% and 30%, respectively, over the state\nof the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digital and Physical-World Attacks on Remote Pulse Detection. (arXiv:2110.11525v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11525","description":"<p>Remote photoplethysmography (rPPG) is a technique for estimating blood volume\nchanges from reflected light without the need for a contact sensor. We present\nthe first examples of presentation attacks in the digital and physical domains\non rPPG from face video. Digital attacks are easily performed by adding\nimperceptible periodic noise to the input videos. Physical attacks are\nperformed with illumination from visible spectrum LEDs placed in close\nproximity to the face, while still being difficult to perceive with the human\neye. We also show that our attacks extend beyond medical applications, since\nthe method can effectively generate a strong periodic pulse on 3D-printed face\nmasks, which presents difficulties for pulse-based face presentation attack\ndetection (PAD). The paper concludes with ideas for using this work to improve\nrobustness of rPPG methods and pulse-based face PAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Speth_J/0/1/0/all/0/1\">Jeremy Speth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1\">Nathan Vance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1\">Patrick Flynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin W. Bowyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1\">Adam Czajka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wide Neural Networks Forget Less Catastrophically. (arXiv:2110.11526v1 [cs.LG])","link":"http://arxiv.org/abs/2110.11526","description":"<p>A growing body of research in continual learning is devoted to overcoming the\n\"Catastrophic Forgetting\" of neural networks by designing new algorithms that\nare more robust to the distribution shifts. While the recent progress in\ncontinual learning literature is encouraging, our understanding of what\nproperties of neural networks contribute to catastrophic forgetting is still\nlimited. To address this, instead of focusing on continual learning algorithms,\nin this work, we focus on the model itself and study the impact of \"width\" of\nthe neural network architecture on catastrophic forgetting, and show that width\nhas a surprisingly significant effect on forgetting. To explain this effect, we\nstudy the learning dynamics of the network from various perspectives such as\ngradient norm and sparsity, orthogonalization, and lazy training regime. We\nprovide potential explanations that are consistent with the empirical results\nacross different architectures and continual learning benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzadeh_S/0/1/0/all/0/1\">Seyed Iman Mirzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhry_A/0/1/0/all/0/1\">Arslan Chaudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huiyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorur_D/0/1/0/all/0/1\">Dilan Gorur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1\">Mehrdad Farajtabar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Supervised Monocular Depth Estimation with Teacher-Student Network. (arXiv:2110.11545v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11545","description":"<p>Despite recent improvement of supervised monocular depth estimation, the lack\nof high quality pixel-wise ground truth annotations has become a major hurdle\nfor further progress. In this work, we propose a new unsupervised depth\nestimation method based on pseudo supervision mechanism by training a\nteacher-student network with knowledge distillation. It strategically\nintegrates the advantages of supervised and unsupervised monocular depth\nestimation, as well as unsupervised binocular depth estimation. Specifically,\nthe teacher network takes advantage of the effectiveness of binocular depth\nestimation to produce accurate disparity maps, which are then used as the\npseudo ground truth to train the student network for monocular depth\nestimation. This effectively converts the problem of unsupervised learning to\nsupervised learning. Our extensive experimental results demonstrate that the\nproposed method outperforms the state-of-the-art on the KITTI benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signature-Graph Networks. (arXiv:2110.11551v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11551","description":"<p>We propose a novel approach for visual representation learning called\nSignature-Graph Neural Networks (SGN). SGN learns latent global structures that\naugment the feature representation of Convolutional Neural Networks (CNN). SGN\nconstructs unique undirected graphs for each image based on the CNN feature\nmaps. The feature maps are partitioned into a set of equal and non-overlapping\npatches. The graph nodes are located on high-contrast sharp convolution\nfeatures with the local maxima or minima in these patches. The node embeddings\nare aggregated through novel Signature-Graphs based on horizontal and vertical\nedge connections. The representation vectors are then computed based on the\nspectral Laplacian eigenvalues of the graphs. SGN outperforms existing methods\nof recent graph convolutional networks, generative adversarial networks, and\nauto-encoders with image classification accuracy of 99.65% on ASIRRA, 99.91% on\nMNIST, 98.55% on Fashion-MNIST, 96.18% on CIFAR-10, 84.71% on CIFAR-100, 94.36%\non STL10, and 95.86% on SVHN datasets. We also introduce a novel implementation\nof the state-of-the-art multi-head attention (MHA) on top of the proposed SGN.\nAdding SGN to MHA improved the image classification accuracy from 86.92% to\n94.36% on the STL10 dataset\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Ali Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora Salim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Du Yong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototypical Classifier for Robust Class-Imbalanced Learning. (arXiv:2110.11553v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11553","description":"<p>Deep neural networks have been shown to be very powerful methods for many\nsupervised learning tasks. However, they can also easily overfit to training\nset biases, i.e., label noise and class imbalance. While both learning with\nnoisy labels and class-imbalanced learning have received tremendous attention,\nexisting works mainly focus on one of these two training set biases. To fill\nthe gap, we propose \\textit{Prototypical Classifier}, which does not require\nfitting additional parameters given the embedding network. Unlike conventional\nclassifiers that are biased towards head classes, Prototypical Classifier\nproduces balanced and comparable predictions for all classes even though the\ntraining set is class-imbalanced. By leveraging this appealing property, we can\neasily detect noisy labels by thresholding the confidence scores predicted by\nPrototypical Classifier, where the threshold is dynamically adjusted through\nthe iteration. A sample reweghting strategy is then applied to mitigate the\ninfluence of noisy labels. We test our method on CIFAR-10-LT, CIFAR-100-LT and\nWebvision datasets, observing that Prototypical Classifier obtains substaintial\nimprovements compared with state of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiang-Xin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min-Ling Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHAttnSurv: Multi-Head Attention for Survival Prediction Using Whole-Slide Pathology Images. (arXiv:2110.11558v1 [eess.IV])","link":"http://arxiv.org/abs/2110.11558","description":"<p>In pathology, whole-slide images (WSI) based survival prediction has\nattracted increasing interest. However, given the large size of WSIs and the\nlack of pathologist annotations, extracting the prognostic information from\nWSIs remains a challenging task. Previous studies have used multiple instance\nlearning approaches to combine the information from multiple randomly sampled\npatches, but different visual patterns may contribute differently to prognosis\nprediction. In this study, we developed a multi-head attention approach to\nfocus on various parts of a tumor slide, for more comprehensive information\nextraction from WSIs. We evaluated our approach on four cancer types from The\nCancer Genome Atlas database. Our model achieved an average c-index of 0.640,\noutperforming two existing state-of-the-art approaches for WSI-based survival\nprediction, which have an average c-index of 0.603 and 0.619 on these datasets.\nVisualization of our attention maps reveals each attention head focuses\nsynergistically on different morphological patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1\">Shuai Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suriawinata_A/0/1/0/all/0/1\">Arief A. Suriawinata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassanpour_S/0/1/0/all/0/1\">Saeed Hassanpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EvoGAN: An Evolutionary Computation Assisted GAN. (arXiv:2110.11583v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11583","description":"<p>The image synthesis technique is relatively well established which can\ngenerate facial images that are indistinguishable even by human beings.\nHowever, all of these approaches uses gradients to condition the output,\nresulting in the outputting the same image with the same input. Also, they can\nonly generate images with basic expression or mimic an expression instead of\ngenerating compound expression. In real life, however, human expressions are of\ngreat diversity and complexity. In this paper, we propose an evolutionary\nalgorithm (EA) assisted GAN, named EvoGAN, to generate various compound\nexpressions with any accurate target compound expression. EvoGAN uses an EA to\nsearch target results in the data distribution learned by GAN. Specifically, we\nuse the Facial Action Coding System (FACS) as the encoding of an EA and use a\npre-trained GAN to generate human facial images, and then use a pre-trained\nclassifier to recognize the expression composition of the synthesized images as\nthe fitness function to guide the search of the EA. Combined random searching\nalgorithm, various images with the target expression can be easily sythesized.\nQuantitative and Qualitative results are presented on several compound\nexpressions, and the experimental results demonstrate the feasibility and the\npotential of EvoGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">HanYang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziwang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiayin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhibin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wide and Narrow: Video Prediction from Context and Motion. (arXiv:2110.11586v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11586","description":"<p>Video prediction, forecasting the future frames from a sequence of input\nframes, is a challenging task since the view changes are influenced by various\nfactors, such as the global context surrounding the scene and local motion\ndynamics. In this paper, we propose a new framework to integrate these\ncomplementary attributes to predict complex pixel dynamics through deep\nnetworks. We present global context propagation networks that iteratively\naggregate the non-local neighboring representations to preserve the contextual\ninformation over the past frames. To capture the local motion pattern of\nobjects, we also devise local filter memory networks that generate adaptive\nfilter kernels by storing the prototypical motion of moving objects in the\nmemory. The proposed framework, utilizing the outputs from both networks, can\naddress blurry predictions and color distortion. We conduct experiments on\nCaltech pedestrian and UCF101 datasets, and demonstrate state-of-the-art\nresults. Especially for multi-step prediction, we obtain an outstanding\nperformance in quantitative and qualitative evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaehoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changjae Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wonil Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIML/CVL RGB-D Dataset: 2M RGB-D Images of Natural Indoor and Outdoor Scenes. (arXiv:2110.11590v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11590","description":"<p>This manual is intended to provide a detailed description of the DIML/CVL\nRGB-D dataset. This dataset is comprised of 2M color images and their\ncorresponding depth maps from a great variety of natural indoor and outdoor\nscenes. The indoor dataset was constructed using the Microsoft Kinect v2, while\nthe outdoor dataset was built using the stereo cameras (ZED stereo camera and\nbuilt-in stereo camera). Table I summarizes the details of our dataset,\nincluding acquisition, processing, format, and toolbox. Refer to Section II and\nIII for more details.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaehoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dongbo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Inspired Autoencoder for Unsupervised Hyperspectral Image Super-Resolution. (arXiv:2110.11591v1 [eess.IV])","link":"http://arxiv.org/abs/2110.11591","description":"<p>This paper focuses on hyperspectral image (HSI) super-resolution that aims to\nfuse a low-spatial-resolution HSI and a high-spatial-resolution multispectral\nimage to form a high-spatial-resolution HSI (HR-HSI). Existing deep\nlearning-based approaches are mostly supervised that rely on a large number of\nlabeled training samples, which is unrealistic. The commonly used model-based\napproaches are unsupervised and flexible but rely on hand-craft priors.\nInspired by the specific properties of model, we make the first attempt to\ndesign a model inspired deep network for HSI super-resolution in an\nunsupervised manner. This approach consists of an implicit autoencoder network\nbuilt on the target HR-HSI that treats each pixel as an individual sample. The\nnonnegative matrix factorization (NMF) of the target HR-HSI is integrated into\nthe autoencoder network, where the two NMF parts, spectral and spatial\nmatrices, are treated as decoder parameters and hidden outputs respectively. In\nthe encoding stage, we present a pixel-wise fusion model to estimate hidden\noutputs directly, and then reformulate and unfold the model's algorithm to form\nthe encoder network. With the specific architecture, the proposed network is\nsimilar to a manifold prior-based model, and can be trained patch by patch\nrather than the entire image. Moreover, we propose an additional unsupervised\nnetwork to estimate the point spread function and spectral response function.\nExperimental results conducted on both synthetic and real datasets demonstrate\nthe effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianjun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zebin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Liang Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Text-Image Joint Embedding for Efficient Cross-Modal Retrieval with Deep Feature Engineering. (arXiv:2110.11592v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11592","description":"<p>This paper introduces a two-phase deep feature engineering framework for\nefficient learning of semantics enhanced joint embedding, which clearly\nseparates the deep feature engineering in data preprocessing from training the\ntext-image joint embedding model. We use the Recipe1M dataset for the technical\ndescription and empirical validation. In preprocessing, we perform deep feature\nengineering by combining deep feature engineering with semantic context\nfeatures derived from raw text-image input data. We leverage LSTM to identify\nkey terms, deep NLP models from the BERT family, TextRank, or TF-IDF to produce\nranking scores for key terms before generating the vector representation for\neach key term by using word2vec. We leverage wideResNet50 and word2vec to\nextract and encode the image category semantics of food images to help semantic\nalignment of the learned recipe and image embeddings in the joint latent space.\nIn joint embedding learning, we perform deep feature engineering by optimizing\nthe batch-hard triplet loss function with soft-margin and double negative\nsampling, taking into account also the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with deep feature engineering significantly outperforms the\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of Injection and Press Mold Parts on 2D Drawing Using Deep Neural Network. (arXiv:2110.11593v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11593","description":"<p>This paper proposes a method to automatically detect the key feature parts in\na CAD of commercial TV and monitor using a deep neural network. We developed a\ndeep learning pipeline that can detect the injection parts such as hook, boss,\nundercut and press parts such as DPS, Embo-Screwless, Embo-Burring, and EMBO in\nthe 2D CAD drawing images. We first cropped the drawing to a specific size for\nthe training efficiency of a deep neural network. Then, we use Cascade R-CNN to\nfind the position of injection and press parts and use Resnet-50 to predict the\norientation of the parts. Finally, we convert the position of the parts found\nthrough the cropped image to the position of the original image. As a result,\nwe obtained detection accuracy of injection and press parts with 84.1% in AP\n(Average Precision), 91.2% in AR(Average Recall), 72.0% in AP, 87.0% in AR, and\norientation accuracy of injection and press parts with 94.4% and 92.0%, which\ncan facilitate the faster design in industrial product design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jumi Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_S/0/1/0/all/0/1\">Seunghyeok Back</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_S/0/1/0/all/0/1\">Seongho Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoobin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable AI. (arXiv:2110.11597v1 [cs.LG])","link":"http://arxiv.org/abs/2110.11597","description":"<p>Unexplainable black-box models create scenarios where anomalies cause\ndeleterious responses, thus creating unacceptable risks. These risks have\nmotivated the field of eXplainable Artificial Intelligence (XAI) to improve\ntrust by evaluating local interpretability in black-box neural networks.\nUnfortunately, the ground truth is unavailable for the model's decision, so\nevaluation is limited to qualitative assessment. Further, interpretability may\nlead to inaccurate conclusions about the model or a false sense of trust. We\npropose to improve XAI from the vantage point of the user's trust by exploring\na black-box model's latent feature space. We present an approach, ProtoShotXAI,\nthat uses a Prototypical few-shot network to explore the contrastive manifold\nbetween nonlinear features of different classes. A user explores the manifold\nby perturbing the input features of a query sample and recording the response\nfor a subset of exemplars from any class. Our approach is the first locally\ninterpretable XAI model that can be extended to, and demonstrated on, few-shot\nnetworks. We compare ProtoShotXAI to the state-of-the-art XAI approaches on\nMNIST, Omniglot, and ImageNet to demonstrate, both quantitatively and\nqualitatively, that ProtoShotXAI provides more flexibility for model\nexploration. Finally, ProtoShotXAI also demonstrates novel explainabilty and\ndetectabilty on adversarial samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hess_S/0/1/0/all/0/1\">Samuel Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ditzler_G/0/1/0/all/0/1\">Gregory Ditzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Fidelity 3D Reconstructions with Limited Physical Views. (arXiv:2110.11599v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11599","description":"<p>Multi-view triangulation is the gold standard for 3D reconstruction from 2D\ncorrespondences given known calibration and sufficient views. However in\npractice, expensive multi-view setups -- involving tens sometimes hundreds of\ncameras -- are required in order to obtain the high fidelity 3D reconstructions\nnecessary for many modern applications. In this paper we present a novel\napproach that leverages recent advances in 2D-3D lifting using neural shape\npriors while also enforcing multi-view equivariance. We show how our method can\nachieve comparable fidelity to expensive calibrated multi-view rigs using a\nlimited (2-3) number of uncalibrated camera views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dabhi_M/0/1/0/all/0/1\">Mosam Dabhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saluja_K/0/1/0/all/0/1\">Kunal Saluja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeni_L/0/1/0/all/0/1\">Laszlo Jeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasel_I/0/1/0/all/0/1\">Ian Fasel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Semi-Supervised Learning for3D Objects. (arXiv:2110.11601v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11601","description":"<p>In recent years, semi-supervised learning has been widely explored and shows\nexcellent data efficiency for 2D data. There is an emerging need to improve\ndata efficiency for 3D tasks due to the scarcity of labeled 3D data. This paper\nexplores how the coherence of different modelities of 3D data (e.g. point\ncloud, image, and mesh) can be used to improve data efficiency for both 3D\nclassification and retrieval tasks. We propose a novel multimodal\nsemi-supervised learning framework by introducing instance-level consistency\nconstraint and a novel multimodal contrastive prototype (M2CP) loss. The\ninstance-level consistency enforces the network to generate consistent\nrepresentations for multimodal data of the same object regardless of its\nmodality. The M2CP maintains a multimodal prototype for each class and learns\nfeatures with small intra-class variations by minimizing the feature distance\nof each object to its prototype while maximizing the distance to the others.\nOur proposed framework significantly outperforms all the state-of-the-art\ncounterparts for both classification and retrieval tasks by a large margin on\nthe modelNet10 and ModelNet40 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">YingLi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Stream Attention Learning for Monocular Vehicle Velocity and Inter-Vehicle Distance Estimation. (arXiv:2110.11608v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11608","description":"<p>Vehicle velocity and inter-vehicle distance estimation are essential for ADAS\n(Advanced driver-assistance systems) and autonomous vehicles. To save the cost\nof expensive ranging sensors, recent studies focus on using a low-cost\nmonocular camera to perceive the environment around the vehicle in a\ndata-driven fashion. Existing approaches treat each vehicle independently for\nperception and cause inconsistent estimation. Furthermore, important\ninformation like context and spatial relation in 2D object detection is often\nneglected in the velocity estimation pipeline. In this paper, we explore the\nrelationship between vehicles of the same frame with a\nglobal-relative-constraint (GLC) loss to encourage consistent estimation. A\nnovel multi-stream attention network (MSANet) is proposed to extract different\naspects of features, e.g., spatial and contextual features, for joint vehicle\nvelocity and inter-vehicle distance estimation. Experiments show the\neffectiveness and robustness of our proposed approach. MSANet outperforms\nstate-of-the-art algorithms on both the KITTI dataset and TuSimple velocity\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Chih Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCICAP: Generating Captions for Scientific Figures. (arXiv:2110.11624v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11624","description":"<p>Researchers use figures to communicate rich, complex information in\nscientific papers. The captions of these figures are critical to conveying\neffective messages. However, low-quality figure captions commonly occur in\nscientific articles and may decrease understanding. In this paper, we propose\nan end-to-end neural framework to automatically generate informative,\nhigh-quality captions for scientific figures. To this end, we introduce SCICAP,\na large-scale figure-caption dataset based on computer science arXiv papers\npublished between 2010 and 2020. After pre-processing - including figure-type\nclassification, sub-figure identification, text normalization, and caption text\nselection - SCICAP contained more than two million figures extracted from over\n290,000 papers. We then established baseline models that caption graph plots,\nthe dominant (19.2%) figure type. The experimental results showed both\nopportunities and steep challenges of generating captions for scientific\nfigures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ting-Yao/0/1/0/all/0/1\">Ting-Yao</a> (Edward)Hsu, <a href=\"http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1\">C. Lee Giles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Generalization Performance of Surgical Phase Recognition with Expert-Generated Annotations. (arXiv:2110.11626v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11626","description":"<p>As the area of application of deep neural networks expands to areas requiring\nexpertise, e.g., in medicine and law, more exquisite annotation processes for\nexpert knowledge training are required. In particular, it is difficult to\nguarantee generalization performance in the clinical field in the case of\nexpert knowledge training where opinions may differ even among experts on\nannotations. To raise the issue of the annotation generation process for\nexpertise training of CNNs, we verified the annotations for surgical phase\nrecognition of laparoscopic cholecystectomy and subtotal gastrectomy for\ngastric cancer. We produce calibrated annotations for the seven phases of\ncholecystectomy by analyzing the discrepancies of previously annotated labels\nand by discussing the criteria of surgical phases. For gastrectomy for gastric\ncancer has more complex twenty-one surgical phases, we generate consensus\nannotation by the revision process with five specialists. By training the\nCNN-based surgical phase recognition networks with revised annotations, we\nachieved improved generalization performance over models trained with original\nannotation under the same cross-validation settings. We showed that the\nexpertise data annotation pipeline for deep neural networks should be more\nrigorous based on the type of problem to apply clinical field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Seungbum Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Bokyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwusaibie_A/0/1/0/all/0/1\">Ahmed A. Alwusaibie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfadhel_A/0/1/0/all/0/1\">Anwar H. Alfadhel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">SungHyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyung_W/0/1/0/all/0/1\">Woo Jin Hyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Min-Kook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Face Recognition with Large Age Gaps by Learning to Distinguish Children. (arXiv:2110.11630v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11630","description":"<p>Despite the unprecedented improvement of face recognition, existing face\nrecognition models still show considerably low performances in determining\nwhether a pair of child and adult images belong to the same identity. Previous\napproaches mainly focused on increasing the similarity between child and adult\nimages of a given identity to overcome the discrepancy of facial appearances\ndue to aging. However, we observe that reducing the similarity between child\nimages of different identities is crucial for learning distinct features among\nchildren and thus improving face recognition performance in child-adult pairs.\nBased on this intuition, we propose a novel loss function called the\nInter-Prototype loss which minimizes the similarity between child images.\nUnlike the previous studies, the Inter-Prototype loss does not require\nadditional child images or training additional learnable parameters. Our\nextensive experiments and in-depth analyses show that our approach outperforms\nexisting baselines in face recognition with child-adult pairs. Our code and\nnewly-constructed test sets of child-adult pairs are available at\nhttps://github.com/leebebeto/Inter-Prototype.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jooyeol Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yonggyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion-Robust Object Pose Estimation with Holistic Representation. (arXiv:2110.11636v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11636","description":"<p>Practical object pose estimation demands robustness against occlusions to the\ntarget object. State-of-the-art (SOTA) object pose estimators take a two-stage\napproach, where the first stage predicts 2D landmarks using a deep network and\nthe second stage solves for 6DOF pose from 2D-3D correspondences. Albeit widely\nadopted, such two-stage approaches could suffer from novel occlusions when\ngeneralising and weak landmark coherence due to disrupted features. To address\nthese issues, we develop a novel occlude-and-blackout batch augmentation\ntechnique to learn occlusion-robust deep features, and a multi-precision\nsupervision architecture to encourage holistic pose representation learning for\naccurate and coherent landmark predictions. We perform careful ablation tests\nto verify the impact of our innovations and compare our method to SOTA pose\nestimators. Without the need of any post-processing or refinement, our method\nexhibits superior performance on the LINEMOD dataset. On the YCB-Video dataset\nour method outperforms all non-refinement methods in terms of the ADD(-S)\nmetric. We also demonstrate the high data-efficiency of our method. Our code is\navailable at <a href=\"http://github.com/BoChenYS/ROPE\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimavicius_M/0/1/0/all/0/1\">Marius Klimavicius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTP-Net For Cross-Domain Trajectory Prediction. (arXiv:2110.11645v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11645","description":"<p>Deep learning based trajectory prediction methods rely on large amount of\nannotated future trajectories, but may not generalize well to a new scenario\ncaptured by another camera. Meanwhile, annotating trajectories for training a\nnetwork for this new scenario is time-consuming and expensive, therefore it is\ndesirable to adapt the model trained with the annotated source domain\ntrajectories to the target domain.\n</p>\n<p>To tackle domain adaptation for trajectory prediction, we propose a\nCross-domain Trajectory Prediction Network (CTP-Net), in which LSTMs are used\nto encode the observed trajectories of both domain, and their features are\naligned by a cross-domain feature discriminator. Further, considering the\nconsistency between the observed trajectories and the predicted trajectories in\nthe target domain, a target domain offset discriminator is utilized to\nadversarially regularize the future trajectory predictions to be consistent\nwith the observed trajectories. Extensive experiments demonstrate the\neffectiveness of the proposed domain adaptation for trajectory prediction\nsetting as well as our method on domain adaptation for trajectory prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pingxuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yanyan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel-by-Pixel Cross-Domain Alignment for Few-Shot Semantic Segmentation. (arXiv:2110.11650v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11650","description":"<p>In this paper we consider the task of semantic segmentation in autonomous\ndriving applications. Specifically, we consider the cross-domain few-shot\nsetting where training can use only few real-world annotated images and many\nannotated synthetic images. In this context, aligning the domains is made more\nchallenging by the pixel-wise class imbalance that is intrinsic in the\nsegmentation and that leads to ignoring the underrepresented classes and\noverfitting the well represented ones. We address this problem with a novel\nframework called Pixel-By-Pixel Cross-Domain Alignment (PixDA). We propose a\nnovel pixel-by-pixel domain adversarial loss following three criteria: (i)\nalign the source and the target domain for each pixel, (ii) avoid negative\ntransfer on the correctly represented pixels, and (iii) regularize the training\nof infrequent classes to avoid overfitting. The pixel-wise adversarial training\nis assisted by a novel sample selection procedure, that handles the imbalance\nbetween source and target data, and a knowledge distillation strategy, that\navoids overfitting towards the few target images. We demonstrate on standard\nsynthetic-to-real benchmarks that PixDA outperforms previous state-of-the-art\nmethods in (1-5)-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavera_A/0/1/0/all/0/1\">Antonio Tavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1\">Fabio Cermelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projective Manifold Gradient Layer for Deep Rotation Regression. (arXiv:2110.11657v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11657","description":"<p>Regressing rotations on SO(3) manifold using deep neural networks is an\nimportant yet unsolved problem. The gap between Euclidean network output space\nand the non-Euclidean SO(3) manifold imposes a severe challenge for neural\nnetwork learning in both forward and backward passes. While several works have\nproposed different regression-friendly rotation representations, very few works\nhave been devoted to improving the gradient backpropagating in the backward\npass. In this paper, we propose a manifold-aware gradient that directly\nbackpropagates into deep network weights. Leveraging the Riemannian gradient\nand a novel projective gradient, our proposed regularized projective manifold\ngradient (RPMG) helps networks achieve new state-of-the-art performance in a\nvariety of rotation estimation tasks. The proposed gradient layer can also be\napplied to other smooth manifolds such as the unit sphere.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiayi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yingda Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"1st Place Solution for the UVO Challenge on Video-based Open-World Segmentation 2021. (arXiv:2110.11661v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11661","description":"<p>In this report, we introduce our (pretty straightforard) two-step\n\"detect-then-match\" video instance segmentation method. The first step performs\ninstance segmentation for each frame to get a large number of instance mask\nproposals. The second step is to do inter-frame instance mask matching with the\nhelp of optical flow. We demonstrate that with high quality mask proposals, a\nsimple matching mechanism is good enough for tracking. Our approach achieves\nthe first place in the UVO 2021 Video-based Open-World Segmentation Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reimagine BiSeNet for Real-Time Domain Adaptation in Semantic Segmentation. (arXiv:2110.11662v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11662","description":"<p>Semantic segmentation models have reached remarkable performance across\nvarious tasks. However, this performance is achieved with extremely large\nmodels, using powerful computational resources and without considering training\nand inference time. Real-world applications, on the other hand, necessitate\nmodels with minimal memory demands, efficient inference speed, and executable\nwith low-resources embedded devices, such as self-driving vehicles. In this\npaper, we look at the challenge of real-time semantic segmentation across\ndomains, and we train a model to act appropriately on real-world data even\nthough it was trained on a synthetic realm. We employ a new lightweight and\nshallow discriminator that was specifically created for this purpose. To the\nbest of our knowledge, we are the first to present a real-time adversarial\napproach for assessing the domain adaption problem in semantic segmentation. We\ntested our framework in the two standard protocol: GTA5 to Cityscapes and\nSYNTHIA to Cityscapes. Code is available at:\nhttps://github.com/taveraantonio/RTDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavera_A/0/1/0/all/0/1\">Antonio Tavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCCN: Global Context Convolutional Network. (arXiv:2110.11664v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11664","description":"<p>In this paper, we propose Global Context Convolutional Network (GCCN) for\nvisual recognition. GCCN computes global features representing contextual\ninformation across image patches. These global contextual features are defined\nas local maxima pixels with high visual sharpness in each patch. These features\nare then concatenated and utilised to augment the convolutional features. The\nlearnt feature vector is normalised using the global context features using\nFrobenius norm. This straightforward approach achieves high accuracy in\ncompassion to the state-of-the-art methods with 94.6% and 95.41% on CIFAR-10\nand STL-10 datasets, respectively. To explore potential impact of GCCN on other\nvisual representation tasks, we implemented GCCN as a based model to few-shot\nimage classification. We learn metric distances between the augmented feature\nvectors and their prototypes representations, similar to Prototypical and\nMatching Networks. GCCN outperforms state-of-the-art few-shot learning methods\nachieving 99.9%, 84.8% and 80.74% on Omniglot, MiniImageNet and CUB-200,\nrespectively. GCCN has significantly improved on the accuracy of\nstate-of-the-art prototypical and matching networks by up to 30% in different\nfew-shot learning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Ali Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora Salim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Du Yong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable, automated urban interventions to improve pedestrian and vehicle safety. (arXiv:2110.11672v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11672","description":"<p>At the moment, urban mobility research and governmental initiatives are\nmostly focused on motor-related issues, e.g. the problems of congestion and\npollution. And yet, we can not disregard the most vulnerable elements in the\nurban landscape: pedestrians, exposed to higher risks than other road users.\nIndeed, safe, accessible, and sustainable transport systems in cities are a\ncore target of the UN's 2030 Agenda. Thus, there is an opportunity to apply\nadvanced computational tools to the problem of traffic safety, in regards\nespecially to pedestrians, who have been often overlooked in the past. This\npaper combines public data sources, large-scale street imagery and computer\nvision techniques to approach pedestrian and vehicle safety with an automated,\nrelatively simple, and universally-applicable data-processing scheme. The steps\ninvolved in this pipeline include the adaptation and training of a Residual\nConvolutional Neural Network to determine a hazard index for each given urban\nscene, as well as an interpretability analysis based on image segmentation and\nclass activation mapping on those same images. Combined, the outcome of this\ncomputational approach is a fine-grained map of hazard levels across a city,\nand an heuristic to identify interventions that might simultaneously improve\npedestrian and vehicle safety. The proposed framework should be taken as a\ncomplement to the work of urban planners and public authorities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bustos_C/0/1/0/all/0/1\">Cristina Bustos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhoads_D/0/1/0/all/0/1\">Daniel Rhoads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sole_Ribalta_A/0/1/0/all/0/1\">Albert Sole-Ribalta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masip_D/0/1/0/all/0/1\">David Masip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arenas_A/0/1/0/all/0/1\">Alex Arenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borge_Holthoefer_J/0/1/0/all/0/1\">Javier Borge-Holthoefer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-only Object Tracking. (arXiv:2110.11679v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11679","description":"<p>Depth (D) indicates occlusion and is less sensitive to illumination changes,\nwhich make depth attractive modality for Visual Object Tracking (VOT). Depth is\nused in RGBD object tracking where the best trackers are deep RGB trackers with\nadditional heuristic using depth maps. There are two potential reasons for the\nheuristics: 1) the lack of large RGBD tracking datasets to train deep RGBD\ntrackers and 2) the long-term evaluation protocol of VOT RGBD that benefits\nfrom heuristics such as depth-based occlusion detection. In this work, we study\nhow far D-only tracking can go if trained with large amounts of depth data. To\ncompensate the lack of depth data, we generate depth maps for tracking. We\ntrain a \"Depth-DiMP\" from the scratch with the generated data and fine-tune it\nwith the available small RGBD tracking datasets. The depth-only DiMP achieves\ngood accuracy in depth-only tracking and combined with the original RGB DiMP\nthe end-to-end trained RGBD-DiMP outperforms the recent VOT 2020 RGBD winners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Song Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamarainen_J/0/1/0/all/0/1\">Joni-Kristian Kamarainen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation. (arXiv:2110.11680v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11680","description":"<p>Several video-based 3D pose and shape estimation algorithms have been\nproposed to resolve the temporal inconsistency of single-image-based methods.\nHowever it still remains challenging to have stable and accurate\nreconstruction. In this paper, we propose a new framework Deep Two-Stream Video\nInference for Human Body Pose and Shape Estimation (DTS-VIBE), to generate 3D\nhuman pose and mesh from RGB videos. We reformulate the task as a\nmulti-modality problem that fuses RGB and optical flow for more reliable\nestimation. In order to fully utilize both sensory modalities (RGB or optical\nflow), we train a two-stream temporal network based on transformer to predict\nSMPL parameters. The supplementary modality, optical flow, helps to maintain\ntemporal consistency by leveraging motion knowledge between two consecutive\nframes. The proposed algorithm is extensively evaluated on the Human3.6 and\n3DPW datasets. The experimental results show that it outperforms other\nstate-of-the-art methods by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Variational Autoencoder for Learned Image Reconstruction. (arXiv:2110.11681v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11681","description":"<p>Learned image reconstruction techniques using deep neural networks have\nrecently gained popularity, and have delivered promising empirical results.\nHowever, most approaches focus on one single recovery for each observation, and\nthus neglect the uncertainty information. In this work, we develop a novel\ncomputational framework that approximates the posterior distribution of the\nunknown image at each query observation. The proposed framework is very\nflexible: It handles implicit noise models and priors, it incorporates the data\nformation process (i.e., the forward operator), and the learned reconstructive\nproperties are transferable between different datasets. Once the network is\ntrained using the conditional variational autoencoder loss, it provides a\ncomputationally efficient sampler for the approximate posterior distribution\nvia feed-forward propagation, and the summarizing statistics of the generated\nsamples are used for both point-estimation and uncertainty quantification. We\nillustrate the proposed framework with extensive numerical experiments on\npositron emission tomography (with both moderate and low count levels) showing\nthat the framework generates high-quality samples when compared with\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Riccardo Barbano Bangti Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal-Boost: Multimodal Medical Image Super-Resolution using Multi-Attention Network with Wavelet Transform. (arXiv:2110.11684v1 [eess.IV])","link":"http://arxiv.org/abs/2110.11684","description":"<p>Multimodal medical images are widely used by clinicians and physicians to\nanalyze and retrieve complementary information from high-resolution images in a\nnon-invasive manner. The loss of corresponding image resolution degrades the\noverall performance of medical image diagnosis. Deep learning based single\nimage super resolution (SISR) algorithms has revolutionized the overall\ndiagnosis framework by continually improving the architectural components and\ntraining strategies associated with convolutional neural networks (CNN) on\nlow-resolution images. However, existing work lacks in two ways: i) the SR\noutput produced exhibits poor texture details, and often produce blurred edges,\nii) most of the models have been developed for a single modality, hence,\nrequire modification to adapt to a new one. This work addresses (i) by\nproposing generative adversarial network (GAN) with deep multi-attention\nmodules to learn high-frequency information from low-frequency data. Existing\napproaches based on the GAN have yielded good SR results; however, the texture\ndetails of their SR output have been experimentally confirmed to be deficient\nfor medical images particularly. The integration of wavelet transform (WT) and\nGANs in our proposed SR model addresses the aforementioned limitation\nconcerning textons. The WT divides the LR image into multiple frequency bands,\nwhile the transferred GAN utilizes multiple attention and upsample blocks to\npredict high-frequency components. Moreover, we present a learning technique\nfor training a domain-specific classifier as a perceptual loss function.\nCombining multi-attention GAN loss with a perceptual loss function results in a\nreliable and efficient performance. Applying the same model for medical images\nfrom diverse modalities is challenging, our work addresses (ii) by training and\nperforming on several modalities via transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deeba_F/0/1/0/all/0/1\">Farah Deeba</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dharejo_F/0/1/0/all/0/1\">Fayaz Ali Dharejo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zawish_M/0/1/0/all/0/1\">Muhammad Zawish</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dev_K/0/1/0/all/0/1\">Kapal Dev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khowaja_S/0/1/0/all/0/1\">Sunder Ali Khowaja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qureshi_N/0/1/0/all/0/1\">Nawab Muhammad Faseeh Qureshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Fusion Affinity Graph with Noise-free Online Low-rank Representation for Natural Image Segmentation. (arXiv:2110.11685v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11685","description":"<p>Affinity graph-based segmentation methods have become a major trend in\ncomputer vision. The performance of these methods relies on the constructed\naffinity graph, with particular emphasis on the neighborhood topology and\npairwise affinities among superpixels. Due to the advantages of assimilating\ndifferent graphs, a multi-scale fusion graph has a better performance than a\nsingle graph with single-scale. However, these methods ignore the noise from\nimages which influences the accuracy of pairwise similarities. Multi-scale\ncombinatorial grouping and graph fusion also generate a higher computational\ncomplexity. In this paper, we propose an adaptive fusion affinity graph\n(AFA-graph) with noise-free low-rank representation in an online manner for\nnatural image segmentation. An input image is first over-segmented into\nsuperpixels at different scales and then filtered by the proposed improved\nkernel density estimation method. Moreover, we select global nodes of these\nsuperpixels on the basis of their subspace-preserving presentation, which\nreveals the feature distribution of superpixels exactly. To reduce time\ncomplexity while improving performance, a sparse representation of global nodes\nbased on noise-free online low-rank representation is used to obtain a global\ngraph at each scale. The global graph is finally used to update a local graph\nwhich is built upon all superpixels at each scale. Experimental results on the\nBSD300, BSD500, MSRC, SBD, and PASCAL VOC show the effectiveness of AFA-graph\nin comparison with state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Moyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guodong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingwu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation. (arXiv:2110.11728v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11728","description":"<p>Generative Adversarial Networks (GANs) have made a dramatic leap in\nhigh-fidelity image synthesis and stylized face generation. Recently, a\nlayer-swapping mechanism has been developed to improve the stylization\nperformance. However, this method is incapable of fitting arbitrary styles in a\nsingle model and requires hundreds of style-consistent training images for each\nstyle. To address the above issues, we propose BlendGAN for arbitrary stylized\nface generation by leveraging a flexible blending strategy and a generic\nartistic dataset. Specifically, we first train a self-supervised style encoder\non the generic artistic dataset to extract the representations of arbitrary\nstyles. In addition, a weighted blending module (WBM) is proposed to blend face\nand style representations implicitly and control the arbitrary stylization\neffect. By doing so, BlendGAN can gracefully fit arbitrary styles in a unified\nmodel while avoiding case-by-case preparation of style-consistent training\nimages. To this end, we also present a novel large-scale artistic face dataset\nAAHQ. Extensive experiments demonstrate that BlendGAN outperforms\nstate-of-the-art methods in terms of visual quality and style diversity for\nboth latent-guided and reference-guided stylized face synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingcong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zekui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UBR$^2$S: Uncertainty-Based Resampling and Reweighting Strategy for Unsupervised Domain Adaptation. (arXiv:2110.11739v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11739","description":"<p>Unsupervised domain adaptation (UDA) deals with the adaptation process of a\nmodel to an unlabeled target domain while annotated data is only available for\na given source domain. This poses a challenging task, as the domain shift\nbetween source and target instances deteriorates a model's performance when not\naddressed. In this paper, we propose UBR$^2$S - the Uncertainty-Based\nResampling and Reweighting Strategy - to tackle this problem. UBR$^2$S employs\na Monte Carlo dropout-based uncertainty estimate to obtain per-class\nprobability distributions, which are then used for dynamic resampling of\npseudo-labels and reweighting based on their sample likelihood and the\naccompanying decision error. Our proposed method achieves state-of-the-art\nresults on multiple UDA datasets with single and multi-source adaptation tasks\nand can be applied to any off-the-shelf network architecture. Code for our\nmethod is available at https://gitlab.com/tringwald/UBR2S.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ringwald_T/0/1/0/all/0/1\">Tobias Ringwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Semantic Segmentation with Self-supervision from Pseudo-classes. (arXiv:2110.11742v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11742","description":"<p>Despite the success of deep learning methods for semantic segmentation,\nfew-shot semantic segmentation remains a challenging task due to the limited\ntraining data and the generalisation requirement for unseen classes. While\nrecent progress has been particularly encouraging, we discover that existing\nmethods tend to have poor performance in terms of meanIoU when query images\ncontain other semantic classes besides the target class. To address this issue,\nwe propose a novel self-supervised task that generates random pseudo-classes in\nthe background of the query images, providing extra training data that would\notherwise be unavailable when predicting individual target classes. To that\nend, we adopted superpixel segmentation for generating the pseudo-classes. With\nthis extra supervision, we improved the meanIoU performance of the\nstate-of-the-art method by 2.5% and 5.1% on the one-shot tasks, as well as 6.7%\nand 4.4% on the five-shot tasks, on the PASCAL-5i and COCO benchmarks,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_G/0/1/0/all/0/1\">Gratianus Wesley Putra Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating and Reenacting Controllable 3D Humans with Differentiable Rendering. (arXiv:2110.11746v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11746","description":"<p>This paper proposes a new end-to-end neural rendering architecture to\ntransfer appearance and reenact human actors. Our method leverages a carefully\ndesigned graph convolutional network (GCN) to model the human body manifold\nstructure, jointly with differentiable rendering, to synthesize new videos of\npeople in different contexts from where they were initially recorded. Unlike\nrecent appearance transferring methods, our approach can reconstruct a fully\ncontrollable 3D texture-mapped model of a person, while taking into account the\nmanifold structure from body shape and texture appearance in the view\nsynthesis. Specifically, our approach models mesh deformations with a\nthree-stage GCN trained in a self-supervised manner on rendered silhouettes of\nthe human body. It also infers texture appearance with a convolutional network\nin the texture domain, which is trained in an adversarial regime to reconstruct\nhuman texture from rendered images of actors in different poses. Experiments on\ndifferent videos show that our method successfully infers specific body\ndeformations and avoid creating texture artifacts while achieving the best\nvalues for appearance in terms of Structural Similarity (SSIM), Learned\nPerceptual Image Patch Similarity (LPIPS), Mean Squared Error (MSE), and\nFr\\'echet Video Distance (FVD). By taking advantages of both differentiable\nrendering and the 3D parametric model, our method is fully controllable, which\nallows controlling the human synthesis from both pose and rendering parameters.\nThe source code is available at\nhttps://www.verlab.dcc.ufmg.br/retargeting-motion/wacv2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomes_T/0/1/0/all/0/1\">Thiago L. Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coutinho_T/0/1/0/all/0/1\">Thiago M. Coutinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_R/0/1/0/all/0/1\">Rafael Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1\">Renato Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1\">Erickson R. Nascimento</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Cross-Modal Prediction and Relation Consistency for Semi-Supervised Image Captioning. (arXiv:2110.11767v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11767","description":"<p>The task of image captioning aims to generate captions directly from images\nvia the automatically learned cross-modal generator. To build a well-performing\ngenerator, existing approaches usually need a large number of described images,\nwhich requires a huge effects on manual labeling. However, in real-world\napplications, a more general scenario is that we only have limited amount of\ndescribed images and a large number of undescribed images. Therefore, a\nresulting challenge is how to effectively combine the undescribed images into\nthe learning of cross-modal generator. To solve this problem, we propose a\nnovel image captioning method by exploiting the Cross-modal Prediction and\nRelation Consistency (CPRC), which aims to utilize the raw image input to\nconstrain the generated sentence in the commonly semantic space. In detail,\nconsidering that the heterogeneous gap between modalities always leads to the\nsupervision difficulty of using the global embedding directly, CPRC turns to\ntransform both the raw image and corresponding generated sentence into the\nshared semantic space, and measure the generated sentence from two aspects: 1)\nPrediction consistency. CPRC utilizes the prediction of raw image as soft label\nto distill useful supervision for the generated sentence, rather than employing\nthe traditional pseudo labeling; 2) Relation consistency. CPRC develops a novel\nrelation consistency between augmented images and corresponding generated\nsentences to retain the important relational knowledge. In result, CPRC\nsupervises the generated sentence from both the informativeness and\nrepresentativeness perspectives, and can reasonably use the undescribed images\nto learn a more effective generator under the semi-supervised scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongchen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hengshu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingshan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation and Active Learning for Fine-Grained Recognition in the Field of Biodiversity. (arXiv:2110.11778v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11778","description":"<p>Deep-learning methods offer unsurpassed recognition performance in a wide\nrange of domains, including fine-grained recognition tasks. However, in most\nproblem areas there are insufficient annotated training samples. Therefore, the\ntopic of transfer learning respectively domain adaptation is particularly\nimportant. In this work, we investigate to what extent unsupervised domain\nadaptation can be used for fine-grained recognition in a biodiversity context\nto learn a real-world classifier based on idealized training data, e.g.\npreserved butterflies and plants. Moreover, we investigate the influence of\ndifferent normalization layers, such as Group Normalization in combination with\nWeight Standardization, on the classifier. We discovered that domain adaptation\nworks very well for fine-grained recognition and that the normalization methods\nhave a great influence on the results. Using domain adaptation and Transferable\nNormalization, the accuracy of the classifier could be increased by up to 12.35\n% compared to the baseline. Furthermore, the domain adaptation system is\ncombined with an active learning component to improve the results. We compare\ndifferent active learning strategies with each other. Surprisingly, we found\nthat more sophisticated strategies provide better results than the random\nselection baseline for only one of the two datasets. In this case, the distance\nand diversity strategy performed best. Finally, we present a problem analysis\nof the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gruner_B/0/1/0/all/0/1\">Bernd Gruner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1\">Matthias K&#xf6;rschens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwiftLane: Towards Fast and Efficient Lane Detection. (arXiv:2110.11779v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11779","description":"<p>Recent work done on lane detection has been able to detect lanes accurately\nin complex scenarios, yet many fail to deliver real-time performance\nspecifically with limited computational resources. In this work, we propose\nSwiftLane: a simple and light-weight, end-to-end deep learning based framework,\ncoupled with the row-wise classification formulation for fast and efficient\nlane detection. This framework is supplemented with a false positive\nsuppression algorithm and a curve fitting technique to further increase the\naccuracy. Our method achieves an inference speed of 411 frames per second,\nsurpassing state-of-the-art in terms of speed while achieving comparable\nresults in terms of accuracy on the popular CULane benchmark dataset. In\naddition, our proposed framework together with TensorRT optimization\nfacilitates real-time lane detection on a Nvidia Jetson AGX Xavier as an\nembedded system while achieving a high inference speed of 56 frames per second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayasinghe_O/0/1/0/all/0/1\">Oshada Jayasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anhettigama_D/0/1/0/all/0/1\">Damith Anhettigama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1\">Sahan Hemachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariyawasam_S/0/1/0/all/0/1\">Shenali Kariyawasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasekara_P/0/1/0/all/0/1\">Peshala Jayasekara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Unlearning via Class-Discriminative Pruning. (arXiv:2110.11794v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11794","description":"<p>We explore the problem of selectively forgetting categories from trained CNN\nclassification models in the federated learning (FL). Given that the data used\nfor training cannot be accessed globally in FL, our insights probe deep into\nthe internal influence of each channel. Through the visualization of feature\nmaps activated by different channels, we observe that different channels have a\nvarying contribution to different categories in image classification. Inspired\nby this, we propose a method for scrubbing the model clean of information about\nparticular categories. The method does not require retraining from scratch, nor\nglobal access to the data used for training. Instead, we introduce the concept\nof Term Frequency Inverse Document Frequency (TF-IDF) to quantize the class\ndiscrimination of channels. Channels with high TF-IDF scores have more\ndiscrimination on the target categories and thus need to be pruned to unlearn.\nThe channel pruning is followed by a fine-tuning process to recover the\nperformance of the pruned model. Evaluated on CIFAR10 dataset, our method\naccelerates the speed of unlearning by 8.9x for the ResNet model, and 7.9x for\nthe VGG model under no degradation in accuracy, compared to retraining from\nscratch. For CIFAR100 dataset, the speedups are 9.9x and 8.4x, respectively. We\nenvision this work as a complementary block for FL towards compliance with\nlegal and ethical criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Song Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Heng Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDRVideo-GAN: Deep Generative HDR Video Reconstruction. (arXiv:2110.11795v1 [eess.IV])","link":"http://arxiv.org/abs/2110.11795","description":"<p>High dynamic range (HDR) videos provide a more visually realistic experience\nthan the standard low dynamic range (LDR) videos. Despite having significant\nprogress in HDR imaging, it is still a challenging task to capture high-quality\nHDR video with a conventional off-the-shelf camera. Existing approaches rely\nentirely on using dense optical flow between the neighboring LDR sequences to\nreconstruct an HDR frame. However, they lead to inconsistencies in color and\nexposure over time when applied to alternating exposures with noisy frames. In\nthis paper, we propose an end-to-end GAN-based framework for HDR video\nreconstruction from LDR sequences with alternating exposures. We first extract\nclean LDR frames from noisy LDR video with alternating exposures with a\ndenoising network trained in a self-supervised setting. Using optical flow, we\nthen align the neighboring alternating-exposure frames to a reference frame and\nthen reconstruct high-quality HDR frames in a complete adversarial setting. To\nfurther improve the robustness and quality of generated frames, we incorporate\ntemporal stability-based regularization term along with content and style-based\nlosses in the cost function during the training procedure. Experimental results\ndemonstrate that our framework achieves state-of-the-art performance and\ngenerates superior quality HDR frames of a video over the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Anand_M/0/1/0/all/0/1\">Mrinal Anand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harilal_N/0/1/0/all/0/1\">Nidhin Harilal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_C/0/1/0/all/0/1\">Chandan Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PropMix: Hard Sample Filtering and Proportional MixUp for Learning with Noisy Labels. (arXiv:2110.11809v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11809","description":"<p>The most competitive noisy label learning methods rely on an unsupervised\nclassification of clean and noisy samples, where samples classified as noisy\nare re-labelled and \"MixMatched\" with the clean samples. These methods have two\nissues in large noise rate problems: 1) the noisy set is more likely to contain\nhard samples that are in-correctly re-labelled, and 2) the number of samples\nproduced by MixMatch tends to be reduced because it is constrained by the small\nclean set size. In this paper, we introduce the learning algorithm PropMix to\nhandle the issues above. PropMix filters out hard noisy samples, with the goal\nof increasing the likelihood of correctly re-labelling the easy noisy samples.\nAlso, PropMix places clean and re-labelled easy noisy samples in a training set\nthat is augmented with MixUp, removing the clean set size constraint and\nincluding a large proportion of correctly re-labelled easy noisy samples. We\nalso include self-supervised pre-training to improve robustness to high noisy\nlabel scenarios. Our experiments show that PropMix has state-of-the-art (SOTA)\nresults on CIFAR-10/-100(with symmetric, asymmetric and semantic label noise),\nRed Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and\nWebVision. In severe label noise bench-marks, our results are substantially\nbetter than other methods. The code is available\nathttps://github.com/filipe-research/PropMix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_F/0/1/0/all/0/1\">Filipe R. Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IVS3D: An Open Source Framework for Intelligent Video Sampling and Preprocessing to Facilitate 3D Reconstruction. (arXiv:2110.11810v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11810","description":"<p>The creation of detailed 3D models is relevant for a wide range of\napplications such as navigation in three-dimensional space, construction\nplanning or disaster assessment. However, the complex processing and long\nexecution time for detailed 3D reconstructions require the original database to\nbe reduced in order to obtain a result in reasonable time. In this paper we\ntherefore present our framework iVS3D for intelligent pre-processing of image\nsequences. Our software is able to down sample entire videos to a specific\nframe rate, as well as to resize and crop the individual images. Furthermore,\nthanks to our modular architecture, it is easy to develop and integrate plugins\nwith additional algorithms. We provide three plugins as baseline methods that\nenable an intelligent selection of suitable images and can enrich them with\nadditional information. To filter out images affected by motion blur, we\ndeveloped a plugin that detects these frames and also searches the spatial\nneighbourhood for suitable images as replacements. The second plugin uses\noptical flow to detect redundant images caused by a temporarily stationary\ncamera. In our experiments, we show how this approach leads to a more balanced\nimage sampling if the camera speed varies, and that excluding such redundant\nimages leads to a time saving of 8.1\\percent for our sequences. A third plugin\nmakes it possible to exclude challenging image regions from the 3D\nreconstruction by performing semantic segmentation. As we think that the\ncommunity can greatly benefit from such an approach, we will publish our\nframework and the developed plugins open source using the MIT licence to allow\nco-development and easy extension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hermann_M/0/1/0/all/0/1\">Max Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollok_T/0/1/0/all/0/1\">Thomas Pollok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brommer_D/0/1/0/all/0/1\">Daniel Brommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahn_D/0/1/0/all/0/1\">Dominic Zahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-based Omnidirectional Object Detection for HermesBot Autonomous Delivery Robot with Preliminary Frame Classification. (arXiv:2110.11829v1 [cs.RO])","link":"http://arxiv.org/abs/2110.11829","description":"<p>Mobile autonomous robots include numerous sensors for environment perception.\nCameras are an essential tool for robot's localization, navigation, and\nobstacle avoidance. To process a large flow of data from the sensors, it is\nnecessary to optimize algorithms, or to utilize substantial computational\npower. In our work, we propose an algorithm for optimizing a neural network for\nobject detection using preliminary binary frame classification. An autonomous\noutdoor mobile robot with 6 rolling-shutter cameras on the perimeter providing\na 360-degree field of view was used as the experimental setup. The obtained\nexperimental results revealed that the proposed optimization accelerates the\ninference time of the neural network in the cases with up to 5 out of 6 cameras\ncontaining target objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Protasov_S/0/1/0/all/0/1\">Saian Protasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpyshev_P/0/1/0/all/0/1\">Pavel Karpyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinov_I/0/1/0/all/0/1\">Ivan Kalinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1\">Pavel Kopanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailovskiy_N/0/1/0/all/0/1\">Nikita Mikhailovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedunin_A/0/1/0/all/0/1\">Alexander Sedunin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-attribute Pizza Generator: Cross-domain Attribute Control with Conditional StyleGAN. (arXiv:2110.11830v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11830","description":"<p>Multi-attribute conditional image generation is a challenging problem in\ncomputervision. We propose Multi-attribute Pizza Generator (MPG), a conditional\nGenerative Neural Network (GAN) framework for synthesizing images from a\ntrichotomy of attributes: content, view-geometry, and implicit visual style. We\ndesign MPG by extending the state-of-the-art StyleGAN2, using a new\nconditioning technique that guides the intermediate feature maps to learn\nmulti-scale multi-attribute entangled representationsof controlling attributes.\nBecause of the complex nature of the multi-attribute image generation problem,\nwe regularize the image generation by predicting the explicit conditioning\nattributes (ingredients and view). To synthesize a pizza image with view\nattributesoutside the range of natural training images, we design a CGI pizza\ndataset PizzaView using 3D pizza models and employ it to train a view attribute\nregressor to regularize the generation process, bridging the real and CGI\ntraining datasets. To verify the efficacy of MPG, we test it on Pizza10, a\ncarefully annotated multi-ingredient pizza image dataset. MPG can successfully\ngenerate photo-realistic pizza images with desired ingredients and view\nattributes, beyond the range of those observed in real-world training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fangda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guoyao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1\">Ricardo Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Contrastive Graph Clustering. (arXiv:2110.11842v1 [cs.LG])","link":"http://arxiv.org/abs/2110.11842","description":"<p>With the explosive growth of information technology, multi-view graph data\nhave become increasingly prevalent and valuable. Most existing multi-view\nclustering techniques either focus on the scenario of multiple graphs or\nmulti-view attributes. In this paper, we propose a generic framework to cluster\nmulti-view attributed graph data. Specifically, inspired by the success of\ncontrastive learning, we propose multi-view contrastive graph clustering (MCGC)\nmethod to learn a consensus graph since the original graph could be noisy or\nincomplete and is not directly applicable. Our method composes of two key\nsteps: we first filter out the undesirable high-frequency noise while\npreserving the graph geometric features via graph filtering and obtain a smooth\nrepresentation of nodes; we then learn a consensus graph regularized by graph\ncontrastive loss. Results on several benchmark datasets show the superiority of\nour method with respect to state-of-the-art approaches. In particular, our\nsimple approach outperforms existing deep learning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_E/0/1/0/all/0/1\">Erlin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrence along Depth: Deep Convolutional Neural Networks with Recurrent Layer Aggregation. (arXiv:2110.11852v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11852","description":"<p>This paper introduces a concept of layer aggregation to describe how\ninformation from previous layers can be reused to better extract features at\nthe current layer. While DenseNet is a typical example of the layer aggregation\nmechanism, its redundancy has been commonly criticized in the literature. This\nmotivates us to propose a very light-weighted module, called recurrent layer\naggregation (RLA), by making use of the sequential structure of layers in a\ndeep CNN. Our RLA module is compatible with many mainstream deep CNNs,\nincluding ResNets, Xception and MobileNetV2, and its effectiveness is verified\nby our extensive experiments on image classification, object detection and\ninstance segmentation tasks. Specifically, improvements can be uniformly\nobserved on CIFAR, ImageNet and MS COCO datasets, and the corresponding\nRLA-Nets can surprisingly boost the performances by 2-3% on the object\ndetection task. This evidences the power of our RLA module in helping main CNNs\nbetter learn structural information in images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yanwen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIR-Nets: An Attention-Based Framework for Locally Conditioned Implicit Representations. (arXiv:2110.11860v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11860","description":"<p>This paper introduces Attentive Implicit Representation Networks (AIR-Nets),\na simple, but highly effective architecture for 3D reconstruction from point\nclouds. Since representing 3D shapes in a local and modular fashion increases\ngeneralization and reconstruction quality, AIR-Nets encode an input point cloud\ninto a set of local latent vectors anchored in 3D space, which locally describe\nthe object's geometry, as well as a global latent description, enforcing global\nconsistency. Our model is the first grid-free, encoder-based approach that\nlocally describes an implicit function. The vector attention mechanism from\n[Zhao et al. 2020] serves as main point cloud processing module, and allows for\npermutation invariance and translation equivariance. When queried with a 3D\ncoordinate, our decoder gathers information from the global and nearby local\nlatent vectors in order to predict an occupancy value. Experiments on the\nShapeNet dataset show that AIR-Nets significantly outperform previous\nstate-of-the-art encoder-based, implicit shape learning methods and especially\ndominate in the sparse setting. Furthermore, our model generalizes well to the\nFAUST dataset in a zero-shot setting. Finally, since AIR-Nets use a sparse\nlatent representation and follow a simple operating scheme, the model offers\nseveral exiting avenues for future work. Our code is available at\nhttps://github.com/SimonGiebenhain/AIR-Nets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giebenhain_S/0/1/0/all/0/1\">Simon Giebenhain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldlucke_B/0/1/0/all/0/1\">Bastian Goldl&#xfc;cke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction. (arXiv:2110.11864v1 [cs.CL])","link":"http://arxiv.org/abs/2110.11864","description":"<p>Scanned documents in electronic health records (EHR) have been a challenge\nfor decades, and are expected to stay in the foreseeable future. Current\napproaches for processing often include image preprocessing, optical character\nrecognition (OCR), and text mining. However, there is limited work that\nevaluates the choice of image preprocessing methods, the selection of NLP\nmodels, and the role of document layout. The impact of each element remains\nunknown. We evaluated this method on a use case of two key indicators for sleep\napnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from\nscanned sleep study reports. Our data that included 955 manually annotated\nreports was secondarily utilized from a previous study in the University of\nTexas Medical Branch. We performed image preprocessing: gray-scaling followed\nby 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was\nimplemented with the Tesseract OCR engine. A total of seven Bag-of-Words models\n(Logistic Regression, Ridge Regression, Lasso Regression, Support Vector\nMachine, k-Nearest Neighbor, Na\\\"ive Bayes, and Random Forest) and three deep\nlearning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also\nevaluated the combinations of image preprocessing methods (gray-scaling, dilate\n&amp; erode, increased contrast by 20%, increased contrast by 60%), and two deep\nlearning architectures (with and without structured input that provides\ndocument layout information). Our proposed method using Clinical BERT reached\nan AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of\n0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper\nuse of image preprocessing and document layout could be beneficial to scanned\ndocument processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_E/0/1/0/all/0/1\">Enshuo Hsu</a> (1, 3, and 4), <a href=\"http://arxiv.org/find/cs/1/au:+Malagaris_I/0/1/0/all/0/1\">Ioannis Malagaris</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1\">Yong-Fang Kuo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sultana_R/0/1/0/all/0/1\">Rizwana Sultana</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_K/0/1/0/all/0/1\">Kirk Roberts</a> (3) ((1) Office of Biostatistics, (2) Division of Pulmonary, Critical Care and Sleep Medicine, Department of Internal Medicine, University of Texas Medical Branch, Galveston, Texas, USA. (3) School of Biomedical Informatics, University of Texas Health Science Center at Houston, Houston, Texas, USA. (4) Center for Outcomes Research, Houston Methodist, Houston, TX, USA.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking Detection. (arXiv:2110.11867v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11867","description":"<p>In this paper, we introduce a novel road marking benchmark dataset for road\nmarking detection, addressing the limitations in the existing publicly\navailable datasets such as lack of challenging scenarios, prominence given to\nlane markings, unavailability of an evaluation script, lack of annotation\nformats and lower resolutions. Our dataset consists of 2887 total images with\n4706 road marking instances belonging to 11 classes. The images have a high\nresolution of 1920 x 1080 and capture a wide range of traffic, lighting and\nweather conditions. We provide road marking annotations in polygons, bounding\nboxes and pixel-level segmentation masks to facilitate a diverse range of road\nmarking detection algorithms. The evaluation metrics and the evaluation script\nwe provide, will further promote direct comparison of novel approaches for road\nmarking detection with existing methods. Furthermore, we evaluate the\neffectiveness of using both instance segmentation and object detection based\napproaches for the road marking detection task. Speed and accuracy scores for\ntwo instance segmentation models and two object detector models are provided as\na performance baseline for our benchmark dataset. The dataset and the\nevaluation script will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayasinghe_O/0/1/0/all/0/1\">Oshada Jayasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1\">Sahan Hemachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anhettigama_D/0/1/0/all/0/1\">Damith Anhettigama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariyawasam_S/0/1/0/all/0/1\">Shenali Kariyawasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasekara_P/0/1/0/all/0/1\">Peshala Jayasekara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Dialogue System with AUDITED. (arXiv:2110.11881v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11881","description":"<p>We devise a multimodal conversation system for dialogue utterances composed\nof text, image or both modalities. We leverage Auxiliary UnsuperviseD vIsual\nand TExtual Data (AUDITED). To improve the performance of text-based task, we\nutilize translations of target sentences from English to French to form the\nassisted supervision. For the image-based task, we employ the DeepFashion\ndataset in which we seek nearest neighbor images of positive and negative\ntarget images of the MMD data. These nearest neighbors form the nearest\nneighbor embedding providing an external context for target images. We form two\nmethods to create neighbor embedding vectors, namely Neighbor Embedding by Hard\nAssignment (NEHA) and Neighbor Embedding by Soft Assignment (NESA) which\ngenerate context subspaces per target image. Subsequently, these subspaces are\nlearnt by our pipeline as a context for the target data. We also propose a\ndiscriminator which switches between the image- and text-based tasks. We show\nimprovements over baselines on the large-scale Multimodal Dialogue Dataset\n(MMD) and SIMMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tas_Y/0/1/0/all/0/1\">Yusuf Tas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C$^{4}$Net: Contextual Compression and Complementary Combination Network for Salient Object Detection. (arXiv:2110.11887v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11887","description":"<p>Deep learning solutions of the salient object detection problem have achieved\ngreat results in recent years. The majority of these models are based on\nencoders and decoders, with a different multi-feature combination. In this\npaper, we show that feature concatenation works better than other combination\nmethods like multiplication or addition. Also, joint feature learning gives\nbetter results, because of the information sharing during their processing. We\ndesigned a Complementary Extraction Module (CEM) to extract necessary features\nwith edge preservation. Our proposed Excessiveness Loss (EL) function helps to\nreduce false-positive predictions and purifies the edges with other weighted\nloss functions. Our designed Pyramid-Semantic Module (PSM) with Global guiding\nflow (G) makes the prediction more accurate by providing high-level\ncomplementary information to shallower layers. Experimental results show that\nthe proposed model outperforms the state-of-the-art methods on all benchmark\ndatasets under three evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tunanyan_H/0/1/0/all/0/1\">Hazarapet Tunanyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Using Clothes Style Transfer for Scenario-aware Person Video Generation. (arXiv:2110.11894v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11894","description":"<p>Clothes style transfer for person video generation is a challenging task, due\nto drastic variations of intra-person appearance and video scenarios. To tackle\nthis problem, most recent AdaIN-based architectures are proposed to extract\nclothes and scenario features for generation. However, these approaches suffer\nfrom being short of fine-grained details and are prone to distort the origin\nperson. To further improve the generation performance, we propose a novel\nframework with disentangled multi-branch encoders and a shared decoder.\nMoreover, to pursue the strong video spatio-temporal consistency, an\ninner-frame discriminator is delicately designed with input being cross-frame\ndifference. Besides, the proposed framework possesses the property of scenario\nadaptation. Extensive experiments on the TEDXPeople benchmark demonstrate the\nsuperiority of our method over state-of-the-art approaches in terms of image\nquality and video coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_b/0/1/0/all/0/1\">benlai Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenyi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges in Procedural Multimodal Machine Comprehension:A Novel Way To Benchmark. (arXiv:2110.11899v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11899","description":"<p>We focus on Multimodal Machine Reading Comprehension (M3C) where a model is\nexpected to answer questions based on given passage (or context), and the\ncontext and the questions can be in different modalities. Previous works such\nas RecipeQA have proposed datasets and cloze-style tasks for evaluation.\nHowever, we identify three critical biases stemming from the question-answer\ngeneration process and memorization capabilities of large deep models. These\nbiases makes it easier for a model to overfit by relying on spurious\ncorrelations or naive data patterns. We propose a systematic framework to\naddress these biases through three Control-Knobs that enable us to generate a\ntest bed of datasets of progressive difficulty levels. We believe that our\nbenchmark (referred to as Meta-RecipeQA) will provide, for the first time, a\nfine grained estimate of a model's generalization capabilities. We also propose\na general M3C model that is used to realize several prior SOTA models and\nmotivate a novel hierarchical transformer based reasoning network (HTRN). We\nperform a detailed evaluation of these models with different language and\nvisual features on our benchmark. We observe a consistent improvement with HTRN\nover SOTA (~18% in Visual Cloze task and ~13% in average over all the tasks).\nWe also observe a drop in performance across all the models when testing on\nRecipeQA and proposed Meta-RecipeQA (e.g. 83.6% versus 67.1% for HTRN), which\nshows that the proposed dataset is relatively less biased. We conclude by\nhighlighting the impact of the control knobs with some quantitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1\">Pritish Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1\">Karan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1\">Ajay Divakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised denoising for massive noisy images. (arXiv:2110.11911v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11911","description":"<p>We propose an effective deep learning model for signal reconstruction, which\nrequires no signal prior, no noise model calibration, and no clean samples.\nThis model only assumes that the noise is independent of the measurement and\nthat the true signals share the same structured information. We demonstrate its\nperformance on a variety of real-world applications, from sub-\\r{A}ngstr\\\"{o}m\nresolution atomic images to sub-arcsecond resolution astronomy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henninen_T/0/1/0/all/0/1\">Trond R. Henninen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_D/0/1/0/all/0/1\">Debora Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erni_R/0/1/0/all/0/1\">Rolf Erni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIGS: Meta Image Generation from Scene Graphs. (arXiv:2110.11918v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11918","description":"<p>Generation of images from scene graphs is a promising direction towards\nexplicit scene generation and manipulation. However, the images generated from\nthe scene graphs lack quality, which in part comes due to high difficulty and\ndiversity in the data. We propose MIGS (Meta Image Generation from Scene\nGraphs), a meta-learning based approach for few-shot image generation from\ngraphs that enables adapting the model to different scenes and increases the\nimage quality by training on diverse sets of tasks. By sampling the data in a\ntask-driven fashion, we train the generator using meta-learning on different\nsets of tasks that are categorized based on the scene attributes. Our results\nshow that using this meta-learning approach for the generation of images from\nscene graphs achieves state-of-the-art performance in terms of image quality\nand capturing the semantic relationships in the scene. Project Website:\nhttps://migs2021.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farshad_A/0/1/0/all/0/1\">Azade Farshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musatian_S/0/1/0/all/0/1\">Sabrina Musatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamo_H/0/1/0/all/0/1\">Helisa Dhamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Activation Functions: Logit-space equivalents of Boolean Operators. (arXiv:2110.11940v1 [cs.LG])","link":"http://arxiv.org/abs/2110.11940","description":"<p>Neuronal representations within artificial neural networks are commonly\nunderstood as logits, representing the log-odds score of presence (versus\nabsence) of features within the stimulus. Under this interpretation, we can\nderive the probability $P(x_0 \\land x_1)$ that a pair of independent features\nare both present in the stimulus from their logits. By converting the resulting\nprobability back into a logit, we obtain a logit-space equivalent of the AND\noperation. However, since this function involves taking multiple exponents and\nlogarithms, it is not well suited to be directly used within neural networks.\nWe thus constructed an efficient approximation named $\\text{AND}_\\text{AIL}$\n(the AND operator Approximate for Independent Logits) utilizing only comparison\nand addition operations, which can be deployed as an activation function in\nneural networks. Like MaxOut, $\\text{AND}_\\text{AIL}$ is a generalization of\nReLU to two-dimensions. Additionally, we constructed efficient approximations\nof the logit-space equivalents to the OR and XNOR operators. We deployed these\nnew activation functions, both in isolation and in conjunction, and\ndemonstrated their effectiveness on a variety of tasks including image\nclassification, transfer learning, abstract reasoning, and compositional\nzero-shot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Scott C. Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earle_R/0/1/0/all/0/1\">Robert Earle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dEon_J/0/1/0/all/0/1\">Jason d&#x27;Eon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trappenberg_T/0/1/0/all/0/1\">Thomas Trappenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oore_S/0/1/0/all/0/1\">Sageev Oore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOFT: Softmax-free Transformer with Linear Complexity. (arXiv:2110.11945v1 [cs.CV])","link":"http://arxiv.org/abs/2110.11945","description":"<p>Vision transformers (ViTs) have pushed the state-of-the-art for various\nvisual recognition tasks by patch-wise image tokenization followed by\nself-attention. However, the employment of self-attention modules results in a\nquadratic complexity in both computation and memory usage. Various attempts on\napproximating the self-attention computation with linear complexity have been\nmade in Natural Language Processing. However, an in-depth analysis in this work\nshows that they are either theoretically flawed or empirically ineffective for\nvisual recognition. We further identify that their limitations are rooted in\nkeeping the softmax self-attention during approximations. Specifically,\nconventional self-attention is computed by normalizing the scaled dot-product\nbetween token feature vectors. Keeping this softmax operation challenges any\nsubsequent linearization efforts. Based on this insight, for the first time, a\nsoftmax-free transformer or SOFT is proposed. To remove softmax in\nself-attention, Gaussian kernel function is used to replace the dot-product\nsimilarity without further normalization. This enables a full self-attention\nmatrix to be approximated via a low-rank matrix decomposition. The robustness\nof the approximation is achieved by calculating its Moore-Penrose inverse using\na Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT\nsignificantly improves the computational efficiency of existing ViT variants.\nCrucially, with a linear complexity, much longer token sequences are permitted\nin SOFT, resulting in superior trade-off between accuracy and complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jinghan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weiguo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Proposals for Practical Energy-Based Regression. (arXiv:2110.11948v1 [cs.LG])","link":"http://arxiv.org/abs/2110.11948","description":"<p>Energy-based models (EBMs) have experienced a resurgence within machine\nlearning in recent years, including as a promising alternative for\nprobabilistic regression. However, energy-based regression requires a proposal\ndistribution to be manually designed for training, and an initial estimate has\nto be provided at test-time. We address both of these issues by introducing a\nconceptually simple method to automatically learn an effective proposal\ndistribution, which is parameterized by a separate network head. To this end,\nwe derive a surprising result, leading to a unified training objective that\njointly minimizes the KL divergence from the proposal to the EBM, and the\nnegative log-likelihood of the EBM. At test-time, we can then employ importance\nsampling with the trained proposal to efficiently evaluate the learned EBM and\nproduce stand-alone predictions. Furthermore, we utilize our derived training\nobjective to learn mixture density networks (MDNs) with a jointly trained\nenergy-based teacher, consistently outperforming conventional MDN training on\nfour real-world regression tasks within computer vision. Code is available at\nhttps://github.com/fregu856/ebms_proposals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gustafsson_F/0/1/0/all/0/1\">Fredrik K. Gustafsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1\">Thomas B. Sch&#xf6;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Approach Coloured Object Tracker with Adaptive Model and Bandwidth using Mean Shift Algorithm. (arXiv:1207.2602v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1207.2602","description":"<p>The traditional color-based mean-shift tracking algorithm is popular among\ntracking methods due to its simple and efficient procedure, however, the lack\nof dynamism in its target model makes it unsuitable for tracking objects which\nhave changes in their sizes and shapes. In this paper, we propose a fast novel\nthreephase colored object tracker algorithm based on mean shift idea while\nutilizing adaptive model. The proposed method can improve the mentioned\nweaknesses of the original mean-shift algorithm. The experimental results show\nthat the new method is feasible, robust and has acceptable speed in comparison\nwith other algorithms.15 page,\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1\">Seyed Amir Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahzoun_M/0/1/0/all/0/1\">Mohammad Reza Mahzoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large RGB-D Dataset for Semi-supervised Monocular Depth Estimation. (arXiv:1904.10230v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.10230","description":"<p>Current self-supervised methods for monocular depth estimation are largely\nbased on deeply nested convolutional networks that leverage stereo image pairs\nor monocular sequences during a training phase. However, they often exhibit\ninaccurate results around occluded regions and depth boundaries. In this paper,\nwe present a simple yet effective approach for monocular depth estimation using\nstereo image pairs. The study aims to propose a student-teacher strategy in\nwhich a shallow student network is trained with the auxiliary information\nobtained from a deeper and more accurate teacher network. Specifically, we\nfirst train the stereo teacher network by fully utilizing the binocular\nperception of 3-D geometry and then use the depth predictions of the teacher\nnetwork to train the student network for monocular depth inference. This\nenables us to exploit all available depth data from massive unlabeled stereo\npairs. We propose a strategy that involves the use of a data ensemble to merge\nthe multiple depth predictions of the teacher network to improve the training\nsamples by collecting non-trivial knowledge beyond a single prediction. To\nrefine the inaccurate depth estimation that is used when training the student\nnetwork, we further propose stereo confidence-guided regression loss that\nhandles the unreliable pseudo depth values in occlusion, texture-less region,\nand repetitive pattern. To complement the existing dataset comprising outdoor\ndriving scenes, we built a novel large-scale dataset consisting of one million\noutdoor stereo images taken using hand-held stereo cameras. Finally, we\ndemonstrate that the monocular depth estimation network provides feature\nrepresentations that are suitable for high-level vision tasks. The experimental\nresults for various outdoor scenarios demonstrate the effectiveness and\nflexibility of our approach, which outperforms state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaehoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dongbo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Region-based Randers Geodesic Approach for Image Segmentation. (arXiv:1912.10122v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.10122","description":"<p>The minimal path model based on the Eikonal partial differential equation has\nserved as a fundamental tool for the applications of image segmentation and\nboundary detection in the passed two decades. However, the existing approaches\ncommonly only exploit the image edge-based features for computing minimal\npaths, potentially limiting their performance in complicated segmentation\nsituations. In this paper, we introduce a new variational image segmentation\nmodel based on the minimal path framework and the eikonal PDE, where the\nregion-based appearance term that defines then regional homogeneity features\ncan be taken into account for estimating the associated minimal paths. This is\ndone by constructing a Randers geodesic metric interpretation to the\nregion-based active contour energy. As a result, the minimization of the active\ncontour energy is transformed to finding the solution to the Randers eikonal\nPDE.\n</p>\n<p>We also suggest a practical interactive image segmentation strategy, where\nthe target boundary can be delineated by the concatenation of the piecewise\ngeodesic paths. We invoke the Finsler variant of the fast marching method to\nestimate the geodesic distance map, yielding an efficient implementation of the\nproposed Eikonal region-based active contour model. Experimental results on\nboth synthetic and real images exhibit that our model indeed achieves\nencouraging segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Da Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirebeau_J/0/1/0/all/0/1\">Jean-Marie Mirebeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Huazhong Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1\">Laurent D. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation. (arXiv:2004.03686v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.03686","description":"<p>Differently from 2D image datasets such as COCO, large-scale human datasets\nwith 3D ground-truth annotations are very difficult to obtain in the wild. In\nthis paper, we address this problem by augmenting existing 2D datasets with\nhigh-quality 3D pose fits. Remarkably, the resulting annotations are sufficient\nto train from scratch 3D pose regressor networks that outperform the current\nstate-of-the-art on in-the-wild benchmarks such as 3DPW. Additionally, training\non our augmented data is straightforward as it does not require to mix multiple\nand incompatible 2D and 3D datasets or to use complicated network architectures\nand training procedures. This simplified pipeline affords additional\nimprovements, including injecting extreme crop augmentations to better\nreconstruct highly truncated people, and incorporating auxiliary inputs to\nimprove 3D pose estimation accuracy. It also reduces the dependency on 3D\ndatasets such as H36M that have restrictive licenses. We also use our method to\nintroduce new benchmarks for the study of real-world challenges such as\nocclusions, truncations, and rare body poses. In order to obtain such high\nquality 3D pseudo-annotations, inspired by progress in internal learning, we\nintroduce Exemplar Fine-Tuning (EFT). EFT combines the re-projection accuracy\nof fitting methods like SMPLify with a 3D pose prior implicitly captured by a\npre-trained 3D pose regressor network. We show that EFT produces 3D annotations\nthat result in better downstream performance and are qualitatively preferable\nin an extensive human-based assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hanbyul Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDDA: a large-scale multi-domain dataset for autonomous driving. (arXiv:2004.08298v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.08298","description":"<p>Semantic segmentation is key in autonomous driving. Using deep visual\nlearning architectures is not trivial in this context, because of the\nchallenges in creating suitable large scale annotated datasets. This issue has\nbeen traditionally circumvented through the use of synthetic datasets, that\nhave become a popular resource in this field. They have been released with the\nneed to develop semantic segmentation algorithms able to close the visual\ndomain shift between the training and test data. Although exacerbated by the\nuse of artificial data, the problem is extremely relevant in this field even\nwhen training on real data. Indeed, weather conditions, viewpoint changes and\nvariations in the city appearances can vary considerably from car to car, and\neven at test time for a single, specific vehicle. How to deal with domain\nadaptation in semantic segmentation, and how to leverage effectively several\ndifferent data distributions (source domains) are important research questions\nin this field. To support work in this direction, this paper contributes a new\nlarge scale, synthetic dataset for semantic segmentation with more than 100\ndifferent source visual domains. The dataset has been created to explicitly\naddress the challenges of domain shift between training and test data in\nvarious weather and view point conditions, in seven different city types.\nExtensive benchmark experiments assess the dataset, showcasing open challenges\nfor the current state of the art. The dataset will be available at:\nhttps://idda-dataset.github.io/home/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberti_E/0/1/0/all/0/1\">Emanuele Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavera_A/0/1/0/all/0/1\">Antonio Tavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Retinex based GAN Pipeline to Utilize Paired and Unpaired Datasets for Enhancing Low Light Images. (arXiv:2006.15304v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2006.15304","description":"<p>Low light image enhancement is an important challenge for the development of\nrobust computer vision algorithms. The machine learning approaches to this have\nbeen either unsupervised, supervised based on paired dataset or supervised\nbased on unpaired dataset. This paper presents a novel deep learning pipeline\nthat can learn from both paired and unpaired datasets. Convolution Neural\nNetworks (CNNs) that are optimized to minimize standard loss, and Generative\nAdversarial Networks (GANs) that are optimized to minimize the adversarial loss\nare used to achieve different steps of the low light image enhancement process.\nCycle consistency loss and a patched discriminator are utilized to further\nimprove the performance. The paper also analyses the functionality and the\nperformance of different components, hidden layers, and the entire pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Weligampola_H/0/1/0/all/0/1\">Harshana Weligampola</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jayatilaka_G/0/1/0/all/0/1\">Gihan Jayatilaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sritharan_S/0/1/0/all/0/1\">Suren Sritharan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Godaliyadda_R/0/1/0/all/0/1\">Roshan Godaliyadda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ekanayaka_P/0/1/0/all/0/1\">Parakrama Ekanayaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ragel_R/0/1/0/all/0/1\">Roshan Ragel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herath_V/0/1/0/all/0/1\">Vijitha Herath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Future Urban Scenes Generation Through Vehicles Synthesis. (arXiv:2007.00323v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.00323","description":"<p>In this work we propose a deep learning pipeline to predict the visual future\nappearance of an urban scene. Despite recent advances, generating the entire\nscene in an end-to-end fashion is still far from being achieved. Instead, here\nwe follow a two stages approach, where interpretable information is included in\nthe loop and each actor is modelled independently. We leverage a per-object\nnovel view synthesis paradigm; i.e. generating a synthetic representation of an\nobject undergoing a geometrical roto-translation in the 3D space. Our model can\nbe easily conditioned with constraints (e.g. input trajectories) provided by\nstate-of-the-art tracking methods or by the user itself. This allows us to\ngenerate a set of diverse realistic futures starting from the same input in a\nmulti-modal fashion. We visually and quantitatively show the superiority of\nthis approach over traditional end-to-end scene-generation methods on CityFlow,\na challenging real world dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simoni_A/0/1/0/all/0/1\">Alessandro Simoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergamini_L/0/1/0/all/0/1\">Luca Bergamini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palazzi_A/0/1/0/all/0/1\">Andrea Palazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shedding Light on Blind Spots: Developing a Reference Architecture to Leverage Video Data for Process Mining. (arXiv:2010.11289v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.11289","description":"<p>Process mining is one of the most active research streams in business process\nmanagement. In recent years, numerous methods have been proposed for analyzing\nstructured process data. Yet, in many cases, it is only the digitized parts of\nprocesses that are directly captured from process-aware information systems,\nand manual activities often result in blind spots. While the use of video\ncameras to observe these activities could help to fill this gap, a standardized\napproach to extracting event logs from unstructured video data remains lacking.\nHere, we propose a reference architecture to bridge the gap between computer\nvision and process mining. Various evaluation activities (i.e., competing\nartifact analysis, prototyping, and real-world application) ensured that the\nproposed reference architecture allows flexible, use-case-driven, and\ncontext-specific instantiations. Our results also show that an exemplary\nsoftware prototype instantiation of the proposed reference architecture is\ncapable of automatically extracting most of the process-relevant events from\nunstructured video data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kratsch_W/0/1/0/all/0/1\">Wolfgang Kratsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenig_F/0/1/0/all/0/1\">Fabian Koenig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roeglinger_M/0/1/0/all/0/1\">Maximilian Roeglinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Motion Blind Video Stabilization. (arXiv:2011.09697v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09697","description":"<p>Despite the advances in the field of generative models in computer vision,\nvideo stabilization still lacks a pure regressive deep-learning-based\nformulation. Deep video stabilization is generally formulated with the help of\nexplicit motion estimation modules due to the lack of a dataset containing\npairs of videos with similar perspective but different motion. Therefore, the\ndeep learning approaches for this task have difficulties in the pixel-level\nsynthesis of latent stabilized frames, and resort to motion estimation modules\nfor indirect transformations of the unstable frames to stabilized frames,\nleading to the loss of visual content near the frame boundaries. In this work,\nwe aim to declutter this over-complicated formulation of video stabilization\nwith the help of a novel dataset that contains pairs of training videos with\nsimilar perspective but different motion, and verify its effectiveness by\nsuccessfully learning motion blind full-frame video stabilization through\nemploying strictly conventional generative techniques and further improve the\nstability through a curriculum-learning inspired adversarial training strategy.\nThrough extensive experimentation, we show the quantitative and qualitative\nadvantages of the proposed approach to the state-of-the-art video stabilization\napproaches. Moreover, our method achieves $\\sim3\\times$ speed-up over the\ncurrently available fastest video stabilization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Muhammad Kashif Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sangjoon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae Hyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations. (arXiv:2011.11953v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11953","description":"<p>Existing person re-identification models often have low generalizability,\nwhich is mostly due to limited availability of large-scale labeled data in\ntraining. However, labeling large-scale training data is very expensive and\ntime-consuming, while large-scale synthetic dataset shows promising value in\nlearning generalizable person re-identification models. Therefore, in this\npaper a novel and practical person re-identification task is proposed,i.e. how\nto use labeled synthetic dataset and unlabeled real-world dataset to train a\nuniversal model. In this way, human annotations are no longer required, and it\nis scalable to large and diverse real-world datasets. To address the task, we\nintroduce a framework with high generalizability, namely DomainMix.\nSpecifically, the proposed method firstly clusters the unlabeled real-world\nimages and selects the reliable clusters. During training, to address the large\ndomain gap between two domains, a domain-invariant feature learning method is\nproposed, which introduces a new loss,i.e. domain balance loss, to conduct an\nadversarial learning between domain-invariant feature learning and domain\ndiscrimination, and meanwhile learns a discriminative feature for person\nre-identification. This way, the domain gap between synthetic and real-world\ndata is much reduced, and the learned feature is generalizable thanks to the\nlarge-scale and diverse training data. Experimental results show that the\nproposed annotation-free method is more or less comparable to the counterpart\ntrained with full human annotations, which is quite promising. In addition, it\nachieves the current state of the art on several person re-identification\ndatasets under direct cross-dataset evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1\">Cuicui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Distributions via Optimal Transport for Semi-Supervised Learning. (arXiv:2012.03790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03790","description":"<p>Semi-Supervised Learning (SSL) approaches have been an influential framework\nfor the usage of unlabeled data when there is not a sufficient amount of\nlabeled data available over the course of training. SSL methods based on\nConvolutional Neural Networks (CNNs) have recently provided successful results\non standard benchmark tasks such as image classification. In this work, we\nconsider the general setting of SSL problem where the labeled and unlabeled\ndata come from the same underlying probability distribution. We propose a new\napproach that adopts an Optimal Transport (OT) technique serving as a metric of\nsimilarity between discrete empirical probability measures to provide\npseudo-labels for the unlabeled data, which can then be used in conjunction\nwith the initial labeled data to train the CNN model in an SSL manner. We have\nevaluated and compared our proposed method with state-of-the-art SSL algorithms\non standard datasets to demonstrate the superiority and effectiveness of our\nSSL algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_H/0/1/0/all/0/1\">Hadi Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding. (arXiv:2101.01881v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.01881","description":"<p>To reduce a model size but retain performance, we often rely on knowledge\ndistillation (KD) which transfers knowledge from a large \"teacher\" model to a\nsmaller \"student\" model. However, KD on multimodal datasets such as\nvision-language tasks is relatively unexplored, and digesting multimodal\ninformation is challenging since different modalities present different types\nof information. In this paper, we perform a large-scale empirical study to\ninvestigate the importance and effects of each modality in knowledge\ndistillation. Furthermore, we introduce a multimodal knowledge distillation\nframework, modality-specific distillation (MSD), to transfer knowledge from a\nteacher on multimodal tasks by learning the teacher's behavior within each\nmodality. The idea aims at mimicking a teacher's modality-specific predictions\nby introducing auxiliary loss terms for each modality. Furthermore, because\neach modality has different saliency for predictions, we define saliency scores\nfor each modality and investigate saliency-based weighting schemes for the\nauxiliary losses. We further study a weight learning approach to learn the\noptimal weights on these loss terms. In our empirical analysis, we examine the\nsaliency of each modality in KD, demonstrate the effectiveness of the weighting\nscheme in MSD, and show that it achieves better performance than KD on four\nmultimodal datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Achieving Efficient Robustness with Adversarial Supervised Contrastive Learning. (arXiv:2101.10027v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.10027","description":"<p>Contrastive learning (CL) has recently emerged as an effective approach to\nlearning representation in a range of downstream tasks. Central to this\napproach is the selection of positive (similar) and negative (dissimilar) sets\nto provide the model the opportunity to `contrast' between data and class\nrepresentation in the latent space. In this paper, we investigate CL for\nimproving model robustness using adversarial samples. We first designed and\nperformed a comprehensive study to understand how adversarial vulnerability\nbehaves in the latent space. Based on this empirical evidence, we propose an\neffective and efficient supervised contrastive learning to achieve model\nrobustness against adversarial attacks. Moreover, we propose a new sample\nselection strategy that optimizes the positive/negative sets by removing\nredundancy and improving correlation with the anchor. Extensive experiments\nshow that our Adversarial Supervised Contrastive Learning (ASCL) approach\nachieves comparable performance with the state-of-the-art defenses while\nsignificantly outperforms other CL-based defense methods by using only $42.8\\%$\npositives and $6.3\\%$ negatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_A/0/1/0/all/0/1\">Anh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montague_P/0/1/0/all/0/1\">Paul Montague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camtepe_S/0/1/0/all/0/1\">Seyit Camtepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Uncertainty Estimation of Learned Variational MRI Reconstruction. (arXiv:2102.06665v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.06665","description":"<p>Recent deep learning approaches focus on improving quantitative scores of\ndedicated benchmarks, and therefore only reduce the observation-related\n(aleatoric) uncertainty. However, the model-immanent (epistemic) uncertainty is\nless frequently systematically analyzed. In this work, we introduce a Bayesian\nvariational framework to quantify the epistemic uncertainty. To this end, we\nsolve the linear inverse problem of undersampled MRI reconstruction in a\nvariational setting. The associated energy functional is composed of a data\nfidelity term and the total deep variation (TDV) as a learned parametric\nregularizer. To estimate the epistemic uncertainty we draw the parameters of\nthe TDV regularizer from a multivariate Gaussian distribution, whose mean and\ncovariance matrix are learned in a stochastic optimal control problem. In\nseveral numerical experiments, we demonstrate that our approach yields\ncompetitive results for undersampled MRI reconstruction. Moreover, we can\naccurately quantify the pixelwise epistemic uncertainty, which can serve\nradiologists as an additional resource to visualize reconstruction reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Narnhofer_D/0/1/0/all/0/1\">Dominik Narnhofer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Effland_A/0/1/0/all/0/1\">Alexander Effland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobler_E/0/1/0/all/0/1\">Erich Kobler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knoll_F/0/1/0/all/0/1\">Florian Knoll</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pock_T/0/1/0/all/0/1\">Thomas Pock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Branch Architecture Search for Unsupervised Domain Adaptation. (arXiv:2102.06679v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06679","description":"<p>Unsupervised Domain Adaptation (UDA) is a key issue in visual recognition, as\nit allows to bridge different visual domains enabling robust performances in\nthe real world. To date, all proposed approaches rely on human expertise to\nmanually adapt a given UDA method (e.g. DANN) to a specific backbone\narchitecture (e.g. ResNet). This dependency on handcrafted designs limits the\napplicability of a given approach in time, as old methods need to be constantly\nadapted to novel backbones.\n</p>\n<p>Existing Neural Architecture Search (NAS) approaches cannot be directly\napplied to mitigate this issue, as they rely on labels that are not available\nin the UDA setting. Furthermore, most NAS methods search for full\narchitectures, which precludes the use of pre-trained models, essential in a\nvast range of UDA settings for reaching SOTA results. To the best of our\nknowledge, no prior work has addressed these aspects in the context of NAS for\nUDA. Here we tackle both aspects with an Adversarial Branch Architecture Search\nfor UDA (ABAS): i. we address the lack of target labels by a novel data-driven\nensemble approach for model selection; and ii. we search for an auxiliary\nadversarial branch, attached to a pre-trained backbone, which drives the domain\nalignment.\n</p>\n<p>We extensively validate ABAS to improve two modern UDA techniques, DANN and\nALDA, on three standard visual recognition datasets (Office31, Office-Home and\nPACS). In all cases, ABAS robustly finds the adversarial branch architectures\nand parameters which yield best performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robbiano_L/0/1/0/all/0/1\">Luca Robbiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Muhammad Rameez Ur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galasso_F/0/1/0/all/0/1\">Fabio Galasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlucci_F/0/1/0/all/0/1\">Fabio Maria Carlucci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HandTailor: Towards High-Precision Monocular 3D Hand Recovery. (arXiv:2102.09244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.09244","description":"<p>3D hand pose estimation and shape recovery are challenging tasks in computer\nvision. We introduce a novel framework HandTailor, which combines a\nlearning-based hand module and an optimization-based tailor module to achieve\nhigh-precision hand mesh recovery from a monocular RGB image. The proposed hand\nmodule unifies perspective projection and weak perspective projection in a\nsingle network towards accuracy-oriented and in-the-wild scenarios. The\nproposed tailor module then utilizes the coarsely reconstructed mesh model\nprovided by the hand module as initialization, and iteratively optimizes an\nenergy function to obtain better results. The tailor module is time-efficient,\ncosts only 8ms per frame on a modern CPU. We demonstrate that HandTailor can\nget state-of-the-art performance on several public benchmarks, with impressive\nqualitative results on in-the-wild experiments. Code and video are available on\nour project webpage https://sites.google.com/view/handtailor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Sucheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chongzhao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Training Enhances Online Continual Learning. (arXiv:2103.14010v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14010","description":"<p>In continual learning, a system must incrementally learn from a\nnon-stationary data stream without catastrophic forgetting. Recently, multiple\nmethods have been devised for incrementally learning classes on large-scale\nimage classification tasks, such as ImageNet. State-of-the-art continual\nlearning methods use an initial supervised pre-training phase, in which the\nfirst 10% - 50% of the classes in a dataset are used to learn representations\nin an offline manner before continual learning of new classes begins. We\nhypothesize that self-supervised pre-training could yield features that\ngeneralize better than supervised learning, especially when the number of\nsamples used for pre-training is small. We test this hypothesis using the\nself-supervised MoCo-V2, Barlow Twins, and SwAV algorithms. On ImageNet, we\nfind that these methods outperform supervised pre-training considerably for\nonline continual learning, and the gains are larger when fewer samples are\navailable. Our findings are consistent across three online continual learning\nalgorithms. Our best system achieves a 14.95% relative increase in top-1\naccuracy on class incremental ImageNet over the prior state of the art for\nonline continual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gallardo_J/0/1/0/all/0/1\">Jhair Gallardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Uncertainty and Expected Gradient Length -- Regression: Two Sides Of The Same Coin?. (arXiv:2104.09493v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09493","description":"<p>Active learning algorithms select a subset of data for annotation to maximize\nthe model performance on a budget. One such algorithm is Expected Gradient\nLength, which as the name suggests uses the approximate gradient induced per\nexample in the sampling process. While Expected Gradient Length has been\nsuccessfully used for classification and regression, the formulation for\nregression remains intuitively driven. Hence, our theoretical contribution\ninvolves deriving this formulation, thereby supporting the experimental\nevidence. Subsequently, we show that expected gradient length in regression is\nequivalent to Bayesian uncertainty. If certain assumptions are infeasible, our\nalgorithmic contribution (EGL++) approximates the effect of ensembles with a\nsingle deterministic network. Instead of computing multiple possible inferences\nper input, we leverage previously annotated samples to quantify the probability\nof previous labels being the true label. Such an approach allows us to extend\nexpected gradient length to a new task: human pose estimation. We perform\nexperimental validation on two human pose datasets (MPII and LSP/LSPET),\nhighlighting the interpretability and competitiveness of EGL++ with different\nactive learning algorithms for human pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1\">Megh Shukla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation. (arXiv:2105.04447v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04447","description":"<p>We propose a novel scene flow estimation approach to capture and infer 3D\nmotions from point clouds. Estimating 3D motions for point clouds is\nchallenging, since a point cloud is unordered and its density is significantly\nnon-uniform. Such unstructured data poses difficulties in matching\ncorresponding points between point clouds, leading to inaccurate flow\nestimation. We propose a novel architecture named Sparse\nConvolution-Transformer Network (SCTN) that equips the sparse convolution with\nthe transformer. Specifically, by leveraging the sparse convolution, SCTN\ntransfers irregular point cloud into locally consistent flow features for\nestimating continuous and consistent motions within an object/local object\npart. We further propose to explicitly learn point relations using a point\ntransformer module, different from exiting methods. We show that the learned\nrelation-based contextual information is rich and helpful for matching\ncorresponding points, benefiting scene flow estimation. In addition, a novel\nloss function is proposed to adaptively encourage flow consistency according to\nfeature similarity. Extensive experiments demonstrate that our proposed\napproach achieves a new state of the art in scene flow estimation. Our approach\nachieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene\nFlow respectively, which significantly outperforms previous methods by large\nmargins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Cheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFENet: A Novel Dimension Fusion Edge Guided Network for Brain MRI Segmentation. (arXiv:2105.07962v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.07962","description":"<p>The rapid increment of morbidity of brain stroke in the last few years have\nbeen a driving force towards fast and accurate segmentation of stroke lesions\nfrom brain MRI images. With the recent development of deep-learning,\ncomputer-aided and segmentation methods of ischemic stroke lesions have been\nuseful for clinicians in early diagnosis and treatment planning. However, most\nof these methods suffer from inaccurate and unreliable segmentation results\nbecause of their inability to capture sufficient contextual features from the\nMRI volumes. To meet these requirements, 3D convolutional neural networks have\nbeen proposed, which, however, suffer from huge computational requirements. To\nmitigate these problems, we propose a novel Dimension Fusion Edge-guided\nnetwork (DFENet) that can meet both of these requirements by fusing the\nfeatures of 2D and 3D CNNs. Unlike other methods, our proposed network uses a\nparallel partial decoder (PPD) module for aggregating and upsampling selected\nfeatures, rich in important contextual information. Additionally, we use an\nedge-guidance and enhanced mixing loss for constantly supervising and\nimprovising the learning process of the network. The proposed method is\nevaluated on publicly available Anatomical Tracings of Lesions After Stroke\n(ATLAS) dataset, resulting in mean DSC, IoU, Precision and Recall values of\n0.5457, 0.4015, 0.6371, and 0.4969 respectively. The results, when compared to\nother state-of-the-art methods, outperforms them by a significant margin.\nTherefore, the proposed model is robust, accurate, superior to the existing\nmethods, and can be relied upon for biomedical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Basak_H/0/1/0/all/0/1\">Hritam Basak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_R/0/1/0/all/0/1\">Rukhshanda Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rana_A/0/1/0/all/0/1\">Ajay Rana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhance to Read Better: A Multi-Task Adversarial Network for Handwritten Document Image Enhancement. (arXiv:2105.12710v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12710","description":"<p>Handwritten document images can be highly affected by degradation for\ndifferent reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.),\nbad scanning process and so on. These artifacts raise many readability issues\nfor current Handwritten Text Recognition (HTR) algorithms and severely devalue\ntheir efficiency. In this paper, we propose an end to end architecture based on\nGenerative Adversarial Networks (GANs) to recover the degraded documents into a\nclean and readable form. Unlike the most well-known document binarization\nmethods, which try to improve the visual quality of the degraded document, the\nproposed architecture integrates a handwritten text recognizer that promotes\nthe generated document image to be more readable. To the best of our knowledge,\nthis is the first work to use the text information while binarizing handwritten\ndocuments. Extensive experiments conducted on degraded Arabic and Latin\nhandwritten documents demonstrate the usefulness of integrating the recognizer\nwithin the GAN architecture, which improves both the visual quality and the\nreadability of the degraded document images. Moreover, we outperform the state\nof the art in H-DIBCO challenges, after fine tuning our pre-trained model with\nsynthetically degraded Latin handwritten images, on this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jemni_S/0/1/0/all/0/1\">Sana Khamekhem Jemni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02636","description":"<p>As humans, we understand events in the visual world contextually, performing\nmultimodal reasoning across time to make inferences about the past, present,\nand future. We introduce MERLOT, a model that learns multimodal script\nknowledge by watching millions of YouTube videos with transcribed speech -- in\nan entirely label-free, self-supervised manner. By pretraining with a mix of\nboth frame-level (spatial) and video-level (temporal) objectives, our model not\nonly learns to match images to temporally corresponding words, but also to\ncontextualize what is happening globally over time. As a result, MERLOT\nexhibits strong out-of-the-box representations of temporal commonsense, and\nachieves state-of-the-art performance on 12 different video QA datasets when\nfinetuned. It also transfers well to the world of static images, allowing\nmodels to reason about the dynamic context behind visual scenes. On Visual\nCommonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,\noutperforming state-of-the-art models of similar size by over 3%, even those\nthat make heavy use of auxiliary supervised data (like object bounding boxes).\n</p>\n<p>Ablation analyses demonstrate the complementary importance of: 1) training on\nvideos versus static images; 2) scaling the magnitude and diversity of the\npretraining video corpus; and 3) using diverse objectives that encourage\nfull-stack multimodal reasoning, from the recognition to cognition level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jize Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Learning from Single Positive Labels. (arXiv:2106.09708v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09708","description":"<p>Predicting all applicable labels for a given image is known as multi-label\nclassification. Compared to the standard multi-class case (where each image has\nonly one label), it is considerably more challenging to annotate training data\nfor multi-label classification. When the number of potential labels is large,\nhuman annotators find it difficult to mention all applicable labels for each\ntraining image. Furthermore, in some settings detection is intrinsically\ndifficult e.g. finding small object instances in high resolution images. As a\nresult, multi-label training data is often plagued by false negatives. We\nconsider the hardest version of this problem, where annotators provide only one\nrelevant label for each image. As a result, training sets will have only one\npositive label per image and no confirmed negatives. We explore this special\ncase of learning from missing labels across four different multi-label image\nclassification datasets for both linear classifiers and end-to-end fine-tuned\ndeep networks. We extend existing multi-label losses to this setting and\npropose novel variants that constrain the number of expected positive labels\nduring training. Surprisingly, we show that in some cases it is possible to\napproach the performance of fully labeled classifiers despite training with\nsignificantly fewer confirmed labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1\">Elijah Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorieul_T/0/1/0/all/0/1\">Titouan Lorieul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1\">Dan Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Exit Vision Transformer for Dynamic Inference. (arXiv:2106.15183v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15183","description":"<p>Deep neural networks can be converted to multi-exit architectures by\ninserting early exit branches after some of their intermediate layers. This\nallows their inference process to become dynamic, which is useful for time\ncritical IoT applications with stringent latency requirements, but with\ntime-variant communication and computation resources. In particular, in edge\ncomputing systems and IoT networks where the exact computation time budget is\nvariable and not known beforehand. Vision Transformer is a recently proposed\narchitecture which has since found many applications across various domains of\ncomputer vision. In this work, we propose seven different architectures for\nearly exit branches that can be used for dynamic inference in Vision\nTransformer backbones. Through extensive experiments involving both\nclassification and regression problems, we show that each one of our proposed\narchitectures could prove useful in the trade-off between accuracy and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1\">Arian Bakhtiarnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ray-ONet: Efficient 3D Reconstruction From A Single RGB Image. (arXiv:2107.01899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01899","description":"<p>We propose Ray-ONet to reconstruct detailed 3D models from monocular images\nefficiently. By predicting a series of occupancy probabilities along a ray that\nis back-projected from a pixel in the camera coordinate, our method Ray-ONet\nimproves the reconstruction accuracy in comparison with Occupancy Networks\n(ONet), while reducing the network inference complexity to O($N^2$). As a\nresult, Ray-ONet achieves state-of-the-art performance on the ShapeNet\nbenchmark with more than 20$\\times$ speed-up at $128^3$ resolution and\nmaintains a similar memory footprint during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_W/0/1/0/all/0/1\">Wenjing Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kejie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09391","description":"<p>We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion, zoom, rotation, image cut and Gaussian perturbations\nimproves, while significantly improving the performance on clean images without\nany data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1\">Sadaf Gulshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning of Occlusion Aware Flow Guided 3D Geometry Perception with Adaptive Cross Weighted Loss from Monocular Videos. (arXiv:2108.03893v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03893","description":"<p>Self-supervised deep learning-based 3D scene understanding methods can\novercome the difficulty of acquiring the densely labeled ground-truth and have\nmade a lot of advances. However, occlusions and moving objects are still some\nof the major limitations. In this paper, we explore the learnable occlusion\naware optical flow guided self-supervised depth and camera pose estimation by\nan adaptive cross weighted loss to address the above limitations. Firstly, we\nexplore to train the learnable occlusion mask fused optical flow network by an\nocclusion-aware photometric loss with the temporally supplemental information\nand backward-forward consistency of adjacent views. And then, we design an\nadaptive cross-weighted loss between the depth-pose and optical flow loss of\nthe geometric and photometric error to distinguish the moving objects which\nviolate the static scene assumption. Our method shows promising results on\nKITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good\ngeneralization ability under a variety of challenging scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiaojiao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSP-SLAM: Object Oriented SLAM with Deep Shape Priors. (arXiv:2108.09481v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09481","description":"<p>We propose DSP-SLAM, an object-oriented SLAM system that builds a rich and\naccurate joint map of dense 3D models for foreground objects, and sparse\nlandmark points to represent the background. DSP-SLAM takes as input the 3D\npoint cloud reconstructed by a feature-based SLAM system and equips it with the\nability to enhance its sparse map with dense reconstructions of detected\nobjects. Objects are detected via semantic instance segmentation, and their\nshape and pose is estimated using category-specific deep shape embeddings as\npriors, via a novel second order optimization. Our object-aware bundle\nadjustment builds a pose-graph to jointly optimize camera poses, object\nlocations and feature points. DSP-SLAM can operate at 10 frames per second on 3\ndifferent input modalities: monocular, stereo, or stereo+LiDAR. We demonstrate\nDSP-SLAM operating at almost frame rate on monocular-RGB sequences from the\nFriburg and Redwood-OS datasets, and on stereo+LiDAR sequences on the KITTI\nodometry dataset showing that it achieves high-quality full object\nreconstructions, even from partial observations, while maintaining a consistent\nglobal map. Our evaluation shows improvements in object pose and shape\nreconstruction with respect to recent deep prior-based reconstruction methods\nand reductions in camera tracking drift on the KITTI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runz_M/0/1/0/all/0/1\">Martin R&#xfc;nz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1\">Lourdes Agapito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Compression for Resource-Constrained Edge Computing Systems. (arXiv:2108.11898v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11898","description":"<p>There has been much interest in deploying deep learning algorithms on\nlow-powered devices, including smartphones, drones, and medical sensors.\nHowever, full-scale deep neural networks are often too resource-intensive in\nterms of energy and storage. As a result, the bulk part of the machine learning\noperation is therefore often carried out on an edge server, where the data is\ncompressed and transmitted. However, compressing data (such as images) leads to\ntransmitting information irrelevant to the supervised task. Another popular\napproach is to split the deep network between the device and the server while\ncompressing intermediate features. To date, however, such split computing\nstrategies have barely outperformed the aforementioned naive data compression\nbaselines due to their inefficient approaches to feature compression. This\npaper adopts ideas from knowledge distillation and neural image compression to\ncompress intermediate feature representations more efficiently. Our supervised\ncompression approach uses a teacher model and a student model with a stochastic\nbottleneck and learnable prior for entropy coding (Entropic Student). We\ncompare our approach to various neural image and feature compression baselines\nin three vision tasks and found that it achieves better supervised\nrate-distortion performance while maintaining smaller end-to-end latency. We\nfurthermore show that the learned feature representations can be tuned to serve\nmultiple downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images. (arXiv:2109.02716v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02716","description":"<p>Crop and weed monitoring is an important challenge for agriculture and food\nproduction nowadays. Thanks to recent advances in data acquisition and\ncomputation technologies, agriculture is evolving to a more smart and precision\nfarming to meet with the high yield and high quality crop production.\nClassification and recognition in Unmanned Aerial Vehicles (UAV) images are\nimportant phases for crop monitoring. Advances in deep learning models relying\non Convolutional Neural Network (CNN) have achieved high performances in image\nclassification in the agricultural domain. Despite the success of this\narchitecture, CNN still faces many challenges such as high computation cost,\nthe need of large labelled datasets, ... Natural language processing's\ntransformer architecture can be an alternative approach to deal with CNN's\nlimitations. Making use of the self-attention paradigm, Vision Transformer\n(ViT) models can achieve competitive or better results without applying any\nconvolution operations. In this paper, we adopt the self-attention mechanism\nvia the ViT models for plant classification of weeds and crops: red beet,\noff-type beet (green leaves), parsley and spinach. Our experiments show that\nwith small set of labelled training data, ViT models perform better compared to\nstate-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy\nof 99.8\\% achieved by the ViT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reedha_R/0/1/0/all/0/1\">Reenul Reedha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dericquebourg_E/0/1/0/all/0/1\">Eric Dericquebourg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canals_R/0/1/0/all/0/1\">Raphael Canals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafiane_A/0/1/0/all/0/1\">Adel Hafiane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invertible Frowns: Video-to-Video Facial Emotion Translation. (arXiv:2109.08061v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08061","description":"<p>We present Wav2Lip-Emotion, a video-to-video translation architecture that\nmodifies facial expressions of emotion in videos of speakers. Previous work\nmodifies emotion in images, uses a single image to produce a video with\nanimated emotion, or puppets facial expressions in videos with landmarks from a\nreference video. However, many use cases such as modifying an actor's\nperformance in post-production, coaching individuals to be more animated\nspeakers, or touching up emotion in a teleconference require a video-to-video\ntranslation approach. We explore a method to maintain speakers' lip movements,\nidentity, and pose while translating their expressed emotion. Our approach\nextends an existing multi-modal lip synchronization architecture to modify the\nspeaker's emotion using L1 reconstruction and pre-trained emotion objectives.\nWe also propose a novel automated emotion evaluation approach and corroborate\nit with a user study. These find that we succeed in modifying emotion while\nmaintaining lip synchronization. Visual quality is somewhat diminished, with a\ntrade off between greater emotion modification and visual quality between model\nvariants. Nevertheless, we demonstrate (1) that facial expressions of emotion\ncan be modified with nothing other than L1 reconstruction and pre-trained\nemotion objectives and (2) that our automated emotion evaluation approach\naligns with human judgements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_I/0/1/0/all/0/1\">Ian Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aruna Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippman_A/0/1/0/all/0/1\">Andrew Lippman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceEraser: Removing Facial Parts for Augmented Reality. (arXiv:2109.10760v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10760","description":"<p>Our task is to remove all facial parts (e.g., eyebrows, eyes, mouth and\nnose), and then impose visual elements onto the ``blank'' face for augmented\nreality. Conventional object removal methods rely on image inpainting\ntechniques (e.g., EdgeConnect, HiFill) that are trained in a self-supervised\nmanner with randomly manipulated image pairs. Specifically, given a set of\nnatural images, randomly masked images are used as inputs and the raw images\nare treated as ground truths. Whereas, this technique does not satisfy the\nrequirements of facial parts removal, as it is hard to obtain ``ground-truth''\nimages with real ``blank'' faces. To address this issue, we propose a novel\ndata generation technique to produce paired training data that well mimic the\n``blank'' faces. In the mean time, we propose a novel network architecture for\nimproved inpainting quality for our task. Finally, we demonstrate various\nface-oriented augmented reality applications on top of our facial parts removal\nmodel. The source codes are released at\n\\href{https://github.com/duxingren14/FaceEraser}{duxingren14/FaceEraser} on\ngithub for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1\">Miao Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingchuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zili Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13238","description":"<p>The design of widespread vision-and-language datasets and pre-trained\nencoders directly adopts, or draws inspiration from, the concepts and images of\nImageNet. While one can hardly overestimate how much this benchmark contributed\nto progress in computer vision, it is mostly derived from lexical databases and\nimage queries in English, resulting in source material with a North American or\nWestern European bias. Therefore, we devise a new protocol to construct an\nImageNet-style hierarchy representative of more languages and cultures. In\nparticular, we let the selection of both concepts and images be entirely driven\nby native speakers, rather than scraping them automatically. Specifically, we\nfocus on a typologically diverse set of languages, namely, Indonesian, Mandarin\nChinese, Swahili, Tamil, and Turkish. On top of the concepts and images\nobtained through this new protocol, we create a multilingual dataset for\n{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting\nstatements from native speaker annotators about pairs of images. The task\nconsists of discriminating whether each grounded statement is true or false. We\nestablish a series of baselines using state-of-the-art models and find that\ntheir cross-lingual transfer performance lags dramatically behind supervised\nperformance in English. These results invite us to reassess the robustness and\naccuracy of current state-of-the-art models beyond a narrow domain, but also\nopen up new exciting challenges for the development of truly multilingual and\nmulticultural systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction. (arXiv:2110.03278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03278","description":"<p>Most existing human matting algorithms tried to separate pure human-only\nforeground from the background. In this paper, we propose a Virtual\nMulti-modality Foreground Matting (VMFM) method to learn human-object\ninteractive foreground (human and objects interacted with him or her) from a\nraw RGB image. The VMFM method requires no additional inputs, e.g. trimap or\nknown background. We reformulate foreground matting as a self-supervised\nmulti-modality problem: factor each input image into estimated depth map,\nsegmentation mask, and interaction heatmap using three auto-encoders. In order\nto fully utilize the characteristics of each modality, we first train a dual\nencoder-to-decoder network to estimate the same alpha matte. Then we introduce\na self-supervised method: Complementary Learning(CL) to predict deviation\nprobability map and exchange reliable gradients across modalities without\nlabel. We conducted extensive experiments to analyze the effectiveness of each\nmodality and the significance of different components in complementary\nlearning. We demonstrate that our model outperforms the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning by Estimating Twin Class Distributions. (arXiv:2110.07402v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07402","description":"<p>We present TWIST, a novel self-supervised representation learning method by\nclassifying large-scale unlabeled datasets in an end-to-end way. We employ a\nsiamese network terminated by a softmax operation to produce twin class\ndistributions of two augmented images. Without supervision, we enforce the\nclass distributions of different augmentations to be consistent. In the\nmeantime, we regularize the class distributions to make them sharp and diverse.\nSpecifically, we minimize the entropy of the distribution for each sample to\nmake the class prediction for each sample assertive and maximize the entropy of\nthe mean distribution to make the predictions of different samples diverse. In\nthis way, TWIST can naturally avoid the trivial solutions without specific\ndesigns such as asymmetric network, stop-gradient operation, or momentum\nencoder. Different from the clustering-based methods which alternate between\nclustering and learning, our method is a single learning process guided by a\nunified loss function. As a result, TWIST outperforms state-of-the-art methods\non a wide range of tasks, including unsupervised classification, linear\nclassification, semi-supervised learning, transfer learning, and some dense\nprediction tasks such as detection and segmentation. Codes and pre-trained\nmodels are given on: https://github.com/bytedance/TWIST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backpropagation with Biologically Plausible Spatio-Temporal Adjustment For Training Deep Spiking Neural Networks. (arXiv:2110.08858v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2110.08858","description":"<p>The spiking neural network (SNN) mimics the information processing operation\nin the human brain, represents and transmits information in spike trains\ncontaining wealthy spatial and temporal information, and shows superior\nperformance on many cognitive tasks. In addition, the event-driven information\nprocessing enables the energy-efficient implementation on neuromorphic chips.\nThe success of deep learning is inseparable from backpropagation. Due to the\ndiscrete information transmission, directly applying the backpropagation to the\ntraining of the SNN still has a performance gap compared with the traditional\ndeep neural networks. Also, a large simulation time is required to achieve\nbetter performance, which results in high latency. To address the problems, we\npropose a biological plausible spatial adjustment, which rethinks the\nrelationship between membrane potential and spikes and realizes a reasonable\nadjustment of gradients to different time steps. And it precisely controls the\nbackpropagation of the error along the spatial dimension. Secondly, we propose\na biologically plausible temporal adjustment making the error propagate across\nthe spikes in the temporal dimension, which overcomes the problem of the\ntemporal dependency within a single spike period of the traditional spiking\nneurons. We have verified our algorithm on several datasets, and the\nexperimental results have shown that our algorithm greatly reduces the network\nlatency and energy consumption while also improving network performance. We\nhave achieved state-of-the-art performance on the neuromorphic datasets\nN-MNIST, DVS-Gesture, and DVS-CIFAR10. For the static datasets MNIST and\nCIFAR10, we have surpassed most of the traditional SNN backpropagation training\nalgorithm and achieved relatively superior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1\">Guobin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongcheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No RL, No Simulation: Learning to Navigate without Navigating. (arXiv:2110.09470v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09470","description":"<p>Most prior methods for learning navigation policies require access to\nsimulation environments, as they need online policy interaction and rely on\nground-truth maps for rewards. However, building simulators is expensive\n(requires manual effort for each and every scene) and creates challenges in\ntransferring learned policies to robotic platforms in the real-world, due to\nthe sim-to-real domain gap. In this paper, we pose a simple question: Do we\nreally need active interaction, ground-truth maps or even\nreinforcement-learning (RL) in order to solve the image-goal navigation task?\nWe propose a self-supervised approach to learn to navigate from only passive\nvideos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and\nscalable, yet highly effective. NRNS outperforms RL-based formulations by a\nsignificant margin. We present NRNS as a strong baseline for any future\nimage-based navigation tasks that use RL or Simulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Meera Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOS: A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation. (arXiv:2110.10953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.10953","description":"<p>With the emergence of service robots and surveillance cameras, dynamic face\nrecognition (DFR) in wild has received much attention in recent years. Face\ndetection and head pose estimation are two important steps for DFR. Very often,\nthe pose is estimated after the face detection. However, such sequential\ncomputations lead to higher latency. In this paper, we propose a low latency\nand lightweight network for simultaneous face detection, landmark localization\nand head pose estimation. Inspired by the observation that it is more\nchallenging to locate the facial landmarks for faces with large angles, a pose\nloss is proposed to constrain the learning. Moreover, we also propose an\nuncertainty multi-task loss to learn the weights of individual tasks\nautomatically. Another challenge is that robots often use low computational\nunits like ARM based computing core and we often need to use lightweight\nnetworks instead of the heavy ones, which lead to performance drop especially\nfor small and hard faces. In this paper, we propose online feedback sampling to\naugment the training samples across different scales, which increases the\ndiversity of training data automatically. Through validation in commonly used\nWIDER FACE, AFLW and AFLW2000 datasets, the results show that the proposed\nmethod achieves the state-of-the-art performance in low computational\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yepeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zaiwang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yusheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Knowledge Distillation With Peer-To-Peer Mutual Learning For Model Compression. (arXiv:2110.11023v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11023","description":"<p>Knowledge distillation (KD) is an effective model compression technique where\na compact student network is taught to mimic the behavior of a complex and\nhighly trained teacher network. In contrast, Mutual Learning (ML) provides an\nalternative strategy where multiple simple student networks benefit from\nsharing knowledge, even in the absence of a powerful but static teacher\nnetwork. Motivated by these findings, we propose a single-teacher,\nmulti-student framework that leverages both KD and ML to achieve better\nperformance. Furthermore, an online distillation strategy is utilized to train\nthe teacher and students simultaneously. To evaluate the performance of the\nproposed approach, extensive experiments were conducted using three different\nversions of teacher-student networks on benchmark biomedical classification\n(MSI vs. MSS) and object detection (Polyp Detection) tasks. Ensemble of student\nnetworks trained in the proposed manner achieved better results than the\nensemble of students trained using KD or ML individually, establishing the\nbenefit of augmenting knowledge transfer from teacher to students with\npeer-to-peer learning between students.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niyaz_U/0/1/0/all/0/1\">Usma Niyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bathula_D/0/1/0/all/0/1\">Deepti R. Bathula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Deployment of Recycling Classification through Efficient Hyper-Parameter Analysis. (arXiv:2110.11043v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11043","description":"<p>The paradigm of automated waste classification has recently seen a shift in\nthe domain of interest from conventional image processing techniques to\npowerful computer vision algorithms known as convolutional neural networks\n(CNN). Historically, CNNs have demonstrated a strong dependency on powerful\nhardware for real-time classification, yet the need for deployment on weaker\nembedded devices is greater than ever. The work in this paper proposes a\nmethodology for reconstructing and tuning conventional image classification\nmodels, using EfficientNets, to decrease their parameterisation with no\ntrade-off in model accuracy and develops a pipeline through TensorRT for\naccelerating such models to run at real-time on an NVIDIA Jetson Nano embedded\ndevice. The train-deployment discrepancy, relating how poor data augmentation\nleads to a discrepancy in model accuracy between training and deployment, is\noften neglected in many papers and thus the work is extended by analysing and\nevaluating the impact real world perturbations had on model accuracy once\ndeployed. The scope of the work concerns developing a more efficient variant of\nWasteNet, a collaborative recycling classification model. The newly developed\nmodel scores a test-set accuracy of 95.8% with a real world accuracy of 95%, a\n14% increase over the original. Our acceleration pipeline boosted model\nthroughput by 750% to 24 inferences per second on the Jetson Nano and real-time\nlatency of the system was verified through servomotor latency analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulmahmood_M/0/1/0/all/0/1\">Mazin Abdulmahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grammenos_R/0/1/0/all/0/1\">Ryan Grammenos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Insight into Measuring Face Image Utility with General and Face-specific Image Quality Metrics. (arXiv:2110.11111v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11111","description":"<p>Quality scores provide a measure to evaluate the utility of biometric samples\nfor biometric recognition. Biometric recognition systems require high-quality\nsamples to achieve optimal performance. This paper focuses on face images and\nthe measurement of face image utility with general and face-specific image\nquality metrics. While face-specific metrics rely on features of aligned face\nimages, general image quality metrics can be used on the global image and\nrelate to human perceptions. In this paper, we analyze the gap between the\ngeneral image quality metrics and the face image quality metrics. Our\ncontribution lies in a thorough examination of how different the image quality\nassessment algorithms relate to the utility for the face recognition task. The\nresults of image quality assessment algorithms are further compared with those\nof dedicated face image quality assessment algorithms. In total, 25 different\nquality metrics are evaluated on three face image databases, BioSecure, LFW,\nand VGGFace2 using three open-source face recognition solutions, SphereFace,\nArcFace, and FaceNet. Our results reveal a clear correlation between learned\nimage metrics to face image utility even without being specifically trained as\na face utility measure. Individual handcrafted features lack general stability\nand perform significantly worse than general face-specific quality metrics. We\nadditionally provide a visual insight into the image areas contributing to the\nquality score of a selected set of quality assessment methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Biying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henniger_O/0/1/0/all/0/1\">Olaf Henniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCV: Hierarchy-Consistency Verification for Incremental Implicitly-Refined Classification. (arXiv:2110.11148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11148","description":"<p>Human beings learn and accumulate hierarchical knowledge over their lifetime.\nThis knowledge is associated with previous concepts for consolidation and\nhierarchical construction. However, current incremental learning methods lack\nthe ability to build a concept hierarchy by associating new concepts to old\nones. A more realistic setting tackling this problem is referred to as\nIncremental Implicitly-Refined Classification (IIRC), which simulates the\nrecognition process from coarse-grained categories to fine-grained categories.\nTo overcome forgetting in this benchmark, we propose Hierarchy-Consistency\nVerification (HCV) as an enhancement to existing continual learning methods.\nOur method incrementally discovers the hierarchical relations between classes.\nWe then show how this knowledge can be exploited during both training and\ninference. Experiments on three setups of varying difficulty demonstrate that\nour HCV module improves performance of existing continual learning methods\nunder this IIRC setting by a large margin. Code is available in\nhttps://github.com/wangkai930418/HCV_IIRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Graph Convolutional Networks for Human Action Synthesis. (arXiv:2110.11191v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11191","description":"<p>Synthesising the spatial and temporal dynamics of the human body skeleton\nremains a challenging task, not only in terms of the quality of the generated\nshapes, but also of their diversity, particularly to synthesise realistic body\nmovements of a specific action (action conditioning). In this paper, we propose\nKinetic-GAN, a novel architecture that leverages the benefits of Generative\nAdversarial Networks and Graph Convolutional Networks to synthesise the\nkinetics of the human body. The proposed adversarial architecture can condition\nup to 120 different actions over local and global body movements while\nimproving sample quality and diversity through latent space disentanglement and\nstochastic variations. Our experiments were carried out in three well-known\ndatasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in\nterms of distribution quality metrics while having the ability to synthesise\nmore than one order of magnitude regarding the number of different actions. Our\ncode and models are publicly available at\nhttps://github.com/DegardinBruno/Kinetic-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Degardin_B/0/1/0/all/0/1\">Bruno Degardin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_J/0/1/0/all/0/1\">Jo&#xe3;o Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_V/0/1/0/all/0/1\">Vasco Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brito_J/0/1/0/all/0/1\">Jo&#xe3;o Brito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoubi_E/0/1/0/all/0/1\">Ehsan Yaghoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1\">Hugo Proen&#xe7;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-resolution of multiphase materials by combining complementary 2D and 3D image data using generative adversarial networks. (arXiv:2110.11281v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11281","description":"<p>Modelling the impact of a material's mesostructure on device level\nperformance typically requires access to 3D image data containing all the\nrelevant information to define the geometry of the simulation domain. This\nimage data must include sufficient contrast between phases to distinguish each\nmaterial, be of high enough resolution to capture the key details, but also\nhave a large enough field-of-view to be representative of the material in\ngeneral. It is rarely possible to obtain data with all of these properties from\na single imaging technique. In this paper, we present a method for combining\ninformation from pairs of distinct but complementary imaging techniques in\norder to accurately reconstruct the desired multi-phase, high resolution,\nrepresentative, 3D images. Specifically, we use deep convolutional generative\nadversarial networks to implement super-resolution, style transfer and\ndimensionality expansion. To demonstrate the widespread applicability of this\ntool, two pairs of datasets are used to validate the quality of the volumes\ngenerated by fusing the information from paired imaging techniques. Three key\nmesostructural metrics are calculated in each case to show the accuracy of this\nmethod. Having confidence in the accuracy of our method, we then demonstrate\nits power by applying to a real data pair from a lithium ion battery electrode,\nwhere the required 3D high resolution image data is not available anywhere in\nthe literature. We believe this approach is superior to previously reported\nstatistical material reconstruction methods both in terms of its fidelity and\nease of use. Furthermore, much of the data required to train this algorithm\nalready exists in the literature, waiting to be combined. As such, our\nopen-access code could precipitate a step change by generating the hard to\nobtain high quality image volumes necessary to simulate behaviour at the\nmesoscale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahari_A/0/1/0/all/0/1\">Amir Dahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kench_S/0/1/0/all/0/1\">Steve Kench</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Squires_I/0/1/0/all/0/1\">Isaac Squires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1\">Samuel J. Cooper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Wearing a Face Mask on Face Image Quality. (arXiv:2110.11283v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11283","description":"<p>Due to the COVID-19 situation, face masks have become a main part of our\ndaily life. Wearing mouth-and-nose protection has been made a mandate in many\npublic places, to prevent the spread of the COVID-19 virus. However, face masks\naffect the performance of face recognition, since a large area of the face is\ncovered. The effect of wearing a face mask on the different components of the\nface recognition system in a collaborative environment is a problem that is\nstill to be fully studied. This work studies, for the first time, the effect of\nwearing a face mask on face image quality by utilising state-of-the-art face\nimage quality assessment methods of different natures. This aims at providing\nbetter understanding on the effect of face masks on the operation of face\nrecognition as a whole system. In addition, we further studied the effect of\nsimulated masks on face image utility in comparison to real face masks. We\ndiscuss the correlation between the mask effect on face image quality and that\non the face verification performance by automatic systems and human experts,\nindicating a consistent trend between both factors. The evaluation is conducted\non the database containing (1) no-masked faces, (2) real face masks, and (3)\nsimulated face masks, by synthetically generating digital facial masks on\nno-masked faces according to the NIST protocols [1, 23]. Finally, a visual\ninterpretation of the face areas contributing to the quality score of a\nselected set of quality assessment methods is provided to give a deeper insight\ninto the difference of network decisions in masked and non-masked faces, among\nother variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Biying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on GANs with Margin Cosine Loss and Relativistic Discriminator. (arXiv:2110.11293v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11293","description":"<p>Generative Adversarial Networks (GANs) have emerged as useful generative\nmodels, which are capable of implicitly learning data distributions of\narbitrarily complex dimensions. However, the training of GANs is empirically\nwell-known for being highly unstable and sensitive. The loss functions of both\nthe discriminator and generator concerning their parameters tend to oscillate\nwildly during training. Different loss functions have been proposed to\nstabilize the training and improve the quality of images generated. In this\npaper, we perform an empirical study on the impact of several loss functions on\nthe performance of standard GAN models, Deep Convolutional Generative\nAdversarial Networks (DCGANs). We introduce a new improvement that employs a\nrelativistic discriminator to replace the classical deterministic discriminator\nin DCGANs and implement a margin cosine loss function for both the generator\nand discriminator. This results in a novel loss function, namely Relativistic\nMargin Cosine Loss (RMCosGAN). We carry out extensive experiments with four\ndatasets: CIFAR-$10$, MNIST, STL-$10$, and CAT. We compare RMCosGAN performance\nwith existing loss functions based on two metrics: Frechet inception distance\nand inception score. The experimental results show that RMCosGAN outperforms\nthe existing ones and significantly improves the quality of images generated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tien-Dung Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_Huu_T/0/1/0/all/0/1\">Tram Truong-Huu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_K/0/1/0/all/0/1\">Khanh N. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLURP: Side Learning Uncertainty for Regression Problems. (arXiv:2110.11182v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2110.11182","description":"<p>It has become critical for deep learning algorithms to quantify their output\nuncertainties to satisfy reliability constraints and provide accurate results.\nUncertainty estimation for regression has received less attention than\nclassification due to the more straightforward standardized output of the\nlatter class of tasks and their high importance. However, regression problems\nare encountered in a wide range of applications in computer vision. We propose\nSLURP, a generic approach for regression uncertainty estimation via a side\nlearner that exploits the output and the intermediate representations generated\nby the main task model. We test SLURP on two critical regression tasks in\ncomputer vision: monocular depth and optical flow estimation. In addition, we\nconduct exhaustive benchmarks comprising transfer to different datasets and the\naddition of aleatoric noise. The results show that our proposal is generic and\nreadily applicable to various regression problems and has a low computational\ncost with respect to existing solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}