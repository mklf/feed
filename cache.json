{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages. (arXiv:2203.01976v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01976","description":"<p>Pre-trained multilingual language models such as mBERT and XLM-R have\ndemonstrated great potential for zero-shot cross-lingual transfer to low\nweb-resource languages (LRL). However, due to limited model capacity, the large\ndifference in the sizes of available monolingual corpora between high\nweb-resource languages (HRL) and LRLs does not provide enough scope of\nco-embedding the LRL with the HRL, thereby affecting downstream task\nperformance of LRLs. In this paper, we argue that relatedness among languages\nin a language family along the dimension of lexical overlap may be leveraged to\novercome some of the corpora limitations of LRLs. We propose Overlap BPE\n(OBPE), a simple yet effective modification to the BPE vocabulary generation\nalgorithm which enhances overlap across related languages. Through extensive\nexperiments on multiple NLP tasks and datasets, we observe that OBPE generates\na vocabulary that increases the representation of LRLs via tokens shared with\nHRLs. This results in improved zero-shot transfer from related HRLs to LRLs\nwithout reducing HRL representation and accuracy. Unlike previous studies that\ndismissed the importance of token-overlap, we show that in the low-resource\nrelated language setting, token overlap matters. Synthetically reducing the\noverlap to zero can cause as much as a four-fold drop in zero-shot transfer\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1\">Vaidehi Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations. (arXiv:2203.02013v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02013","description":"<p>The ability for a human to understand an Artificial Intelligence (AI) model's\ndecision-making process is critical in enabling stakeholders to visualize model\nbehavior, perform model debugging, promote trust in AI models, and assist in\ncollaborative human-AI decision-making. As a result, the research fields of\ninterpretable and explainable AI have gained traction within AI communities as\nwell as interdisciplinary scientists seeking to apply AI in their subject\nareas. In this paper, we focus on advancing the state-of-the-art in\ninterpreting multimodal models - a class of machine learning methods that\ntackle core challenges in representing and capturing interactions between\nheterogeneous data sources such as images, text, audio, and time-series data.\nMultimodal models have proliferated numerous real-world applications across\nhealthcare, robotics, multimedia, affective computing, and human-computer\ninteraction. By performing model disentanglement into unimodal contributions\n(UC) and multimodal interactions (MI), our proposed approach, DIME, enables\naccurate and fine-grained analysis of multimodal models while maintaining\ngenerality across arbitrary modalities, model architectures, and tasks. Through\na comprehensive suite of experiments on both synthetic and real-world\nmultimodal tasks, we show that DIME generates accurate disentangled\nexplanations, helps users of multimodal models gain a deeper understanding of\nmodel behavior, and presents a step towards debugging and improving these\nmodels for real-world deployment. Code for our experiments can be found at\nhttps://github.com/lvyiwei1/DIME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. (arXiv:2203.02053v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02053","description":"<p>We present modality gap, an intriguing geometric phenomenon of the\nrepresentation space of multi-modal models. Specifically, we show that\ndifferent data modalities (e.g. images and text) are embedded at arm's length\nin their shared representation in multi-modal models such as CLIP. Our\nsystematic analysis demonstrates that this gap is caused by a combination of\nmodel initialization and contrastive learning optimization. In model\ninitialization, we show empirically and theoretically that the representation\nof a common deep neural network is restricted to a narrow cone. As a\nconsequence, in a multi-modal model with two encoders, the representations of\nthe two modalities are clearly apart when the model is initialized. During\noptimization, contrastive learning keeps the different modalities separate by a\ncertain distance, which is influenced by the temperature parameter in the loss\nfunction. Our experiments further demonstrate that varying the modality gap\ndistance has a significant impact in improving the model's downstream zero-shot\nclassification performance and fairness. Our code and data are available at\nhttps://modalitygap.readthedocs.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yongchan Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Serena Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Latent-Variable Models for Text Generation. (arXiv:2203.02055v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02055","description":"<p>Text generation aims to produce human-like natural language output for\ndown-stream tasks. It covers a wide range of applications like machine\ntranslation, document summarization, dialogue generation and so on. Recently\ndeep neural network-based end-to-end architectures have been widely adopted.\nThe end-to-end approach conflates all sub-modules, which used to be designed by\ncomplex handcrafted rules, into a holistic encode-decode architecture. Given\nenough training data, it is able to achieve state-of-the-art performance yet\navoiding the need of language/domain-dependent knowledge. Nonetheless, deep\nlearning models are known to be extremely data-hungry, and text generated from\nthem usually suffer from low diversity, interpretability and controllability.\nAs a result, it is difficult to trust the output from them in real-life\napplications. Deep latent-variable models, by specifying the probabilistic\ndistribution over an intermediate latent process, provide a potential way of\naddressing these problems while maintaining the expressive power of deep neural\nnetworks. This dissertation presents how deep latent-variable models can\nimprove over the standard encoder-decoder model for text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Lexical Hypothesis: Identifying personality structure in natural language. (arXiv:2203.02092v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02092","description":"<p>Recent advances in natural language processing (NLP) have produced general\nmodels that can perform complex tasks such as summarizing long passages and\ntranslating across languages. Here, we introduce a method to extract adjective\nsimilarities from language models as done with survey-based ratings in\ntraditional psycholexical studies but using millions of times more text in a\nnatural setting. The correlational structure produced through this method is\nhighly similar to that of self- and other-ratings of 435 terms reported by\nSaucier and Goldberg (1996a). The first three unrotated factors produced using\nNLP are congruent with those in survey data, with coefficients of 0.89, 0.79,\nand 0.79. This structure is robust to many modeling decisions: adjective set,\nincluding those with 1,710 terms (Goldberg, 1982) and 18,000 terms (Allport &amp;\nOdbert, 1936); the query used to extract correlations; and language model.\nNotably, Neuroticism and Openness are only weakly and inconsistently recovered.\nThis is a new source of signal that is closer to the original (semantic) vision\nof the Lexical Hypothesis. The method can be applied where surveys cannot: in\ndozens of languages simultaneously, with tens of thousands of items, on\nhistorical text, and at extremely large scale for little cost. The code is made\npublic to facilitate reproduction and fast iteration in new directions of\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cutler_A/0/1/0/all/0/1\">Andrew Cutler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condon_D/0/1/0/all/0/1\">David M. Condon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiteTransformerSearch: Training-free On-device Search for Efficient Autoregressive Language Models. (arXiv:2203.02094v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02094","description":"<p>The transformer architecture is ubiquitously used as the building block of\nmost large-scale language models. However, it remains a painstaking guessing\ngame of trial and error to set its myriad of architectural hyperparameters,\ne.g., number of layers, number of attention heads, and inner size of the feed\nforward network, and find architectures with the optimal trade-off between task\nperformance like perplexity and compute constraints like memory and latency.\nThis challenge is further exacerbated by the proliferation of various hardware.\nIn this work, we leverage the somewhat surprising empirical observation that\nthe number of non-embedding parameters in autoregressive transformers has a\nhigh rank correlation with task performance, irrespective of the architectural\nhyperparameters. Since architectural hyperparameters affect the latency and\nmemory footprint in a hardware-dependent manner, the above observation\norganically induces a simple search algorithm that can be directly run on\ntarget devices. We rigorously show that the latency and perplexity\npareto-frontier can be found without need for any model training, using\nnon-embedding parameters as a proxy for perplexity. We evaluate our method,\ndubbed Lightweight Transformer Search (LTS), on diverse devices from ARM CPUs\nto Nvidia GPUs and show that the perplexity of Transformer-XL can be achieved\nwith up to 2x lower latency. LTS extracts the pareto-frontier in less than 3\nhours while running on a commodity laptop. We effectively remove the carbon\nfootprint of training for hundreds of GPU hours, offering a strong simple\nbaseline for future NAS methods in autoregressive language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javaheripi_M/0/1/0/all/0/1\">Mojan Javaheripi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shital Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Religa_T/0/1/0/all/0/1\">Tomasz L. Religa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_C/0/1/0/all/0/1\">Caio C. T. Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo H. de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">Sebastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1\">Debadeepta Dey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In the Service of Online Order: Tackling Cyber-Bullying with Machine Learning and Affect Analysis. (arXiv:2203.02116v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02116","description":"<p>One of the burning problems lately in Japan has been cyber-bullying, or\nslandering and bullying people online. The problem has been especially noticed\non unofficial Web sites of Japanese schools. Volunteers consisting of school\npersonnel and PTA (Parent-Teacher Association) members have started Online\nPatrol to spot malicious contents within Web forums and blogs. In practise,\nOnline Patrol assumes reading through the whole Web contents, which is a task\ndifficult to perform manually. With this paper we introduce a research intended\nto help PTA members perform Online Patrol more efficiently. We aim to develop a\nset of tools that can automatically detect malicious entries and report them to\nPTA members. First, we collected cyber-bullying data from unofficial school Web\nsites. Then we performed analysis of this data in two ways. Firstly, we\nanalysed the entries with a multifaceted affect analysis system in order to\nfind distinctive features for cyber-bullying and apply them to a machine\nlearning classifier. Secondly, we applied a SVM based machine learning method\nto train a classifier for detection of cyber-bullying. The system was able to\nclassify cyber-bullying entries with 88.2% of balanced F-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dybala_P/0/1/0/all/0/1\">Pawel Dybala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuba_T/0/1/0/all/0/1\">Tatsuaki Matsuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepka_R/0/1/0/all/0/1\">Rafal Rzepka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_K/0/1/0/all/0/1\">Kenji Araki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momouchi_Y/0/1/0/all/0/1\">Yoshio Momouchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation. (arXiv:2203.02135v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02135","description":"<p>Existing continual relation learning (CRL) methods rely on plenty of labeled\ntraining data for learning a new task, which can be hard to acquire in real\nscenario as getting large and representative labeled data is often expensive\nand time-consuming. It is therefore necessary for the model to learn novel\nrelational patterns with very few labeled data while avoiding catastrophic\nforgetting of previous task knowledge. In this paper, we formulate this\nchallenging yet practical problem as continual few-shot relation learning\n(CFRL). Based on the finding that learning for new emerging few-shot tasks\noften results in feature distributions that are incompatible with previous\ntasks' learned distributions, we propose a novel method based on embedding\nspace regularization and data augmentation. Our method generalizes to new\nfew-shot tasks and avoids catastrophic forgetting of previous tasks by\nenforcing extra constraints on the relational embeddings and by adding extra\n{relevant} data in a self-supervised manner. With extensive experiments we\ndemonstrate that our method can significantly outperform previous\nstate-of-the-art methods in CFRL task settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training language models to follow instructions with human feedback. (arXiv:2203.02155v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02155","description":"<p>Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1\">Diogo Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wainwright_C/0/1/0/all/0/1\">Carroll L. Wainwright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1\">Pamela Mishkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sandhini Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slama_K/0/1/0/all/0/1\">Katarina Slama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Alex Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelton_F/0/1/0/all/0/1\">Fraser Kelton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_L/0/1/0/all/0/1\">Luke Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simens_M/0/1/0/all/0/1\">Maddie Simens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welinder_P/0/1/0/all/0/1\">Peter Welinder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1\">Paul Christiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_R/0/1/0/all/0/1\">Ryan Lowe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models. (arXiv:2203.02167v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02167","description":"<p>Knowledge graph completion (KGC) aims to reason over known facts and infer\nthe missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn\nentity representations from natural language descriptions, and have the\npotential for inductive KGC. However, the performance of text-based methods\nstill largely lag behind graph embedding-based methods like TransE (Bordes et\nal., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the\nkey issue is efficient contrastive learning. To improve the learning\nefficiency, we introduce three types of negatives: in-batch negatives,\npre-batch negatives, and self-negatives which act as a simple form of hard\nnegatives. Combined with InfoNCE loss, our proposed model SimKGC can\nsubstantially outperform embedding-based methods on several benchmark datasets.\nIn terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19%\non WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the\nWikidata5M inductive setting. Thorough analyses are conducted to gain insights\ninto each component. Our code is available at\nhttps://github.com/intfloat/SimKGC .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhuoyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation. (arXiv:2203.02177v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02177","description":"<p>Conversations have become a critical data format on social media platforms.\nUnderstanding conversation from emotion, content, and other aspects also\nattracts increasing attention from researchers due to its widespread\napplication in human-computer interaction. In real-world environments, we often\nencounter the problem of incomplete modalities, which has become a core issue\nof conversation understanding. To address this problem, researchers propose\nvarious methods. However, existing approaches are mainly designed for\nindividual utterances or medical images rather than conversational data, which\ncannot exploit temporal and speaker information in conversations. To this end,\nwe propose a novel framework for incomplete multimodal learning in\nconversations, called \"Graph Complete Network (GCNet)\", filling the gap of\nexisting works. Our GCNet contains two well-designed graph neural network-based\nmodules, \"Speaker GNN\" and \"Temporal GNN\", to capture temporal and speaker\ninformation in conversations. To make full use of complete and incomplete data\nin feature learning, we jointly optimize classification and reconstruction in\nan end-to-end manner. To verify the effectiveness of our method, we conduct\nexperiments on three benchmark conversational datasets. Experimental results\ndemonstrate that our GCNet is superior to existing state-of-the-art approaches\nin incomplete multimodal learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zheng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Licai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAG: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation. (arXiv:2203.02180v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02180","description":"<p>Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior\nperformance against the conventional MNMT by constructing multi-way aligned\ncorpus, i.e., aligning bilingual training examples from different language\npairs when either their source or target sides are identical. However, since\nexactly identical sentences from different language pairs are scarce, the power\nof the multi-way aligned corpus is limited by its scale. To handle this\nproblem, this paper proposes \"Extract and Generate\" (EAG), a two-step approach\nto construct large-scale and high-quality multi-way aligned corpus from\nbilingual data. Specifically, we first extract candidate aligned examples by\npairing the bilingual examples from different language pairs with highly\nsimilar source or target sentences; and then generate the final aligned\nexamples from the candidates with a well-trained generation model. With this\ntwo-step pipeline, EAG can construct a large-scale and multi-way aligned corpus\nwhose diversity is almost identical to the original bilingual corpus.\nExperiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show\nthat the proposed method achieves significant improvements over strong\nbaselines, with +1.1 and +1.4 BLEU points improvements on the two datasets\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yulin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+JieZhou/0/1/0/all/0/1\">JieZhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification. (arXiv:2203.02225v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02225","description":"<p>Generating new events given context with correlated ones plays a crucial role\nin many event-centric reasoning tasks. Existing works either limit their scope\nto specific scenarios or overlook event-level correlations. In this paper, we\npropose to pre-train a general Correlation-aware context-to-Event Transformer\n(ClarET) for event-centric reasoning. To achieve this, we propose three novel\nevent-centric objectives, i.e., whole event recovering, contrastive\nevent-correlation encoding and prompt-based event locating, which highlight\nevent-level correlations with effective training. The proposed ClarET is\napplicable to a wide range of event-centric reasoning scenarios, considering\nits versatility of (i) event-correlation types (e.g., causal, temporal,\ncontrast), (ii) application formulations (i.e., generation and classification),\nand (iii) reasoning types (e.g., abductive, counterfactual and ending\nreasoning). Empirical fine-tuning results, as well as zero- and few-shot\nlearning, on 9 benchmarks (5 generation and 4 classification tasks covering 4\nreasoning types with diverse event correlations), verify its effectiveness and\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IISERB Brains at SemEval 2022 Task 6: A Deep-learning Framework to Identify Intended Sarcasm in English. (arXiv:2203.02244v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02244","description":"<p>This paper describes the system architectures and the models submitted by our\nteam \"IISERBBrains\" to SemEval 2022 Task 6 competition. We contested for all\nthree sub-tasks floated for the English dataset. On the leader-board, wegot19th\nrank out of43 teams for sub-taskA, the 8th rank out of22 teams for sub-task\nB,and13th rank out of 16 teams for sub-taskC. Apart from the submitted results\nand models, we also report the other models and results that we obtained\nthrough our experiments after organizers published the gold labels of their\nevaluation data\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhawat_T/0/1/0/all/0/1\">Tanuj Singh Shekhawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">Manoj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathore_U/0/1/0/all/0/1\">Udaybhan Rathore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Aditya Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patro_J/0/1/0/all/0/1\">Jasabanta Patro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Discounting of Implicit Language Models in RNN-Transducers. (arXiv:2203.02317v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02317","description":"<p>RNN-Transducer (RNN-T) models have become synonymous with streaming\nend-to-end ASR systems. While they perform competitively on a number of\nevaluation categories, rare words pose a serious challenge to RNN-T models. One\nmain reason for the degradation in performance on rare words is that the\nlanguage model (LM) internal to RNN-Ts can become overconfident and lead to\nhallucinated predictions that are acoustically inconsistent with the underlying\nspeech. To address this issue, we propose a lightweight adaptive LM discounting\ntechnique AdaptLMD, that can be used with any RNN-T architecture without\nrequiring any external resources or additional parameters. AdaptLMD uses a\ntwo-pronged approach: 1) Randomly mask the prediction network output to\nencourage the RNN-T to not be overly reliant on it's outputs. 2) Dynamically\nchoose when to discount the implicit LM (ILM) based on rarity of recently\npredicted tokens and divergence between ILM and implicit acoustic model (IAM)\nscores. Comparing AdaptLMD to a competitive RNN-T baseline, we obtain up to 4%\nand 14% relative reductions in overall WER and rare word PER, respectively, on\na conversational, code-mixed Hindi-English ASR task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unni_V/0/1/0/all/0/1\">Vinit Unni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1\">Shreya Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ashish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations. (arXiv:2203.02385v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02385","description":"<p>Emotion Recognition in Conversations (ERC) has considerable prospects for\ndeveloping empathetic machines. For multimodal ERC, it is vital to understand\ncontext and fuse modality information in conversations. Recent graph-based\nfusion methods generally aggregate multimodal information by exploring unimodal\nand cross-modal interactions in a graph. However, they accumulate redundant\ninformation at each layer, limiting the context understanding between\nmodalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network\n(MM-DFN) to recognize emotions by fully understanding multimodal conversational\ncontext. Specifically, we design a new graph-based dynamic fusion module to\nfuse multimodal contextual features in a conversation. The module reduces\nredundancy and enhances complementarity between modalities by capturing the\ndynamics of contextual information in different semantic spaces. Extensive\nexperiments on two public benchmark datasets demonstrate the effectiveness and\nsuperiority of MM-DFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xiaolong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lianxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Plain Toxic: Detection of Inappropriate Statements on Flammable Topics for the Russian Language. (arXiv:2203.02392v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02392","description":"<p>Toxicity on the Internet, such as hate speech, offenses towards particular\nusers or groups of people, or the use of obscene words, is an acknowledged\nproblem. However, there also exist other types of inappropriate messages which\nare usually not viewed as toxic, e.g. as they do not contain explicit offences.\nSuch messages can contain covered toxicity or generalizations, incite harmful\nactions (crime, suicide, drug use), provoke \"heated\" discussions. Such messages\nare often related to particular sensitive topics, e.g. on politics, sexual\nminorities, social injustice which more often than other topics, e.g. cars or\ncomputing, yield toxic emotional reactions. At the same time, clearly not all\nmessages within such flammable topics are inappropriate.\n</p>\n<p>Towards this end, in this work, we present two text collections labelled\naccording to binary notion of inapropriateness and a multinomial notion of\nsensitive topic. Assuming that the notion of inappropriateness is common among\npeople of the same culture, we base our approach on human intuitive\nunderstanding of what is not acceptable and harmful. To objectivise the notion\nof inappropriateness, we define it in a data-driven way though crowdsourcing.\nNamely we run a large-scale annotation study asking workers if a given chatbot\ntextual statement could harm reputation of a company created it. Acceptably\nhigh values of inter-annotator agreement suggest that the notion of\ninappropriateness exists and can be uniformly understood by different people.\nTo define the notion of sensitive topics in an objective way we use on\nguidelines suggested commonly by specialists of legal and PR department of a\nlarge public company as potentially harmful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Babakov_N/0/1/0/all/0/1\">Nikolay Babakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logacheva_V/0/1/0/all/0/1\">Varvara Logacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehension of Subtitles from Re-Translating Simultaneous Speech Translation. (arXiv:2203.02458v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02458","description":"<p>In simultaneous speech translation, one can vary the size of the output\nwindow, system latency and sometimes the allowed level of rewriting. The effect\nof these properties on readability and comprehensibility has not been tested\nwith modern neural translation systems. In this work, we propose an evaluation\nmethod and investigate the effects on comprehension and user preferences. It is\na pilot study with 14 users on 2 hours of German documentaries or speeches with\nonline translations into Czech. We collect continuous feedback and answers on\nfactual questions. Our results show that the subtitling layout or flicker have\na little effect on comprehension, in contrast to machine translation itself and\nindividual competence. Other results show that users with a limited knowledge\nof the source language have different preferences to stability and latency than\nthe users with zero knowledge. The results are statistically insignificant,\nhowever, we show that our method works and can be reproduced in larger volume.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javorsky_D/0/1/0/all/0/1\">D&#xe1;vid Javorsk&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1\">Dominik Mach&#xe1;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History. (arXiv:2203.02459v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02459","description":"<p>Simultaneous Machine Translation is the task of incrementally translating an\ninput sentence before it is fully available. Currently, simultaneous\ntranslation is carried out by translating each sentence independently of the\npreviously translated text. More generally, Streaming MT can be understood as\nan extension of Simultaneous MT to the incremental translation of a continuous\ninput text stream. In this work, a state-of-the-art simultaneous sentence-level\nMT system is extended to the streaming setup by leveraging the streaming\nhistory. Extensive empirical results are reported on IWSLT Translation Tasks,\nshowing that leveraging the streaming history leads to significant quality\ngains. In particular, the proposed system proves to compare favorably to the\nbest performing systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Javier Iranzo-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Jorge Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_A/0/1/0/all/0/1\">Alfons Juan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples. (arXiv:1912.04853v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/1912.04853","description":"<p>Embeddings mapping high-dimensional discrete input to lower-dimensional\ncontinuous vector spaces have been widely adopted in machine learning\napplications as a way to capture domain semantics. Interviewing 13 embedding\nusers across disciplines, we find comparing embeddings is a key task for\ndeployment or downstream analysis but unfolds in a tedious fashion that poorly\nsupports systematic exploration. In response, we present the Embedding\nComparator, an interactive system that presents a global comparison of\nembedding spaces alongside fine-grained inspection of local neighborhoods. It\nsystematically surfaces points of comparison by computing the similarity of the\n$k$-nearest neighbors of every embedded object between a pair of spaces.\nThrough case studies across multiple modalities, we demonstrate our system\nrapidly reveals insights, such as semantic changes following fine-tuning,\nlanguage changes over time, and differences between seemingly similar models.\nIn evaluations with 15 participants, we find our system accelerates comparisons\nby shifting from laborious manual specification to browsing and manipulating\nvisualizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carter_B/0/1/0/all/0/1\">Brandon Carter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1\">Arvind Satyanarayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Classifying Continuous Constraint Satisfaction Problems. (arXiv:2106.02397v3 [cs.CC] UPDATED)","link":"http://arxiv.org/abs/2106.02397","description":"<p>A continuous constraint satisfaction problem (CCSP) is a constraint\nsatisfaction problem (CSP) with an interval domain $U \\subset \\mathbb{R}$. We\nengage in a systematic study to classify CCSPs that are complete of the\nExistential Theory of the Reals, i.e., ER-complete. To define this class, we\nfirst consider the problem ETR, which also stands for Existential Theory of the\nReals. In an instance of this problem we are given some sentence of the form\n$\\exists x_1, \\ldots, x_n \\in \\mathbb{R} : \\Phi(x_1, \\ldots, x_n)$, where\n$\\Phi$ is a well-formed quantifier-free formula consisting of the symbols $\\{0,\n1, +, \\cdot, \\geq, &gt;, \\wedge, \\vee, \\neg\\}$, the goal is to check whether this\nsentence is true. Now the class ER is the family of all problems that admit a\npolynomial-time many-one reduction to ETR. It is known that NP $\\subseteq$ ER\n$\\subseteq$ PSPACE.\n</p>\n<p>We restrict our attention on CCSPs with addition constraints ($x + y = z$)\nand some other mild technical condition. Previously, it was shown that\nmultiplication constraints ($x \\cdot y = z$), squaring constraints ($x^2 = y$),\nor inversion constraints ($x\\cdot y = 1$) are sufficient to establish\nER-completeness. We extend this in the strongest possible sense for equality\nconstraints as follows. We show that CCSPs (with addition constraints and some\nother mild technical condition) that have any one well-behaved curved equality\nconstraint ($f(x,y) = 0$) are ER-complete. We further extend our results to\ninequality constraints. We show that any well-behaved convexly curved and any\nwell-behaved concavely curved inequality constraint ($f(x,y) \\geq 0$ and\n$g(x,y) \\geq 0$) imply ER-completeness on the class of such CCSPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1\">Tillmann Miltzow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmiermann_R/0/1/0/all/0/1\">Reinier F. Schmiermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear-time calculation of the expected sum of edge lengths in random projective linearizations of trees. (arXiv:2107.03277v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03277","description":"<p>The syntactic structure of a sentence is often represented using syntactic\ndependency trees. The sum of the distances between syntactically related words\nhas been in the limelight for the past decades. Research on dependency\ndistances led to the formulation of the principle of dependency distance\nminimization whereby words in sentences are ordered so as to minimize that sum.\nNumerous random baselines have been defined to carry out related quantitative\nstudies on languages. The simplest random baseline is the expected value of the\nsum in unconstrained random permutations of the words in the sentence, namely\nwhen all the shufflings of the words of a sentence are allowed and equally\nlikely. Here we focus on a popular baseline: random projective permutations of\nthe words of the sentence, that is, permutations where the syntactic dependency\nstructure is projective, a formal constraint that sentences satisfy often in\nlanguages. Thus far, the expectation of the sum of dependency distances in\nrandom projective shufflings of a sentence has been estimated approximately\nwith a Monte Carlo procedure whose cost is of the order of $Rn$, where $n$ is\nthe number of words of the sentence and $R$ is the number of samples; it is\nwell known that the larger $R$, the lower the error of the estimation but the\nlarger the time cost. Here we present formulae to compute that expectation\nwithout error in time of the order of $n$. Furthermore, we show that star trees\nmaximize it, and give an algorithm to retrieve the trees that minimize it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Sentence Classification: Detecting Sustainability Initiatives in Company Reports. (arXiv:2110.03727v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03727","description":"<p>We introduce the novel task of detecting sustainability initiatives in\ncompany reports. Given a full report, the aim is to automatically identify\nmentions of practical activities that a company has performed in order to\ntackle specific societal issues. New methods for identifying continuous\nsentence spans need to be developed for capturing the multi-sentence structure\nof individual sustainability initiatives. We release a new dataset of company\nreports in which the text has been manually annotated with sustainability\ninitiatives. We also evaluate different models for initiative detection,\nintroducing a novel aggregation and evaluation methodology. Our proposed\narchitecture uses sequences of consecutive sentences to account for contextual\ninformation when making classification decisions at the individual sentence\nlevel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirlea_D/0/1/0/all/0/1\">Dan Hirlea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryant_C/0/1/0/all/0/1\">Christopher Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollo_M/0/1/0/all/0/1\">Maurizio Zollo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00374","description":"<p>Existing neural style transfer methods require reference style images to\ntransfer texture information of style images to content images. However, in\nmany practical situations, users may not have reference style images but still\nbe interested in transferring styles by just imagining them. In order to deal\nwith such applications, we propose a new framework that enables a style\ntransfer `without' a style image, but only with a text description of the\ndesired style. Using the pre-trained text-image embedding model of CLIP, we\ndemonstrate the modulation of the style of content images only with a single\ntext condition. Specifically, we propose a patch-wise text-image matching loss\nwith multiview augmentations for realistic texture transfer. Extensive\nexperimental results confirmed the successful image style transfer with\nrealistic textures that reflect semantic query texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An unsupervised extractive summarization method based on multi-round computation. (arXiv:2112.03203v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03203","description":"<p>Text summarization methods have attracted much attention all the time. In\nrecent years, deep learning has been applied to text summarization, and it\nturned out to be pretty effective. However, most of the current text\nsummarization methods based on deep learning need large-scale datasets, which\nis difficult to achieve in practical applications. In this paper, an\nunsupervised extractive text summarization method based on multi-round\ncalculation is proposed. Based on the directed graph algorithm, we change the\ntraditional method of calculating the sentence ranking at one time to\nmulti-round calculation, and the summary sentences are dynamically optimized\nafter each round of calculation to better match the characteristics of the\ntext. In this paper, experiments are carried out on four data sets, each\nseparately containing Chinese, English, long and short texts. The experiment\nresults show that our method has better performance than both baseline methods\nand other unsupervised methods and is robust on different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dehao Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yingzhu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongliang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minghu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kevin Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNMT: Neural Machine Translation Toolkit. (arXiv:2112.15272v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15272","description":"<p>We present an open-source toolkit for neural machine translation (NMT). The\nnew toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along\nwith many other improvements detailed below, in order to create a\nself-contained, simple to use, consistent and comprehensive framework for\nMachine Translation tasks of various domains. It is tooled to support both\nbilingual and multilingual translation tasks, starting from building the model\nfrom respective corpora, to inferring new predictions or packaging the model to\nserving-capable JIT format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_N/0/1/0/all/0/1\">Nguyen Hoang Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dat_N/0/1/0/all/0/1\">Nguyen Thanh Dat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_N/0/1/0/all/0/1\">Nguyen Hoang Minh Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Nguyen Van Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Ngo Thi Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_N/0/1/0/all/0/1\">Nguyen Phuong Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_T/0/1/0/all/0/1\">Tran Hong Viet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Planck Radiation and Quantization Scheme for Human Cognition and Language. (arXiv:2201.03306v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2201.03306","description":"<p>As a result of the identification of 'identity' and 'indistinguishability'\nand strong experimental evidence for the presence of the associated\nBose-Einstein statistics in human cognition and language, we argued in previous\nwork for an extension of the research domain of quantum cognition. In addition\nto quantum complex vector spaces and quantum probability models, we showed that\nquantization itself, with words as quanta, is relevant and potentially\nimportant to human cognition. In the present work, we build on this result, and\nintroduce a powerful radiation quantization scheme for human cognition. We show\nthat the lack of independence of the Bose-Einstein statistics compared to the\nMaxwell-Boltzmann statistics can be explained by the presence of a 'meaning\ndynamics', which causes words to be attracted to the same words. And so words\nclump together in the same states, a phenomenon well known for photons in the\nearly years of quantum mechanics, leading to fierce disagreements between\nPlanck and Einstein. Using a simple example, we introduce all the elements to\nget a better and detailed view of this 'meaning dynamics', such as micro and\nmacro states, and Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac numbers and\nweights, and compare this example and its graphs, with the radiation\nquantization scheme of a Winnie the Pooh story, also with its graphs. By\nconnecting a concept directly to human experience, we show that entanglement is\na necessity for preserving the 'meaning dynamics' we identified, and it becomes\nclear in what way Fermi-Dirac addresses human memory. Within the human mind, as\na crucial aspect of memory, in spaces with internal parameters, identical words\ncan nevertheless be assigned different states and hence realize locally and\ncontextually the necessary distinctiveness, structured by a Pauli exclusion\nprinciple, for human thought to thrive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Aerts_D/0/1/0/all/0/1\">Diederik Aerts</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Beltran_L/0/1/0/all/0/1\">Lester Beltran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06009","description":"<p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are\nobvious to humans. For example, GPT-3 would mistakenly interpret \"What word is\nsimilar to good?\" to mean a homonym, while the user intended a synonym. Our\ngoal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be prohibitively costly. We pair\nGPT-3 with a growing memory of recorded cases where the model misunderstood the\nuser's intents, along with user feedback for clarification. Such a memory\nallows our system to produce enhanced prompts for any new query based on the\nuser feedback for error correction on similar cases in the past. On four tasks\n(two lexical tasks, two advanced ethical reasoning tasks), we show how a\n(simulated) user can interactively teach a deployed GPT-3, substantially\nincreasing its accuracy over the queries with different kinds of\nmisunderstandings by the GPT-3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained LMs. All the code and data is\navailable at https://github.com/madaan/memprompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TourBERT: A pretrained language model for the tourism industry. (arXiv:2201.07449v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07449","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) is\ncurrently one of the most important and state-of-the-art models for natural\nlanguage. However, it has also been shown that for domain-specific tasks it is\nhelpful to pretrain BERT on a domain-specific corpus. In this paper, we present\nTourBERT, a pretrained language model for tourism. We describe how TourBERT was\ndeveloped and evaluated. The evaluations show that TourBERT is outperforming\nBERT in all tourism-specific tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arefieva_V/0/1/0/all/0/1\">Veronika Arefieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_R/0/1/0/all/0/1\">Roman Egger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure Extraction in Task-Oriented Dialogues with Slot Clustering. (arXiv:2203.00073v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00073","description":"<p>Extracting structure information from dialogue data can help us better\nunderstand user and system behaviors. In task-oriented dialogues, dialogue\nstructure has often been considered as transition graphs among dialogue states.\nHowever, annotating dialogue states manually is expensive and time-consuming.\nIn this paper, we propose a simple yet effective approach for structure\nextraction in task-oriented dialogues. We first detect and cluster possible\nslot tokens with a pre-trained model to approximate dialogue ontology for a\ntarget domain. Then we track the status of each identified token group and\nderive a state transition structure. Empirical results show that our approach\noutperforms unsupervised baseline models by far in dialogue structure\nextraction. In addition, we show that data augmentation based on extracted\nstructures enriches the surface formats of training data and can achieve a\nsignificant performance boost in dialogue response generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01311","description":"<p>Learning multimodal representations involves discovering correspondences and\nintegrating information from multiple heterogeneous sources of data. While\nrecent research has begun to explore the design of more general-purpose\nmultimodal models (contrary to prior focus on domain and modality-specific\narchitectures), these methods are still largely focused on a small set of\nmodalities in the language, vision, and audio space. In order to accelerate\ngeneralization towards diverse and understudied modalities, we investigate\nmethods for high-modality (a large set of diverse modalities) and\npartially-observable (each task only defined on a small subset of modalities)\nscenarios. To tackle these challenges, we design a general multimodal model\nthat enables multitask and transfer learning: multitask learning with shared\nparameters enables stable parameter counts (addressing scalability), and\ncross-modal transfer learning enables information sharing across modalities and\ntasks (addressing partial observability). Our resulting model generalizes\nacross text, image, video, audio, time-series, sensors, tables, and set\nmodalities from different research areas, improves the tradeoff between\nperformance and efficiency, transfers to new modalities and tasks, and reveals\nsurprising insights on the nature of information sharing in multitask models.\nWe release our code and benchmarks which we hope will present a unified\nplatform for subsequent theoretical and empirical analysis:\nhttps://github.com/pliang279/HighMMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives. (arXiv:2203.01445v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01445","description":"<p>The volume of available data has grown dramatically in recent years in many\napplications. Furthermore, the age of networks that used multiple modalities\nseparately has practically ended. Therefore, enabling bidirectional\ncross-modality data retrieval capable of processing has become a requirement\nfor many domains and disciplines of research. This is especially true in the\nmedical field, as data comes in a multitude of types, including various types\nof images and reports as well as molecular data. Most contemporary works apply\ncross attention to highlight the essential elements of an image or text in\nrelation to the other modalities and try to match them together. However,\nregardless of their importance in their own modality, these approaches usually\nconsider features of each modality equally. In this study, self-attention as an\nadditional loss term will be proposed to enrich the internal representation\nprovided into the cross attention module. This work suggests a novel\narchitecture with a new loss term to help represent images and texts in the\njoint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO\nand ARCH, show the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maleki_D/0/1/0/all/0/1\">Danial Maleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R Tizhoosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition. (arXiv:2203.01543v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01543","description":"<p>Recently, prompt-based learning for pre-trained language models has succeeded\nin few-shot Named Entity Recognition (NER) by exploiting prompts as task\nguidance to increase label efficiency. However, previous prompt-based methods\nfor few-shot NER have limitations such as a higher computational complexity,\npoor zero-shot ability, requiring manual prompt engineering, or lack of prompt\nrobustness. In this work, we address these shortcomings by proposing a new\nprompt-based learning NER method with Question Answering (QA), called QaNER.\nOur approach includes 1) a refined strategy for converting NER problems into\nthe QA formulation; 2) NER prompt generation for QA models; 3) prompt-based\ntuning with QA models on a few annotated NER examples; 4) zero-shot NER by\nprompting the QA model. Comparing the proposed approach with previous methods,\nQaNER is faster at inference, insensitive to the prompt quality, and robust to\nhyper-parameters, as well as demonstrating significantly better low-resource\nperformance and zero-shot capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Neural Networks on SVD Boosted Latent Spaces for Semantic Classification. (arXiv:2101.00563v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2101.00563","description":"<p>The availability of large amounts of data and compelling computation power\nhave made deep learning models much popular for text classification and\nsentiment analysis. Deep neural networks have achieved competitive performance\non the above tasks when trained on naive text representations such as word\ncount, term frequency, and binary matrix embeddings. However, many of the above\nrepresentations result in the input space having a dimension of the order of\nthe vocabulary size, which is enormous. This leads to a blow-up in the number\nof parameters to be learned, and the computational cost becomes infeasible when\nscaling to domains that require retaining a colossal vocabulary. This work\nproposes using singular value decomposition to transform the high dimensional\ninput space to a lower-dimensional latent space. We show that neural networks\ntrained on this lower-dimensional space are not only able to retain performance\nwhile savoring significant reduction in the computational complexity but, in\nmany situations, also outperforms the classical neural networks trained on the\nnative input space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1\">Sahil Sidheekh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Contextual Attention Network: Transformer Meets U-Net. (arXiv:2203.01932v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01932","description":"<p>Currently, convolutional neural networks (CNN) (e.g., U-Net) have become the\nde facto standard and attained immense success in medical image segmentation.\nHowever, as a downside, CNN based methods are a double-edged sword as they fail\nto build long-range dependencies and global context connections due to the\nlimited receptive field that stems from the intrinsic characteristics of the\nconvolution operation. Hence, recent articles have exploited Transformer\nvariants for medical image segmentation tasks which open up great opportunities\ndue to their innate capability of capturing long-range correlations through the\nattention mechanism. Although being feasibly designed, most of the cohort\nstudies incur prohibitive performance in capturing local information, thereby\nresulting in less lucidness of boundary areas. In this paper, we propose a\ncontextual attention network to tackle the aforementioned limitations. The\nproposed method uses the strength of the Transformer module to model the\nlong-range contextual dependency. Simultaneously, it utilizes the CNN encoder\nto capture local semantic information. In addition, an object-level\nrepresentation is included to model the regional interaction map. The extracted\nhierarchical features are then fed to the contextual attention module to\nadaptively recalibrate the representation space using the local information.\nThen, they emphasize the informative regions while taking into account the\nlong-range contextual dependency derived by the Transformer module. We validate\nour method on several large-scale public medical image segmentation datasets\nand achieve state-of-the-art performance. We have provided the implementation\ncode in https://github.com/rezazad68/TMUnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Reza_A/0/1/0/all/0/1\">Azad Reza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moein_H/0/1/0/all/0/1\">Heidari Moein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuli_W/0/1/0/all/0/1\">Wu Yuli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorit_M/0/1/0/all/0/1\">Merhof Dorit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Context Matters: Enhancing Single Image Prediction with Disease Progression Representations. (arXiv:2203.01933v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01933","description":"<p>Clinical outcome or severity prediction from medical images has largely\nfocused on learning representations from single-timepoint or snapshot scans. It\nhas been shown that disease progression can be better characterized by temporal\nimaging. We therefore hypothesized that outcome predictions can be improved by\nutilizing the disease progression information from sequential images. We\npresent a deep learning approach that leverages temporal progression\ninformation to improve clinical outcome predictions from single-timepoint\nimages. In our method, a self-attention based Temporal Convolutional Network\n(TCN) is used to learn a representation that is most reflective of the disease\ntrajectory. Meanwhile, a Vision Transformer is pretrained in a self-supervised\nfashion to extract features from single-timepoint images. The key contribution\nis to design a recalibration module that employs maximum mean discrepancy loss\n(MMD) to align distributions of the above two contextual representations. We\ntrain our system to predict clinical outcomes and severity grades from\nsingle-timepoint images. Experiments on chest and osteoarthritis radiography\ndatasets demonstrate that our approach outperforms other state-of-the-art\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Konwer_A/0/1/0/all/0/1\">Aishik Konwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality or Quantity: Toward a Unified Approach for Multi-organ Segmentation in Body CT. (arXiv:2203.01934v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01934","description":"<p>Organ segmentation of medical images is a key step in virtual imaging trials.\nHowever, organ segmentation datasets are limited in terms of quality (because\nlabels cover only a few organs) and quantity (since case numbers are limited).\nIn this study, we explored the tradeoffs between quality and quantity. Our goal\nis to create a unified approach for multi-organ segmentation of body CT, which\nwill facilitate the creation of large numbers of accurate virtual phantoms.\nInitially, we compared two segmentation architectures, 3D-Unet and DenseVNet,\nwhich were trained using XCAT data that is fully labeled with 22 organs, and\nchose the 3D-Unet as the better performing model. We used the XCAT-trained\nmodel to generate pseudo-labels for the CT-ORG dataset that has only 7 organs\nsegmented. We performed two experiments: First, we trained 3D-UNet model on the\nXCAT dataset, representing quality data, and tested it on both XCAT and CT-ORG\ndatasets. Second, we trained 3D-UNet after including the CT-ORG dataset into\nthe training set to have more quantity. Performance improved for segmentation\nin the organs where we have true labels in both datasets and degraded when\nrelying on pseudo-labels. When organs were labeled in both datasets, Exp-2\nimproved Average DSC in XCAT and CT-ORG by 1. This demonstrates that quality\ndata is the key to improving the model's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tushar_F/0/1/0/all/0/1\">Fakrul Islam Tushar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nujaim_H/0/1/0/all/0/1\">Husam Nujaim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_W/0/1/0/all/0/1\">Wanyi Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abadi_E/0/1/0/all/0/1\">Ehsan Abadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samei_E/0/1/0/all/0/1\">Ehsan Samei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Segars_W/0/1/0/all/0/1\">William P. Segars</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lo_J/0/1/0/all/0/1\">Joseph Y. Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-CIR: Event-Enhanced Continuous Intensity Recovery. (arXiv:2203.01935v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01935","description":"<p>A camera begins to sense light the moment we press the shutter button. During\nthe exposure interval, relative motion between the scene and the camera causes\nmotion blur, a common undesirable visual artifact. This paper presents E-CIR,\nwhich converts a blurry image into a sharp video represented as a parametric\nfunction from time to intensity. E-CIR leverages events as an auxiliary input.\nWe discuss how to exploit the temporal event structure to construct the\nparametric bases. We demonstrate how to train a deep learning model to predict\nthe function coefficients. To improve the appearance consistency, we further\nintroduce a refinement module to propagate visual features among consecutive\nframes. Compared to state-of-the-art event-enhanced deblurring approaches,\nE-CIR generates smoother and more realistic results. The implementation of\nE-CIR is available at https://github.com/chensong1995/E-CIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_C/0/1/0/all/0/1\">Chen Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit Bajaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-guided Image Virtual Attribute Learning for Noisy Multi-label Chest X-ray Classification. (arXiv:2203.01937v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01937","description":"<p>Deep learning methods have shown outstanding classification accuracy in\nmedical image analysis problems, which is largely attributed to the\navailability of large datasets manually annotated with clean labels. However,\nsuch manual annotation can be expensive to obtain for large datasets, so we may\nrely on machine-generated noisy labels. Many Chest X-ray (CXR) classifiers are\nmodelled from datasets with machine-generated labels, but their training\nprocedure is in general not robust to the presence of noisy-label samples and\ncan overfit those samples to produce sub-optimal solutions. Furthermore, CXR\ndatasets are mostly multi-label, so current noisy-label learning methods\ndesigned for multi-class problems cannot be easily adapted. To address such\nnoisy multi-label CXR learning problem, we propose a new learning method based\non estimating image virtual attributes using semantic information from the\nlabel to assist in the identification and correction of noisy multi-labels from\ntraining samples. Our experiments on diverse noisy multi-label training sets\nand clean testing sets show that our model has state-of-the-art accuracy and\nrobustness across all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Color Space-based HoVer-Net for Nuclei Instance Segmentation and Classification. (arXiv:2203.01940v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01940","description":"<p>Nuclei segmentation and classification is the first and most crucial step\nthat is utilized for many different microscopy medical analysis applications.\nHowever, it suffers from many issues such as the segmentation of small objects,\nimbalance, and fine-grained differences between types of nuclei. In this paper,\nmultiple different contributions were done tackling these problems present.\nFirstly, the recently released \"ConvNeXt\" was used as the encoder for HoVer-Net\nmodel since it leverages the key components of transformers that make them\nperform well. Secondly, to enhance the visual differences between nuclei, a\nmulti-channel color space-based approach is used to aid the model in extracting\ndistinguishing features. Thirdly, Unified Focal loss (UFL) was used to tackle\nthe background-foreground imbalance. Finally, Sharpness-Aware Minimization\n(SAM) was used to ensure generalizability of the model. Overall, we were able\nto outperform the current state-of-the-art (SOTA), HoVer-Net, on the\npreliminary test set of the CoNiC Challenge 2022 by 12.489% mPQ+.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Azzuni_H/0/1/0/all/0/1\">Hussam Azzuni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Image Generation using Residual Quantization. (arXiv:2203.01941v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01941","description":"<p>For autoregressive (AR) modeling of high-resolution images, vector\nquantization (VQ) represents an image as a sequence of discrete codes. A short\nsequence length is important for an AR model to reduce its computational costs\nto consider long-range interactions of codes. However, we postulate that\nprevious VQ cannot shorten the code sequence and generate high-fidelity images\ntogether in terms of the rate-distortion trade-off. In this study, we propose\nthe two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and\nRQ-Transformer, to effectively generate high-resolution images. Given a fixed\ncodebook size, RQ-VAE can precisely approximate a feature map of an image and\nrepresent the image as a stacked map of discrete codes. Then, RQ-Transformer\nlearns to predict the quantized feature vector at the next position by\npredicting the next stack of codes. Thanks to the precise approximation of\nRQ-VAE, we can represent a 256$\\times$256 image as 8$\\times$8 resolution of the\nfeature map, and RQ-Transformer can efficiently reduce the computational costs.\nConsequently, our framework outperforms the existing AR models on various\nbenchmarks of unconditional and conditional image generation. Our approach also\nhas a significantly faster sampling speed than previous AR models to generate\nhigh-quality images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Doyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chiheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Saehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wook-Shin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multi-stream convolutional neural network for classification of progressive MCI in Alzheimer's disease using structural MRI images. (arXiv:2203.01944v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01944","description":"<p>Early diagnosis of Alzheimer's disease and its prodromal stage, also known as\nmild cognitive impairment (MCI), is critical since some patients with\nprogressive MCI will develop the disease. We propose a multi-stream deep\nconvolutional neural network fed with patch-based imaging data to classify\nstable MCI and progressive MCI. First, we compare MRI images of Alzheimer's\ndisease with cognitively normal subjects to identify distinct anatomical\nlandmarks using a multivariate statistical test. These landmarks are then used\nto extract patches that are fed into the proposed multi-stream convolutional\nneural network to classify MRI images. Next, we train the architecture in a\nseparate scenario using samples from Alzheimer's disease images, which are\nanatomically similar to the progressive MCI ones and cognitively normal images\nto compensate for the lack of progressive MCI training data. Finally, we\ntransfer the trained model weights to the proposed architecture in order to\nfine-tune the model using progressive MCI and stable MCI data. Experimental\nresults on the ADNI-1 dataset indicate that our method outperforms existing\nmethods for MCI classification, with an F1-score of 85.96%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ashtari_Majlan_M/0/1/0/all/0/1\">Mona Ashtari-Majlan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seifi_A/0/1/0/all/0/1\">Abbas Seifi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehshibi_M/0/1/0/all/0/1\">Mohammad Mahdi Dehshibi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining. (arXiv:2203.01969v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01969","description":"<p>Retrospective analysis of brain MRI scans acquired in the clinic has the\npotential to enable neuroimaging studies with sample sizes much larger than\nthose found in research datasets. However, analysing such clinical images \"in\nthe wild\" is challenging, since subjects are scanned with highly variable\nprotocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent\nadvances in convolutional neural networks (CNNs) and domain randomisation for\nimage segmentation, best represented by the publicly available method SynthSeg,\nmay enable morphometry of clinical MRI at scale. In this work, we first\nevaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000\nscans acquired at Massachusetts General Hospital. We show that SynthSeg is\ngenerally robust, but frequently falters on scans with low signal-to-noise\nratio or poor tissue contrast. Next, we propose SynthSeg+, a novel method that\ngreatly mitigates these problems using a hierarchy of conditional segmentation\nand denoising CNNs. We show that this method is considerably more robust than\nSynthSeg, while also outperforming cascaded networks and state-of-the-art\nsegmentation denoising methods. Finally, we apply our approach to a\nproof-of-concept volumetric study of ageing, where it closely replicates\natrophy patterns observed in research studies conducted on high-quality, 1mm,\nT1-weighted scans. The code and trained model are publicly available at\nhttps://github.com/BBillot/SynthSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1\">Benjamin Billot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colin_M/0/1/0/all/0/1\">Magdamo Colin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_S/0/1/0/all/0/1\">Sean E. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sudeshna Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan. E. Iglesias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Rich, Portable, and Large-Scale Pedestrian Data Collection. (arXiv:2203.01974v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01974","description":"<p>Recently, pedestrian behavior research has shifted towards machine learning\nbased methods and converged on the topic of modeling pedestrian interactions.\nFor this, a large-scale dataset that contains rich information is needed. We\npropose a data collection system that is portable, which facilitates accessible\nlarge-scale data collection in diverse environments. We also couple the system\nwith a semi-autonomous labeling pipeline for fast trajectory label production.\nWe demonstrate the effectiveness of our system by further introducing a dataset\nwe have collected -- the TBD pedestrian dataset. Compared with existing\npedestrian datasets, our dataset contains three components: human verified\nlabels grounded in the metric space, a combination of top-down and perspective\nviews, and naturalistic human behavior in the presence of a socially\nappropriate \"robot\". In addition, the TBD pedestrian dataset is larger in\nquantity compared to similar existing datasets and contains unique pedestrian\nbehavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Allan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Abhijat Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Admoni_H/0/1/0/all/0/1\">Henny Admoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinfeld_A/0/1/0/all/0/1\">Aaron Steinfeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Object Classification for Human-Robot Collaboration. (arXiv:2203.01977v1 [cs.MM])","link":"http://arxiv.org/abs/2203.01977","description":"<p>Human-robot collaboration requires the contactless estimation of the physical\nproperties of containers manipulated by a person, for example while pouring\ncontent in a cup or moving a food box. Acoustic and visual signals can be used\nto estimate the physical properties of such objects, which may vary\nsubstantially in shape, material and size, and also be occluded by the hands of\nthe person. To facilitate comparisons and stimulate progress in solving this\nproblem, we present the CORSMAL challenge and a dataset to assess the\nperformance of the algorithms through a set of well-defined performance scores.\nThe tasks of the challenge are the estimation of the mass, capacity, and\ndimensions of the object (container), and the classification of the type and\namount of its content. A novel feature of the challenge is our\nreal-to-simulation framework for visualising and assessing the impact of\nestimation errors in human-to-robot handovers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">A. Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Y. L. Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patten_T/0/1/0/all/0/1\">T. Patten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_A/0/1/0/all/0/1\">A. Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calli_B/0/1/0/all/0/1\">B. Calli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">A. Cavallaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-of-Interest Based Neural Video Compression. (arXiv:2203.01978v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01978","description":"<p>Humans do not perceive all parts of a scene with the same resolution, but\nrather focus on few regions of interest (ROIs). Traditional Object-Based codecs\ntake advantage of this biological intuition, and are capable of non-uniform\nallocation of bits in favor of salient regions, at the expense of increased\ndistortion the remaining areas: such a strategy allows a boost in perceptual\nquality under low rate constraints. Recently, several neural codecs have been\nintroduced for video compression, yet they operate uniformly over all spatial\nlocations, lacking the capability of ROI-based processing. In this paper, we\nintroduce two models for ROI-based neural video coding. First, we propose an\nimplicit model that is fed with a binary ROI mask and it is trained by\nde-emphasizing the distortion of the background. Secondly, we design an\nexplicit latent scaling method, that allows control over the quantization\nbinwidth for different spatial regions of latent variables, conditioned on the\nROI mask. By extensive experiments, we show that our methods outperform all our\nbaselines in terms of Rate-Distortion (R-D) performance in the ROI. Moreover,\nthey can generalize to different datasets and to any arbitrary ROI at inference\ntime. Finally, they do not require expensive pixel-level annotations during\ntraining, as synthetic ROI masks can be used with little to no degradation in\nperformance. To the best of our knowledge, our proposals are the first\nsolutions that integrate ROI-based capabilities into neural video compression\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Perugachi_Diaz_Y/0/1/0/all/0/1\">Yura Perugachi-Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sautiere_G/0/1/0/all/0/1\">Guillaume Sauti&#xe8;re</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abati_D/0/1/0/all/0/1\">Davide Abati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habibian_A/0/1/0/all/0/1\">Amirhossein Habibian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco S Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values. (arXiv:2203.01993v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01993","description":"<p>We present Polarity Sampling, a theoretically justified plug-and-play method\nfor controlling the generation quality and diversity of pre-trained deep\ngenerative networks DGNs). Leveraging the fact that DGNs are, or can be\napproximated by, continuous piecewise affine splines, we derive the analytical\nDGN output space distribution as a function of the product of the DGN's\nJacobian singular values raised to a power $\\rho$. We dub $\\rho$ the\n$\\textbf{polarity}$ parameter and prove that $\\rho$ focuses the DGN sampling on\nthe modes ($\\rho &lt; 0$) or anti-modes ($\\rho &gt; 0$) of the DGN output-space\ndistribution. We demonstrate that nonzero polarity values achieve a better\nprecision-recall (quality-diversity) Pareto frontier than standard methods,\nsuch as truncation, for a number of state-of-the-art DGNs. We also present\nquantitative and qualitative results on the improvement of overall generation\nquality (e.g., in terms of the Frechet Inception Distance) for a number of\nstate-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different\nconditional and unconditional image generation tasks. In particular, Polarity\nSampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to\nFID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the\nAFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-demo-colab\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Neural Architecture Search for Lightweight Dense Prediction Networks. (arXiv:2203.01994v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01994","description":"<p>Dense prediction is a class of computer vision problems aiming at mapping\nevery pixel of the input image with some predicted values. Depending on the\nproblem, the output values can be either continous or discrete. For instance,\nmonocular depth estimation and image super-resolution are often formulated as\nregression, while semantic segmentation is a dense classification, i.e.\ndiscrete, problem. More specifically, the monocular depth estimation problem\nproduces a dense depth map from a single image to be used in various\napplications including robotics, scene understanding, and augmented reality.\nSingle image super-resolution (SISR) is a low-level vision task that generates\na high-resolution image from its low-resolution counterpart. SISR is widely\nutilized in medical and surveillance imaging, where images with more precise\ndetails can provide invaluable information. On the other hand, semantic\nsegmentation predicts a dense annotated map of different semantic categories\nfrom a given image that is crucial for image understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1\">Lam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counting Molecules: Python based scheme for automated enumeration and categorization of molecules in scanning tunneling microscopy images. (arXiv:2203.01998v1 [cond-mat.mes-hall])","link":"http://arxiv.org/abs/2203.01998","description":"<p>Scanning tunneling and atomic force microscopies (STM/nc-AFM) are rapidly\nprogressing to offer unprecedented spatial resolution of a diverse array of\nchemical species. In particular, they are employed to characterize on-surface\nchemical reactions by directly examining precursors and products. Chiral\neffects and self-assembled structures can also be investigated. This open\nsource, modular, python based scheme automates the categorization of a variety\nof molecules present in medium sized (10$\\times$10 to 100$\\times$100 nm)\nscanned probe images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Hellerstedt_J/0/1/0/all/0/1\">Jack Hellerstedt</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Cahlik_A/0/1/0/all/0/1\">Ale&#x161; Cahl&#xed;k</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Svec_M/0/1/0/all/0/1\">Martin &#x160;vec</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Stetsovych_O/0/1/0/all/0/1\">Oleksandr Stetsovych</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Hennen_T/0/1/0/all/0/1\">Tyler Hennen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why adversarial training can hurt robust accuracy. (arXiv:2203.02006v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02006","description":"<p>Machine learning classifiers with high test accuracy often perform poorly\nunder adversarial attacks. It is commonly believed that adversarial training\nalleviates this issue. In this paper, we demonstrate that, surprisingly, the\nopposite may be true -- Even though adversarial training helps when enough data\nis available, it may hurt robust generalization in the small sample size\nregime. We first prove this phenomenon for a high-dimensional linear\nclassification setting with noiseless observations. Our proof provides\nexplanatory insights that may also transfer to feature learning models.\nFurther, we observe in experiments on standard image datasets that the same\nbehavior occurs for perceptible attacks that effectively reduce class\ninformation such as mask attacks and object corruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clarysse_J/0/1/0/all/0/1\">Jacob Clarysse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_J/0/1/0/all/0/1\">Julia H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fanny Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations. (arXiv:2203.02013v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02013","description":"<p>The ability for a human to understand an Artificial Intelligence (AI) model's\ndecision-making process is critical in enabling stakeholders to visualize model\nbehavior, perform model debugging, promote trust in AI models, and assist in\ncollaborative human-AI decision-making. As a result, the research fields of\ninterpretable and explainable AI have gained traction within AI communities as\nwell as interdisciplinary scientists seeking to apply AI in their subject\nareas. In this paper, we focus on advancing the state-of-the-art in\ninterpreting multimodal models - a class of machine learning methods that\ntackle core challenges in representing and capturing interactions between\nheterogeneous data sources such as images, text, audio, and time-series data.\nMultimodal models have proliferated numerous real-world applications across\nhealthcare, robotics, multimedia, affective computing, and human-computer\ninteraction. By performing model disentanglement into unimodal contributions\n(UC) and multimodal interactions (MI), our proposed approach, DIME, enables\naccurate and fine-grained analysis of multimodal models while maintaining\ngenerality across arbitrary modalities, model architectures, and tasks. Through\na comprehensive suite of experiments on both synthetic and real-world\nmultimodal tasks, we show that DIME generates accurate disentangled\nexplanations, helps users of multimodal models gain a deeper understanding of\nmodel behavior, and presents a step towards debugging and improving these\nmodels for real-world deployment. Code for our experiments can be found at\nhttps://github.com/lvyiwei1/DIME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection-Inspired Few-Shot Medical Image Segmentation Through Self-Supervision With Supervoxels. (arXiv:2203.02048v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02048","description":"<p>Recent work has shown that label-efficient few-shot learning through\nself-supervision can achieve promising medical image segmentation results.\nHowever, few-shot segmentation models typically rely on prototype\nrepresentations of the semantic classes, resulting in a loss of local\ninformation that can degrade performance. This is particularly problematic for\nthe typically large and highly heterogeneous background class in medical image\nsegmentation problems. Previous works have attempted to address this issue by\nlearning additional prototypes for each class, but since the prototypes are\nbased on a limited number of slices, we argue that this ad-hoc solution is\ninsufficient to capture the background properties. Motivated by this, and the\nobservation that the foreground class (e.g., one organ) is relatively\nhomogeneous, we propose a novel anomaly detection-inspired approach to few-shot\nmedical image segmentation in which we refrain from modeling the background\nexplicitly. Instead, we rely solely on a single foreground prototype to compute\nanomaly scores for all query pixels. The segmentation is then performed by\nthresholding these anomaly scores using a learned threshold. Assisted by a\nnovel self-supervision task that exploits the 3D structure of medical images\nthrough supervoxels, our proposed anomaly detection-inspired few-shot medical\nimage segmentation model outperforms previous state-of-the-art approaches on\ntwo representative MRI datasets for the tasks of abdominal organ segmentation\nand cardiac segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hansen_S/0/1/0/all/0/1\">Stine Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gautam_S/0/1/0/all/0/1\">Srishti Gautam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenssen_R/0/1/0/all/0/1\">Robert Jenssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. (arXiv:2203.02053v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02053","description":"<p>We present modality gap, an intriguing geometric phenomenon of the\nrepresentation space of multi-modal models. Specifically, we show that\ndifferent data modalities (e.g. images and text) are embedded at arm's length\nin their shared representation in multi-modal models such as CLIP. Our\nsystematic analysis demonstrates that this gap is caused by a combination of\nmodel initialization and contrastive learning optimization. In model\ninitialization, we show empirically and theoretically that the representation\nof a common deep neural network is restricted to a narrow cone. As a\nconsequence, in a multi-modal model with two encoders, the representations of\nthe two modalities are clearly apart when the model is initialized. During\noptimization, contrastive learning keeps the different modalities separate by a\ncertain distance, which is influenced by the temperature parameter in the loss\nfunction. Our experiments further demonstrate that varying the modality gap\ndistance has a significant impact in improving the model's downstream zero-shot\nclassification performance and fairness. Our code and data are available at\nhttps://modalitygap.readthedocs.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yongchan Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Serena Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim2Real Instance-Level Style Transfer for 6D Pose Estimation. (arXiv:2203.02069v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02069","description":"<p>In recent years, synthetic data has been widely used in the training of 6D\npose estimation networks, in part because it automatically provides perfect\nannotation at low cost. However, there are still non-trivial domain gaps, such\nas differences in textures/materials, between synthetic and real data. These\ngaps have a measurable impact on performance. To solve this problem, we\nintroduce a simulation to reality (sim2real) instance-level style transfer for\n6D pose estimation network training. Our approach transfers the style of target\nobjects individually, from synthetic to real, without human intervention. This\nimproves the quality of synthetic data for training pose estimation networks.\nWe also propose a complete pipeline from data collection to the training of a\npose estimation network and conduct extensive evaluation on a real-world\nrobotic platform. Our evaluation shows significant improvement achieved by our\nmethod in both pose estimation performance and the realism of images adapted by\nthe style transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_T/0/1/0/all/0/1\">Takuya Ikeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanishige_S/0/1/0/all/0/1\">Suomi Tanishige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amma_A/0/1/0/all/0/1\">Ayako Amma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudano_M/0/1/0/all/0/1\">Michael Sudano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audren_H/0/1/0/all/0/1\">Herv&#xe9; Audren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishiwaki_K/0/1/0/all/0/1\">Koichi Nishiwaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Subpopulation-based Membership Inference Attack. (arXiv:2203.02080v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02080","description":"<p>Membership inference attacks allow a malicious entity to predict whether a\nsample is used during training of a victim model or not. State-of-the-art\nmembership inference attacks have shown to achieve good accuracy which poses a\ngreat privacy threat. However, majority of SOTA attacks require training dozens\nto hundreds of shadow models to accurately infer membership. This huge\ncomputation cost raises questions about practicality of these attacks on deep\nmodels. In this paper, we introduce a fundamentally different MI attack\napproach which obviates the need to train hundreds of shadow models. Simply\nput, we compare the victim model output on the target sample versus the samples\nfrom the same subpopulation (i.e., semantically similar samples), instead of\ncomparing it with the output of hundreds of shadow models. The intuition is\nthat the model response should not be significantly different between the\ntarget sample and its subpopulation if it was not a training sample. In cases\nwhere subpopulation samples are not available to the attacker, we show that\ntraining only a single generative model can fulfill the requirement. Hence, we\nachieve the state-of-the-art membership inference accuracy while significantly\nreducing the training computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Shahbaz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WPNAS: Neural Architecture Search by jointly using Weight Sharing and Predictor. (arXiv:2203.02086v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02086","description":"<p>Weight sharing based and predictor based methods are two major types of fast\nneural architecture search methods. In this paper, we propose to jointly use\nweight sharing and predictor in a unified framework. First, we construct a\nSuperNet in a weight-sharing way and probabilisticly sample architectures from\nthe SuperNet. To increase the correctness of the evaluation of architectures,\nbesides direct evaluation using the inherited weights, we further apply a\nfew-shot predictor to assess the architecture on the other hand. The final\nevaluation of the architecture is the combination of direct evaluation, the\nprediction from the predictor and the cost of the architecture. We regard the\nevaluation as a reward and apply a self-critical policy gradient approach to\nupdate the architecture probabilities. To further reduce the side effects of\nweight sharing, we propose a weakly weight sharing method by introducing\nanother HyperNet. We conduct experiments on datasets including CIFAR-10,\nCIFAR-100 and ImageNet under NATS-Bench, DARTS and MobileNet search space. The\nproposed WPNAS method achieves state-of-the-art performance on these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Ke Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_Y/0/1/0/all/0/1\">Yong A</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhuoxin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingying Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Segmentation of 33 Anatomies. (arXiv:2203.02098v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02098","description":"<p>In the paper, we present an approach for learning a single model that\nuniversally segments 33 anatomical structures, including vertebrae, pelvic\nbones, and abdominal organs. Our model building has to address the following\nchallenges. Firstly, while it is ideal to learn such a model from a\nlarge-scale, fully-annotated dataset, it is practically hard to curate such a\ndataset. Thus, we resort to learn from a union of multiple datasets, with each\ndataset containing the images that are partially labeled. Secondly, along the\nline of partial labelling, we contribute an open-source, large-scale vertebra\nsegmentation dataset for the benefit of spine analysis community, CTSpine1K,\nboasting over 1,000 3D volumes and over 11K annotated vertebrae. Thirdly, in a\n3D medical image segmentation task, due to the limitation of GPU memory, we\nalways train a model using cropped patches as inputs instead a whole 3D volume,\nwhich limits the amount of contextual information to be learned. To this, we\npropose a cross-patch transformer module to fuse more information in adjacent\npatches, which enlarges the aggregated receptive field for improved\nsegmentation performance. This is especially important for segmenting, say, the\nelongated spine. Based on 7 partially labeled datasets that collectively\ncontain about 2,800 3D volumes, we successfully learn such a universal model.\nFinally, we evaluate the universal model on multiple open-source datasets,\nproving that our model has a good generalization performance and can\npotentially serve as a solid foundation for downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Pengbo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Ce Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hui_Y/0/1/0/all/0/1\">Yuan Hui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_S/0/1/0/all/0/1\">Shiwei Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1\">Mengke Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hao_Y/0/1/0/all/0/1\">You Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_H/0/1/0/all/0/1\">Honghu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Chunpeng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xinbao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Incrementally to Segment Multiple Organs in a CT Image. (arXiv:2203.02100v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02100","description":"<p>There exists a large number of datasets for organ segmentation, which are\npartially annotated and sequentially constructed. A typical dataset is\nconstructed at a certain time by curating medical images and annotating the\norgans of interest. In other words, new datasets with annotations of new organ\ncategories are built over time. To unleash the potential behind these partially\nlabeled, sequentially-constructed datasets, we propose to incrementally learn a\nmulti-organ segmentation model. In each incremental learning (IL) stage, we\nlose the access to previous data and annotations, whose knowledge is assumingly\ncaptured by the current model, and gain the access to a new dataset with\nannotations of new organ categories, from which we learn to update the organ\nsegmentation model to include the new organs. While IL is notorious for its\n`catastrophic forgetting' weakness in the context of natural image analysis, we\nexperimentally discover that such a weakness mostly disappears for CT\nmulti-organ segmentation. To further stabilize the model performance across the\nIL stages, we introduce a light memory module and some loss functions to\nrestrain the representation of different categories in feature space,\naggregating feature representation of the same class and separating feature\nrepresentation of different classes. Extensive experiments on five open-sourced\ndatasets are conducted to illustrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Pengbo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xia Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_M/0/1/0/all/0/1\">Mengsi Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_H/0/1/0/all/0/1\">Hongli Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_M/0/1/0/all/0/1\">Minmin Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaohong Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_D/0/1/0/all/0/1\">Dandan Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoying Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_L/0/1/0/all/0/1\">Lian Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xingwang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Image Synthesis with Panoptic Layout Generation. (arXiv:2203.02104v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02104","description":"<p>Interactive image synthesis from user-guided input is a challenging task when\nusers wish to control the scene structure of a generated image with\nease.Although remarkable progress has been made on layout-based image synthesis\napproaches, in order to get realistic fake image in interactive scene, existing\nmethods require high-precision inputs, which probably need adjustment several\ntimes and are unfriendly to novice users. When placement of bounding boxes is\nsubject to perturbation, layout-based models suffer from \"missing regions\" in\nthe constructed semantic layouts and hence undesirable artifacts in the\ngenerated images. In this work, we propose Panoptic Layout Generative\nAdversarial Networks (PLGAN) to address this challenge. The PLGAN employs\npanoptic theory which distinguishes object categories between \"stuff\" with\namorphous boundaries and \"things\" with well-defined shapes, such that stuff and\ninstance layouts are constructed through separate branches and later fused into\npanoptic layouts. In particular, the stuff layouts can take amorphous shapes\nand fill up the missing regions left out by the instance layouts. We\nexperimentally compare our PLGAN with state-of-the-art layout-based models on\nthe COCO-Stuff, Visual Genome, and Landscape datasets. The advantages of PLGAN\nare not only visually demonstrated but quantitatively verified in terms of\ninception score, Fr\\'echet inception distance, classification accuracy score,\nand coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Peng Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision. (arXiv:2203.02106v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02106","description":"<p>Medical image segmentation plays an irreplaceable role in computer-assisted\ndiagnosis, treatment planning, and following-up. Collecting and annotating a\nlarge-scale dataset is crucial to training a powerful segmentation model, but\nproducing high-quality segmentation masks is an expensive and time-consuming\nprocedure. Recently, weakly-supervised learning that uses sparse annotations\n(points, scribbles, bounding boxes) for network training has achieved\nencouraging performance and shown the potential for annotation cost reduction.\nHowever, due to the limited supervision signal of sparse annotations, it is\nstill challenging to employ them for networks training directly. In this work,\nwe propose a simple yet efficient scribble-supervised image segmentation method\nand apply it to cardiac MRI segmentation. Specifically, we employ a dual-branch\nnetwork with one encoder and two slightly different decoders for image\nsegmentation and dynamically mix the two decoders' predictions to generate\npseudo labels for auxiliary supervision. By combining the scribble supervision\nand auxiliary pseudo labels supervision, the dual-branch network can\nefficiently learn from scribble annotations end-to-end. Experiments on the\npublic ACDC dataset show that our method performs better than current\nscribble-supervised segmentation methods and also outperforms several\nsemi-supervised segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_M/0/1/0/all/0/1\">Minhao Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_W/0/1/0/all/0/1\">Wenjun Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_S/0/1/0/all/0/1\">Shuwei Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_T/0/1/0/all/0/1\">Tao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Category-Level Generalizable Object Manipulation Policy via Generative Adversarial Self-Imitation Learning from Demonstrations. (arXiv:2203.02107v1 [cs.RO])","link":"http://arxiv.org/abs/2203.02107","description":"<p>Generalizable object manipulation skills are critical for intelligent and\nmulti-functional robots to work in real-world complex scenes. Despite the\nrecent progress in reinforcement learning, it is still very challenging to\nlearn a generalizable manipulation policy that can handle a category of\ngeometrically diverse articulated objects. In this work, we tackle this\ncategory-level object manipulation policy learning problem via imitation\nlearning in a task-agnostic manner, where we assume no handcrafted dense\nrewards but only a terminal reward. Given this novel and challenging\ngeneralizable policy learning problem, we identify several key issues that can\nfail the previous imitation learning algorithms and hinder the generalization\nto unseen instances. We then propose several general but critical techniques,\nincluding generative adversarial self-imitation learning from demonstrations,\nprogressive growing of discriminator, and instance-balancing for expert buffer,\nthat accurately pinpoints and tackles these issues and can benefit\ncategory-level manipulation policy learning regardless of the tasks. Our\nexperiments on ManiSkill benchmarks demonstrate a remarkable improvement on all\ntasks and our ablation studies further validate the contribution of each\nproposed technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weikang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis. (arXiv:2203.02110v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02110","description":"<p>Many works have shown that deep learning-based medical image classification\nmodels can exhibit bias toward certain demographic attributes like race,\ngender, and age. Existing bias mitigation methods primarily focus on learning\ndebiased models, which may not necessarily guarantee all sensitive information\ncan be removed and usually comes with considerable accuracy degradation on both\nprivileged and unprivileged groups. To tackle this issue, we propose a method,\nFairPrune, that achieves fairness by pruning. Conventionally, pruning is used\nto reduce the model size for efficient inference. However, we show that pruning\ncan also be a powerful tool to achieve fairness. Our observation is that during\npruning, each parameter in the model has different importance for different\ngroups' accuracy. By pruning the parameters based on this importance\ndifference, we can reduce the accuracy difference between the privileged group\nand the unprivileged group to improve fairness without a large accuracy drop.\nTo this end, we use the second derivative of the parameters of a pre-trained\nmodel to quantify the importance of each parameter with respect to the model\naccuracy for each group. Experiments on two skin lesion diagnosis datasets over\nmultiple sensitive attributes demonstrate that our method can greatly improve\nfairness while keeping the average accuracy of both groups as high as possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yawen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Jingtong Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving. (arXiv:2203.02112v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02112","description":"<p>Pseudo-LiDAR 3D detectors have made remarkable progress in monocular 3D\ndetection by enhancing the capability of perceiving depth with depth estimation\nnetworks, and using LiDAR-based 3D detection architectures. The advanced stereo\n3D detectors can also accurately localize 3D objects. The gap in image-to-image\ngeneration for stereo views is much smaller than that in image-to-LiDAR\ngeneration. Motivated by this, we propose a Pseudo-Stereo 3D detection\nframework with three novel virtual view generation methods, including\nimage-level generation, feature-level generation, and feature-clone, for\ndetecting 3D objects from a single image. Our analysis of depth-aware learning\nshows that the depth loss is effective in only feature-level virtual view\ngeneration and the estimated depth map is effective in both image-level and\nfeature-level in our framework. We propose a disparity-wise dynamic convolution\nwith dynamic kernels sampled from the disparity feature map to filter the\nfeatures adaptively from a single image for generating virtual image features,\nwhich eases the feature degradation caused by the depth estimation errors. Till\nsubmission (November 18, 2021), our Pseudo-Stereo 3D detection framework ranks\n1st on car, pedestrian, and cyclist among the monocular 3D detectors with\npublications on the KITTI-3D benchmark. The code is released at\nhttps://github.com/revisitq/Pseudo-Stereo-3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Nan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yong Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context. (arXiv:2203.02113v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02113","description":"<p>We advance sketch research to scenes with the first dataset of freehand scene\nsketches, FS-COCO. With practical applications in mind, we collect sketches\nthat convey well scene content but can be sketched within a few minutes by a\nperson with any sketching skills. Our dataset comprises 10,000 freehand scene\nvector sketches with per point space-time information by 100 non-expert\nindividuals, offering both object- and scene-level abstraction. Each sketch is\naugmented with its text description. Using our dataset, we study for the first\ntime the problem of the fine-grained image retrieval from freehand scene\nsketches and sketch captions. We draw insights on (i) Scene salience encoded in\nsketches with strokes temporal order; (ii) The retrieval performance accuracy\nfrom scene sketches against image captions; (iii) Complementarity of\ninformation in sketches and image captions, as well as the potential benefit of\ncombining the two modalities. In addition, we propose new solutions enabled by\nour dataset (i) We adopt meta-learning to show how the retrieval model can be\nfine-tuned to a new user style given just a small set of sketches, (ii) We\nextend a popular vector sketch LSTM-based encoder to handle sketches with\nlarger complexity than was supported by previous work. Namely, we propose a\nhierarchical sketch decoder, which we leverage at a sketch-specific \"pretext\"\ntask. Our dataset enables for the first time research on freehand scene sketch\nunderstanding and its practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1\">Yulia Gryaditskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixCL: Pixel label matters to contrastive learning. (arXiv:2203.02114v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02114","description":"<p>Contrastive learning and self-supervised techniques have gained prevalence in\ncomputer vision for the past few years. It is essential for medical image\nanalysis, which is often notorious for its lack of annotations. Most existing\nself-supervised methods applied in natural imaging tasks focus on designing\nproxy tasks for unlabeled data. For example, contrastive learning is often\nbased on the fact that an image and its transformed version share the same\nidentity. However, pixel annotations contain much valuable information for\nmedical image segmentation, which is largely ignored in contrastive learning.\nIn this work, we propose a novel pre-training framework called Mixed\nContrastive Learning (MixCL) that leverages both image identities and pixel\nlabels for better modeling by maintaining identity consistency, label\nconsistency, and reconstruction consistency together. Consequently, thus\npre-trained model has more robust representations that characterize medical\nimages. Extensive experiments demonstrate the effectiveness of the proposed\nmethod, improving the baseline by 5.28% and 14.12% in Dice coefficient when 5%\nlabeled data of Spleen and 15% of BTVC are used in fine-tuning, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Benchmarking and Evaluating Deepfake Detection. (arXiv:2203.02115v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02115","description":"<p>Deepfake detection automatically recognizes the manipulated medias through\nthe analysis of the difference between manipulated and non-altered videos. It\nis natural to ask which are the top performers among the existing deepfake\ndetection approaches to identify promising research directions and provide\npractical guidance. Unfortunately, it's difficult to conduct a sound\nbenchmarking comparison of existing detection approaches using the results in\nthe literature because evaluation conditions are inconsistent across studies.\nOur objective is to establish a comprehensive and consistent benchmark, to\ndevelop a repeatable evaluation procedure, and to measure the performance of a\nrange of detection approaches so that the results can be compared soundly. A\nchallenging dataset consisting of the manipulated samples generated by more\nthan 13 different methods has been collected, and 11 popular detection\napproaches (9 algorithms) from the existing literature have been implemented\nand evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92\nmodels have been trained and 644 experiments have been performed for the\nevaluation. The results along with the shared data and evaluation methodology\nconstitute a benchmark for comparing deepfake detection approaches and\nmeasuring progress.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenhao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingyi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Pengbin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D endoscopic depth estimation using 3D surface-aware constraints. (arXiv:2203.02131v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02131","description":"<p>Robotic-assisted surgery allows surgeons to conduct precise surgical\noperations with stereo vision and flexible motor control. However, the lack of\n3D spatial perception limits situational awareness during procedures and\nhinders mastering surgical skills in the narrow abdominal space. Depth\nestimation, as a representative perception task, is typically defined as an\nimage reconstruction problem. In this work, we show that depth estimation can\nbe reformed from a 3D surface perspective. We propose a loss function for depth\nestimation that integrates the surface-aware constraints, leading to a faster\nand better convergence with the valid information from spatial information. In\naddition, camera parameters are incorporated into the training pipeline to\nincrease the control and transparency of the depth estimation. We also\nintegrate a specularity removal module to recover more buried image\ninformation. Quantitative experimental results on endoscopic datasets and user\nstudies with medical professionals demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Shang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Ce Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qiyuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanzhe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Versatile Multi-View Framework for LiDAR-based 3D Object Detection with Guidance from Panoptic Segmentation. (arXiv:2203.02133v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02133","description":"<p>3D object detection using LiDAR data is an indispensable component for\nautonomous driving systems. Yet, only a few LiDAR-based 3D object detection\nmethods leverage segmentation information to further guide the detection\nprocess. In this paper, we propose a novel multi-task framework that jointly\nperforms 3D object detection and panoptic segmentation. In our method, the 3D\nobject detection backbone in Bird's-Eye-View (BEV) plane is augmented by the\ninjection of Range-View (RV) feature maps from the 3D panoptic segmentation\nbackbone. This enables the detection backbone to leverage multi-view\ninformation to address the shortcomings of each projection view. Furthermore,\nforeground semantic information is incorporated to ease the detection task by\nhighlighting the locations of each object class in the feature maps. Finally, a\nnew center density heatmap generated based on the instance-level information\nfurther guides the detection backbone by suggesting possible box center\nlocations for objects. Our method works with any BEV-based 3D object detection\nmethod, and as shown by extensive experiments on the nuScenes dataset, it\nprovides significant performance gains. Notably, the proposed method based on a\nsingle-stage CenterPoint 3D object detection network achieved state-of-the-art\nperformance on nuScenes 3D Detection Benchmark with 67.3 NDS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fazlali_H/0/1/0/all/0/1\">Hamidreza Fazlali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingbing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACVNet: Attention Concatenation Volume for Accurate and Efficient Stereo Matching. (arXiv:2203.02146v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02146","description":"<p>Stereo matching is a fundamental building block for many vision and robotics\napplications. An informative and concise cost volume representation is vital\nfor stereo matching of high accuracy and efficiency. In this paper, we present\na novel cost volume construction method which generates attention weights from\ncorrelation clues to suppress redundant information and enhance\nmatching-related information in the concatenation volume. To generate reliable\nattention weights, we propose multi-level adaptive patch matching to improve\nthe distinctiveness of the matching cost at different disparities even for\ntextureless regions. The proposed cost volume is named attention concatenation\nvolume (ACV) which can be seamlessly embedded into most stereo matching\nnetworks, the resulting networks can use a more lightweight aggregation network\nand meanwhile achieve higher accuracy, e.g. using only 1/25 parameters of the\naggregation network can achieve higher accuracy for GwcNet. Furthermore, we\ndesign a highly accurate network (ACVNet) based on our ACV, which achieves\nstate-of-the-art performance on several benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Junda Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Peng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging. (arXiv:2203.02149v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02149","description":"<p>The rapid development of deep learning provides a better solution for the\nend-to-end reconstruction of hyperspectral image (HSI). However, existing\nlearning-based methods have two major defects. Firstly, networks with\nself-attention usually sacrifice internal resolution to balance model\nperformance against complexity, losing fine-grained high-resolution (HR)\nfeatures. Secondly, even if the optimization focusing on spatial-spectral\ndomain learning (SDL) converges to the ideal solution, there is still a\nsignificant visual difference between the reconstructed HSI and the truth.\nTherefore, we propose a high-resolution dual-domain learning network (HDNet)\nfor HSI reconstruction. On the one hand, the proposed HR spatial-spectral\nattention module with its efficient feature fusion provides continuous and fine\npixel-level features. On the other hand, frequency domain learning (FDL) is\nintroduced for HSI reconstruction to narrow the frequency domain discrepancy.\nDynamic FDL supervision forces the model to reconstruct fine-grained\nfrequencies and compensate for excessive smoothing and distortion caused by\npixel-level losses. The HR pixel-level attention and frequency-level refinement\nin our HDNet mutually promote HSI perceptual quality. Extensive quantitative\nand qualitative evaluation experiments show that our method achieves SOTA\nperformance on simulated and real HSI datasets. Code and models will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchMVSNet: Patch-wise Unsupervised Multi-View Stereo for Weakly-Textured Surface Reconstruction. (arXiv:2203.02156v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02156","description":"<p>Learning-based multi-view stereo (MVS) has gained fine reconstructions on\npopular datasets. However, supervised learning methods require ground truth for\ntraining, which is hard to be collected, especially for the large-scale\ndatasets. Though nowadays unsupervised learning methods have been proposed and\nhave gotten gratifying results, those methods still fail to reconstruct intact\nresults in challenging scenes, such as weakly-textured surfaces, as those\nmethods primarily depend on pixel-wise photometric consistency which is\nsubjected to various illuminations. To alleviate matching ambiguity in those\nchallenging scenes, this paper proposes robust loss functions leveraging\nconstraints beneath multi-view images: 1) Patch-wise photometric consistency\nloss, which expands the receptive field of the features in multi-view\nsimilarity measuring, 2) Robust twoview geometric consistency, which includes a\ncross-view depth consistency checking with the minimum occlusion. Our\nunsupervised strategy can be implemented with arbitrary depth estimation\nframeworks and can be trained with arbitrary large-scale MVS datasets.\nExperiments show that our method can decrease the matching ambiguity and\nparticularly improve the completeness of weakly-textured reconstruction.\nMoreover, our method reaches the performance of the state-of-the-art methods on\npopular benchmarks, like DTU, Tanks and Temples and ETH3D. The code will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haonan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jian Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetFlowTrack: 3D Multi-object Tracking based on Simultaneous Optimization of Object Detection and Scene Flow Estimation. (arXiv:2203.02157v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02157","description":"<p>3D Multi-Object Tracking (MOT) is an important part of the unmanned vehicle\nperception module. Most methods optimize object detection and data association\nindependently. These methods make the network structure complicated and limit\nthe improvement of MOT accuracy. we proposed a 3D MOT framework based on\nsimultaneous optimization of object detection and scene flow estimation. In the\nframework, a detection-guidance scene flow module is proposed to relieve the\nproblem of incorrect inter-frame assocation. For more accurate scene flow label\nespecially in the case of motion with rotation, a box-transformation-based\nscene flow ground truth calculation method is proposed. Experimental results on\nthe KITTI MOT dataset show competitive results over the state-of-the-arts and\nthe robustness under extreme motion with rotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yueling Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformations in Learned Image Compression from a Communication Perspective. (arXiv:2203.02158v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02158","description":"<p>In this paper, a unified transformation method in learned image\ncompression(LIC) is proposed from the perspective of communication. Firstly,\nthe quantization in LIC is considered as a generalized channel with additive\nuniform noise. Moreover, the LIC is interpreted as a particular communication\nsystem according to the consistency in structures and optimization objectives.\nThus, the technology of communication systems can be applied to guide the\ndesign of modules in LIC. Furthermore, a unified transform method based on\nsignal modulation (TSM) is defined. In the view of TSM, the existing\ntransformation methods are mathematically reduced to a linear modulation. A\nseries of transformation methods, e.g. TPM and TJM, are obtained by extending\nto nonlinear modulation. The experimental results on various datasets and\nbackbone architectures verify that the effectiveness and robustness of the\nproposed method. More importantly, it further confirms the feasibility of\nguiding LIC design from a communication perspective. For example, when backbone\narchitecture is hyperprior combining context model, our method achieves\n3.52$\\%$ BD-rate reduction over GDN on Kodak dataset without increasing\ncomplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Youneng Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1\">Fangyang Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Wen Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yongsheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MF-Hovernet: An Extension of Hovernet for Colon Nuclei Identification and Counting (CoNiC) Challenge. (arXiv:2203.02161v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02161","description":"<p>Nuclei Identification and Counting is the most important morphological\nfeature of cancers, especially in the colon. Many deep learning-based methods\nhave been proposed to deal with this problem. In this work, we construct an\nextension of Hovernet for nuclei identification and counting to address the\nproblem named MF-Hovernet. Our proposed model is the combination of multiple\nfiler block to Hovernet architecture. The current result shows the efficiency\nof multiple filter block to improve the performance of the original Hovernet\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vo_V/0/1/0/all/0/1\">Vi Thi-Tuong Vo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Hyung Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Taebum Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Analysis Operator Learning by End-To-End Training of Iterative Neural Networks. (arXiv:2203.02166v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02166","description":"<p>The concept of sparsity has been extensively applied for regularization in\nimage reconstruction. Typically, sparsifying transforms are either pre-trained\non ground-truth images or adaptively trained during the reconstruction.\nThereby, learning algorithms are designed to minimize some target function\nwhich encodes the desired properties of the transform. However, this procedure\nignores the subsequently employed reconstruction algorithm as well as the\nphysical model which is responsible for the image formation process. Iterative\nneural networks - which contain the physical model - can overcome these issues.\nIn this work, we demonstrate how convolutional sparsifying filters can be\nefficiently learned by end-to-end training of iterative neural networks. We\nevaluated our approach on a non-Cartesian 2D cardiac cine MRI example and show\nthat the obtained filters are better suitable for the corresponding\nreconstruction algorithm than the ones obtained by decoupled pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kofler_A/0/1/0/all/0/1\">Andreas Kofler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wald_C/0/1/0/all/0/1\">Christian Wald</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaeffter_T/0/1/0/all/0/1\">Tobias Schaeffter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolbitsch_C/0/1/0/all/0/1\">Christoph Kolbitsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels. (arXiv:2203.02172v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02172","description":"<p>Training the multi-label image recognition models with partial labels, in\nwhich merely some labels are known while others are unknown for each image, is\na considerably challenging and practical task. To address this task, current\nalgorithms mainly depend on pre-training classification or similarity models to\ngenerate pseudo labels for the unknown labels. However, these algorithms depend\non sufficient multi-label annotations to train the models, leading to poor\nperformance especially with low known label proportion. In this work, we\npropose to blend category-specific representation across different images to\ntransfer information of known labels to complement unknown labels, which can\nget rid of pre-training models and thus does not depend on sufficient\nannotations. To this end, we design a unified semantic-aware representation\nblending (SARB) framework that exploits instance-level and prototype-level\nsemantic representation to complement unknown labels by two complementary\nmodules: 1) an instance-level representation blending (ILRB) module blends the\nrepresentations of the known labels in an image to the representations of the\nunknown labels in another image to complement these unknown labels. 2) a\nprototype-level representation blending (PLRB) module learns more stable\nrepresentation prototypes for each category and blends the representation of\nunknown labels with the prototypes of corresponding labels to complement these\nlabels. Extensive experiments on the MS-COCO, Visual Genome, Pascal VOC 2007\ndatasets show that the proposed SARB framework obtains superior performance\nover current leading competitors on all known label proportion settings, i.e.,\nwith the mAP improvement of 4.6%, 4.%, 2.2% on these three datasets when the\nknown label proportion is 10%. Codes are available at\nhttps://github.com/HCPLab-SYSU/HCP-MLR-PL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1\">Tao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hefeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-to-Label: Temporal Consistency for Self-Supervised Monocular 3D Object Detection. (arXiv:2203.02193v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02193","description":"<p>Monocular 3D object detection continues to attract attention due to the cost\nbenefits and wider availability of RGB cameras. Despite the recent advances and\nthe ability to acquire data at scale, annotation cost and complexity still\nlimit the size of 3D object detection datasets in the supervised settings.\nSelf-supervised methods, on the other hand, aim at training deep networks\nrelying on pretext tasks or various consistency constraints. Moreover, other 3D\nperception tasks (such as depth estimation) have shown the benefits of temporal\npriors as a self-supervision signal. In this work, we argue that the temporal\nconsistency on the level of object poses, provides an important supervision\nsignal given the strong prior on physical motion. Specifically, we propose a\nself-supervised loss which uses this consistency, in addition to\nrender-and-compare losses, to refine noisy pose predictions and derive\nhigh-quality pseudo labels. To assess the effectiveness of the proposed method,\nwe finetune a synthetically trained monocular 3D object detection model using\nthe pseudo-labels that we generated on real data. Evaluation on the standard\nKITTI3D benchmark demonstrates that our method reaches competitive performance\ncompared to other monocular self-supervised and supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mouawad_I/0/1/0/all/0/1\">Issa Mouawad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brasch_N/0/1/0/all/0/1\">Nikolas Brasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odone_F/0/1/0/all/0/1\">Francesca Odone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02194","description":"<p>In some scenarios, classifier requires detecting out-of-distribution samples\nfar from its training data. With desirable characteristics, reconstruction\nautoencoder-based methods deal with this problem by using input reconstruction\nerror as a metric of novelty vs. normality. We formulate the essence of such\napproach as a quadruplet domain translation with an intrinsic bias to only\nquery for a proxy of conditional data uncertainty. Accordingly, an improvement\ndirection is formalized as maximumly compressing the autoencoder's latent space\nwhile ensuring its reconstructive power for acting as a described domain\ntranslator. From it, strategies are introduced including semantic\nreconstruction, data certainty decomposition and normalized L2 distance to\nsubstantially improve original methods, which together establish\nstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of\nCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method\nworks without any additional data, hard-to-implement structure, time-consuming\npipeline, and even harming the classification accuracy of known classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yibo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice-Face Homogeneity Tells Deepfake. (arXiv:2203.02195v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02195","description":"<p>Detecting forgery videos is highly desired due to the abuse of deepfake.\nExisting detection approaches contribute to exploring the specific artifacts in\ndeepfake videos and fit well on certain data. However, the growing technique on\nthese artifacts keeps challenging the robustness of traditional deepfake\ndetectors. As a result, the development of generalizability of these approaches\nhas reached a blockage. To address this issue, given the empirical results that\nthe identities behind voices and faces are often mismatched in deepfake videos,\nand the voices and faces have homogeneity to some extent, in this paper, we\npropose to perform the deepfake detection from an unexplored voice-face\nmatching view. To this end, a voice-face matching detection model is devised to\nmeasure the matching degree of these two on a generic audio-visual dataset.\nThereafter, this model can be smoothly transferred to deepfake datasets without\nany fine-tuning, and the generalization across datasets is accordingly\nenhanced. We conduct extensive experiments over two widely exploited datasets -\nDFDC and FakeAVCeleb. Our model obtains significantly improved performance as\ncompared to other state-of-the-art competitors and maintains favorable\ngeneralizability. The code has been released at\nhttps://github.com/xaCheng1996/VFD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Harry Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Carbon Footprint of Selecting and Training Deep Learning Models for Medical Image Analysis. (arXiv:2203.02202v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02202","description":"<p>The increasing energy consumption and carbon footprint of deep learning (DL)\ndue to growing compute requirements has become a cause of concern. In this\nwork, we focus on the carbon footprint of developing DL models for medical\nimage analysis (MIA), where volumetric images of high spatial resolution are\nhandled. In this study, we present and compare the features of four tools from\nliterature to quantify the carbon footprint of DL. Using one of these tools we\nestimate the carbon footprint of medical image segmentation pipelines. We\nchoose nnU-net as the proxy for a medical image segmentation pipeline and\nexperiment on three common datasets. With our work we hope to inform on the\nincreasing energy costs incurred by MIA. We discuss simple strategies to\ncut-down the environmental impact that can make model selection and training\nprocesses more efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhagwat_N/0/1/0/all/0/1\">Nikhil Bhagwat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anthony_L/0/1/0/all/0/1\">Lasse F. Wolff Anthony</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanding_B/0/1/0/all/0/1\">Benjamin Kanding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dam_E/0/1/0/all/0/1\">Erik B. Dam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety-aware metrics for object detectors in autonomous driving. (arXiv:2203.02205v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02205","description":"<p>We argue that object detectors in the safety critical domain should\nprioritize detection of objects that are most likely to interfere with the\nactions of the autonomous actor. Especially, this applies to objects that can\nimpact the actor's safety and reliability. In the context of autonomous\ndriving, we propose new object detection metrics that reward the correct\nidentification of objects that are most likely to interact with the subject\nvehicle (i.e., the actor), and that may affect its driving decision. To achieve\nthis, we build a criticality model to reward the detection of the objects based\non proximity, orientation, and relative velocity with respect to the subject\nvehicle. Then, we apply our model on the recent autonomous driving dataset\nnuScenes, and we compare eight different object detectors. Results show that,\nin several settings, object detectors that perform best according to the\nnuScenes ranking are not the preferable ones when the focus is shifted on\nsafety and reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1\">Andrea Ceccarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montecchi_L/0/1/0/all/0/1\">Leonardo Montecchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration. (arXiv:2203.02227v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02227","description":"<p>Given two point sets, the problem of registration is to recover a\ntransformation that matches one set to the other. This task is challenging due\nto the presence of the large number of outliers, the unknown non-rigid\ndeformations and the large sizes of point sets. To obtain strong robustness\nagainst outliers, we formulate the registration problem as a partial\ndistribution matching (PDM) problem, where the goal is to partially match the\ndistributions represented by point sets in a metric space. To handle large\npoint sets, we propose a scalable PDM algorithm by utilizing the efficient\npartial Wasserstein-1 (PW) discrepancy. Specifically, we derive the\nKantorovich-Rubinstein duality for the PW discrepancy, and show its gradient\ncan be explicitly computed. Based on these results, we propose a partial\nWasserstein adversarial network (PWAN), which is able to approximate the PW\ndiscrepancy by a neural network, and minimize it by gradient descent. In\naddition, it also incorporates an efficient coherence regularizer for non-rigid\ntransformations to avoid unrealistic deformations. We evaluate PWAN on\npractical point set registration tasks, and show that the proposed PWAN is\nrobust, scalable and performs more favorably than the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zi-Ming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Ling Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPAL: Occlusion Pattern Aware Loss for Unsupervised Light Field Disparity Estimation. (arXiv:2203.02231v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02231","description":"<p>Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiayin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingyao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting GAN-generated Images by Orthogonal Training of Multiple CNNs. (arXiv:2203.02246v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02246","description":"<p>In the last few years, we have witnessed the rise of a series of deep\nlearning methods to generate synthetic images that look extremely realistic.\nThese techniques prove useful in the movie industry and for artistic purposes.\nHowever, they also prove dangerous if used to spread fake news or to generate\nfake online accounts. For this reason, detecting if an image is an actual\nphotograph or has been synthetically generated is becoming an urgent necessity.\nThis paper proposes a detector of synthetic images based on an ensemble of\nConvolutional Neural Networks (CNNs). We consider the problem of detecting\nimages generated with techniques not available at training time. This is a\ncommon scenario, given that new image generators are published more and more\nfrequently. To solve this issue, we leverage two main ideas: (i) CNNs should\nprovide orthogonal results to better contribute to the ensemble; (ii) original\nimages are better defined than synthetic ones, thus they should be better\ntrusted at testing time. Experiments show that pursuing these two ideas\nimproves the detector accuracy on NVIDIA's newly generated StyleGAN3 images,\nnever used in training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandelli_S/0/1/0/all/0/1\">Sara Mandelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonettini_N/0/1/0/all/0/1\">Nicol&#xf2; Bonettini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch Similarity Aware Data-Free Quantization for Vision Transformers. (arXiv:2203.02250v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02250","description":"<p>Vision transformers have recently gained great success on various computer\nvision tasks; nevertheless, their high model complexity makes it challenging to\ndeploy on resource-constrained devices. Quantization is an effective approach\nto reduce model complexity, and data-free quantization, which can address data\nprivacy and security concerns during model deployment, has received widespread\ninterest. Unfortunately, all existing methods, such as BN regularization, were\ndesigned for convolutional neural networks and cannot be applied to vision\ntransformers with significantly different model architectures. In this paper,\nwe propose PSAQ-ViT, a Patch Similarity Aware data-free Quantization framework\nfor Vision Transformers, to enable the generation of \"realistic\" samples based\non the vision transformer's unique properties for calibrating the quantization\nparameters. Specifically, we analyze the self-attention module's properties and\nreveal a general difference (patch similarity) in its processing of Gaussian\nnoise and real images. The above insights guide us to design a relative value\nmetric to optimize the Gaussian noise to approximate the real images, which are\nthen utilized to calibrate the quantization parameters. Extensive experiments\nand ablation studies are conducted on various benchmarks to validate the\neffectiveness of PSAQ-ViT, which can even outperform the real-data-driven\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liping Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengjuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junrui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qingyi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Aware Contrastive Semi-Supervised Learning. (arXiv:2203.02261v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02261","description":"<p>Pseudo-label-based semi-supervised learning (SSL) has achieved great success\non raw data utilization. However, its training procedure suffers from\nconfirmation bias due to the noise contained in self-generated artificial\nlabels. Moreover, the model's judgment becomes noisier in real-world\napplications with extensive out-of-distribution data. To address this issue, we\npropose a general method named Class-aware Contrastive Semi-Supervised Learning\n(CCSSL), which is a drop-in helper to improve the pseudo-label quality and\nenhance the model's robustness in the real-world setting. Rather than treating\nreal-world data as a union set, our method separately handles reliable\nin-distribution data with class-wise clustering for blending into downstream\ntasks and noisy out-of-distribution data with image-wise contrastive for better\ngeneralization. Furthermore, by applying target re-weighting, we successfully\nemphasize clean label learning and simultaneously reduce noisy label learning.\nDespite its simplicity, our proposed CCSSL has significant performance\nimprovements over the state-of-the-art SSL methods on the standard datasets\nCIFAR100 and STL10. On the real-world dataset Semi-iNat 2021, we improve\nFixMatch by 9.80% and CoMatch by 3.18%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guannan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1\">Long Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Explanations Explain? Model Knows Best. (arXiv:2203.02269v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02269","description":"<p>It is a mystery which input features contribute to a neural network's output.\nVarious explanation (feature attribution) methods are proposed in the\nliterature to shed light on the problem. One peculiar observation is that these\nexplanations (attributions) point to different features as being important. The\nphenomenon raises the question, which explanation to trust? We propose a\nframework for evaluating the explanations using the neural network model\nitself. The framework leverages the network to generate input features that\nimpose a particular behavior on the output. Using the generated features, we\ndevise controlled experimental setups to evaluate whether an explanation method\nconforms to an axiom. Thus we propose an empirical framework for axiomatic\nevaluation of explanation methods. We evaluate well-known and promising\nexplanation solutions using the proposed framework. The framework provides a\ntoolset to reveal properties and drawbacks within existing and future\nexplanation solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorsandi_P/0/1/0/all/0/1\">Pedram Khorsandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nobahari_R/0/1/0/all/0/1\">Rozhin Nobahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Transformation for Cross-domain Few-shot Remote Sensing Scene Classification. (arXiv:2203.02270v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02270","description":"<p>Effectively classifying remote sensing scenes is still a challenge due to the\nincreasing spatial resolution of remote imaging and large variances between\nremote sensing images. Existing research has greatly improved the performance\nof remote sensing scene classification (RSSC). However, these methods are not\napplicable to cross-domain few-shot problems where target domain is with very\nlimited training samples available and has a different data distribution from\nsource domain. To improve the model's applicability, we propose the\nfeature-wise transformation module (FTM) in this paper. FTM transfers the\nfeature distribution learned on source domain to that of target domain by a\nvery simple affine operation with negligible additional parameters. Moreover,\nFTM can be effectively learned on target domain in the case of few training\ndata available and is agnostic to specific network structures. Experiments on\nRSSC and land-cover mapping tasks verified its capability to handle\ncross-domain few-shot problems. By comparison with directly finetuning, FTM\nachieves better performance and possesses better transferability and\nfine-grained discriminability. \\textit{Code will be publicly available.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiaoling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wei Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends and Research Directions. (arXiv:2203.02281v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02281","description":"<p>Recent developments in video analysis of sports and computer vision\ntechniques have achieved significant improvements to enable a variety of\ncritical operations. To provide enhanced information, such as detailed complex\nanalysis in sports like soccer, basketball, cricket, badminton, etc., studies\nhave focused mainly on computer vision techniques employed to carry out\ndifferent tasks. This paper presents a comprehensive review of sports video\nanalysis for various applications high-level analysis such as detection and\nclassification of players, tracking player or ball in sports and predicting the\ntrajectories of player or ball, recognizing the teams strategies, classifying\nvarious events in sports. The paper further discusses published works in a\nvariety of application-specific tasks related to sports and the present\nresearchers views regarding them. Since there is a wide research scope in\nsports for deploying computer vision techniques in various sports, some of the\npublicly available datasets related to a particular sport have been provided.\nThis work reviews a detailed discussion on some of the artificial\nintelligence(AI)applications in sports vision, GPU-based work stations, and\nembedded platforms. Finally, this review identifies the research directions,\nprobable challenges, and future trends in the area of visual recognition in\nsports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_B/0/1/0/all/0/1\">Banoth Thulasya Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashmi_M/0/1/0/all/0/1\">Mohammad Farukh Hashmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bokde_N/0/1/0/all/0/1\">Neeraj Dhanraj Bokde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaseen_Z/0/1/0/all/0/1\">Zaher Mundher Yaseen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nuclei segmentation and classification in histopathology images with StarDist for the CoNIC Challenge 2022. (arXiv:2203.02284v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02284","description":"<p>Segmentation and classification of nuclei in histopathology images is an\nimportant task in computational pathology. Here we describe how we used\nStarDist, a deep learning based approach based on star-convex shape\nrepresentations, for the Colon Nuclei Identification and Counting (CoNIC)\nchallenge 2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weigert_M/0/1/0/all/0/1\">Martin Weigert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_U/0/1/0/all/0/1\">Uwe Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-parametric Makeup Transfer via Semantic-aware Correspondence. (arXiv:2203.02286v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02286","description":"<p>The large discrepancy between the source non-makeup image and the reference\nmakeup image is one of the key challenges in makeup transfer. Conventional\napproaches for makeup transfer either learn disentangled representation or\nperform pixel-wise correspondence in a parametric way between two images. We\nargue that non-parametric techniques have a high potential for addressing the\npose, expression, and occlusion discrepancies. To this end, this paper proposes\na \\textbf{S}emi-\\textbf{p}arametric \\textbf{M}akeup \\textbf{T}ransfer (SpMT)\nmethod, which combines the reciprocal strengths of non-parametric and\nparametric mechanisms. The non-parametric component is a novel\n\\textbf{S}emantic-\\textbf{a}ware \\textbf{C}orrespondence (SaC) module that\nexplicitly reconstructs content representation with makeup representation under\nthe strong constraint of component semantics. The reconstructed representation\nis desired to preserve the spatial and identity information of the source image\nwhile \"wearing\" the makeup of the reference image. The output image is\nsynthesized via a parametric decoder that draws on the reconstructed\nrepresentation. Extensive experiments demonstrate the superiority of our method\nin terms of visual quality, robustness, and flexibility. Code and pre-trained\nmodel are available at \\url{https://github.com/AnonymScholar/SpMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingrui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1\">Yun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Freeform Body Motion Generation from Speech. (arXiv:2203.02291v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02291","description":"<p>People naturally conduct spontaneous body motions to enhance their speeches\nwhile giving talks. Body motion generation from speech is inherently difficult\ndue to the non-deterministic mapping from speech to body motions. Most existing\nworks map speech to motion in a deterministic way by conditioning on certain\nstyles, leading to sub-optimal results. Motivated by studies in linguistics, we\ndecompose the co-speech motion into two complementary parts: pose modes and\nrhythmic dynamics. Accordingly, we introduce a novel freeform motion generation\nmodel (FreeMo) by equipping a two-stream architecture, i.e., a pose mode branch\nfor primary posture generation, and a rhythmic motion branch for rhythmic\ndynamics synthesis. On one hand, diverse pose modes are generated by\nconditional sampling in a latent space, guided by speech semantics. On the\nother hand, rhythmic dynamics are synced with the speech prosody. Extensive\nexperiments demonstrate the superior performance against several baselines, in\nterms of motion diversity, quality and syncing with speech. Code and\npre-trained models will be publicly available through\nhttps://github.com/TheTempAccount/Co-Speech-Motion-Generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yalong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qibin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Reality Depth Contour Occlusion Using Binocular Similarity Matching and Three-dimensional Contour Optimisation. (arXiv:2203.02300v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02300","description":"<p>Mixed reality applications often require virtual objects that are partly\noccluded by real objects. However, previous research and commercial products\nhave limitations in terms of performance and efficiency. To address these\nchallenges, we propose a novel depth contour occlusion (DCO) algorithm. The\nproposed method is based on the sensitivity of contour occlusion and a\nbinocular stereoscopic vision device. In this method, a depth contour map is\ncombined with a sparse depth map obtained from a two-stage adaptive filter area\nstereo matching algorithm and the depth contour information of the objects\nextracted by a digital image stabilisation optical flow method. We also propose\na quadratic optimisation model with three constraints to generate an accurate\ndense map of the depth contour for high-quality real-virtual occlusion. The\nwhole process is accelerated by GPU. To evaluate the effectiveness of the\nalgorithm, we demonstrate a time con-sumption statistical analysis for each\nstage of the DCO algorithm execution. To verify the relia-bility of the\nreal-virtual occlusion effect, we conduct an experimental analysis on\nsingle-sided, enclosed, and complex occlusions; subsequently, we compare it\nwith the occlusion method without quadratic optimisation. With our GPU\nimplementation for real-time DCO, the evaluation indicates that applying the\npresented DCO algorithm can enhance the real-time performance and the visual\nquality of real-virtual occlusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_N/0/1/0/all/0/1\">Naye Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Youbing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dingguo Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Levenberg--Marquardt Algorithm for optimization in Bundle Adjustment. (arXiv:2203.02311v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02311","description":"<p>In this paper we develop a quantum optimization algorithm and use it to solve\nthe bundle adjustment problem with a simulated quantum computer. Bundle\nadjustment is the process of optimizing camera poses and sensor properties to\nbest reconstruct the three-dimensional structure and viewing parameters. This\nproblem is often solved using some implementation of the Levenberg--Marquardt\nalgorithm. In this case we implement a quantum algorithm for solving the linear\nsystem of normal equations that calculates the optimization step in\nLevenberg--Marquardt. This procedure is the current bottleneck in the\nalgorithmic complexity of bundle adjustment. The proposed quantum algorithm\ndramatically reduces the complexity of this operation with respect to the\nnumber of points.\n</p>\n<p>We investigate 9 configurations of a toy-model for bundle adjustment, limited\nto 10 points and 2 cameras. This optimization problem is solved both by using\nthe sparse Levenberg-Marquardt algorithm and our quantum implementation. The\nresulting solutions are presented, showing an improved rate of convergence,\ntogether with an analysis of the theoretical speed up and the probability of\nrunning the algorithm successfully on a current quantum computer.\n</p>\n<p>The presented quantum algorithm is a seminal implementation of using quantum\ncomputing algorithms in order to solve complex optimization problems in\ncomputer vision, in particular bundle adjustment, which offers several avenues\nof further investigations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bernecker_L/0/1/0/all/0/1\">Luca Bernecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idini_A/0/1/0/all/0/1\">Andrea Idini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F2DNet: Fast Focal Detection Network for Pedestrian Detection. (arXiv:2203.02331v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02331","description":"<p>Two-stage detectors are state-of-the-art in object detection as well as\npedestrian detection. However, the current two-stage detectors are inefficient\nas they do bounding box regression in multiple steps i.e. in region proposal\nnetworks and bounding box heads. Also, the anchor-based region proposal\nnetworks are computationally expensive to train. We propose F2DNet, a novel\ntwo-stage detection architecture which eliminates redundancy of current\ntwo-stage detectors by replacing the region proposal network with our focal\ndetection network and bounding box head with our fast suppression head. We\nbenchmark F2DNet on top pedestrian detection datasets, thoroughly compare it\nagainst the existing state-of-the-art detectors and conduct cross dataset\nevaluation to test the generalizability of our model to unseen data. Our F2DNet\nachieves 8.7%, 2.2%, and 6.1% MR-2 on City Persons, Caltech Pedestrian, and\nEuro City Person datasets respectively when trained on a single dataset and\nreaches 20.4% and 26.2% MR-2 in heavy occlusion setting of Caltech Pedestrian\nand City Persons datasets when using progressive fine-tunning. On top of that\nF2DNet have significantly lesser inference time compared to the current\nstate-of-the-art. Code and trained models will be available at\nhttps://github.com/AbdulHannanKhan/F2DNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Hannan Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munir_M/0/1/0/all/0/1\">Mohsin Munir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elst_L/0/1/0/all/0/1\">Ludger van Elst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Estimation for Heatmap-based Landmark Localization. (arXiv:2203.02351v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02351","description":"<p>Automatic anatomical landmark localization has made great strides by\nleveraging deep learning methods in recent years. The ability to quantify the\nuncertainty of these predictions is a vital ingredient needed to see these\nmethods adopted in clinical use, where it is imperative that erroneous\npredictions are caught and corrected. We propose Quantile Binning, a\ndata-driven method to categorise predictions by uncertainty with estimated\nerror bounds. This framework can be applied to any continuous uncertainty\nmeasure, allowing straightforward identification of the best subset of\npredictions with accompanying estimated error bounds. We facilitate easy\ncomparison between uncertainty measures by constructing two evaluation metrics\nderived from Quantile Binning. We demonstrate this framework by comparing and\ncontrasting three uncertainty measures (a baseline, the current gold standard,\nand a proposed method combining aspects of the two), across two datasets (one\neasy, one hard) and two heatmap-based landmark localization model paradigms\n(U-Net and patch-based). We conclude by illustrating how filtering out gross\nmispredictions caught in our Quantile Bins significantly improves the\nproportion of predictions under an acceptable error threshold, and offer\nrecommendations on which uncertainty measure to use and how to use it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1\">Lawrence Schobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swift_A/0/1/0/all/0/1\">Andrew J. Swift</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer-Aided Road Inspection: Systems and Algorithms. (arXiv:2203.02355v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02355","description":"<p>Road damage is an inconvenience and a safety hazard, severely affecting\nvehicle condition, driving comfort, and traffic safety. The traditional manual\nvisual road inspection process is pricey, dangerous, exhausting, and\ncumbersome. Also, manual road inspection results are qualitative and\nsubjective, as they depend entirely on the inspector's personal experience.\nTherefore, there is an ever-increasing need for automated road inspection\nsystems. This chapter first compares the five most common road damage types.\nThen, 2-D/3-D road imaging systems are discussed. Finally, state-of-the-art\nmachine vision and intelligence-based road damage detection algorithms are\nintroduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sicen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bocus_M/0/1/0/all/0/1\">Mohammud Junaid Bocus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViT-P: Rethinking Data-efficient Vision Transformers from Locality. (arXiv:2203.02358v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02358","description":"<p>Recent advances of Transformers have brought new trust to computer vision\ntasks. However, on small dataset, Transformers is hard to train and has lower\nperformance than convolutional neural networks. We make vision transformers as\ndata-efficient as convolutional neural networks by introducing multi-focal\nattention bias. Inspired by the attention distance in a well-trained ViT, we\nconstrain the self-attention of ViT to have multi-scale localized receptive\nfield. The size of receptive field is adaptable during training so that optimal\nconfiguration can be learned. We provide empirical evidence that proper\nconstrain of receptive field can reduce the amount of training data for vision\ntransformers. On Cifar100, our ViT-P Base model achieves the state-of-the-art\naccuracy (83.16%) trained from scratch. We also perform analysis on ImageNet to\nshow our method does not lose accuracy on large data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_D/0/1/0/all/0/1\">Di Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xin Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiT: Self-supervised Pre-training for Document Image Transformer. (arXiv:2203.02378v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02378","description":"<p>Image Transformer has recently achieved significant progress for natural\nimage understanding, either using supervised (ViT, DeiT, etc.) or\nself-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we\npropose DiT, a self-supervised pre-trained Document Image Transformer model\nusing large-scale unlabeled text images for Document AI tasks, which is\nessential since no supervised counterparts ever exist due to the lack of human\nlabeled document images. We leverage DiT as the backbone network in a variety\nof vision-based Document AI tasks, including document image classification,\ndocument layout analysis, as well as table detection. Experiment results have\nillustrated that the self-supervised pre-trained DiT model achieves new\nstate-of-the-art results on these downstream tasks, e.g. document image\nclassification (91.11 $\\rightarrow$ 92.69), document layout analysis (91.0\n$\\rightarrow$ 94.9) and table detection (94.23 $\\rightarrow$ 96.55). The code\nand pre-trained models are publicly available at \\url{https://aka.ms/msdit}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoMO-Mixer: An automated multi-objective Mixer model for balanced, safe and robust prediction in medicine. (arXiv:2203.02384v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02384","description":"<p>Accurately identifying patient's status through medical images plays an\nimportant role in diagnosis and treatment. Artificial intelligence (AI),\nespecially the deep learning, has achieved great success in many fields.\nHowever, more reliable AI model is needed in image guided diagnosis and\ntherapy. To achieve this goal, developing a balanced, safe and robust model\nwith a unified framework is desirable. In this study, a new unified model\ntermed as automated multi-objective Mixer (AutoMO-Mixer) model was developed,\nwhich utilized a recent developed multiple layer perceptron Mixer (MLP-Mixer)\nas base. To build a balanced model, sensitivity and specificity were considered\nas the objective functions simultaneously in training stage. Meanwhile, a new\nevidential reasoning based on entropy was developed to achieve a safe and\nrobust model in testing stage. The experiment on an optical coherence\ntomography dataset demonstrated that AutoMO-Mixer can obtain safer, more\nbalanced, and robust results compared with MLP-Mixer and other available\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jiahuan Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1\">Dehua Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mou_X/0/1/0/all/0/1\">Xuanqin Mou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1\">Ling Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiguo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Alignment and Surface Regression Using Hybrid 2D-3D Networks for 3D Coherent Layer Segmentation of Retina OCT Images. (arXiv:2203.02390v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02390","description":"<p>Automated surface segmentation of retinal layer is important and challenging\nin analyzing optical coherence tomography (OCT). Recently, many deep learning\nbased methods have been developed for this task and yield remarkable\nperformance. However, due to large spatial gap and potential mismatch between\nthe B-scans of OCT data, all of them are based on 2D segmentation of individual\nB-scans, which may loss the continuity information across the B-scans. In\naddition, 3D surface of the retina layers can provide more diagnostic\ninformation, which is crucial in quantitative image analysis. In this study, a\nnovel framework based on hybrid 2D-3D convolutional neural networks (CNNs) is\nproposed to obtain continuous 3D retinal layer surfaces from OCT. The 2D\nfeatures of individual B-scans are extracted by an encoder consisting of 2D\nconvolutions. These 2D features are then used to produce the alignment\ndisplacement field and layer segmentation by two 3D decoders, which are coupled\nvia a spatial transformer module. The entire framework is trained end-to-end.\nTo the best of our knowledge, this is the first study that attempts 3D retinal\nlayer segmentation in volumetric OCT images based on CNNs. Experiments on a\npublicly available dataset show that our framework achieves superior results to\nstate-of-the-art 2D methods in terms of both layer segmentation accuracy and\ncross-B-scan 3D continuity, thus offering more clinical values than previous\nworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mobile authentication of copy detection patterns. (arXiv:2203.02397v1 [cs.CR])","link":"http://arxiv.org/abs/2203.02397","description":"<p>In the recent years, the copy detection patterns (CDP) attracted a lot of\nattention as a link between the physical and digital worlds, which is of great\ninterest for the internet of things and brand protection applications. However,\nthe security of CDP in terms of their reproducibility by unauthorized parties\nor clonability remains largely unexplored. In this respect this paper addresses\na problem of anti-counterfeiting of physical objects and aims at investigating\nthe authentication aspects and the resistances to illegal copying of the modern\nCDP from machine learning perspectives. A special attention is paid to a\nreliable authentication under the real life verification conditions when the\ncodes are printed on an industrial printer and enrolled via modern mobile\nphones under regular light conditions. The theoretical and empirical\ninvestigation of authentication aspects of CDP is performed with respect to\nfour types of copy fakes from the point of view of (i) multi-class supervised\nclassification as a baseline approach and (ii) one-class classification as a\nreal-life application case. The obtained results show that the modern\nmachine-learning approaches and the technical capacities of modern mobile\nphones allow to reliably authenticate CDP on end-user mobile phones under the\nconsidered classes of fakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taran_O/0/1/0/all/0/1\">Olga Taran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutt_J/0/1/0/all/0/1\">Joakim Tutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holotyak_T/0/1/0/all/0/1\">Taras Holotyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaban_R/0/1/0/all/0/1\">Roman Chaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonev_S/0/1/0/all/0/1\">Slavi Bonev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1\">Slava Voloshynovskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Control Barrier Functions for Vision-based End-to-End Autonomous Driving. (arXiv:2203.02401v1 [cs.RO])","link":"http://arxiv.org/abs/2203.02401","description":"<p>Guaranteeing safety of perception-based learning systems is challenging due\nto the absence of ground-truth state information unlike in state-aware control\nscenarios. In this paper, we introduce a safety guaranteed learning framework\nfor vision-based end-to-end autonomous driving. To this end, we design a\nlearning system equipped with differentiable control barrier functions (dCBFs)\nthat is trained end-to-end by gradient descent. Our models are composed of\nconventional neural network architectures and dCBFs. They are interpretable at\nscale, achieve great test performance under limited training data, and are\nsafety guaranteed in a series of autonomous driving scenarios such as lane\nkeeping and obstacle avoidance. We evaluated our framework in a sim-to-real\nenvironment, and tested on a real autonomous car, achieving safe lane following\nand obstacle avoidance via Augmented Reality (AR) and real parked vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tsun-Hsuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chahine_M/0/1/0/all/0/1\">Makram Chahine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Renal Structures with 3D Block Aggregate Transformers. (arXiv:2203.02430v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02430","description":"<p>Efficiently quantifying renal structures can provide distinct spatial context\nand facilitate biomarker discovery for kidney morphology. However, the\ndevelopment and evaluation of the transformer model to segment the renal\ncortex, medulla, and collecting system remains challenging due to data\ninefficiency. Inspired by the hierarchical structures in vision transformer, we\npropose a novel method using a 3D block aggregation transformer for segmenting\nkidney components on contrast-enhanced CT scans. We construct the first cohort\nof renal substructures segmentation dataset with 116 subjects under\ninstitutional review board (IRB) approval. Our method yields the\nstate-of-the-art performance (Dice of 0.8467) against the baseline approach of\n0.8308 with the data-efficient design. The Pearson R achieves 0.9891 between\nthe proposed method and manual standards and indicates the strong correlation\nand reproducibility for volumetric analysis. We extend the proposed method to\nthe public KiTS dataset, the method leads to improved accuracy compared to\ntransformer-based approaches. We show that the 3D block aggregation transformer\ncan achieve local communication between sequence representations without\nmodifying self-attention, and it can serve as an accurate and efficient\nquantification tool for characterizing renal structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yinchi Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Thomas Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_S/0/1/0/all/0/1\">Shunxing Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lasko_T/0/1/0/all/0/1\">Thomas A. Lasko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abramson_R/0/1/0/all/0/1\">Richard G. Abramson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Efficient Lane Detection via Curve Modeling. (arXiv:2203.02431v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02431","description":"<p>This paper presents a novel parametric curve-based method for lane detection\nin RGB images. Unlike state-of-the-art segmentation-based and point\ndetection-based methods that typically require heuristics to either decode\npredictions or formulate a large sum of anchors, the curve-based methods can\nlearn holistic lane representations naturally. To handle the optimization\ndifficulties of existing polynomial curve methods, we propose to exploit the\nparametric B\\'ezier curve due to its ease of computation, stability, and high\nfreedom degrees of transformations. In addition, we propose the deformable\nconvolution-based feature flip fusion, for exploiting the symmetry properties\nof lanes in driving scenes. The proposed method achieves a new state-of-the-art\nperformance on the popular LLAMAS benchmark. It also achieves favorable\naccuracy on the TuSimple and CULane datasets, while retaining both low latency\n(&gt; 150 FPS) and small model size (&lt; 10M). Our method can serve as a new\nbaseline, to shed the light on the parametric curves modeling for lane\ndetection. Codes of our model and PytorchAutoDrive: a unified framework for\nself-driving perception, are available at:\nhttps://github.com/voldemortX/pytorch-auto-drive .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shaohua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SFPN: Synthetic FPN for Object Detection. (arXiv:2203.02445v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02445","description":"<p>FPN (Feature Pyramid Network) has become a basic component of most SoTA one\nstage object detectors. Many previous studies have repeatedly proved that FPN\ncan caputre better multi-scale feature maps to more precisely describe objects\nif they are with different sizes. However, for most backbones such VGG, ResNet,\nor DenseNet, the feature maps at each layer are downsized to their quarters due\nto the pooling operation or convolutions with stride 2. The gap of\ndown-scaling-by-2 is large and makes its FPN not fuse the features smoothly.\nThis paper proposes a new SFPN (Synthetic Fusion Pyramid Network) arichtecture\nwhich creates various synthetic layers between layers of the original FPN to\nenhance the accuracy of light-weight CNN backones to extract objects' visual\nfeatures more accurately. Finally, experiments prove the SFPN architecture\noutperforms either the large backbone VGG16, ResNet50 or light-weight backbones\nsuch as MobilenetV2 based on AP score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu-Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chun-Chieh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kuo-Chin Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextformer: A Transformer with Spatio-Channel Attention for Context Modeling in Learned Image Compression. (arXiv:2203.02452v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02452","description":"<p>Entropy modeling is a key component for high-performance image compression\nalgorithms. Recent developments in autoregressive context modeling helped\nlearning-based methods to surpass their classical counterparts. However, the\nperformance of those models can be further improved due to the underexploited\nspatio-channel dependencies in latent space, and the suboptimal implementation\nof context adaptivity. Inspired by the adaptive characteristics of the\ntransformers, we propose a transformer-based context model, a.k.a.\nContextformer, which generalizes the de facto standard attention mechanism to\nspatio-channel attention. We replace the context model of a modern compression\nframework with the Contextformer and test it on the widely used Kodak image\ndataset. Our experimental results show that the proposed model provides up to\n10% rate savings compared to the standard Versatile Video Coding (VVC) Test\nModel (VTM) 9.1, and outperforms various learning-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Koyuncu_A/0/1/0/all/0/1\">A. Burakhan Koyuncu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_H/0/1/0/all/0/1\">Han Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Steinbach_E/0/1/0/all/0/1\">Eckehard Steinbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Hybrid Mapping of Populated Indoor Scenes using a Low-Cost Monocular UAV. (arXiv:2203.02453v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02453","description":"<p>Unmanned aerial vehicles (UAVs) have been used for many applications in\nrecent years, from urban search and rescue, to agricultural surveying, to\nautonomous underground mine exploration. However, deploying UAVs in tight,\nindoor spaces, especially close to humans, remains a challenge. One solution,\nwhen limited payload is required, is to use micro-UAVs, which pose less risk to\nhumans and typically cost less to replace after a crash. However, micro-UAVs\ncan only carry a limited sensor suite, e.g. a monocular camera instead of a\nstereo pair or LiDAR, complicating tasks like dense mapping and markerless\nmulti-person 3D human pose estimation, which are needed to operate in tight\nenvironments around people. Monocular approaches to such tasks exist, and dense\nmonocular mapping approaches have been successfully deployed for UAV\napplications. However, despite many recent works on both marker-based and\nmarkerless multi-UAV single-person motion capture, markerless single-camera\nmulti-person 3D human pose estimation remains a much earlier-stage technology,\nand we are not aware of existing attempts to deploy it in an aerial context. In\nthis paper, we present what is thus, to our knowledge, the first system to\nperform simultaneous mapping and multi-person 3D human pose estimation from a\nmonocular camera mounted on a single UAV. In particular, we show how to loosely\ncouple state-of-the-art monocular depth estimation and monocular 3D human pose\nestimation approaches to reconstruct a hybrid map of a populated indoor scene\nin real time. We validate our component-level design choices via extensive\nexperiments on the large-scale ScanNet and GTA-IM datasets. To evaluate our\nsystem-level performance, we also construct a new Oxford Hybrid Mapping dataset\nof populated indoor scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golodetz_S/0/1/0/all/0/1\">Stuart Golodetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vankadari_M/0/1/0/all/0/1\">Madhu Vankadari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everitt_A/0/1/0/all/0/1\">Aluna Everitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Sangyun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Didn't see that coming: a survey on non-verbal social human behavior forecasting. (arXiv:2203.02480v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02480","description":"<p>Non-verbal social human behavior forecasting has increasingly attracted the\ninterest of the research community in recent years. Its direct applications to\nhuman-robot interaction and socially-aware human motion generation make it a\nvery attractive field. In this survey, we define the behavior forecasting\nproblem for multiple interactive agents in a generic way that aims at unifying\nthe fields of social signals prediction and human motion forecasting,\ntraditionally separated. We hold that both problem formulations refer to the\nsame conceptual problem, and identify many shared fundamental challenges:\nfuture stochasticity, context awareness, history exploitation, etc. We also\npropose a taxonomy that comprises methods published in the last 5 years in a\nvery informative way and describes the current main concerns of the community\nwith regard to this problem. In order to promote further research on this\nfield, we also provide a summarised and friendly overview of audiovisual\ndatasets featuring non-acted social interactions. Finally, we describe the most\ncommon metrics used in this task and their particular issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barquero_G/0/1/0/all/0/1\">German Barquero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunez_J/0/1/0/all/0/1\">Johnny N&#xfa;&#xf1;ez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wei-Wei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmero_C/0/1/0/all/0/1\">Cristina Palmero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods. (arXiv:2203.02486v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02486","description":"<p>In many object recognition applications, the set of possible categories is an\nopen set, and the deployed recognition system will encounter novel objects\nbelonging to categories unseen during training. Detecting such \"novel category\"\nobjects is usually formulated as an anomaly detection problem. Anomaly\ndetection algorithms for feature-vector data identify anomalies as outliers,\nbut outlier detection has not worked well in deep learning. Instead, methods\nbased on the computed logits of visual object classifiers give state-of-the-art\nperformance. This paper proposes the Familiarity Hypothesis that these methods\nsucceed because they are detecting the absence of familiar learned features\nrather than the presence of novelty. The paper reviews evidence from the\nliterature and presents additional evidence from our own experiments that\nprovide strong support for this hypothesis. The paper concludes with a\ndiscussion of whether familiarity detection is an inevitable consequence of\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1\">Thomas G. Dietterich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyer_A/0/1/0/all/0/1\">Alexander Guyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behavioural Curves Analysis Using Near-Infrared-Iris Image Sequences. (arXiv:2203.02488v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02488","description":"<p>This paper proposes a new method to estimate behavioural curves from a stream\nof Near-Infra-Red (NIR) iris video frames. This method can be used in a Fitness\nFor Duty system (FFD). The research focuses on determining the effect of\nexternal factors such as alcohol, drugs, and sleepiness on the Central Nervous\nSystem (CNS). The aim is to analyse how this behaviour is represented on iris\nand pupil movements and if it is possible to capture these changes with a\nstandard NIR camera. The behaviour analysis showed essential differences in\npupil and iris behaviour to classify the workers in \"Fit\" or \"Unfit\"\nconditions. The best results can distinguish subjects robustly under alcohol,\ndrug consumption, and sleep conditions. The Multi-Layer-Perceptron and Gradient\nBoosted Machine reached the best results in all groups with an overall accuracy\nfor Fit and Unfit classes of 74.0% and 75.5%, respectively. These results open\na new application for iris capture devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Causa_L/0/1/0/all/0/1\">L. Causa</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">J. E. Tapia</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Droguett_E/0/1/0/all/0/1\">E. Lopez-Droguett</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Valenzuela_A/0/1/0/all/0/1\">A. Valenzuela</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Benalcazar_D/0/1/0/all/0/1\">D. Benalcazar</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">C. Busch</a> (3) ((1) TOC Biometrics, Research and Development Centre, Chile. (2) Universidad de Chile, DIMEC, Chile. (3) da/sec-Biometrics and Internet Security Research Group, Hochschule Darmstadt, Germany. (4) Department of Civil and Environmental Engineering, and Garrick Institute for the Risk Sciences, University ofCalifornia, Los Angeles, USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedestrian Stop and Go Forecasting with Hybrid Feature Fusion. (arXiv:2203.02489v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02489","description":"<p>Forecasting pedestrians' future motions is essential for autonomous driving\nsystems to safely navigate in urban areas. However, existing prediction\nalgorithms often overly rely on past observed trajectories and tend to fail\naround abrupt dynamic changes, such as when pedestrians suddenly start or stop\nwalking. We suggest that predicting these highly non-linear transitions should\nform a core component to improve the robustness of motion prediction\nalgorithms. In this paper, we introduce the new task of pedestrian stop and go\nforecasting. Considering the lack of suitable existing datasets for it, we\nrelease TRANS, a benchmark for explicitly studying the stop and go behaviors of\npedestrians in urban traffic. We build it from several existing datasets\nannotated with pedestrians' walking motions, in order to have various scenarios\nand behaviors. We also propose a novel hybrid model that leverages\npedestrian-specific and scene features from several modalities, both video\nsequences and high-level attributes, and gradually fuses them to integrate\nmultiple levels of context. We evaluate our model and several baselines on\nTRANS, and set a new benchmark for the community to work on pedestrian stop and\ngo forecasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongxu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordan_T/0/1/0/all/0/1\">Taylor Mordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02503","description":"<p>Pansharpening aims to fuse a registered high-resolution panchromatic image\n(PAN) with a low-resolution hyperspectral image (LR-HSI) to generate an\nenhanced HSI with high spectral and spatial resolution. Existing pansharpening\napproaches neglect using an attention mechanism to transfer HR texture features\nfrom PAN to LR-HSI features, resulting in spatial and spectral distortions. In\nthis paper, we present a novel attention mechanism for pansharpening called\nHyperTransformer, in which features of LR-HSI and PAN are formulated as queries\nand keys in a transformer, respectively. HyperTransformer consists of three\nmain modules, namely two separate feature extractors for PAN and HSI, a\nmulti-head feature soft attention module, and a spatial-spectral feature fusion\nmodule. Such a network improves both spatial and spectral quality measures of\nthe pansharpened HSI by learning cross-feature space dependencies and\nlong-range details of PAN and LR-HSI. Furthermore, HyperTransformer can be\nutilized across multiple spatial scales at the backbone for obtaining improved\nperformance. Extensive experiments conducted on three widely used datasets\ndemonstrate that HyperTransformer achieves significant improvement over the\nstate-of-the-art methods on both spatial and spectral quality measures.\nImplementation code and pre-trained weights can be accessed at\nhttps://github.com/wgcban/HyperTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.05938","description":"<p>RGB cameras are one of the most relevant sensors for autonomous driving\napplications. It is undeniable that failures of vehicle cameras may compromise\nthe autonomous driving task, possibly leading to unsafe behaviors when images\nthat are subsequently processed by the driving system are altered. To support\nthe definition of safe and robust vehicle architectures and intelligent\nsystems, in this paper we define the failure modes of a vehicle camera,\ntogether with an analysis of effects and known mitigations. Further, we build a\nsoftware library for the generation of the corresponding failed images and we\nfeed them to six object detectors for mono and stereo cameras and to the\nself-driving agent of an autonomous driving simulator. The resulting\nmisbehaviors with respect to operating with clean images allow a better\nunderstanding of failures effects and the related safety risks in image-based\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1\">Francesco Secci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1\">Andrea Ceccarelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does anatomical contextual information improve 3D U-Net based brain tumor segmentation?. (arXiv:2010.13460v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.13460","description":"<p>Effective, robust, and automatic tools for brain tumor segmentation are\nneeded for the extraction of information useful in treatment planning from\nmagnetic resonance (MR) images. Context-aware artificial intelligence is an\nemerging concept for the development of deep learning applications for\ncomputer-aided medical image analysis. In this work, it is investigated whether\nthe addition of contextual information from the brain anatomy in the form of\nwhite matter, gray matter, and cerebrospinal fluid masks and probability maps\nimproves U-Net-based brain tumor segmentation. The BraTS2020 dataset was used\nto train and test two standard 3D U-Net models that, in addition to the\nconventional MR image modalities, used the anatomical contextual information as\nextra channels in the form of binary masks (CIM) or probability maps (CIP). A\nbaseline model (BLM) that only used the conventional MR image modalities was\nalso trained. The impact of adding contextual information was investigated in\nterms of overall segmentation accuracy, model training time, domain\ngeneralization, and compensation for fewer MR modalities available for each\nsubject. Results show that there is no statistically significant difference\nwhen comparing Dice scores between the baseline model and the contextual\ninformation models, even when comparing performances for high- and low-grade\ntumors independently. Only in the case of compensation for fewer MR modalities\navailable for each subject did the addition of anatomical contextual\ninformation significantly improve the segmentation of the whole tumor. Overall,\nthere is no overall significant improvement in segmentation performance when\nusing anatomical contextual information in the form of either binary masks or\nprobability maps as extra channels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tampu_I/0/1/0/all/0/1\">Iulian Emil Tampu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haj_Hosseini_N/0/1/0/all/0/1\">Neda Haj-Hosseini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_A/0/1/0/all/0/1\">Anders Eklund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMI: A Mutual Information Based Framework For Unsupervised Deep Learning of Tasks. (arXiv:2101.06411v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06411","description":"<p>In this work, we propose an information theory based framework DeepMI to\ntrain deep neural networks (DNN) using Mutual Information (MI). The DeepMI\nframework is especially targeted but not limited to the learning of real world\ntasks in an unsupervised manner. The primary motivation behind this work is the\nlimitation of the traditional loss functions for unsupervised learning of a\ngiven task. Directly using MI for the training purpose is quite challenging to\ndeal with because of its unbounded above nature. Hence, we develop an\nalternative linearized representation of MI as a part of the framework.\nContributions of this paper are three fold: i) investigation of MI to train\ndeep neural networks, ii) novel loss function LLMI , and iii) a fuzzy logic\nbased end-to-end differentiable pipeline to integrate DeepMI into deep learning\nframework. Due to the unavailability of a standard benchmark, we carefully\ndesign the experimental analysis and select three different tasks for the\nexperimental study. We demonstrate that L LMI alone provides better gradients\nto achieve a neural network better performance over the popular loss functions,\nalso in the cases when multiple loss functions are used for a given task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behera_L/0/1/0/all/0/1\">Laxmidhar Behera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Inverse Problems by Joint Posterior Maximization with Autoencoding Prior. (arXiv:2103.01648v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2103.01648","description":"<p>In this work we address the problem of solving ill-posed inverse problems in\nimaging where the prior is a variational autoencoder (VAE). Specifically we\nconsider the decoupled case where the prior is trained once and can be reused\nfor many different log-concave degradation models without retraining. Whereas\nprevious MAP-based approaches to this problem lead to highly non-convex\noptimization algorithms, our approach computes the joint (space-latent) MAP\nthat naturally leads to alternate optimization algorithms and to the use of a\nstochastic encoder to accelerate computations. The resulting technique (JPMAP)\nperforms Joint Posterior Maximization using an Autoencoding Prior. We show\ntheoretical and experimental evidence that the proposed objective function is\nquite close to bi-convex. Indeed it satisfies a weak bi-convexity property\nwhich is sufficient to guarantee that our optimization scheme converges to a\nstationary point. We also highlight the importance of correctly training the\nVAE using a denoising criterion, in order to ensure that the encoder\ngeneralizes well to out-of-distribution images, without affecting the quality\nof the generative model. This simple modification is key to providing\nrobustness to the whole procedure. Finally we show how our joint MAP\nmethodology relates to more common MAP approaches, and we propose a\ncontinuation scheme that makes use of our JPMAP algorithm to provide more\nrobust MAP estimates. Experimental results also show the higher quality of the\nsolutions obtained by our JPMAP approach with respect to other non-convex MAP\napproaches which more often get stuck in spurious local optima.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mario Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tan_P/0/1/0/all/0/1\">Pauline Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransCrowd: Weakly-Supervised Crowd Counting with Transformer. (arXiv:2104.09116v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09116","description":"<p>The mainstream crowd counting methods usually utilize the convolution neural\nnetwork (CNN) to regress a density map, requiring point-level annotations.\nHowever, annotating each person with a point is an expensive and laborious\nprocess. During the testing phase, the point-level annotations are not\nconsidered to evaluate the counting accuracy, which means the point-level\nannotations are redundant. Hence, it is desirable to develop weakly-supervised\ncounting methods that just rely on count-level annotations, a more economical\nway of labeling. Current weakly-supervised counting methods adopt the CNN to\nregress a total count of the crowd by an image-to-count paradigm. However,\nhaving limited receptive fields for context modeling is an intrinsic limitation\nof these weakly-supervised CNN-based methods. These methods thus can not\nachieve satisfactory performance, with limited applications in the real-word.\nThe Transformer is a popular sequence-to-sequence prediction model in NLP,\nwhich contains a global receptive field. In this paper, we propose TransCrowd,\nwhich reformulates the weakly-supervised crowd counting problem from the\nperspective of sequence-to-count based on Transformer. We observe that the\nproposed TransCrowd can effectively extract the semantic crowd information by\nusing the self-attention mechanism of Transformer. To the best of our\nknowledge, this is the first work to adopt a pure Transformer for crowd\ncounting research. Experiments on five benchmark datasets demonstrate that the\nproposed TransCrowd achieves superior performance compared with all the\nweakly-supervised CNN-based counting methods and gains highly competitive\ncounting performance compared with some popular fully-supervised counting\nmethods. An implementation of our method is available at\nhttps://github.com/dk-liang/TransCrowd\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dingkang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiwu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09857","description":"<p>Deep neural networks (DNNs) are effective in solving many real-world\nproblems. Larger DNN models usually exhibit better quality (e.g., accuracy) but\ntheir excessive computation results in long inference time. Model\nsparsification can reduce the computation and memory cost while maintaining\nmodel quality. Most existing sparsification algorithms unidirectionally remove\nweights, while others randomly or greedily explore a small subset of weights in\neach layer for pruning. The limitations of these algorithms reduce the level of\nachievable sparsity. In addition, many algorithms still require pre-trained\ndense models and thus suffer from large memory footprint. In this paper, we\npropose a novel scheduled grow-and-prune (GaP) methodology without having to\npre-train a dense model. It addresses the shortcomings of the previous works by\nrepeatedly growing a subset of layers to dense and then pruning them back to\nsparse after some training. Experiments show that the models pruned using the\nproposed methods match or beat the quality of the highly optimized dense models\nat 80% sparsity on a variety of tasks, such as image classification, objective\ndetection, 3D object part segmentation, and translation. They also outperform\nother state-of-the-art (SOTA) methods for model sparsification. As an example,\na 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1\naccuracy on ImageNet, improving the previous SOTA results by 1.5%. Code\navailable at: https://github.com/boone891214/GaP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Kuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in the Wild. (arXiv:2106.11145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11145","description":"<p>Image-based age estimation aims to predict a person's age from facial images.\nIt is used in a variety of real-world applications. Although end-to-end deep\nmodels have achieved impressive results for age estimation on benchmark\ndatasets, their performance in-the-wild still leaves much room for improvement\ndue to the challenges caused by large variations in head pose, facial\nexpressions, and occlusions. To address this issue, we propose a simple yet\neffective method to explicitly incorporate facial semantics into age\nestimation, so that the model would learn to correctly focus on the most\ninformative facial components from unaligned facial images regardless of head\npose and non-rigid deformation. To this end, we design a face parsing-based\nnetwork to learn semantic information at different scales and a novel face\nparsing attention module to leverage these semantic features for age\nestimation. To evaluate our method on in-the-wild data, we also introduce a new\nchallenging large-scale benchmark called IMDB-Clean. This dataset is created by\nsemi-automatically cleaning the noisy IMDB-WIKI dataset using a constrained\nclustering method. Through comprehensive experiment on IMDB-Clean and other\nbenchmark datasets, under both intra-dataset and cross-dataset evaluation\nprotocols, we show that our method consistently outperforms all existing age\nestimation methods and achieves a new state-of-the-art performance. To the best\nof our knowledge, our work presents the first attempt of leveraging face\nparsing attention to achieve semantic-aware age estimation, which may be\ninspiring to other high level facial analysis tasks. Code and data are\navailable on \\url{https://github.com/ibug-group/fpage}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modality Deep Restoration of Extremely Compressed Face Videos. (arXiv:2107.05548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05548","description":"<p>Arguably the most common and salient object in daily video communications is\nthe talking head, as encountered in social media, virtual classrooms,\nteleconferences, news broadcasting, talk shows, etc. When communication\nbandwidth is limited by network congestions or cost effectiveness, compression\nartifacts in talking head videos are inevitable. The resulting video quality\ndegradation is highly visible and objectionable due to high acuity of human\nvisual system to faces. To solve this problem, we develop a multi-modality deep\nconvolutional neural network method for restoring face videos that are\naggressively compressed. The main innovation is a new DCNN architecture that\nincorporates known priors of multiple modalities: the video-synchronized speech\nsignal and semantic elements of the compression code stream, including motion\nvectors, code partition map and quantization parameters. These priors strongly\ncorrelate with the latent video and hence they are able to enhance the\ncapability of deep learning to remove compression artifacts. Ample empirical\nevidences are presented to validate the superior performance of the proposed\nDCNN method on face videos over the existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10756","description":"<p>Faces generated using generative adversarial networks (GANs) have reached\nunprecedented realism. These faces, also known as \"Deep Fakes\", appear as\nrealistic photographs with very little pixel-level distortions. While some work\nhas enabled the training of models that lead to the generation of specific\nproperties of the subject, generating a facial image based on a natural\nlanguage description has not been fully explored. For security and criminal\nidentification, the ability to provide a GAN-based system that works like a\nsketch artist would be incredibly useful. In this paper, we present a novel\napproach to generate facial images from semantic text descriptions. The learned\nmodel is provided with a text description and an outline of the type of face,\nwhich the model uses to sketch the features. Our models are trained using an\nAffine Combination Module (ACM) mechanism to combine the text embedding from\nBERT and the GAN latent space using a self-attention matrix. This avoids the\nloss of features due to inadequate \"attention\", which may happen if text\nembedding and latent vector are simply concatenated. Our approach is capable of\ngenerating images that are very accurately aligned to the exhaustive textual\ndescriptions of faces with many fine detail features of the face and helps in\ngenerating better images. The proposed method is also capable of making\nincremental changes to a previously generated image if it is provided with\nadditional textual descriptions or sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oza_M/0/1/0/all/0/1\">Manan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Vision Transformers See Like Convolutional Neural Networks?. (arXiv:2108.08810v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08810","description":"<p>Convolutional neural networks (CNNs) have so far been the de-facto model for\nvisual data. Recent work has shown that (Vision) Transformer models (ViT) can\nachieve comparable or even superior performance on image classification tasks.\nThis raises a central question: how are Vision Transformers solving these\ntasks? Are they acting like convolutional networks, or learning entirely\ndifferent visual representations? Analyzing the internal representation\nstructure of ViTs and CNNs on image classification benchmarks, we find striking\ndifferences between the two architectures, such as ViT having more uniform\nrepresentations across all layers. We explore how these differences arise,\nfinding crucial roles played by self-attention, which enables early aggregation\nof global information, and ViT residual connections, which strongly propagate\nfeatures from lower to higher layers. We study the ramifications for spatial\nlocalization, demonstrating ViTs successfully preserve input spatial\ninformation, with noticeable effects from different classification methods.\nFinally, we study the effect of (pretraining) dataset scale on intermediate\nfeatures and transfer learning, and conclude with a discussion on connections\nto new architectures such as the MLP-Mixer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1\">Maithra Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Visual Navigation under Partial Observability. (arXiv:2109.07752v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.07752","description":"<p>How can a robot navigate successfully in rich and diverse environments,\nindoors or outdoors, along office corridors or trails on the grassland, on the\nflat ground or the staircase? To this end, this work aims to address three\nchallenges: (i) complex visual observations, (ii) partial observability of\nlocal visual sensing, and (iii) multimodal robot behaviors conditioned on both\nthe local environment and the global navigation objective. We propose to train\na neural network (NN) controller for local navigation via imitation learning.\nTo tackle complex visual observations, we extract multi-scale spatial\nrepresentations through CNNs. To tackle partial observability, we aggregate\nmulti-scale spatial information over time and encode it in LSTMs. To learn\nmultimodal behaviors, we use a separate memory module for each behavior mode.\nImportantly, we integrate the multiple neural network modules into a unified\ncontroller that achieves robust performance for visual navigation in complex,\npartially observable environments. We implemented the controller on the\nquadrupedal Spot robot and evaluated it on three challenging tasks: adversarial\npedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. The\nexperiments show that the proposed NN architecture significantly improves\nnavigation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_B/0/1/0/all/0/1\">Bo Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinay/0/1/0/all/0/1\">Vinay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Assemble: Leveraging Multiple Datasets with Partial Labels. (arXiv:2109.12265v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12265","description":"<p>The success of deep learning relies heavily on large and diverse datasets\nwith extensive labels, but we often only have access to several small datasets\nassociated with partial labels. In this paper, we start a new initiative,\n\"Label-Assemble\", that aims to unleash the full potential of partially labeled\ndata from an assembly of public datasets. Specifically, we introduce a new\ndynamic adapter to encode different visual tasks, which addresses the\nchallenges of incomparable, heterogeneous, or even conflicting labeling\nprotocols. We also employ pseudo-labeling and consistency constraints to\nharness data with missing labels and to mitigate the domain gap across\ndatasets. From rigorous evaluations on three natural imaging and six medical\nimaging tasks, we discover that learning from \"negative examples\" facilitates\nboth classification and segmentation of classes of interest. This sheds new\nlight on the computer-aided diagnosis of rare diseases and emerging pandemics,\nwherein \"positive examples\" are hard to collect, yet \"negative examples\" are\nrelatively easier to assemble. Apart from exceeding prior arts in the ChestXray\nbenchmark, our model is particularly strong in identifying diseases of minority\nclasses, yielding over 3-point improvement on average. Remarkably, when using\nexisting partial labels, our model performance is on-par with that using full\nlabels, eliminating the need for an additional 40% of annotation costs. Code\nwill be made available at https://github.com/MrGiovanni/LabelAssemble.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mintong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yongyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan L. Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zongwei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer. (arXiv:2110.02178v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02178","description":"<p>Light-weight convolutional neural networks (CNNs) are the de-facto for mobile\nvision tasks. Their spatial inductive biases allow them to learn\nrepresentations with fewer parameters across different vision tasks. However,\nthese networks are spatially local. To learn global representations,\nself-attention-based vision trans-formers (ViTs) have been adopted. Unlike\nCNNs, ViTs are heavy-weight. In this paper, we ask the following question: is\nit possible to combine the strengths of CNNs and ViTs to build a light-weight\nand low latency network for mobile vision tasks? Towards this end, we introduce\nMobileViT, a light-weight and general-purpose vision transformer for mobile\ndevices. MobileViT presents a different perspective for the global processing\nof information with transformers, i.e., transformers as convolutions. Our\nresults show that MobileViT significantly outperforms CNN- and ViT-based\nnetworks across different tasks and datasets. On the ImageNet-1k dataset,\nMobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters,\nwhich is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT\n(ViT-based) for a similar number of parameters. On the MS-COCO object detection\ntask, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of\nparameters.\n</p>\n<p>Our source code is open-source and available at:\nhttps://github.com/apple/ml-cvnets\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sachin Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gray Matter Segmentation in Ultra High Resolution 7 Tesla ex vivo T2w MRI of Human Brain Hemispheres. (arXiv:2110.07711v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.07711","description":"<p>Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for\nvisualizing and characterizing detailed neuroanatomy. However, automated\ncortical segmentation methods in ex vivo MRI are not well developed, primarily\ndue to limited availability of labeled datasets, and heterogeneity in scanner\nhardware and acquisition protocols. In this work, we present a high resolution\n7 Tesla dataset of 32 ex vivo human brain specimens. We benchmark the cortical\nmantle segmentation performance of nine neural network architectures, trained\nand evaluated using manually-segmented 3D patches sampled from specific\ncortical regions, and show excellent generalizing capabilities across whole\nbrain hemispheres in different specimens, and also on unseen images acquired at\ndifferent magnetic field strength and imaging sequences. Finally, we provide\ncortical thickness measurements across key regions in 3D ex vivo human brain\nimages. Our code and processed datasets are publicly available at\nhttps://github.com/Pulkit-Khandelwal/picsl-ex-vivo-segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khandelwal_P/0/1/0/all/0/1\">Pulkit Khandelwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sadaghiani_S/0/1/0/all/0/1\">Shokufeh Sadaghiani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_M/0/1/0/all/0/1\">Michael Tran Duong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_S/0/1/0/all/0/1\">Sadhana Ravikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lim_S/0/1/0/all/0/1\">Sydney Lim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arezoumandan_S/0/1/0/all/0/1\">Sanaz Arezoumandan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peterson_C/0/1/0/all/0/1\">Claire Peterson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_E/0/1/0/all/0/1\">Eunice Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bedard_M/0/1/0/all/0/1\">Madigan Bedard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Capp_N/0/1/0/all/0/1\">Noah Capp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ittyerah_R/0/1/0/all/0/1\">Ranjit Ittyerah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Migdal_E/0/1/0/all/0/1\">Elyse Migdal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_G/0/1/0/all/0/1\">Grace Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kopp_E/0/1/0/all/0/1\">Emily Kopp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loja_B/0/1/0/all/0/1\">Bridget Loja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_E/0/1/0/all/0/1\">Eusha Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhakaran_K/0/1/0/all/0/1\">Karthik Prabhakaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mizsei_G/0/1/0/all/0/1\">Gabor Mizsei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabrielyan_M/0/1/0/all/0/1\">Marianna Gabrielyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schuck_T/0/1/0/all/0/1\">Theresa Schuck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_J/0/1/0/all/0/1\">John Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohm_D/0/1/0/all/0/1\">Daniel Ohm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Edward Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trojanowski_J/0/1/0/all/0/1\">John Q. Trojanowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McMillan_C/0/1/0/all/0/1\">Corey McMillan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grossman_M/0/1/0/all/0/1\">Murray Grossman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irwin_D/0/1/0/all/0/1\">David Irwin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tisdall_M/0/1/0/all/0/1\">M. Dylan Tisdall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sandhitsu R. Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wisse_L/0/1/0/all/0/1\">Laura E.M. Wisse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolk_D/0/1/0/all/0/1\">David A. Wolk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A. Yushkevich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral-ViT for Robust Fovea Localization. (arXiv:2110.09860v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09860","description":"<p>The fovea is an important anatomical landmark of the retina. Detecting the\nlocation of the fovea is essential for the analysis of many retinal diseases.\nHowever, robust fovea localization remains a challenging problem, as the fovea\nregion often appears fuzzy, and retina diseases may further obscure its\nappearance. This paper proposes a novel Vision Transformer (ViT) approach that\nintegrates information both inside and outside the fovea region to achieve\nrobust fovea localization. Our proposed network, named\nBilateral-Vision-Transformer (Bilateral-ViT), consists of two network branches:\na transformer-based main network branch for integrating global context across\nthe entire fundus image and a vessel branch for explicitly incorporating the\nstructure of blood vessels. The encoded features from both network branches are\nsubsequently merged with a customized Multi-scale Feature Fusion (MFF) module.\nOur comprehensive experiments demonstrate that the proposed approach is\nsignificantly more robust for diseased images and establishes the new state of\nthe arts using the Messidor and PALM datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_K/0/1/0/all/0/1\">Kang Dang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Q/0/1/0/all/0/1\">Qinji Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coenen_F/0/1/0/all/0/1\">Frans Coenen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xiaowei Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Multimodal Person Verification Using Audio-Visual-Thermal Data. (arXiv:2110.12136v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12136","description":"<p>In this paper, we study an approach to multimodal person verification using\naudio, visual, and thermal modalities. The combination of audio and visual\nmodalities has already been shown to be effective for robust person\nverification. From this perspective, we investigate the impact of further\nincreasing the number of modalities by adding thermal images. In particular, we\nimplemented unimodal, bimodal, and trimodal verification systems using\nstate-of-the-art deep learning architectures and compared their performance\nunder clean and noisy conditions. We also compared two popular fusion\napproaches based on simple score averaging and the soft attention mechanism.\nThe experiment conducted on the SpeakingFaces dataset demonstrates the superior\nperformance of the trimodal verification system. Specifically, on the easy test\nset, the trimodal system outperforms the best unimodal and bimodal systems by\nover 50% and 18% relative equal error rates, respectively, under both the clean\nand noisy conditions. On the hard test set, the trimodal system outperforms the\nbest unimodal and bimodal systems by over 40% and 13% relative equal error\nrates, respectively, under both the clean and noisy conditions. To enable\nreproducibility of the experiment and facilitate research into multimodal\nperson verification, we made our code, pretrained models, and preprocessed\ndataset freely available in our GitHub repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdrakhmanova_M/0/1/0/all/0/1\">Madina Abdrakhmanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abushakimova_S/0/1/0/all/0/1\">Saniya Abushakimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Texture-enhanced Light Field Super-resolution with Spatio-Angular Decomposition Kernels. (arXiv:2111.04069v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.04069","description":"<p>Despite the recent progress in light field super-resolution (LFSR) achieved\nby convolutional neural networks, the correlation information of light field\n(LF) images has not been sufficiently studied and exploited due to the\ncomplexity of 4D LF data. To cope with such high-dimensional LF data, most of\nthe existing LFSR methods resorted to decomposing it into lower dimensions and\nsubsequently performing optimization on the decomposed sub-spaces. However,\nthese methods are inherently limited as they neglected the characteristics of\nthe decomposition operations and only utilized a limited set of LF sub-spaces\nending up failing to sufficiently extract spatio-angular features and leading\nto a performance bottleneck. To overcome these limitations, in this paper, we\ncomprehensively discover the potentials of LF decomposition and propose a novel\nconcept of decomposition kernels. In particular, we systematically unify the\ndecomposition operations of various sub-spaces into a series of such\ndecomposition kernels, which are incorporated into our proposed Decomposition\nKernel Network (DKNet) for comprehensive spatio-angular feature extraction. The\nproposed DKNet is experimentally verified to achieve considerable improvements\ncompared with the state-of-the-art methods. To further improve DKNet in\nproducing more visually pleasing LFSR results, based on the VGG network, we\npropose a LFVGG loss to guide the Texture-Enhanced DKNet (TE-DKNet) to generate\nrich authentic textures and enhance LF images' visual quality significantly. We\nalso propose an indirect evaluation metric by taking advantage of LF material\nrecognition to objectively assess the perceptual enhancement brought by the\nLFVGG loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1\">Zexi Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoming Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_H/0/1/0/all/0/1\">Henry Wing Fung Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_Y/0/1/0/all/0/1\">Yuk Ying Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Aegis: Robust adversarial protectors for medical images. (arXiv:2111.10969v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10969","description":"<p>Deep neural network based medical image systems are vulnerable to adversarial\nexamples. Many defense mechanisms have been proposed in the literature,\nhowever, the existing defenses assume a passive attacker who knows little about\nthe defense system and does not change the attack strategy according to the\ndefense. Recent works have shown that a strong adaptive attack, where an\nattacker is assumed to have full knowledge about the defense system, can easily\nbypass the existing defenses. In this paper, we propose a novel adversarial\nexample defense system called Medical Aegis. To the best of our knowledge,\nMedical Aegis is the first defense in the literature that successfully\naddresses the strong adaptive adversarial example attacks to medical images.\nMedical Aegis boasts two-tier protectors: The first tier of Cushion weakens the\nadversarial manipulation capability of an attack by removing its high-frequency\ncomponents, yet posing a minimal effect on classification performance of the\noriginal image; the second tier of Shield learns a set of per-class DNN models\nto predict the logits of the protected model. Deviation from the Shield's\nprediction indicates adversarial examples. Shield is inspired by the\nobservations in our stress tests that there exist robust trails in the shallow\nlayers of a DNN model, which the adaptive attacks can hardly destruct.\nExperimental results show that the proposed defense accurately detects adaptive\nattacks, with negligible overhead for model inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qingsong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zecheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces. (arXiv:2111.12448v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12448","description":"<p>Learning a disentangled, interpretable, and structured latent representation\nin 3D generative models of faces and bodies is still an open problem. The\nproblem is particularly acute when control over identity features is required.\nIn this paper, we propose an intuitive yet effective self-supervised approach\nto train a 3D shape variational autoencoder (VAE) which encourages a\ndisentangled latent representation of identity features. Curating the\nmini-batch generation by swapping arbitrary features across different shapes\nallows to define a loss function leveraging known differences and similarities\nin the latent representations. Experimental results conducted on 3D meshes show\nthat state-of-the-art methods for latent disentanglement are not able to\ndisentangle identity features of faces and bodies. Our proposed method properly\ndecouples the generation of such features while maintaining good representation\nand reconstruction capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1\">Simone Foti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1\">Bongjin Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity. (arXiv:2111.14330v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14330","description":"<p>DETR is the first end-to-end object detector using a transformer\nencoder-decoder architecture and demonstrates competitive performance but low\ncomputational efficiency on high resolution feature maps. The subsequent work,\nDeformable DETR, enhances the efficiency of DETR by replacing dense attention\nwith deformable attention, which achieves 10x faster convergence and improved\nperformance. Deformable DETR uses the multiscale feature to ameliorate\nperformance, however, the number of encoder tokens increases by 20x compared to\nDETR, and the computation cost of the encoder attention remains a bottleneck.\nIn our preliminary experiment, we observe that the detection performance hardly\ndeteriorates even if only a part of the encoder token is updated. Inspired by\nthis observation, we propose Sparse DETR that selectively updates only the\ntokens expected to be referenced by the decoder, thus help the model\neffectively detect objects. In addition, we show that applying an auxiliary\ndetection loss on the selected tokens in the encoder improves the performance\nwhile minimizing computational overhead. We validate that Sparse DETR achieves\nbetter performance than Deformable DETR even with only 10% encoder tokens on\nthe COCO dataset. Albeit only the encoder tokens are sparsified, the total\ncomputation cost decreases by 38% and the frames per second (FPS) increases by\n42% compared to Deformable DETR.\n</p>\n<p>Code is available at https://github.com/kakaobrain/sparse-detr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roh_B/0/1/0/all/0/1\">Byungseok Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">JaeWoong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Wuhyun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Saehoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00374","description":"<p>Existing neural style transfer methods require reference style images to\ntransfer texture information of style images to content images. However, in\nmany practical situations, users may not have reference style images but still\nbe interested in transferring styles by just imagining them. In order to deal\nwith such applications, we propose a new framework that enables a style\ntransfer `without' a style image, but only with a text description of the\ndesired style. Using the pre-trained text-image embedding model of CLIP, we\ndemonstrate the modulation of the style of content images only with a single\ntext condition. Specifically, we propose a patch-wise text-image matching loss\nwith multiview augmentations for realistic texture transfer. Extensive\nexperimental results confirmed the successful image style transfer with\nrealistic textures that reflect semantic query texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Saliency Enhancement using Superpixel Similarity. (arXiv:2112.00665v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00665","description":"<p>Saliency Object Detection (SOD) has several applications in image analysis.\nThe methods have evolved from image-intrinsic to object-inspired\n(deep-learning-based) models. When a model fail, however, there is no\nalternative to enhance its saliency map. We fill this gap by introducing a\nhybrid approach, named \\textit{Iterative Saliency Enhancement over Superpixel\nSimilarity} (ISESS), that iteratively generates enhanced saliency maps by\nexecuting two operations alternately: object-based superpixel segmentation and\nsuperpixel-based saliency estimation -- cycling operations never exploited.\nISESS estimates seeds for superpixel delineation from a given saliency map and\ndefines superpixel queries in the foreground and background. A new saliency map\nresults from color similarities between queries and superpixels at each\niteration. The process repeats and, after a given number of iterations, the\ngenerated saliency maps are combined into one by cellular automata. Finally,\nthe resulting map is merged with the initial one by the maximum bewteen their\naverage values per superpixel. We demonstrate that our hybrid model can\nconsistently outperform three state-of-the-art deep-learning-based methods on\nfive image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joao_L/0/1/0/all/0/1\">Leonardo de Melo Joao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1\">Alexandre Xavier Falcao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Modeling of Turbulence. (arXiv:2112.02548v2 [physics.flu-dyn] UPDATED)","link":"http://arxiv.org/abs/2112.02548","description":"<p>We present a mathematically well founded approach for the synthetic modeling\nof turbulent flows using generative adversarial networks (GAN). Based on the\nanalysis of chaotic, deterministic systems in terms of ergodicity, we outline a\nmathematical proof that GAN can actually learn to sample state snapshots form\nthe invariant measure of the chaotic system. Based on this analysis, we study a\nhierarchy of chaotic systems starting with the Lorenz attractor and then carry\non to the modeling of turbulent flows with GAN. As training data, we use fields\nof velocity fluctuations obtained from large eddy simulations (LES). Two\narchitectures are investigated in detail: we use a deep, convolutional GAN\n(DCGAN) to synthesise the turbulent flow around a cylinder. We furthermore\nsimulate the flow around a low pressure turbine stator using the pix2pixHD\narchitecture for a conditional DCGAN being conditioned on the position of a\nrotating wake in front of the stator. The settings of adversarial training and\nthe effects of using specific GAN architectures are explained. We thereby show\nthat GAN are efficient in simulating turbulence in technically challenging flow\nproblems on the basis of a moderate amount of training data. GAN training and\ninference times significantly fall short when compared with classical numerical\nmethods, in particular LES, while still providing turbulent flows in high\nresolution. We furthermore analyse the statistical properties of the\nsynthesized and LES flow fields, which agree excellently. We also show the\nability of the conditional GAN to generalize over changes of geometry by\ngenerating turbulent flow fields for positions of the wake that are not\nincluded in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Drygala_C/0/1/0/all/0/1\">Claudia Drygala</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Winhart_B/0/1/0/all/0/1\">Benjamin Winhart</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mare_F/0/1/0/all/0/1\">Francesca di Mare</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-based Image Captioning. (arXiv:2112.06558v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06558","description":"<p>Text-based image captioning (TextCap) requires simultaneous comprehension of\nvisual content and reading the text of images to generate a natural language\ndescription. Although a task can teach machines to understand the complex human\nenvironment further given that text is omnipresent in our daily surroundings,\nit poses additional challenges in normal captioning. A text-based image\nintuitively contains abundant and complex multimodal relational content, that\nis, image details can be described diversely from multiview rather than a\nsingle caption. Certainly, we can introduce additional paired training data to\nshow the diversity of images' descriptions, this process is labor-intensive and\ntime-consuming for TextCap pair annotations with extra texts. Based on the\ninsight mentioned above, we investigate how to generate diverse captions that\nfocus on different image parts using an unpaired training paradigm. We propose\nthe Multimodal relAtional Graph adversarIal inferenCe (MAGIC) framework for\ndiverse and unpaired TextCap. This framework can adaptively construct multiple\nmultimodal relational graphs of images and model complex relationships among\ngraphs to represent descriptive diversity. Moreover, a cascaded generative\nadversarial network is developed from modeled graphs to infer the unpaired\ncaption generation in image-sentence feature alignment and linguistic coherence\nlevels. We validate the effectiveness of MAGIC in generating diverse captions\nfrom different relational information items of an image. Experimental results\nshow that MAGIC can generate very promising outcomes without using any\nimage-caption training pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiannan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qingpeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Sihui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forensic Analysis of Synthetically Generated Scientific Images. (arXiv:2112.08739v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08739","description":"<p>The widespread diffusion of synthetically generated content is a serious\nthreat that needs urgent countermeasures. The generation of synthetic content\nis not restricted to multimedia data like videos, photographs, or audio\nsequences, but covers a significantly vast area that can include biological\nimages as well, such as western-blot and microscopic images. In this paper, we\nfocus on the detection of synthetically generated western-blot images. These\nimages are largely explored in the biomedical literature and it has been\nalready shown how they can be easily counterfeited with few hopes to spot\nmanipulations by visual inspection or by standard forensics detectors. To\novercome the absence of a publicly available dataset in this area, we create a\nnew dataset comprising more than 14K original western-blot images and 18K\nsynthetic western-blot images, generated using four different state-of-the-art\ngeneration methods. We investigate different strategies to detect synthetic\nwestern blots, exploring binary classification methods as well as one-class\ndetectors. In both scenarios, we never exploit synthetic western-blot images at\ntraining stage. The achieved results show that synthetically generated\nwestern-blot images can be spot with good accuracy, even though the exploited\ndetectors are not optimized over synthetic versions of these scientific images.\nWe also test the robustness of the developed detectors against common\npost-processing operations performed on scientific images, showing that we can\nbe robust to JPEG compression and that some generative models can be easily\ndetected, independently of the post-processing applied.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandelli_S/0/1/0/all/0/1\">Sara Mandelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozzolino_D/0/1/0/all/0/1\">Davide Cozzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannas_E/0/1/0/all/0/1\">Edoardo D. Cannas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardenuto_J/0/1/0/all/0/1\">Joao P. Cardenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_D/0/1/0/all/0/1\">Daniel Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdoliva_L/0/1/0/all/0/1\">Luisa Verdoliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Semantic Transfer for Multi-Label Recognition with Partial Labels. (arXiv:2112.10941v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10941","description":"<p>Multi-label image recognition is a fundamental yet practical task because\nreal-world images inherently possess multiple semantic labels. However, it is\ndifficult to collect large-scale multi-label annotations due to the complexity\nof both the input images and output label spaces. To reduce the annotation\ncost, we propose a structured semantic transfer (SST) framework that enables\ntraining multi-label recognition models with partial labels, i.e., merely some\nlabels are known while other labels are missing (also called unknown labels)\nper image. The framework consists of two complementary transfer modules that\nexplore within-image and cross-image semantic correlations to transfer\nknowledge of known labels to generate pseudo labels for unknown labels.\nSpecifically, an intra-image semantic transfer module learns image-specific\nlabel co-occurrence matrix and maps the known labels to complement unknown\nlabels based on this matrix. Meanwhile, a cross-image transfer module learns\ncategory-specific feature similarities and helps complement unknown labels with\nhigh similarities. Finally, both known and generated labels are used to train\nthe multi-label recognition models. Extensive experiments on the Microsoft\nCOCO, Visual Genome and Pascal VOC datasets show that the proposed SST\nframework obtains superior performance over current state-of-the-art\nalgorithms. Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1\">Tao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hefeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition. (arXiv:2112.12496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12496","description":"<p>Current state-of-the-art deep learning based face recognition (FR) models\nrequire a large number of face identities for central training. However, due to\nthe growing privacy awareness, it is prohibited to access the face images on\nuser devices to continually improve face recognition models. Federated Learning\n(FL) is a technique to address the privacy issue, which can collaboratively\noptimize the model without sharing the data between clients. In this work, we\npropose a FL based framework called FedFR to improve the generic face\nrepresentation in a privacy-aware manner. Besides, the framework jointly\noptimizes personalized models for the corresponding clients via the proposed\nDecoupled Feature Customization module. The client-specific personalized model\ncan serve the need of optimized face recognition experience for registered\nidentities at the local device. To the best of our knowledge, we are the first\nto explore the personalized face recognition in FL setup. The proposed\nframework is validated to be superior to previous approaches on several generic\nand personalized face recognition benchmarks with diverse FL scenarios. The\nsource codes and our proposed personalized FR benchmark under FL setup are\navailable at https://github.com/jackie840129/FedFR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chih-Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chien-Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shang-Hong Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Curse of Zero Task Diversity: On the Failure of Transfer Learning to Outperform MAML and their Empirical Equivalence. (arXiv:2112.13121v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.13121","description":"<p>Recently, it has been observed that a transfer learning solution might be all\nwe need to solve many few-shot learning benchmarks -- thus raising important\nquestions about when and how meta-learning algorithms should be deployed. In\nthis paper, we seek to clarify these questions by proposing a novel metric --\nthe diversity coefficient -- to measure the diversity of tasks in a few-shot\nlearning benchmark. We hypothesize that the diversity coefficient of the\nfew-shot learning benchmark is predictive of whether meta-learning solutions\nwill succeed or not. Using the diversity coefficient, we show that the\nMiniImagenet benchmark has zero diversity. This novel insight contextualizes\nclaims that transfer learning solutions are better than meta-learned solutions.\nSpecifically, we empirically find that a diversity coefficient of zero\ncorrelates with a high similarity between transfer learning and Model-Agnostic\nMeta-Learning (MAML) learned solutions in terms of meta-accuracy (at meta-test\ntime). Therefore, we conjecture meta-learned solutions have the same meta-test\nperformance as transfer learning when the diversity coefficient is zero. Our\nwork provides the first test of whether diversity correlates with meta-learning\nsuccess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miranda_B/0/1/0/all/0/1\">Brando Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlertTrap: A study on object detection in remote insects trap monitoring system using on-the-edge deep learning platform. (arXiv:2112.13341v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13341","description":"<p>Fruit flies are one of the most harmful insect species to fruit yields. In\nAlertTrap, implementation of SSD architecture with different state-of-the-art\nbackbone feature extractors such as MobileNetV1 and MobileNetV2 appear to be\npotential solutions for the real-time detection problem. SSD-MobileNetV1 and\nSSD-MobileNetV2 perform well and result in AP@0.5 of 0.957 and 1.0\nrespectively. YOLOv4-tiny outperforms the SSD family with 1.0 in AP@0.5;\nhowever, its throughput velocity is slightly slower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1\">An D. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_D/0/1/0/all/0/1\">Duy A. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_D/0/1/0/all/0/1\">Dong T. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Hien B. Vo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks. (arXiv:2201.02263v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02263","description":"<p>State-of-the-art stereo matching networks trained only on synthetic data\noften fail to generalize to more challenging real data domains. In this paper,\nwe attempt to unfold an important factor that hinders the networks from\ngeneralizing across domains: through the lens of shortcut learning. We\ndemonstrate that the learning of feature representations in stereo matching\nnetworks is heavily influenced by synthetic data artefacts (shortcut\nattributes). To mitigate this issue, we propose an Information-Theoretic\nShortcut Avoidance~(ITSA) approach to automatically restrict shortcut-related\ninformation from being encoded into the feature representations. As a result,\nour proposed method learns robust and shortcut-invariant features by minimizing\nthe sensitivity of latent features to input variations. To avoid the\nprohibitive computational cost of direct input sensitivity optimization, we\npropose an effective yet feasible algorithm to achieve robustness. We show that\nusing this method, state-of-the-art stereo matching networks that are trained\npurely on synthetic data can effectively generalize to challenging and\npreviously unseen real data scenarios. Importantly, the proposed method\nenhances the robustness of the synthetic trained networks to the point that\nthey outperform their fine-tuned counterparts (on real data) for challenging\nout-of-domain stereo datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1\">WeiQin Chuah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1\">Ruwan Tennakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoseinnezhad_R/0/1/0/all/0/1\">Reza Hoseinnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1\">Alireza Bab-Hadiashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02980","description":"<p>Deep convolutional neural networks (CNNs) are broadly considered to be\nstate-of-the-art generic end-to-end image classification systems. However, they\nare known to underperform when training data are limited and thus require data\naugmentation strategies that render the method computationally expensive and\nnot always effective. Rather than using a data augmentation strategy to encode\ninvariances as typically done in machine learning, here we propose to\nmathematically augment a nearest subspace classification model in\nsliced-Wasserstein space by exploiting certain mathematical properties of the\nRadon Cumulative Distribution Transform (R-CDT), a recently introduced image\ntransform. We demonstrate that for a particular type of learning problem, our\nmathematical solution has advantages over data augmentation with deep CNNs in\nterms of classification accuracy and computational complexity, and is\nparticularly effective under a limited training data setting. The method is\nsimple, effective, computationally efficient, non-iterative, and requires no\nparameters to be tuned. Python code implementing our method is available at\nhttps://github.com/rohdelab/mathematical_augmentation. Our method is integrated\nas a part of the software package PyTransKit, which is available at\nhttps://github.com/rohdelab/PyTransKit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabbi_M/0/1/0/all/0/1\">Mohammad Shifat E Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubaiyat_A/0/1/0/all/0/1\">Abu Hasnat Mohammad Rubaiyat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1\">Gustavo K. Rohde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.08582","description":"<p>Current research on deep learning for medical image segmentation exposes\ntheir limitations in learning either global semantic information or local\ncontextual information. To tackle these issues, a novel network named\nSegTransVAE is proposed in this paper. SegTransVAE is built upon\nencoder-decoder architecture, exploiting transformer with the variational\nautoencoder (VAE) branch to the network to reconstruct the input images jointly\nwith segmentation. To the best of our knowledge, this is the first method\ncombining the success of CNN, transformer, and VAE. Evaluation on various\nrecently introduced datasets shows that SegTransVAE outperforms previous\nmethods in Dice Score and $95\\%$-Haudorff Distance while having comparable\ninference time to a simple CNN-based architecture network. The source code is\navailable at: https://github.com/itruonghai/SegTransVAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pham_Q/0/1/0/all/0/1\">Quan-Dung Pham</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_Truong_H/0/1/0/all/0/1\">Hai Nguyen-Truong</a> (1, 2 and 3), <a href=\"http://arxiv.org/find/eess/1/au:+Phuong_N/0/1/0/all/0/1\">Nam Nguyen Phuong</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoa N. A. Nguyen</a> (1, 2 and 3) ((1) VinBrain JSC., Vietnam, (2) University of Science, Ho Chi Minh City, Vietnam, (3) Vietnam National University, Ho Chi Minh City, Vietnam)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10943","description":"<p>Neuromorphic vision sensor is a new bio-inspired imaging paradigm that\nreports asynchronous, continuously per-pixel brightness changes called `events'\nwith high temporal resolution and high dynamic range. So far, the event-based\nimage reconstruction methods are based on artificial neural networks (ANN) or\nhand-crafted spatiotemporal smoothing techniques. In this paper, we first\nimplement the image reconstruction work via fully spiking neural network (SNN)\narchitecture. As the bio-inspired neural networks, SNNs operating with\nasynchronous binary spikes distributed over time, can potentially lead to\ngreater computational efficiency on event-driven hardware. We propose a novel\nEvent-based Video reconstruction framework based on a fully Spiking Neural\nNetwork (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and\nMembrane Potential (MP) neuron. We find that the spiking neurons have the\npotential to store useful temporal information (memory) to complete such\ntime-dependent tasks. Furthermore, to better utilize the temporal information,\nwe propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane\npotential of spiking neuron. The proposed neuron is referred as Adaptive\nMembrane Potential (AMP) neuron, which adaptively updates the membrane\npotential according to the input spikes. The experimental results demonstrate\nthat our models achieve comparable performance to ANN-based models on IJRR,\nMVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are\n19.36$\\times$ and 7.75$\\times$ more computationally efficient than their ANN\narchitectures, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLA-NeRF: Category-Level Articulated Neural Radiance Field. (arXiv:2202.00181v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00181","description":"<p>We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field\nthat can perform view synthesis, part segmentation, and articulated pose\nestimation. CLA-NeRF is trained at the object category level using no CAD\nmodels and no depth, but a set of RGB images with ground truth camera poses and\npart segments. During inference, it only takes a few RGB views (i.e., few-shot)\nof an unseen 3D object instance within the known category to infer the object\npart segmentation and the neural radiance field. Given an articulated pose as\ninput, CLA-NeRF can perform articulation-aware volume rendering to generate the\ncorresponding RGB image at any camera pose. Moreover, the articulated pose of\nan object can be estimated via inverse rendering. In our experiments, we\nevaluate the framework across five categories on both synthetic and real-world\ndata. In all cases, our method shows realistic deformation results and accurate\narticulated pose estimation. We believe that both few-shot articulated object\nrendering and articulated pose estimation open doors for robots to perceive and\ninteract with unseen articulated objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hung-Ju Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion. (arXiv:2202.02000v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02000","description":"<p>Multi-atlas segmentation (MAS) is a promising framework for medical image\nsegmentation. Generally, MAS methods register multiple atlases, i.e., medical\nimages with corresponding labels, to a target image; and the transformed atlas\nlabels can be combined to generate target segmentation via label fusion\nschemes. Many conventional MAS methods employed the atlases from the same\nmodality as the target image. However, the number of atlases with the same\nmodality may be limited or even missing in many clinical applications. Besides,\nconventional MAS methods suffer from the computational burden of registration\nor label fusion procedures. In this work, we design a novel cross-modality MAS\nframework, which uses available atlases from a certain modality to segment a\ntarget image from another modality. To boost the computational efficiency of\nthe framework, both the image registration and label fusion are achieved by\nwell-designed deep neural networks. For the atlas-to-target image registration,\nwe propose a bi-directional registration network (BiRegNet), which can\nefficiently align images from different modalities. For the label fusion, we\ndesign a similarity estimation network (SimNet), which estimates the fusion\nweight of each atlas by measuring its similarity to the target image. SimNet\ncan learn multi-scale information for similarity estimation to improve the\nperformance of label fusion. The proposed framework was evaluated by the left\nventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets,\nrespectively. Results have shown that the framework is effective for\ncross-modality MAS in both registration and label fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wangbin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liqin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical and Topological Summaries Aid Disease Detection for Segmented Retinal Vascular Images. (arXiv:2202.09708v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2202.09708","description":"<p>Disease complications can alter vascular network morphology and disrupt\ntissue functioning. Diabetic retinopathy, for example, is a complication of\ntype 1 and 2 diabetus mellitus that can cause blindness. Microvascular diseases\nare assessed by visual inspection of retinal images, but this can be\nchallenging when diseases exhibit silent symptoms or patients cannot attend\nin-person meetings. We examine the performance of machine learning algorithms\nin detecting microvascular disease when trained on either statistical or\ntopological summaries of segmented retinal vascular images. We apply our\nmethods to four publicly-available datasets and find that the fractal dimension\nperforms best for high resolution images. By contrast, we find that topological\ndescriptor vectors quantifying the number of loops in the data achieve the\nhighest accuracy for low resolution images. Further analysis, using the\ntopological approach, reveals that microvascular disease may alter morphology\nby reducing the number of loops in the retinal vasculature. Our work provides\npreliminary guidelines on which methods are most appropriate for assessing\ndisease in high and low resolution images. In the longer term, these methods\ncould be incorporated into automated disease assessment tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Nardini_J/0/1/0/all/0/1\">John T. Nardini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pugh_C/0/1/0/all/0/1\">Charles W. J. Pugh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Byrne_H/0/1/0/all/0/1\">Helen M. Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11855","description":"<p>We present a method to learn compositional predictive models from image\nobservations based on implicit object encoders, Neural Radiance Fields (NeRFs),\nand graph neural networks. A central question in learning dynamic models from\nsensor observations is on which representations predictions should be\nperformed. NeRFs have become a popular choice for representing scenes due to\ntheir strong 3D prior. However, most NeRF approaches are trained on a single\nscene, representing the whole scene with a global model, making generalization\nto novel scenes, containing different numbers of objects, challenging. Instead,\nwe present a compositional, object-centric auto-encoder framework that maps\nmultiple views of the scene to a \\emph{set} of latent vectors representing each\nobject separately. The latent vectors parameterize individual NeRF models from\nwhich the scene can be reconstructed and rendered from novel viewpoints. We\ntrain a graph neural network dynamics model in the latent space to achieve\ncompositionality for dynamics prediction. A key feature of our approach is that\nthe learned 3D information of the scene through the NeRF model enables us to\nincorporate structural priors in learning the dynamics models, making long-term\npredictions more stable. The model can further be used to synthesize new scenes\nfrom individual object observations. For planning, we utilize RRTs in the\nlearned latent space, where we can exploit our model and the implicit object\nencoder to make sampling the latent space informative and more efficient. In\nthe experiments, we show that the model outperforms several baselines on a\npushing task containing many objects. Video:\nhttps://dannydriess.github.io/compnerfdyn/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1\">Danny Driess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedrake_R/0/1/0/all/0/1\">Russ Tedrake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1\">Marc Toussaint</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?. (arXiv:2202.12426v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12426","description":"<p>While several works have studied the vulnerability of automated FRS and have\nproposed morphing attack detection (MAD) methods, very few have focused on\nstudying the human ability to detect morphing attacks. The examiner/observer's\nface morph detection ability is based on their observation, domain knowledge,\nexperience, and familiarity with the problem, and no works report the detailed\nfindings from observers who check identity documents as a part of their\neveryday professional life. This work creates a new benchmark database of\nrealistic morphing attacks from 48 unique subjects leading to 400 morphed\nimages presented to the observers in a Differential-MAD (D-MAD) setting. Unlike\nthe existing databases, the newly created morphed image database has been\ncreated with careful considerations to age, gender and ethnicity to create\nrealistic morph attacks. Further, unlike the previous works, we also capture\nten images from Automated Border Control (ABC) gates to mimic the realistic\nD-MAD setting leading to 400 probe images in border crossing scenarios. The\nnewly created dataset is further used to study the ability of human observers'\nability to detect morphed images. In addition, a new dataset of 180 morphed\nimages is also created using the FRGCv2 dataset under the Single Image-MAD\n(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a\nnew evaluation platform is created to conduct S-MAD and D-MAD analysis. The\nbenchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who\nare primarily governmental employees from more than 40 countries. The analysis\nprovides interesting insights and points to expert observers' missing\ncompetence and failure to detect a considerable amount of morphing attacks.\nHuman observers tend to detect morphed images to a lower accuracy as compared\nto the automated MAD algorithms evaluated in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godage_S/0/1/0/all/0/1\">Sankini Rancha Godage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovaasdal_F/0/1/0/all/0/1\">Fr&#xf8;y L&#xf8;v&#xe5;sdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Sushma Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pattern Based Multivariable Regression using Deep Learning (PBMR-DP). (arXiv:2202.13541v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13541","description":"<p>We propose a deep learning methodology for multivariate regression that is\nbased on pattern recognition that triggers fast learning over sensor data. We\nused a conversion of sensors-to-image which enables us to take advantage of\nComputer Vision architectures and training processes. In addition to this data\npreparation methodology, we explore the use of state-of-the-art architectures\nto generate regression outputs to predict agricultural crop continuous yield\ninformation. Finally, we compare with some of the top models reported in\nMLCAS2021. We found that using a straightforward training process, we were able\nto accomplish an MAE of 4.394, RMSE of 5.945, and R^2 of 0.861.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jiztom Kavalakkatt Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1\">Chandan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hererra_Garena_J/0/1/0/all/0/1\">Jansel Hererra-Garena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Kundan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darr_M/0/1/0/all/0/1\">Matthew J Darr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01311","description":"<p>Learning multimodal representations involves discovering correspondences and\nintegrating information from multiple heterogeneous sources of data. While\nrecent research has begun to explore the design of more general-purpose\nmultimodal models (contrary to prior focus on domain and modality-specific\narchitectures), these methods are still largely focused on a small set of\nmodalities in the language, vision, and audio space. In order to accelerate\ngeneralization towards diverse and understudied modalities, we investigate\nmethods for high-modality (a large set of diverse modalities) and\npartially-observable (each task only defined on a small subset of modalities)\nscenarios. To tackle these challenges, we design a general multimodal model\nthat enables multitask and transfer learning: multitask learning with shared\nparameters enables stable parameter counts (addressing scalability), and\ncross-modal transfer learning enables information sharing across modalities and\ntasks (addressing partial observability). Our resulting model generalizes\nacross text, image, video, audio, time-series, sensors, tables, and set\nmodalities from different research areas, improves the tradeoff between\nperformance and efficiency, transfers to new modalities and tasks, and reveals\nsurprising insights on the nature of information sharing in multitask models.\nWe release our code and benchmarks which we hope will present a unified\nplatform for subsequent theoretical and empirical analysis:\nhttps://github.com/pliang279/HighMMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives. (arXiv:2203.01445v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01445","description":"<p>The volume of available data has grown dramatically in recent years in many\napplications. Furthermore, the age of networks that used multiple modalities\nseparately has practically ended. Therefore, enabling bidirectional\ncross-modality data retrieval capable of processing has become a requirement\nfor many domains and disciplines of research. This is especially true in the\nmedical field, as data comes in a multitude of types, including various types\nof images and reports as well as molecular data. Most contemporary works apply\ncross attention to highlight the essential elements of an image or text in\nrelation to the other modalities and try to match them together. However,\nregardless of their importance in their own modality, these approaches usually\nconsider features of each modality equally. In this study, self-attention as an\nadditional loss term will be proposed to enrich the internal representation\nprovided into the cross attention module. This work suggests a novel\narchitecture with a new loss term to help represent images and texts in the\njoint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO\nand ARCH, show the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maleki_D/0/1/0/all/0/1\">Danial Maleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R Tizhoosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relative distance matters for one-shot landmark detection. (arXiv:2203.01687v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01687","description":"<p>Contrastive learning based methods such as cascade comparing to detect (CC2D)\nhave shown great potential for one-shot medical landmark detection. However,\nthe important cue of relative distance between landmarks is ignored in CC2D. In\nthis paper, we upgrade CC2D to version II by incorporating a\nsimple-yet-effective relative distance bias in the training stage, which is\ntheoretically proved to encourage the encoder to project the relatively distant\nlandmarks to the embeddings with low similarities. As consequence, CC2Dv2 is\nless possible to detect a wrong point far from the correct landmark.\nFurthermore, we present an open-source, landmark-labeled dataset for the\nmeasurement of biomechanical parameters of the lower extremity to alleviate the\nburden of orthopedic surgeons. The effectiveness of CC2Dv2 is evaluated on the\npublic dataset from the ISBI 2015 Grand-Challenge of cephalometric radiographs\nand our new dataset, which greatly outperforms the state-of-the-art one-shot\nlandmark detection approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qingsong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yihua Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Attended Meta-Learning for Few-Shot Learning. (arXiv:2106.10642v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2106.10642","description":"<p>Meta-learning (ML) has emerged as a promising direction in learning models\nunder constrained resource settings like few-shot learning. The popular\napproaches for ML either learn a generalizable initial model or a generic\nparametric optimizer through episodic training. The former approaches leverage\nthe knowledge from a batch of tasks to learn an optimal prior. In this work, we\nstudy the importance of a batch for ML. Specifically, we first incorporate a\nbatch episodic training regimen to improve the learning of the generic\nparametric optimizer. We also hypothesize that the common assumption in batch\nepisodic training that each task in a batch has an equal contribution to\nlearning an optimal meta-model need not be true. We propose to weight the tasks\nin a batch according to their \"importance\" in improving the meta-model's\nlearning. To this end, we introduce a training curriculum motivated by\nselective focus in humans, called task attended meta-training, to weight the\ntasks in a batch. Task attention is a standalone module that can be integrated\nwith any batch episodic training regimen. The comparisons of the models with\ntheir non-task-attended counterparts on complex datasets like miniImageNet and\ntieredImageNet validate its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aimen_A/0/1/0/all/0/1\">Aroof Aimen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1\">Sahil Sidheekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}