{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Graph-based Multi-View Fusion and Local Adaptation: Mitigating Within-Household Confusability for Speaker Identification. (arXiv:2207.04081v1 [eess.AS])","link":"http://arxiv.org/abs/2207.04081","description":"<p>Speaker identification (SID) in the household scenario (e.g., for smart\nspeakers) is an important but challenging problem due to limited number of\nlabeled (enrollment) utterances, confusable voices, and demographic imbalances.\nConventional speaker recognition systems generalize from a large random sample\nof speakers, causing the recognition to underperform for households drawn from\nspecific cohorts or otherwise exhibiting high confusability. In this work, we\npropose a graph-based semi-supervised learning approach to improve\nhousehold-level SID accuracy and robustness with locally adapted graph\nnormalization and multi-signal fusion with multi-view graphs. Unlike other work\non household SID, fairness, and signal fusion, this work focuses on speaker\nlabel inference (scoring) and provides a simple solution to realize\nhousehold-specific adaptation and multi-signal fusion without tuning the\nembeddings or training a fusion network. Experiments on the VoxCeleb dataset\ndemonstrate that our approach consistently improves the performance across\nhouseholds with different customer cohorts and degrees of confusability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1\">Yixiong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravichandran_V/0/1/0/all/0/1\">Venkatesh Ravichandran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Entity Disambiguation by Reasoning over a Knowledge Base. (arXiv:2207.04106v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04106","description":"<p>Recent work in entity disambiguation (ED) has typically neglected structured\nknowledge base (KB) facts, and instead relied on a limited subset of KB\ninformation, such as entity descriptions or types. This limits the range of\ncontexts in which entities can be disambiguated. To allow the use of all KB\nfacts, as well as descriptions and types, we introduce an ED model which links\nentities by reasoning over a symbolic knowledge base in a fully differentiable\nfashion. Our model surpasses state-of-the-art baselines on six well-established\nED datasets by 1.3 F1 on average. By allowing access to all KB information, our\nmodel is less reliant on popularity-based entity priors, and improves\nperformance on the challenging ShadowLink dataset (which emphasises infrequent\nand ambiguous entities) by 12.7 F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayoola_T/0/1/0/all/0/1\">Tom Ayoola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_J/0/1/0/all/0/1\">Joseph Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierleoni_A/0/1/0/all/0/1\">Andrea Pierleoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking. (arXiv:2207.04108v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04108","description":"<p>We introduce ReFinED, an efficient end-to-end entity linking model which uses\nfine-grained entity types and entity descriptions to perform linking. The model\nperforms mention detection, fine-grained entity typing, and entity\ndisambiguation for all mentions within a document in a single forward pass,\nmaking it more than 60 times faster than competitive existing approaches.\nReFinED also surpasses state-of-the-art performance on standard entity linking\ndatasets by an average of 3.7 F1. The model is capable of generalising to\nlarge-scale knowledge bases such as Wikidata (which has 15 times more entities\nthan Wikipedia) and of zero-shot entity linking. The combination of speed,\naccuracy and scale makes ReFinED an effective and cost-efficient system for\nextracting entities from web-scale datasets, for which the model has been\nsuccessfully deployed. Our code and pre-trained models are available at\nhttps://github.com/alexa/ReFinED\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayoola_T/0/1/0/all/0/1\">Tom Ayoola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1\">Shubhi Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_J/0/1/0/all/0/1\">Joseph Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1\">Christos Christodoulopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierleoni_A/0/1/0/all/0/1\">Andrea Pierleoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04153","description":"<p>Neural network models trained on text data have been found to encode\nundesired linguistic or sensitive attributes in their representation. Removing\nsuch attributes is non-trivial because of a complex relationship between the\nattribute, text input, and the learnt representation. Recent work has proposed\npost-hoc and adversarial methods to remove such unwanted attributes from a\nmodel's representation. Through an extensive theoretical and empirical\nanalysis, we show that these methods can be counter-productive: they are unable\nto remove the attributes entirely, and in the worst case may end up destroying\nall task-relevant features. The reason is the methods' reliance on a probing\nclassifier as a proxy for the attribute. Even under the most favorable\nconditions when an attribute's features in representation space can alone\nprovide 100% accuracy for learning the probing classifier, we prove that\npost-hoc or adversarial methods will fail to remove the attribute correctly.\nThese theoretical implications are confirmed by empirical experiments on models\ntrained on synthetic, Multi-NLI, and Twitter datasets. For sensitive\napplications of attribute removal such as fairness, we recommend caution\nagainst using these methods and propose a spuriousness metric to gauge the\nquality of the final classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TalkToModel: Understanding Machine Learning Models With Open Ended Dialogues. (arXiv:2207.04154v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04154","description":"<p>Machine Learning (ML) models are increasingly used to make critical decisions\nin real-world applications, yet they have also become more complex, making them\nharder to understand. To this end, several techniques to explain model\npredictions have been proposed. However, practitioners struggle to leverage\nexplanations because they often do not know which to use, how to interpret the\nresults, and may have insufficient data science experience to obtain\nexplanations. In addition, most current works focus on generating one-shot\nexplanations and do not allow users to follow up and ask fine-grained questions\nabout the explanations, which can be frustrating. In this work, we address\nthese challenges by introducing TalkToModel: an open-ended dialogue system for\nunderstanding machine learning models. Specifically, TalkToModel comprises\nthree key components: 1) a natural language interface for engaging in\ndialogues, making understanding ML models highly accessible, 2) a dialogue\nengine that adapts to any tabular model and dataset, interprets natural\nlanguage, maps it to appropriate operations (e.g., feature importance\nexplanations, counterfactual explanations, showing model errors), and generates\ntext responses, and 3) an execution component that run the operations and\nensures explanations are accurate. We carried out quantitative and human\nsubject evaluations of TalkToModel. We found the system understands user\nquestions on novel datasets and models with high accuracy, demonstrating the\nsystem's capacity to generalize to new situations. In human evaluations, 73% of\nhealthcare workers (e.g., doctors and nurses) agreed they would use TalkToModel\nover baseline point-and-click systems, and 84.6% of ML graduate students agreed\nTalkToModel was easier to use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1\">Dylan Slack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1\">Satyapriya Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Audio Captioning and Language-Based Audio Retrieval. (arXiv:2207.04156v1 [cs.SD])","link":"http://arxiv.org/abs/2207.04156","description":"<p>This project involved participation in the DCASE 2022 Competition (Task 6)\nwhich had two subtasks: (1) Automated Audio Captioning and (2) Language-Based\nAudio Retrieval. The first subtask involved the generation of a textual\ndescription for audio samples, while the goal of the second was to find audio\nsamples within a fixed dataset that match a given description. For both\nsubtasks, the Clotho dataset was used. The models were evaluated on BLEU1,\nBLEU2, BLEU3, ROUGEL, METEOR, CIDEr, SPICE, and SPIDEr scores for audio\ncaptioning and R1, R5, R10 and mARP10 scores for audio retrieval. We have\nconducted a handful of experiments that modify the baseline models for these\ntasks. Our final architecture for Automated Audio Captioning is close to the\nbaseline performance, while our model for Language-Based Audio Retrieval has\nsurpassed its counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1\">Clive Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyejin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollman_P/0/1/0/all/0/1\">Patrick Kollman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internal Language Model Estimation based Language Model Fusion for Cross-Domain Code-Switching Speech Recognition. (arXiv:2207.04176v1 [eess.AS])","link":"http://arxiv.org/abs/2207.04176","description":"<p>Internal Language Model Estimation (ILME) based language model (LM) fusion\nhas been shown significantly improved recognition results over conventional\nshallow fusion in both intra-domain and cross-domain speech recognition tasks.\nIn this paper, we attempt to apply our ILME method to cross-domain\ncode-switching speech recognition (CSSR) work. Specifically, our curiosity\ncomes from several aspects. First, we are curious about how effective the\nILME-based LM fusion is for both intra-domain and cross-domain CSSR tasks. We\nverify this with or without merging two code-switching domains. More\nimportantly, we train an end-to-end (E2E) speech recognition model by means of\nmerging two monolingual data sets and observe the efficacy of the proposed\nILME-based LM fusion for CSSR. Experimental results on SEAME that is from\nSoutheast Asian and another Chinese Mainland CS data set demonstrate the\neffectiveness of the proposed ILME-based LM fusion method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yizhou Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yufei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Haihua Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yi He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation. (arXiv:2207.04206v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04206","description":"<p>It is difficult for non-autoregressive translation (NAT) models to capture\nthe multi-modal distribution of target translations due to their conditional\nindependence assumption, which is known as the \"multi-modality problem\",\nincluding the lexical multi-modality and the syntactic multi-modality. While\nthe first one has been well studied, the syntactic multi-modality brings severe\nchallenge to the standard cross entropy (XE) loss in NAT and is under studied.\nIn this paper, we conduct a systematic study on the syntactic multi-modality\nproblem. Specifically, we decompose it into short- and long-range syntactic\nmulti-modalities and evaluate several recent NAT algorithms with advanced loss\nfunctions on both carefully designed synthesized datasets and real datasets. We\nfind that the Connectionist Temporal Classification (CTC) loss and the\nOrder-Agnostic Cross Entropy (OAXE) loss can better handle short- and\nlong-range syntactic multi-modalities respectively. Furthermore, we take the\nbest of both and design a new loss function to better handle the complicated\nsyntactic multi-modality in real-world datasets. To facilitate practical usage,\nwe provide a guide to use different loss functions for different kinds of\nsyntactic multi-modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kexun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Chest X-ray Pathologies in Natural Language. (arXiv:2207.04343v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04343","description":"<p>Most deep learning algorithms lack explanations for their predictions, which\nlimits their deployment in clinical practice. Approaches to improve\nexplainability, especially in medical imaging, have often been shown to convey\nlimited information, be overly reassuring, or lack robustness. In this work, we\nintroduce the task of generating natural language explanations (NLEs) to\njustify predictions made on medical images. NLEs are human-friendly and\ncomprehensive, and enable the training of intrinsically explainable models. To\nthis goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging\ndataset with NLEs. It contains over 38,000 NLEs, which explain the presence of\nvarious thoracic pathologies and chest X-ray findings. We propose a general\napproach to solve the task and evaluate several architectures on this dataset,\nincluding via clinician assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsons_G/0/1/0/all/0/1\">Guy Parsons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papiez_B/0/1/0/all/0/1\">Bartlomiej Papiez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. (arXiv:2207.04429v1 [cs.RO])","link":"http://arxiv.org/abs/2207.04429","description":"<p>Goal-conditioned policies for robotic navigation can be trained on large,\nunannotated datasets, providing for good generalization to real-world settings.\nHowever, particularly in vision-based settings where specifying goals requires\nan image, this makes for an unnatural interface. Language provides a more\nconvenient modality for communication with robots, but contemporary methods\ntypically require expensive supervision, in the form of trajectories annotated\nwith language descriptions. We present a system, LM-Nav, for robotic navigation\nthat enjoys the benefits of training on unannotated large datasets of\ntrajectories, while still providing a high-level interface to the user. Instead\nof utilizing a labeled instruction following dataset, we show that such a\nsystem can be constructed entirely out of pre-trained models for navigation\n(ViNG), image-language association (CLIP), and language modeling (GPT-3),\nwithout requiring any fine-tuning or language-annotated robot data. We\ninstantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon\nnavigation through complex, outdoor environments from natural language\ninstructions. For videos of our experiments, code release, and an interactive\nColab notebook that runs in your browser, please check out our project page\nhttps://sites.google.com/view/lmnav\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Dhruv Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Blazej Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Centric Research for NLP: Towards a Definition and Guiding Questions. (arXiv:2207.04447v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04447","description":"<p>With Human-Centric Research (HCR) we can steer research activities so that\nthe research outcome is beneficial for human stakeholders, such as end users.\nBut what exactly makes research human-centric? We address this question by\nproviding a working definition and define how a research pipeline can be split\ninto different stages in which human-centric components can be added.\nAdditionally, we discuss existing NLP with HCR components and define a series\nof guiding questions, which can serve as starting points for researchers\ninterested in exploring human-centric research approaches. We hope that this\nwork would inspire researchers to refine the proposed definition and to pose\nother questions that might be meaningful for achieving HCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gastinger_J/0/1/0/all/0/1\">Julia Gastinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alesiani_F/0/1/0/all/0/1\">Francesco Alesiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sztyler_T/0/1/0/all/0/1\">Timo Sztyler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Ammar Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Na Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Persuasion Detection: Video Games as an Invaluable Data Source for NLP. (arXiv:2207.04453v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04453","description":"<p>Role-playing games (RPGs) have a considerable amount of text in video game\ndialogues. Quite often this text is semi-annotated by the game developers. In\nthis paper, we extract a multilingual dataset of persuasive dialogue from\nseveral RPGs. We show the viability of this data in building a persuasion\ndetection system using a natural language processing (NLP) model called BERT.\nWe believe that video games have a lot of unused potential as a datasource for\na variety of NLP tasks. The code and data described in this paper are available\non Zenodo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poyhonen_T/0/1/0/all/0/1\">Teemu P&#xf6;yh&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Myers-Briggs personality classification from social media text using pre-trained language models. (arXiv:2207.04476v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04476","description":"<p>In Natural Language Processing, the use of pre-trained language models has\nbeen shown to obtain state-of-the-art results in many downstream tasks such as\nsentiment analysis, author identification and others. In this work, we address\nthe use of these methods for personality classification from text. Focusing on\nthe Myers-Briggs (MBTI) personality model, we describe a series of experiments\nin which the well-known Bidirectional Encoder Representations from Transformers\n(BERT) model is fine-tuned to perform MBTI classification. Our main findings\nsuggest that the current approach significantly outperforms well-known text\nclassification models based on bag-of-words and static word embeddings alike\nacross multiple evaluation scenarios, and generally outperforms previous work\nin the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1\">Vitor Garcia dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraboni_I/0/1/0/all/0/1\">Ivandr&#xe9; Paraboni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling up ML-based Black-box Planning with Partial STRIPS Models. (arXiv:2207.04479v1 [cs.AI])","link":"http://arxiv.org/abs/2207.04479","description":"<p>A popular approach for sequential decision-making is to perform\nsimulator-based search guided with Machine Learning (ML) methods like policy\nlearning. On the other hand, model-relaxation heuristics can guide the search\neffectively if a full declarative model is available. In this work, we consider\nhow a practitioner can improve ML-based black-box planning on settings where a\ncomplete symbolic model is not available. We show that specifying an incomplete\nSTRIPS model that describes only part of the problem enables the use of\nrelaxation heuristics. Our findings on several planning domains suggest that\nthis is an effective way to improve ML-based black-box planning beyond\ncollecting more data or tuning ML architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Greco_M/0/1/0/all/0/1\">Matias Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">&#xc1;lvaro Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baier_J/0/1/0/all/0/1\">Jorge A. Baier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_H/0/1/0/all/0/1\">Hector Palacios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairDistillation: Mitigating Stereotyping in Language Models. (arXiv:2207.04546v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04546","description":"<p>Large pre-trained language models are successfully being used in a variety of\ntasks, across many languages. With this ever-increasing usage, the risk of\nharmful side effects also rises, for example by reproducing and reinforcing\nstereotypes. However, detecting and mitigating these harms is difficult to do\nin general and becomes computationally expensive when tackling multiple\nlanguages or when considering different biases. To address this, we present\nFairDistillation: a cross-lingual method based on knowledge distillation to\nconstruct smaller language models while controlling for specific biases. We\nfound that our distillation method does not negatively affect the downstream\nperformance on most tasks and successfully mitigates stereotyping and\nrepresentational harms. We demonstrate that FairDistillation can create fairer\nlanguage models at a considerably lower cost than alternative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delobelle_P/0/1/0/all/0/1\">Pieter Delobelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berendt_B/0/1/0/all/0/1\">Bettina Berendt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Sentiment and Emotion Analysis for Computational Literary Studies. (arXiv:1808.03137v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1808.03137","description":"<p>Emotions are a crucial part of compelling narratives: literature tells us\nabout people with goals, desires, passions, and intentions. Emotion analysis is\npart of the broader and larger field of sentiment analysis, and receives\nincreasing attention in literary studies. In the past, the affective dimension\nof literature was mainly studied in the context of literary hermeneutics.\nHowever, with the emergence of the research field known as Digital Humanities\n(DH), some studies of emotions in a literary context have taken a computational\nturn. Given the fact that DH is still being formed as a field, this direction\nof research can be rendered relatively new. In this survey, we offer an\noverview of the existing body of research on emotion analysis as applied to\nliterature. The research under review deals with a variety of topics including\ntracking dramatic changes of a plot development, network analysis of a literary\ntext, and understanding the emotionality of texts, among other topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Evgeny Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. (arXiv:2101.00436v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00436","description":"<p>Multi-hop reasoning (i.e., reasoning across two or more documents) is a key\ningredient for NLP models that leverage large corpora to exhibit broad\nknowledge. To retrieve evidence passages, multi-hop models must contend with a\nfast-growing search space across the hops, represent complex queries that\ncombine multiple information needs, and resolve ambiguity about the best order\nin which to hop between training passages. We tackle these problems via Baleen,\na system that improves the accuracy of multi-hop retrieval while learning\nrobustly from weak training signals in the many-hop setting. To tame the search\nspace, we propose condensed retrieval, a pipeline that summarizes the retrieved\npassages after each hop into a single compact context. To model complex\nqueries, we introduce a focused late interaction retriever that allows\ndifferent parts of the same query representation to match disparate relevant\npassages. Lastly, to infer the hopping dependencies among unordered training\npassages, we devise latent hop ordering, a weak-supervision strategy in which\nthe trained retriever itself selects the sequence of hops. We evaluate Baleen\non retrieval for two-hop question answering and many-hop claim verification,\nestablishing state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiWOZ 2.4: A Multi-Domain Task-Oriented Dialogue Dataset with Essential Annotation Corrections to Improve State Tracking Evaluation. (arXiv:2104.00773v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.00773","description":"<p>The MultiWOZ 2.0 dataset has greatly stimulated the research of task-oriented\ndialogue systems. However, its state annotations contain substantial noise,\nwhich hinders a proper evaluation of model performance. To address this issue,\nmassive efforts were devoted to correcting the annotations. Three improved\nversions (i.e., MultiWOZ 2.1-2.3) have then been released. Nonetheless, there\nare still plenty of incorrect and inconsistent annotations. This work\nintroduces MultiWOZ 2.4, which refines the annotations in the validation set\nand test set of MultiWOZ 2.1. The annotations in the training set remain\nunchanged (same as MultiWOZ 2.1) to elicit robust and noise-resilient model\ntraining. We benchmark eight state-of-the-art dialogue state tracking models on\nMultiWOZ 2.4. All of them demonstrate much higher performance than on MultiWOZ\n2.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fanghua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manotumruksa_J/0/1/0/all/0/1\">Jarana Manotumruksa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries. (arXiv:2109.09195v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09195","description":"<p>Current pre-trained models applied to summarization are prone to factual\ninconsistencies which either misrepresent the source text or introduce\nextraneous information. Thus, comparing the factual consistency of summaries is\nnecessary as we develop improved models. However, the optimal human evaluation\nsetup for factual consistency has not been standardized. To address this issue,\nwe crowdsourced evaluations for factual consistency using the rating-based\nLikert scale and ranking-based Best-Worst Scaling protocols, on 100 articles\nfrom each of the CNN-Daily Mail and XSum datasets over four state-of-the-art\nmodels, to determine the most reliable evaluation framework. We find that\nranking-based protocols offer a more reliable measure of summary quality across\ndatasets, while the reliability of Likert ratings depends on the target dataset\nand the evaluation design. Our crowdsourcing templates and summary evaluations\nwill be publicly available to facilitate future research on factual consistency\nin summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_G/0/1/0/all/0/1\">Griffin Thomas Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Borui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10255","description":"<p>The recognition of hate speech and offensive language (HOF) is commonly\nformulated as a classification task to decide if a text contains HOF. We\ninvestigate whether HOF detection can profit by taking into account the\nrelationships between HOF and similar concepts: (a) HOF is related to sentiment\nanalysis because hate speech is typically a negative statement and expresses a\nnegative opinion; (b) it is related to emotion analysis, as expressed hate\npoints to the author experiencing (or pretending to experience) anger while the\naddressees experience (or are intended to experience) fear. (c) Finally, one\nconstituting element of HOF is the mention of a targeted person or group. On\nthis basis, we hypothesize that HOF detection shows improvements when being\nmodeled jointly with these concepts, in a multi-task learning setup. We base\nour experiments on existing data sets for each of these concepts (sentiment,\nemotion, target of HOF) and evaluate our models as a participant (as team\nIMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection\nexperiments in which we consider multiple available resources and submissions\nto the shared task, we find that the combination of the CrowdFlower emotion\ncorpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target\ndetection data leads to an F1 =.79 in a multi-head multi-task learning model\nbased on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test\ndata, this result is more substantial with an increase by 2pp in F1 and a\nconsiderable increase in recall. Across both data sets (2019, 2021), the recall\nis particularly increased for the class of HOF (6pp for the 2019 data and 3pp\nfor the 2021 data), showing that MTL with emotion, sentiment, and target\nidentification is an appropriate approach for early warning systems that might\nbe deployed in social media platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plaza_del_Arco_F/0/1/0/all/0/1\">Flor Miriam Plaza-del-Arco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halat_S/0/1/0/all/0/1\">Sercan Halat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Attention always needed? A Case Study on Language Identification from Speech. (arXiv:2110.03427v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03427","description":"<p>Language Identification (LID), a recommended initial step to Automatic Speech\nRecognition (ASR), is used to detect a spoken language from audio specimens. In\nstate-of-the-art systems capable of multilingual speech processing, however,\nusers have to explicitly set one or more languages before using them. LID,\ntherefore, plays a very important role in situations where ASR based systems\ncannot parse the uttered language in multilingual contexts causing failure in\nspeech recognition. We propose an attention based convolutional recurrent\nneural network (CRNN with Attention) that works on Mel-frequency Cepstral\nCoefficient (MFCC) features of audio specimens. Additionally, we reproduce some\nstate-of-the-art approaches, namely Convolutional Neural Network (CNN) and\nConvolutional Recurrent Neural Network (CRNN), and compare them to our proposed\nmethod. We performed extensive evaluation on thirteen different Indian\nlanguages and our model achieves classification accuracy over 98%. Our LID\nmodel is robust to noise and provides 91.2% accuracy in a noisy scenario. The\nproposed model is easily extensible to new languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1\">Atanu Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Santanu Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_I/0/1/0/all/0/1\">Indranil Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_M/0/1/0/all/0/1\">Mahidas Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.05679","description":"<p>Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and attempts at straightforwardly applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained models; (2) hyperparameters that suit DP optimization; and (3)\nfine-tuning objectives aligned with the pretraining procedure. With these\nfactors set right, we obtain private NLP models that outperform\nstate-of-the-art private training approaches and strong non-private baselines\n-- by directly fine-tuning pretrained models with DP optimization on\nmoderately-sized corpora. To address the computational challenge of running\nDP-SGD with large Transformers, we propose a memory saving technique that\nallows clipping in DP-SGD to run without instantiating per-example gradients\nfor any layer in the model. The technique enables privately training\nTransformers with almost the same memory cost as non-private training at a\nmodest run-time overhead. Contrary to conventional wisdom that DP optimization\nfails at learning high-dimensional models (due to noise that scales with\ndimension) empirical results reveal that private learning with pretrained\nmodels tends to not suffer from dimension-dependent performance degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings. (arXiv:2111.07993v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07993","description":"<p>This paper presents CoLLIE: a simple, yet effective model for continual\nlearning of how language is grounded in vision. Given a pre-trained multimodal\nembedding model, where language and images are projected in the same semantic\nspace (in this case CLIP by OpenAI), CoLLIE learns a transformation function\nthat adjusts the language embeddings when needed to accommodate new language\nuse. This is done by predicting the difference vector that needs to be applied,\nas well as a scaling factor for this vector, so that the adjustment is only\napplied when needed. Unlike traditional few-shot learning, the model does not\njust learn new classes and labels, but can also generalize to similar language\nuse and leverage semantic compositionality. We verify the model's performance\non two different tasks of identifying the targets of referring expressions,\nwhere it has to learn new language use. The results show that the model can\nefficiently learn and generalize from only a few examples, with little\ninterference with the model's original zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willemsen_B/0/1/0/all/0/1\">Bram Willemsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. (arXiv:2112.01488v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2112.01488","description":"<p>Neural information retrieval (IR) has greatly advanced search and other\nknowledge-intensive language tasks. While many neural IR methods encode queries\nand documents into single-vector representations, late interaction models\nproduce multi-vector representations at the granularity of each token and\ndecompose relevance modeling into scalable token-level computations. This\ndecomposition has been shown to make late interaction more effective, but it\ninflates the space footprint of these models by an order of magnitude. In this\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\ncompression mechanism with a denoised supervision strategy to simultaneously\nimprove the quality and space footprint of late interaction. We evaluate\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\nquality within and outside the training domain while reducing the space\nfootprint of late interaction models by 6--10$\\times$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1\">Keshav Santhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1\">Jon Saad-Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Use of External Data for Spoken Named Entity Recognition. (arXiv:2112.07648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07648","description":"<p>Spoken language understanding (SLU) tasks involve mapping from speech audio\nsignals to semantic labels. Given the complexity of such tasks, good\nperformance might be expected to require large labeled datasets, which are\ndifficult to collect for each new task and domain. However, recent advances in\nself-supervised speech representations have made it feasible to consider\nlearning SLU models with limited labeled data. In this work we focus on\nlow-resource spoken named entity recognition (NER) and address the question:\nBeyond self-supervised pre-training, how can we use external speech and/or text\ndata that are not annotated for the task? We draw on a variety of approaches,\nincluding self-training, knowledge distillation, and transfer learning, and\nconsider their applicability to both end-to-end models and pipeline (speech\nrecognition followed by text NER model) approaches. We find that several of\nthese approaches improve performance in resource-constrained settings beyond\nthe benefits from pre-trained representations alone. Compared to prior work, we\nfind improved F1 scores of up to 16%. While the best baseline model is a\npipeline approach, the best performance when using external data is ultimately\nachieved by an end-to-end model. We provide detailed comparisons and analyses,\nshowing for example that end-to-end models are able to focus on the more\nNER-specific words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1\">Suwon Shon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyu J. Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Rich Representation of Keyphrases from Text. (arXiv:2112.08547v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08547","description":"<p>In this work, we explore how to train task-specific language models aimed\ntowards learning rich representation of keyphrases from text documents. We\nexperiment with different masking strategies for pre-training transformer\nlanguage models (LMs) in discriminative as well as generative settings. In the\ndiscriminative setting, we introduce a new pre-training objective - Keyphrase\nBoundary Infilling with Replacement (KBIR), showing large gains in performance\n(upto 8.16 points in F1) over SOTA, when the LM pre-trained using KBIR is\nfine-tuned for the task of keyphrase extraction. In the generative setting, we\nintroduce a new pre-training setup for BART - KeyBART, that reproduces the\nkeyphrases related to the input text in the CatSeq format, instead of the\ndenoised original input. This also led to gains in performance (upto 4.33\npoints in F1@M) over SOTA for keyphrase generation. Additionally, we also\nfine-tune the pre-trained language models on named entity recognition (NER),\nquestion answering (QA), relation extraction (RE), abstractive summarization\nand achieve comparable performance with that of the SOTA, showing that learning\nrich representation of keyphrases is indeed beneficial for many other\nfundamental NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_M/0/1/0/all/0/1\">Mayank Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahata_D/0/1/0/all/0/1\">Debanjan Mahata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1\">Ravneet Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmik_R/0/1/0/all/0/1\">Rajarshi Bhowmik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Isochrony-Aware Neural Machine Translation for Automatic Dubbing. (arXiv:2112.08548v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08548","description":"<p>We introduce the task of isochrony-aware machine translation which aims at\ngenerating translations suitable for dubbing. Dubbing of a spoken sentence\nrequires transferring the content as well as the speech-pause structure of the\nsource into the target language to achieve audiovisual coherence. Practically,\nthis implies correctly projecting pauses from the source to the target and\nensuring that target speech segments have roughly the same duration of the\ncorresponding source speech segments. In this work, we propose implicit and\nexplicit modeling approaches to integrate isochrony information into neural\nmachine translation. Experiments on English-German/French language pairs with\nautomatic metrics show that the simplest of the considered approaches works\nbest. Results are confirmed by human evaluations of translations and dubbed\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakew_S/0/1/0/all/0/1\">Surafel M. Lakew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1\">Yogesh Virkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning. (arXiv:2112.08713v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08713","description":"<p>Factual inconsistencies in generated summaries severely limit the practical\napplications of abstractive dialogue summarization. Although significant\nprogress has been achieved by using pre-trained models, substantial amounts of\nhallucinated content are found during the human evaluation. Pre-trained models\nare most commonly fine-tuned with cross-entropy loss for text summarization,\nwhich may not be an optimal strategy. In this work, we provide a typology of\nfactual errors with annotation data to highlight the types of errors and move\naway from a binary understanding of factuality. We further propose a training\nstrategy that improves the factual consistency and overall quality of summaries\nvia a novel contrastive fine-tuning, called ConFiT. Based on our\nlinguistically-informed typology of errors, we design different modular\nobjectives that each target a specific type. Specifically, we utilize hard\nnegative samples with errors to reduce the generation of factual inconsistency.\nIn order to capture the key information between speakers, we also design a\ndialogue-specific loss. Using human evaluation and automatic faithfulness\nmetrics, we show that our model significantly reduces all kinds of factual\nerrors on the dialogue summarization, SAMSum corpus. Moreover, our model could\nbe generalized to the meeting summarization, AMI corpus, and it produces\nsignificantly higher scores than most of the baselines on both datasets\nregarding word-overlap metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Arjun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Borui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_J/0/1/0/all/0/1\">Jai Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_A/0/1/0/all/0/1\">Aaron Wade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow. (arXiv:2201.02662v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02662","description":"<p>Lifelong experiences and learned knowledge lead to shared expectations about\nhow common situations tend to unfold. Such knowledge of narrative event flow\nenables people to weave together a story. However, comparable computational\ntools to evaluate the flow of events in narratives are limited. We quantify the\ndifferences between autobiographical and imagined stories by introducing\nsequentiality, a measure of narrative flow of events, drawing probabilistic\ninferences from a cutting-edge large language model (GPT-3). Sequentiality\ncaptures the flow of a narrative by comparing the probability of a sentence\nwith and without its preceding story context. We applied our measure to study\nthousands of diary-like stories, collected from crowdworkers about either a\nrecent remembered experience or an imagined story on the same topic. The\nresults show that imagined stories have higher sequentiality than\nautobiographical stories and that the sequentiality of autobiographical stories\nincreases when the memories are retold several months later. In pursuit of\ndeeper understandings of how sequentiality measures the flow of narratives, we\nexplore proportions of major and minor events in story sentences, as annotated\nby crowdworkers. We find that lower sequentiality is associated with higher\nproportions of major events. The methods and results highlight opportunities to\nuse cutting-edge computational analyses, such as sequentiality, on large\ncorpora of matched imagined and autobiographical stories to investigate the\ninfluences of memory and reasoning on language generation processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafarpour_A/0/1/0/all/0/1\">Anna Jafarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pennebaker_J/0/1/0/all/0/1\">James W. Pennebaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Adversarial Concept Erasure. (arXiv:2201.12091v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12091","description":"<p>Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem.\n</p>\n<p>We formulate the problem of identifying and erasing a linear subspace that\ncorresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear minimax\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, R-LACE, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod -- despite being linear -- is highly expressive, effectively mitigating\nbias in deep nonlinear classifiers while maintaining tractability and\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twiton_M/0/1/0/all/0/1\">Michael Twiton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference. (arXiv:2202.10408v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10408","description":"<p>The task of abductive natural language inference (\\alpha{}nli), to decide\nwhich hypothesis is the more likely explanation for a set of observations, is a\nparticularly difficult type of NLI. Instead of just determining a causal\nrelationship, it requires common sense to also evaluate how reasonable an\nexplanation is. All recent competitive systems build on top of contextualized\nrepresentations and make use of transformer architectures for learning an NLI\nmodel. When somebody is faced with a particular NLI task, they need to select\nthe best model that is available. This is a time-consuming and resource-intense\nendeavour. To solve this practical problem, we propose a simple method for\npredicting the performance without actually fine-tuning the model. We do this\nby testing how well the pre-trained models perform on the \\alpha{}nli task when\njust comparing sentence embeddings with cosine similarity to what the\nperformance that is achieved when training a classifier on top of these\nembeddings. We show that the accuracy of the cosine similarity approach\ncorrelates strongly with the accuracy of the classification approach with a\nPearson correlation coefficient of 0.65. Since the similarity computation is\norders of magnitude faster to compute on a given dataset (less than a minute\nvs. hours), our method can lead to significant time savings in the process of\nmodel selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadikis_E/0/1/0/all/0/1\">Em&#x12b;ls Kadi&#x137;is</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vaibhav Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Identification of Toxic Code Reviews Using ToxiCR. (arXiv:2202.13056v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2202.13056","description":"<p>Toxic conversations during software development interactions may have serious\nrepercussions on a Free and Open Source Software (FOSS) development project.\nFor example, victims of toxic conversations may become afraid to express\nthemselves, therefore get demotivated, and may eventually leave the project.\nAutomated filtering of toxic conversations may help a FOSS community to\nmaintain healthy interactions among its members. However, off-the-shelf\ntoxicity detectors perform poorly on Software Engineering (SE) dataset, such as\none curated from code review comments. To encounter this challenge, we present\nToxiCR, a supervised learning-based toxicity identification tool for code\nreview interactions. ToxiCR includes a choice to select one of the ten\nsupervised learning algorithms, an option to select text vectorization\ntechniques, eight preprocessing steps, and a large scale labeled dataset of\n19,571 code review comments. Two out of those eight preprocessing steps are SE\ndomain specific. With our rigorous evaluation of the models with various\ncombinations of preprocessing steps and vectorization techniques, we have\nidentified the best combination for our dataset that boosts 95.8% accuracy and\n88.9% F1 score. ToxiCR significantly outperforms existing toxicity detectors on\nour dataset. We have released our dataset, pretrained models, evaluation\nresults, and source code publicly available at:\nhttps://github.com/WSU-SEAL/ToxiCR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarker_J/0/1/0/all/0/1\">Jaydeb Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turzo_A/0/1/0/all/0/1\">Asif Kamal Turzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosu_A/0/1/0/all/0/1\">Amiangshu Bosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Recent Advances and Challenges in Reinforcement Learning Methods for Task-Oriented Dialogue Policy Learning. (arXiv:2202.13675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13675","description":"<p>Dialogue Policy Learning is a key component in a task-oriented dialogue\nsystem (TDS) that decides the next action of the system given the dialogue\nstate at each turn. Reinforcement Learning (RL) is commonly chosen to learn the\ndialogue policy, regarding the user as the environment and the system as the\nagent. Many benchmark datasets and algorithms have been created to facilitate\nthe development and evaluation of dialogue policy based on RL. In this paper,\nwe survey recent advances and challenges in dialogue policy from the\nprescriptive of RL. More specifically, we identify the major problems and\nsummarize corresponding solutions for RL-based dialogue policy learning.\nBesides, we provide a comprehensive survey of applying RL to dialogue policy\nlearning by categorizing recent methods into basic elements in RL. We believe\nthis survey can shed a light on future research in dialogue management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwan_W/0/1/0/all/0/1\">Wai-Chung Kwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01322","description":"<p>Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1\">Venkat Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Discriminative Representations and Decision Boundaries for Open Intent Detection. (arXiv:2203.05823v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05823","description":"<p>Open intent detection is a significant problem in natural language\nunderstanding, which aims to detect the unseen open intent with the prior\nknowledge of only known intents. Current methods have two core challenges in\nthis task. On the one hand, they have limitations in learning friendly\nrepresentations to detect the open intent. On the other hand, there lacks an\neffective approach to obtaining specific and compact decision boundaries for\nknown intents. To address these issues, this paper introduces an original\nframework, DA-ADB, which successively learns distance-aware intent\nrepresentations and adaptive decision boundaries for open intent detection.\nSpecifically, we first leverage distance information to enhance the\ndistinguishing capability of the intent representations. Then, we design a\nnovel loss function to obtain appropriate decision boundaries by balancing both\nempirical and open space risks. Extensive experiments show the effectiveness of\ndistance-aware and boundary learning strategies. Compared with the\nstate-of-the-art methods, our method achieves substantial improvements on three\nbenchmark datasets. It also yields robust performance with different\nproportions of labeled data and known categories. The full data and codes are\navailable at https://github.com/thuiar/TEXTOIR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shaojie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianrui Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELLE: Efficient Lifelong Pre-training for Emerging Data. (arXiv:2203.06311v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06311","description":"<p>Current pre-trained language models (PLM) are typically trained with static\ndata, ignoring that in real-world scenarios, streaming data of various sources\nmay continuously grow. This requires PLMs to integrate the information from all\nthe sources in a lifelong manner. Although this goal could be achieved by\nexhaustive pre-training on all the existing data, such a process is known to be\ncomputationally expensive. To this end, we propose ELLE, aiming at efficient\nlifelong pre-training for emerging data. Specifically, ELLE consists of (1)\nfunction preserved model expansion, which flexibly expands an existing PLM's\nwidth and depth to improve the efficiency of knowledge acquisition; and (2)\npre-trained domain prompts, which disentangle the versatile knowledge learned\nduring pre-training and stimulate the proper knowledge for downstream tasks. We\nexperiment ELLE with streaming data from 5 domains on BERT and GPT. The results\nshow the superiority of ELLE over various lifelong learning baselines in both\npre-training efficiency and downstream performances. The codes are publicly\navailable at https://github.com/thunlp/ELLE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks. (arXiv:2203.16773v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.16773","description":"<p>Speech representations learned from Self-supervised learning (SSL) models can\nbenefit various speech processing tasks. However, utilizing SSL representations\nusually requires fine-tuning the pre-trained models or designing task-specific\ndownstream models and loss functions, causing much memory usage and human\nlabor. Recently, prompting in Natural Language Processing (NLP) has been found\nto be an efficient technique to leverage pre-trained language models (LMs).\nSpecifically, prompt tuning optimizes a limited number of task-specific\nparameters with a fixed pre-trained model; as a result, only a small set of\nparameters is needed to be stored for each task. Prompt tuning improves\ncomputation and memory efficiency by leveraging the pre-trained LM's prediction\nability. Nevertheless, such a paradigm is little studied in the speech\ncommunity. We report in this paper the first exploration of the prompt tuning\nparadigm for speech processing tasks based on Generative Spoken Language Model\n(GSLM). Experiment results show that the prompt tuning technique achieves\ncompetitive performance in speech classification tasks with fewer trainable\nparameters than fine-tuning specialized downstream models. We further study the\ntechnique in challenging sequence generation tasks. Prompt tuning also\ndemonstrates its potential, while the limitation and possible research\ndirections are discussed in this paper. The source code is available on\nhttps://github.com/ga642381/SpeechPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavThruVec: Latent speech representation as intermediate features for neural speech synthesis. (arXiv:2203.16930v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.16930","description":"<p>Recent advances in neural text-to-speech research have been dominated by\ntwo-stage pipelines utilizing low-level intermediate speech representation such\nas mel-spectrograms. However, such predetermined features are fundamentally\nlimited, because they do not allow to exploit the full potential of a\ndata-driven approach through learning hidden representations. For this reason,\nseveral end-to-end methods have been proposed. However, such models are harder\nto train and require a large number of high-quality recordings with\ntranscriptions. Here, we propose WavThruVec - a two-stage architecture that\nresolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as\nintermediate speech representation. Since these hidden activations provide\nhigh-level linguistic features, they are more robust to noise. That allows us\nto utilize annotated speech datasets of a lower quality to train the\nfirst-stage module. At the same time, the second-stage component can be trained\non large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are\nalready time-aligned. This results in an increased generalization capability to\nout-of-vocabulary words, as well as to a better generalization to unseen\nspeakers. We show that the proposed model not only matches the quality of\nstate-of-the-art neural models, but also presents useful properties enabling\ntasks like voice conversion or zero-shot synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siuzdak_H/0/1/0/all/0/1\">Hubert Siuzdak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dura_P/0/1/0/all/0/1\">Piotr Dura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-aware Document Summarization: A Survey of Knowledge, Embedding Methods and Architectures. (arXiv:2204.11190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11190","description":"<p>Knowledge-aware methods have boosted a range of natural language processing\napplications over the last decades. With the gathered momentum, knowledge\nrecently has been pumped into enormous attention in document summarization, one\nof natural language processing applications. Previous works reported that\nknowledge-embedded document summarizers excel at generating superior digests,\nespecially in terms of informativeness, coherence, and fact consistency. This\npaper pursues to present the first systematic survey for the state-of-the-art\nmethodologies that embed knowledge into document summarizers. Particularly, we\npropose novel taxonomies to recapitulate knowledge and knowledge embeddings\nunder the document summarization view. We further explore how embeddings are\ngenerated in embedding learning architectures of document summarization models,\nespecially of deep learning models. At last, we discuss the challenges of this\ntopic and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yutong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Connectivity Reveals Generalization Strategies. (arXiv:2205.12411v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12411","description":"<p>It is widely accepted in the mode connectivity literature that when two\nneural networks are trained similarly on the same data, they are connected by a\npath through parameter space over which test set accuracy is maintained. Under\nsome circumstances, including transfer learning from pretrained models, these\npaths are presumed to be linear. In contrast to existing results, we find that\namong text classifiers (trained on MNLI, QQP, and CoLA), some pairs of\nfinetuned models have large barriers of increasing loss on the linear paths\nbetween them. On each task, we find distinct clusters of models which are\nlinearly connected on the test loss surface, but are disconnected from models\noutside the cluster -- models that occupy separate basins on the surface. By\nmeasuring performance on specially-crafted diagnostic datasets, we find that\nthese clusters correspond to different generalization strategies: one cluster\nbehaves like a bag of words model under domain shift, while another cluster\nuses syntactic heuristics. Our work demonstrates how the geometry of the loss\nsurface can guide models towards different heuristic functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1\">Jeevesh Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rachit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Short Math Answer Grading via In-context Meta-learning. (arXiv:2205.15219v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15219","description":"<p>Automatic short answer grading is an important research direction in the\nexploration of how to use artificial intelligence (AI)-based tools to improve\neducation. Current state-of-the-art approaches use neural language models to\ncreate vectorized representations of students responses, followed by\nclassifiers to predict the score. However, these approaches have several key\nlimitations, including i) they use pre-trained language models that are not\nwell-adapted to educational subject domains and/or student-generated text and\nii) they almost always train one model per question, ignoring the linkage\nacross a question and result in a significant model storage problem due to the\nsize of advanced language models. In this paper, we study the problem of\nautomatic short answer grading for students' responses to math questions and\npropose a novel framework for this task. First, we use MathBERT, a variant of\nthe popular language model BERT adapted to mathematical content, as our base\nmodel and fine-tune it for the downstream task of student response grading.\nSecond, we use an in-context learning approach that provides scoring examples\nas input to the language model to provide additional context information and\npromote generalization to previously unseen questions. We evaluate our\nframework on a real-world dataset of student responses to open-ended math\nquestions and show that our framework (often significantly) outperforms\nexisting approaches, especially for new questions that are not seen during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_S/0/1/0/all/0/1\">Sami Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kwame for Science: An AI Teaching Assistant Based on Sentence-BERT for Science Education in West Africa. (arXiv:2206.13703v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.13703","description":"<p>Africa has a high student-to-teacher ratio which limits students' access to\nteachers. Consequently, students struggle to get answers to their questions. In\nthis work, we extended Kwame, our previous AI teaching assistant, adapted it\nfor science education, and deployed it as a web app. Kwame for Science answers\nquestions of students based on the Integrated Science subject of the West\nAfrican Senior Secondary Certificate Examination (WASSCE). Kwame for Science is\na Sentence-BERT-based question-answering web app that displays 3 paragraphs as\nanswers along with a confidence score in response to science questions.\nAdditionally, it displays the top 5 related past exam questions and their\nanswers in addition to the 3 paragraphs. Our preliminary evaluation of the\nKwame for Science with a 2.5-week real-world deployment showed a top 3 accuracy\nof 87.5% (n=56) with 190 users across 11 countries. Kwame for Science will\nenable the delivery of scalable, cost-effective, and quality remote education\nto millions of people across Africa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1\">George Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1\">Samuel John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glago_A/0/1/0/all/0/1\">Andrew Glago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boateng_S/0/1/0/all/0/1\">Samuel Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumbol_V/0/1/0/all/0/1\">Victor Kumbol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Understand Depth?. (arXiv:2207.01077v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01077","description":"<p>Besides image classification, Contrastive Language-Image Pre-training (CLIP)\nhas accomplished extraordinary success for a wide range of vision tasks,\nincluding object-level and 3D space understanding. However, it's still\nchallenging to transfer semantic knowledge learned from CLIP into more\nintricate tasks of quantified targets, such as depth estimation with geometric\ninformation. In this paper, we propose to apply CLIP for zero-shot monocular\ndepth estimation, named DepthCLIP. We found that the patches of the input image\ncould respond to a certain semantic distance token and then be projected to a\nquantified depth bin for coarse estimation. Without any training, our DepthCLIP\nsurpasses existing unsupervised methods and even approaches the early\nfully-supervised networks. To our best knowledge, we are the first to conduct\nzero-shot adaptation from the semantic language knowledge to quantified\ndownstream tasks and perform zero-shot monocular depth estimation. We hope our\nwork could cast a light on future research. The code is available at\nhttps://github.com/Adonis-galaxy/DepthCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01079","description":"<p>A crucial component in the curation of KB for a scientific domain is\ninformation extraction from tables in the domain's published articles -- tables\ncarry important information (often numeric), which must be adequately extracted\nfor a comprehensive machine understanding of an article. Existing table\nextractors assume prior knowledge of table structure and format, which may not\nbe known in scientific tables. We study a specific and challenging table\nextraction problem: extracting compositions of materials (e.g., glasses,\nalloys). We first observe that materials science researchers organize similar\ncompositions in a wide variety of table styles, necessitating an intelligent\nmodel for table understanding and composition extraction. Consequently, we\ndefine this novel task as a challenge for the ML community and create a\ntraining dataset comprising 4,408 distantly supervised tables, along with 1,475\nmanually annotated dev and test tables. We also present DiSCoMaT, a strong\nbaseline geared towards this specific task, which combines multiple graph\nneural networks with several task-specific regular expressions, features, and\nconstraints. We show that DiSCoMaT outperforms recent table processing\narchitectures by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohd Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">N. M. Anoop Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer. (arXiv:2207.01964v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2207.01964","description":"<p>Increasing capabilities of quantum computing hardware and the challenge to\nrealize deep quantum circuits call for fully automated and efficient tools to\ncompile quantum circuits. To express arbitrary circuits in a sequence of native\ngates pertaining to the specific quantum computer architecture is necessary to\nmake algorithms portable across the landscape of quantum hardware providers. In\nthis work, we present a compiler capable of transforming and optimizing a\nquantum circuit, targeting a shuttling-based trapped-ion quantum processor. It\nconsists of custom algorithms set on top of the Cambridge Quantum Computer's\nquantum circuit framework Pytket. The performance is evaluated for a wide range\nof quantum circuits, showing that the gate counts can be reduced by a factor of\nup to 3.6 compared to standard Pytket and up to 2.2 compared to standard Qiskit\ncompilation, while we achieve similar gate counts as compared to a Pytket\nextension targeting the AQT linear-static trapped ion addressing-based\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kreppel_F/0/1/0/all/0/1\">Fabian Kreppel</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Melzer_C/0/1/0/all/0/1\">Christian Melzer</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wagner_J/0/1/0/all/0/1\">Janis Wagner</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hilder_J/0/1/0/all/0/1\">Janine Hilder</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Poschinger_U/0/1/0/all/0/1\">Ulrich Poschinger</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Schmidt_Kaler_F/0/1/0/all/0/1\">Ferdinand Schmidt-Kaler</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Brinkmann_A/0/1/0/all/0/1\">Andr&#xe9; Brinkmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SInGE: Sparsity via Integrated Gradients Estimation of Neuron Relevance. (arXiv:2207.04089v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04089","description":"<p>The leap in performance in state-of-the-art computer vision methods is\nattributed to the development of deep neural networks. However it often comes\nat a computational price which may hinder their deployment. To alleviate this\nlimitation, structured pruning is a well known technique which consists in\nremoving channels, neurons or filters, and is commonly applied in order to\nproduce more compact models. In most cases, the computations to remove are\nselected based on a relative importance criterion. At the same time, the need\nfor explainable predictive models has risen tremendously and motivated the\ndevelopment of robust attribution methods that highlight the relative\nimportance of pixels of an input image or feature map. In this work, we discuss\nthe limitations of existing pruning heuristics, among which magnitude and\ngradient-based methods. We draw inspiration from attribution methods to design\na novel integrated gradient pruning criterion, in which the relevance of each\nneuron is defined as the integral of the gradient variation on a path towards\nthis neuron removal. Furthermore, we propose an entwined DNN pruning and\nfine-tuning flowchart to better preserve DNN accuracy while removing\nparameters. We show through extensive validation on several datasets,\narchitectures as well as pruning scenarios that the proposed method, dubbed\nSInGE, significantly outperforms existing state-of-the-art DNN pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yvinec_E/0/1/0/all/0/1\">Edouard Yvinec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1\">Kevin Bailly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAIVConf: Face enhancement for AI-based Video Conference with Low Bit-rate. (arXiv:2207.04090v1 [eess.IV])","link":"http://arxiv.org/abs/2207.04090","description":"<p>Recently, high-quality video conferencing with fewer transmission bits has\nbecome a very hot and challenging problem. We propose FAIVConf, a specially\ndesigned video compression framework for video conferencing, based on the\neffective neural human face generation techniques. FAIVConf brings together\nseveral designs to improve the system robustness in real video conference\nscenarios: face-swapping to avoid artifacts in background animation; facial\nblurring to decrease transmission bit-rate and maintain the quality of\nextracted facial landmarks; and dynamic source update for face view\ninterpolation to accommodate a large range of head poses. Our method achieves a\nsignificant bit-rate reduction in the video conference and gives much better\nvisual quality under the same bit-rate compared with H.264 and H.265 coding\nschemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1\">Sheng Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Songnan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StatMix: Data augmentation method that relies on image statistics in federated learning. (arXiv:2207.04103v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04103","description":"<p>Availability of large amount of annotated data is one of the pillars of deep\nlearning success. Although numerous big datasets have been made available for\nresearch, this is often not the case in real life applications (e.g. companies\nare not able to share data due to GDPR or concerns related to intellectual\nproperty rights protection). Federated learning (FL) is a potential solution to\nthis problem, as it enables training a global model on data scattered across\nmultiple nodes, without sharing local data itself. However, even FL methods\npose a threat to data privacy, if not handled properly. Therefore, we propose\nStatMix, an augmentation approach that uses image statistics, to improve\nresults of FL scenario(s). StatMix is empirically tested on CIFAR-10 and\nCIFAR-100, using two neural network architectures. In all FL experiments,\napplication of StatMix improves the average accuracy, compared to the baseline\ntraining (with no use of StatMix). Some improvement can also be observed in\nnon-FL setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lewy_D/0/1/0/all/0/1\">Dominik Lewy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganzha_M/0/1/0/all/0/1\">Maria Ganzha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paprzycki_M/0/1/0/all/0/1\">Marcin Paprzycki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Systemic Error Detection Methods using Synthetic Images. (arXiv:2207.04104v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04104","description":"<p>We introduce SpotCheck, a framework for generating synthetic datasets to use\nfor evaluating methods for discovering blindspots (i.e., systemic errors) in\nimage classifiers. We use SpotCheck to run controlled studies of how various\nfactors influence the performance of blindspot discovery methods. Our\nexperiments reveal several shortcomings of existing methods, such as relatively\npoor performance in settings with multiple blindspots and sensitivity to\nhyperparameters. Further, we find that a method based on dimensionality\nreduction, PlaneSpot, is competitive with existing methods, which has promising\nimplications for the development of interactive tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1\">Gregory Plumb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_N/0/1/0/all/0/1\">Nari Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1\">&#xc1;ngel Alexander Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out of Distribution Detection via Neural Network Anchoring. (arXiv:2207.04125v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04125","description":"<p>Our goal in this paper is to exploit heteroscedastic temperature scaling as a\ncalibration strategy for out of distribution (OOD) detection.\nHeteroscedasticity here refers to the fact that the optimal temperature\nparameter for each sample can be different, as opposed to conventional\napproaches that use the same value for the entire distribution. To enable this,\nwe propose a new training strategy called anchoring that can estimate\nappropriate temperature values for each sample, leading to state-of-the-art OOD\ndetection performance across several benchmarks. Using NTK theory, we show that\nthis temperature function estimate is closely linked to the epistemic\nuncertainty of the classifier, which explains its behavior. In contrast to some\nof the best-performing OOD detection approaches, our method does not require\nexposure to additional outlier datasets, custom calibration objectives, or\nmodel ensembling. Through empirical studies with different OOD detection\nsettings -- far OOD, near OOD, and semantically coherent OOD - we establish a\nhighly effective OOD detection approach. Code and models can be accessed here\n-- https://github.com/rushilanirudh/AMP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Attention for gestational age at birth prediction. (arXiv:2207.04130v1 [eess.IV])","link":"http://arxiv.org/abs/2207.04130","description":"<p>We present our method for gestational age at birth prediction for the SLCN\n(surface learning for clinical neuroimaging) challenge. Our method is based on\na multi-view shape analysis technique that captures 2D renderings of a 3D\nobject from different viewpoints. We render the brain features on the surface\nof the sphere and then the 2D images are analyzed via 2D CNNs and an attention\nlayer for the regression task. The regression task achieves a MAE of 1.637 +-\n1.3 on the Native space and MAE of 1.38 +- 1.14 on the template space. The\nsource code for this project is available in our github repository\nhttps://github.com/MathieuLeclercq/SLCN_challenge_UNC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Leclercq_M/0/1/0/all/0/1\">Mathieu Leclercq</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Styner_M/0/1/0/all/0/1\">Martin Styner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prieto_J/0/1/0/all/0/1\">Juan Carlos Prieto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Attention Transformer for Video Interpolation. (arXiv:2207.04132v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04132","description":"<p>We propose TAIN (Transformers and Attention for video INterpolation), a\nresidual neural network for video interpolation, which aims to interpolate an\nintermediate frame given two consecutive image frames around it. We first\npresent a novel visual transformer module, named Cross-Similarity (CS), to\nglobally aggregate input image features with similar appearance as those of the\npredicted interpolated frame. These CS features are then used to refine the\ninterpolated prediction. To account for occlusions in the CS features, we\npropose an Image Attention (IA) module to allow the network to focus on CS\nfeatures from one frame over those of the other. Additionally, we augment our\ntraining dataset with an occluder patch that moves across frames to improve the\nnetwork's robustness to occlusions and large motion. Because existing methods\nyield smooth predictions especially near MBs, we use an additional training\nloss based on image gradient to yield sharper predictions. TAIN outperforms\nexisting methods that do not require flow estimation and performs comparably to\nflow-based methods while being computationally efficient in terms of inference\ntime on Vimeo90k, UCF101, and SNU-FILM benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hannah Halin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shuzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shuai Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasi_C/0/1/0/all/0/1\">Carlo Tomasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L$_0$onie: Compressing COINs with L$_0$-constraints. (arXiv:2207.04144v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04144","description":"<p>Advances in Implicit Neural Representations (INR) have motivated research on\ndomain-agnostic compression techniques. These methods train a neural network to\napproximate an object, and then store the weights of the trained model. For\nexample, given an image, a network is trained to learn the mapping from pixel\nlocations to RGB values. In this paper, we propose L$_0$onie, a\nsparsity-constrained extension of the COIN compression method. Sparsity allows\nto leverage the faster learning of overparameterized networks, while retaining\nthe desirable compression rate of smaller models. Moreover, our constrained\nformulation ensures that the final model respects a pre-determined compression\nrate, dispensing of the need for expensive architecture search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1\">Juan Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_Posada_J/0/1/0/all/0/1\">Jose Gallego-Posada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Task-Based Machine Learning Content Extraction Services for VIDINT. (arXiv:2207.04158v1 [cs.ET])","link":"http://arxiv.org/abs/2207.04158","description":"<p>This paper provides a comparison of current video content extraction tools\nwith a focus on comparing commercial task-based machine learning services.\nVideo intelligence (VIDINT) data has become a critical intelligence source in\nthe past decade. The need for AI-based analytics and automation tools to\nextract and structure content from video has quickly become a priority for\norganizations needing to search, analyze and exploit video at scale. With rapid\ngrowth in machine learning technology, the maturity of machine transcription,\nmachine translation, topic tagging, and object recognition tasks are improving\nat an exponential rate, breaking performance records in speed and accuracy as\nnew applications evolve. Each section of this paper reviews and compares\nproducts, software resources and video analytics capabilities based on tasks\nrelevant to extracting information from video with machine learning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brunk_J/0/1/0/all/0/1\">Joshua Brunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermann_N/0/1/0/all/0/1\">Nathan Jermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharp_R/0/1/0/all/0/1\">Ryan Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_C/0/1/0/all/0/1\">Carl D. Hoover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few 'Zero Level Set'-Shot Learning of Shape Signed Distance Functions in Feature Space. (arXiv:2207.04161v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04161","description":"<p>We explore a new idea for learning based shape reconstruction from a point\ncloud, based on the recently popularized implicit neural shape representations.\nWe cast the problem as a few-shot learning of implicit neural signed distance\nfunctions in feature space, that we approach using gradient based\nmeta-learning. We use a convolutional encoder to build a feature space given\nthe input point cloud. An implicit decoder learns to predict signed distance\nvalues given points represented in this feature space. Setting the input point\ncloud, i.e. samples from the target shape function's zero level set, as the\nsupport (i.e. context) in few-shot learning terms, we train the decoder such\nthat it can adapt its weights to the underlying shape of this context with a\nfew (5) tuning steps. We thus combine two types of implicit neural network\nconditioning mechanisms simultaneously for the first time, namely feature\nencoding and meta-learning. Our numerical and qualitative evaluation shows that\nin the context of implicit reconstruction from a sparse point cloud, our\nproposed strategy, i.e. meta-learning in feature space, outperforms existing\nalternatives, namely standard supervised learning in feature space, and\nmeta-learning in euclidean space, while still providing fast inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouasfi_A/0/1/0/all/0/1\">Amine Ouasfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1\">Adnane Boukhayma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Multimodal Vision-Language Models Generating Non-Generic Text. (arXiv:2207.04174v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04174","description":"<p>Vision-language models can assess visual context in an image and generate\ndescriptive text. While the generated text may be accurate and syntactically\ncorrect, it is often overly general. To address this, recent work has used\noptical character recognition to supplement visual information with text\nextracted from an image. In this work, we contend that vision-language models\ncan benefit from additional information that can be extracted from an image,\nbut are not used by current models. We modify previous multimodal frameworks to\naccept relevant information from any number of auxiliary classifiers. In\nparticular, we focus on person names as an additional set of tokens and create\na novel image-caption dataset to facilitate captioning with person names. The\ndataset, Politicians and Athletes in Captions (PAC), consists of captioned\nimages of well-known people in context. By fine-tuning pretrained models with\nthis dataset, we demonstrate a model that can naturally integrate facial\nrecognition tokens into generated text by training on limited data. For the PAC\ndataset, we provide a discussion on collection and baseline benchmark scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robbins_W/0/1/0/all/0/1\">Wes Robbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohourianshahzadi_Z/0/1/0/all/0/1\">Zanyar Zohourianshahzadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Handheld Burst Imaging to Simulated Defocus. (arXiv:2207.04175v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04175","description":"<p>A shallow depth-of-field image keeps the subject in focus, and the foreground\nand background contexts blurred. This effect requires much larger lens\napertures than those of smartphone cameras. Conventional methods acquire RGB-D\nimages and blur image regions based on their depth. However, this approach is\nnot suitable for reflective or transparent surfaces, or finely detailed object\nsilhouettes, where the depth value is inaccurate or ambiguous.\n</p>\n<p>We present a learning-based method to synthesize the defocus blur in shallow\ndepth-of-field images from handheld bursts acquired with a single small\naperture lens. Our deep learning model directly produces the shallow\ndepth-of-field image, avoiding explicit depth-based blurring. The simulated\naperture diameter equals the camera translation during burst acquisition. Our\nmethod does not suffer from artifacts due to inaccurate or ambiguous depth\nestimation, and it is well-suited to portrait photography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meng-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayana_V/0/1/0/all/0/1\">Venkata Ravi Kiran Dayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hau Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement. (arXiv:2207.04183v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04183","description":"<p>Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causes\nof permanent blindness worldwide. Designing an automatic grading system with\ngood generalization ability for DR and DME is vital in clinical practice.\nHowever, prior works either grade DR or DME independently, without considering\ninternal correlations between them, or grade them jointly by shared feature\nrepresentation, yet ignoring potential generalization issues caused by\ndifficult samples and data bias. Aiming to address these problems, we propose a\nframework for joint grading with the dynamic difficulty-aware weighted loss\n(DAW) and the dual-stream disentangled learning architecture (DETACH). Inspired\nby curriculum learning, DAW learns from simple samples to difficult samples\ndynamically via measuring difficulty adaptively. DETACH separates features of\ngrading tasks to avoid potential emphasis on the bias. With the addition of DAW\nand DETACH, the model learns robust disentangled feature representations to\nexplore internal correlations between DR and DME and achieve better grading\nperformance. Experiments on three benchmarks show the effectiveness and\nrobustness of our framework under both the intra-dataset and cross-dataset\ntests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Haoxuan Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Alignment Meets Fully Test-Time Adaptation. (arXiv:2207.04185v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04185","description":"<p>A foundational requirement of a deployed ML model is to generalize to data\ndrawn from a testing distribution that is different from training. A popular\nsolution to this problem is to adapt a pre-trained model to novel domains using\nonly unlabeled data. In this paper, we focus on a challenging variant of this\nproblem, where access to the original source data is restricted. While fully\ntest-time adaptation (FTTA) and unsupervised domain adaptation (UDA) are\nclosely related, the advances in UDA are not readily applicable to TTA, since\nmost UDA methods require access to the source data. Hence, we propose a new\napproach, CATTAn, that bridges UDA and FTTA, by relaxing the need to access\nentire source data, through a novel deep subspace alignment strategy. With a\nminimal overhead of storing the subspace basis set for the source data, CATTAn\nenables unsupervised alignment between source and target data during\nadaptation. Through extensive experimental evaluation on multiple 2D and 3D\nvision benchmarks (ImageNet-C, Office-31, OfficeHome, DomainNet, PointDA-10)\nand model architectures, we demonstrate significant gains in FTTA performance.\nFurthermore, we make a number of crucial findings on the utility of the\nalignment objective even with inherently robust models, pre-trained ViT\nrepresentations and under low sample availability in the target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thopalli_K/0/1/0/all/0/1\">Kowshik Thopalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan Turaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on Self-Supervised Object Detection Pretraining. (arXiv:2207.04186v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04186","description":"<p>In this work, we study different approaches to self-supervised pretraining of\nobject detection models. We first design a general framework to learn a\nspatially consistent dense representation from an image, by randomly sampling\nand projecting boxes to each augmented view and maximizing the similarity\nbetween corresponding box features. We study existing design choices in the\nliterature, such as box generation, feature extraction strategies, and using\nmultiple views inspired by its success on instance-level image representation\nlearning techniques. Our results suggest that the method is robust to different\nchoices of hyperparameters, and using multiple views is not as effective as\nshown for instance-level image representation learning. We also design two\nauxiliary tasks to predict boxes in one view from their features in the other\nview, by (1) predicting boxes from the sampled set by using a contrastive loss,\nand (2) predicting box coordinates using a transformer, which potentially\nbenefits downstream object detection tasks. We found that these tasks do not\nlead to better object detection performance when finetuning the pretrained\nmodel on labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Trung Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1\">Peter Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_M/0/1/0/all/0/1\">Maryam Khademi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Structured Representations of Visual Scenes. (arXiv:2207.04200v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04200","description":"<p>As the intermediate-level representations bridging the two levels, structured\nrepresentations of visual scenes, such as visual relationships between pairwise\nobjects, have been shown to not only benefit compositional models in learning\nto reason along with the structures but provide higher interpretability for\nmodel decisions. Nevertheless, these representations receive much less\nattention than traditional recognition tasks, leaving numerous open challenges\nunsolved. In the thesis, we study how machines can describe the content of the\nindividual image or video with visual relationships as the structured\nrepresentations. Specifically, we explore how structured representations of\nvisual scenes can be effectively constructed and learned in both the\nstatic-image and video settings, with improvements resulting from external\nknowledge incorporation, bias-reducing mechanism, and enhanced representation\nmodels. At the end of this thesis, we also discuss some open challenges and\nlimitations to shed light on future directions of structured representation\nlearning for visual scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiou_M/0/1/0/all/0/1\">Meng-Jiun Chiou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Multi-tenant Federated Learning. (arXiv:2207.04202v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04202","description":"<p>Federated learning (FL) is an emerging distributed machine learning method\nthat empowers in-situ model training on decentralized edge devices. However,\nmultiple simultaneous training activities could overload resource-constrained\ndevices. In this work, we propose a smart multi-tenant FL system, MuFL, to\neffectively coordinate and execute simultaneous training activities. We first\nformalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and\nintroduce a vanilla multi-tenant FL system that trains activities sequentially\nto form baselines. Then, we propose two approaches to optimize multi-tenant FL:\n1) activity consolidation merges training activities into one activity with a\nmulti-task architecture; 2) after training it for rounds, activity splitting\ndivides it into groups by employing affinities among activities such that\nactivities within a group have better synergy. Extensive experiments\ndemonstrate that MuFL outperforms other methods while consuming 40% less\nenergy. We hope this work will inspire the community to further study and\noptimize multi-tenant FL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Approach for Intensity Domain Multi-exposure Image Fusion. (arXiv:2207.04204v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04204","description":"<p>Recent innovations shows that blending of details captured by single Low\nDynamic Range (LDR) sensor overcomes the limitations of standard digital\ncameras to capture details from high dynamic range scene. We present a method\nto produce well-exposed fused image that can be displayed directly on\nconventional display devices. The ambition is to preserve details in poorly\nilluminated and brightly illuminated regions. Proposed approach does not\nrequire true radiance reconstruction and tone manipulation steps. The aforesaid\nobjective is achieved by taking into account local information measure that\nselect well-exposed regions across input exposures. In addition, Contrast\nLimited Adaptive Histogram equalization (CLAHE) is introduced to improve\nuniformity of input multi-exposure image prior to fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harbinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_D/0/1/0/all/0/1\">Dinesh Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vinay Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BOSS: Bottom-up Cross-modal Semantic Composition with Hybrid Counterfactual Training for Robust Content-based Image Retrieval. (arXiv:2207.04211v1 [cs.AI])","link":"http://arxiv.org/abs/2207.04211","description":"<p>Content-Based Image Retrieval (CIR) aims to search for a target image by\nconcurrently comprehending the composition of an example image and a\ncomplementary text, which potentially impacts a wide variety of real-world\napplications, such as internet search and fashion retrieval. In this scenario,\nthe input image serves as an intuitive context and background for the search,\nwhile the corresponding language expressly requests new traits on how specific\ncharacteristics of the query image should be modified in order to get the\nintended target image. This task is challenging since it necessitates learning\nand understanding the composite image-text representation by incorporating\ncross-granular semantic updates. In this paper, we tackle this task by a novel\n\\underline{\\textbf{B}}ottom-up cr\\underline{\\textbf{O}}ss-modal\n\\underline{\\textbf{S}}emantic compo\\underline{\\textbf{S}}ition (\\textbf{BOSS})\nwith Hybrid Counterfactual Training framework, which sheds new light on the CIR\ntask by studying it from two previously overlooked perspectives:\n\\emph{implicitly bottom-up composition of visiolinguistic representation} and\n\\emph{explicitly fine-grained correspondence of query-target construction}. On\nthe one hand, we leverage the implicit interaction and composition of\ncross-modal embeddings from the bottom local characteristics to the top global\nsemantics, preserving and transforming the visual representation conditioned on\nlanguage semantics in several continuous steps for effective target image\nsearch. On the other hand, we devise a hybrid counterfactual training strategy\nthat can reduce the model's ambiguity for similar queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiannan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Disease Identification on Chest-CT images using CNN and VGG16. (arXiv:2207.04212v1 [eess.IV])","link":"http://arxiv.org/abs/2207.04212","description":"<p>A newly identified coronavirus disease called COVID-19 mainly affects the\nhuman respiratory system. COVID-19 is an infectious disease caused by a virus\noriginating in Wuhan, China, in December 2019. Early diagnosis is the primary\nchallenge of health care providers. In the earlier stage, medical organizations\nwere dazzled because there were no proper health aids or medicine to detect a\nCOVID-19. A new diagnostic tool RT-PCR (Reverse Transcription Polymerase Chain\nReaction), was introduced. It collects swab specimens from the patient's nose\nor throat, where the COVID-19 virus gathers. This method has some limitations\nrelated to accuracy and testing time. Medical experts suggest an alternative\napproach called CT (Computed Tomography) that can quickly diagnose the infected\nlung areas and identify the COVID-19 in an earlier stage. Using chest CT\nimages, computer researchers developed several deep learning models identifying\nthe COVID-19 disease. This study presents a Convolutional Neural Network (CNN)\nand VGG16-based model for automated COVID-19 identification on chest CT images.\nThe experimental results using a public dataset of 14320 CT images showed a\nclassification accuracy of 96.34% and 96.99% for CNN and VGG16, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+S_B/0/1/0/all/0/1\">Briskline Kiruba S</a>, <a href=\"http://arxiv.org/find/eess/1/au:+A_P/0/1/0/all/0/1\">Petchiammal A</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murugan_D/0/1/0/all/0/1\">D. Murugan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-path Attention is All You Need for Audio-Visual Speech Extraction. (arXiv:2207.04213v1 [cs.MM])","link":"http://arxiv.org/abs/2207.04213","description":"<p>Audio-visual target speech extraction, which aims to extract a certain\nspeaker's speech from the noisy mixture by looking at lip movements, has made\nsignificant progress combining time-domain speech separation models and visual\nfeature extractors (CNN). One problem of fusing audio and video information is\nthat they have different time resolutions. Most current research upsamples the\nvisual features along the time dimension so that audio and video features are\nable to align in time. However, we believe that lip movement should mostly\ncontain long-term, or phone-level information. Based on this assumption, we\npropose a new way to fuse audio-visual features. We observe that for DPRNN\n\\cite{dprnn}, the interchunk dimension's time resolution could be very close to\nthe time resolution of video frames. Like \\cite{sepformer}, the LSTM in DPRNN\nis replaced by intra-chunk and inter-chunk self-attention, but in the proposed\nalgorithm, inter-chunk attention incorporates the visual features as an\nadditional feature stream. This prevents the upsampling of visual cues,\nresulting in more efficient audio-visual fusion. The result shows we achieve\nsuperior results compared with other time-domain based audio-visual fusion\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhongweiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xulin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Persistent Homology for Visual Recognition. (arXiv:2207.04220v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04220","description":"<p>Persistent topological properties of an image serve as an additional\ndescriptor providing an insight that might not be discovered by traditional\nneural networks. The existing research in this area focuses primarily on\nefficiently integrating topological properties of the data in the learning\nprocess in order to enhance the performance. However, there is no existing\nstudy to demonstrate all possible scenarios where introducing topological\nproperties can boost or harm the performance. This paper performs a detailed\nanalysis of the effectiveness of topological properties for image\nclassification in various training scenarios, defined by: the number of\ntraining samples, the complexity of the training data and the complexity of the\nbackbone network. We identify the scenarios that benefit the most from\ntopological features, e.g., training simple networks on small datasets.\nAdditionally, we discuss the problem of topological consistency of the datasets\nwhich is one of the major bottlenecks for using topological features for\nclassification. We further demonstrate how the topological inconsistency can\nharm the performance for certain scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khramtsova_E/0/1/0/all/0/1\">Ekaterina Khramtsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Register Unbalanced Point Pairs. (arXiv:2207.04221v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04221","description":"<p>Recent 3D registration methods can effectively handle large-scale or\npartially overlapping point pairs. However, despite its practicality, matching\nthe unbalanced pairs in terms of spatial scale and density has been overlooked.\nWe present a novel 3D registration method, called UPPNet, for the unbalanced\npoint pairs. We propose a hierarchical framework to find inlier correspondences\neffectively by gradually reducing search space. Our method predicts the\nsubregions of the target points likely to be overlapped with the query points.\nThe following super-point matching module and fine-grained refinement module\nestimate accurate inlier correspondences between two point clouds. Furthermore,\nwe apply geometric constraints to refine the correspondences that satisfy\nspatial compatibility. Correspondence prediction is trained end-to-end, and our\napproach can predict the proper rigid transformation with a single forward pass\ngiven unbalanced point cloud pairs. To validate the efficacy of the proposed\nmethod, we create a KITTI-UPP dataset by augmenting the KITTI LiDAR dataset.\nExperiments on this dataset reveal that the proposed approach significantly\noutperforms state-of-the-art pairwise point cloud registration methods by a\nlarge margin, resulting in 78% improvement in Registration Recall when the\ntarget point cloud is about 10$\\times$ spatially larger and about 10$\\times$\ntimes denser than the query point cloud.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kanghee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiaTrans: Siamese Transformer Network for RGB-D Salient Object Detection with Depth Image Classification. (arXiv:2207.04224v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04224","description":"<p>RGB-D SOD uses depth information to handle challenging scenes and obtain\nhigh-quality saliency maps. Existing state-of-the-art RGB-D saliency detection\nmethods overwhelmingly rely on the strategy of directly fusing depth\ninformation. Although these methods improve the accuracy of saliency prediction\nthrough various cross-modality fusion strategies, misinformation provided by\nsome poor-quality depth images can affect the saliency prediction result. To\naddress this issue, a novel RGB-D salient object detection model (SiaTrans) is\nproposed in this paper, which allows training on depth image quality\nclassification at the same time as training on SOD. In light of the common\ninformation between RGB and depth images on salient objects, SiaTrans uses a\nSiamese transformer network with shared weight parameters as the encoder and\nextracts RGB and depth features concatenated on the batch dimension, saving\nspace resources without compromising performance. SiaTrans uses the Class token\nin the backbone network (T2T-ViT) to classify the quality of depth images\nwithout preventing the token sequence from going on with the saliency detection\ntask. Transformer-based cross-modality fusion module (CMF) can effectively fuse\nRGB and depth information. And in the testing process, CMF can choose to fuse\ncross-modality information or enhance RGB information according to the quality\nclassification signal of the depth image. The greatest benefit of our designed\nCMF and decoder is that they maintain the consistency of RGB and RGB-D\ninformation decoding: SiaTrans decodes RGB-D or RGB information under the same\nmodel parameters according to the classification signal during testing.\nComprehensive experiments on nine RGB-D SOD benchmark datasets show that\nSiaTrans has the best overall performance and the least computation compared\nwith recent state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xingzhao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changlei_D/0/1/0/all/0/1\">Dongye Changlei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yanjun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Batch-efficient EigenDecomposition for Small and Medium Matrices. (arXiv:2207.04228v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04228","description":"<p>EigenDecomposition (ED) is at the heart of many computer vision algorithms\nand applications. One crucial bottleneck limiting its usage is the expensive\ncomputation cost, particularly for a mini-batch of matrices in the deep neural\nnetworks. In this paper, we propose a QR-based ED method dedicated to the\napplication scenarios of computer vision. Our proposed method performs the ED\nentirely by batched matrix/vector multiplication, which processes all the\nmatrices simultaneously and thus fully utilizes the power of GPUs. Our\ntechnique is based on the explicit QR iterations by Givens rotation with double\nWilkinson shifts. With several acceleration techniques, the time complexity of\nQR iterations is reduced from $O{(}n^5{)}$ to $O{(}n^3{)}$. The numerical test\nshows that for small and medium batched matrices (\\emph{e.g.,} $dim{&lt;}32$) our\nmethod can be much faster than the Pytorch SVD function. Experimental results\non visual recognition and image generation demonstrate that our methods also\nachieve competitive performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Ellipsometry: Portable Acquisition of Polarimetric SVBRDF and Shape with Unstructured Flash Photography. (arXiv:2207.04236v1 [cs.GR])","link":"http://arxiv.org/abs/2207.04236","description":"<p>Ellipsometry techniques allow to measure polarization information of\nmaterials, requiring precise rotations of optical components with different\nconfigurations of lights and sensors. This results in cumbersome capture\ndevices, carefully calibrated in lab conditions, and in very long acquisition\ntimes, usually in the order of a few days per object. Recent techniques allow\nto capture polarimetric spatially-varying reflectance information, but limited\nto a single view, or to cover all view directions, but limited to spherical\nobjects made of a single homogeneous material. We present sparse ellipsometry,\na portable polarimetric acquisition method that captures both polarimetric\nSVBRDF and 3D shape simultaneously. Our handheld device consists of\noff-the-shelf, fixed optical components. Instead of days, the total acquisition\ntime varies between twenty and thirty minutes per object. We develop a complete\npolarimetric SVBRDF model that includes diffuse and specular components, as\nwell as single scattering, and devise a novel polarimetric inverse rendering\nalgorithm with data augmentation of specular reflection samples via generative\nmodeling. Our results show a strong agreement with a recent ground-truth\ndataset of captured polarimetric BRDFs of real-world objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inseung Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_D/0/1/0/all/0/1\">Daniel S. Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_A/0/1/0/all/0/1\">Adolfo Mu&#xf1;oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1\">Diego Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min H. Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PI-Trans: Parallel-ConvMLP and Implicit-Transformation Based GAN for Cross-View Image Translation. (arXiv:2207.04242v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04242","description":"<p>For semantic-guided cross-view image translation, it is crucial to learn\nwhere to sample pixels from the source view image and where to reallocate them\nguided by the target view semantic map, especially when there is little overlap\nor drastic view difference between the source and target images. Hence, one not\nonly needs to encode the long-range dependencies among pixels in both the\nsource view image and target view the semantic map but also needs to translate\nthese learned dependencies. To this end, we propose a novel generative\nadversarial network, PI-Trans, which mainly consists of a novel\nParallel-ConvMLP module and an Implicit Transformation module at multiple\nsemantic levels. Extensive experimental results show that the proposed PI-Trans\nachieves the best qualitative and quantitative performance by a large margin\ncompared to the state-of-the-art methods on two challenging datasets. The code\nwill be made available at https://github.com/Amazingren/PI-Trans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving saliency models' predictions of the next fixation with humans' intrinsic cost of gaze shifts. (arXiv:2207.04250v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04250","description":"<p>The human prioritization of image regions can be modeled in a time invariant\nfashion with saliency maps or sequentially with scanpath models. However, while\nboth types of models have steadily improved on several benchmarks and datasets,\nthere is still a considerable gap in predicting human gaze. Here, we leverage\ntwo recent developments to reduce this gap: theoretical analyses establishing a\nprincipled framework for predicting the next gaze target and the empirical\nmeasurement of the human cost for gaze switches independently of image content.\nWe introduce an algorithm in the framework of sequential decision making, which\nconverts any static saliency map into a sequence of dynamic history-dependent\nvalue maps, which are recomputed after each gaze shift. These maps are based on\n1) a saliency map provided by an arbitrary saliency model, 2) the recently\nmeasured human cost function quantifying preferences in magnitude and direction\nof eye movements, and 3) a sequential exploration bonus, which changes with\neach subsequent gaze shift. The parameters of the spatial extent and temporal\ndecay of this exploration bonus are estimated from human gaze data. The\nrelative contributions of these three components were optimized on the MIT1003\ndataset for the NSS score and are sufficient to significantly outperform\npredictions of the next gaze target on NSS and AUC scores for five state of the\nart saliency models on three image data sets. Thus, we provide an\nimplementation of human gaze preferences, which can be used to improve\narbitrary saliency models' predictions of humans' next gaze targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadner_F/0/1/0/all/0/1\">Florian Kadner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_T/0/1/0/all/0/1\">Tobias Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoppe_D/0/1/0/all/0/1\">David Hoppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1\">Constantin A. Rothkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank-Enhanced Low-Dimensional Convolution Set for Hyperspectral Image Denoising. (arXiv:2207.04266v1 [eess.IV])","link":"http://arxiv.org/abs/2207.04266","description":"<p>This paper tackles the challenging problem of hyperspectral (HS) image\ndenoising. Unlike existing deep learning-based methods usually adopting\ncomplicated network architectures or empirically stacking off-the-shelf modules\nto pursue performance improvement, we focus on the efficient and effective\nfeature extraction manner for capturing the high-dimensional characteristics of\nHS images. To be specific, based on the theoretical analysis that increasing\nthe rank of the matrix formed by the unfolded convolutional kernels can promote\nfeature diversity, we propose rank-enhanced low-dimensional convolution set\n(Re-ConvSet), which separately performs 1-D convolution along the three\ndimensions of an HS image side-by-side, and then aggregates the resulting\nspatial-spectral embeddings via a learnable compression layer. Re-ConvSet not\nonly learns the diverse spatial-spectral features of HS images, but also\nreduces the parameters and complexity of the network. We then incorporate\nRe-ConvSet into the widely-used U-Net architecture to construct an HS image\ndenoising method. Surprisingly, we observe such a concise framework outperforms\nthe most recent method to a large extent in terms of quantitative metrics,\nvisual results, and efficiency. We believe our work may shed light on deep\nlearning-based HS image processing and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Jinhui Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiyu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable AI (XAI) in Biomedical Signal and Image Processing: Promises and Challenges. (arXiv:2207.04295v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04295","description":"<p>Artificial intelligence has become pervasive across disciplines and fields,\nand biomedical image and signal processing is no exception. The growing and\nwidespread interest on the topic has triggered a vast research activity that is\nreflected in an exponential research effort. Through study of massive and\ndiverse biomedical data, machine and deep learning models have revolutionized\nvarious tasks such as modeling, segmentation, registration, classification and\nsynthesis, outperforming traditional techniques. However, the difficulty in\ntranslating the results into biologically/clinically interpretable information\nis preventing their full exploitation in the field. Explainable AI (XAI)\nattempts to fill this translational gap by providing means to make the models\ninterpretable and providing explanations. Different solutions have been\nproposed so far and are gaining increasing interest from the community. This\npaper aims at providing an overview on XAI in biomedical data processing and\npoints to an upcoming Special Issue on Deep Learning in Biomedical Image and\nSignal Processing of the IEEE Signal Processing Magazine that is going to\nappear in March 2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Arvind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Maloigne_C/0/1/0/all/0/1\">Christine Fernandez-Maloigne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1\">Vince Calhoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menegaz_G/0/1/0/all/0/1\">Gloria Menegaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHDM-NET: Heat Map Detail Guidance with Image Matting for Industrial Weld Semantic Segmentation Network. (arXiv:2207.04297v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04297","description":"<p>In actual industrial production, the assessment of the steel plate welding\neffect is an important task, and the segmentation of the weld section is the\nbasis of the assessment. This paper proposes an industrial weld segmentation\nnetwork based on a deep learning semantic segmentation algorithm fused with\nheatmap detail guidance and Image Matting to solve the automatic segmentation\nproblem of weld regions. In the existing semantic segmentation networks, the\nboundary information can be preserved by fusing the features of both high-level\nand low-level layers. However, this method can lead to insufficient expression\nof the spatial information in the low-level layer, resulting in inaccurate\nsegmentation boundary positioning. We propose a detailed guidance module based\non heatmaps to fully express the segmented region boundary information in the\nlow-level network to address this problem. Specifically, the expression of\nboundary information can be enhanced by adding a detailed branch to predict\nsegmented boundary and then matching it with the boundary heat map generated by\nmask labels to calculate the mean square error loss. In addition, although deep\nlearning has achieved great success in the field of semantic segmentation, the\nprecision of the segmentation boundary region is not high due to the loss of\ndetailed information caused by the classical segmentation network in the\nprocess of encoding and decoding process. This paper introduces a matting\nalgorithm to calibrate the boundary of the segmentation region of the semantic\nsegmentation network to solve this problem. Through many experiments on\nindustrial weld data sets, the effectiveness of our method is demonstrated, and\nthe MIOU reaches 97.93%. It is worth noting that this performance is comparable\nto human manual segmentation ( MIOU 97.96%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jingwu Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QKVA grid: Attention in Image Perspective and Stacked DETR. (arXiv:2207.04313v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04313","description":"<p>We present a new model named Stacked-DETR(SDETR), which inherits the main\nideas in canonical DETR. We improve DETR in two directions: simplifying the\ncost of training and introducing the stacked architecture to enhance the\nperformance. To the former, we focus on the inside of the Attention block and\npropose the QKVA grid, a new perspective to describe the process of attention.\nBy this, we can step further on how Attention works for image problems and the\neffect of multi-head. These two ideas contribute the design of single-head\nencoder-layer. To the latter, SDETR reaches great improvement(+1.1AP, +3.4APs)\nto DETR. Especially to the performance on small objects, SDETR achieves better\nresults to the optimized Faster R-CNN baseline, which was a shortcoming in\nDETR. Our changes are based on the code of DETR. Training code and pretrained\nmodels are available at https://github.com/shengwenyuan/sdetr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_W/0/1/0/all/0/1\">Wenyuan Sheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Diffusion Model Efficiency Through Patching. (arXiv:2207.04316v1 [cs.LG])","link":"http://arxiv.org/abs/2207.04316","description":"<p>Diffusion models are a powerful class of generative models that iteratively\ndenoise samples to produce data. While many works have focused on the number of\niterations in this sampling procedure, few have focused on the cost of each\niteration. We find that adding a simple ViT-style patching transformation can\nconsiderably reduce a diffusion model's sampling time and memory usage. We\njustify our approach both through an analysis of the diffusion model objective,\nand through empirical experiments on LSUN Church, ImageNet 256, and FFHQ 1024.\nWe provide implementations in Tensorflow and Pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luhman_T/0/1/0/all/0/1\">Troy Luhman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luhman_E/0/1/0/all/0/1\">Eric Luhman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet. (arXiv:2207.04320v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04320","description":"<p>Multi-person pose understanding from RGB videos includes three complex tasks:\npose estimation, tracking and motion forecasting. Among these three tasks, pose\nestimation and tracking are correlated, and tracking is crucial to motion\nforecasting. Most existing works either focus on a single task or employ\ncascaded methods to solve each individual task separately. In this paper, we\npropose Snipper, a framework to perform multi-person 3D pose estimation,\ntracking and motion forecasting simultaneously in a single inference.\nSpecifically, we first propose a deformable attention mechanism to aggregate\nspatiotemporal information from video snippets. Building upon this deformable\nattention, a visual transformer is learned to encode the spatiotemporal\nfeatures from multi-frame images and to decode informative pose features to\nupdate multi-person pose queries. Last, these queries are regressed to predict\nmulti-person pose trajectories and future motions in one forward pass. In the\nexperiments, we show the effectiveness of Snipper on three challenging public\ndatasets where a generic model rivals specialized state-of-art baselines for\npose estimation, tracking, and forecasting. Code is available at\n\\href{https://github.com/JimmyZou/Snipper}{https://github.com/JimmyZou/Snipper}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shihao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lingni Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh Vo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Coding Using Learned Latent GAN Compression. (arXiv:2207.04324v1 [eess.IV])","link":"http://arxiv.org/abs/2207.04324","description":"<p>We propose in this paper a new paradigm for facial video compression. We\nleverage the generative capacity of GANs such as StyleGAN to represent and\ncompress a video, including intra and inter compression. Each frame is inverted\nin the latent space of StyleGAN, from which the optimal compression is learned.\nTo do so, a diffeomorphic latent representation is learned using a normalizing\nflows model, where an entropy model can be optimized for image coding. In\naddition, we propose a new perceptual loss that is more efficient than other\ncounterparts. Finally, an entropy model for video inter coding with residual is\nalso learned in the previously constructed latent representation. Our method\n(SGANC) is simple, faster to train, and achieves better results for image and\nvideo coding compared to state-of-the-art codecs such as VTM, AV1, and recent\ndeep learning techniques. In particular, it drastically minimizes perceptual\ndistortion at low bit rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shukor_M/0/1/0/all/0/1\">Mustafa Shukor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bushan Damodaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Joint Image Transfer and Uncertainty Quantification using Patch Invariant Networks. (arXiv:2207.04325v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04325","description":"<p>Unsupervised image transfer enables intra- and inter-modality transfer for\nmedical applications where a large amount of paired training data is not\nabundant. To ensure a structure-preserving mapping from the input to the target\ndomain, existing methods for unpaired medical image transfer are commonly based\non cycle-consistency, causing additional computation resources and instability\ndue to the learning of an inverse mapping. This paper presents a novel method\nfor uni-directional domain mapping where no paired data is needed throughout\nthe entire training process. A reasonable transfer is ensured by employing the\nGAN architecture and a novel generator loss based on patch invariance. To be\nmore precise, generator outputs are evaluated and compared on different scales,\nwhich brings increased attention to high-frequency details as well as implicit\ndata augmentation. This novel term also gives the opportunity to predict\naleatoric uncertainty by modeling an input-dependent scale map for the patch\nresiduals. The proposed method is comprehensively evaluated on three renowned\nmedical databases. Superior accuracy on these datasets compared to four\ndifferent state-of-the-art methods for unpaired image transfer suggests the\ngreat potential of this approach for uncertainty-aware medical image\ntranslation. Implementation of the proposed framework is released here:\nhttps://github.com/anger-man/unsupervised-image-transfer-and-uq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siyal_A/0/1/0/all/0/1\">Ahsan Raza Siyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Chest X-ray Pathologies in Natural Language. (arXiv:2207.04343v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04343","description":"<p>Most deep learning algorithms lack explanations for their predictions, which\nlimits their deployment in clinical practice. Approaches to improve\nexplainability, especially in medical imaging, have often been shown to convey\nlimited information, be overly reassuring, or lack robustness. In this work, we\nintroduce the task of generating natural language explanations (NLEs) to\njustify predictions made on medical images. NLEs are human-friendly and\ncomprehensive, and enable the training of intrinsically explainable models. To\nthis goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging\ndataset with NLEs. It contains over 38,000 NLEs, which explain the presence of\nvarious thoracic pathologies and chest X-ray findings. We propose a general\napproach to solve the task and evaluate several architectures on this dataset,\nincluding via clinician assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsons_G/0/1/0/all/0/1\">Guy Parsons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papiez_B/0/1/0/all/0/1\">Bartlomiej Papiez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of Blood Vessels, Optic Disc Localization, Detection of Exudates and Diabetic Retinopathy Diagnosis from Digital Fundus Images. (arXiv:2207.04345v1 [eess.IV])","link":"http://arxiv.org/abs/2207.04345","description":"<p>Diabetic Retinopathy (DR) is a complication of long-standing, unchecked\ndiabetes and one of the leading causes of blindness in the world. This paper\nfocuses on improved and robust methods to extract some of the features of DR,\nviz. Blood Vessels and Exudates. Blood vessels are segmented using multiple\nmorphological and thresholding operations. For the segmentation of exudates,\nk-means clustering and contour detection on the original images are used.\nExtensive noise reduction is performed to remove false positives from the\nvessel segmentation algorithm's results. The localization of Optic Disc using\nk-means clustering and template matching is also performed. Lastly, this paper\npresents a Deep Convolutional Neural Network (DCNN) model with 14 Convolutional\nLayers and 2 Fully Connected Layers, for the automatic, binary diagnosis of DR.\nThe vessel segmentation, optic disc localization and DCNN achieve accuracies of\n95.93%, 98.77% and 75.73% respectively. The source code and pre-trained model\nare available https://github.com/Sohambasu07/DR_2021\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Basu_S/0/1/0/all/0/1\">Soham Basu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sayantan Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Ankit Bhattacharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sen_A/0/1/0/all/0/1\">Anindya Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays. (arXiv:2207.04394v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04394","description":"<p>Before the recent success of deep learning methods for automated medical\nimage analysis, practitioners used handcrafted radiomic features to\nquantitatively describe local patches of medical images. However, extracting\ndiscriminative radiomic features relies on accurate pathology localization,\nwhich is difficult to acquire in real-world settings. Despite advances in\ndisease classification and localization from chest X-rays, many approaches fail\nto incorporate clinically-informed domain knowledge. For these reasons, we\npropose a Radiomics-Guided Transformer (RGT) that fuses \\textit{global} image\ninformation with \\textit{local} knowledge-guided radiomics information to\nprovide accurate cardiopulmonary pathology localization and classification\n\\textit{without any bounding box annotations}. RGT consists of an image\nTransformer branch, a radiomics Transformer branch, and fusion layers that\naggregate image and radiomic information. Using the learned self-attention of\nits image branch, RGT extracts a bounding box for which to compute radiomic\nfeatures, which are further processed by the radiomics branch; learned image\nand radiomic features are then fused and mutually interact via cross-attention\nlayers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap\naccurate pathology localization only using image-level disease labels.\nExperiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior\nworks in weakly supervised disease localization (by an average margin of 3.6\\%\nover various intersection-over-union thresholds) and classification (by 1.1\\%\nin average area under the receiver operating characteristic curve). Code and\ntrained models will be released upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holste_G/0/1/0/all/0/1\">Gregory Holste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds. (arXiv:2207.04397v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04397","description":"<p>As camera and LiDAR sensors capture complementary information used in\nautonomous driving, great efforts have been made to develop semantic\nsegmentation algorithms through multi-modality data fusion. However,\nfusion-based approaches require paired data, i.e., LiDAR point clouds and\ncamera images with strict point-to-pixel mappings, as the inputs in both\ntraining and inference, which seriously hinders their application in practical\nscenarios. Thus, in this work, we propose the 2D Priors Assisted Semantic\nSegmentation (2DPASS), a general training scheme, to boost the representation\nlearning on point clouds, by fully taking advantage of 2D images with rich\nappearance. In practice, by leveraging an auxiliary modal fusion and\nmulti-scale fusion-to-single knowledge distillation (MSFSKD), 2DPASS acquires\nricher semantic and structural information from the multi-modal data, which are\nthen online distilled to the pure 3D network. As a result, equipped with\n2DPASS, our baseline shows significant improvement with only point cloud\ninputs. Specifically, it achieves the state-of-the-arts on two large-scale\nbenchmarks (i.e. SemanticKITTI and NuScenes), including top-1 results in both\nsingle and multiple scan(s) competitions of SemanticKITTI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiantao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaoda Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shenghui Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning with Local Contrastive Loss for Detection and Semantic Segmentation. (arXiv:2207.04398v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04398","description":"<p>We present a self-supervised learning (SSL) method suitable for semi-global\ntasks such as object detection and semantic segmentation. We enforce local\nconsistency between self-learned features, representing corresponding image\nlocations of transformed versions of the same image, by minimizing a\npixel-level local contrastive (LC) loss during training. LC-loss can be added\nto existing self-supervised learning methods with minimal overhead. We evaluate\nour SSL approach on two downstream tasks -- object detection and semantic\nsegmentation, using COCO, PASCAL VOC, and CityScapes datasets. Our method\noutperforms the existing state-of-the-art SSL approaches by 1.9% on COCO object\ndetection, 1.4% on PASCAL VOC detection, and 0.6% on CityScapes segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashraful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundell_B/0/1/0/all/0/1\">Ben Lundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawhney_H/0/1/0/all/0/1\">Harpreet Sawhney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sudipta Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_P/0/1/0/all/0/1\">Peter Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1\">Richard J. Radke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Horizontal and Vertical Attention in Transformers. (arXiv:2207.04399v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04399","description":"<p>Transformers are built upon multi-head scaled dot-product attention and\npositional encoding, which aim to learn the feature representations and token\ndependencies. In this work, we focus on enhancing the distinctive\nrepresentation by learning to augment the feature maps with the self-attention\nmechanism in Transformers. Specifically, we propose the horizontal attention to\nre-weight the multi-head output of the scaled dot-product attention before\ndimensionality reduction, and propose the vertical attention to adaptively\nre-calibrate channel-wise feature responses by explicitly modelling\ninter-dependencies among different channels. We demonstrate the Transformer\nmodels equipped with the two attentions have a high generalization capability\nacross different supervised learning tasks, with a very minor additional\ncomputational cost overhead. The proposed horizontal and vertical attentions\nare highly modular, which can be inserted into various Transformer models to\nfurther improve the performance. Our code is available in the supplementary\nmaterial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Litao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-attention on Multi-Shifted Windows for Scene Segmentation. (arXiv:2207.04403v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04403","description":"<p>Scene segmentation in images is a fundamental yet challenging problem in\nvisual content understanding, which is to learn a model to assign every image\npixel to a categorical label. One of the challenges for this learning task is\nto consider the spatial and semantic relationships to obtain descriptive\nfeature representations, so learning the feature maps from multiple scales is a\ncommon practice in scene segmentation. In this paper, we explore the effective\nuse of self-attention within multi-scale image windows to learn descriptive\nvisual features, then propose three different strategies to aggregate these\nfeature maps to decode the feature representation for dense prediction. Our\ndesign is based on the recently proposed Swin Transformer models, which totally\ndiscards convolution operations. With the simple yet effective multi-scale\nfeature learning and aggregation, our models achieve very promising performance\non four public scene segmentation datasets, PASCAL VOC2012, COCO-Stuff 10K,\nADE20K and Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Litao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhibin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition. (arXiv:2207.04410v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04410","description":"<p>The Transformer-based encoder-decoder architecture has recently made\nsignificant advances in recognizing handwritten mathematical expressions.\nHowever, the transformer model still suffers from the lack of coverage problem,\nmaking its expression recognition rate (ExpRate) inferior to its RNN\ncounterpart. Coverage information, which records the alignment information of\nthe past steps, has proven effective in the RNN models. In this paper, we\npropose CoMER, a model that adopts the coverage information in the transformer\ndecoder. Specifically, we propose a novel Attention Refinement Module (ARM) to\nrefine the attention weights with past alignment information without hurting\nits parallelism. Furthermore, we take coverage information to the extreme by\nproposing self-coverage and cross-coverage, which utilize the past alignment\ninformation from the current and previous layers. Experiments show that CoMER\nimproves the ExpRate by 0.61%/2.09%/1.59% compared to the current\nstate-of-the-art model, and reaches 59.33%/59.81%/62.97% on the CROHME\n2014/2016/2019 test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Liangcai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SFNet: Faster, Accurate, and Domain Agnostic Semantic Segmentation via Semantic Flow. (arXiv:2207.04415v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04415","description":"<p>In this paper, we focus on exploring effective methods for faster, accurate,\nand domain agnostic semantic segmentation. Inspired by the Optical Flow for\nmotion alignment between adjacent video frames, we propose a Flow Alignment\nModule (FAM) to learn \\textit{Semantic Flow} between feature maps of adjacent\nlevels, and broadcast high-level features to high resolution features\neffectively and efficiently. Furthermore, integrating our FAM to a common\nfeature pyramid structure exhibits superior performance over other real-time\nmethods even on light-weight backbone networks, such as ResNet-18 and DFNet.\nThen to further speed up the inference procedure, we also present a novel Gated\nDual Flow Alignment Module to directly align high resolution feature maps and\nlow resolution feature maps where we term improved version network as\nSFNet-Lite. Extensive experiments are conducted on several challenging\ndatasets, where results show the effectiveness of both SFNet and SFNet-Lite. In\nparticular, the proposed SFNet-Lite series achieve 80.1 mIoU while running at\n60 FPS using ResNet-18 backbone and 78.8 mIoU while running at 120 FPS using\nSTDC backbone on RTX-3090. Moreover, we unify four challenging driving datasets\n(i.e., Cityscapes, Mapillary, IDD and BDD) into one large dataset, which we\nnamed Unified Driving Segmentation (UDS) dataset. It contains diverse domain\nand style information. We benchmark several representative works on UDS. Both\nSFNet and SFNet-Lite still achieve the best speed and accuracy trade-off on UDS\nwhich serves as a strong baseline in such a new challenging setting. All the\ncode and models are publicly available at https://github.com/lxtGH/SFSegNets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Correction Adaptation Network for Noisy Knowledge Transfer. (arXiv:2207.04423v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04423","description":"<p>Previous unsupervised domain adaptation (UDA) methods aim to promote target\nlearning via a single-directional knowledge transfer from label-rich source\ndomain to unlabeled target domain, while its reverse adaption from target to\nsource has not jointly been considered yet so far. In fact, in some real\nteaching practice, a teacher helps students learn while also gets promotion\nfrom students to some extent, which inspires us to explore a dual-directional\nknowledge transfer between domains, and thus propose a Dual-Correction\nAdaptation Network (DualCAN) in this paper. However, due to the asymmetrical\nlabel knowledge across domains, transfer from unlabeled target to labeled\nsource poses a more difficult challenge than the common source-to-target\ncounterpart. First, the target pseudo-labels predicted by source commonly\ninvolve noises due to model bias, hence in the reverse adaptation, they may\nhurt the source performance and bring a negative target-to-source transfer.\nSecondly, source domain usually contains innate noises, which will inevitably\naggravate the target noises, leading to noise amplification across domains. To\nthis end, we further introduce a Noise Identification and Correction (NIC)\nmodule to correct and recycle noises in both domains. To our best knowledge,\nthis is the first naive attempt of dual-directional adaptation for noisy UDA,\nand naturally applicable to noise-free UDA. A theory justification is given to\nstate the rationality of our intuition. Empirical results confirm the\neffectiveness of DualCAN with remarkable performance gains over\nstate-of-the-arts, particularly for extreme noisy tasks (e.g., ~+ 15% on Pw-&gt;Pr\nand Pr-&gt;Rw of Office-Home).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weiwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songcan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hiding Your Signals: A Security Analysis of PPG-based Biometric Authentication. (arXiv:2207.04434v1 [cs.CR])","link":"http://arxiv.org/abs/2207.04434","description":"<p>Recently, physiological signal-based biometric systems have received wide\nattention. Unlike traditional biometric features, physiological signals can not\nbe easily compromised (usually unobservable to human eyes).\nPhotoplethysmography (PPG) signal is easy to measure, making it more attractive\nthan many other physiological signals for biometric authentication. However,\nwith the advent of remote PPG (rPPG), unobservability has been challenged when\nthe attacker can remotely steal the rPPG signals by monitoring the victim's\nface, subsequently posing a threat to PPG-based biometrics. In PPG-based\nbiometric authentication, current attack approaches mandate the victim's PPG\nsignal, making rPPG-based attacks neglected. In this paper, we firstly analyze\nthe security of PPG-based biometrics, including user authentication and\ncommunication protocols. We evaluate the signal waveforms, heart rate and\ninter-pulse-interval information extracted by five rPPG methods, including four\ntraditional optical computing methods (CHROM, POS, LGI, PCA) and one deep\nlearning method (CL_rPPG). We conducted experiments on five datasets (PURE,\nUBFC_rPPG, UBFC_Phys, LGI_PPGI, and COHFACE) to collect a comprehensive set of\nresults. Our empirical studies show that rPPG poses a serious threat to the\nauthentication system. The success rate of the rPPG signal spoofing attack in\nthe user authentication system reached 0.35. The bit hit rate is 0.6 in\ninter-pulse-interval-based security protocols. Further, we propose an active\ndefence strategy to hide the physiological signals of the face to resist the\nattack. It reduces the success rate of rPPG spoofing attacks in user\nauthentication to 0.05. The bit hit rate was reduced to 0.5, which is at the\nlevel of a random guess. Our strategy effectively prevents the exposure of PPG\nsignals to protect users' sensitive physiological data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yonghang Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SRRT: Search Region Regulation Tracking. (arXiv:2207.04438v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04438","description":"<p>Dominant trackers generate a fixed-size rectangular region based on the\nprevious prediction or initial bounding box as the model input, i.e., search\nregion. While this manner leads to improved tracking efficiency, a fixed-size\nsearch region lacks flexibility and is likely to fail in cases, e.g., fast\nmotion and distractor interference. Trackers tend to lose the target object due\nto the limited search region or be interfered by distractors due to excessive\nsearch region. In this work, we propose a novel tracking paradigm, called\nSearch Region Regulation Tracking (SRRT), which applies a proposed search\nregion regulator to estimate an optimal search region dynamically for every\nframe. To adapt the object's appearance variation during tracking, we further\npropose a locking-state determined updating strategy for reference frame\nupdating. Our SRRT framework is very concise without fancy design, yet achieves\nevident improvements on the baselines and competitive results with other\nstate-of-the-art trackers on seven challenging benchmarks. On the large-scale\nLaSOT benchmark, our SRRT improves SiamRPN++ and TransT with the absolute gains\nof 4.6% and 3.1% in terms of AUC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenda Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mix-Teaching: A Simple, Unified and Effective Semi-Supervised Learning Framework for Monocular 3D Object Detection. (arXiv:2207.04448v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04448","description":"<p>Monocular 3D object detection is an essential perception task for autonomous\ndriving. However, the high reliance on large-scale labeled data make it costly\nand time-consuming during model optimization. To reduce such over-reliance on\nhuman annotations, we propose Mix-Teaching, an effective semi-supervised\nlearning framework applicable to employ both labeled and unlabeled images in\ntraining stage. Mix-Teaching first generates pseudo-labels for unlabeled images\nby self-training. The student model is then trained on the mixed images\npossessing much more intensive and precise labeling by merging instance-level\nimage patches into empty backgrounds or labeled images. This is the first to\nbreak the image-level limitation and put high-quality pseudo labels from multi\nframes into one image for semi-supervised training. Besides, as a result of the\nmisalignment between confidence score and localization quality, it's hard to\ndiscriminate high-quality pseudo-labels from noisy predictions using only\nconfidence-based criterion. To that end, we further introduce an\nuncertainty-based filter to help select reliable pseudo boxes for the above\nmixing operation. To the best of our knowledge, this is the first unified SSL\nframework for monocular 3D object detection. Mix-Teaching consistently improves\nMonoFlex and GUPNet by significant margins under various labeling ratios on\nKITTI dataset. For example, our method achieves around +6.34% AP@0.7\nimprovement against the GUPNet baseline on validation set when using only 10%\nlabeled data. Besides, by leveraging full training set and the additional 48K\nraw images of KITTI, it can further improve the MonoFlex by +4.65% improvement\non AP@0.7 for car detection, reaching 18.54% AP@0.7, which ranks the 1st place\namong all monocular based methods on KITTI test leaderboard. The code and\npretrained models will be released at\nhttps://github.com/yanglei18/Mix-Teaching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minghan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively-connected Light Field Network for Efficient View Synthesis. (arXiv:2207.04465v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04465","description":"<p>This paper presents a Progressively-connected Light Field network (ProLiF),\nfor the novel view synthesis of complex forward-facing scenes. ProLiF encodes a\n4D light field, which allows rendering a large batch of rays in one training\nstep for image- or patch-level losses. Directly learning a neural light field\nfrom images has difficulty in rendering multi-view consistent images due to its\nunawareness of the underlying 3D geometry. To address this problem, we propose\na progressive training scheme and regularization losses to infer the underlying\ngeometry during training, both of which enforce the multi-view consistency and\nthus greatly improves the rendering quality. Experiments demonstrate that our\nmethod is able to achieve significantly better rendering quality than the\nvanilla neural light fields and comparable results to NeRF-like rendering\nmethods on the challenging LLFF dataset and Shiny Object dataset. Moreover, we\ndemonstrate better compatibility with LPIPS loss to achieve robustness to\nvarying light conditions and CLIP loss to control the rendering style of the\nscene. Project page: https://totoro97.github.io/projects/prolif.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPText-DETR: Towards Better Scene Text Detection with Dynamic Points in Transformer. (arXiv:2207.04491v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04491","description":"<p>Recently, Transformer-based methods, which predict polygon points or Bezier\ncurve control points to localize texts, are quite popular in scene text\ndetection. However, the used point label form implies the reading order of\nhumans, which affects the robustness of Transformer model. As for the model\narchitecture, the formulation of queries used in decoder has not been fully\nexplored by previous methods. In this paper, we propose a concise dynamic point\nscene text detection Transformer network termed DPText-DETR, which directly\nuses point coordinates as queries and dynamically updates them between decoder\nlayers. We point out a simple yet effective positional point label form to\ntackle the side effect of the original one. Moreover, an Enhanced Factorized\nSelf-Attention module is designed to explicitly model the circular shape of\npolygon point sequences beyond non-local attention. Extensive experiments prove\nthe training efficiency, robustness, and state-of-the-art performance on\nvarious arbitrary shape scene text benchmarks. Beyond detector, we observe that\nexisting end-to-end spotters struggle to recognize inverse-like texts. To\nevaluate their performance objectively and facilitate future research, we\npropose an Inverse-Text test set containing 500 manually labeled images. The\ncode and Inverse-Text test set will be available at\nhttps://github.com/ymy-k/DPText-DETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Maoyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shanshan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Adaptive Unknown Authentication for Universal Domain Adaptation by Classifier Paradox. (arXiv:2207.04494v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04494","description":"<p>Universal domain adaptation (UniDA) is a general unsupervised domain\nadaptation setting, which addresses both domain and label shifts in adaptation.\nIts main challenge lies in how to identify target samples in unshared or\nunknown classes. Previous methods commonly strive to depict sample \"confidence\"\nalong with a threshold for rejecting unknowns, and align feature distributions\nof shared classes across domains. However, it is still hard to pre-specify a\n\"confidence\" criterion and threshold which are adaptive to various real tasks,\nand a mis-prediction of unknowns further incurs misalignment of features in\nshared classes. In this paper, we propose a new UniDA method with adaptive\nUnknown Authentication by Classifier Paradox (UACP), considering that samples\nwith paradoxical predictions are probably unknowns belonging to none of the\nsource classes. In UACP, a composite classifier is jointly designed with two\ntypes of predictors. That is, a multi-class (MC) predictor classifies samples\nto one of the multiple source classes, while a binary one-vs-all (OVA)\npredictor further verifies the prediction by MC predictor. Samples with\nverification failure or paradox are identified as unknowns. Further, instead of\nfeature alignment for shared classes, implicit domain alignment is conducted in\noutput space such that samples across domains share the same decision boundary,\nthough with feature discrepancy. Empirical results validate UACP under both\nopen-set and universal UDA settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songcan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-based Monocular 3D Reconstruction of Birds: A Contemporary Survey. (arXiv:2207.04512v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04512","description":"<p>In nature, the collective behavior of animals, such as flying birds is\ndominated by the interactions between individuals of the same species. However,\nthe study of such behavior among the bird species is a complex process that\nhumans cannot perform using conventional visual observational techniques such\nas focal sampling in nature. For social animals such as birds, the mechanism of\ngroup formation can help ecologists understand the relationship between social\ncues and their visual characteristics over time (e.g., pose and shape). But,\nrecovering the varying pose and shapes of flying birds is a highly challenging\nproblem. A widely-adopted solution to tackle this bottleneck is to extract the\npose and shape information from 2D image to 3D correspondence. Recent advances\nin 3D vision have led to a number of impressive works on the 3D shape and pose\nestimation, each with different pros and cons. To the best of our knowledge,\nthis work is the first attempt to provide an overview of recent advances in 3D\nbird reconstruction based on monocular vision, give both computer vision and\nbiology researchers an overview of existing approaches, and compare their\ncharacteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahromi_M/0/1/0/all/0/1\">Mohammad N.S. Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaghanix_J/0/1/0/all/0/1\">Javad Khaghanix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodsman_D/0/1/0/all/0/1\">Devin Goodsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1\">Nadir Erbilgin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facilitated machine learning for image-based fruit quality assessment in developing countries. (arXiv:2207.04523v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04523","description":"<p>Automated image classification is a common task for supervised machine\nlearning in food science. An example is the image-based classification of the\nfruit's external quality or ripeness. For this purpose, deep convolutional\nneural networks (CNNs) are typically used. These models usually require a large\nnumber of labeled training samples and enhanced computational resources. While\ncommercial fruit sorting lines readily meet these requirements, the use of\nmachine learning approaches can be hindered by these prerequisites, especially\nfor smallholder farmers in the developing world. We propose an alternative\nmethod based on pre-trained vision transformers (ViTs) that is particularly\nsuitable for domains with low availability of data and limited computational\nresources. It can be easily implemented with limited resources on a standard\ndevice, which can democratize the use of these models for smartphone-based\nimage classification in developing countries. We demonstrate the\ncompetitiveness of our method by benchmarking two different classification\ntasks on domain data sets of banana and apple fruits with well-established CNN\napproaches. Our method achieves a classification accuracy of less than one\npercent below the best-performing CNN (0.950 vs. 0.958) on a training data set\nof 3745 images. At the same time, our method is superior when only a small\nnumber of labeled training samples is available. It requires three times less\ndata to achieve a 0.90 accuracy compared to CNNs. In addition, visualizations\nof low-dimensional feature embeddings show that the model used in our study\nextracts excellent features from unseen data without allocating labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knott_M/0/1/0/all/0/1\">Manuel Knott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1\">Fernando Perez-Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Defraeye_T/0/1/0/all/0/1\">Thijs Defraeye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Multi-Task RGB-D Scene Analysis for Indoor Environments. (arXiv:2207.04526v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04526","description":"<p>Semantic scene understanding is essential for mobile agents acting in various\nenvironments. Although semantic segmentation already provides a lot of\ninformation, details about individual objects as well as the general scene are\nmissing but required for many real-world applications. However, solving\nmultiple tasks separately is expensive and cannot be accomplished in real time\ngiven limited computing and battery capabilities on a mobile platform. In this\npaper, we propose an efficient multi-task approach for RGB-D scene\nanalysis~(EMSANet) that simultaneously performs semantic and instance\nsegmentation~(panoptic segmentation), instance orientation estimation, and\nscene classification. We show that all tasks can be accomplished using a single\nneural network in real time on a mobile platform without diminishing\nperformance - by contrast, the individual tasks are able to benefit from each\nother. In order to evaluate our multi-task approach, we extend the annotations\nof the common RGB-D indoor datasets NYUv2 and SUNRGB-D for instance\nsegmentation and orientation estimation. To the best of our knowledge, we are\nthe first to provide results in such a comprehensive multi-task setting for\nindoor scene analysis on NYUv2 and SUNRGB-D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seichter_D/0/1/0/all/0/1\">Daniel Seichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischedick_S/0/1/0/all/0/1\">S&#xf6;hnke Benedikt Fischedick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_M/0/1/0/all/0/1\">Mona K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gross_H/0/1/0/all/0/1\">Horst-Michael Gro&#xdf;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Open-Source Tool for Longitudinal Whole-Brain and White Matter Lesion Segmentation. (arXiv:2207.04534v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04534","description":"<p>In this paper we describe and validate a longitudinal method for whole-brain\nsegmentation of longitudinal MRI scans. It builds upon an existing whole-brain\nsegmentation method that can handle multi-contrast data and robustly analyze\nimages with white matter lesions. This method is here extended with\nsubject-specific latent variables that encourage temporal consistency between\nits segmentation results, enabling it to better track subtle morphological\nchanges in dozens of neuroanatomical structures and white matter lesions. We\nvalidate the proposed method on multiple datasets of control subjects and\npatients suffering from Alzheimer's disease and multiple sclerosis, and compare\nits results against those obtained with its original cross-sectional\nformulation and two benchmark longitudinal methods. The results indicate that\nthe method attains a higher test-retest reliability, while being more sensitive\nto longitudinal disease effect differences between patient groups. An\nimplementation is publicly available as part of the open-source neuroimaging\npackage FreeSurfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cerri_S/0/1/0/all/0/1\">Stefano Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greve_D/0/1/0/all/0/1\">Douglas N. Greve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoopes_A/0/1/0/all/0/1\">Andrew Hoopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundell_H/0/1/0/all/0/1\">Henrik Lundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebner_H/0/1/0/all/0/1\">Hartwig R. Siebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlau_M/0/1/0/all/0/1\">Mark M&#xfc;hlau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leemput_K/0/1/0/all/0/1\">Koen Van Leemput</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion. (arXiv:2207.04535v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04535","description":"<p>Attention-based models such as transformers have shown outstanding\nperformance on dense prediction tasks, such as semantic segmentation, owing to\ntheir capability of capturing long-range dependency in an image. However, the\nbenefit of transformers for monocular depth prediction has seldom been explored\nso far. This paper benchmarks various transformer-based models for the depth\nestimation task on an indoor NYUV2 dataset and an outdoor KITTI dataset. We\npropose a novel attention-based architecture, Depthformer for monocular depth\nestimation that uses multi-head self-attention to produce the multiscale\nfeature maps, which are effectively combined by our proposed decoder network.\nWe also propose a Transbins module that divides the depth range into bins whose\ncenter value is estimated adaptively per image. The final depth estimated is a\nlinear combination of bin centers for each pixel. Transbins module takes\nadvantage of the global receptive field using the transformer module in the\nencoding stage. Experimental results on NYUV2 and KITTI depth estimation\nbenchmark demonstrate that our proposed method improves the state-of-the-art by\n3.3%, and 3.3% respectively in terms of Root Mean Squared Error (RMSE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ashutosh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Perspective-aware Multiple Object Tracking. (arXiv:2207.04551v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04551","description":"<p>This paper aims to tackle Multiple Object Tracking (MOT), an important\nproblem in computer vision but remains challenging due to many practical\nissues, especially occlusions. Indeed, we propose a new real-time Depth\nPerspective-aware Multiple Object Tracking (DP-MOT) approach to tackle the\nocclusion problem in MOT. A simple yet efficient Subject-Ordered Depth\nEstimation (SODE) is first proposed to automatically order the depth positions\nof detected subjects in a 2D scene in an unsupervised manner. Using the output\nfrom SODE, a new Active pseudo-3D Kalman filter, a simple but effective\nextension of Kalman filter with dynamic control variables, is then proposed to\ndynamically update the movement of objects. In addition, a new high-order\nassociation approach is presented in the data association step to incorporate\nfirst-order and second-order relationships between the detected objects. The\nproposed approach consistently achieves state-of-the-art performance compared\nto recent MOT methods on standard MOT benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1\">Kha Gia Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Huu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Pha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tien Dai Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN-based Virtual Re-Staining: A Promising Solution for Whole Slide Image Analysis. (arXiv:1901.04059v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1901.04059","description":"<p>Histopathological cancer diagnosis is based on visual examination of stained\ntissue slides. Hematoxylin and eosin (H\\&amp;E) is a standard stain routinely\nemployed worldwide. It is easy to acquire and cost effective, but cells and\ntissue components show low-contrast with varying tones of dark blue and pink,\nwhich makes difficult visual assessments, digital image analysis, and\nquantifications. These limitations can be overcome by IHC staining of target\nproteins of the tissue slide. IHC provides a selective, high-contrast imaging\nof cells and tissue components, but their use is largely limited by a\nsignificantly more complex laboratory processing and high cost. We proposed a\nconditional CycleGAN (cCGAN) network to transform the H\\&amp;E stained images into\nIHC stained images, facilitating virtual IHC staining on the same slide. This\ndata-driven method requires only a limited amount of labelled data but will\ngenerate pixel level segmentation results. The proposed cCGAN model improves\nthe original network \\cite{zhu_unpaired_2017} by adding category conditions and\nintroducing two structural loss functions, which realize a multi-subdomain\ntranslation and improve the translation accuracy as well. % need to give\nreasons here. Experiments demonstrate that the proposed model outperforms the\noriginal method in unpaired image translation with multi-subdomains. We also\nexplore the potential of unpaired images to image translation method applied on\nother histology images related tasks with different staining techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhaoyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xingru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moro_C/0/1/0/all/0/1\">Carlos Fern&#xe1;ndez Moro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozoky_B/0/1/0/all/0/1\">B&#xe9;la Boz&#xf3;ky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning. (arXiv:2001.01385v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2001.01385","description":"<p>Classifiers trained with class-imbalanced data are known to perform poorly on\ntest data of the \"minor\" classes, of which we have insufficient training data.\nIn this paper, we investigate learning a ConvNet classifier under such a\nscenario. We found that a ConvNet significantly over-fits the minor classes,\nwhich is quite opposite to traditional machine learning algorithms that often\nunder-fit minor classes. We conducted a series of analysis and discovered the\nfeature deviation phenomenon -- the learned ConvNet generates deviated features\nbetween the training and test data of minor classes -- which explains how\nover-fitting happens. To compensate for the effect of feature deviation which\npushes test data toward low decision value regions, we propose to incorporate\nclass-dependent temperatures (CDT) in training a ConvNet. CDT simulates feature\ndeviation in the training phase, forcing the ConvNet to enlarge the decision\nvalues for minor-class data so that it can overcome real feature deviation in\nthe test phase. We validate our approach on benchmark datasets and achieve\npromising performance. We hope that our insights can inspire new ways of\nthinking in resolving class-imbalanced deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong-You Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Symmetry to Geometry: Tractable Nonconvex Problems. (arXiv:2007.06753v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.06753","description":"<p>As science and engineering have become increasingly data-driven, the role of\noptimization has expanded to touch almost every stage of the data analysis\npipeline, from signal and data acquisition to modeling and prediction. The\noptimization problems encountered in practice are often nonconvex. While\nchallenges vary from problem to problem, one common source of nonconvexity is\nnonlinearity in the data or measurement model. Nonlinear models often exhibit\nsymmetries, creating complicated, nonconvex objective landscapes, with multiple\nequivalent solutions. Nevertheless, simple methods (e.g., gradient descent)\noften perform surprisingly well in practice.\n</p>\n<p>The goal of this survey is to highlight a class of tractable nonconvex\nproblems, which can be understood through the lens of symmetries. These\nproblems exhibit a characteristic geometric structure: local minimizers are\nsymmetric copies of a single \"ground truth\" solution, while other critical\npoints occur at balanced superpositions of symmetric copies of the ground\ntruth, and exhibit negative curvature in directions that break the symmetry.\nThis structure enables efficient methods to obtain global minimizers. We\ndiscuss examples of this phenomenon arising from a wide range of problems in\nimaging, signal processing, and data analysis. We highlight the key role of\nsymmetry in shaping the objective landscape and discuss the different roles of\nrotational and discrete symmetries. This area is rich with observed phenomena\nand open problems; we close by highlighting directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1\">Qing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">John Wright</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-end Car License Plate Location and Recognition in Unconstrained Scenarios. (arXiv:2008.10916v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.10916","description":"<p>Benefiting from the rapid development of convolutional neural networks, the\nperformance of car license plate detection and recognition has been largely\nimproved. Nonetheless, most existing methods solve detection and recognition\nproblems separately, and focus on specific scenarios, which hinders the\ndeployment for real-world applications. To overcome these challenges, we\npresent an efficient and accurate framework to solve the license plate\ndetection and recognition tasks simultaneously. It is a lightweight and unified\ndeep neural network, that can be optimized end-to-end and work in real-time.\nSpecifically, for unconstrained scenarios, an anchor-free method is adopted to\nefficiently detect the bounding box and four corners of a license plate, which\nare used to extract and rectify the target region features. Then, a novel\nconvolutional neural network branch is designed to further extract features of\ncharacters without segmentation. Finally, the recognition task is treated as\nsequence labeling problems, which are solved by Connectionist Temporal\nClassification (CTC) directly. Several public datasets including images\ncollected from different scenarios under various conditions are chosen for\nevaluation. Experimental results indicate that the proposed method\nsignificantly outperforms the previous state-of-the-art methods in both speed\nand precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Shuxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amodal Segmentation through Out-of-Task and Out-of-Distribution Generalization with a Bayesian Model. (arXiv:2010.13175v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.13175","description":"<p>Amodal completion is a visual task that humans perform easily but which is\ndifficult for computer vision algorithms. The aim is to segment those object\nboundaries which are occluded and hence invisible. This task is particularly\nchallenging for deep neural networks because data is difficult to obtain and\nannotate. Therefore, we formulate amodal segmentation as an out-of-task and\nout-of-distribution generalization problem. Specifically, we replace the fully\nconnected classifier in neural networks with a Bayesian generative model of the\nneural network features. The model is trained from non-occluded images using\nbounding box annotations and class labels only, but is applied to generalize\nout-of-task to object segmentation and to generalize out-of-distribution to\nsegment occluded objects. We demonstrate how such Bayesian models can naturally\ngeneralize beyond the training task labels when they learn a prior that models\nthe object's background context and shape. Moreover, by leveraging an outlier\nprocess, Bayesian models can further generalize out-of-distribution to segment\npartially occluded objects and to predict their amodal object boundaries. Our\nalgorithm outperforms alternative methods that use the same supervision by a\nlarge margin, and even outperforms methods where annotated amodal segmentations\nare used during training, when the amount of occlusion is large. Code is\npublicly available at https://github.com/YihongSun/Bayesian-Amodal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yihong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal. (arXiv:2012.15685v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.15685","description":"<p>Single image crowd counting is a challenging computer vision problem with\nwide applications in public safety, city planning, traffic management, etc.\nWith the recent development of deep learning techniques, crowd counting has\naroused much attention and achieved great success in recent years. This survey\nis to provide a comprehensive summary of recent advances on deep learning-based\ncrowd counting techniques via density map estimation by systematically\nreviewing and summarizing more than 200 works in the area since 2015. Our goals\nare to provide an up-to-date review of recent approaches, and educate new\nresearchers in this field the design principles and trade-offs. After\npresenting publicly available datasets and evaluation metrics, we review the\nrecent advances with detailed comparisons on three major design modules for\ncrowd counting: deep neural network designs, loss functions, and supervisory\nsignals. We study and compare the approaches using the public datasets and\nevaluation metrics. We conclude the survey with some future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoyue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">S.-H. Gary Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Interactive Image Segmentation: Feature Space Annotation. (arXiv:2101.04378v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.04378","description":"<p>Despite the progress of interactive image segmentation methods, high-quality\npixel-level annotation is still time-consuming and laborious - a bottleneck for\nseveral deep learning applications. We take a step back to propose interactive\nand simultaneous segment annotation from multiple images guided by feature\nspace projection. This strategy is in stark contrast to existing interactive\nsegmentation methodologies, which perform annotation in the image domain. We\nshow that feature space annotation achieves competitive results with\nstate-of-the-art methods in foreground segmentation datasets: iCoSeg, DAVIS,\nand Rooftop. Moreover, in the semantic segmentation context, it achieves 91.5%\naccuracy in the Cityscapes dataset, being 74.75 times faster than the original\nannotation procedure. Further, our contribution sheds light on a novel\ndirection for interactive image annotation that can be integrated with existing\nmethodologies. The supplementary material presents video demonstrations. Code\navailable at\nhttps://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bragantini_J/0/1/0/all/0/1\">Jord{&#xe3;}o Bragantini</a> (IC), <a href=\"http://arxiv.org/find/cs/1/au:+Falc%7Ba%7Do_A/0/1/0/all/0/1\">Alexandre X Falc{&#xe3;}o</a> (IC), <a href=\"http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1\">Laurent Najman</a> (LIGM)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual explanation of black-box model: Similarity Difference and Uniqueness (SIDU) method. (arXiv:2101.10710v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.10710","description":"<p>Explainable Artificial Intelligence (XAI) has in recent years become a\nwell-suited framework to generate human understandable explanations of\n\"black-box\" models. In this paper, a novel XAI visual explanation algorithm\nknown as the Similarity Difference and Uniqueness (SIDU) method that can\neffectively localize entire object regions responsible for prediction is\npresented in full detail. The SIDU algorithm robustness and effectiveness is\nanalyzed through various computational and human subject experiments. In\nparticular, the SIDU algorithm is assessed using three different types of\nevaluations (Application, Human and Functionally-Grounded) to demonstrate its\nsuperior performance. The robustness of SIDU is further studied in the presence\nof adversarial attack on \"black-box\" models to better understand its\nperformance. Our code is available at:\nhttps://github.com/satyamahesh84/SIDU_XAI_CODE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muddamsetty_S/0/1/0/all/0/1\">Satya M. Muddamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahromi_M/0/1/0/all/0/1\">Mohammad N. S. Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciontos_A/0/1/0/all/0/1\">Andreea E. Ciontos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenoy_L/0/1/0/all/0/1\">Laura M. Fenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Camera Gain and Exposure Control for Improved Visual Feature Detection and Matching. (arXiv:2102.04341v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2102.04341","description":"<p>Successful visual navigation depends upon capturing images that contain\nsufficient useful information. In this letter, we explore a data-driven\napproach to account for environmental lighting changes, improving the quality\nof images for use in visual odometry (VO) or visual simultaneous localization\nand mapping (SLAM). We train a deep convolutional neural network model to\npredictively adjust camera gain and exposure time parameters such that\nconsecutive images contain a maximal number of matchable features. The training\nprocess is fully self-supervised: our training signal is derived from an\nunderlying VO or SLAM pipeline and, as a result, the model is optimized to\nperform well with that specific pipeline. We demonstrate through extensive\nreal-world experiments that our network can anticipate and compensate for\ndramatic lighting changes (e.g., transitions into and out of road tunnels),\nmaintaining a substantially higher number of inlier feature matches than\ncompeting camera parameter control algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomasi_J/0/1/0/all/0/1\">Justin Tomasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1\">Brandon Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven L. Waslander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Purified Feature Representations from Task-irrelevant Labels. (arXiv:2102.10955v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.10955","description":"<p>Learning an empirically effective model with generalization using limited\ndata is a challenging task for deep neural networks. In this paper, we propose\na novel learning framework called PurifiedLearning to exploit task-irrelevant\nfeatures extracted from task-irrelevant labels when training models on\nsmall-scale datasets. Particularly, we purify feature representations by using\nthe expression of task-irrelevant information, thus facilitating the learning\nprocess of classification. Our work is built on solid theoretical analysis and\nextensive experiments, which demonstrate the effectiveness of PurifiedLearning.\nAccording to the theory we proved, PurifiedLearning is model-agnostic and\ndoesn't have any restrictions on the model needed, so it can be combined with\nany existing deep neural networks with ease to achieve better performance. The\nsource code of this paper will be available in the future for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Playbacks in Unsupervised Domain Adaptation for 3D Object Detection. (arXiv:2103.14198v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14198","description":"<p>Self-driving cars must detect other vehicles and pedestrians in 3D to plan\nsafe routes and avoid collisions. State-of-the-art 3D object detectors, based\non deep learning, have shown promising accuracy but are prone to over-fit to\ndomain idiosyncrasies, making them fail in new environments -- a serious\nproblem if autonomous vehicles are meant to operate freely. In this paper, we\npropose a novel learning approach that drastically reduces this gap by\nfine-tuning the detector on pseudo-labels in the target domain, which our\nmethod generates while the vehicle is parked, based on replays of previously\nrecorded driving sequences. In these replays, objects are tracked over time,\nand detections are interpolated and extrapolated -- crucially, leveraging\nfuture information to catch hard cases. We show, on five autonomous driving\ndatasets, that fine-tuning the object detector on these pseudo-labels\nsubstantially reduces the domain gap to new driving environments, yielding\ndrastic improvements in accuracy and detection reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yurong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Ruiz_C/0/1/0/all/0/1\">Carlos Andres Diaz-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1\">Mark Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q Weinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised object detection from audio-visual correspondence. (arXiv:2104.06401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06401","description":"<p>We tackle the problem of learning object detectors without supervision.\nDifferently from weakly-supervised object detection, we do not assume\nimage-level class labels. Instead, we extract a supervisory signal from\naudio-visual data, using the audio component to \"teach\" the object detector.\nWhile this problem is related to sound source localisation, it is considerably\nharder because the detector must classify the objects by type, enumerate each\ninstance of the object, and do so even when the object is silent. We tackle\nthis problem by first designing a self-supervised framework with a contrastive\nobjective that jointly learns to classify and localise objects. Then, without\nusing any supervision, we simply use these self-supervised labels and boxes to\ntrain an image-based object detector. With this, we outperform previous\nunsupervised and weakly-supervised detectors for the task of object detection\nand sound source localization. We also show that we can align this detector to\nground-truth classes with as little as one label per pseudo-class, and show how\nour method can learn to detect generic objects that go beyond instruments, such\nas airplanes and cats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagan_F/0/1/0/all/0/1\">Francois Fagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exact Hypergraph Matching Algorithm for Nuclear Identification in Embryonic Caenorhabditis elegans. (arXiv:2104.10003v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10003","description":"<p>Finding an optimal correspondence between point sets is a common task in\ncomputer vision. Existing techniques assume relatively simple relationships\namong points and do not guarantee an optimal match. We introduce an algorithm\ncapable of exactly solving point set matching by modeling the task as\nhypergraph matching. The algorithm extends the classical branch and bound\nparadigm to select and aggregate vertices under a proposed decomposition of the\nmultilinear objective function. The methodology is motivated by Caenorhabditis\nelegans, a model organism used frequently in developmental biology and\nneurobiology. The embryonic C. elegans contains seam cells that can act as\nfiducial markers allowing the identification of other nuclei during embryo\ndevelopment. The proposed algorithm identifies seam cells more accurately than\nestablished point-set matching methods, while providing a framework to approach\nother similarly complex point set matching tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauziere_A/0/1/0/all/0/1\">Andrew Lauziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christensen_R/0/1/0/all/0/1\">Ryan Christensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_H/0/1/0/all/0/1\">Hari Shroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balan_R/0/1/0/all/0/1\">Radu Balan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FourierNets enable the design of highly non-local optical encoders for computational imaging. (arXiv:2104.10611v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.10611","description":"<p>Differentiable simulations of optical systems can be combined with deep\nlearning-based reconstruction networks to enable high performance computational\nimaging via end-to-end (E2E) optimization of both the optical encoder and the\ndeep decoder. This has enabled imaging applications such as 3D localization\nmicroscopy, depth estimation, and lensless photography via the optimization of\nlocal optical encoders. More challenging computational imaging applications,\nsuch as 3D snapshot microscopy which compresses 3D volumes into single 2D\nimages, require a highly non-local optical encoder. We show that existing deep\nnetwork decoders have a locality bias which prevents the optimization of such\nhighly non-local optical encoders. We address this with a decoder based on a\nshallow neural network architecture using global kernel Fourier convolutional\nneural networks (FourierNets). We show that FourierNets surpass existing deep\nnetwork based decoders at reconstructing photographs captured by the highly\nnon-local DiffuserCam optical encoder. Further, we show that FourierNets enable\nE2E optimization of highly non-local optical encoders for 3D snapshot\nmicroscopy. By combining FourierNets with a large-scale multi-GPU\ndifferentiable optical simulation, we are able to optimize non-local optical\nencoders 170$\\times$ to 7372$\\times$ larger than prior state of the art, and\ndemonstrate the potential for ROI-type specific optical encoding with a\nprogrammable microscope.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deb_D/0/1/0/all/0/1\">Diptodip Deb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1\">Zhenfei Jiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sims_R/0/1/0/all/0/1\">Ruth Sims</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alex B. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Broxton_M/0/1/0/all/0/1\">Michael Broxton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahrens_M/0/1/0/all/0/1\">Misha B. Ahrens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Podgorski_K/0/1/0/all/0/1\">Kaspar Podgorski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turaga_S/0/1/0/all/0/1\">Srinivas C. Turaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Neural Networks for Automatic Cell Tracking in Microscopy Image Sequences of Bacterial Colonies. (arXiv:2104.13482v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13482","description":"<p>Our work targets automated analysis to quantify the growth dynamics of a\npopulation of bacilliform bacteria. We propose an innovative approach to\nframe-sequence tracking of deformable-cell motion by the automated minimization\nof a new, specific cost functional. This minimization is implemented by\ndedicated Boltzmann machines (stochastic recurrent neural networks). Automated\ndetection of cell divisions is handled similarly by successive minimizations of\ntwo cost functions, alternating the identification of children pairs and parent\nidentification. We validate the proposed automatic cell tracking algorithm\nusing (i) recordings of simulated cell colonies that closely mimic the growth\ndynamics of E. coli in microfluidic traps and (ii) real data. On a batch of\n1100 simulated image frames, cell registration accuracies per frame ranged from\n94.5% to 100%, with a high average. Our initial tests using experimental image\nsequences (i.e., real data) of E. coli colonies also yield convincing results,\nwith a registration accuracy ranging from 90% to 100%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarmadi_S/0/1/0/all/0/1\">Sorena Sarmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkle_J/0/1/0/all/0/1\">James J. Winkle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnahhas_R/0/1/0/all/0/1\">Razan N. Alnahhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_M/0/1/0/all/0/1\">Matthew R. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Josic_K/0/1/0/all/0/1\">Kre&#x161;imir Josi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mang_A/0/1/0/all/0/1\">Andreas Mang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azencott_R/0/1/0/all/0/1\">Robert Azencott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN Cocktail: mixing GANs without dataset access. (arXiv:2106.03847v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03847","description":"<p>Today's generative models are capable of synthesizing high-fidelity images,\nbut each model specializes on a specific target domain. This raises the need\nfor model merging: combining two or more pretrained generative models into a\nsingle unified one. In this work we tackle the problem of model merging, given\ntwo constraints that often come up in the real world: (1) no access to the\noriginal training data, and (2) without increasing the size of the neural\nnetwork. To the best of our knowledge, model merging under these constraints\nhas not been studied thus far. We propose a novel, two-stage solution. In the\nfirst stage, we transform the weights of all the models to the same parameter\nspace by a technique we term model rooting. In the second stage, we merge the\nrooted models by averaging their weights and fine-tuning them for each specific\ndomain, using only data generated by the original trained models. We\ndemonstrate that our approach is superior to baseline methods and to existing\ntransfer learning techniques, and investigate several applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1\">Omri Avrahami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Coupling of Depth and Egomotion Networks for Self-Supervised Structure from Motion. (arXiv:2106.04007v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04007","description":"<p>Structure from motion (SfM) has recently been formulated as a self-supervised\nlearning problem, where neural network models of depth and egomotion are\nlearned jointly through view synthesis. Herein, we address the open problem of\nhow to best couple, or link, the depth and egomotion network components, so\nthat information such as a common scale factor can be shared between the\nnetworks. Towards this end, we introduce several notions of coupling,\ncategorize existing approaches, and present a novel tightly-coupled approach\nthat leverages the interdependence of depth and egomotion at training time and\nat test time. Our approach uses iterative view synthesis to recursively update\nthe egomotion network input, permitting contextual information to be passed\nbetween the components. We demonstrate through substantial experiments that our\napproach promotes consistency between the depth and egomotion predictions at\ntest time, improves generalization, and leads to state-of-the-art accuracy on\nindoor and outdoor depth and egomotion evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1\">Brandon Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peretroukhin_V/0/1/0/all/0/1\">Valentin Peretroukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06927","description":"<p>Despite unconditional feature inversion being the foundation of many image\nsynthesis applications, training an inverter demands a high computational\nbudget, large decoding capacity and imposing conditions such as autoregressive\npriors. To address these limitations, we propose the use of adversarially\nrobust representations as a perceptual primitive for feature inversion. We\ntrain an adversarially robust encoder to extract disentangled and\nperceptually-aligned image representations, making them easily invertible. By\ntraining a simple generator with the mirror architecture of the encoder, we\nachieve superior reconstruction quality and generalization over standard\nmodels. Based on this, we propose an adversarially robust autoencoder and\ndemonstrate its improved performance on style transfer, image denoising and\nanomaly detection tasks. Compared to recent ImageNet feature inversion methods,\nour model attains improved performance with significantly less complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1\">Renan A. Rojas-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1\">Raymond A. Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1\">Minh N. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Images Real Again: A Comprehensive Survey on Deep Image Composition. (arXiv:2106.14490v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14490","description":"<p>As a common image editing operation, image composition aims to cut the\nforeground from one image and paste it on another image, resulting in a\ncomposite image. However, there are many issues that could make the composite\nimages unrealistic. These issues can be summarized as the inconsistency between\nforeground and background, which includes appearance inconsistency (e.g.,\nincompatible illumination), geometry inconsistency (e.g., unreasonable size),\nand semantic inconsistency (e.g., mismatched semantic context). Previous works\ndivide image composition task into multiple sub-tasks, in which each sub-task\ntargets at one or more issues. Specifically, object placement aims to find\nreasonable scale, location, and shape for the foreground. Image blending aims\nto address the unnatural boundary between foreground and background. Image\nharmonization aims to adjust the illumination statistics of foreground. Shadow\ngeneration aims to generate plausible shadow for the foreground. By putting all\nthe abovementioned efforts together, we can acquire realistic composite images.\nTo the best of our knowledge, there is no previous survey on image composition.\nIn this paper, we conduct comprehensive survey over the sub-tasks of image\ncomposition. For each sub-task, we summarize the traditional methods, deep\nlearning based methods, datasets and evaluation. We also point out the\nlimitations of existing methods in each sub-task and the problem of the whole\nimage composition task. Datasets and codes for image composition are summarized\nat https://github.com/bcmi/Awesome-Image-Composition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1\">Wenyan Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.16245","description":"<p>Model-agnostic meta-learning (MAML) is arguably one of the most popular\nmeta-learning algorithms nowadays. Nevertheless, its performance on few-shot\nclassification is far behind many recent algorithms dedicated to the problem.\nIn this paper, we point out several key facets of how to train MAML to excel in\nfew-shot classification. First, we find that MAML needs a large number of\ngradient steps in its inner loop update, which contradicts its common usage in\nfew-shot classification. Second, we find that MAML is sensitive to the class\nlabel assignments during meta-testing. Concretely, MAML meta-trains the\ninitialization of an $N$-way classifier. These $N$ ways, during meta-testing,\nthen have \"$N!$\" different permutations to be paired with a few-shot task of\n$N$ novel classes. We find that these permutations lead to a huge variance of\naccuracy, making MAML unstable in few-shot classification. Third, we\ninvestigate several approaches to make MAML permutation-invariant, among which\nmeta-training a single vector to initialize all the $N$ weight vectors in the\nclassification head performs the best. On benchmark datasets like MiniImageNet\nand TieredImageNet, our approach, which we name UNICORN-MAML, performs on a par\nwith or even outperforms many recent few-shot classification algorithms,\nwithout sacrificing MAML's simplicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Learning with a Strong Teacher. (arXiv:2107.00197v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00197","description":"<p>Few-shot learning (FSL) aims to generate a classifier using limited labeled\nexamples. Many existing works take the meta-learning approach, constructing a\nfew-shot learner that can learn from few-shot examples to generate a\nclassifier. Typically, the few-shot learner is constructed or meta-trained by\nsampling multiple few-shot tasks in turn and optimizing the few-shot learner's\nperformance in generating classifiers for those tasks. The performance is\nmeasured by how well the resulting classifiers classify the test (i.e., query)\nexamples of those tasks. In this paper, we point out two potential weaknesses\nof this approach. First, the sampled query examples may not provide sufficient\nsupervision for meta-training the few-shot learner. Second, the effectiveness\nof meta-learning diminishes sharply with the increasing number of shots. To\nresolve these issues, we propose a novel meta-training objective for the\nfew-shot learner, which is to encourage the few-shot learner to generate\nclassifiers that perform like strong classifiers. Concretely, we associate each\nsampled few-shot task with a strong classifier, which is trained with ample\nlabeled examples. The strong classifiers can be seen as the target classifiers\nthat we hope the few-shot learner to generate given few-shot examples, and we\nuse the strong classifiers to supervise the few-shot learner. We present an\nefficient way to construct the strong classifier, making our proposed objective\nan easily plug-and-play term to existing meta-learning based FSL methods. We\nvalidate our approach, LastShot, in combinations with many representative\nmeta-learning methods. On several benchmark datasets, our approach leads to a\nnotable improvement across a variety of tasks. More importantly, with our\napproach, meta-learning based FSL methods can outperform non-meta-learning\nbased methods at different numbers of shots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_L/0/1/0/all/0/1\">Lu Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2107.01999","description":"<p>As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we released our code as\nwell as reference implementation at: https://github.com/zhishan01/FINT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation. (arXiv:2107.05274v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.05274","description":"<p>Accurate segmentation of organs or lesions from medical images is crucial for\nreliable diagnosis of diseases and organ morphometry. In recent years,\nconvolutional encoder-decoder solutions have achieved substantial progress in\nthe field of automatic medical image segmentation. Due to the inherent bias in\nthe convolution operations, prior models mainly focus on local visual cues\nformed by the neighboring pixels, but fail to fully model the long-range\ncontextual dependencies. In this paper, we propose a novel Transformer-based\nAttention Guided Network called TransAttUnet, in which the multi-level guided\nattention and multi-scale skip connection are designed to jointly enhance the\nperformance of the semantical segmentation architecture. Inspired by\nTransformer, the self-aware attention (SAA) module with Transformer Self\nAttention (TSA) and Global Spatial Attention (GSA) is incorporated into\nTransAttUnet to effectively learn the non-local interactions among encoder\nfeatures. Moreover, we also use additional multi-scale skip connections between\ndecoder blocks to aggregate the upsampled features with different semantic\nscales. In this way, the representation ability of multi-scale context\ninformation is strengthened to generate discriminative features. Benefitting\nfrom these complementary components, the proposed TransAttUnet can effectively\nalleviate the loss of fine details caused by the stacking of convolution layers\nand the consecutive sampling operations, finally improving the segmentation\nquality of medical images. Extensive experiments on multiple medical image\nsegmentation datasets from different imaging modalities demonstrate that the\nproposed method consistently outperforms the state-of-the-art baselines. Our\ncode and pre-trained models are available at:\nhttps://github.com/YishuLiu/TransAttUnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Bingzhi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yishu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_A/0/1/0/all/0/1\">Adams Wai Kin Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two New Low Rank Tensor Completion Methods Based on Sum Nuclear Norm. (arXiv:2108.03002v4 [math.NA] UPDATED)","link":"http://arxiv.org/abs/2108.03002","description":"<p>The low rank tensor completion (LRTC) problem has attracted great attention\nin computer vision and signal processing. How to acquire high quality image\nrecovery effect is still an urgent task to be solved at present. This paper\nproposes a new tensor $L_{2,1}$ norm minimization model (TLNM) that integrates\nsum nuclear norm (SNN) method, differing from the classical tensor nuclear norm\n(TNN)-based tensor completion method, with $L_{2,1}$ norm and Qatar Riyal\ndecomposition for solving the LRTC problem. To improve the utilization rate of\nthe local prior information of the image, a total variation (TV) regularization\nterm is introduced, resulting in a new class of tensor $L_{2,1}$ norm\nminimization with total variation model (TLNMTV). Both proposed models are\nconvex and therefore have global optimal solutions. Moreover, we adopt the\nAlternating Direction Multiplier Method (ADMM) to obtain the closed-form\nsolution of each variable, thus ensuring the feasibility of the algorithm.\nNumerical experiments show that the two proposed algorithms are convergent and\noutperform compared methods. In particular, our method significantly\noutperforms the contrastive methods when the sampling rate of hyperspectral\nimages is 2.5\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Full Feature Measure and Its Nonconvex Relaxation Applications to Tensor Recovery. (arXiv:2109.12257v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12257","description":"<p>Tensor sparse modeling as a promising approach, in the whole of science and\nengineering has been a huge success. As is known to all, various data in\npractical application are often generated by multiple factors, so the use of\ntensors to represent the data containing the internal structure of multiple\nfactors came into being. However, different from the matrix case, constructing\nreasonable sparse measure of tensor is a relatively difficult and very\nimportant task. Therefore, in this paper, we propose a new tensor sparsity\nmeasure called Tensor Full Feature Measure (FFM). It can simultaneously\ndescribe the feature information of each dimension of the tensor and the\nrelated features between two dimensions, and connect the Tucker rank with the\ntensor tube rank. This measurement method can describe the sparse features of\nthe tensor more comprehensively. On this basis, we establish its non-convex\nrelaxation, and apply FFM to low rank tensor completion (LRTC) and tensor\nrobust principal component analysis (TRPCA). LRTC and TRPCA models based on FFM\nare proposed, and two efficient Alternating Direction Multiplier Method (ADMM)\nalgorithms are developed to solve the proposed model. A variety of real\nnumerical experiments substantiate the superiority of the proposed methods\nbeyond state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTRN: Class-Temporal Relational Network for Action Detection. (arXiv:2110.13473v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13473","description":"<p>Action detection is an essential and challenging task, especially for densely\nlabelled datasets of untrimmed videos. There are many real-world challenges in\nthose datasets, such as composite action, co-occurring action, and high\ntemporal variation of instance duration. For handling these challenges, we\npropose to explore both the class and temporal relations of detected actions.\nIn this work, we introduce an end-to-end network: Class-Temporal Relational\nNetwork (CTRN). It contains three key components: (1) The Representation\nTransform Module filters the class-specific features from the mixed\nrepresentations to build graph-structured data. (2) The Class-Temporal Module\nmodels the class and temporal relations in a sequential manner. (3)\nG-classifier leverages the privileged knowledge of the snippet-wise\nco-occurring action pairs to further improve the co-occurring action detection.\nWe evaluate CTRN on three challenging densely labelled datasets and achieve\nstate-of-the-art performance, reflecting the effectiveness and robustness of\nour method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust deep learning-based semantic organ segmentation in hyperspectral images. (arXiv:2111.05408v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.05408","description":"<p>Semantic image segmentation is an important prerequisite for\ncontext-awareness and autonomous robotics in surgery. The state of the art has\nfocused on conventional RGB video data acquired during minimally invasive\nsurgery, but full-scene semantic segmentation based on spectral imaging data\nand obtained during open surgery has received almost no attention to date. To\naddress this gap in the literature, we are investigating the following research\nquestions based on hyperspectral imaging (HSI) data of pigs acquired in an open\nsurgery setting: (1) What is an adequate representation of HSI data for neural\nnetwork-based fully automated organ segmentation, especially with respect to\nthe spatial granularity of the data (pixels vs. superpixels vs. patches vs.\nfull images)? (2) Is there a benefit of using HSI data compared to other\nmodalities, namely RGB data and processed HSI data (e.g. tissue parameters like\noxygenation), when performing semantic organ segmentation? According to a\ncomprehensive validation study based on 506 HSI images from 20 pigs, annotated\nwith a total of 19 classes, deep learning-based segmentation performance\nincreases, consistently across modalities, with the spatial context of the\ninput data. Unprocessed HSI data offers an advantage over RGB data or processed\ndata from the camera provider, with the advantage increasing with decreasing\nsize of the input to the neural network. Maximum performance (HSI applied to\nwhole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04),\nwhich is in the range of the inter-rater variability (DSC of 0.89 ((standard\ndeviation (SD)) 0.07)). We conclude that HSI could become a powerful image\nmodality for fully-automatic surgical scene understanding with many advantages\nover traditional imaging, including the ability to recover additional\nfunctional tissue information. Code and pre-trained models:\nhttps://github.com/IMSY-DKFZ/htc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Seidlitz_S/0/1/0/all/0/1\">Silvia Seidlitz</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Sellner_J/0/1/0/all/0/1\">Jan Sellner</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Odenthal_J/0/1/0/all/0/1\">Jan Odenthal</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Ozdemir_B/0/1/0/all/0/1\">Berkin &#xd6;zdemir</a> (3 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Studier_Fischer_A/0/1/0/all/0/1\">Alexander Studier-Fischer</a> (3 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Knodler_S/0/1/0/all/0/1\">Samuel Kn&#xf6;dler</a> (3 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Ayala_L/0/1/0/all/0/1\">Leonardo Ayala</a> (1 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Adler_T/0/1/0/all/0/1\">Tim J. Adler</a> (1 and 6), <a href=\"http://arxiv.org/find/eess/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes G. Kenngott</a> (2 and 3), <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu Tizabi</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_M/0/1/0/all/0/1\">Martin Wagner</a> (2 and 3 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Nickel_F/0/1/0/all/0/1\">Felix Nickel</a> (2 and 3 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat P. M&#xfc;ller-Stich</a> (3 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a> (1 and 2 and 4 and 5 and 6) ((1) Division of Intelligent Medical Systems, German Cancer Research Center (DKFZ), Heidelberg, Germany, (2) Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany, (3) Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany, (4) Medical Faculty, Heidelberg University, Heidelberg, Germany, (5) HIP Helmholtz Imaging Platform, German Cancer Research Center (DKFZ), Heidelberg, Germany, (6) Faculty of Mathematics and Computer Science, Heidelberg University, Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Hypothesis Hypergraph Tracking for Posture Identification in Embryonic Caenorhabditis elegans. (arXiv:2111.06425v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.06425","description":"<p>Current methods in multiple object tracking (MOT) rely on independent object\ntrajectories undergoing predictable motion to effectively track large numbers\nof objects. Adversarial conditions such as volatile object motion and imperfect\ndetections create a challenging tracking landscape in which established methods\nmay yield inadequate results. Multiple hypothesis hypergraph tracking (MHHT) is\ndeveloped to perform MOT among interdependent objects amid noisy detections.\nThe method extends traditional multiple hypothesis tracking (MHT) via\nhypergraphs to model correlated object motion, allowing for robust tracking in\nchallenging scenarios. MHHT is applied to perform seam cell tracking during\nlate-stage embryogenesis in embryonic C. elegans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lauziere_A/0/1/0/all/0/1\">Andrew Lauziere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ardiel_E/0/1/0/all/0/1\">Evan Ardiel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Stephen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shroff_H/0/1/0/all/0/1\">Hari Shroff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedestrian Detection by Exemplar-Guided Contrastive Learning. (arXiv:2111.08974v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08974","description":"<p>Typical methods for pedestrian detection focus on either tackling mutual\nocclusions between crowded pedestrians, or dealing with the various scales of\npedestrians. Detecting pedestrians with substantial appearance diversities such\nas different pedestrian silhouettes, different viewpoints or different\ndressing, remains a crucial challenge. Instead of learning each of these\ndiverse pedestrian appearance features individually as most existing methods\ndo, we propose to perform contrastive learning to guide the feature learning in\nsuch a way that the semantic distance between pedestrians with different\nappearances in the learned feature space is minimized to eliminate the\nappearance diversities, whilst the distance between pedestrians and background\nis maximized. To facilitate the efficiency and effectiveness of contrastive\nlearning, we construct an exemplar dictionary with representative pedestrian\nappearances as prior knowledge to construct effective contrastive training\npairs and thus guide contrastive learning. Besides, the constructed exemplar\ndictionary is further leveraged to evaluate the quality of pedestrian proposals\nduring inference by measuring the semantic distance between the proposal and\nthe exemplar dictionary. Extensive experiments on both daytime and nighttime\npedestrian detection validate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zebin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.11191","description":"<p>A method of a Convolutional Neural Networks (CNN) for image classification\nwith image preprocessing and hyperparameters tuning was proposed. The method\naims at increasing the predictive performance for COVID-19 diagnosis while more\ncomplex model architecture. Firstly, the CNN model includes four similar\nconvolutional layers followed by a flattening and two dense layers. This work\nproposes a less complex solution based on simply classifying 2D-slices of\nComputed Tomography scans. Despite the simplicity in architecture, the proposed\nCNN model showed improved quantitative results exceeding state-of-the-art when\npredicting slice cases. The results were achieved on the annotated CT slices of\nthe COV-19-CT-DB dataset. Secondly, the original dataset was processed via\nanatomy-relevant masking of slice, removing none-representative slices from the\nCT volume, and hyperparameters tuning. For slice processing, a fixed-sized\nrectangular area was used for cropping an anatomy-relevant region-of-interest\nin the images, and a threshold based on the number of white pixels in binarized\nslices was employed to remove none-representative slices from the 3D-CT scans.\nThe CNN model with a learning rate schedule and an exponential decay and slice\nflipping techniques was deployed on the processed slices. The proposed method\nwas used to make predictions on the 2D slices and for final diagnosis at\npatient level, majority voting was applied on the slices of each CT scan to\ntake the diagnosis. The macro F1 score of the proposed method well-exceeded the\nbaseline approach and other alternatives on the validation set as well as on a\ntest partition of previously unseen images from COV-19CT-DB dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1\">Kenan Morani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unay_D/0/1/0/all/0/1\">Devrim Unay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification-Regression for Chart Comprehension. (arXiv:2111.14792v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14792","description":"<p>Chart question answering (CQA) is a task used for assessing chart\ncomprehension, which is fundamentally different from understanding natural\nimages. CQA requires analyzing the relationships between the textual and the\nvisual components of a chart, in order to answer general questions or infer\nnumerical values. Most existing CQA datasets and models are based on\nsimplifying assumptions that often enable surpassing human performance. In this\nwork, we address this outcome and propose a new model that jointly learns\nclassification and regression. Our language-vision setup uses co-attention\ntransformers to capture the complex real-world interactions between the\nquestion and the textual elements. We validate our design with extensive\nexperiments on the realistic PlotQA dataset, outperforming previous approaches\nby a large margin, while showing competitive performance on FigureQA. Our model\nis particularly well suited for realistic questions with out-of-vocabulary\nanswers that require regression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1\">Matan Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1\">Rami Ben-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF-SR: High-Quality Neural Radiance Fields using Super-Sampling. (arXiv:2112.01759v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01759","description":"<p>We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis\nwith mostly low-resolution (LR) inputs. Our method is built upon Neural\nRadiance Fields (NeRF) that predicts per-point density and color with a\nmulti-layer perceptron. While producing images at arbitrary scales, NeRF\nstruggles with resolutions that go beyond observed images. Our key insight is\nthat NeRF benefits from 3D consistency, which means an observed pixel absorbs\ninformation from nearby views. We first exploit it by a super-sampling strategy\nthat shoots multiple rays at each image pixel, which further enforces\nmulti-view constraint at a sub-pixel level. Then, we show that NeRF-SR can\nfurther boost the performance of super-sampling by a refinement network that\nleverages the estimated depth at hand to hallucinate details from related\npatches on only one HR reference image. Experiment results demonstrate that\nNeRF-SR generates high-quality results for novel view synthesis at HR on both\nsynthetic and real-world datasets without any external information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation. (arXiv:2112.02478v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.02478","description":"<p>Background and Objective: Artificial intelligence (AI) methods coupled with\nbiomedical analysis has a critical role during pandemics as it helps to release\nthe overwhelming pressure from healthcare systems and physicians. As the\nongoing COVID-19 crisis worsens in countries having dense populations and\ninadequate testing kits like Brazil and India, radiological imaging can act as\nan important diagnostic tool to accurately classify covid-19 patients and\nprescribe the necessary treatment in due time. With this motivation, we present\nour study based on deep learning architecture for detecting covid-19 infected\nlungs using chest X-rays. Dataset: We collected a total of 2470 images for\nthree different class labels, namely, healthy lungs, ordinary pneumonia, and\ncovid-19 infected pneumonia, out of which 470 X-ray images belong to the\ncovid-19 category. Methods: We first pre-process all the images using histogram\nequalization techniques and segment them using U-net architecture. VGG-16\nnetwork is then used for feature extraction from the pre-processed images which\nis further sampled by SMOTE oversampling technique to achieve a balanced\ndataset. Finally, the class-balanced features are classified using a support\nvector machine (SVM) classifier with 10-fold cross-validation and the accuracy\nis evaluated. Result and Conclusion: Our novel approach combining well-known\npre-processing techniques, feature extraction methods, and dataset balancing\nmethod, lead us to an outstanding rate of recognition of 98% for COVID-19\nimages over a dataset of 2470 X-ray images. Our model is therefore fit to be\nutilized in healthcare facilities for screening purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Swaraj_A/0/1/0/all/0/1\">Aman Swaraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verma_K/0/1/0/all/0/1\">Karan Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoodleFormer: Creative Sketch Drawing with Transformers. (arXiv:2112.03258v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03258","description":"<p>Creative sketching or doodling is an expressive activity, where imaginative\nand previously unseen depictions of everyday visual objects are drawn. Creative\nsketch image generation is a challenging vision problem, where the task is to\ngenerate diverse, yet realistic creative sketches possessing the unseen\ncomposition of the visual-world objects. Here, we propose a novel\ncoarse-to-fine two-stage framework, DoodleFormer, that decomposes the creative\nsketch generation problem into the creation of coarse sketch composition\nfollowed by the incorporation of fine-details in the sketch. We introduce\ngraph-aware transformer encoders that effectively capture global dynamic as\nwell as local static structural relations among different body parts. To ensure\ndiversity of the generated creative sketches, we introduce a probabilistic\ncoarse sketch decoder that explicitly models the variations of each sketch body\npart to be drawn. Experiments are performed on two creative sketch datasets:\nCreative Birds and Creative Creatures. Our qualitative, quantitative and\nhuman-based evaluations show that DoodleFormer outperforms the state-of-the-art\non both datasets, yielding realistic and diverse creative sketches. On Creative\nCreatures, DoodleFormer achieves an absolute gain of 25 in terms of Fr`echet\ninception distance (FID) over the state-of-the-art. We also demonstrate the\neffectiveness of DoodleFormer for related applications of text to creative\nsketch generation and sketch completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ankan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1\">Hisham Cholakkal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laaksonen_J/0/1/0/all/0/1\">Jorma Laaksonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Attentional Network for Semantic Segmentation. (arXiv:2112.04108v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04108","description":"<p>Recent non-local self-attention methods have proven to be effective in\ncapturing long-range dependencies for semantic segmentation. These methods\nusually form a similarity map of RC*C (by compressing spatial dimensions) or\nRHW*HW (by compressing channels) to describe the feature relations along either\nchannel or spatial dimensions, where C is the number of channels, H and W are\nthe spatial dimensions of the input feature map. However, such practices tend\nto condense feature dependencies along the other dimensions,hence causing\nattention missing, which might lead to inferior results for small/thin\ncategories or inconsistent segmentation inside large objects. To address this\nproblem, we propose anew approach, namely Fully Attentional Network (FLANet),to\nencode both spatial and channel attentions in a single similarity map while\nmaintaining high computational efficiency. Specifically, for each channel map,\nour FLANet can harvest feature responses from all other channel maps, and the\nassociated spatial positions as well, through a novel fully attentional module.\nOur new method has achieved state-of-the-art performance on three challenging\nsemantic segmentation datasets,i.e., 83.6%, 46.99%, and 88.5% on the Cityscapes\ntest set,the ADE20K validation set, and the PASCAL VOC test set,respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Beyond Corners: Contrastive Learning of Visual Representations for Keypoint Detection and Description Extraction. (arXiv:2112.12002v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12002","description":"<p>Learnable keypoint detectors and descriptors are beginning to outperform\nclassical hand-crafted feature extraction methods. Recent studies on\nself-supervised learning of visual representations have driven the increasing\nperformance of learnable models based on deep networks. By leveraging\ntraditional data augmentations and homography transformations, these networks\nlearn to detect corners under adverse conditions such as extreme illumination\nchanges. However, their generalization capabilities are limited to corner-like\nfeatures detected a priori by classical methods or synthetically generated\ndata.\n</p>\n<p>In this paper, we propose the Correspondence Network (CorrNet) that learns to\ndetect repeatable keypoints and to extract discriminative descriptions via\nunsupervised contrastive learning under spatial constraints. Our experiments\nshow that CorrNet is not only able to detect low-level features such as\ncorners, but also high-level features that represent similar objects present in\na pair of input images through our proposed joint guided backpropagation of\ntheir latent space. Our approach obtains competitive results under viewpoint\nchanges and achieves state-of-the-art performance under illumination changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_H/0/1/0/all/0/1\">Henrique Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhkamp_P/0/1/0/all/0/1\">Patrick Ruhkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halfaoui_I/0/1/0/all/0/1\">Ibrahim Halfaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmann_M/0/1/0/all/0/1\">Markus Karmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urfalioglu_O/0/1/0/all/0/1\">Onay Urfalioglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Free-Viewpoint RGB-D Human Performance Capture and Rendering. (arXiv:2112.13889v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13889","description":"<p>Capturing and faithfully rendering photo-realistic humans from novel views is\na fundamental problem for AR/VR applications. While prior work has shown\nimpressive performance capture results in laboratory settings, it is\nnon-trivial to achieve casual free-viewpoint human capture and rendering for\nunseen identities with high fidelity, especially for facial expressions, hands,\nand clothes. To tackle these challenges we introduce a novel view synthesis\nframework that generates realistic renders from unseen views of any human\ncaptured from a single-view and sparse RGB-D sensor, similar to a low-cost\ndepth camera, and without actor-specific models. We propose an architecture to\ncreate dense feature maps in novel views obtained by sphere-based neural\nrendering, and create complete renders using a global context inpainting model.\nAdditionally, an enhancer network leverages the overall fidelity, even in\noccluded areas from the original view, producing crisp renders with fine\ndetails. We show that our method generates high-quality novel views of\nsynthetic and real human actors given a single-stream, sparse RGB-D input. It\ngeneralizes to unseen identities, and new poses and faithfully reconstructs\nfacial expressions. Our approach outperforms prior view synthesis methods and\nis robust to different levels of depth sparsity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Recovery Based on Tensor Equivalent Minimax-Concave Penalty. (arXiv:2201.12709v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12709","description":"<p>Tensor recovery is an important problem in computer vision and machine\nlearning. It usually uses the convex relaxation of tensor rank and $l_{0}$\nnorm, i.e., the nuclear norm and $l_{1}$ norm respectively, to solve such\nproblem. Convex approximations are known to produce biased estimators. To\novercome this problem, a corresponding non-convex regularizer is adopted and\ndesigned. Inspired by the recently developed matrix equivalent Minimax-Concave\nPenalty (EMCP) theorem, a theorem of tensor equivalent Minimax-Concave Penalty\n(TEMCP) is established in this paper. Tensor equivalent MCP (TEMCP) as the\nnon-convex regularizer part and equivalent weighted tensor $\\gamma$ norm\n(EWTGN) as the low-rank part are constructed, both of which can achieve weight\nadaptive. Meanwhile, we propose two corresponding adaptive models for two\nclassical tensor recovery problems, namely, low-rank tensor completion (LRTC)\nand tensor robust principal component analysis (TRPCA), in which the\noptimization algorithm is based on alternating direction multiplier (ADMM).\nThis novel iterative adaptive algorithm is devised, which can produce more\naccurate tensor recovery effect. For the tensor completion model, multispectral\nimage (MSI), magnetic resonance imaging (MRI) and color video (CV) data are\nconsidered, while for the tensor robust principal component analysis model,\nhyperspectral image (HSI) denoising under gaussian noise plus salt and pepper\nnoise is considered. The proposed algorithm is superior to the state-of-arts\nmethod, and the reduction and convergence of which are guaranteed through\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-Plane-Adaptive Inter Prediction in 360-Degree Video Coding. (arXiv:2202.03323v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.03323","description":"<p>Inter prediction is one of the key technologies enabling the high compression\nefficiency of modern video coding standards. 360-degree video needs to be\nmapped to the 2D image plane prior to coding in order to allow compression\nusing existing video coding standards. The distortions that inevitably occur\nwhen mapping spherical data onto the 2D image plane, however, impair the\nperformance of classical inter prediction techniques. In this paper, we propose\na motion-plane-adaptive inter prediction technique (MPA) for 360-degree video\nthat takes the spherical characteristics of 360-degree video into account.\nBased on the known projection format of the video, MPA allows to perform inter\nprediction on different motion planes in 3D space instead of having to work on\nthe - in theory arbitrarily mapped - 2D image representation directly. We\nfurthermore derive a motion-plane-adaptive motion vector prediction technique\n(MPA-MVP) that allows to translate motion information between different motion\nplanes and motion models. Our proposed integration of MPA together with MPA-MVP\ninto the state-of-the-art H.266/VVC video coding standard shows significant\nBjontegaard Delta rate savings of 1.72% with a peak of 3.97% based on PSNR and\n1.56% with a peak of 3.40% based on WS-PSNR compared to the VTM-14.2 baseline\non average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Regensky_A/0/1/0/all/0/1\">Andy Regensky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herglotz_C/0/1/0/all/0/1\">Christian Herglotz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Puzzle: Arbitrary Motion Style Transfer by Body Part. (arXiv:2202.05274v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2202.05274","description":"<p>This paper presents Motion Puzzle, a novel motion style transfer network that\nadvances the state-of-the-art in several important respects. The Motion Puzzle\nis the first that can control the motion style of individual body parts,\nallowing for local style editing and significantly increasing the range of\nstylized motions. Designed to keep the human's kinematic structure, our\nframework extracts style features from multiple style motions for different\nbody parts and transfers them locally to the target body parts. Another major\nadvantage is that it can transfer both global and local traits of motion style\nby integrating the adaptive instance normalization and attention modules while\nkeeping the skeleton topology. Thus, it can capture styles exhibited by dynamic\nmovements, such as flapping and staggering, significantly better than previous\nwork. In addition, our framework allows for arbitrary motion style transfer\nwithout datasets with style labeling or motion pairing, making many publicly\navailable motion datasets available for training. Our framework can be easily\nintegrated with motion generation frameworks to create many applications, such\nas real-time motion transfer. We demonstrate the advantages of our framework\nwith a number of examples and comparisons with previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Deok-Kyeong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Soomin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-Cooperated Trimodal Network for Video Salient Object Detection. (arXiv:2202.06060v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06060","description":"<p>Depth can provide useful geographical cues for salient object detection\n(SOD), and has been proven helpful in recent RGB-D SOD methods. However,\nexisting video salient object detection (VSOD) methods only utilize\nspatiotemporal information and seldom exploit depth information for detection.\nIn this paper, we propose a depth-cooperated trimodal network, called DCTNet\nfor VSOD, which is a pioneering work to incorporate depth information to assist\nVSOD. To this end, we first generate depth from RGB frames, and then propose an\napproach to treat the three modalities unequally. Specifically, a multi-modal\nattention module (MAM) is designed to model multi-modal long-range dependencies\nbetween the main modality (RGB) and the two auxiliary modalities (depth,\noptical flow). We also introduce a refinement fusion module (RFM) to suppress\nnoises in each modality and select useful information dynamically for further\nfeature refinement. Lastly, a progressive fusion strategy is adopted after the\nrefined features to achieve final cross-modal fusion. Experiments on five\nbenchmark datasets demonstrate the superiority of our depth-cooperated model\nagainst 12 state-of-the-art methods, and the necessity of depth is also\nvalidated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yukang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dingyao Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark. (arXiv:2202.07054v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07054","description":"<p>Deep neural networks have achieved great success in many important remote\nsensing tasks. Nevertheless, their vulnerability to adversarial examples should\nnot be neglected. In this study, we systematically analyze the universal\nadversarial examples in remote sensing data for the first time, without any\nknowledge from the victim model. Specifically, we propose a novel black-box\nadversarial attack method, namely Mixup-Attack, and its simple variant\nMixcut-Attack, for remote sensing data. The key idea of the proposed methods is\nto find common vulnerabilities among different networks by attacking the\nfeatures in the shallow layer of a given surrogate model. Despite their\nsimplicity, the proposed methods can generate transferable adversarial examples\nthat deceive most of the state-of-the-art deep neural networks in both scene\nclassification and semantic segmentation tasks with high success rates. We\nfurther provide the generated universal adversarial examples in the dataset\nnamed UAE-RS, which is the first dataset that provides black-box adversarial\nsamples in the remote sensing field. We hope UAE-RS may serve as a benchmark\nthat helps researchers to design deep neural networks with strong resistance\ntoward adversarial attacks in the remote sensing field. Codes and the UAE-RS\ndataset are available online (https://github.com/YonghaoXu/UAE-RS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yonghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MANet: Improving Video Denoising with a Multi-Alignment Network. (arXiv:2202.09704v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09704","description":"<p>In video denoising, the adjacent frames often provide very useful\ninformation, but accurate alignment is needed before such information can be\nharnassed. In this work, we present a multi-alignment network, which generates\nmultiple flow proposals followed by attention-based averaging. It serves to\nmimic the non-local mechanism, suppressing noise by averaging multiple\nobservations. Our approach can be applied to various state-of-the-art models\nthat are based on flow estimation. Experiments on a large-scale video dataset\ndemonstrate that our method improves the denoising baseline model by 0.2dB, and\nfurther reduces the parameters by 47% with model distillation. Code is\navailable at https://github.com/IndigoPurple/MANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1\">Edmund Y. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Network. (arXiv:2202.09741v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09741","description":"<p>While originally designed for natural language processing tasks, the\nself-attention mechanism has recently taken various computer vision areas by\nstorm. However, the 2D nature of images brings three challenges for applying\nself-attention in computer vision. (1) Treating images as 1D sequences neglects\ntheir 2D structures. (2) The quadratic complexity is too expensive for\nhigh-resolution images. (3) It only captures spatial adaptability but ignores\nchannel adaptability. In this paper, we propose a novel linear attention named\nlarge kernel attention (LKA) to enable self-adaptive and long-range\ncorrelations in self-attention while avoiding its shortcomings. Furthermore, we\npresent a neural network based on LKA, namely Visual Attention Network (VAN).\nWhile extremely simple, VAN surpasses similar size vision transformers(ViTs)\nand convolutional neural networks(CNNs) in various tasks, including image\nclassification, object detection, semantic segmentation, panoptic segmentation,\npose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet\nbenchmark and set new state-of-the-art performance (58.2 PQ) for panoptic\nsegmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for\nsemantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object\ndetection on COCO dataset. It provides a novel method and a simple yet strong\nbaseline for the community. Code is available at\nhttps://github.com/Visual-Attention-Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-Ze Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng-Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning classification of large-scale point clouds: A case study on cuneiform tablets. (arXiv:2202.10851v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10851","description":"<p>This paper introduces a novel network architecture for the classification of\nlarge-scale point clouds. The network is used to classify metadata from\ncuneiform tablets. As more than half a million tablets remain unprocessed, this\ncan help create an overview of the tablets. The network is tested on a\ncomparison dataset and obtains state-of-the-art performance. We also introduce\nnew metadata classification tasks on which the network shows promising results.\nFinally, we introduce the novel Maximum Attention visualization, demonstrating\nthat the trained network focuses on the intended features. Code available at\nhttps://github.com/fhagelskjaer/dlc-cuneiform\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagelskjaer_F/0/1/0/all/0/1\">Frederik Hagelskjaer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RelMobNet: End-to-end relative camera pose estimation using a robust two-stage training. (arXiv:2202.12838v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12838","description":"<p>Relative camera pose estimation, i.e. estimating the translation and rotation\nvectors using a pair of images taken in different locations, is an important\npart of systems in augmented reality and robotics. In this paper, we present an\nend-to-end relative camera pose estimation network using a siamese architecture\nthat is independent of camera parameters. The network is trained using the\nCambridge Landmarks data with four individual scene datasets and a dataset\ncombining the four scenes. To improve generalization, we propose a novel\ntwo-stage training that alleviates the need of a hyperparameter to balance the\ntranslation and rotation loss scale. The proposed method is compared with\none-stage training CNN-based methods such as RPNet and RCPNet and demonstrate\nthat the proposed model improves translation vector estimation by 16.11%,\n28.88%, and 52.27% on the Kings College, Old Hospital, and St Marys Church\nscenes, respectively. For proving texture invariance, we investigate the\ngeneralization of the proposed method augmenting the datasets to different\nscene styles, as ablation studies, using generative adversarial networks. Also,\nwe present a qualitative assessment of epipolar lines of our network\npredictions and ground truth poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_P/0/1/0/all/0/1\">Praveen Kumar Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sumit Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vecchietti_L/0/1/0/all/0/1\">Luiz Felipe Vecchietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Har_D/0/1/0/all/0/1\">Dongsoo Har</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Label Aware Superpixels for Multi-species Segmentation of Underwater Imagery. (arXiv:2202.13487v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13487","description":"<p>Monitoring coral reefs using underwater vehicles increases the range of\nmarine surveys and availability of historical ecological data by collecting\nsignificant quantities of images. Analysis of this imagery can be automated\nusing a model trained to perform semantic segmentation, however it is too\ncostly and time-consuming to densely label images for training supervised\nmodels. In this letter, we leverage photo-quadrat imagery labeled by ecologists\nwith sparse point labels. We propose a point label aware method for propagating\nlabels within superpixel regions to obtain augmented ground truth for training\na semantic segmentation model. Our point label aware superpixel method utilizes\nthe sparse point labels, and clusters pixels using learned features to\naccurately generate single-species segments in cluttered, complex coral images.\nOur method outperforms prior methods on the UCSD Mosaics dataset by 3.62% for\npixel accuracy and 8.35% for mean IoU for the label propagation task, while\nreducing computation time reported by previous approaches by 76%. We train a\nDeepLabv3+ architecture and outperform state-of-the-art for semantic\nsegmentation by 2.91% for pixel accuracy and 9.65% for mean IoU on the UCSD\nMosaics dataset and by 4.19% for pixel accuracy and 14.32% mean IoU for the\nEilat dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raine_S/0/1/0/all/0/1\">Scarlett Raine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchant_R/0/1/0/all/0/1\">Ross Marchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusy_B/0/1/0/all/0/1\">Brano Kusy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maire_F/0/1/0/all/0/1\">Frederic Maire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation. (arXiv:2203.01074v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01074","description":"<p>Environment perception in autonomous driving vehicles often heavily relies on\ndeep neural networks (DNNs), which are subject to domain shifts, leading to a\nsignificantly decreased performance during DNN deployment. Usually, this\nproblem is addressed by unsupervised domain adaptation (UDA) approaches trained\neither simultaneously on source and target domain datasets or even source-free\nonly on target data in an offline fashion. In this work, we further expand a\nsource-free UDA approach to a continual and therefore online-capable UDA on a\nsingle-image basis for semantic segmentation. Accordingly, our method only\nrequires the pre-trained model from the supplier (trained in the source domain)\nand the current (unlabeled target domain) camera image. Our method Continual\nBatchNorm Adaptation (CBNA) modifies the source domain statistics in the batch\nnormalization layers, using target domain images in an unsupervised fashion,\nwhich yields consistent performance improvements during inference. Thereby, in\ncontrast to existing works, our approach can be applied to improve a DNN\ncontinuously on a single-image basis during deployment without access to source\ndata, without algorithmic delay, and nearly without computational overhead. We\nshow the consistent effectiveness of our method across a wide variety of\nsource/target domain settings for semantic segmentation. Code is available at\nhttps://github.com/ifnspaml/CBNA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klingner_M/0/1/0/all/0/1\">Marvin Klingner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayache_M/0/1/0/all/0/1\">Mouadh Ayache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01322","description":"<p>Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1\">Venkat Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction. (arXiv:2203.01474v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01474","description":"<p>Predicting future motion based on historical motion sequence is a fundamental\nproblem in computer vision, and it has wide applications in autonomous driving\nand robotics. Some recent works have shown that Graph Convolutional\nNetworks(GCN) are instrumental in modeling the relationship between different\njoints. However, considering the variants and diverse action types in human\nmotion data, the cross-dependency of the spatio-temporal relationships will be\ndifficult to depict due to the decoupled modeling strategy, which may also\nexacerbate the problem of insufficient generalization. Therefore, we propose\nthe Spatio-Temporal Gating-Adjacency GCN(GAGCN) to learn the complex\nspatio-temporal dependencies over diverse action types. Specifically, we adopt\ngating networks to enhance the generalization of GCN via the trainable adaptive\nadjacency matrix obtained by blending the candidate spatio-temporal adjacency\nmatrices. Moreover, GAGCN addresses the cross-dependency of space and time by\nbalancing the weights of spatio-temporal modeling and fusing the decoupled\nspatio-temporal features. Extensive experiments on Human 3.6M, AMASS, and 3DPW\ndemonstrate that GAGCN achieves state-of-the-art performance in both short-term\nand long-term predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Chongyang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yongjing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shihong Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.02651","description":"<p>Conventional NAS-based pruning algorithms aim to find the sub-network with\nthe best validation performance. However, validation performance does not\nsuccessfully represent test performance, i.e., potential performance. Also,\nalthough fine-tuning the pruned network to restore the performance drop is an\ninevitable process, few studies have handled this issue. This paper provides a\nnovel Ensemble Knowledge Guidance (EKG) to solve both problems at once. First,\nwe experimentally prove that the fluctuation of loss landscape can be an\neffective metric to evaluate the potential performance. In order to search a\nsub-network with the smoothest loss landscape at a low cost, we employ EKG as a\nsearch reward. EKG utilized for the following search iteration is composed of\nthe ensemble knowledge of interim sub-networks, i.e., the by-products of the\nsub-network evaluation. Next, we reuse EKG to provide a gentle and informative\nguidance to the pruned network while fine-tuning the pruned network. Since EKG\nis implemented as a memory bank in both phases, it requires a negligible cost.\nFor example, when pruning and training ResNet-50, just 315 GPU hours are\nrequired to remove around 45.04% of FLOPS without any performance degradation,\nwhich can operate even on a low-spec workstation. the implemented code is\navailable at https://github.com/sseung0703/EKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Byung Cheol Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aware Latent Space Exploration for Face Image Restoration. (arXiv:2203.03005v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03005","description":"<p>For image restoration, the majority of existing deep learning-based\nalgorithms have a tendency to overfit the training data, resulting in poor\nperformance when confronted with unseen degradations. To achieve more robust\nrestoration, generative adversarial network (GAN) prior based methods have been\nproposed, demonstrating a promising capacity to restore photo-realistic and\nhigh-quality results. However, these methods are susceptible to semantic\nambiguity, particularly with semantically relevant images such as facial\nimages. In this paper, we propose a semantic-aware latent space exploration\nmethod for image restoration (SAIR). By explicitly modeling referenced\nsemantics information, SAIR is able to reliably restore severely degraded\nimages not only to high-resolution highly-realistic looks but also to correct\nsemantics. Quantitative and qualitative experiments collectively demonstrate\nthe effectiveness of the proposed SAIR. Our code can be found in\nhttps://github.com/Liamkuo/SAIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fangzhou Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03605","description":"<p>We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising\nanch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in\nthis paper. DINO improves over previous DETR-like models in performance and\nefficiency by using a contrastive way for denoising training, a mixed query\nselection method for anchor initialization, and a look forward twice scheme for\nbox prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$\nepochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a\nsignificant improvement of $\\textbf{+6.0}$\\textbf{AP} and\n$\\textbf{+2.7}$\\textbf{AP}, respectively, compared to DN-DETR, the previous\nbest DETR-like model. DINO scales well in both model size and data size.\nWithout bells and whistles, after pre-training on the Objects365 dataset with a\nSwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017}\n($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev}\n(\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO\nsignificantly reduces its model size and pre-training data size while achieving\nbetter results. Our code will be available at\n\\url{https://github.com/IDEACVR/DINO}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers. (arXiv:2203.03952v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03952","description":"<p>Recently, vision transformers started to show impressive results which\noutperform large convolution based models significantly. However, in the area\nof small models for mobile or resource constrained devices, ConvNet still has\nits own advantages in both performance and model complexity. We propose\nEdgeFormer, a pure ConvNet based backbone model that further strengthens these\nadvantages by fusing the merits of vision transformers into ConvNets.\nSpecifically, we propose global circular convolution (GCC) with position\nembeddings, a light-weight convolution op which boasts a global receptive field\nwhile producing location sensitive features as in local convolutions. We\ncombine the GCCs and squeeze-exictation ops to form a meta-former like model\nblock, which further has the attention mechanism like transformers. The\naforementioned block can be used in plug-and-play manner to replace relevant\nblocks in ConvNets or transformers. Experiment results show that the proposed\nEdgeFormer achieves better performance than popular light-weight ConvNets and\nvision transformer based models in common vision tasks and datasets, while\nhaving fewer parameters and faster inference speed. For classification on\nImageNet-1k, EdgeFormer achieves 78.6% top-1 accuracy with about 5.0 million\nparameters, saving 11% parameters and 13% computational cost but gaining 0.2%\nhigher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288)\ncompared with MobileViT, and uses only 0.5 times parameters but gaining 2.7%\naccuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC\nsegmentation tasks, EdgeFormer also shows better performance. Code is available\nat https://github.com/hkzhang91/EdgeFormer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition. (arXiv:2203.04559v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04559","description":"<p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the\nrobustness of video models, enabling them to be applied to action recognition\ntasks across different environments. However, these methods require constant\naccess to source data during the adaptation process. Yet in many real-world\napplications, subjects and scenes in the source video domain should be\nirrelevant to those in the target video domain. With the increasing emphasis on\ndata privacy, such methods that require source data access would raise serious\nprivacy issues. Therefore, to cope with such concern, a more practical domain\nadaptation scenario is formulated as the Source-Free Video-based Domain\nAdaptation (SFVDA). Though there are a few methods for Source-Free Domain\nAdaptation (SFDA) on image data, these methods yield degenerating performance\nin SFVDA due to the multi-modality nature of videos, with the existence of\nadditional temporal features. In this paper, we propose a novel Attentive\nTemporal Consistent Network (ATCoN) to address SFVDA by learning temporal\nconsistency, guaranteed by two novel consistency objectives, namely feature\nconsistency and source prediction consistency, performed across local temporal\nfeatures. ATCoN further constructs effective overall temporal features by\nattending to local temporal features based on prediction confidence. Empirical\nresults demonstrate the state-of-the-art performance of ATCoN across various\ncross-domain action recognition benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Keyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Wu Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction. (arXiv:2203.04845v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04845","description":"<p>Many algorithms have been developed to solve the inverse problem of coded\naperture snapshot spectral imaging (CASSI), i.e., recovering the 3D\nhyperspectral images (HSIs) from a 2D compressive measurement. In recent years,\nlearning-based methods have demonstrated promising performance and dominated\nthe mainstream research direction. However, existing CNN-based methods show\nlimitations in capturing long-range dependencies and non-local self-similarity.\nPrevious Transformer-based methods densely sample tokens, some of which are\nuninformative, and calculate the multi-head self-attention (MSA) between some\ntokens that are unrelated in content. This does not fit the spatially sparse\nnature of HSI signals and limits the model scalability. In this paper, we\npropose a novel Transformer-based method, coarse-to-fine sparse Transformer\n(CST), firstly embedding HSI sparsity into deep learning for HSI\nreconstruction. In particular, CST uses our proposed spectra-aware screening\nmechanism (SASM) for coarse patch selecting. Then the selected patches are fed\ninto our customized spectra-aggregation hashing multi-head self-attention\n(SAH-MSA) for fine pixel clustering and self-similarity capturing.\nComprehensive experiments show that our CST significantly outperforms\nstate-of-the-art methods while requiring cheaper computational costs. The code\nand models will be released at https://github.com/caiyuanhao1998/MST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Curve Translator for High-Resolution Photorealistic Image Translation. (arXiv:2203.07756v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07756","description":"<p>The dominant image-to-image translation methods are based on fully\nconvolutional networks, which extract and translate an image's features and\nthen reconstruct the image. However, they have unacceptable computational costs\nwhen working with high-resolution images. To this end, we present the\nMulti-Curve Translator (MCT), which not only predicts the translated pixels for\nthe corresponding input pixels but also for their neighboring pixels. And if a\nhigh-resolution image is downsampled to its low-resolution version, the lost\npixels are the remaining pixels' neighboring pixels. So MCT makes it possible\nto feed the network only the downsampled image to perform the mapping for the\nfull-resolution image, which can dramatically lower the computational cost.\nBesides, MCT is a plug-in approach that utilizes existing base models and\nrequires only replacing their output layers. Experiments demonstrate that the\nMCT variants can process 4K images in real-time and achieve comparable or even\nbetter performance than the base models on various photorealistic\nimage-to-image translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Multi-Domain Long-Tailed Recognition, Imbalanced Domain Generalization and Beyond. (arXiv:2203.09513v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.09513","description":"<p>Real-world data often exhibit imbalanced label distributions. Existing\nstudies on data imbalance focus on single-domain settings, i.e., samples are\nfrom the same data distribution. However, natural data can originate from\ndistinct domains, where a minority class in one domain could have abundant\ninstances from other domains. We formalize the task of Multi-Domain Long-Tailed\nRecognition (MDLT), which learns from multi-domain imbalanced data, addresses\nlabel imbalance, domain shift, and divergent label distributions across\ndomains, and generalizes to all domain-class pairs. We first develop the\ndomain-class transferability graph, and show that such transferability governs\nthe success of learning in MDLT. We then propose BoDA, a theoretically grounded\nlearning strategy that tracks the upper bound of transferability statistics,\nand ensures balanced alignment and calibration across imbalanced domain-class\ndistributions. We curate five MDLT benchmarks based on widely-used multi-domain\ndatasets, and compare BoDA to twenty algorithms that span different learning\nstrategies. Extensive and rigorous experiments verify the superior performance\nof BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on\nDomain Generalization benchmarks, highlighting the importance of addressing\ndata imbalance across domains, which can be crucial for improving\ngeneralization to unseen domains. Code and data are available at:\nhttps://github.com/YyzHarry/multi-domain-imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuzhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00298","description":"<p>To make full use of computer vision technology in stores, it is required to\nconsider the actual needs that fit the characteristics of the retail scene.\nPursuing this goal, we introduce the United Retail Datasets (Unitail), a\nlarge-scale benchmark of basic visual tasks on products that challenges\nalgorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped\ninstances annotated, the Unitail offers a detection dataset to align product\nappearance better. Furthermore, it provides a gallery-style OCR dataset\ncontaining 1454 product categories, 30k text regions, and 21k transcriptions to\nenable robust reading on products and motivate enhanced product matching.\nBesides benchmarking the datasets using various state-of-the-arts, we customize\na new detector for product detection and provide a simple OCR-based matching\nsolution that verifies its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaiwang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_J/0/1/0/all/0/1\">Jiachen Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_U/0/1/0/all/0/1\">Uzair Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.02779","description":"<p>Deep learning models for medical image segmentation can fail unexpectedly and\nspectacularly for pathological cases and images acquired at different centers\nthan training images, with labeling errors that violate expert knowledge. Such\nerrors undermine the trustworthiness of deep learning models for medical image\nsegmentation. Mechanisms for detecting and correcting such failures are\nessential for safely translating this technology into clinics and are likely to\nbe a requirement of future regulations on artificial intelligence (AI). In this\nwork, we propose a trustworthy AI theoretical framework and a practical system\nthat can augment any backbone AI system using a fallback method and a fail-safe\nmechanism based on Dempster-Shafer theory. Our approach relies on an actionable\ndefinition of trustworthy AI. Our method automatically discards the voxel-level\nlabeling predicted by the backbone AI that violate expert knowledge and relies\non a fallback for those voxels. We demonstrate the effectiveness of the\nproposed trustworthy AI approach on the largest reported annotated dataset of\nfetal MRI consisting of 540 manually annotated fetal brain 3D T2w MRIs from 13\ncenters. Our trustworthy AI method improves the robustness of a\nstate-of-the-art backbone AI for fetal brain MRIs acquired across various\ncenters and for fetuses with various brain abnormalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1\">Florian Kofler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bink_A/0/1/0/all/0/1\">Andrea Bink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1\">Thomas Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1\">Doaa Emam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Guffens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jakab_A/0/1/0/all/0/1\">Andr&#xe1;s Jakab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kasprian_G/0/1/0/all/0/1\">Gregor Kasprian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kienast_P/0/1/0/all/0/1\">Patric Kienast</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1\">Andrew Melbourne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1\">Nada Mufti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pogledic_I/0/1/0/all/0/1\">Ivana Pogledic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prayer_D/0/1/0/all/0/1\">Daniela Prayer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stuempflen_M/0/1/0/all/0/1\">Marlene Stuempflen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elslander_E/0/1/0/all/0/1\">Esther Van Elslander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fashionformer: A simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition. (arXiv:2204.04654v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04654","description":"<p>Human fashion understanding is one crucial computer vision task since it has\ncomprehensive information for real-world applications. This focus on joint\nhuman fashion segmentation and attribute recognition. Contrary to the previous\nworks that separately model each task as a multi-head prediction problem, our\ninsight is to bridge these two tasks with one unified model via vision\ntransformer modeling to benefit each task. In particular, we introduce the\nobject query for segmentation and the attribute query for attribute prediction.\nBoth queries and their corresponding features can be linked via mask\nprediction. Then we adopt a two-stream query learning framework to learn the\ndecoupled query representations.We design a novel Multi-Layer Rendering module\nfor attribute stream to explore more fine-grained features. The decoder design\nshares the same spirit as DETR. Thus we name the proposed method\n\\textit{Fahsionformer}. Extensive experiments on three human fashion datasets\nillustrate the effectiveness of our approach. In particular, our method with\nthe same backbone achieve \\textbf{relative 10\\% improvements} than previous\nworks in case of \\textit{a joint metric (AP$^{\\text{mask}}_{\\text{IoU+F}_1}$)\nfor both segmentation and attribute recognition}. To the best of our knowledge,\nwe are the first unified end-to-end vision transformer framework for human\nfashion analysis. We hope this simple yet effective method can serve as a new\nflexible baseline for fashion analysis. Code is available at\nhttps://github.com/xushilin1/FashionFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation. (arXiv:2204.04655v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04655","description":"<p>Panoptic Part Segmentation (PPS) aims to unify panoptic segmentation and part\nsegmentation into one task. Previous work mainly utilizes separated approaches\nto handle thing, stuff, and part predictions individually without performing\nany shared computation and task association. In this work, we aim to unify\nthese tasks at the architectural level, designing the first end-to-end unified\nmethod named Panoptic-PartFormer. In particular, motivated by the recent\nprogress in Vision Transformer, we model things, stuff, and part as object\nqueries and directly learn to optimize the all three predictions as unified\nmask prediction and classification problem. We design a decoupled decoder to\ngenerate part feature and thing/stuff feature respectively. Then we propose to\nutilize all the queries and corresponding features to perform reasoning jointly\nand iteratively. The final mask can be obtained via inner product between\nqueries and the corresponding features. The extensive ablation studies and\nanalysis prove the effectiveness of our framework. Our Panoptic-PartFormer\nachieves the new state-of-the-art results on both Cityscapes PPS and Pascal\nContext PPS datasets with at least 70% GFlops and 50% parameters decrease. In\nparticular, we get 3.4% relative improvements with ResNet50 backbone and 10%\nimprovements after adopting Swin Transformer on Pascal Context PPS dataset. To\nthe best of our knowledge, we are the first to solve the PPS problem via\n\\textit{a unified and end-to-end transformer model. Given its effectiveness and\nconceptual simplicity, we hope our Panoptic-PartFormer can serve as a good\nbaseline and aid future unified research for PPS. Our code and models are\navailable at https://github.com/lxtGH/Panoptic-PartFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Class Weights with Multi-Modal Information for Partial Video Domain Adaptation. (arXiv:2204.06187v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06187","description":"<p>Assuming the source label space subsumes the target one, Partial Video Domain\nAdaptation (PVDA) is a more general and practical scenario for cross-domain\nvideo classification problems. The key challenge of PVDA is to mitigate the\nnegative transfer caused by the source-only outlier classes. To tackle this\nchallenge, a crucial step is to aggregate target predictions to assign class\nweights by up-weighing target classes and down-weighing outlier classes.\nHowever, the incorrect predictions of class weights can mislead the network and\nlead to negative transfer. Previous works improve the class weight accuracy by\nutilizing temporal features and attention mechanisms, but these methods may\nfall short when trying to generate accurate class weight when domain shifts are\nsignificant, as in most real-world scenarios. To deal with these challenges, we\npropose the Multi-modality Cluster-calibrated partial Adversarial Network\n(MCAN). MCAN enhances video feature extraction with multi-modal features from\nmultiple temporal scales to form more robust overall features. It utilizes a\nnovel class weight calibration method to alleviate the negative transfer caused\nby incorrect class weights. The calibration method tries to identify and weigh\ncorrect and incorrect predictions using distributional information implied by\nunsupervised clustering. Extensive experiments are conducted on prevailing PVDA\nbenchmarks, and the proposed MCAN achieves significant improvements when\ncompared to state-of-the-art PVDA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood Attention Transformer. (arXiv:2204.07143v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07143","description":"<p>We present Neighborhood Attention Transformer (NAT), an efficient, accurate\nand scalable hierarchical transformer that works well on both image\nclassification and downstream vision tasks. It is built upon Neighborhood\nAttention (NA), a simple and flexible attention mechanism that localizes the\nreceptive field for each query to its nearest neighboring pixels. NA is a\nlocalization of self-attention, and approaches it as the receptive field size\nincreases. It is also equivalent in FLOPs and memory usage to Swin\nTransformer's shifted-window attention given the same receptive field size,\nwhile being less constrained. Furthermore, NA includes local inductive biases,\nwhich eliminate the need for extra operations such as pixel shifts.\nExperimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1\naccuracy on ImageNet with only 4.3 GFLOPs and 28M parameters, 51.4% mAP on\nMS-COCO and 48.4% mIoU on ADE20k. We open-sourced our checkpoints, code and\nCUDA kernel at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotationally Equivariant 3D Object Detection. (arXiv:2204.13630v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13630","description":"<p>Rotation equivariance has recently become a strongly desired property in the\n3D deep learning community. Yet most existing methods focus on equivariance\nregarding a global input rotation while ignoring the fact that rotation\nsymmetry has its own spatial support. Specifically, we consider the object\ndetection problem in 3D scenes, where an object bounding box should be\nequivariant regarding the object pose, independent of the scene motion. This\nsuggests a new desired property we call object-level rotation equivariance. To\nincorporate object-level rotation equivariance into 3D object detectors, we\nneed a mechanism to extract equivariant features with local object-level\nspatial support while being able to model cross-object context information. To\nthis end, we propose Equivariant Object detection Network (EON) with a rotation\nequivariance suspension design to achieve object-level equivariance. EON can be\napplied to modern point cloud object detectors, such as VoteNet and PointRCNN,\nenabling them to exploit object rotation symmetry in scene-scale inputs. Our\nexperiments on both indoor scene and autonomous driving datasets show that\nsignificant improvements are obtained by plugging our EON design into existing\nstate-of-the-art 3D object detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs. (arXiv:2205.00303v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00303","description":"<p>In this paper, we study the graphic layout generation problem of producing\nhigh-quality visual-textual presentation designs for given images. We note that\nimage compositions, which contain not only global semantics but also spatial\ninformation, would largely affect layout results. Hence, we propose a deep\ngenerative model, dubbed as composition-aware graphic layout GAN (CGL-GAN), to\nsynthesize layouts based on the global and spatial visual contents of input\nimages. To obtain training images from images that already contain manually\ndesigned graphic layout data, previous work suggests masking design elements\n(e.g., texts and embellishments) as model inputs, which inevitably leaves hint\nof the ground truth. We study the misalignment between the training inputs\n(with hint masks) and test inputs (without masks), and design a novel domain\nalignment module (DAM) to narrow this gap. For training, we built a large-scale\nlayout dataset which consists of 60,548 advertising posters with annotated\nlayout information. To evaluate the generated layouts, we propose three novel\nmetrics according to aesthetic intuitions. Through both quantitative and\nqualitative evaluations, we demonstrate that the proposed model can synthesize\nhigh-quality graphic layouts according to image compositions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Min Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Ye Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation. (arXiv:2205.01271v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01271","description":"<p>Pose estimation plays a critical role in human-centered vision applications.\nHowever, it is difficult to deploy state-of-the-art HRNet-based pose estimation\nmodels on resource-constrained edge devices due to the high computational cost\n(more than 150 GMACs per frame). In this paper, we study efficient architecture\ndesign for real-time multi-person pose estimation on edge. We reveal that\nHRNet's high-resolution branches are redundant for models at the\nlow-computation region via our gradual shrinking experiments. Removing them\nimproves both efficiency and performance. Inspired by this finding, we design\nLitePose, an efficient single-branch architecture for pose estimation, and\nintroduce two simple approaches to enhance the capacity of LitePose, including\nFusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the\nredundancy in high-resolution branches, allowing scale-aware feature fusion\nwith low overhead. Large Kernel Convs significantly improve the model's\ncapacity and receptive field while maintaining a low computational cost. With\nonly 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3\nkernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the\nlatency by up to 5.0x without sacrificing performance, compared with prior\nstate-of-the-art efficient pose estimation models, pushing the frontier of\nreal-time multi-person pose estimation on edge. Our code and pre-trained models\nare released at https://github.com/mit-han-lab/litepose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Han Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate and Edit Your Own Character in a Canonical View. (arXiv:2205.02974v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02974","description":"<p>Recently, synthesizing personalized characters from a single user-given\nportrait has received remarkable attention as a drastic popularization of\nsocial media and the metaverse. The input image is not always in frontal view,\nthus it is important to acquire or predict canonical view for 3D modeling or\nother applications. Although the progress of generative models enables the\nstylization of a portrait, obtaining the stylized image in canonical view is\nstill a challenging task. There have been several studies on face\nfrontalization but their performance significantly decreases when input is not\nin the real image domain, e.g., cartoon or painting. Stylizing after\nfrontalization also results in degenerated output. In this paper, we propose a\nnovel and unified framework which generates stylized portraits in canonical\nview. With a proposed latent mapper, we analyze and discover frontalization\nmapping in a latent space of StyleGAN to stylize and frontalize at once. In\naddition, our model can be trained with unlabelled 2D image sets, without any\n3D supervision. The effectiveness of our method is demonstrated by experimental\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jeong-gi Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">Dongsik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlowBot3D: Learning 3D Articulation Flow to Manipulate Articulated Objects. (arXiv:2205.04382v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2205.04382","description":"<p>We explore a novel method to perceive and manipulate 3D articulated objects\nthat generalizes to enable a robot to articulate unseen classes of objects. We\npropose a vision-based system that learns to predict the potential motions of\nthe parts of a variety of articulated objects to guide downstream motion\nplanning of the system to articulate the objects. To predict the object\nmotions, we train a neural network to output a dense vector field representing\nthe point-wise motion direction of the points in the point cloud under\narticulation. We then deploy an analytical motion planner based on this vector\nfield to achieve a policy that yields maximum articulation. We train the vision\nsystem entirely in simulation, and we demonstrate the capability of our system\nto generalize to unseen object instances and novel categories in both\nsimulation and the real world, deploying our policy on a Sawyer robot with no\nfinetuning. Results show that our system achieves state-of-the-art performance\nin both simulated and real-world experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisner_B/0/1/0/all/0/1\">Ben Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Harry Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation for Multi-Target Domain Adaptation in Real-Time Person Re-Identification. (arXiv:2205.06237v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06237","description":"<p>Despite the recent success of deep learning architectures, person\nre-identification (ReID) remains a challenging problem in real-word\napplications. Several unsupervised single-target domain adaptation (STDA)\nmethods have recently been proposed to limit the decline in ReID accuracy\ncaused by the domain shift that typically occurs between source and target\nvideo data. Given the multimodal nature of person ReID data (due to variations\nacross camera viewpoints and capture conditions), training a common CNN\nbackbone to address domain shifts across multiple target domains, can provide\nan efficient solution for real-time ReID applications. Although multi-target\ndomain adaptation (MTDA) has not been widely addressed in the ReID literature,\na straightforward approach consists in blending different target datasets, and\nperforming STDA on the mixture to train a common CNN. However, this approach\nmay lead to poor generalization, especially when blending a growing number of\ndistinct target domains to train a smaller CNN.\n</p>\n<p>To alleviate this problem, we introduce a new MTDA method based on knowledge\ndistillation (KD-ReID) that is suitable for real-time person ReID applications.\nOur method adapts a common lightweight student backbone CNN over the target\ndomains by alternatively distilling from multiple specialized teacher CNNs,\neach one adapted on data from a specific target domain. Extensive experiments\nconducted on several challenging person ReID datasets indicate that our\napproach outperforms state-of-art methods for MTDA, including blending methods,\nparticularly when training a compact CNN backbone like OSNet. Results suggest\nthat our flexible MTDA approach can be employed to design cost-effective ReID\nsystems for real-time video surveillance applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remigereau_F/0/1/0/all/0/1\">F&#xe9;lix Remigereau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekhazni_D/0/1/0/all/0/1\">Djebril Mekhazni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdoli_S/0/1/0/all/0/1\">Sajjad Abdoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Meidine_L/0/1/0/all/0/1\">Le Thanh Nguyen-Meidine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1\">Rafael M. O. Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification. (arXiv:2205.12029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12029","description":"<p>Multimodal learning from document data has achieved great success lately as\nit allows to pre-train semantically meaningful features as a prior into a\nlearnable downstream approach. In this paper, we approach the document\nclassification problem by learning cross-modal representations through language\nand vision cues, considering intra- and inter-modality relationships. Instead\nof merging features from different modalities into a common representation\nspace, the proposed method exploits high-level interactions and learns relevant\nsemantic information from effective attention flows within and across\nmodalities. The proposed learning objective is devised between intra- and\ninter-modality alignment tasks, where the similarity distribution per task is\ncomputed by contracting positive sample pairs while simultaneously contrasting\nnegative ones in the common feature representation space}. Extensive\nexperiments on public document classification datasets demonstrate the\neffectiveness and the generalization capacity of our model on both low-scale\nand large-scale datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakkali_S/0/1/0/all/0/1\">Souhail Bakkali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zuheng Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coustaty_M/0/1/0/all/0/1\">Mickael Coustaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinol_M/0/1/0/all/0/1\">Mar&#xe7;al Rusi&#xf1;ol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terrades_O/0/1/0/all/0/1\">Oriol Ramos Terrades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution. (arXiv:2205.12089v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12089","description":"<p>Service robots should be able to interact naturally with non-expert human\nusers, not only to help them in various tasks but also to receive guidance in\norder to resolve ambiguities that might be present in the instruction. We\nconsider the task of visual grounding, where the agent segments an object from\na crowded scene given a natural language description. Modern holistic\napproaches to visual grounding usually ignore language structure and struggle\nto cover generic domains, therefore relying heavily on large datasets.\nAdditionally, their transfer performance in RGB-D datasets suffers due to high\nvisual discrepancy between the benchmark and the target domains. Modular\napproaches marry learning with domain modeling and exploit the compositional\nnature of language to decouple visual representation from language parsing, but\neither rely on external parsers or are trained in an end-to-end fashion due to\nthe lack of strong supervision. In this work, we seek to tackle these\nlimitations by introducing a fully decoupled modular framework for\ncompositional visual grounding of entities, attributes, and spatial relations.\nWe exploit rich scene graph annotations generated in a synthetic domain and\ntrain each module independently. Our approach is evaluated both in simulation\nand in two real RGB-D scene datasets. Experimental results show that the\ndecoupled nature of our framework allows for easy integration with domain\nadaptation approaches for Sim-To-Real visual recognition, offering a\ndata-efficient, robust, and interpretable solution to visual grounding in\nrobotic applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tziafas_G/0/1/0/all/0/1\">Georgios Tziafas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1\">Hamidreza Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Adaptation of Pre-Trained Networks for Domain Shift. (arXiv:2205.15234v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15234","description":"<p>Deep networks are prone to performance degradation when there is a domain\nshift between the source (training) data and target (test) data. Recent\ntest-time adaptation methods update batch normalization layers of pre-trained\nsource models deployed in new target environments with streaming data to\nmitigate such performance degradation. Although such methods can adapt\non-the-fly without first collecting a large target domain dataset, their\nperformance is dependent on streaming conditions such as mini-batch size and\nclass-distribution, which can be unpredictable in practice. In this work, we\npropose a framework for few-shot domain adaptation to address the practical\nchallenges of data-efficient adaptation. Specifically, we propose a constrained\noptimization of feature normalization statistics in pre-trained source models\nsupervised by a small support set from the target domain. Our method is easy to\nimplement and improves source model performance with as few as one sample per\nclass for classification tasks. Extensive experiments on 5 cross-domain\nclassification and 4 semantic segmentation datasets show that our method\nachieves more accurate and reliable performance than test-time adaptation,\nwhile not being constrained by streaming conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan-Sheng Foo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.04459","description":"<p>In order to deploy deep models in a computationally efficient manner, model\nquantization approaches have been frequently used. In addition, as new hardware\nthat supports mixed bitwidth arithmetic operations, recent research on mixed\nprecision quantization (MPQ) begins to fully leverage the capacity of\nrepresentation by searching optimized bitwidths for different layers and\nmodules in a network. However, previous studies mainly search the MPQ strategy\nin a costly scheme using reinforcement learning, neural architecture search,\netc., or simply utilize partial prior knowledge for bitwidth assignment, which\nmight be biased and sub-optimal. In this work, we present a novel Stochastic\nDifferentiable Quantization (SDQ) method that can automatically learn the MPQ\nstrategy in a more flexible and globally-optimized space with smoother gradient\napproximation. Particularly, Differentiable Bitwidth Parameters (DBPs) are\nemployed as the probability factors in stochastic quantization between adjacent\nbitwidth choices. After the optimal MPQ strategy is acquired, we further train\nour network with entropy-aware bin regularization and knowledge distillation.\nWe extensively evaluate our method for several networks on different hardware\n(GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or\nsingle precision quantization with a lower bitwidth and is even better than the\nfull-precision counterparts across various ResNet and MobileNet families,\ndemonstrating the effectiveness and superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xianghong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicaksana_J/0/1/0/all/0/1\">Jeffry Wicaksana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned reconstruction methods with convergence guarantees. (arXiv:2206.05431v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05431","description":"<p>In recent years, deep learning has achieved remarkable empirical success for\nimage reconstruction. This has catalyzed an ongoing quest for precise\ncharacterization of correctness and reliability of data-driven methods in\ncritical use-cases, for instance in medical imaging. Notwithstanding the\nexcellent performance and efficacy of deep learning-based methods, concerns\nhave been raised regarding their stability, or lack thereof, with serious\npractical implications. Significant advances have been made in recent years to\nunravel the inner workings of data-driven image recovery methods, challenging\ntheir widely perceived black-box nature. In this article, we will specify\nrelevant notions of convergence for data-driven image reconstruction, which\nwill form the basis of a survey of learned methods with mathematically rigorous\nreconstruction guarantees. An example that is highlighted is the role of ICNN,\noffering the possibility to combine the power of deep learning with classical\nconvex regularization theory for devising methods that are provably convergent.\n</p>\n<p>This survey article is aimed at both methodological researchers seeking to\nadvance the frontiers of our understanding of data-driven image reconstruction\nmethods as well as practitioners, by providing an accessible description of\nuseful convergence concepts and by placing some of the existing empirical\npractices on a solid mathematical foundation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhadip Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Andreas Hauptmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktem_O/0/1/0/all/0/1\">Ozan &#xd6;ktem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereyra_M/0/1/0/all/0/1\">Marcelo Pereyra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Med-DANet: Dynamic Architecture Network for Efficient Medical Volumetric Segmentation. (arXiv:2206.06575v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.06575","description":"<p>For 3D medical image (e.g. CT and MRI) segmentation, the difficulty of\nsegmenting each slice in a clinical case varies greatly. Previous research on\nvolumetric medical image segmentation in a slice-by-slice manner conventionally\nuse the identical 2D deep neural network to segment all the slices of the same\ncase, ignoring the data heterogeneity among image slices. In this paper, we\nfocus on multi-modal 3D MRI brain tumor segmentation and propose a dynamic\narchitecture network named Med-DANet based on adaptive model selection to\nachieve effective accuracy and efficiency trade-off. For each slice of the\ninput 3D MRI volume, our proposed method learns a slice-specific decision by\nthe Decision Network to dynamically select a suitable model from the predefined\nModel Bank for the subsequent 2D segmentation task. Extensive experimental\nresults on both BraTS 2019 and 2020 datasets show that our proposed method\nachieves comparable or better results than previous state-of-the-art methods\nfor 3D MRI brain tumor segmentation with much less model complexity. Compared\nwith the state-of-the-art 3D method TransBTS, the proposed framework improves\nthe model efficiency by up to 3.5x without sacrificing the accuracy. Our code\nwill be publicly available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervision on Images and Text Reduces Reliance on Visual Shortcut Features. (arXiv:2206.07155v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.07155","description":"<p>Deep learning models trained in a fully supervised manner have been shown to\nrely on so-called \"shortcut\" features. Shortcut features are inputs that are\nassociated with the outcome of interest in the training data, but are either no\nlonger associated or not present in testing or deployment settings. Here we\nprovide experiments that show recent self-supervised models trained on images\nand text provide more robust image representations and reduce the model's\nreliance on visual shortcut features on a realistic medical imaging example.\nAdditionally, we find that these self-supervised models \"forget\" shortcut\nfeatures more quickly than fully supervised ones when fine-tuned on labeled\ndata. Though not a complete solution, our experiments provide compelling\nevidence that self-supervised models trained on images and text provide some\nresilience to visual shortcut features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1\">Anil Palepu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1\">Andrew L Beam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real3D-Aug: Point Cloud Augmentation by Placing Real Objects with Occlusion Handling for 3D Detection and Segmentation. (arXiv:2206.07634v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07634","description":"<p>Object detection and semantic segmentation with the 3D lidar point cloud data\nrequire expensive annotation. We propose a data augmentation method that takes\nadvantage of already annotated data multiple times. We propose an augmentation\nframework that reuses real data, automatically finds suitable placements in the\nscene to be augmented, and handles occlusions explicitly. Due to the usage of\nthe real data, the scan points of newly inserted objects in augmentation\nsustain the physical characteristics of the lidar, such as intensity and\nraydrop. The pipeline proves competitive in training top-performing models for\n3D object detection and semantic segmentation. The new augmentation provides a\nsignificant performance gain in rare and essential classes, notably 6.65%\naverage precision gain for \"Hard\" pedestrian class in KITTI object detection or\n2.14 mean IoU gain in the SemanticKITTI segmentation challenge over the state\nof the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sebek_P/0/1/0/all/0/1\">Petr &#x160;ebek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokorny_S/0/1/0/all/0/1\">&#x160;imon Pokorn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vacek_P/0/1/0/all/0/1\">Patrik Vacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svoboda_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Svoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone fine-tuning without episodic meta-learning dominates for few-shot learning image classification. (arXiv:2206.08138v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08138","description":"<p>Although deep neural networks are capable of achieving performance superior\nto humans on various tasks, they are notorious for requiring large amounts of\ndata and computing resources, restricting their success to domains where such\nresources are available. Metalearning methods can address this problem by\ntransferring knowledge from related tasks, thus reducing the amount of data and\ncomputing resources needed to learn new tasks. We organize the MetaDL\ncompetition series, which provide opportunities for research groups all over\nthe world to create and experimentally assess new meta-(deep)learning solutions\nfor real problems. In this paper, authored collaboratively between the\ncompetition organizers and the top-ranked participants, we describe the design\nof the competition, the datasets, the best experimental results, as well as the\ntop-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active\nteams who made it to the final phase (by outperforming the baseline), making\nover 100 code submissions during the feedback phase. The solutions of the top\nparticipants have been open-sourced. The lessons learned include that learning\ngood representations is essential for effective transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baz_A/0/1/0/all/0/1\">Adrian El Baz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullah_I/0/1/0/all/0/1\">Ihsan Ullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcobaca_E/0/1/0/all/0/1\">Edesio Alcoba&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1\">Andr&#xe9; C. P. L. F. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1\">Fabio Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouk_H/0/1/0/all/0/1\">Henry Gouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Chaoyu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shell Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huisman_M/0/1/0/all/0/1\">Mike Huisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohr_F/0/1/0/all/0/1\">Felix Mohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturk_E/0/1/0/all/0/1\">Ekrem &#xd6;zt&#xfc;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_J/0/1/0/all/0/1\">Jan N. van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haozhe Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Audio-visual Synchronization for Active Speaker Detection. (arXiv:2206.10421v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.10421","description":"<p>Active speaker detection (ASD) systems are important modules for analyzing\nmulti-talker conversations. They aim to detect which speakers or none are\ntalking in a visual scene at any given time. Existing research on ASD does not\nagree on the definition of active speakers. We clarify the definition in this\nwork and require synchronization between the audio and visual speaking\nactivities. This clarification of definition is motivated by our extensive\nexperiments, through which we discover that existing ASD methods fail in\nmodeling the audio-visual synchronization and often classify unsynchronized\nvideos as active speaking. To address this problem, we propose a cross-modal\ncontrastive learning strategy and apply positional encoding in attention\nmodules for supervised ASD models to leverage the synchronization cue.\nExperimental results suggest that our model can successfully detect\nunsynchronized speaking as not speaking, addressing the limitation of current\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wuerkaixi_A/0/1/0/all/0/1\">Abudukelimu Wuerkaixi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer for Contrastive Clustering. (arXiv:2206.12925v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12925","description":"<p>Vision Transformer (ViT) has shown its advantages over the convolutional\nneural network (CNN) with its ability to capture global long-range dependencies\nfor visual representation learning. Besides ViT, contrastive learning is\nanother popular research topic recently. While previous contrastive learning\nworks are mostly based on CNNs, some recent studies have attempted to combine\nViT and contrastive learning for enhanced self-supervised learning. Despite the\nconsiderable progress, these combinations of ViT and contrastive learning\nmostly focus on the instance-level contrastiveness, which often overlook the\nglobal contrastiveness and also lack the ability to directly learn the\nclustering result (e.g., for images). In view of this, this paper presents a\nnovel deep clustering approach termed Vision Transformer for Contrastive\nClustering (VTCC), which for the first time, to our knowledge, unifies the\nTransformer and the contrastive learning for the image clustering task.\nSpecifically, with two random augmentations performed on each image, we utilize\na ViT encoder with two weight-sharing views as the backbone. To remedy the\npotential instability of the ViT, we incorporate a convolutional stem to split\neach augmented sample into a sequence of patches, which uses multiple stacked\nsmall convolutions instead of a big convolution in the patch projection layer.\nBy learning the feature representations for the sequences of patches via the\nbackbone, an instance projector and a cluster projector are further utilized to\nperform the instance-level contrastive learning and the global clustering\nstructure learning, respectively. Experiments on eight image datasets\ndemonstrate the stability (during the training-from-scratch) and the\nsuperiority (in clustering performance) of our VTCC approach over the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Hua-Bao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bowen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Ding-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Implies Generalization via Data-Dependent Generalization Bounds. (arXiv:2206.13497v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.13497","description":"<p>This paper proves that robustness implies generalization via data-dependent\ngeneralization bounds. As a result, robustness and generalization are shown to\nbe connected closely in a data-dependent manner. Our bounds improve previous\nbounds in two directions, to solve an open problem that has seen little\ndevelopment since 2010. The first is to reduce the dependence on the covering\nnumber. The second is to remove the dependence on the hypothesis space. We\npresent several examples, including ones for lasso and deep learning, in which\nour bounds are provably preferable. The experiments on real-world data and\ntheoretical models demonstrate near-exponential improvements in various\nsituations. To achieve these improvements, we do not require additional\nassumptions on the unknown distribution; instead, we only incorporate an\nobservable and computable property of the training samples. A key technical\ninnovation is an improved concentration bound for multinomial random variables\nthat is of independent interest beyond robustness and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luh_K/0/1/0/all/0/1\">Kyle Luh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaoyang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolarFormer: Multi-camera 3D Object Detection with Polar Transformers. (arXiv:2206.15398v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15398","description":"<p>3D object detection in autonomous driving aims to reason \"what\" and \"where\"\nthe objects of interest present in a 3D world. Following the conventional\nwisdom of previous 2D object detection, existing methods often adopt the\ncanonical Cartesian coordinate system with perpendicular axis. However, we\nconjugate that this does not fit the nature of the ego car's perspective, as\neach onboard camera perceives the world in shape of wedge intrinsic to the\nimaging geometry with radical (non-perpendicular) axis. Hence, in this paper we\nadvocate the exploitation of the Polar coordinate system and propose a new\nPolar Transformer (PolarFormer) for more accurate 3D object detection in the\nbird's-eye-view (BEV) taking as input only multi-camera 2D images.\nSpecifically, we design a cross attention based Polar detection head without\nrestriction to the shape of input structure to deal with irregular Polar grids.\nFor tackling the unconstrained object scale variations along Polar's distance\ndimension, we further introduce a multi-scalePolar representation learning\nstrategy. As a result, our model can make best use of the Polar representation\nrasterized via attending to the corresponding image observation in a\nsequence-to-sequence fashion subject to the geometric constraints. Thorough\nexperiments on the nuScenes dataset demonstrate that our PolarFormer\noutperforms significantly state-of-the-art 3D object detection alternatives, as\nwell as yielding competitive performance on BEV semantic segmentation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yanqin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhenwei Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Far Can I Go ? : A Self-Supervised Approach for Deterministic Video Depth Forecasting. (arXiv:2207.00506v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00506","description":"<p>In this paper we present a novel self-supervised method to anticipate the\ndepth estimate for a future, unobserved real-world urban scene. This work is\nthe first to explore self-supervised learning for estimation of monocular depth\nof future unobserved frames of a video. Existing works rely on a large number\nof annotated samples to generate the probabilistic prediction of depth for\nunseen frames. However, this makes it unrealistic due to its requirement for\nlarge amount of annotated depth samples of video. In addition, the\nprobabilistic nature of the case, where one past can have multiple future\noutcomes often leads to incorrect depth estimates. Unlike previous methods, we\nmodel the depth estimation of the unobserved frame as a view-synthesis problem,\nwhich treats the depth estimate of the unseen video frame as an auxiliary task\nwhile synthesizing back the views using learned pose. This approach is not only\ncost effective - we do not use any ground truth depth for training (hence\npractical) but also deterministic (a sequence of past frames map to an\nimmediate future). To address this task we first develop a novel depth\nforecasting network DeFNet which estimates depth of unobserved future by\nforecasting latent features. Second, we develop a channel-attention based pose\nestimation network that estimates the pose of the unobserved frame. Using this\nlearned pose, estimated depth map is reconstructed back into the image domain,\nthus forming a self-supervised solution. Our proposed approach shows\nsignificant improvements in Abs Rel metric compared to state-of-the-art\nalternatives on both short and mid-term forecasting setting, benchmarked on\nKITTI and Cityscapes. Code is available at\nhttps://github.com/sauradip/depthForecasting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sauradip Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_A/0/1/0/all/0/1\">Anran Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift. (arXiv:2207.00769v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.00769","description":"<p>Class distribution plays an important role in learning deep classifiers. When\nthe proportion of each class in the test set differs from the training set, the\nperformance of classification nets usually degrades. Such a label distribution\nshift problem is common in medical diagnosis since the prevalence of disease\nvary over location and time. In this paper, we propose the first method to\ntackle label shift for medical image classification, which effectively adapt\nthe model learned from a single training label distribution to arbitrary\nunknown test label distribution. Our approach innovates distribution\ncalibration to learn multiple representative classifiers, which are capable of\nhandling different one-dominating-class distributions. When given a test image,\nthe diverse classifiers are dynamically aggregated via the consistency-driven\ntest-time adaptation, to deal with the unknown test label distribution. We\nvalidate our method on two important medical image classification tasks\nincluding liver fibrosis staging and COVID-19 severity prediction. Our\nexperiments clearly show the decreased model performance under label shift.\nWith our method, model performance significantly improves on all the test\ndatasets with different label shifts for both medical image diagnosis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1\">Wenao Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Shuang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huimao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Understand Depth?. (arXiv:2207.01077v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01077","description":"<p>Besides image classification, Contrastive Language-Image Pre-training (CLIP)\nhas accomplished extraordinary success for a wide range of vision tasks,\nincluding object-level and 3D space understanding. However, it's still\nchallenging to transfer semantic knowledge learned from CLIP into more\nintricate tasks of quantified targets, such as depth estimation with geometric\ninformation. In this paper, we propose to apply CLIP for zero-shot monocular\ndepth estimation, named DepthCLIP. We found that the patches of the input image\ncould respond to a certain semantic distance token and then be projected to a\nquantified depth bin for coarse estimation. Without any training, our DepthCLIP\nsurpasses existing unsupervised methods and even approaches the early\nfully-supervised networks. To our best knowledge, we are the first to conduct\nzero-shot adaptation from the semantic language knowledge to quantified\ndownstream tasks and perform zero-shot monocular depth estimation. We hope our\nwork could cast a light on future research. The code is available at\nhttps://github.com/Adonis-galaxy/DepthCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DecisioNet: A Binary-Tree Structured Neural Network. (arXiv:2207.01127v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01127","description":"<p>Deep neural networks (DNNs) and decision trees (DTs) are both\nstate-of-the-art classifiers. DNNs perform well due to their representational\nlearning capabilities, while DTs are computationally efficient as they perform\ninference along one route (root-to-leaf) that is dependent on the input data.\nIn this paper, we present DecisioNet (DN), a binary-tree structured neural\nnetwork. We propose a systematic way to convert an existing DNN into a DN to\ncreate a lightweight version of the original model. DecisioNet takes the best\nof both worlds - it uses neural modules to perform representational learning\nand utilizes its tree structure to perform only a portion of the computations.\nWe evaluate various DN architectures, along with their corresponding baseline\nmodels on the FashionMNIST, CIFAR10, and CIFAR100 datasets. We show that the DN\nvariants achieve similar accuracy while significantly reducing the\ncomputational cost of the original network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gottlieb_N/0/1/0/all/0/1\">Noam Gottlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1\">Michael Werman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of ADHD based on Eye Movements during Natural Viewing. (arXiv:2207.01377v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01377","description":"<p>Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental\ndisorder that is highly prevalent and requires clinical specialists to\ndiagnose. It is known that an individual's viewing behavior, reflected in their\neye movements, is directly related to attentional mechanisms and higher-order\ncognitive processes. We therefore explore whether ADHD can be detected based on\nrecorded eye movements together with information about the video stimulus in a\nfree-viewing task. To this end, we develop an end-to-end deep learning-based\nsequence model which we pre-train on a related task for which more data are\navailable. We find that the method is in fact able to detect ADHD and\noutperforms relevant baselines. We investigate the relevance of the input\nfeatures in an ablation study. Interestingly, we find that the model's\nperformance is closely related to the content of the video, which provides\ninsights for future experimental designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuwen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1\">Paul Prasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1\">David R. Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziemian_S/0/1/0/all/0/1\">Sabine Dziemian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stegenwallner_Schutz_M/0/1/0/all/0/1\">Maja Stegenwallner-Sch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krakowczyk_D/0/1/0/all/0/1\">Daniel Krakowczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_S/0/1/0/all/0/1\">Silvia Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_N/0/1/0/all/0/1\">Nicolas Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1\">Tobias Scheffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena A. J&#xe4;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory Efficient Patch-based Training for INR-based GANs. (arXiv:2207.01395v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01395","description":"<p>Recent studies have shown remarkable progress in GANs based on implicit\nneural representation (INR) - an MLP that produces an RGB value given its (x,\ny) coordinate. They represent an image as a continuous version of the\nunderlying 2D signal instead of a 2D array of pixels, which opens new horizons\nfor GAN applications (e.g., zero-shot super-resolution, image outpainting).\nHowever, training existing approaches require a heavy computational cost\nproportional to the image resolution, since they compute an MLP operation for\nevery (x, y) coordinate. To alleviate this issue, we propose a multi-stage\npatch-based training, a novel and scalable approach that can train INR-based\nGANs with a flexible computational cost regardless of the image resolution.\nSpecifically, our method allows to generate and discriminate by patch to learn\nthe local details of the image and learn global structural information by a\nnovel reconstruction loss to enable efficient GAN training. We conduct\nexperiments on several benchmark datasets to demonstrate that our approach\nenhances baseline models in GPU memory while maintaining FIDs at a reasonable\nlevel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Namwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gayoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sungjoo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Detection with Cardboard Human Modeling. (arXiv:2207.02013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02013","description":"<p>Multiview detection uses multiple calibrated cameras with overlapping fields\nof views to locate occluded pedestrians. In this field, existing methods\ntypically adopt a \"human modeling - aggregation\" strategy. To find robust\npedestrian representations, some intuitively use locations of detected 2D\nbounding boxes, while others use entire frame features projected to the ground\nplane. However, the former does not consider human appearance and leads to many\nambiguities, and the latter suffers from projection errors due to the lack of\naccurate height of the human torso and head. In this paper, we propose a new\npedestrian representation scheme based on human point clouds modeling.\nSpecifically, using ray tracing for holistic human depth estimation, we model\npedestrians as upright, thin cardboard point clouds on the ground. Then, we\naggregate the point clouds of the pedestrian cardboard across multiple views\nfor a final decision. Compared with existing representations, the proposed\nmethod explicitly leverages human appearance and reduces projection errors\nsignificantly by relatively accurate height estimation. On two standard\nevaluation benchmarks, the proposed method achieves very competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiahao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zicheng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yunzhong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.02327","description":"<p>Diffusion MRI tractography is an advanced imaging technique for quantitative\nmapping of the brain's structural connectivity. Whole brain tractography (WBT)\ndata contains over hundreds of thousands of individual fiber streamlines\n(estimated brain connections), and this data is usually parcellated to create\ncompact representations for data analysis applications such as disease\nclassification. In this paper, we propose a novel parcellation-free WBT\nanalysis framework, TractoFormer, that leverages tractography information at\nthe level of individual fiber streamlines and provides a natural mechanism for\ninterpretation of results using the attention mechanism of transformers.\nTractoFormer includes two main contributions. First, we propose a novel and\nsimple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber\nspatial relationships and any feature of interest that can be computed from\nindividual fibers (such as FA or MD). Second, we design a network based on\nvision transformers (ViTs) that includes: 1) data augmentation to overcome\nmodel overfitting on small datasets, 2) identification of discriminative fibers\nfor interpretation of results, and 3) ensemble learning to leverage fiber\ninformation from different brain regions. In a synthetic data experiment,\nTractoFormer successfully identifies discriminative fibers with simulated group\ndifferences. In a disease classification experiment comparing several methods,\nTractoFormer achieves the highest accuracy in classifying schizophrenia vs\ncontrol. Discriminative fibers are identified in left hemispheric frontal and\nparietal superficial white matter regions, which have previously been shown to\nbe affected in schizophrenia patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_T/0/1/0/all/0/1\">Tengfei Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Westin_C/0/1/0/all/0/1\">Carl-Fredrik Westin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage Decision Improves Open-Set Panoptic Segmentation. (arXiv:2207.02504v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02504","description":"<p>Open-set panoptic segmentation (OPS) problem is a new research direction\naiming to perform segmentation for both \\known classes and \\unknown classes,\ni.e., the objects (\"things\") that are never annotated in the training set. The\nmain challenges of OPS are twofold: (1) the infinite possibility of the\n\\unknown object appearances makes it difficult to model them from a limited\nnumber of training data. (2) at training time, we are only provided with the\n\"void\" category, which essentially mixes the \"unknown thing\" and \"background\"\nclasses. We empirically find that directly using \"void\" category to supervise\n\\known class or \"background\" without screening will not lead to a satisfied OPS\nresult. In this paper, we propose a divide-and-conquer scheme to develop a\ntwo-stage decision process for OPS. We show that by properly combining a \\known\nclass discriminator with an additional class-agnostic object prediction head,\nthe OPS performance can be significantly improved. Specifically, we first\npropose to create a classifier with only \\known categories and let the \"void\"\nclass proposals achieve low prediction probability from those categories. Then\nwe distinguish the \"unknown things\" from the background by using the additional\nobject prediction head. To further boost performance, we introduce \"unknown\nthings\" pseudo-labels generated from up-to-date models and a heuristic rule to\nenrich the training set. Our extensive experimental evaluation shows that our\napproach significantly improves \\unknown class panoptic quality, with more than\n30\\% relative improvements than the existing best-performed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hai-Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yufei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIC 4th Challenge: Semantic-Assisted Multi-Feature Encoding and Multi-Head Decoding for Dense Video Captioning. (arXiv:2207.02583v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02583","description":"<p>The task of Dense Video Captioning (DVC) aims to generate captions with\ntimestamps for multiple events in one video. Semantic information plays an\nimportant role for both localization and description of DVC. We present a\nsemantic-assisted dense video captioning model based on the encoding-decoding\nframework. In the encoding stage, we design a concept detector to extract\nsemantic information, which is then fused with multi-modal visual features to\nsufficiently represent the input video. In the decoding stage, we design a\nclassification head, paralleled with the localization and captioning heads, to\nprovide semantic supervision. Our method achieves significant improvements on\nthe YouMakeup dataset under DVC evaluation metrics and achieves high\nperformance in the Makeup Dense Video Captioning (MDVC) task of PIC 4th\nChallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunfeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Human Pose Estimation in Art-historical Images. (arXiv:2207.02976v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02976","description":"<p>Gesture as language of non-verbal communication has been theoretically\nestablished since the 17th century. However, its relevance for the visual arts\nhas been expressed only sporadically. This may be primarily due to the sheer\noverwhelming amount of data that traditionally had to be processed by hand.\nWith the steady progress of digitization, though, a growing number of\nhistorical artifacts have been indexed and made available to the public,\ncreating a need for automatic retrieval of art-historical motifs with similar\nbody constellations or poses. Since the domain of art differs significantly\nfrom existing real-world data sets for human pose estimation due to its style\nvariance, this presents new challenges. In this paper, we propose a novel\napproach to estimate human poses in art-historical images. In contrast to\nprevious work that attempts to bridge the domain gap with pre-trained models or\nthrough style transfer, we suggest semi-supervised learning for both object and\nkeypoint detection. Furthermore, we introduce a novel domain-specific art data\nset that includes both bounding box and keypoint annotations of human figures.\nOur approach achieves significantly better results than methods that use\npre-trained models or style transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Springstein_M/0/1/0/all/0/1\">Matthias Springstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Stefanie Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althaus_C/0/1/0/all/0/1\">Christian Althaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design of Human Machine Interface through vision-based low-cost Hand Gesture Recognition system based on deep CNN. (arXiv:2207.03112v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03112","description":"<p>In this work, a real-time hand gesture recognition system-based\nhuman-computer interface (HCI) is presented. The system consists of six stages:\n(1) hand detection, (2) gesture segmentation, (3) use of six pre-trained CNN\nmodels by using the transfer-learning method, (4) building an interactive\nhuman-machine interface, (5) development of a gesture-controlled virtual mouse,\n(6) use of Kalman filter to estimate the hand position, based on that the\nsmoothness of the motion of pointer is improved. Six pre-trained convolutional\nneural network (CNN) models (VGG16, VGG19, ResNet50, ResNet101, Inception-V1,\nand MobileNet-V1) have been used to classify hand gesture images. Three\nmulti-class datasets (two publicly and one custom) have been used to evaluate\nthe model performances. Considering the models' performances, it has been\nobserved that Inception-V1 has significantly shown a better classification\nperformance compared to the other five pre-trained models in terms of accuracy,\nprecision, recall, and F-score values. The gesture recognition system is\nexpanded and used to control multimedia applications (like VLC player, audio\nplayer, file management, playing 2D Super-Mario-Bros game, etc.) with different\ncustomized gesture commands in real-time scenarios. The average speed of this\nsystem has reached 35 fps (frame per seconds), which meets the requirements for\nthe real-time scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1\">Abir Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1\">Tapas Kumar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_R/0/1/0/all/0/1\">Ratnakar Dash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Zero-shot Learning via Contrastive Optimization of Attribute Representations. (arXiv:2207.03824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03824","description":"<p>Zero-shot learning (ZSL) aims to recognize classes that do not have samples\nin the training set. One representative solution is to directly learn an\nembedding function associating visual features with corresponding class\nsemantics for recognizing new classes. Many methods extend upon this solution,\nand recent ones are especially keen on extracting rich features from images,\ne.g. attribute features. These attribute features are normally extracted within\neach individual image; however, the common traits for features across images\nyet belonging to the same attribute are not emphasized. In this paper, we\npropose a new framework to boost ZSL by explicitly learning attribute\nprototypes beyond images and contrastively optimizing them with attribute-level\nfeatures within images. Besides the novel architecture, two elements are\nhighlighted for attribute representations: a new prototype generation module is\ndesigned to generate attribute prototypes from attribute semantics; a hard\nexample-based contrastive optimization scheme is introduced to reinforce\nattribute-level features in the embedding space. We explore two alternative\nbackbones, CNN-based and transformer-based, to build our framework and conduct\nexperiments on three standard benchmarks, CUB, SUN, AwA2. Results on these\nbenchmarks demonstrate that our method improves the state of the art by a\nconsiderable margin. Our codes will be available at\nhttps://github.com/dyabel/CoAR-ZSL.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Collapse in Contrast Maximization Frameworks. (arXiv:2207.04007v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04007","description":"<p>Contrast maximization (CMax) is a framework that provides state-of-the-art\nresults on several event-based computer vision tasks, such as ego-motion or\noptical flow estimation. However, it may suffer from a problem called event\ncollapse, which is an undesired solution where events are warped into too few\npixels. As prior works have largely ignored the issue or proposed workarounds,\nit is imperative to analyze this phenomenon in detail. Our work demonstrates\nevent collapse in its simplest form and proposes collapse metrics by using\nfirst principles of space-time deformation based on differential geometry and\nphysics. We experimentally show on publicly available datasets that the\nproposed metrics mitigate event collapse and do not harm well-posed warps. To\nthe best of our knowledge, regularizers based on the proposed metrics are the\nonly effective solution against event collapse in the experimental settings\nconsidered, compared with other methods. We hope that this work inspires\nfurther research to tackle more complex warp models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiba_S/0/1/0/all/0/1\">Shintaro Shiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_Y/0/1/0/all/0/1\">Yoshimitsu Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Early Exit Networks. (arXiv:2207.03644v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2207.03644","description":"<p>Deep learning models that perform well often have high computational costs.\nIn this paper, we combine two approaches that try to reduce the computational\ncost while keeping the model performance high: pruning and early exit networks.\nWe evaluate two approaches of pruning early exit networks: (1) pruning the\nentire network at once, (2) pruning the base network and additional linear\nclassifiers in an ordered fashion. Experimental results show that pruning the\nentire network at once is a better strategy in general. However, at high\naccuracy rates, the two approaches have a similar performance, which implies\nthat the processes of pruning and early exit can be separated without loss of\noptimality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gormez_A/0/1/0/all/0/1\">Alperen G&#xf6;rmez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyuncu_E/0/1/0/all/0/1\">Erdem Koyuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}