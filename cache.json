{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Reference-Centric Models for Grounded Collaborative Dialogue. (arXiv:2109.05042v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05042","description":"<p>We present a grounded neural dialogue model that successfully collaborates\nwith people in a partially-observable reference game. We focus on a setting\nwhere two agents each observe an overlapping part of a world context and need\nto identify and agree on some object they share. Therefore, the agents should\npool their information and communicate pragmatically to solve the task. Our\ndialogue agent accurately grounds referents from the partner's utterances using\na structured reference resolver, conditions on these referents using a\nrecurrent memory, and uses a pragmatic generation procedure to ensure the\npartner can resolve the references the agent produces. We evaluate on the\nOneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving\na number of dots arranged on a board with continuously varying positions,\nsizes, and shades. Our agent substantially outperforms the previous state of\nthe art for the task, obtaining a 20% relative improvement in successful task\ncompletion in self-play evaluations and a 50% relative improvement in success\nin human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin T. Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05052","description":"<p>Knowledge-dependent tasks typically use two sources of knowledge: parametric,\nlearned at training time, and contextual, given as a passage at inference time.\nTo understand how models use these sources together, we formalize the problem\nof knowledge conflicts, where the contextual information contradicts the\nlearned information. Analyzing the behaviour of popular models, we measure\ntheir over-reliance on memorized information (the cause of hallucinations), and\nuncover important factors that exacerbate this behaviour. Lastly, we propose a\nsimple method to mitigate over-reliance on parametric knowledge, which\nminimizes hallucination, and improves out-of-distribution generalization by\n4%-7%. Our findings demonstrate the importance for practitioners to evaluate\nmodel tendency to hallucinate rather than read, and show that our mitigation\nstrategy encourages generalization to evolving information (i.e.,\ntime-dependent queries). To encourage these practices, we have released our\nframework for generating knowledge conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perisetla_K/0/1/0/all/0/1\">Kartik Perisetla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_N/0/1/0/all/0/1\">Nikhil Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Chris DuBois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Turn Modeling for Dialogue Act Classification. (arXiv:2109.05056v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05056","description":"<p>Dialogue Act (DA) classification is the task of classifying utterances with\nrespect to the function they serve in a dialogue. Existing approaches to DA\nclassification model utterances without incorporating the turn changes among\nspeakers throughout the dialogue, therefore treating it no different than\nnon-interactive written text. In this paper, we propose to integrate the turn\nchanges in conversations among speakers when modeling DAs. Specifically, we\nlearn conversation-invariant speaker turn embeddings to represent the speaker\nturns in a conversation; the learned speaker turn embeddings are then merged\nwith the utterance embeddings for the downstream task of DA classification.\nWith this simple yet effective mechanism, our model is able to capture the\nsemantics from the dialogue content while accounting for different speaker\nturns in a conversation. Validation on three benchmark public datasets\ndemonstrates superior performance of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavabi_L/0/1/0/all/0/1\">Leili Tavabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1\">Mohammad Soleymani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FBERT: A Neural Transformer for Identifying Offensive Content. (arXiv:2109.05074v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05074","description":"<p>Transformer-based models such as BERT, XLNET, and XLM-R have achieved\nstate-of-the-art performance across various NLP tasks including the\nidentification of offensive language and hate speech, an important problem in\nsocial media. In this paper, we present fBERT, a BERT model retrained on SOLID,\nthe largest English offensive language identification corpus available with\nover $1.4$ million offensive instances. We evaluate fBERT's performance on\nidentifying offensive content on multiple English datasets and we test several\nthresholds for selecting instances from SOLID. The fBERT model will be made\nfreely available to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Diptanu Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Self-Disclosure In Neural Dialog Models By Candidate Re-ranking. (arXiv:2109.05090v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05090","description":"<p>Neural language modelling has progressed the state-of-the-art in different\ndownstream Natural Language Processing (NLP) tasks. One such area is of\nopen-domain dialog modelling, neural dialog models based on GPT-2 such as\nDialoGPT have shown promising performance in single-turn conversation. However,\nsuch (neural) dialog models have been criticized for generating responses which\nalthough may have relevance to the previous human response, tend to quickly\ndissipate human interest and descend into trivial conversation. One reason for\nsuch performance is the lack of explicit conversation strategy being employed\nin human-machine conversation. Humans employ a range of conversation strategies\nwhile engaging in a conversation, one such key social strategies is\nSelf-disclosure(SD). A phenomenon of revealing information about one-self to\nothers. Social penetration theory (SPT) proposes that communication between two\npeople moves from shallow to deeper levels as the relationship progresses\nprimarily through self-disclosure. Disclosure helps in creating rapport among\nthe participants engaged in a conversation. In this paper, Self-disclosure\nenhancement architecture (SDEA) is introduced utilizing Self-disclosure Topic\nModel (SDTM) during inference stage of a neural dialog model to re-rank\nresponse candidates to enhance self-disclosure in single-turn responses from\nfrom the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_M/0/1/0/all/0/1\">Mayank Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowan_B/0/1/0/all/0/1\">Benjamin Cowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_V/0/1/0/all/0/1\">Vincent Wade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models. (arXiv:2109.05093v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05093","description":"<p>Large pre-trained language models for textual data have an unconstrained\noutput space; at each decoding step, they can produce any of 10,000s of\nsub-word tokens. When fine-tuned to target constrained formal languages like\nSQL, these models often generate invalid code, rendering it unusable. We\npropose PICARD (code and trained models available at\nhttps://github.com/ElementAI/picard), a method for constraining auto-regressive\ndecoders of language models through incremental parsing. PICARD helps to find\nvalid output sequences by rejecting inadmissible tokens at each decoding step.\nOn the challenging Spider and CoSQL text-to-SQL translation tasks, we show that\nPICARD transforms fine-tuned T5 models with passable performance into\nstate-of-the-art solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schucher_N/0/1/0/all/0/1\">Nathan Schucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. (arXiv:2109.05097v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05097","description":"<p>A hyperbole is an intentional and creative exaggeration not to be taken\nliterally. Despite its ubiquity in daily life, the computational explorations\nof hyperboles are scarce. In this paper, we tackle the under-explored and\nchallenging task: sentence-level hyperbole generation. We start with a\nrepresentative syntactic pattern for intensification and systematically study\nthe semantic (commonsense and counterfactual) relationships between each\ncomponent in such hyperboles. Next, we leverage the COMeT and reverse COMeT\nmodels to do commonsense and counterfactual inference. We then generate\nmultiple hyperbole candidates based on our findings from the pattern, and train\nneural classifiers to rank and select high-quality hyperboles. Automatic and\nhuman evaluations show that our generation method is able to generate\nhyperboles creatively with high success rate and intensity scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Arvind krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models. (arXiv:2109.05105v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05105","description":"<p>Can we get existing language models and refine them for zero-shot commonsense\nreasoning? This paper presents an initial study exploring the feasibility of\nzero-shot commonsense reasoning for the Winograd Schema Challenge by\nformulating the task as self-supervised refinement of a pre-trained language\nmodel. In contrast to previous studies that rely on fine-tuning annotated\ndatasets, we seek to boost conceptualization via loss landscape refinement. To\nthis end, we propose a novel self-supervised learning approach that refines the\nlanguage model utilizing a set of linguistic perturbations of similar concept\nrelationships. Empirical analysis of our conceptually simple framework\ndemonstrates the viability of zero-shot commonsense reasoning on multiple\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1\">Tassilo Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Contrastive Learning for Winograd Schemas. (arXiv:2109.05108v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05108","description":"<p>Self-supervised learning has recently attracted considerable attention in the\nNLP community for its ability to learn discriminative features using a\ncontrastive objective. This paper investigates whether contrastive learning can\nbe extended to Transfomer attention to tackling the Winograd Schema Challenge.\nTo this end, we propose a novel self-supervised framework, leveraging a\ncontrastive loss directly at the level of self-attention. Experimental analysis\nof our attention-based models on multiple datasets demonstrates superior\ncommonsense reasoning capabilities. The proposed approach outperforms all\ncomparable unsupervised approaches while occasionally surpassing supervised\nones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1\">Tassilo Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Latent Tree Induction with Distant Supervision via Span Constraints. (arXiv:2109.05112v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05112","description":"<p>For over thirty years, researchers have developed and analyzed methods for\nlatent tree induction as an approach for unsupervised syntactic parsing.\nNonetheless, modern systems still do not perform well enough compared to their\nsupervised counterparts to have any practical use as structural annotation of\ntext. In this work, we present a technique that uses distant supervision in the\nform of span constraints (i.e. phrase bracketing) to improve performance in\nunsupervised constituency parsing. Using a relatively small number of span\nconstraints we can substantially improve the output from DIORA, an already\ncompetitive unsupervised parsing system. Compared with full parse tree\nannotation, span constraints can be acquired with minimal effort, such as with\na lexicon derived from Wikipedia, to find exact text matches. Our experiments\nshow span constraints based on entities improves constituency parsing on\nEnglish WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to\nany domain where span constraints are easily attainable, and as a case study we\ndemonstrate its effectiveness by parsing biomedical text from the CRAFT\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1\">Andrew Drozdov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jay Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rongali_S/0/1/0/all/0/1\">Subendhu Rongali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkbeiner_D/0/1/0/all/0/1\">Dylan Finkbeiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1\">Shilpa Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05115","description":"<p>In this paper, we propose an approach to improve image captioning solutions\nfor images with novel objects that do not have caption labels in the training\ndataset. Our approach is agnostic to model architecture, and primarily focuses\non training technique that uses existing fully paired image-caption data and\nthe images with only the novel object detection labels (partially paired data).\nWe create synthetic paired captioning data for these novel objects by\nleveraging context from existing image-caption pairs. We further re-use these\npartially paired images with novel objects to create pseudo-label captions that\nare used to fine-tune the captioning model. Using a popular captioning model\n(Up-Down) as baseline, our approach achieves state-of-the-art results on\nheld-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for\nnovel object images by 75.8 and 26.6 points respectively, compared to baseline\nmodel that does not use partially paired images during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bujimalla_S/0/1/0/all/0/1\">Shashank Bujimalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subedar_M/0/1/0/all/0/1\">Mahesh Subedar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tickoo_O/0/1/0/all/0/1\">Omesh Tickoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])","link":"http://arxiv.org/abs/2109.05125","description":"<p>Both image-caption pairs and translation pairs provide the means to learn\ndeep representations of and connections between languages. We use both types of\npairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual\nencoder that solves two tasks: 1) image-text matching and 2) translation pair\nmatching. By incorporating billions of translation pairs, MURAL extends ALIGN\n(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion\nnoisy image-text pairs. When using the same encoders, MURAL's performance\nmatches or exceeds ALIGN's cross-modal retrieval performance on well-resourced\nlanguages across several datasets. More importantly, it considerably improves\nperformance on under-resourced languages, showing that text-text learning can\novercome a paucity of image-caption examples for these languages. On the\nWikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean\nrecall by 8.1% on average for eight under-resourced languages and by 6.8% on\naverage when fine-tuning. We additionally show that MURAL's text\nrepresentations cluster not only with respect to genealogical connections but\nalso based on areal linguistics, such as the Balkan Sprachbund.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aashi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-REX: Dialogue Relation Extraction with Explanations. (arXiv:2109.05126v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05126","description":"<p>Existing research studies on cross-sentence relation extraction in long-form\nmulti-party conversations aim to improve relation extraction without\nconsidering the explainability of such methods. This work addresses that gap by\nfocusing on extracting explanations that indicate that a relation exists while\nusing only partially labeled data. We propose our model-agnostic framework,\nD-REX, a policy-guided semi-supervised algorithm that explains and ranks\nrelations. We frame relation extraction as a re-ranking task and include\nrelation- and entity-specific explanations as an intermediate step of the\ninference process. We find that about 90% of the time, human annotators prefer\nD-REX's explanations over a strong BERT-based joint relation extraction and\nexplanation model. Finally, our evaluations on a dialogue relation extraction\ndataset show that our method is simple yet effective and achieves a\nstate-of-the-art F1 score on relation extraction, improving upon existing\nmethods by 13.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Embar_V/0/1/0/all/0/1\">Varun Embar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refocusing on Relevance: Personalization in NLG. (arXiv:2109.05140v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05140","description":"<p>Many NLG tasks such as summarization, dialogue response, or open domain\nquestion answering focus primarily on a source text in order to generate a\ntarget response. This standard approach falls short, however, when a user's\nintent or context of work is not easily recoverable based solely on that source\ntext -- a scenario that we argue is more of the rule than the exception. In\nthis work, we argue that NLG systems in general should place a much higher\nlevel of emphasis on making use of additional context, and suggest that\nrelevance (as used in Information Retrieval) be thought of as a crucial tool\nfor designing user-oriented text-generating tasks. We further discuss possible\nharms and hazards around such personalization, and argue that value-sensitive\ndesign represents a crucial path forward through these challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dudy_S/0/1/0/all/0/1\">Shiran Dudy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedrick_S/0/1/0/all/0/1\">Steven Bedrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webber_B/0/1/0/all/0/1\">Bonnie Webber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extract, Integrate, Compete: Towards Verification Style Reading Comprehension. (arXiv:2109.05149v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05149","description":"<p>In this paper, we present a new verification style reading comprehension\ndataset named VGaokao from Chinese Language tests of Gaokao. Different from\nexisting efforts, the new dataset is originally designed for native speakers'\nevaluation, thus requiring more advanced language understanding skills. To\naddress the challenges in VGaokao, we propose a novel Extract-Integrate-Compete\napproach, which iteratively selects complementary evidence with a novel query\nupdating mechanism and adaptively distills supportive evidence, followed by a\npairwise competition to push models to learn the subtle difference among\nsimilar text pieces. Experiments show that our methods outperform various\nbaselines on VGaokao with retrieved complementary evidence, while having the\nmerits of efficiency and explainability. Our dataset and code are released for\nfurther research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural SQL: Making SQL Easier to Infer from Natural Language Specifications. (arXiv:2109.05153v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05153","description":"<p>Addressing the mismatch between natural language descriptions and the\ncorresponding SQL queries is a key challenge for text-to-SQL translation. To\nbridge this gap, we propose an SQL intermediate representation (IR) called\nNatural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities\nof SQL, while it simplifies the queries as follows: (1) dispensing with\noperators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are\nusually hard to find counterparts for in the text descriptions; (2) removing\nthe need for nested subqueries and set operators; and (3) making schema linking\neasier by reducing the required number of schema items. On Spider, a\nchallenging text-to-SQL benchmark that contains complex and nested SQL queries,\nwe demonstrate that NatSQL outperforms other IRs, and significantly improves\nthe performance of several previous SOTA models. Furthermore, for existing\nmodels that do not support executable SQL generation, NatSQL easily enables\nthem to generate executable SQL queries, and achieves the new state-of-the-art\nexecution accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yujian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinxia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodward_J/0/1/0/all/0/1\">John R. Woodward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drake_J/0/1/0/all/0/1\">John Drake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiaofu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization. (arXiv:2109.05157v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05157","description":"<p>Recently, there has been significant progress in studying neural networks for\ntranslating text descriptions into SQL queries under the zero-shot cross-domain\nsetting. Despite achieving good performance on some public benchmarks, we\nobserve that existing text-to-SQL models do not generalize when facing domain\nknowledge that does not frequently appear in the training data, which may\nrender the worse prediction performance for unseen domains. In this work, we\ninvestigate the robustness of text-to-SQL models when the questions require\nrarely observed domain knowledge. In particular, we define five types of domain\nknowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge),\na human-curated dataset based on the Spider benchmark for text-to-SQL\ntranslation. NL questions in Spider-DK are selected from Spider, and we modify\nsome samples by adding domain knowledge that reflects real-world question\nparaphrases. We demonstrate that the prediction accuracy dramatically drops on\nsamples that require such domain knowledge, even if the domain knowledge\nappears in the training set, and the model provides the correct predictions for\nrelated training samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yujian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StreamHover: Livestream Transcript Summarization and Annotation. (arXiv:2109.05160v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05160","description":"<p>With the explosive growth of livestream broadcasting, there is an urgent need\nfor new summarization technology that enables us to create a preview of\nstreamed content and tap into this wealth of knowledge. However, the problem is\nnontrivial due to the informal nature of spoken language. Further, there has\nbeen a shortage of annotated datasets that are necessary for transcript\nsummarization. In this paper, we present StreamHover, a framework for\nannotating and summarizing livestream transcripts. With a total of over 500\nhours of videos annotated with both extractive and abstractive summaries, our\nbenchmark dataset is significantly larger than currently existing annotated\ncorpora. We explore a neural extractive summarization model that leverages\nvector-quantized variational autoencoder to learn latent vector representations\nof spoken utterances and identify salient utterances from the transcripts to\nform summaries. We show that our model generalizes better and improves\nperformance over strong baselines. The results of this study provide an avenue\nfor future research to improve summarization solutions for efficient browsing\nof livestreams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganter_T/0/1/0/all/0/1\">Tim Ganter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Walter Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1\">Jonathan Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1\">Hassan Foroosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Categorization of Social Knowledge for Commonsense Question Answering. (arXiv:2109.05168v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05168","description":"<p>Large pre-trained language models (PLMs) have led to great success on various\ncommonsense question answering (QA) tasks in an end-to-end fashion. However,\nlittle attention has been paid to what commonsense knowledge is needed to\ndeeply characterize these QA tasks. In this work, we proposed to categorize the\nsemantics needed for these tasks using the SocialIQA as an example. Building\nupon our labeled social knowledge categories dataset on top of SocialIQA, we\nfurther train neural QA models to incorporate such social knowledge categories\nand relation information from a knowledge base. Unlike previous work, we\nobserve our models with semantic categorizations of social knowledge can\nachieve comparable performance with a relatively simple model and smaller size\ncompared to other complex approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xiaochen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"College Student Retention Risk Analysis From Educational Database using Multi-Task Multi-Modal Neural Fusion. (arXiv:2109.05178v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05178","description":"<p>We develop a Multimodal Spatiotemporal Neural Fusion network for Multi-Task\nLearning (MSNF-MTCL) to predict 5 important students' retention risks: future\ndropout, next semester dropout, type of dropout, duration of dropout and cause\nof dropout. First, we develop a general purpose multi-modal neural fusion\nnetwork model MSNF for learning students' academic information representation\nby fusing spatial and temporal unstructured advising notes with spatiotemporal\nstructured data. MSNF combines a Bidirectional Encoder Representations from\nTransformers (BERT)-based document embedding framework to represent each\nadvising note, Long-Short Term Memory (LSTM) network to model temporal advising\nnote embeddings, LSTM network to model students' temporal performance variables\nand students' static demographics altogether. The final fused representation\nfrom MSNF has been utilized on a Multi-Task Cascade Learning (MTCL) model\ntowards building MSNF-MTCL for predicting 5 student retention risks. We\nevaluate MSNFMTCL on a large educational database consists of 36,445 college\nstudents over 18 years period of time that provides promising performances\ncomparing with the nearest state-of-art models. Additionally, we test the\nfairness of such model given the existence of biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Arif Ul Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05179","description":"<p>Generating high quality question-answer pairs is a hard but meaningful task.\nAlthough previous works have achieved great results on answer-aware question\ngeneration, it is difficult to apply them into practical application in the\neducation field. This paper for the first time addresses the question-answer\npair generation task on the real-world examination data, and proposes a new\nunified framework on RACE. To capture the important information of the input\npassage we first automatically generate(rather than extracting) keyphrases,\nthus this task is reduced to keyphrase-question-answer triplet joint\ngeneration. Accordingly, we propose a multi-agent communication model to\ngenerate and optimize the question and keyphrases iteratively, and then apply\nthe generated question and keyphrases to guide the generation of answers. To\nestablish a solid benchmark, we build our model on the strong generative\npre-training model. Experimental results show that our model makes great\nbreakthroughs in the question-answer pair generation task. Moreover, we make a\ncomprehensive analysis on our model, suggesting new directions for this\nchallenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_F/0/1/0/all/0/1\">Fanyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker-Oriented Latent Structures for Dialogue-Based Relation Extraction. (arXiv:2109.05182v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05182","description":"<p>Dialogue-based relation extraction (DiaRE) aims to detect the structural\ninformation from unstructured utterances in dialogues. Existing relation\nextraction models may be unsatisfactory under such a conversational setting,\ndue to the entangled logic and information sparsity issues in utterances\ninvolving multiple speakers. To this end, we introduce SOLS, a novel model\nwhich can explicitly induce speaker-oriented latent structures for better\nDiaRE. Specifically, we learn latent structures to capture the relationships\namong tokens beyond the utterance boundaries, alleviating the entangled logic\nissue. During the learning process, our speaker-specific regularization method\nprogressively highlights speaker-related key clues and erases the irrelevant\nones, alleviating the information sparsity issue. Experiments on three public\ndatasets demonstrate the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guoqing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1\">Sicong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v1 [cs.MM])","link":"http://arxiv.org/abs/2109.05184","description":"<p>Internet memes have become powerful means to transmit political,\npsychological, and socio-cultural ideas. Although memes are typically humorous,\nrecent days have witnessed an escalation of harmful memes used for trolling,\ncyberbullying, and abusing social entities. Detecting such harmful memes is\nchallenging as they can be highly satirical and cryptic. Moreover, while\nprevious work has focused on specific aspects of memes such as hate speech and\npropaganda, there has been little work on harm in general, and only one\nspecialized dataset for it. Here, we focus on bridging this gap. In particular,\nwe aim to solve two novel tasks: detecting harmful memes and identifying the\nsocial entities they target. We further extend the recently released HarMeme\ndataset to generalize on two prevalent topics - COVID-19 and US politics and\nname the two datasets as Harm-C and Harm-P, respectively. We then propose\nMOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a\nnovel multimodal (text + image) deep neural model, which uses global and local\nperspectives to detect harmful memes. MOMENTA identifies the object proposals\nand attributes and uses a multimodal model to perceive the comprehensive\ncontext in which the objects and the entities are portrayed in a given meme.\nMOMENTA is interpretable and generalizable, and it outperforms numerous\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05186","description":"<p>This paper investigates continual learning for semantic parsing. In this\nsetting, a neural semantic parser learns tasks sequentially without accessing\nfull training data from previous tasks. Direct application of the SOTA\ncontinual learning algorithms to this problem fails to achieve comparable\nperformance with re-training models with all seen tasks because they have not\nconsidered the special properties of structured outputs yielded by semantic\nparsers. Therefore, we propose TotalRecall, a continual learning method\ndesigned for neural semantic parsers from two aspects: i) a sampling method for\nmemory replay that diversifies logical form templates and balances\ndistributions of parse actions in a memory; ii) a two-stage training method\nthat significantly improves generalization capability of the parsers across\ntasks. We conduct extensive experiments to study the research problems involved\nin continual semantic parsing and demonstrate that a neural semantic parser\ntrained with TotalRecall achieves superior performance than the one trained\ndirectly with the SOTA continual learning algorithms and achieve a 3-6 times\nspeedup compared to re-training from scratch. Code and datasets are available\nat: https://github.com/zhuang-li/cl_nsp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopicRefine: Joint Topic Prediction and Dialogue Response Generation for Multi-turn End-to-End Dialogue System. (arXiv:2109.05187v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05187","description":"<p>A multi-turn dialogue always follows a specific topic thread, and topic shift\nat the discourse level occurs naturally as the conversation progresses,\nnecessitating the model's ability to capture different topics and generate\ntopic-aware responses. Previous research has either predicted the topic first\nand then generated the relevant response, or simply applied the attention\nmechanism to all topics, ignoring the joint distribution of the topic\nprediction and response generation models and resulting in uncontrollable and\nunrelated responses. In this paper, we propose a joint framework with a topic\nrefinement mechanism to learn these two tasks simultaneously. Specifically, we\ndesign a three-pass iteration mechanism to generate coarse response first, then\npredict corresponding topics, and finally generate refined response conditioned\non predicted topics. Moreover, we utilize GPT2DoubleHeads and BERT for the\ntopic prediction task respectively, aiming to investigate the effects of joint\nlearning and the understanding ability of GPT model. Experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance at response generation task and the great potential understanding\ncapability of GPT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Mingyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zimo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Knowledge from Language Models for Event Extraction. (arXiv:2109.05190v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05190","description":"<p>Eliciting knowledge contained in language models via prompt-based learning\nhas shown great potential in many natural language processing tasks, such as\ntext classification and generation. Whereas, the applications for more complex\ntasks such as event extraction are less studied, since the design of prompt is\nnot straightforward due to the complicated types and arguments. In this paper,\nwe explore to elicit the knowledge from pre-trained language models for event\ntrigger detection and argument extraction. Specifically, we present various\njoint trigger/argument prompt methods, which can elicit more complementary\nknowledge by modeling the interactions between different triggers or arguments.\nThe experimental results on the benchmark dataset, namely ACE2005, show the\ngreat advantages of our proposed approach. In particular, our approach is\nsuperior to the recent advanced methods in the few-shot scenario where only a\nfew samples are used for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jin Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Multi-modal Summarization. (arXiv:2109.05199v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05199","description":"<p>The new era of technology has brought us to the point where it is convenient\nfor people to share their opinions over an abundance of platforms. These\nplatforms have a provision for the users to express themselves in multiple\nforms of representations, including text, images, videos, and audio. This,\nhowever, makes it difficult for users to obtain all the key information about a\ntopic, making the task of automatic multi-modal summarization (MMS) essential.\nIn this paper, we present a comprehensive survey of the existing research in\nthe area of MMS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jangra_A/0/1/0/all/0/1\">Anubhav Jangra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanuzzaman_M/0/1/0/all/0/1\">Mohammad Hasanuzzaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering Main Causalities for Long-tailed Information Extraction. (arXiv:2109.05213v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05213","description":"<p>Information Extraction (IE) aims to extract structural information from\nunstructured texts. In practice, long-tailed distributions caused by the\nselection bias of a dataset, may lead to incorrect correlations, also known as\nspurious correlations, between entities and labels in the conventional\nlikelihood models. This motivates us to propose counterfactual IE (CFIE), a\nnovel framework that aims to uncover the main causalities behind data in the\nview of causal inference. Specifically, 1) we first introduce a unified\nstructural causal model (SCM) for various IE tasks, describing the\nrelationships among variables; 2) with our SCM, we then generate\ncounterfactuals based on an explicit language structure to better calculate the\ndirect causal effect during the inference stage; 3) we further propose a novel\ndebiasing approach to yield more robust predictions. Experiments on three IE\ntasks across five public datasets show the effectiveness of our CFIE model in\nmitigating the spurious correlation issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiaqi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Rui Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems. (arXiv:2109.05217v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05217","description":"<p>In recent years, several high-performance conversational systems have been\nproposed based on the Transformer encoder-decoder model. Although previous\nstudies analyzed the effects of the model parameters and the decoding method on\nsubjective dialogue evaluations with overall metrics, they did not analyze how\nthe differences of fine-tuning datasets affect on user's detailed impression.\nIn addition, the Transformer-based approach has only been verified for English,\nnot for such languages with large inter-language distances as Japanese. In this\nstudy, we develop large-scale Transformer-based Japanese dialogue models and\nJapanese chit-chat datasets to examine the effectiveness of the\nTransformer-based approach for building chit-chat dialogue systems. We\nevaluated and analyzed the impressions of human dialogues in different\nfine-tuning datasets, model parameters, and the use of additional information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_H/0/1/0/all/0/1\">Hiroaki Sugiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizukami_M/0/1/0/all/0/1\">Masahiro Mizukami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arimoto_T/0/1/0/all/0/1\">Tsunehiro Arimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narimatsu_H/0/1/0/all/0/1\">Hiromi Narimatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiba_Y/0/1/0/all/0/1\">Yuya Chiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_H/0/1/0/all/0/1\">Hideharu Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meguro_T/0/1/0/all/0/1\">Toyomi Meguro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations. (arXiv:2109.05233v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05233","description":"<p>State-of-the-art Named Entity Recognition(NER) models rely heavily on large\namountsof fully annotated training data. However, ac-cessible data are often\nincompletely annotatedsince the annotators usually lack comprehen-sive\nknowledge in the target domain. Normallythe unannotated tokens are regarded as\nnon-entities by default, while we underline thatthese tokens could either be\nnon-entities orpart of any entity. Here, we study NER mod-eling with incomplete\nannotated data whereonly a fraction of the named entities are la-beled, and the\nunlabeled tokens are equiva-lently multi-labeled by every possible label.Taking\nmulti-labeled tokens into account, thenumerous possible paths can distract the\ntrain-ing model from the gold path (ground truthlabel sequence), and thus\nhinders the learn-ing ability. In this paper, we propose AdaK-NER, named the\nadaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion\nwhere the gold path is more likely to belocated. We demonstrate the superiority\nofour approach through extensive experimentson both English and Chinese\ndatasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on\ntwo Chinese datasetscompared with the prior state-of-the-art works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1\">Hongtao Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liying Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peixian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning. (arXiv:2109.05234v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05234","description":"<p>Few-shot slot tagging is an emerging research topic in the field of Natural\nLanguage Understanding (NLU). With sufficient annotated data from source\ndomains, the key challenge is how to train and adapt the model to another\ntarget domain which only has few labels. Conventional few-shot approaches use\nall the data from the source domains without considering inter-domain relations\nand implicitly assume each sample in the domain contributes equally. However,\nour experiments show that the data distribution bias among different domains\nwill significantly affect the adaption performance. Moreover, transferring\nknowledge from dissimilar domains will even introduce some extra noises so that\naffect the performance of models. To tackle this problem, we propose an\neffective similarity-based method to select data from the source domains. In\naddition, we propose a Shared-Private Network (SP-Net) for the few-shot slot\ntagging task. The words from the same class would have some shared features. We\nextract those shared features from the limited annotated data on the target\ndomain and merge them together as the label embedding to help us predict other\nunlabelled data on the target domain. The experiment shows that our method\noutperforms the state-of-the-art approaches with fewer source data. The result\nalso proves that some training data from dissimilar sources are redundant and\neven negative for the adaption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_K/0/1/0/all/0/1\">Kwan Wai Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05238","description":"<p>Simultaneous machine translation (SiMT) generates translation before reading\nthe entire source sentence and hence it has to trade off between translation\nquality and latency. To fulfill the requirements of different translation\nquality and latency in practical applications, the previous methods usually\nneed to train multiple SiMT models for different latency levels, resulting in\nlarge computational costs. In this paper, we propose a universal SiMT model\nwith Mixture-of-Experts Wait-k Policy to achieve the best translation quality\nunder arbitrary latency with only one trained model. Specifically, our method\nemploys multi-head attention to accomplish the mixture of experts where each\nhead is treated as a wait-k expert with its own waiting words number, and given\na test latency and source inputs, the weights of the experts are accordingly\nadjusted to produce the best translation. Experiments on three datasets show\nthat our method outperforms all the strong baselines under different latency,\nincluding the state-of-the-art adaptive policy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05244","description":"<p>Cross-attention is an important component of neural machine translation\n(NMT), which is always realized by dot-product attention in previous methods.\nHowever, dot-product attention only considers the pair-wise correlation between\nwords, resulting in dispersion when dealing with long sentences and neglect of\nsource neighboring relationships. Inspired by linguistics, the above issues are\ncaused by ignoring a type of cross-attention, called concentrated attention,\nwhich focuses on several central words and then spreads around them. In this\nwork, we apply Gaussian Mixture Model (GMM) to model the concentrated attention\nin cross-attention. Experiments and analyses we conducted on three datasets\nshow that the proposed method outperforms the baseline and has significant\nimprovement on alignment quality, N-gram accuracy, and long sentence\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qualitative and Quantitative Analysis of Diversity in Cross-document Coreference Resolution Datasets. (arXiv:2109.05250v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05250","description":"<p>Cross-document coreference resolution (CDCR) datasets, such as ECB+, contain\nmanually annotated event-centric mentions of events and entities that form\ncoreference chains with identity relations. ECB+ is a state-of-the-art CDCR\ndataset that focuses on the resolution of events and their descriptive\nattributes, i.e., actors, location, and date-time. NewsWCL50 is a dataset that\nannotates coreference chains of both events and entities with a strong variance\nof word choice and more loosely-related coreference anaphora, e.g., bridging or\nnear-identity relations. In this paper, we qualitatively and quantitatively\ncompare annotation schemes of ECB+ and NewsWCL50 with multiple criteria. We\npropose a phrasing diversity metric (PD) that compares lexical diversity within\ncoreference chains on a more detailed level than previously proposed metric,\ne.g., a number of unique lemmas. We discuss the different tasks that both CDCR\ndatasets create, i.e., lexical disambiguation and lexical diversity challenges,\nand propose a direction for further CDCR evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XCoref: Cross-document Coreference Resolution in the Wild. (arXiv:2109.05252v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05252","description":"<p>Datasets and methods for cross-document coreference resolution (CDCR) focus\non events or entities with strict coreference relations. They lack, however,\nannotating and resolving coreference mentions with more abstract or loose\nrelations that may occur when news articles report about controversial and\npolarized events. Bridging and loose coreference relations trigger associations\nthat may lead to exposing news readers to bias by word choice and labeling. For\nexample, coreferential mentions of \"direct talks between U.S. President Donald\nTrump and Kim\" such as \"an extraordinary meeting following months of heated\nrhetoric\" or \"great chance to solve a world problem\" form a more positive\nperception of this event. A step towards bringing awareness of bias by word\nchoice and labeling is the reliable resolution of coreferences with high\nlexical diversity. We propose an unsupervised method named XCoref, which is a\nCDCR method that capably resolves not only previously prevalent entities, such\nas persons, e.g., \"Donald Trump,\" but also abstractly defined concepts, such as\ngroups of persons, \"caravan of immigrants,\" events and actions, e.g., \"marching\nto the U.S. border.\" In an extensive evaluation, we compare the proposed XCoref\nto a state-of-the-art CDCR method and a previous method TCA that resolves such\ncomplex coreference relations and find that XCoref outperforms these methods.\nOutperforming an established CDCR model shows that the new CDCR models need to\nbe evaluated on semantically complex mentions with more loose coreference\nrelations to indicate their applicability of models to resolve mentions in the\n\"wild\" of political news articles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Translation via Grafting Pre-trained Language Models. (arXiv:2109.05256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05256","description":"<p>Can pre-trained BERT for one language and GPT for another be glued together\nto translate texts? Self-supervised training using only monolingual data has\nled to the success of pre-trained (masked) language models in many NLP tasks.\nHowever, directly connecting BERT as an encoder and GPT as a decoder can be\nchallenging in machine translation, for GPT-like models lack a cross-attention\ncomponent that is needed in seq2seq decoders. In this paper, we propose\nGraformer to graft separately pre-trained (masked) language models for machine\ntranslation. With monolingual data for pre-training and parallel data for\ngrafting training, we maximally take advantage of the usage of both types of\ndata. Experiments on 60 directions show that our method achieves average\nimprovements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with\nthe multilingual Transformer of the same size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05281","description":"<p>Developers of text generation models rely on automated evaluation metrics as\na stand-in for slow and expensive manual evaluations. However, image captioning\nmetrics have struggled to give accurate learned estimates of the semantic and\npragmatic success of output text. We address this weakness by introducing the\nfirst discourse-aware learned generation metric for evaluating image\ndescriptions. Our approach is inspired by computational theories of discourse\nfor capturing information goals using coherence. We present a dataset of\nimage$\\unicode{x2013}$description pairs annotated with coherence relations. We\nthen train a coherence-aware metric on a subset of the Conceptual Captions\ndataset and measure its effectiveness$\\unicode{x2014}$its ability to predict\nhuman ratings of output captions$\\unicode{x2014}$on a test set composed of\nout-of-domain images. We demonstrate a higher Kendall Correlation Coefficient\nfor our proposed metric with the human judgments for the results of a number of\nstate-of-the-art coherence-aware caption generation models when compared to\nseveral other metrics including recently proposed learned metrics such as\nBLEURT and BERTScore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_B/0/1/0/all/0/1\">Baber Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in a Name? Answer Equivalence For Open-Domain Question Answering. (arXiv:2109.05289v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05289","description":"<p>A flaw in QA evaluation is that annotations often only provide one gold\nanswer. Thus, model predictions semantically equivalent to the answer but\nsuperficially different are considered incorrect. This work explores mining\nalias entities from knowledge bases and using them as additional gold answers\n(i.e., equivalent answers). We incorporate answers for two settings: evaluation\nwith additional answers and model training with equivalent answers. We analyse\nthree QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion\nincreases the exact match score on all datasets for evaluation, while\nincorporating it helps model training over real-world datasets. We ensure the\nadditional answers are valid through a human post hoc evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy. (arXiv:2109.05312v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05312","description":"<p>Generating goal-oriented questions in Visual Dialogue tasks is a challenging\nand long-standing problem. State-Of-The-Art systems are shown to generate\nquestions that, although grammatically correct, often lack an effective\nstrategy and sound unnatural to humans. Inspired by the cognitive literature on\ninformation search and cross-situational word learning, we design Confirm-it, a\nmodel based on a beam search re-ranking algorithm that guides an effective\ngoal-oriented strategy by asking questions that confirm the model's conjecture\nabout the referent. We take the GuessWhat?! game as a case-study. We show that\ndialogues generated by Confirm-it are more natural and effective than beam\nsearch decoding without re-ranking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Testoni_A/0/1/0/all/0/1\">Alberto Testoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardi_R/0/1/0/all/0/1\">Raffaella Bernardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Topic Regression for Causal Inference. (arXiv:2109.05317v1 [stat.ML])","link":"http://arxiv.org/abs/2109.05317","description":"<p>Causal inference using observational text data is becoming increasingly\npopular in many research areas. This paper presents the Bayesian Topic\nRegression (BTR) model that uses both text and numerical information to model\nan outcome variable. It allows estimation of both discrete and continuous\ntreatment effects. Furthermore, it allows for the inclusion of additional\nnumerical confounding factors next to text data. To this end, we combine a\nsupervised Bayesian topic model with a Bayesian regression framework and\nperform supervised representation learning for the text features jointly with\nthe regression parameter training, respecting the Frisch-Waugh-Lovell theorem.\nOur paper makes two main contributions. First, we provide a regression\nframework that allows causal inference in settings when both text and numerical\nconfounders are of relevance. We show with synthetic and semi-synthetic\ndatasets that our joint approach recovers ground truth with lower bias than any\nbenchmark model, when text and numerical features are correlated. Second,\nexperiments on two real-world datasets demonstrate that a joint and supervised\nlearning strategy also yields superior prediction results compared to\nstrategies that estimate regression weights for text and non-text features\nseparately, being even competitive with more complex deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Ahrens_M/0/1/0/all/0/1\">Maximilian Ahrens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ashwin_J/0/1/0/all/0/1\">Julian Ashwin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calliess_J/0/1/0/all/0/1\">Jan-Peter Calliess</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Hatred: A Benchmark for Understanding Implicit Hate Speech. (arXiv:2109.05322v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05322","description":"<p>Hate speech has grown significantly on social media, causing serious\nconsequences for victims of all demographics. Despite much attention being paid\nto characterize and detect discriminatory speech, most work has focused on\nexplicit or overt hate speech, failing to address a more pervasive form based\non coded or indirect language. To fill this gap, this work introduces a\ntheoretically-justified taxonomy of implicit hate speech and a benchmark corpus\nwith fine-grained labels for each message and its implication. We present\nsystematic analyses of our dataset using contemporary baselines to detect and\nexplain implicit hate speech, and we discuss key features that challenge\nexisting models. This dataset will continue to serve as a useful benchmark for\nunderstanding this multifaceted issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1\">Mai ElSherief</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muchlinski_D/0/1/0/all/0/1\">David Muchlinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anupindi_V/0/1/0/all/0/1\">Vaishnavi Anupindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seybolt_J/0/1/0/all/0/1\">Jordyn Seybolt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Munmun De Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence. (arXiv:2109.05325v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05325","description":"<p>Framing has significant but subtle effects on public opinion and policy. We\npropose an NLP framework to measure entity-centric frames. We use it to\nunderstand media coverage on police violence in the United States in a new\nPolice Violence Frames Corpus of 82k news articles spanning 7k police killings.\nOur work uncovers more than a dozen framing devices and reveals significant\ndifferences in the way liberal and conservative news sources frame both the\nissue of police violence and the entities involved. Conservative sources\nemphasize when the victim is armed or attacking an officer and are more likely\nto mention the victim's criminal record. Liberal sources focus more on the\nunderlying systemic injustice, highlighting the victim's race and that they\nwere unarmed. We discover temporary spikes in these injustice frames near\nhigh-profile shooting events, and finally, we show protest volume correlates\nwith and precedes media framing decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v1 [cs.AI])","link":"http://arxiv.org/abs/2109.05327","description":"<p>Numerous government initiatives (e.g. the EU with GDPR) are coming to the\nconclusion that the increasing complexity of modern software systems must be\ncontrasted with some Rights to Explanation and metrics for the Impact\nAssessment of these tools, that allow humans to understand and oversee the\noutput of Automated Decision Making systems. Explainable AI was born as a\npathway to allow humans to explore and understand the inner working of complex\nsystems. But establishing what is an explanation and objectively evaluating\nexplainability, are not trivial tasks. With this paper, we present a new\nmodel-agnostic metric to measure the Degree of eXplainability of correct\ninformation in an objective way, exploiting a specific model from Ordinary\nLanguage Philosophy called the Achinstein's Theory of Explanations. In order to\nunderstand whether this metric is actually behaving as explainability is\nexpected to, we designed a few experiments and a user-study on two realistic\nAI-based systems for healthcare and finance, involving famous AI technology\nincluding Artificial Neural Networks and TreeSHAP. The results we obtained are\nvery encouraging, suggesting that our proposed metric for measuring the Degree\nof eXplainability is robust on several scenarios and it can be eventually\nexploited for a lawful Impact Assessment of an Automated Decision Making\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sovrano_F/0/1/0/all/0/1\">Francesco Sovrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitali_F/0/1/0/all/0/1\">Fabio Vitali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling and Acceleration of Three-dimensional Structure Determination for Single-Particle Imaging Experiments with SpiniFEL. (arXiv:2109.05339v1 [physics.comp-ph])","link":"http://arxiv.org/abs/2109.05339","description":"<p>The Linac Coherent Light Source (LCLS) is an X- ray free electron laser\n(XFEL) facility enabling the study of the structure and dynamics of single\nmacromolecules. A major upgrade will bring the repetition rate of the X-ray\nsource from 120 to 1 million pulses per second. Exascale high performance\ncomputing (HPC) capabilities will be required to process the corresponding data\nrates. We present SpiniFEL, an application used for structure determination of\nproteins from single-particle imaging (SPI) experiments. An emerging technique\nfor imaging individual proteins and other large molecular complexes by\noutrunning radiation damage, SPI breaks free from the need for crystallization\n(which is difficult for some proteins) and allows for imaging molecular\ndynamics at near ambient conditions. SpiniFEL is being developed to run on\nsupercomputers in near real-time while an experiment is taking place, so that\nthe feedback about the data can guide the data collection strategy. We describe\nhere how we reformulated the mathematical framework for parallelizable\nimplementation and accelerated the most compute intensive parts of the\napplication. We also describe the use of Pygion, a Python interface for the\nLegion task-based programming model and compare to our existing MPI+GPU\nimplementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Chang_H/0/1/0/all/0/1\">Hsing-Yin Chang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Slaughter_E/0/1/0/all/0/1\">Elliott Slaughter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mirchandaney_S/0/1/0/all/0/1\">Seema Mirchandaney</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Donatelli_J/0/1/0/all/0/1\">Jeffrey Donatelli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yoon_C/0/1/0/all/0/1\">Chun Hong Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HYDRA -- Hyper Dependency Representation Attentions. (arXiv:2109.05349v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05349","description":"<p>Attention is all we need as long as we have enough data. Even so, it is\nsometimes not easy to determine how much data is enough while the models are\nbecoming larger and larger. In this paper, we propose HYDRA heads, lightweight\npretrained linguistic self-attention heads to inject knowledge into transformer\nmodels without pretraining them again. Our approach is a balanced paradigm\nbetween leaving the models to learn unsupervised and forcing them to conform to\nlinguistic knowledge rigidly as suggested in previous studies. Our experiment\nproves that the approach is not only the boost performance of the model but\nalso lightweight and architecture friendly. We empirically verify our framework\non benchmark datasets to show the contribution of linguistic knowledge to a\ntransformer model. This is a promising result for a new approach to\ntransferring knowledge from linguistic resources into transformer-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tran-Binh Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Minh-Quan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework. (arXiv:2109.05357v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05357","description":"<p>In this work, we study the problem of named entity recognition (NER) in a low\nresource scenario, focusing on few-shot and zero-shot settings. Built upon\nlarge-scale pre-trained language models, we propose a novel NER framework,\nnamely SpanNER, which learns from natural language supervision and enables the\nidentification of never-seen entity classes without using in-domain labeled\ndata. We perform extensive experiments on 5 benchmark datasets and evaluate the\nproposed method in the few-shot learning, domain transfer and zero-shot\nlearning settings. The experimental results show that the proposed method can\nbring 10%, 23% and 26% improvements in average over the best baselines in\nfew-shot learning, domain transfer and zero-shot learning settings\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Haoda Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models. (arXiv:2109.05358v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05358","description":"<p>Enthymemes are defined as arguments where a premise or conclusion is left\nimplicit. We tackle the task of generating the implicit premise in an\nenthymeme, which requires not only an understanding of the stated conclusion\nand premise but also additional inferences that could depend on commonsense\nknowledge. The largest available dataset for enthymemes (Habernal et al., 2018)\nconsists of 1.7k samples, which is not large enough to train a neural text\ngeneration model. To address this issue, we take advantage of a similar task\nand dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020).\nHowever, we show that simply using a state-of-the-art seq2seq model fine-tuned\non this data might not generate meaningful implicit premises associated with\nthe given enthymemes. We demonstrate that encoding discourse-aware commonsense\nduring fine-tuning improves the quality of the generated implicit premises and\noutperforms all other baselines both in automatic and human evaluations on\nthree different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Aadit Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMBO: State-of-the-Art Morphosyntactic Analysis. (arXiv:2109.05361v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05361","description":"<p>We introduce COMBO - a fully neural NLP system for accurate part-of-speech\ntagging, morphological analysis, lemmatisation, and (enhanced) dependency\nparsing. It predicts categorical morphosyntactic features whilst also exposes\ntheir vector representations, extracted from hidden layers. COMBO is an easy to\ninstall Python package with automatically downloadable pre-trained models for\nover 40 languages. It maintains a balance between efficiency and quality. As it\nis an end-to-end system and its modules are jointly trained, its training is\ncompetitively fast. As its models are optimised for accuracy, they achieve\noften better prediction quality than SOTA. The COMBO library is available at:\nhttps://gitlab.clarin-pl.eu/syntactic-tools/combo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klimaszewski_M/0/1/0/all/0/1\">Mateusz Klimaszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Alina Wr&#xf3;blewska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular Self-Supervision for Document-Level Relation Extraction. (arXiv:2109.05362v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05362","description":"<p>Extracting relations across large text spans has been relatively\nunderexplored in NLP, but it is particularly important for high-value domains\nsuch as biomedicine, where obtaining high recall of the latest findings is\ncrucial for practical applications. Compared to conventional information\nextraction confined to short text spans, document-level relation extraction\nfaces additional challenges in both inference and learning. Given longer text\nspans, state-of-the-art neural architectures are less effective and\ntask-specific self-supervision such as distant supervision becomes very noisy.\nIn this paper, we propose decomposing document-level relation extraction into\nrelation detection and argument resolution, taking inspiration from Davidsonian\nsemantics. This enables us to incorporate explicit discourse modeling and\nleverage modular self-supervision for each sub-problem, which is less\nnoise-prone and can be further refined end-to-end via variational EM. We\nconduct a thorough evaluation in biomedical machine reading for precision\noncology, where cross-paragraph relation mentions are prevalent. Our method\noutperforms prior state of the art, such as multi-scale learning and graph\nneural networks, by over 20 absolute F1 points. The gain is particularly\npronounced among the most challenging relation instances whose arguments never\nco-occur in a paragraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Positional Encodings on Multilingual Compression. (arXiv:2109.05388v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05388","description":"<p>In order to preserve word-order information in a non-autoregressive setting,\ntransformer architectures tend to include positional knowledge, by (for\ninstance) adding positional encodings to token embeddings. Several\nmodifications have been proposed over the sinusoidal positional encodings used\nin the original transformer architecture; these include, for instance,\nseparating position encodings and token embeddings, or directly modifying\nattention weights based on the distance between word pairs. We first show that\nsurprisingly, while these modifications tend to improve monolingual language\nmodels, none of them result in better multilingual language models. We then\nanswer why that is: Sinusoidal encodings were explicitly designed to facilitate\ncompositionality by allowing linear projections over arbitrary time steps.\nHigher variances in multilingual training distributions requires higher\ncompression, in which case, compositionality becomes indispensable. Learned\nabsolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal\nembeddings in multilingual settings, but more complex positional encoding\narchitectures lack the inductive bias to effectively learn compositionality and\ncross-lingual alignment. In other words, while sinusoidal positional encodings\nwere originally designed for monolingual applications, they are particularly\nuseful in multilingual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_V/0/1/0/all/0/1\">Vinit Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning. (arXiv:2109.05395v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05395","description":"<p>Single-table text-to-SQL aims to transform a natural language question into a\nSQL query according to one single table. Recent work has made promising\nprogress on this task by pre-trained language models and a multi-submodule\nframework. However, zero-shot table, that is, the invisible table in the\ntraining set, is currently the most critical bottleneck restricting the\napplication of existing approaches to real-world scenarios. Although some work\nhas utilized auxiliary tasks to help handle zero-shot tables, expensive extra\nmanual annotation limits their practicality. In this paper, we propose a new\napproach for the zero-shot text-to-SQL task which does not rely on any\nadditional manual annotations. Our approach consists of two parts. First, we\npropose a new model that leverages the abundant information of table content to\nhelp establish the mapping between questions and zero-shot tables. Further, we\npropose a simple but efficient meta-learning strategy to train our model. The\nstrategy utilizes the two-step gradient update to force the model to learn a\ngeneralization ability towards zero-shot tables. We conduct extensive\nexperiments on a public open-domain text-to-SQL dataset WikiSQL and a\ndomain-specific dataset ESQL. Compared to existing approaches using the same\npre-trained model, our approach achieves significant improvements on both\ndatasets. Compared to the larger pre-trained model and the tabular-specific\npre-trained model, our approach is still competitive. More importantly, on the\nzero-shot subsets of both the datasets, our approach further increases the\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xinnan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jian Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05406","description":"<p>Human conversations consist of reasonable and natural topic flows, which are\nobserved as the shifts of the mentioned concepts across utterances. Previous\nchatbots that incorporate the external commonsense knowledge graph prove that\nmodeling the concept shifts can effectively alleviate the dull and\nuninformative response dilemma. However, there still exists a gap between the\nconcept relations in the natural conversation and those in the external\ncommonsense knowledge graph, which is an issue to solve. Specifically, the\nconcept relations in the external commonsense knowledge graph are not\nintuitively built from the conversational scenario but the world knowledge,\nwhich makes them insufficient for the chatbot construction. To bridge the above\ngap, we propose the method to supply more concept relations extracted from the\nconversational corpora and reconstruct an enhanced concept graph for the\nchatbot construction. In addition, we present a novel, powerful, and fast graph\nencoding architecture named the Edge-Transformer to replace the traditional GNN\narchitecture. Experimental results on the Reddit conversation dataset indicate\nour proposed method significantly outperforms strong baseline systems and\nachieves new SOTA results. Further analysis individually proves the\neffectiveness of the enhanced concept graph and the Edge-Transformer\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_P/0/1/0/all/0/1\">Pengda Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05424","description":"<p>Many recent successes in sentence representation learning have been achieved\nby simply fine-tuning on the Natural Language Inference (NLI) datasets with\ntriplet loss or siamese loss. Nevertheless, they share a common weakness:\nsentences in a contradiction pair are not necessarily from different semantic\ncategories. Therefore, optimizing the semantic entailment and contradiction\nreasoning objective alone is inadequate to capture the high-level semantic\nstructure. The drawback is compounded by the fact that the vanilla siamese or\ntriplet losses only learn from individual sentence pairs or triplets, which\noften suffer from bad local optima. In this paper, we propose PairSupCon, an\ninstance discrimination based approach aiming to bridge semantic entailment and\ncontradiction understanding with high-level categorical concept encoding. We\nevaluate PairSupCon on various downstream tasks that involve understanding\nsentence semantics at different granularities. We outperform the previous\nstate-of-the-art method with $10\\%$--$13\\%$ averaged improvement on eight\nclustering tasks, and $5\\%$--$6\\%$ averaged improvement on seven semantic\ntextual similarity (STS) tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification. (arXiv:2109.05427v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05427","description":"<p>Fine-grained classification involves dealing with datasets with larger number\nof classes with subtle differences between them. Guiding the model to focus on\ndifferentiating dimensions between these commonly confusable classes is key to\nimproving performance on fine-grained tasks. In this work, we analyse the\ncontrastive fine-tuning of pre-trained language models on two fine-grained text\nclassification tasks, emotion classification and sentiment analysis. We\nadaptively embed class relationships into a contrastive objective function to\nhelp differently weigh the positives and negatives, and in particular,\nweighting closely confusable negatives more than less similar negative\nexamples. We find that Label-aware Contrastive Loss outperforms previous\ncontrastive methods, in the presence of larger number and/or more confusable\nclasses, and helps models to produce output distributions that are more\ndifferentiated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1\">Varsha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05433","description":"<p>Internet search affects people's cognition of the world, so mitigating biases\nin search results and learning fair models is imperative for social good. We\nstudy a unique gender bias in image search in this work: the search images are\noften gender-imbalanced for gender-neutral natural language queries. We\ndiagnose two typical image search models, the specialized model trained on\nin-domain datasets and the generalized representation model pre-trained on\nmassive image and text data across the internet. Both models suffer from severe\ngender bias. Therefore, we introduce two novel debiasing approaches: an\nin-processing fair sampling method to address the gender imbalance issue for\ntraining models, and a post-processing feature clipping method base on mutual\ninformation to debias multimodal representations of pre-trained models.\nExtensive experiments on MS-COCO and Flickr30K benchmarks show that our methods\nsignificantly reduce the gender bias in image search models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Let Your Characters Tell Their Story\": A Dataset for Character-Centric Narrative Understanding. (arXiv:2109.05438v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05438","description":"<p>When reading a literary piece, readers often make inferences about various\ncharacters' roles, personalities, relationships, intents, actions, etc. While\nhumans can readily draw upon their past experiences to build such a\ncharacter-centric view of the narrative, understanding characters in narratives\ncan be a challenging task for machines. To encourage research in this field of\ncharacter-centric narrative understanding, we present LiSCU -- a new dataset of\nliterary pieces and their summaries paired with descriptions of characters that\nappear in them. We also introduce two new tasks on LiSCU: Character\nIdentification and Character Description Generation. Our experiments with\nseveral pre-trained language models adapted for these tasks demonstrate that\nthere is a need for better models of narrative comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Meng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Conversational Search for Online Shopping with Utterance Transfer. (arXiv:2109.05460v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05460","description":"<p>Successful conversational search systems can present natural, adaptive and\ninteractive shopping experience for online shopping customers. However,\nbuilding such systems from scratch faces real word challenges from both\nimperfect product schema/knowledge and lack of training dialog data.In this\nwork we first propose ConvSearch, an end-to-end conversational search system\nthat deeply combines the dialog system with search. It leverages the text\nprofile to retrieve products, which is more robust against imperfect product\nschema/knowledge compared with using product attributes alone. We then address\nthe lack of data challenges by proposing an utterance transfer approach that\ngenerates dialogue utterances by using existing dialog from other domains, and\nleveraging the search behavior data from e-commerce retailer. With utterance\ntransfer, we introduce a new conversational search dataset for online shopping.\nExperiments show that our utterance transfer method can significantly improve\nthe availability of training dialogue data without crowd-sourcing, and the\nconversational search system significantly outperformed the best tested\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Liqiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma2_J/0/1/0/all/0/1\">Jun Ma2</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Luna Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gomez_P/0/1/0/all/0/1\">Pascual Martinez-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1\">Nasser Zalmout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Logic Traps in Evaluating Post-hoc Interpretations. (arXiv:2109.05463v1 [cs.LG])","link":"http://arxiv.org/abs/2109.05463","description":"<p>Post-hoc interpretation aims to explain a trained model and reveal how the\nmodel arrives at a decision. Though research on post-hoc interpretations has\ndeveloped rapidly, one growing pain in this field is the difficulty in\nevaluating interpretations. There are some crucial logic traps behind existing\nevaluation methods, which are ignored by most works. In this opinion piece, we\nsummarize four kinds evaluation methods and point out the corresponding logic\ntraps behind them. We argue that we should be clear about these traps rather\nthan ignore them and draw conclusions assertively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yiming Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongtao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05473","description":"<p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by\nlearning with merely a handful of annotated instances. Meta-learning has been\nwidely adopted for such a task, which trains on randomly generated few-shot\ntasks to learn generic data representations. Despite impressive results\nachieved, existing models still perform suboptimally when handling hard FSRE\ntasks, where the relations are fine-grained and similar to each other. We argue\nthis is largely because existing models do not distinguish hard tasks from easy\nones in the learning process. In this paper, we introduce a novel approach\nbased on contrastive learning that learns better representations by exploiting\nrelation label information. We further design a method that allows the model to\nadaptively learn how to focus on hard tasks. Experiments on two standard\ndatasets demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiale Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stylistic Retrieval-based Dialogue System with Unparallel Training Data. (arXiv:2109.05477v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05477","description":"<p>The ability of a dialog system to express consistent language style during\nconversations has a direct, positive impact on its usability and on user\nsatisfaction. Although previous studies have demonstrated that style transfer\nis feasible with a large amount of parallel data, it is often impossible to\ncollect such data for different styles. In this paper, instead of manually\nconstructing conversation data with a certain style, we propose a flexible\nframework that adapts a generic retrieval-based dialogue system to mimic the\nlanguage style of a specified persona without any parallel data. Our approach\nis based on automatic generation of stylized data by learning the usage of\njargon, and then rewriting the generic conversations to a stylized one by\nincorporating the jargon. In experiments we implemented dialogue systems with\nfive distinct language styles, and the result shows our framework significantly\noutperforms baselines in terms of the average score of responses' relevance and\nstyle degree, and content diversity. A/B testing on a commercial chatbot shows\nthat users are more satisfied with our system. This study demonstrates the\nfeasibility of building stylistic dialogue systems by simple data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianran Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jianyun Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation. (arXiv:2109.05487v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05487","description":"<p>Although pre-training models have achieved great success in dialogue\ngeneration, their performance drops dramatically when the input contains an\nentity that does not appear in pre-training and fine-tuning datasets (unseen\nentity). To address this issue, existing methods leverage an external knowledge\nbase to generate appropriate responses. In real-world scenario, the entity may\nnot be included by the knowledge base or suffer from the precision of knowledge\nretrieval. To deal with this problem, instead of introducing knowledge base as\nthe input, we force the model to learn a better semantic representation by\npredicting the information in the knowledge base, only based on the input\ncontext. Specifically, with the help of a knowledge base, we introduce two\nauxiliary training objectives: 1) Interpret Masked Word, which conjectures the\nmeaning of the masked entity given the context; 2) Hypernym Generation, which\npredicts the hypernym of the entity based on the context. Experiment results on\ntwo dialogue corpus verify the effectiveness of our methods under both\nknowledge available and unavailable settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05494","description":"<p>Building an automatic speech recognition (ASR) system from scratch requires a\nlarge amount of annotated speech data, which is difficult to collect in many\nlanguages. However, there are cases where the low-resource language shares a\ncommon acoustic space with a high-resource language having enough annotated\ndata to build an ASR. In such cases, we show that the domain-independent\nacoustic models learned from the high-resource language through unsupervised\ndomain adaptation (UDA) schemes can enhance the performance of the ASR in the\nlow-resource language. We use the specific example of Hindi in the source\ndomain and Sanskrit in the target domain. We explore two architectures: i)\ndomain adversarial training using gradient reversal layer (GRL) and ii) domain\nseparation networks (DSN). The GRL and DSN architectures give absolute\nimprovements of 6.71% and 7.32%, respectively, in word error rate over the\nbaseline deep neural network model when trained on just 5.5 hours of data in\nthe target domain. We also show that choosing a proper language (Telugu) in the\nsource domain can bring further improvement. The results suggest that UDA\nschemes can be helpful in the development of ASR systems for low-resource\nlanguages, mitigating the hassle of collecting large amounts of annotated\nspeech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1\">Anoop C S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P_P/0/1/0/all/0/1\">Prathosh A P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">A G Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEASEL: A Transformer-Based Speech-Prefixed Language Model. (arXiv:2109.05522v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05522","description":"<p>Multimodal language analysis is a burgeoning field of NLP that aims to\nsimultaneously model a speaker's words, acoustical annotations, and facial\nexpressions. In this area, lexicon features usually outperform other modalities\nbecause they are pre-trained on large corpora via Transformer-based models.\nDespite their strong performance, training a new self-supervised learning (SSL)\nTransformer on any modality is not usually attainable due to insufficient data,\nwhich is the case in multimodal language learning. This work proposes a\nTransformer-Based Speech-Prefixed Language Model called TEASEL to approach the\nmentioned constraints without training a complete Transformer model. TEASEL\nmodel includes speech modality as a dynamic prefix besides the textual modality\ncompared to a conventional language model. This method exploits a conventional\npre-trained language model as a cross-modal Transformer model. We evaluated\nTEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.\nExtensive experiments show that our model outperforms unimodal baseline\nlanguage models by 4% and outperforms the current multimodal state-of-the-art\n(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%\nsmaller than the SoTA model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arjmand_M/0/1/0/all/0/1\">Mehdi Arjmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dousti_M/0/1/0/all/0/1\">Mohammad Javad Dousti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05523","description":"<p>Existing research for image text retrieval mainly relies on sentence-level\nsupervision to distinguish matched and mismatched sentences for a query image.\nHowever, semantic mismatch between an image and sentences usually happens in\nfiner grain, i.e., phrase level. In this paper, we explore to introduce\nadditional phrase-level supervision for the better identification of mismatched\nunits in the text. In practice, multi-grained semantic labels are automatically\nconstructed for a query image in both sentence-level and phrase-level. We\nconstruct text scene graphs for the matched sentences and extract entities and\ntriples as the phrase-level labels. In order to integrate both supervision of\nsentence-level and phrase-level, we propose Semantic Structure Aware Multimodal\nTransformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,\nwe utilize different kinds of attention mechanisms to enforce interactions of\nmulti-grain semantic units in both sides of vision and language. For the\ntraining, we propose multi-scale matching losses from both global and local\nperspectives, and penalize mismatched phrases. Experimental results on MS-COCO\nand Flickr30K show the effectiveness of our approach compared to some\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Haijun Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Event Temporal Relations via Hyperbolic Geometry. (arXiv:2109.05527v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05527","description":"<p>Detecting events and their evolution through time is a crucial task in\nnatural language understanding. Recent neural approaches to event temporal\nrelation extraction typically map events to embeddings in the Euclidean space\nand train a classifier to detect temporal relations between event pairs.\nHowever, embeddings in the Euclidean space cannot capture richer asymmetric\nrelations such as event temporal relations. We thus propose to embed events\ninto hyperbolic spaces, which are intrinsically oriented at modeling\nhierarchical structures. We introduce two approaches to encode events and their\ntemporal relations in hyperbolic spaces. One approach leverages hyperbolic\nembeddings to directly infer event relations through simple geometrical\noperations. In the second one, we devise an end-to-end architecture composed of\nhyperbolic neural units tailored for the temporal relation extraction task.\nThorough experimental assessments on widely used datasets have shown the\nbenefits of revisiting the tasks on a different geometrical space, resulting in\nstate-of-the-art performance on several standard metrics. Finally, the ablation\nstudy and several qualitative analyses highlighted the rich event semantics\nimplicitly encoded into hyperbolic spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xingwei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good-Enough Example Extrapolation. (arXiv:2109.05602v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05602","description":"<p>This paper asks whether extrapolating the hidden space distribution of text\nexamples from one class onto another is a valid inductive bias for data\naugmentation. To operationalize this question, I propose a simple data\naugmentation protocol called \"good-enough example extrapolation\" (GE3). GE3 is\nlightweight and has no hyperparameters. Applied to three text classification\ndatasets for various data imbalance scenarios, GE3 improves performance more\nthan upsampling and other hidden-space data augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05611","description":"<p>We propose a novel scheme to use the Levenshtein Transformer to perform the\ntask of word-level quality estimation. A Levenshtein Transformer is a natural\nfit for this task: trained to perform decoding in an iterative manner, a\nLevenshtein Transformer can learn to post-edit without explicit supervision. To\nfurther minimize the mismatch between the translation task and the word-level\nQE task, we propose a two-stage transfer learning procedure on both augmented\ndata and human post-editing data. We also propose heuristics to construct\nreference labels that are compatible with subword-level finetuning and\ninference. Results on WMT 2020 QE shared task dataset show that our proposed\nmethod has superior data efficiency under the data-constrained setting and\ncompetitive performance under the unconstrained setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models. (arXiv:2109.05620v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05620","description":"<p>To audit the robustness of named entity recognition (NER) models, we propose\nRockNER, a simple yet effective method to create natural adversarial examples.\nSpecifically, at the entity level, we replace target entities with other\nentities of the same semantic class in Wikidata; at the context level, we use\npre-trained language models (e.g., BERT) to generate word substitutions.\nTogether, the two levels of attack produce natural adversarial examples that\nresult in a shifted distribution from the training data on which our target\nmodels have been trained. We apply the proposed method to the OntoNotes dataset\nand create a new benchmark named OntoRock for evaluating the robustness of\nexisting NER models via a systematic evaluation protocol. Our experiments and\nanalysis reveal that even the best model has a significant performance drop,\nand these models seem to memorize in-domain entity patterns instead of\nreasoning from the context. Our work also studies the effects of a few simple\ndata augmentation methods to improve the robustness of NER models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wenyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_R/0/1/0/all/0/1\">Ryan Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement. (arXiv:2004.03760v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.03760","description":"<p>Disentanglement is a problem in which multiple conversations occur in the\nsame channel simultaneously, and the listener should decide which utterance is\npart of the conversation he will respond to. We propose a new model, named\nDialogue BERT (DialBERT), which integrates local and global semantics in a\nsingle stream of messages to disentangle the conversations that mixed together.\nWe employ BERT to capture the matching information in each utterance pair at\nthe utterance-level, and use a BiLSTM to aggregate and incorporate the\ncontext-level information. With only a 3% increase in parameters, a 12%\nimprovement has been attained in comparison to BERT, based on the F1-Score. The\nmodel achieves a state-of-the-art result on the a new dataset proposed by IBM\nand surpasses previous work by a substantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhiming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Si Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00033","description":"<p>With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Ad-dressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus con-firming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1\">Alex Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts. (arXiv:2006.12289v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.12289","description":"<p>We present and make available MedLatinEpi and MedLatinLit, two datasets of\nmedieval Latin texts to be used in research on computational authorship\nanalysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,\nrespectively, labelled by author; MedLatinEpi texts are of epistolary nature,\nwhile MedLatinLit texts consist of literary comments and treatises about\nvarious subjects. As such, these two datasets lend themselves to supporting\nresearch in authorship analysis tasks, such as authorship attribution,\nauthorship verification, or same-author verification. Along with the datasets\nwe provide experimental results, obtained on these datasets, for the authorship\nverification task, i.e., the task of predicting whether a text of unknown\nauthorship was written by a candidate author or not. We also make available the\nsource code of the authorship verification system we have used, thus allowing\nour experiments to be reproduced, and to be used as baselines, by other\nresearchers. We also describe the application of the above authorship\nverification system, using these datasets as training data, for investigating\nthe authorship of two medieval epistles whose authorship has been disputed by\nscholars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corbara_S/0/1/0/all/0/1\">Silvia Corbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavoni_M/0/1/0/all/0/1\">Mirko Tavoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings. (arXiv:2007.00049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.00049","description":"<p>Language representations are known to carry stereotypical biases and, as a\nresult, lead to biased predictions in downstream tasks. While existing methods\nare effective at mitigating biases by linear projection, such methods are too\naggressive: they not only remove bias, but also erase valuable information from\nword embeddings. We develop new measures for evaluating specific information\nretention that demonstrate the tradeoff between bias removal and information\nretention. To address this challenge, we propose OSCaR (Orthogonal Subspace\nCorrection and Rectification), a bias-mitigating method that focuses on\ndisentangling biased associations between concepts instead of removing concepts\nwholesale. Our experiments on gender biases show that OSCaR is a well-balanced\napproach that ensures that semantic information is retained in the embeddings\nand bias is also effectively mitigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robustness and Bias Analysis of BERT-based Relation Extraction. (arXiv:2009.06206v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.06206","description":"<p>Fine-tuning pre-trained models have achieved impressive performance on\nstandard natural language processing benchmarks. However, the resultant model\ngeneralizability remains poorly understood. We do not know, for example, how\nexcellent performance can lead to the perfection of generalization models. In\nthis study, we analyze a fine-tuned BERT model from different perspectives\nusing relation extraction. We also characterize the differences in\ngeneralization techniques according to our proposed improvements. From\nempirical experimentation, we find that BERT suffers a bottleneck in terms of\nrobustness by way of randomizations, adversarial and counterfactual tests, and\nbiases (i.e., selection and semantic). These findings highlight opportunities\nfor future improvements. Our open-sourced testbed DiagnoseRE is available in\n\\url{https://github.com/zjunlp/DiagnoseRE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphCodeBERT: Pre-training Code Representations with Data Flow. (arXiv:2009.08366v4 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2009.08366","description":"<p>Pre-trained models for programming language have achieved dramatic empirical\nimprovements on a variety of code-related tasks such as code search, code\ncompletion, code summarization, etc. However, existing pre-trained models\nregard a code snippet as a sequence of tokens, while ignoring the inherent\nstructure of code, which provides crucial code semantics and would enhance the\ncode understanding process. We present GraphCodeBERT, a pre-trained model for\nprogramming language that considers the inherent structure of code. Instead of\ntaking syntactic-level structure of code like abstract syntax tree (AST), we\nuse data flow in the pre-training stage, which is a semantic-level structure of\ncode that encodes the relation of \"where-the-value-comes-from\" between\nvariables. Such a semantic-level structure is neat and does not bring an\nunnecessarily deep hierarchy of AST, the property of which makes the model more\nefficient. We develop GraphCodeBERT based on Transformer. In addition to using\nthe task of masked language modeling, we introduce two structure-aware\npre-training tasks. One is to predict code structure edges, and the other is to\nalign representations between source code and code structure. We implement the\nmodel in an efficient way with a graph-guided masked attention function to\nincorporate the code structure. We evaluate our model on four tasks, including\ncode search, clone detection, code translation, and code refinement. Results\nshow that code structure and newly introduced pre-training tasks can improve\nGraphCodeBERT and achieves state-of-the-art performance on the four downstream\ntasks. We further show that the model prefers structure-level attentions over\ntoken-level attentions in the task of code search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Shengyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1\">Michele Tufano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shao Kun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v6 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2010.03084","description":"<p>Performing fact verification based on structured data is important for many\nreal-life applications and is a challenging research problem, particularly when\nit involves both symbolic operations and informal inference based on language\nunderstanding. In this paper, we present a Program-enhanced Verbalization and\nGraph Attention Network (ProgVGAT) to integrate programs and execution into\ntextual inference models. Specifically, a verbalization with program execution\nmodel is proposed to accumulate evidences that are embedded in operations over\nthe tables. Built on that, we construct the graph attention verification\nnetworks, which are designed to fuse different sources of evidences from\nverbalized program execution, program structures, and the original statements\nand tables, to make the final verification decision. To support the above\nframework, we propose a program selection module optimized with a new training\nstrategy based on margin loss, to produce more accurate programs, which is\nshown to be effective in enhancing the final verification results. Experimental\nresults show that the proposed framework achieves the new state-of-the-art\nperformance, a 74.4% accuracy, on the benchmark dataset TABFACT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1\">Feng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yufei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuoRAT: Towards Simpler Text-to-SQL Models. (arXiv:2010.11119v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11119","description":"<p>Recent neural text-to-SQL models can effectively translate natural language\nquestions to corresponding SQL queries on unseen databases. Working mostly on\nthe Spider dataset, researchers have proposed increasingly sophisticated\nsolutions to the problem. Contrary to this trend, in this paper we focus on\nsimplifications. We begin by building DuoRAT, a re-implementation of the\nstate-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware\nor vanilla transformers as the building blocks. We perform several ablation\nexperiments using DuoRAT as the baseline model. Our experiments confirm the\nusefulness of some techniques and point out the redundancy of others, including\nstructural SQL features and features that link the question with the schema.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Chris Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Paraphrasing with Pretrained Language Models. (arXiv:2010.12885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12885","description":"<p>Paraphrase generation has benefited extensively from recent progress in the\ndesigning of training objectives and model architectures. However, previous\nexplorations have largely focused on supervised methods, which require a large\namount of labeled data that is costly to collect. To address this drawback, we\nadopt a transfer learning approach and propose a training pipeline that enables\npre-trained language models to generate high-quality paraphrases in an\nunsupervised setting. Our recipe consists of task-adaptation, self-supervision,\nand a novel decoding algorithm named Dynamic Blocking (DB). To enforce a\nsurface form dissimilar from the input, whenever the language model emits a\ntoken contained in the source sequence, DB prevents the model from outputting\nthe subsequent source token for the next generation step. We show with\nautomatic and human evaluations that our approach achieves state-of-the-art\nperformance on both the Quora Question Pair (QQP) and the ParaNMT datasets and\nis robust to domain shift between the two datasets of distinct distributions.\nWe also demonstrate that our model transfers to paraphrasing in other languages\nwithout any additional finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskar_N/0/1/0/all/0/1\">Nitish Shirish Keskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Language Modeling for Improving Speech Recognition of Rare Words. (arXiv:2011.11715v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.11715","description":"<p>End-to-end automatic speech recognition (ASR) systems are increasingly\npopular due to their relative architectural simplicity and competitive\nperformance. However, even though the average accuracy of these systems may be\nhigh, the performance on rare content words often lags behind hybrid ASR\nsystems. To address this problem, second-pass rescoring is often applied\nleveraging upon language modeling. In this paper, we propose a second-pass\nsystem with multi-task learning, utilizing semantic targets (such as intent and\nslot prediction) to improve speech recognition performance. We show that our\nrescoring model trained with these additional tasks outperforms the baseline\nrescoring model, trained with only the language modeling task, by 1.4% on a\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\nrelative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR\ndeduction compared with RNN Transducer only ASR baseline for rare words\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filimonov_D/0/1/0/all/0/1\">Denis Filimonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02339","description":"<p>Image captioning models generally lack the capability to take into account\nuser interest, and usually default to global descriptions that try to balance\nreadability, informativeness, and information overload. On the other hand, VQA\nmodels generally lack the ability to provide long descriptive answers, while\nexpecting the textual question to be quite precise. We present a method to\ncontrol the concepts that an image caption should focus on, using an additional\ninput called the guiding text that refers to either groundable or ungroundable\nconcepts in the image. Our model consists of a Transformer-based multimodal\nencoder that uses the guiding text together with global and object-level image\nfeatures to derive early-fusion representations used to generate the guided\ncaption. While models trained on Visual Genome data have an in-domain advantage\nof fitting well when guided with automatic object labels, we find that guided\ncaptioning models trained on Conceptual Captions generalize better on\nout-of-domain images and guiding texts. Our human-evaluation results indicate\nthat attempting in-the-wild guided image captioning requires access to large,\nunrestricted-domain training datasets, and that increased style diversity (even\nwithout increasing the number of unique tokens) is a key factor for improved\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Sense Language Modelling. (arXiv:2012.05776v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.05776","description":"<p>The effectiveness of a language model is influenced by its token\nrepresentations, which must encode contextual information and handle the same\nword form having a plurality of meanings (polysemy). Currently, none of the\ncommon language modelling architectures explicitly model polysemy. We propose a\nlanguage model which not only predicts the next word, but also its sense in\ncontext. We argue that this higher prediction granularity may be useful for end\ntasks such as assistive writing, and allow for more a precise linking of\nlanguage models with knowledge bases. We find that multi-sense language\nmodelling requires architectures that go beyond standard language models, and\nhere propose a structured prediction framework that decomposes the task into a\nword followed by a sense prediction task. To aid sense prediction, we utilise a\nGraph Attention Network, which encodes definitions and example uses of word\nsenses. Overall, we find that multi-sense language modelling is a highly\nchallenging task, and suggest that future work focus on the creation of more\nannotated training datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lekkas_A/0/1/0/all/0/1\">Andrea Lekkas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_Kamp_P/0/1/0/all/0/1\">Peter Schneider-Kamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14541","description":"<p>Current TSA evaluation in a cross-domain setup is restricted to the small set\nof review domains available in existing datasets. Such an evaluation is\nlimited, and may not reflect true performance on sites like Amazon or Yelp that\nhost diverse reviews from many domains. To address this gap, we present YASO -\na new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215\nEnglish sentences from dozens of review domains, annotated with target terms\nand their sentiment. Our analysis verifies the reliability of these\nannotations, and explores the characteristics of the collected data. Benchmark\nresults using five contemporary TSA systems show there is ample room for\nimprovement on this challenging new dataset. YASO is available at\nhttps://github.com/IBM/yaso-tsa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orbach_M/0/1/0/all/0/1\">Matan Orbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_Ronen_O/0/1/0/all/0/1\">Orith Toledo-Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_A/0/1/0/all/0/1\">Artem Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15283","description":"<p>While pre-trained language models (PTLMs) have achieved noticeable success on\nmany NLP tasks, they still struggle for tasks that require event temporal\nreasoning, which is essential for event-centric applications. We present a\ncontinual pre-training approach that equips PTLMs with targeted knowledge about\nevent temporal relations. We design self-supervised learning objectives to\nrecover masked-out event and temporal indicators and to discriminate sentences\nfrom their corrupted counterparts (where event or temporal indicators got\nreplaced). By further pre-training a PTLM with these objectives jointly, we\nreinforce its attention to event and temporal information, yielding enhanced\ncapability on event temporal reasoning. This effective continual pre-training\nframework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning\nperformances across five relation extraction and question answering tasks and\nachieves new or on-par state-of-the-art performances in most of our downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From How Human Correct For Data-Centric Deep Learning. (arXiv:2102.00225v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% baseline is based on BERT training on the corrected dataset, which is\nhard to surpass.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Transformer Modifications Transfer Across Implementations and Applications?. (arXiv:2102.11972v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.11972","description":"<p>The research community has proposed copious modifications to the Transformer\narchitecture since it was introduced over three years ago, relatively few of\nwhich have seen widespread adoption. In this paper, we comprehensively evaluate\nmany of these modifications in a shared experimental setting that covers most\nof the common uses of the Transformer in natural language processing.\nSurprisingly, we find that most modifications do not meaningfully improve\nperformance. Furthermore, most of the Transformer variants we found beneficial\nwere either developed in the same codebase that we used or are relatively minor\nchanges. We conjecture that performance improvements may strongly depend on\nimplementation details and correspondingly make some recommendations for\nimproving the generality of experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matena_M/0/1/0/all/0/1\">Michael Matena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkan_K/0/1/0/all/0/1\">Karishma Malkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1\">Noah Fiedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_J/0/1/0/all/0/1\">Jake Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14583","description":"<p>Pre-trained speech representations like wav2vec 2.0 are a powerful tool for\nautomatic speech recognition (ASR). Yet many endangered languages lack\nsufficient data for pre-training such models, or are predominantly oral\nvernaculars without a standardised writing system, precluding fine-tuning.\nQuery-by-example spoken term detection (QbE-STD) offers an alternative for\niteratively indexing untranscribed speech corpora by locating spoken query\nterms. Using data from 7 Australian Aboriginal languages and a regional variety\nof Dutch, all of which are endangered or vulnerable, we show that QbE-STD can\nbe improved by leveraging representations developed for ASR (wav2vec 2.0: the\nEnglish monolingual model and XLSR53 multilingual model). Surprisingly, the\nEnglish model outperformed the multilingual model on 4 Australian language\ndatasets, raising questions around how to optimally leverage self-supervised\nspeech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0\nrepresentations (either English or XLSR53) offer large improvements (56-86%\nrelative) over state-of-the-art approaches on our endangered language datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Browne_M/0/1/0/all/0/1\">Mitchell Browne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_L/0/1/0/all/0/1\">Lily Clifford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_F/0/1/0/all/0/1\">Fiona Gibson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_J/0/1/0/all/0/1\">John Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_D/0/1/0/all/0/1\">David Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">Jane Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turpin_M/0/1/0/all/0/1\">Myfany Turpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollmer_M/0/1/0/all/0/1\">Maria Vollmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilmoth_S/0/1/0/all/0/1\">Sasha Wilmoth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convex Aggregation for Opinion Summarization. (arXiv:2104.01371v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01371","description":"<p>Recent advances in text autoencoders have significantly improved the quality\nof the latent space, which enables models to generate grammatical and\nconsistent text from aggregated latent vectors. As a successful application of\nthis property, unsupervised opinion summarization models generate a summary by\ndecoding the aggregated latent vectors of inputs. More specifically, they\nperform the aggregation via simple average. However, little is known about how\nthe vector aggregation step affects the generation quality. In this study, we\nrevisit the commonly used simple average approach by examining the latent space\nand generated summaries. We found that text autoencoders tend to generate\noverly generic summaries from simply averaged latent vectors due to an\nunexpected $L_2$-norm shrinkage in the aggregated latent vectors, which we\nrefer to as summary vector degeneration. To overcome this issue, we develop a\nframework Coop, which searches input combinations for the latent vector\naggregation using input-output word overlap. Experimental results show that\nCoop successfully alleviates the summary vector degeneration issue and\nestablishes new state-of-the-art performance on two opinion summarization\nbenchmarks. Code is available at \\url{https://github.com/megagonlabs/coop}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshihiko Suhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelidis_S/0/1/0/all/0/1\">Stefanos Angelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wang-Chiew Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Role of BERT Token Representations to Explain Sentence Probing Results. (arXiv:2104.01477v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01477","description":"<p>Several studies have been carried out on revealing linguistic features\ncaptured by BERT. This is usually achieved by training a diagnostic classifier\non the representations obtained from different layers of BERT. The subsequent\nclassification accuracy is then interpreted as the ability of the model in\nencoding the corresponding linguistic property. Despite providing insights,\nthese studies have left out the potential role of token representations. In\nthis paper, we provide a more in-depth analysis on the representation space of\nBERT in search for distinct and meaningful subspaces that can explain the\nreasons behind these probing results. Based on a set of probing tasks and with\nthe help of attribution methods we show that BERT tends to encode meaningful\nknowledge in specific token representations (which are often ignored in\nstandard classification setups), allowing the model to detect syntactic and\nsemantic abnormalities, and to distinctively separate grammatical number and\ntense subspaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohebbi_H/0/1/0/all/0/1\">Hosein Mohebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2104.03416","description":"<p>We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits of\nnon-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,\nFisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC\non giant Conformer neural network architectures with SpecAugment and wav2vec2\npre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,\n5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without\na language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering. (arXiv:2104.06239v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06239","description":"<p>Discontinuous constituent parsers have always lagged behind continuous\napproaches in terms of accuracy and speed, as the presence of constituents with\ndiscontinuous yield introduces extra complexity to the task. However, a\ndiscontinuous tree can be converted into a continuous variant by reordering\ntokens. Based on that, we propose to reduce discontinuous parsing to a\ncontinuous problem, which can then be directly solved by any off-the-shelf\ncontinuous parser. To that end, we develop a Pointer Network capable of\naccurately generating the continuous token arrangement for a given input\nsentence and define a bijective function to recover the original order.\nExperiments on the main benchmarks with two continuous parsers prove that our\napproach is on par in accuracy with purely discontinuous state-of-the-art\nalgorithms, but considerably faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Interpretable Clauses Semantically using Pretrained Word Representation. (arXiv:2104.06901v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06901","description":"<p>Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based\non propositional logic, which has demonstrated competitive performance in many\nNatural Language Processing (NLP) tasks, including sentiment analysis, text\nclassification, and Word Sense Disambiguation. To obtain human-level\ninterpretability, legacy TM employs Boolean input features such as bag-of-words\n(BOW). However, the BOW representation makes it difficult to use any\npre-trained information, for instance, word2vec and GloVe word representations.\nThis restriction has constrained the performance of TM compared to deep neural\nnetworks (DNNs) in NLP. To reduce the performance gap, in this paper, we\npropose a novel way of using pre-trained word representations for TM. The\napproach significantly enhances the performance and interpretability of TM. We\nachieve this by extracting semantically related words from pre-trained word\nrepresentations as input features to the TM. Our experiments show that the\naccuracy of the proposed approach is significantly higher than the previous\nBOW-based TM, reaching the level of DNN-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Rohan Kumar Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Lei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1\">Ole-Christoffer Granmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Representations of Text by Masking Transformers. (arXiv:2104.07155v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07155","description":"<p>Representations from large pretrained models such as BERT encode a range of\nfeatures into monolithic vectors, affording strong predictive accuracy across a\nmultitude of downstream tasks. In this paper we explore whether it is possible\nto learn disentangled representations by identifying existing subnetworks\nwithin pretrained models that encode distinct, complementary aspect\nrepresentations. Concretely, we learn binary masks over transformer weights or\nhidden units to uncover subsets of features that correlate with a specific\nfactor of variation; this eliminates the need to train a disentangled model\nfrom scratch for a particular task. We evaluate this method with respect to its\nability to disentangle representations of sentiment from genre in movie\nreviews, \"toxicity\" from dialect in Tweets, and syntax from semantics.\n</p>\n<p>By combining masking with magnitude pruning we find that we can identify\nsparse subnetworks within BERT that strongly encode particular aspects (e.g.,\ntoxicity) while only weakly encoding others (e.g., race). Moreover, despite\nonly learning masks, we find that disentanglement-via-masking performs as well\nas -- and often better than -- previously proposed methods based on variational\nautoencoders and adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiongyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Label-Adaptive Stance Detection. (arXiv:2104.07467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07467","description":"<p>Stance detection concerns the classification of a writer's viewpoint towards\na target. There are different task variants, e.g., stance of a tweet vs. a full\narticle, or stance with respect to a claim vs. an (implicit) topic. Moreover,\ntask definitions vary, which includes the label inventory, the data collection,\nand the annotation protocol. All these aspects hinder cross-domain studies, as\nthey require changes to standard domain adaptation approaches. In this paper,\nwe perform an in-depth analysis of 16 stance detection datasets, and we explore\nthe possibility for cross-domain learning from them. Moreover, we propose an\nend-to-end unsupervised framework for out-of-domain prediction of unseen,\nuser-defined labels. In particular, we combine domain adaptation techniques\nsuch as mixture of experts and domain-adversarial training with label\nembeddings, and we demonstrate sizable performance gains over strong baselines,\nboth (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for\nunseen targets. Finally, we perform an exhaustive analysis of the cross-domain\nresults, and we highlight the important factors influencing the model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy. (arXiv:2104.07571v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07571","description":"<p>The goal of question answering (QA) is to answer any question. However, major\nQA datasets have skewed distributions over gender, profession, and nationality.\nDespite that skew, model accuracy analysis reveals little evidence that\naccuracy is lower for people based on gender or nationality; instead, there is\nmore variation on professions (question topic). But QA's lack of representation\ncould itself hide evidence of bias, necessitating QA datasets that better\nrepresent global diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gor_M/0/1/0/all/0/1\">Maharshi Gor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_K/0/1/0/all/0/1\">Kellie Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07814","description":"<p>Growing polarization of the news media has been blamed for fanning\ndisagreement, controversy and even violence. Early identification of polarized\ntopics is thus an urgent matter that can help mitigate conflict. However,\naccurate measurement of topic-wise polarization is still an open research\nchallenge. To address this gap, we propose Partisanship-aware Contextualized\nTopic Embeddings (PaCTE), a method to automatically detect polarized topics\nfrom partisan news sources. Specifically, utilizing a language model that has\nbeen finetuned on recognizing partisanship of the news articles, we represent\nthe ideology of a news corpus on a topic by corpus-contextualized topic\nembedding and measure the polarization using cosine distance. We apply our\nmethod to a dataset of news articles about the COVID-19 pandemic. Extensive\nexperiments on different news sources and topics demonstrate the efficacy of\nour method to capture topical polarization, as indicated by its effectiveness\nof retrieving the most polarized topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1\">Negar Mokhberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camara_A/0/1/0/all/0/1\">Antonio Camara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abeliuk_A/0/1/0/all/0/1\">Andres Abeliuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07858","description":"<p>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.\nRecent studies propose supervised PQ, where the embedding and quantization\nmodels can be jointly trained with supervised learning. However, there is a\nlack of appropriate formulation of the joint training objective; thus, the\nimprovements over previous non-supervised baselines are limited in reality. In\nthis work, we propose the Matching-oriented Product Quantization (MoPQ), where\na novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the\nminimization of MCL, we are able to maximize the matching probability of query\nand ground-truth key, which contributes to the optimal retrieval accuracy.\nGiven that the exact computation of MCL is intractable due to the demand of\nvast contrastive samples, we further propose the Differentiable Cross-device\nSampling (DCS), which significantly augments the contrastive samples for\nprecise approximation of MCL. We conduct extensive experimental studies on four\nreal-world datasets, whose results verify the effectiveness of MoPQ. The code\nis available at https://github.com/microsoft/MoPQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counter-Interference Adapter for Multilingual Machine Translation. (arXiv:2104.08154v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08154","description":"<p>Developing a unified multilingual model has long been a pursuit for machine\ntranslation. However, existing approaches suffer from performance degradation\n-- a single multilingual model is inferior to separately trained bilingual ones\non rich-resource languages. We conjecture that such a phenomenon is due to\ninterference caused by joint training with multiple languages. To accommodate\nthe issue, we propose CIAT, an adapted Transformer model with a small parameter\noverhead for multilingual machine translation. We evaluate CIAT on multiple\nbenchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that\nCIAT consistently outperforms strong multilingual baselines on 64 of total 66\nlanguage directions, 42 of which see above 0.5 BLEU improvement. Our code is\navailable at \\url{https://github.com/Yaoming95/CIAT}~.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning. (arXiv:2104.08350v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08350","description":"<p>Understanding how events are semantically related to each other is the\nessence of reading comprehension. Recent event-centric reading comprehension\ndatasets focus mostly on event arguments or temporal relations. While these\ntasks partially evaluate machines' ability of narrative understanding,\nhuman-like reading comprehension requires the capability to process event-based\ninformation beyond arguments and temporal reasoning. For example, to understand\ncausality between events, we need to infer motivation or purpose; to establish\nevent hierarchy, we need to understand the composition of events. To facilitate\nthese tasks, we introduce ESTER, a comprehensive machine reading comprehension\n(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages\nnatural language queries to reason about the five most common event semantic\nrelations, provides more than 6K questions and captures 10.1K event relation\npairs. Experimental results show that the current SOTA systems achieve 22.1%,\n63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,\nwhich are all significantly below human performances (36.0%, 79.6%, 100%\nrespectively), highlighting our dataset as a challenging benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baylon_J/0/1/0/all/0/1\">Julia Baylon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qiang Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs. (arXiv:2104.08692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08692","description":"<p>Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive\nmonolingual texts, which has shown promising results on many cross-lingual\ntasks. In this paper, we improve multilingual text-to-text transfer Transformer\nwith translation pairs (mT6). Specifically, we explore three cross-lingual\ntext-to-text pre-training tasks, namely, machine translation, translation pair\nspan corruption, and translation span corruption. In addition, we propose a\npartially non-autoregressive objective for text-to-text pre-training. We\nevaluate the methods on eight multilingual benchmark datasets, including\nsentence classification, named entity recognition, question answering, and\nabstractive summarization. Experimental results show that the proposed mT6\nimproves cross-lingual transferability over mT5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaohan Huang Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GooAQ: Open Question Answering with Diverse Answer Types. (arXiv:2104.08727v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08727","description":"<p>While day-to-day questions come with a variety of answer types, the current\nquestion-answering (QA) literature has failed to adequately address the answer\ndiversity of questions. To this end, we present GooAQ, a large-scale dataset\nwith a variety of answer types. This dataset contains over 5 million questions\nand 3 million answers collected from Google. GooAQ questions are collected\nsemi-automatically from the Google search engine using its autocomplete\nfeature. This results in naturalistic questions of practical interest that are\nnonetheless short and expressed using simple language. GooAQ answers are mined\nfrom Google's responses to our collected questions, specifically from the\nanswer boxes in the search results. This yields a rich space of answer types,\ncontaining both textual answers (short and long) as well as more structured\nones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)\nin line with recent work, LM's strong performance on GooAQ's short-answer\nquestions heavily benefit from annotated data; however, (b) their quality in\ngenerating coherent and accurate responses for questions requiring long\nresponses (such as 'how' and 'why' questions) is less reliant on observing\nannotated data and mainly supported by their pre-training. We release GooAQ to\nfacilitate further research on improving QA with diverse response types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Amos Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can NLI Models Verify QA Systems' Predictions?. (arXiv:2104.08731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08731","description":"<p>To build robust question answering systems, we need the ability to verify\nwhether answers to questions are truly correct, not just \"good enough\" in the\ncontext of imperfect QA datasets. We explore the use of natural language\ninference (NLI) as a way to achieve this goal, as NLI inherently requires the\npremise (document context) to contain all necessary information to support the\nhypothesis (proposed answer to the question). We leverage large pre-trained\nmodels and recent prior datasets to construct powerful question converter and\ndecontextualization modules, which can reformulate QA instances as\npremise-hypothesis pairs with very high reliability. Then, by combining\nstandard NLI datasets with NLI examples automatically derived from QA training\ndata, we can train NLI models to judge the correctness of QA models' proposed\nanswers. We show that our NLI approach can generally improve the confidence\nestimation of a QA model across different domains, evaluated in a selective QA\nsetting. Careful manual analysis over the predictions of our NLI model shows\nthat it can further identify cases where the QA model produces the right answer\nfor the wrong reason, or where the answer cannot be verified as addressing all\naspects of the question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling End-to-End Models for Large-Scale Multilingual ASR. (arXiv:2104.14830v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.14830","description":"<p>Building ASR models across many languages is a challenging multi-task\nlearning problem due to large variations and heavily unbalanced data. Existing\nwork has shown positive transfer from high resource to low resource languages.\nHowever, degradations on high resource languages are commonly observed due to\ninterference from the heterogeneous multilingual data and reduction in\nper-language capacity. We conduct a capacity study on a 15-language task, with\nthe amount of data per language varying from 7.6K to 53.5K hours. We adopt\nGShard [1] to efficiently scale up to 10B parameters. Empirically, we find that\n(1) scaling the number of model parameters is an effective way to solve the\ncapacity bottleneck - our 500M-param model already outperforms monolingual\nbaselines and scaling it to 1B and 10B brought further quality gains; (2)\nlarger models are not only more data efficient, but also more efficient in\nterms of training cost as measured in TPU days - the 1B-param model reaches the\nsame accuracy at 34% of training time as the 500M-param model; (3) given a\nfixed capacity budget, adding depth works better than width and large encoders\ndo better than large decoders; (4) with continuous training, they can be\nadapted to new languages and domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulati_A/0/1/0/all/0/1\">Anmol Gulati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghani_P/0/1/0/all/0/1\">Parisa Haghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Junwen Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v2 [cs.CC] UPDATED)","link":"http://arxiv.org/abs/2106.02397","description":"<p>A continuous constraint satisfaction problem (CCSP) is a constraint\nsatisfaction problem (CSP) with a domain $U \\subset \\mathbb{R}$. We engage in a\nsystematic study to classify CCSPs that are complete of the Existential Theory\nof the Reals, i.e., ER-complete. To define this class, we first consider the\nproblem ETR, which also stands for Existential Theory of the Reals. In an\ninstance of this problem we are given some sentence of the form $\\exists x_1,\n\\ldots, x_n \\in \\mathbb{R} : \\Phi(x_1, \\ldots, x_n)$, where $\\Phi$ is a\nwell-formed quantifier-free formula consisting of the symbols $\\{0, 1, +,\n\\cdot, \\geq, &gt;, \\wedge, \\vee, \\neg\\}$, the goal is to check whether this\nsentence is true. Now the class ER is the family of all problems that admit a\npolynomial-time reduction to ETR. It is known that NP $\\subseteq$ ER\n$\\subseteq$ PSPACE.\n</p>\n<p>We restrict our attention on CCSPs with addition constraints ($x + y = z$)\nand some other mild technical condition. Previously, it was shown that\nmultiplication constraints ($x \\cdot y = z$), squaring constraints ($x^2 = y$),\nor inversion constraints ($x\\cdot y = 1$) are sufficient to establish\nER-completeness. We extend this in the strongest possible sense for equality\nconstraints as follows. We show that CCSPs (with addition constraints and some\nother mild technical condition) that have any one well-behaved curved equality\nconstraint ($f(x,y) = 0$) are ER-complete. We further extend our results to\ninequality constraints. We show that any well-behaved convexly curved and any\nwell-behaved concavely curved inequality constraint ($f(x,y) \\geq 0$ and\n$g(x,y) \\geq 0$) imply ER-completeness on the class of such CCSPs.\n</p>\n<p>We apply our findings to geometric packing and answer an open question by\nAbrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing\nconvex pieces into a square container under rotations and translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1\">Tillmann Miltzow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmiermann_R/0/1/0/all/0/1\">Reinier F. Schmiermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment. (arXiv:2106.06381v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06381","description":"<p>The cross-lingual language models are typically pretrained with masked\nlanguage modeling on multilingual text or parallel sentences. In this paper, we\nintroduce denoising word alignment as a new cross-lingual pre-training task.\nSpecifically, the model first self-labels word alignments for parallel\nsentences. Then we randomly mask tokens in a bitext pair. Given a masked token,\nthe model uses a pointer network to predict the aligned token in the other\nlanguage. We alternately perform the above two steps in an\nexpectation-maximization manner. Experimental results show that our method\nimproves cross-lingual transferability on various datasets, especially on the\ntoken-level tasks, such as question answering, and structured prediction.\nMoreover, the model can serve as a pretrained word aligner, which achieves\nreasonably low error rates on the alignment benchmarks. The code and pretrained\nparameters are available at https://github.com/CZWin32768/XLM-Align.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02401","description":"<p>This paper introduces WeChat AI's participation in WMT 2021 shared news\ntranslation task on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and\nEnglish-&gt;German. Our systems are based on the Transformer (Vaswani et al.,\n2017) with several novel and effective variants. In our experiments, we employ\ndata filtering, large-scale synthetic data generation (i.e., back-translation,\nknowledge distillation, forward-translation, iterative in-domain knowledge\ntransfer), advanced finetuning approaches, and boosted Self-BLEU based model\nensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3\ncase-sensitive BLEU scores on English-&gt;Chinese, English-&gt;Japanese,\nJapanese-&gt;English and English-&gt;German, respectively. The BLEU scores of\nEnglish-&gt;Chinese, English-&gt;Japanese and Japanese-&gt;English are the highest among\nall submissions, and that of English-&gt;German is the highest among all\nconstrained submissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianfeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Ernan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1\">Qiu Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08676","description":"<p>Existing system dealing with online complaint provides a final decision\nwithout explanations. We propose to analyse the complaint text of internet\nfraud in a fine-grained manner. Considering the complaint text includes\nmultiple clauses with various functions, we propose to identify the role of\neach clause and classify them into different types of fraud element. We\nconstruct a large labeled dataset originated from a real finance service\nplatform. We build an element identification model on top of BERT and propose\nadditional two modules to utilize the context of complaint text for better\nelement label classification, namely, global context encoder and label refiner.\nExperimental results show the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingchao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Heng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liaosa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weiqiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts. (arXiv:2108.11830v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11830","description":"<p>Dialogue models trained on human conversations inadvertently learn to\ngenerate toxic responses. In addition to producing explicitly offensive\nutterances, these models can also implicitly insult a group or individual by\naligning themselves with an offensive statement. To better understand the\ndynamics of contextually offensive language, we investigate the stance of\ndialogue model responses in offensive Reddit conversations. Specifically, we\ncreate ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model\nresponses labeled with offensive language and stance. Our analysis reveals that\n42% of human responses agree with toxic comments, whereas only 13% agree with\nsafe comments. This undesirable behavior is learned by neural dialogue models,\nsuch as DialoGPT, which we show are two times more likely to agree with\noffensive comments. To enable automatic detection of offensive language, we\nfine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for\noffensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the\neffectiveness of controllable text generation (CTG) methods to mitigate the\ntendency of neural dialogue models to agree with offensive comments. Compared\nto the baseline, our best CTG model achieves a 19% reduction in agreement with\noffensive comments and produces 29% fewer offensive replies. Our work\nhighlights the need for further efforts to characterize and analyze\ninappropriate behavior in dialogue models, in order to help make them safer.\nOur code and corpus are available at https://github.com/abaheti95/ToxiChat .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12084","description":"<p>Gender is widely discussed in the context of language tasks and when\nexamining the stereotypes propagated by language models. However, current\ndiscussions primarily treat gender as binary, which can perpetuate harms such\nas the cyclical erasure of non-binary gender identities. These harms are driven\nby model and dataset biases, which are consequences of the non-recognition and\nlack of understanding of non-binary genders in society. In this paper, we\nexplain the complexity of gender and language around it, and survey non-binary\npersons to understand harms associated with the treatment of gender as binary\nin English language technologies. We also detail how current language\nrepresentations (e.g., GloVe, BERT) capture and perpetuate these harms and\nrelated challenges that need to be acknowledged and addressed for\nrepresentations to equitably encode gender information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work has claimed, our auxiliary experiments suggest that\nrelation prediction is contributory to named entity prediction in a\nnon-negligible way. The source code can be found at\nhttps://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00055","description":"<p>Representation learning for text via pretraining a language model on a large\ncorpus has become a standard starting point for building NLP systems. This\napproach stands in contrast to autoencoders, also trained on raw text, but with\nthe objective of learning to encode each input as a vector that allows full\nreconstruction. Autoencoders are attractive because of their latent space\nstructure and generative properties. We therefore explore the construction of a\nsentence-level autoencoder from a pretrained, frozen transformer language\nmodel. We adapt the masked language modeling objective as a generative,\ndenoising one, while only training a sentence bottleneck and a single-layer\nmodified transformer decoder. We demonstrate that the sentence representations\ndiscovered by our model achieve better quality than previous methods that\nextract representations from pretrained transformers on text similarity tasks,\nstyle transfer (an example of controlled generation), and single-sentence\nclassification tasks in the GLUE benchmark, while using fewer parameters than\nlarge pretrained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montero_I/0/1/0/all/0/1\">Ivan Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00544","description":"<p>Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models'\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP models, which\nwe name Attacking to Training (A2T). The core part of A2T is a new and cheaper\nword substitution attack optimized for vanilla adversarial training. We use A2T\nto train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI\ndatasets. Our results empirically show that it is possible to train robust NLP\nmodels using a much cheaper adversary. We demonstrate that vanilla adversarial\ntraining with A2T can improve an NLP model's robustness to the attack it was\noriginally trained with and also defend the model against other types of word\nsubstitution attacks. Furthermore, we show that A2T can improve NLP models'\nstandard accuracy, cross-domain generalization, and interpretability. Code is\navailable at https://github.com/QData/Textattack-A2T .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01951","description":"<p>The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (&lt; 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02102","description":"<p>This paper demonstrates that by fine-tuning an autoregressive language model\n(GPT-Neo) on appropriately structured step-by-step demonstrations, it is\npossible to teach it to execute a mathematical task that has previously proved\ndifficult for Transformers - longhand modulo operations - with a relatively\nsmall number of examples. Specifically, we fine-tune GPT-Neo to solve the\nnumbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et\nal. (<a href=\"/abs/1904.01557\">arXiv:1904.01557</a>) reported below 40% accuracy on this task with 2 million\ntraining examples. We show that after fine-tuning on 200 appropriately\nstructured demonstrations of solving long division problems and reporting the\nremainders, the smallest available GPT-Neo model achieves over 80% accuracy.\nThis is achieved by constructing an appropriate dataset for fine-tuning, with\nno changes to the learning algorithm. These results suggest that fine-tuning\nautoregressive language models on small sets of well-crafted demonstrations may\nbe a useful paradigm for enabling individuals without training in machine\nlearning to coax such models to perform some kinds of complex multi-step tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Recchia_G/0/1/0/all/0/1\">Gabriel Recchia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining. (arXiv:2109.04080v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04080","description":"<p>With the rapid increase in the volume of dialogue data from daily life, there\nis a growing demand for dialogue summarization. Unfortunately, training a large\nsummarization model is generally infeasible due to the inadequacy of dialogue\ndata with annotated summaries. Most existing works for low-resource dialogue\nsummarization directly pretrain models in other domains, e.g., the news domain,\nbut they generally neglect the huge difference between dialogues and\nconventional articles. To bridge the gap between out-of-domain pretraining and\nin-domain fine-tuning, in this work, we propose a multi-source pretraining\nparadigm to better leverage the external summary data. Specifically, we exploit\nlarge-scale in-domain non-summary data to separately pretrain the dialogue\nencoder and the summary decoder. The combined encoder-decoder model is then\npretrained on the out-of-domain summary data using adversarial critics, aiming\nto facilitate domain-agnostic summarization. The experimental results on two\npublic datasets show that with only limited training data, our approach\nachieves competitive performance and generalizes well in different dialogue\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bolin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xingwu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexico-semantic and affective modelling of Spanish poetry: A semi-supervised learning approach. (arXiv:2109.04152v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.04152","description":"<p>Text classification tasks have improved substantially during the last years\nby the usage of transformers. However, the majority of researches focus on\nprose texts, with poetry receiving less attention, specially for Spanish\nlanguage. In this paper, we propose a semi-supervised learning approach for\ninferring 21 psychological categories evoked by a corpus of 4572 sonnets, along\nwith 10 affective and lexico-semantic multiclass ones. The subset of poems used\nfor training an evaluation includes 270 sonnets. With our approach, we achieve\nan AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65\nfor 60% on the multiclass ones. The sonnets are modelled using transformers,\nthrough sentence embeddings, along with lexico-semantic and affective features,\nobtained by using external lexicons. Consequently, we see that this approach\nprovides an AUC increase of up to 0.12, as opposed to using transformers alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1\">Alberto Barbado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mar&#xed;a Dolores Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrera_D/0/1/0/all/0/1\">D&#xe9;bora Carrera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Different Amounts of Annotation: From Zero to Many Labels. (arXiv:2109.04408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04408","description":"<p>Training NLP systems typically assumes access to annotated data that has a\nsingle human label per example. Given imperfect labeling from annotators and\ninherent ambiguity of language, we hypothesize that single label is not\nsufficient to learn the spectrum of language interpretation. We explore new\nannotation distribution schemes, assigning multiple labels per example for a\nsmall subset of training examples. Introducing such multi label examples at the\ncost of annotating fewer examples brings clear gains on natural language\ninference task and entity typing task, even when we simply first train with a\nsingle label data and then fine tune with multi label examples. Extending a\nMixUp data augmentation framework, we propose a learning algorithm that can\nlearn from training examples with different amount of annotation (with zero,\none, or multiple labels). This algorithm efficiently combines signals from\nuneven training data and brings additional gains in low annotation budget and\ncross domain settings. Together, our method achieves consistent gains in two\ntasks, suggesting distributing labels unevenly among training examples can be\nbeneficial for many NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05021","description":"<p>Red-lesions, i.e., microaneurysms (MAs) and hemorrhages (HMs), are the early\nsigns of diabetic retinopathy (DR). The automatic detection of MAs and HMs on\nretinal fundus images is a challenging task. Most of the existing methods\ndetect either only MAs or only HMs because of the difference in their texture,\nsizes, and morphology. Though some methods detect both MAs and HMs, they suffer\nfrom the curse of dimensionality of shape and colors features and fail to\ndetect all shape variations of HMs such as flame-shaped HM. Leveraging the\nprogress in deep learning, we proposed a two-stream red lesions detection\nsystem dealing simultaneously with small and large red lesions. For this\nsystem, we introduced a new ROIs candidates generation method for large red\nlesions fundus images; it is based on blood vessel segmentation and\nmorphological operations, and reduces the computational complexity, and\nenhances the detection accuracy by generating a small number of potential\ncandidates. For detection, we adapted the Faster RCNN framework with two\nstreams. We used pre-trained VGGNet as a bone model and carried out several\nextensive experiments to tune it for vessels segmentation and candidates\ngeneration, and finally learning the appropriate mapping, which yields better\ndetection of the red lesions comparing with the state-of-the-art methods. The\nexperimental results validated the effectiveness of the system in the detection\nof both MAs and HMs; the method yields higher performance for per lesion\ndetection according to sensitivity under 4 FPIs on DiaretDB1-MA and\nDiaretDB1-HM datasets, and 1 FPI on e-ophtha and ROCh datasets than the state\nof the art methods w.r.t. various evaluation metrics. For DR screening, the\nsystem outperforms other methods on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asiri_N/0/1/0/all/0/1\">Norah Asiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Muhammad Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adel_F/0/1/0/all/0/1\">Fadwa Al Adel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aboalsamh_H/0/1/0/all/0/1\">Hatim Aboalsamh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time multimodal image registration with partial intraoperative point-set data. (arXiv:2109.05023v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05023","description":"<p>We present Free Point Transformer (FPT) - a deep neural network architecture\nfor non-rigid point-set registration. Consisting of two modules, a global\nfeature extraction module and a point transformation module, FPT does not\nassume explicit constraints based on point vicinity, thereby overcoming a\ncommon requirement of previous learning-based point-set registration methods.\nFPT is designed to accept unordered and unstructured point-sets with a variable\nnumber of points and uses a \"model-free\" approach without heuristic\nconstraints. Training FPT is flexible and involves minimizing an intuitive\nunsupervised loss function, but supervised, semi-supervised, and partially- or\nweakly-supervised training are also supported. This flexibility makes FPT\namenable to multimodal image registration problems where the ground-truth\ndeformations are difficult or impossible to measure. In this paper, we\ndemonstrate the application of FPT to non-rigid registration of prostate\nmagnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound\n(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete\nTRUS imaging and sparsely-sampled TRUS imaging, respectively. The results\nindicate superior accuracy to the alternative rigid and non-rigid registration\nalgorithms tested and substantially lower computation time. The rapid inference\npossible with FPT makes it particularly suitable for applications where\nreal-time registration is beneficial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M C Baum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C Barratt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medulloblastoma Tumor Classification using Deep Transfer Learning with Multi-Scale EfficientNets. (arXiv:2109.05025v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05025","description":"<p>Medulloblastoma (MB) is the most common malignant brain tumor in childhood.\nThe diagnosis is generally based on the microscopic evaluation of\nhistopathological tissue slides. However, visual-only assessment of\nhistopathological patterns is a tedious and time-consuming task and is also\naffected by observer variability. Hence, automated MB tumor classification\ncould assist pathologists by promoting consistency and robust quantification.\nRecently, convolutional neural networks (CNNs) have been proposed for this\ntask, while transfer learning has shown promising results. In this work, we\npropose an end-to-end MB tumor classification and explore transfer learning\nwith various input sizes and matching network dimensions. We focus on\ndifferentiating between the histological subtypes classic and\ndesmoplastic/nodular. For this purpose, we systematically evaluate recently\nproposed EfficientNets, which uniformly scale all dimensions of a CNN. Using a\ndata set with 161 cases, we demonstrate that pre-trained EfficientNets with\nlarger input resolutions lead to significant performance improvements compared\nto commonly used pre-trained CNN architectures. Also, we highlight the\nimportance of transfer learning, when using such large architectures. Overall,\nour best performing method achieves an F1-Score of 80.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bengs_M/0/1/0/all/0/1\">Marcel Bengs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bockmayr_M/0/1/0/all/0/1\">Michael Bockmayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schuller_U/0/1/0/all/0/1\">Ulrich Sch&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1\">Alexander Schlaefer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-Conditioned GAN. (arXiv:2109.05070v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05070","description":"<p>Generative Adversarial Networks (GANs) can generate near photo realistic\nimages in narrow domains such as human faces. Yet, modeling complex\ndistributions of datasets such as ImageNet and COCO-Stuff remains challenging\nin unconditional settings. In this paper, we take inspiration from kernel\ndensity estimation techniques and introduce a non-parametric approach to\nmodeling distributions of complex datasets. We partition the data manifold into\na mixture of overlapping neighborhoods described by a datapoint and its nearest\nneighbors, and introduce a model, called instance-conditioned GAN (IC-GAN),\nwhich learns the distribution around each datapoint. Experimental results on\nImageNet and COCO-Stuff show that IC-GAN significantly improves over\nunconditional models and unsupervised data partitioning baselines. Moreover, we\nshow that IC-GAN can effortlessly transfer to datasets not seen during training\nby simply changing the conditioning instances, and still generate realistic\nimages. Finally, we extend IC-GAN to the class-conditional case and show\nsemantically controllable generation and competitive quantitative results on\nImageNet; while improving over BigGAN on ImageNet-LT. We will opensource our\ncode and trained models to reproduce the reported results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casanova_A/0/1/0/all/0/1\">Arantxa Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Careil_M/0/1/0/all/0/1\">Marl&#xe8;ne Careil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1\">Michal Drozdzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1\">Adriana Romero-Soriano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A semi-supervised self-training method to develop assistive intelligence for segmenting multiclass bridge elements from inspection videos. (arXiv:2109.05078v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05078","description":"<p>Bridge inspection is an important step in preserving and rehabilitating\ntransportation infrastructure for extending their service lives. The\nadvancement of mobile robotic technology allows the rapid collection of a large\namount of inspection video data. However, the data are mainly images of complex\nscenes, wherein a bridge of various structural elements mix with a cluttered\nbackground. Assisting bridge inspectors in extracting structural elements of\nbridges from the big complex video data, and sorting them out by classes, will\nprepare inspectors for the element-wise inspection to determine the condition\nof bridges. This paper is motivated to develop an assistive intelligence model\nfor segmenting multiclass bridge elements from inspection videos captured by an\naerial inspection platform. With a small initial training dataset labeled by\ninspectors, a Mask Region-based Convolutional Neural Network (Mask R-CNN)\npre-trained on a large public dataset was transferred to the new task of\nmulticlass bridge element segmentation. Besides, the temporal coherence\nanalysis attempts to recover false negatives and identify the weakness that the\nneural network can learn to improve. Furthermore, a semi-supervised\nself-training (S$^3$T) method was developed to engage experienced inspectors in\nrefining the network iteratively. Quantitative and qualitative results from\nevaluating the developed deep neural network demonstrate that the proposed\nmethod can utilize a small amount of time and guidance from experienced\ninspectors (3.58 hours for labeling 66 images) to build the network of\nexcellent performance (91.8% precision, 93.6% recall, and 92.7% f1-score).\nImportantly, the paper illustrates an approach to leveraging the domain\nknowledge and experiences of bridge professionals into computational\nintelligence models to efficiently adapt the models to varied bridges in the\nNational Bridge Inventory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaozheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_e/0/1/0/all/0/1\">enda Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preliminary Wildfire Detection Using State-of-the-art PTZ (Pan, Tilt, Zoom) Camera Technology and Convolutional Neural Networks. (arXiv:2109.05083v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05083","description":"<p>Wildfires are uncontrolled fires in the environment that can be caused by\nhumans or nature. In 2020 alone, wildfires in California have burned 4.2\nmillion acres, damaged 10,500 buildings or structures, and killed more than 31\npeople, exacerbated by climate change and a rise in average global\ntemperatures. This also means there has been an increase in the costs of\nextinguishing these treacherous wildfires. The objective of the research is to\ndetect forest fires in their earlier stages to prevent them from spreading,\nprevent them from causing damage to a variety of things, and most importantly,\nreduce or eliminate the chances of someone dying from a wildfire. A fire\ndetection system should be efficient and accurate with respect to extinguishing\nwildfires in their earlier stages to prevent the spread of them along with\ntheir consequences. Computer Vision is potentially a more reliable, fast, and\nwidespread method we need. The current research in the field of preliminary\nfire detection has several problems related to unrepresentative data being used\nto train models and their existing varied amounts of label imbalance in the\nclasses of their dataset. We propose a more representative and evenly\ndistributed data through better settings, lighting, atmospheres, etc., and\nclass distribution in the entire dataset. After thoroughly examining the\nresults of this research, it can be inferred that they supported the datasets\nstrengths by being a viable resource when tested in the real world on\nunfamiliar data. This is evident since as the model trains on the dataset, it\nis able to generalize on it, hence confirming this is a viable Machine Learning\nsetting that has practical impact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Samarth Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05115","description":"<p>In this paper, we propose an approach to improve image captioning solutions\nfor images with novel objects that do not have caption labels in the training\ndataset. Our approach is agnostic to model architecture, and primarily focuses\non training technique that uses existing fully paired image-caption data and\nthe images with only the novel object detection labels (partially paired data).\nWe create synthetic paired captioning data for these novel objects by\nleveraging context from existing image-caption pairs. We further re-use these\npartially paired images with novel objects to create pseudo-label captions that\nare used to fine-tune the captioning model. Using a popular captioning model\n(Up-Down) as baseline, our approach achieves state-of-the-art results on\nheld-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for\nnovel object images by 75.8 and 26.6 points respectively, compared to baseline\nmodel that does not use partially paired images during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bujimalla_S/0/1/0/all/0/1\">Shashank Bujimalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subedar_M/0/1/0/all/0/1\">Mahesh Subedar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tickoo_O/0/1/0/all/0/1\">Omesh Tickoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Correcting: Noise-tolerant Medical Image Classification via mutual Label Correction. (arXiv:2109.05159v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05159","description":"<p>With the development of deep learning, medical image classification has been\nsignificantly improved. However, deep learning requires massive data with\nlabels. While labeling the samples by human experts is expensive and\ntime-consuming, collecting labels from crowd-sourcing suffers from the noises\nwhich may degenerate the accuracy of classifiers. Therefore, approaches that\ncan effectively handle label noises are highly desired. Unfortunately, recent\nprogress on handling label noise in deep learning has gone largely unnoticed by\nthe medical image. To fill the gap, this paper proposes a noise-tolerant\nmedical image classification framework named Co-Correcting, which significantly\nimproves classification accuracy and obtains more accurate labels through\ndual-network mutual learning, label probability estimation, and curriculum\nlabel correcting. On two representative medical image datasets and the MNIST\ndataset, we test six latest Learning-with-Noisy-Labels methods and conduct\ncomparative studies. The experiments show that Co-Correcting achieves the best\naccuracy and generalization under different noise ratios in various tasks. Our\nproject can be found at: https://github.com/JiarunLiu/Co-Correcting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiarun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_C/0/1/0/all/0/1\">Chuan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-Supervised Deep Framework for Reference Bony Shape Estimation in Orthognathic Surgical Planning. (arXiv:2109.05191v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05191","description":"<p>Virtual orthognathic surgical planning involves simulating surgical\ncorrections of jaw deformities on 3D facial bony shape models. Due to the lack\nof necessary guidance, the planning procedure is highly experience-dependent\nand the planning results are often suboptimal. A reference facial bony shape\nmodel representing normal anatomies can provide an objective guidance to\nimprove planning accuracy. Therefore, we propose a self-supervised deep\nframework to automatically estimate reference facial bony shape models. Our\nframework is an end-to-end trainable network, consisting of a simulator and a\ncorrector. In the training stage, the simulator maps jaw deformities of a\npatient bone to a normal bone to generate a simulated deformed bone. The\ncorrector then restores the simulated deformed bone back to normal. In the\ninference stage, the trained corrector is applied to generate a\npatient-specific normal-looking reference bone from a real deformed bone. The\nproposed framework was evaluated using a clinical dataset and compared with a\nstate-of-the-art method that is based on a supervised point-cloud network.\nExperimental results show that the estimated shape models given by our approach\nare clinically acceptable and significantly more accurate than that of the\ncompeting method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Deqiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hannah Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_T/0/1/0/all/0/1\">Tianshu Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_Y/0/1/0/all/0/1\">Yankun Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gateno_J/0/1/0/all/0/1\">Jaime Gateno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Steve Guofang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">James J. Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Follow the Curve: Robotic-Ultrasound Navigation with Learning Based Localization of Spinous Processes for Scoliosis Assessment. (arXiv:2109.05196v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05196","description":"<p>The scoliosis progression in adolescents requires close monitoring to timely\ntake treatment measures. Ultrasound imaging is a radiation-free and low-cost\nalternative in scoliosis assessment to X-rays, which are typically used in\nclinical practice. However, ultrasound images are prone to speckle noises,\nmaking it challenging for sonographers to detect bony features and follow the\nspine's curvature. This paper introduces a robotic-ultrasound approach for\nspinal curvature tracking and automatic navigation. A fully connected network\nwith deconvolutional heads is developed to locate the spinous process\nefficiently with real-time ultrasound images. We use this machine\nlearning-based method to guide the motion of the robot-held ultrasound probe\nand follow the spinal curvature while capturing ultrasound images and\ncorrespondent position. We developed a new force-driven controller that\nautomatically adjusts the probe's pose relative to the skin surface to ensure a\ngood acoustic coupling between the probe and skin. After the scanning, the\nacquired data is used to reconstruct the coronal spinal image, where the\ndeformity of the scoliosis spine can be assessed and measured. To evaluate the\nperformance of our methodology, we conducted an experimental study with human\nsubjects where the deviations from the image center during the robotized\nprocedure are compared to that obtained from manual scanning. The angles of\nspinal deformity measured on spinal reconstruction images were similar for both\nmethods, implying that they equally reflect human anatomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Victorova_M/0/1/0/all/0/1\">Maria Victorova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_M/0/1/0/all/0/1\">Michael Ka-Shing Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navarro_Alarcon_D/0/1/0/all/0/1\">David Navarro-Alarcon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yongping Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Generation of Synthetic Geospatial Images from Pixel-level and Feature-level Inputs. (arXiv:2109.05201v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05201","description":"<p>Training robust supervised deep learning models for many geospatial\napplications of computer vision is difficult due to dearth of class-balanced\nand diverse training data. Conversely, obtaining enough training data for many\napplications is financially prohibitive or may be infeasible, especially when\nthe application involves modeling rare or extreme events. Synthetically\ngenerating data (and labels) using a generative model that can sample from a\ntarget distribution and exploit the multi-scale nature of images can be an\ninexpensive solution to address scarcity of labeled data. Towards this goal, we\npresent a deep conditional generative model, called VAE-Info-cGAN, that\ncombines a Variational Autoencoder (VAE) with a conditional Information\nMaximizing Generative Adversarial Network (InfoGAN), for synthesizing\nsemantically rich images simultaneously conditioned on a pixel-level condition\n(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC\ncan only vary in the channel dimension from the synthesized image and is meant\nto be a task-specific input. The FLC is modeled as an attribute vector in the\nlatent space of the generated image which controls the contributions of various\ncharacteristic attributes germane to the target distribution. Experiments on a\nGPS trajectories dataset show that the proposed model can accurately generate\nvarious forms of spatiotemporal aggregates across different geographic\nlocations while conditioned only on a raster representation of the road\nnetwork. The primary intended application of the VAE-Info-cGAN is synthetic\ndata (and label) generation for targeted data augmentation for computer\nvision-based modeling of problems relevant to geospatial analysis and remote\nsensing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuerong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Swetava Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vipul Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Quantization with Code Memory for Unsupervised Image Retrieval. (arXiv:2109.05205v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05205","description":"<p>The high efficiency in computation and storage makes hashing (including\nbinary hashing and quantization) a common strategy in large-scale retrieval\nsystems. To alleviate the reliance on expensive annotations, unsupervised deep\nhashing becomes an important research problem. This paper provides a novel\nsolution to unsupervised deep quantization, namely Contrastive Quantization\nwith Code Memory (MeCoQ). Different from existing reconstruction-based\nstrategies, we learn unsupervised binary descriptors by contrastive learning,\nwhich can better capture discriminative visual semantics. Besides, we uncover\nthat codeword diversity regularization is critical to prevent contrastive\nlearning-based quantization from model degeneration. Moreover, we introduce a\nnovel quantization code memory module that boosts contrastive learning with\nlower feature drift than conventional feature memories. Extensive experiments\non benchmark datasets show that MeCoQ outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05206","description":"<p>Deep hashing approaches, including deep quantization and deep binary hashing,\nhave become a common solution to large-scale image retrieval due to high\ncomputation and storage efficiency. Most existing hashing methods can not\nproduce satisfactory results for fine-grained retrieval, because they usually\nadopt the outputs of the last CNN layer to generate binary codes, which is less\neffective to capture subtle but discriminative visual details. To improve\nfine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization\n(PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to\ncapture and preserve fine-grained semantic information from multi-level\nfeatures. Besides, we propose a learnable quantization module with a partial\nattention mechanism, which helps to optimize the most relevant codewords and\nimproves the quantization. Comprehensive experiments demonstrate that PHPQ\noutperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (arXiv:2109.05211v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05211","description":"<p>Deep neural networks (DNNs) are vulnerable to adversarial noises, which\nmotivates the benchmark of model robustness. Existing benchmarks mainly focus\non evaluating the defenses, but there are no comprehensive studies of how\narchitecture design and general training techniques affect robustness.\nComprehensively benchmarking their relationships will be highly beneficial for\nbetter understanding and developing robust DNNs. Thus, we propose RobustART,\nthe first comprehensive Robustness investigation benchmark on ImageNet\n(including open-source toolkit, pre-trained model zoo, datasets, and analyses)\nregarding ARchitecture design (44 human-designed off-the-shelf architectures\nand 1200+ networks from neural architecture search) and Training techniques\n(10+ general techniques, e.g., data augmentation) towards diverse noises\n(adversarial, natural, and system noises). Extensive experiments revealed and\nsubstantiated several insights for the first time, for example: (1) adversarial\ntraining largely improves the clean accuracy and all types of robustness for\nTransformers and MLP-Mixers; (2) with comparable sizes, CNNs &gt; Transformers &gt;\nMLP-Mixers on robustness against natural and system noises; Transformers &gt;\nMLP-Mixers &gt; CNNs on adversarial robustness; (3) for some light-weight\narchitectures (e.g., EfficientNet, MobileNetV2, and MobileNetV3), increasing\nmodel sizes or using extra training data cannot improve robustness. Our\nbenchmark <a href=\"http://robust.art/\">this http URL</a> : (1) presents an open-source platform for\nconducting comprehensive evaluation on diverse robustness types; (2) provides a\nvariety of pre-trained models with different training techniques to facilitate\nrobustness evaluation; (3) proposes a new view to better understand the\nmechanism towards designing robust DNN architectures, backed up by the\nanalysis. We will continuously contribute to building this ecosystem for the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shiyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bornon: Bengali Image Captioning with Transformer-based Deep learning approach. (arXiv:2109.05218v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05218","description":"<p>Image captioning using Encoder-Decoder based approach where CNN is used as\nthe Encoder and sequence generator like RNN as Decoder has proven to be very\neffective. However, this method has a drawback that is sequence needs to be\nprocessed in order. To overcome this drawback some researcher has utilized the\nTransformer model to generate captions from images using English datasets.\nHowever, none of them generated captions in Bengali using the transformer\nmodel. As a result, we utilized three different Bengali datasets to generate\nBengali captions from images using the Transformer model. Additionally, we\ncompared the performance of the transformer-based model with a visual\nattention-based Encoder-Decoder approach. Finally, we compared the result of\nthe transformer-based model with other models that employed different Bengali\nimage captioning datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1\">Faisal Muhammad Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humaira_M/0/1/0/all/0/1\">Mayeesha Humaira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jim_M/0/1/0/all/0/1\">Md Abidur Rahman Khan Jim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ami_A/0/1/0/all/0/1\">Amit Saha Ami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shimul Paul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Hough Matching Networks for Robust and Efficient Visual Correspondence. (arXiv:2109.05221v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05221","description":"<p>Despite advances in feature representation, leveraging geometric relations is\ncrucial for establishing reliable visual correspondences under large variations\nof images. In this work we introduce a Hough transform perspective on\nconvolutional matching and propose an effective geometric matching algorithm,\ndubbed Convolutional Hough Matching (CHM). The method distributes similarities\nof candidate matches over a geometric transformation space and evaluates them\nin a convolutional manner. We cast it into a trainable neural layer with a\nsemi-isotropic high-dimensional kernel, which learns non-rigid matching with a\nsmall number of interpretable parameters. To further improve the efficiency of\nhigh-dimensional voting, we also propose to use an efficient kernel\ndecomposition with center-pivot neighbors, which significantly sparsifies the\nproposed semi-isotropic kernels without performance degradation. To validate\nthe proposed techniques, we develop the neural network with CHM layers that\nperform convolutional matching in the space of translation and scaling. Our\nmethod sets a new state of the art on standard benchmarks for semantic visual\ncorrespondence, proving its strong robustness to challenging intra-class\nvariations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1\">Juhong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Computer Vision Techniques for Urban Mobility on Large-Scale, Unconstrained Roads. (arXiv:2109.05226v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05226","description":"<p>Conventional approaches for addressing road safety rely on manual\ninterventions or immobile CCTV infrastructure. Such methods are expensive in\nenforcing compliance to traffic rules and do not scale to large road networks.\nThis paper proposes a simple mobile imaging setup to address several common\nproblems in road safety at scale. We use recent computer vision techniques to\nidentify possible irregularities on roads, the absence of street lights, and\ndefective traffic signs using videos from a moving camera-mounted vehicle.\nBeyond the inspection of static road infrastructure, we also demonstrate the\nmobile imaging solution's applicability to spot traffic violations. Before\ndeploying our system in the real-world, we investigate the strengths and\nshortcomings of computer vision techniques on thirteen condition-based\nhierarchical labels. These conditions include different timings, road type,\ntraffic density, and state of road damage. Our demonstrations are then carried\nout on 2000 km of unconstrained road scenes, captured across an entire city.\nThrough this, we quantitatively measure the overall safety of roads in the city\nthrough carefully constructed metrics. We also show an interactive dashboard\nfor visually inspecting and initiating action in a time, labor and\ncost-efficient manner. Code, models, and datasets used in this work will be\npublicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rithish_H/0/1/0/all/0/1\">Harish Rithish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modhugu_R/0/1/0/all/0/1\">Raghava Modhugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Ranjith Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saluja_R/0/1/0/all/0/1\">Rohit Saluja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Statistical Representation with Joint Deep Embedded Clustering. (arXiv:2109.05232v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05232","description":"<p>One of the most promising approaches for unsupervised learning is combining\ndeep representation learning and deep clustering. Some recent works propose to\nsimultaneously learn representation using deep neural networks and perform\nclustering by defining a clustering loss on top of embedded features. However,\nthese approaches are sensitive to imbalanced data and out-of-distribution\nsamples. Hence, these methods optimize clustering by pushing data close to\nrandomly initialized cluster centers. This is problematic when the number of\ninstances varies largely in different classes or a cluster with few samples has\nless chance to be assigned a good centroid. To overcome these limitations, we\nintroduce StatDEC, a new unsupervised framework for joint statistical\nrepresentation learning and clustering. StatDEC simultaneously trains two deep\nlearning models, a deep statistics network that captures the data distribution,\nand a deep clustering network that learns embedded features and performs\nclustering by explicitly defining a clustering loss. Specifically, the\nclustering network and representation network both take advantage of our\nproposed statistics pooling layer that represents mean, variance, and\ncardinality to handle the out-of-distribution samples as well as a class\nimbalance. Our experiments show that using these representations, one can\nconsiderably improve results on imbalanced image clustering across a variety of\nimage datasets. Moreover, the learned representations generalize well when\ntransferred to the out-of-distribution dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1\">Mina Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorigatti_E/0/1/0/all/0/1\">Emilio Dorigatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruegamer_D/0/1/0/all/0/1\">David Ruegamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Distribution-Aware Calibration for Long-Tailed Visual Recognition. (arXiv:2109.05263v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05263","description":"<p>Despite impressive accuracy, deep neural networks are often miscalibrated and\ntend to overly confident predictions. Recent techniques like temperature\nscaling (TS) and label smoothing (LS) show effectiveness in obtaining a\nwell-calibrated model by smoothing logits and hard labels with scalar factors,\nrespectively. However, the use of uniform TS or LS factor may not be optimal\nfor calibrating models trained on a long-tailed dataset where the model\nproduces overly confident probabilities for high-frequency classes. In this\nstudy, we propose class-distribution-aware TS (CDA-TS) and LS (CDA-LS) by\nincorporating class frequency information in model calibration in the context\nof long-tailed distribution. In CDA-TS, the scalar temperature value is\nreplaced with the CDA temperature vector encoded with class frequency to\ncompensate for the over-confidence. Similarly, CDA-LS uses a vector smoothing\nfactor and flattens the hard labels according to their corresponding class\ndistribution. We also integrate CDA optimal temperature vector with\ndistillation loss, which reduces miscalibration in self-distillation (SD). We\nempirically show that class-distribution-aware TS and LS can accommodate the\nimbalanced data distribution yielding superior performance in both calibration\nerror and predictive accuracy. We also observe that SD with an extremely\nimbalanced dataset is less effective in terms of calibration performance. Code\nis available in https://github.com/mobarakol/Class-Distribution-Aware-TS-LS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seenivasan_L/0/1/0/all/0/1\">Lalithkumar Seenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v1 [cs.RO])","link":"http://arxiv.org/abs/2109.05265","description":"<p>Stereoscopy exposits a natural perception of distance in a scene, and its\nmanifestation in 3D world understanding is an intuitive phenomenon. However, an\ninnate rigid calibration of binocular vision sensors is crucial for accurate\ndepth estimation. Alternatively, a monocular camera alleviates the limitation\nat the expense of accuracy in estimating depth, and the challenge exacerbates\nin harsh environmental conditions. Moreover, an optical sensor often fails to\nacquire vital signals in harsh environments, and radar is used instead, which\ngives coarse but more accurate signals. This work explores the utility of\ncoarse signals from radar when fused with fine-grained data from a monocular\ncamera for depth estimation in harsh environmental conditions. A variant of\nfeature pyramid network (FPN) extensively operates on fine-grained image\nfeatures at multiple scales with a fewer number of parameters. FPN feature maps\nare fused with sparse radar features extracted with a Convolutional neural\nnetwork. The concatenated hierarchical features are used to predict the depth\nwith ordinal regression. We performed experiments on the nuScenes dataset, and\nthe proposed architecture stays on top in quantitative evaluations with reduced\nparameters and faster inference. The depth estimation results suggest that the\nproposed techniques can be used as an alternative to stereo depth estimation in\ncritical applications in robotics and self-driving cars. The source code will\nbe available in the following: \\url{https://github.com/MI-Hussain/RVMDE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1\">Muhamamd Ishfaq Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_M/0/1/0/all/0/1\">Muhammad Aasim Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05281","description":"<p>Developers of text generation models rely on automated evaluation metrics as\na stand-in for slow and expensive manual evaluations. However, image captioning\nmetrics have struggled to give accurate learned estimates of the semantic and\npragmatic success of output text. We address this weakness by introducing the\nfirst discourse-aware learned generation metric for evaluating image\ndescriptions. Our approach is inspired by computational theories of discourse\nfor capturing information goals using coherence. We present a dataset of\nimage$\\unicode{x2013}$description pairs annotated with coherence relations. We\nthen train a coherence-aware metric on a subset of the Conceptual Captions\ndataset and measure its effectiveness$\\unicode{x2014}$its ability to predict\nhuman ratings of output captions$\\unicode{x2014}$on a test set composed of\nout-of-domain images. We demonstrate a higher Kendall Correlation Coefficient\nfor our proposed metric with the human judgments for the results of a number of\nstate-of-the-art coherence-aware caption generation models when compared to\nseveral other metrics including recently proposed learned metrics such as\nBLEURT and BERTScore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_B/0/1/0/all/0/1\">Baber Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-view Snapshot Compressive Imaging via Optical Flow Aided Recurrent Neural Network. (arXiv:2109.05287v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05287","description":"<p>Dual-view snapshot compressive imaging (SCI) aims to capture videos from two\nfield-of-views (FoVs) using a 2D sensor (detector) in a single snapshot,\nachieving joint FoV and temporal compressive sensing, and thus enjoying the\nadvantages of low-bandwidth, low-power, and low-cost. However, it is\nchallenging for existing model-based decoding algorithms to reconstruct each\nindividual scene, which usually require exhaustive parameter tuning with\nextremely long running time for large scale data. In this paper, we propose an\noptical flow-aided recurrent neural network for dual video SCI systems, which\nprovides high-quality decoding in seconds. Firstly, we develop a diversity\namplification method to enlarge the differences between scenes of two FoVs, and\ndesign a deep convolutional neural network with dual branches to separate\ndifferent scenes from the single measurement. Secondly, we integrate the\nbidirectional optical flow extracted from adjacent frames with the recurrent\nneural network to jointly reconstruct each video in a sequential manner.\nExtensive results on both simulation and real data demonstrate the superior\nperformance of our proposed model in a short inference time. The code and data\nare available at https://github.com/RuiyingLu/OFaNet-for-Dual-view-SCI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_R/0/1/0/all/0/1\">Ruiying Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1\">Guanliang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziheng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_M/0/1/0/all/0/1\">Mu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation. (arXiv:2109.05346v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05346","description":"<p>Scene graphs are nodes and edges consisting of objects and object-object\nrelationships, respectively. Scene graph generation (SGG) aims to identify the\nobjects and their relationships. We propose a bidirectional GRU (BiGRU)\ntransformer network (BGT-Net) for the scene graph generation for images. This\nmodel implements novel object-object communication to enhance the object\ninformation using a BiGRU layer. Thus, the information of all objects in the\nimage is available for the other objects, which can be leveraged later in the\nobject prediction step. This object information is used in a transformer\nencoder to predict the object class as well as to create object-specific edge\ninformation via the use of another transformer encoder. To handle the dataset\nbias induced by the long-tailed relationship distribution, softening with a\nlog-softmax function and adding a bias adaptation term to regulate the bias for\nevery relation prediction individually showed to be an effective approach. We\nconducted an elaborate study on experiments and ablations using open-source\ndatasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection\ndatasets, demonstrating the effectiveness of the proposed model over state of\nthe art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_N/0/1/0/all/0/1\">Naina Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_F/0/1/0/all/0/1\">Florian Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunz_A/0/1/0/all/0/1\">Andreas Kunz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPyram: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos. (arXiv:2109.05352v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05352","description":"<p>Semantic segmentation in cataract surgery has a wide range of applications\ncontributing to surgical outcome enhancement and clinical risk reduction.\nHowever, the varying issues in segmenting the different relevant instances make\nthe designation of a unique network quite challenging. This paper proposes a\nsemantic segmentation network termed as DeepPyram that can achieve superior\nperformance in segmenting relevant objects in cataract surgery videos with\nvarying issues. This superiority mainly originates from three modules: (i)\nPyramid View Fusion, which provides a varying-angle global view of the\nsurrounding region centering at each pixel position in the input convolutional\nfeature map; (ii) Deformable Pyramid Reception, which enables a wide deformable\nreceptive field that can adapt to geometric transformations in the object of\ninterest; and (iii) Pyramid Loss that adaptively supervises multi-scale\nsemantic feature maps. These modules can effectively boost semantic\nsegmentation performance, especially in the case of transparency,\ndeformability, scalability, and blunt edges in objects. The proposed approach\nis evaluated using four datasets of cataract surgery for objects with different\ncontextual features and compared with thirteen state-of-the-art segmentation\nnetworks. The experimental results confirm that DeepPyram outperforms the rival\napproaches without imposing additional trainable parameters. Our comprehensive\nablation study further proves the effectiveness of the proposed modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghamsarian_N/0/1/0/all/0/1\">Negin Ghamsarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschwer_M/0/1/0/all/0/1\">Mario Taschwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoeffmann_k/0/1/0/all/0/1\">klaus Schoeffmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline using Graph Convolutional Network. (arXiv:2109.05353v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05353","description":"<p>We present Border-SegGCN, a novel architecture to improve semantic\nsegmentation by refining the border outline using graph convolutional networks\n(GCN). The semantic segmentation network such as Unet or DeepLabV3+ is used as\na base network to have pre-segmented output. This output is converted into a\ngraphical structure and fed into the GCN to improve the border pixel prediction\nof the pre-segmented output. We explored and studied the factors such as border\nthickness, number of edges for a node, and the number of features to be fed\ninto the GCN by performing experiments. We demonstrate the effectiveness of the\nBorder-SegGCN on the CamVid and Carla dataset, achieving a test set performance\nof 81.96% without any post-processing on CamVid dataset. It is higher than the\nreported state of the art mIoU achieved on CamVid dataset by 0.404%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_N/0/1/0/all/0/1\">Naina Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chogovadze_G/0/1/0/all/0/1\">George Chogovadze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunz_A/0/1/0/all/0/1\">Andreas Kunz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sickle Cell Disease Severity Prediction from Percoll Gradient Images using Graph Convolutional Networks. (arXiv:2109.05372v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05372","description":"<p>Sickle cell disease (SCD) is a severe genetic hemoglobin disorder that\nresults in premature destruction of red blood cells. Assessment of the severity\nof the disease is a challenging task in clinical routine since the causes of\nbroad variance in SCD manifestation despite the common genetic cause remain\nunclear. Identification of the biomarkers that would predict the severity grade\nis of importance for prognosis and assessment of responsiveness of patients to\ntherapy. Detection of the changes in red blood cell (RBC) density through\nseparation of Percoll density gradient could be such marker as it allows to\nresolve intercellular differences and follow the most damaged dense cells prone\nto destruction and vaso-occlusion. Quantification of the images obtained from\nthe distribution of RBCs in Percoll gradient and interpretation of the obtained\nis an important prerequisite for establishment of this approach. Here, we\npropose a novel approach combining a graph convolutional network, a\nconvolutional neural network, fast Fourier transform, and recursive feature\nelimination to predict the severity of SCD directly from a Percoll image. Two\nimportant but expensive laboratory blood test parameters measurements are used\nfor training the graph convolutional network. To make the model independent\nfrom such tests during prediction, the two parameters are estimated by a neural\nnetwork from the Percoll image directly. On a cohort of 216 subjects, we\nachieve a prediction performance that is only slightly below an approach where\nthe groundtruth laboratory measurements are used. Our proposed method is the\nfirst computational approach for the difficult task of SCD severity prediction.\nThe two-step approach relies solely on inexpensive and simple blood analysis\ntools and can have a significant impact on the patients' survival in\nunderdeveloped countries where access to medical instruments and doctors is\nlimited\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sadafi_A/0/1/0/all/0/1\">Ario Sadafi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makhro_A/0/1/0/all/0/1\">Asya Makhro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Livshits_L/0/1/0/all/0/1\">Leonid Livshits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogdanova_A/0/1/0/all/0/1\">Anna Bogdanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Albarqouni_S/0/1/0/all/0/1\">Shadi Albarqouni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marr_C/0/1/0/all/0/1\">Carsten Marr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges and Solutions in DeepFakes. (arXiv:2109.05397v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05397","description":"<p>Deep learning has been successfully appertained to solve various complex\nproblems in the area of big data analytics to computer vision. A deep\nlearning-powered application recently emerged is Deep Fake. It helps to create\nfake images and videos that human cannot distinguish them from the real ones\nand are recent off-shelf manipulation technique that allows swapping two\nidentities in a single video. Technology is a controversial technology with\nmany wide-reaching issues impacting society. So, to counter this emerging\nproblem, we introduce a dataset of 140k real and fake faces which contain 70k\nreal faces from the Flickr dataset collected by Nvidia, as well as 70k fake\nfaces sampled from 1 million fake faces generated by style GAN. We will train\nour model in the dataset so that our model can identify real or fake faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jatin Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sahil Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge. (arXiv:2109.05409v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05409","description":"<p>This paper gives a detailed description of the pipelines used for the 2nd\nedition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.\nAn overview of the data preprocessing steps applied is provided along with a\nbrief description of the pipelines used, in terms of the architecture and the\nhyperparameters. Our code for this work can be found at:\nhttps://github.com/ivadomed/ms-challenge-2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Macar_U/0/1/0/all/0/1\">Uzay Macar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karthik_E/0/1/0/all/0/1\">Enamundram Naga Karthik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gros_C/0/1/0/all/0/1\">Charley Gros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andr&#xe9;anne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?. (arXiv:2109.05422v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05422","description":"<p>Transformers have sprung up in the field of computer vision. In this work, we\nexplore whether the core self-attention module in Transformer is the key to\nachieving excellent performance in image recognition. To this end, we build an\nattention-free network called sMLPNet based on the existing MLP-based vision\nmodels. Specifically, we replace the MLP module in the token-mixing step with a\nnovel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along\nthe axial directions and the parameters are shared among rows or columns. By\nsparse connection and weight sharing, sMLP module significantly reduces the\nnumber of model parameters and computational complexity, avoiding the common\nover-fitting problem that plagues the performance of MLP-like models. When only\ntrained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1\naccuracy with only 24M parameters, which is much better than most CNNs and\nvision Transformers under the same model size constraint. When scaling up to\n66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the\nstate-of-the-art Swin Transformer. The success of sMLPNet suggests that the\nself-attention mechanism is not necessarily a silver bullet in computer vision.\nCode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuanxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yucheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenxuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prioritized Subnet Sampling for Resource-Adaptive Supernet Training. (arXiv:2109.05432v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05432","description":"<p>A resource-adaptive supernet adjusts its subnets for inference to fit the\ndynamically available resources. In this paper, we propose Prioritized Subnet\nSampling to train a resource-adaptive supernet, termed PSS-Net. We maintain\nmultiple subnet pools, each of which stores the information of substantial\nsubnets with similar resource consumption. Considering a resource constraint,\nsubnets conditioned on this resource constraint are sampled from a pre-defined\nsubnet structure space and high-quality ones will be inserted into the\ncorresponding subnet pool. Then, the sampling will gradually be prone to\nsampling subnets from the subnet pools. Moreover, the one with a better\nperformance metric is assigned with higher priority to train our PSS-Net, if\nsampling is from a subnet pool. At the end of training, our PSS-Net retains the\nbest subnet in each pool to entitle a fast switch of high-quality subnets for\ninference when the available resources vary. Experiments on ImageNet using\nMobileNetV1/V2 show that our PSS-Net can well outperform state-of-the-art\nresource-adaptive supernets. Our project is at\nhttps://github.com/chenbong/PSS-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bohong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05433","description":"<p>Internet search affects people's cognition of the world, so mitigating biases\nin search results and learning fair models is imperative for social good. We\nstudy a unique gender bias in image search in this work: the search images are\noften gender-imbalanced for gender-neutral natural language queries. We\ndiagnose two typical image search models, the specialized model trained on\nin-domain datasets and the generalized representation model pre-trained on\nmassive image and text data across the internet. Both models suffer from severe\ngender bias. Therefore, we introduce two novel debiasing approaches: an\nin-processing fair sampling method to address the gender imbalance issue for\ntraining models, and a post-processing feature clipping method base on mutual\ninformation to debias multimodal representations of pre-trained models.\nExtensive experiments on MS-COCO and Flickr30K benchmarks show that our methods\nsignificantly reduce the gender bias in image search models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR-based Perception. (arXiv:2109.05441v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05441","description":"<p>State-of-the-art methods for driving-scene LiDAR-based perception (including\npoint cloud semantic segmentation, panoptic segmentation and 3D detection,\n\\etc) often project the point clouds to 2D space and then process them via 2D\nconvolution. Although this cooperation shows the competitiveness in the point\ncloud, it inevitably alters and abandons the 3D topology and geometric\nrelations. A natural remedy is to utilize the 3D voxelization and 3D\nconvolution network. However, we found that in the outdoor point cloud, the\nimprovement obtained in this way is quite limited. An important reason is the\nproperty of the outdoor point cloud, namely sparsity and varying density.\nMotivated by this investigation, we propose a new framework for the outdoor\nLiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution\nnetworks are designed to explore the 3D geometric pattern while maintaining\nthese inherent properties. The proposed model acts as a backbone and the\nlearned features from this model can be used for downstream tasks such as point\ncloud semantic and panoptic segmentation or 3D detection. In this paper, we\nbenchmark our model on these three tasks. For semantic segmentation, we\nevaluate the proposed model on several large-scale datasets, \\ie,\nSemanticKITTI, nuScenes and A2D2. Our method achieves the state-of-the-art on\nthe leaderboard of SemanticKITTI (both single-scan and multi-scan challenge),\nand significantly outperforms existing methods on nuScenes and A2D2 dataset.\nFurthermore, the proposed 3D framework also shows strong performance and good\ngeneralization on LiDAR panoptic segmentation and LiDAR 3D detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fangzhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAN3D: Fast 3D Medical Image Segmentation via Compact Context Aggregation. (arXiv:2109.05443v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05443","description":"<p>Direct automatic segmentation of objects from 3D medical imaging, such as\nmagnetic resonance (MR) imaging, is challenging as it often involves accurately\nidentifying a number of individual objects with complex geometries within a\nlarge volume under investigation. To address these challenges, most deep\nlearning approaches typically enhance their learning capability by\nsubstantially increasing the complexity or the number of trainable parameters\nwithin their models. Consequently, these models generally require long\ninference time on standard workstations operating clinical MR systems and are\nrestricted to high-performance computing hardware due to their large memory\nrequirement. Further, to fit 3D dataset through these large models using\nlimited computer memory, trade-off techniques such as patch-wise training are\noften used which sacrifice the fine-scale geometric information from input\nimages which could be clinically significant for diagnostic purposes. To\naddress these challenges, we present a compact convolutional neural network\nwith a shallow memory footprint to efficiently reduce the number of model\nparameters required for state-of-art performance. This is critical for\npractical employment as most clinical environments only have low-end hardware\nwith limited computing power and memory. The proposed network can maintain data\nintegrity by directly processing large full-size 3D input volumes with no\npatches required and significantly reduces the computational time required for\nboth training and inference. We also propose a novel loss function with extra\nshape constraint to improve the accuracy for imbalanced classes in 3D MR\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_B/0/1/0/all/0/1\">Boyeong Woo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Siyu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marques_M/0/1/0/all/0/1\">Matthew Marques</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Engstrom_C/0/1/0/all/0/1\">Craig B. Engstrom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greer_P/0/1/0/all/0/1\">Peter B. Greer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crozier_S/0/1/0/all/0/1\">Stuart Crozier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dowling_J/0/1/0/all/0/1\">Jason A. Dowling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandras_S/0/1/0/all/0/1\">Shekhar S. Chandras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What happens in Face during a facial expression? Using data mining techniques to analyze facial expression motion vectors. (arXiv:2109.05457v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05457","description":"<p>One of the most common problems encountered in human-computer interaction is\nautomatic facial expression recognition. Although it is easy for human observer\nto recognize facial expressions, automatic recognition remains difficult for\nmachines. One of the methods that machines can recognize facial expression is\nanalyzing the changes in face during facial expression presentation. In this\npaper, optical flow algorithm was used to extract deformation or motion vectors\ncreated in the face because of facial expressions. Then, these extracted motion\nvectors are used to be analyzed. Their positions and directions were exploited\nfor automatic facial expression recognition using different data mining\ntechniques. It means that by employing motion vector features used as our data,\nfacial expressions were recognized. Some of the most state-of-the-art\nclassification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL),\nSVM and Discriminant algorithms were used to classify the extracted motion\nvectors. Using 10-fold cross validation, their performances were calculated. To\ncompare their performance more precisely, the test was repeated 50 times.\nMeanwhile, the deformation of face was also analyzed in this research. For\nexample, what exactly happened in each part of face when a person showed fear?\nExperimental results on Extended Cohen-Kanade (CK+) facial expression dataset\ndemonstrated that the best methods were DL, SVM and C5.0, with the accuracy of\n95.3%, 92.8% and 90.2% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roshanzamir_M/0/1/0/all/0/1\">Mohamad Roshanzamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshanzamir_M/0/1/0/all/0/1\">Mahdi Roshanzamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosrave_A/0/1/0/all/0/1\">Abbas Khosrave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Re-parameterization Residual Attention Network For Nonhomogeneous Image Dehazing. (arXiv:2109.05479v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05479","description":"<p>This paper proposes an end-to-end Efficient Re-parameterizationResidual\nAttention Network(ERRA-Net) to directly restore the nonhomogeneous hazy image.\nThe contribution of this paper mainly has the following three aspects: 1) A\nnovel Multi-branch Attention (MA) block. The spatial attention mechanism better\nreconstructs high-frequency features, and the channel attention mechanism\ntreats the features of different channels differently. Multi-branch structure\ndramatically improves the representation ability of the model and can be\nchanged into a single path structure after re-parameterization to speed up the\nprocess of inference. Local Residual Connection allows the low-frequency\ninformation in the nonhomogeneous area to pass through the block without\nprocessing so that the block can focus on detailed features. 2) A lightweight\nnetwork structure. We use cascaded MA blocks to extract high-frequency features\nstep by step, and the Multi-layer attention fusion tail combines the shallow\nand deep features of the model to get the residual of the clean image finally.\n3)We propose two novel loss functions to help reconstruct the hazy image\nColorAttenuation loss and Laplace Pyramid loss. ERRA-Net has an impressive\nspeed, processing 1200x1600 HD quality images with an average runtime of 166.11\nfps. Extensive evaluations demonstrate that ERSANet performs favorably against\nthe SOTA approaches on the real-world hazy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1\">ErKang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">XinRui Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Anatomical Landmark Detection using Regularized Transfer Learning with Application to Fetal Alcohol Syndrome Recognition. (arXiv:2109.05485v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05485","description":"<p>Fetal alcohol syndrome (FAS) caused by prenatal alcohol exposure can result\nin a series of cranio-facial anomalies, and behavioral and neurocognitive\nproblems. Current diagnosis of FAS is typically done by identifying a set of\nfacial characteristics, which are often obtained by manual examination.\nAnatomical landmark detection, which provides rich geometric information, is\nimportant to detect the presence of FAS associated facial anomalies. This\nimaging application is characterized by large variations in data appearance and\nlimited availability of labeled data. Current deep learning-based heatmap\nregression methods designed for facial landmark detection in natural images\nassume availability of large datasets and are therefore not wellsuited for this\napplication. To address this restriction, we develop a new regularized transfer\nlearning approach that exploits the knowledge of a network learned on large\nfacial recognition datasets. In contrast to standard transfer learning which\nfocuses on adjusting the pre-trained weights, the proposed learning approach\nregularizes the model behavior. It explicitly reuses the rich visual semantics\nof a domain-similar source model on the target task data as an additional\nsupervisory signal for regularizing landmark detection optimization.\nSpecifically, we develop four regularization constraints for the proposed\ntransfer learning, including constraining the feature outputs from\nclassification and intermediate layers, as well as matching activation\nattention maps in both spatial and channel levels. Experimental evaluation on a\ncollected clinical imaging dataset demonstrate that the proposed approach can\neffectively improve model generalizability under limited training samples, and\nis advantageous to other approaches in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zeyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jianbo Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suttie_M/0/1/0/all/0/1\">Michael Suttie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05488","description":"<p>Estimating the articulated 3D hand-object pose from a single RGB image is a\nhighly ambiguous and challenging problem requiring large-scale datasets that\ncontain diverse hand poses, object poses, and camera viewpoints. Most\nreal-world datasets lack this diversity. In contrast, synthetic datasets can\neasily ensure vast diversity, but learning from them is inefficient and suffers\nfrom heavy training consumption. To address the above issues, we propose\nArtiBoost, a lightweight online data enrichment method that boosts articulated\nhand-object pose estimation from the data perspective. ArtiBoost is employed\nalong with a real-world source dataset. During training, ArtiBoost\nalternatively performs data exploration and synthesis. ArtiBoost can cover\nvarious hand-object poses and camera viewpoints based on a Compositional\nhand-object Configuration and Viewpoint space (CCV-space) and can adaptively\nenrich the current hard-discernable samples by a mining strategy. We apply\nArtiBoost on a simple learning baseline network and demonstrate the performance\nboost on several hand-object benchmarks. As an illustrative example, with\nArtiBoost, even a simple baseline network can outperform the previous\nstart-of-the-art based on Transformer on the HO3D dataset. Our code is\navailable at https://github.com/MVIG-SJTU/ArtiBoost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEA-Net: Layer-wise External Attention Network for Efficient Color Anomaly Detection. (arXiv:2109.05493v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05493","description":"<p>The utilization of prior knowledge about anomalies is an essential issue for\nanomaly detections. Recently, the visual attention mechanism has become a\npromising way to improve the performance of CNNs for some computer vision\ntasks. In this paper, we propose a novel model called Layer-wise External\nAttention Network (LEA-Net) for efficient image anomaly detection. The core\nidea relies on the integration of unsupervised and supervised anomaly detectors\nvia the visual attention mechanism. Our strategy is as follows: (i) Prior\nknowledge about anomalies is represented as the anomaly map generated by\nunsupervised learning of normal instances, (ii) The anomaly map is translated\nto an attention map by the external network, (iii) The attention map is then\nincorporated into intermediate layers of the anomaly detection network.\nNotably, this layer-wise external attention can be applied to any CNN model in\nan end-to-end training manner. For a pilot study, we validate LEA-Net on color\nanomaly detection tasks. Through extensive experiments on PlantVillage, MVTec\nAD, and Cloud datasets, we demonstrate that the proposed layer-wise visual\nattention mechanism consistently boosts anomaly detection performances of an\nexisting CNN model, even on imbalanced datasets. Moreover, we show that our\nattention mechanism successfully boosts the performance of several CNN models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katafuchi_R/0/1/0/all/0/1\">Ryoya Katafuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokunaga_T/0/1/0/all/0/1\">Terumasa Tokunaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Complex Constrained Total Variation Image Denoising Algorithm with Application to Phase Retrieval. (arXiv:2109.05496v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05496","description":"<p>This paper considers the constrained total variation (TV) denoising problem\nfor complex-valued images. We extend the definition of TV seminorms for\nreal-valued images to dealing with complex-valued ones. In particular, we\nintroduce two types of complex TV in both isotropic and anisotropic forms. To\nsolve the constrained denoising problem, we adopt a dual approach and derive an\naccelerated gradient projection algorithm. We further generalize the proposed\ndenoising algorithm as a key building block of the proximal gradient scheme to\nsolve a vast class of complex constrained optimization problems with TV\nregularizers. As an example, we apply the proposed algorithmic framework to\nphase retrieval. We combine the complex TV regularizer with the conventional\nprojection-based method within the constraint complex TV model. Initial results\nfrom both simulated and optical experiments demonstrate the validity of the\nconstrained TV model in extracting sparsity priors within complex-valued\nimages, while also utilizing physically tractable constraints that help speed\nup convergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhui Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangcai Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain. (arXiv:2109.05507v1 [cs.CR])","link":"http://arxiv.org/abs/2109.05507","description":"<p>Deep Neural Networks (DNNs) have been utilized in various applications\nranging from image classification and facial recognition to medical imagery\nanalysis and real-time object detection. As our models become more\nsophisticated and complex, the computational cost of training such models\nbecomes a burden for small companies and individuals; for this reason,\noutsourcing the training process has been the go-to option for such users.\nUnfortunately, outsourcing the training process comes at the cost of\nvulnerability to backdoor attacks. These attacks aim at establishing hidden\nbackdoors in the DNN such that the model performs well on benign samples but\noutputs a particular target label when a trigger is applied to the input.\nCurrent backdoor attacks rely on generating triggers in the image/pixel domain;\nhowever, as we show in this paper, it is not the only domain to exploit and one\nshould always \"check the other doors\". In this work, we propose a complete\npipeline for generating a dynamic, efficient, and invisible backdoor attack in\nthe frequency domain. We show the advantages of utilizing the frequency domain\nfor establishing undetectable and powerful backdoor attacks through extensive\nexperiments on various datasets and network architectures. The backdoored\nmodels are shown to break various state-of-the-art defences. We also show two\npossible defences that succeed against frequency-based backdoor attacks and\npossible ways for the attacker to bypass them. We conclude the work with some\nremarks regarding a network's learning capacity and the capability of embedding\na backdoor attack in the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1\">Hasan Abed Al Kader Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Monocular Visual Odometry for Flying Robots on Planetary Missions. (arXiv:2109.05509v1 [cs.RO])","link":"http://arxiv.org/abs/2109.05509","description":"<p>In the future, extraterrestrial expeditions will not only be conducted by\nrovers but also by flying robots. The technical demonstration drone Ingenuity,\nthat just landed on Mars, will mark the beginning of a new era of exploration\nunhindered by terrain traversability. Robust self-localization is crucial for\nthat. Cameras that are lightweight, cheap and information-rich sensors are\nalready used to estimate the ego-motion of vehicles. However, methods proven to\nwork in man-made environments cannot simply be deployed on other planets. The\nhighly repetitive textures present in the wastelands of Mars pose a huge\nchallenge to descriptor matching based approaches.\n</p>\n<p>In this paper, we present an advanced robust monocular odometry algorithm\nthat uses efficient optical flow tracking to obtain feature correspondences\nbetween images and a refined keyframe selection criterion. In contrast to most\nother approaches, our framework can also handle rotation-only motions that are\nparticularly challenging for monocular odometry systems. Furthermore, we\npresent a novel approach to estimate the current risk of scale drift based on a\nprincipal component analysis of the relative translation information matrix.\nThis way we obtain an implicit measure of uncertainty. We evaluate the validity\nof our approach on all sequences of a challenging real-world dataset captured\nin a Mars-like environment and show that it outperforms state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wudenka_M/0/1/0/all/0/1\">Martin Wudenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Marcus G. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedler_A/0/1/0/all/0/1\">Armin Wedler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturzl_W/0/1/0/all/0/1\">Wolfgang St&#xfc;rzl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05523","description":"<p>Existing research for image text retrieval mainly relies on sentence-level\nsupervision to distinguish matched and mismatched sentences for a query image.\nHowever, semantic mismatch between an image and sentences usually happens in\nfiner grain, i.e., phrase level. In this paper, we explore to introduce\nadditional phrase-level supervision for the better identification of mismatched\nunits in the text. In practice, multi-grained semantic labels are automatically\nconstructed for a query image in both sentence-level and phrase-level. We\nconstruct text scene graphs for the matched sentences and extract entities and\ntriples as the phrase-level labels. In order to integrate both supervision of\nsentence-level and phrase-level, we propose Semantic Structure Aware Multimodal\nTransformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,\nwe utilize different kinds of attention mechanisms to enforce interactions of\nmulti-grain semantic units in both sides of vision and language. For the\ntraining, we propose multi-scale matching losses from both global and local\nperspectives, and penalize mismatched phrases. Experimental results on MS-COCO\nand Flickr30K show the effectiveness of our approach compared to some\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Haijun Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Decidability-Based Loss Function. (arXiv:2109.05524v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05524","description":"<p>Nowadays, deep learning is the standard approach for a wide range of\nproblems, including biometrics, such as face recognition and speech\nrecognition, etc. Biometric problems often use deep learning models to extract\nfeatures from images, also known as embeddings. Moreover, the loss function\nused during training strongly influences the quality of the generated\nembeddings. In this work, a loss function based on the decidability index is\nproposed to improve the quality of embeddings for the verification routine. Our\nproposal, the D-loss, avoids some Triplet-based loss disadvantages such as the\nuse of hard samples and tricky parameter tuning, which can lead to slow\nconvergence. The proposed approach is compared against the Softmax\n(cross-entropy), Triplets Soft-Hard, and the Multi Similarity losses in four\ndifferent benchmarks: MNIST, Fashion-MNIST, CIFAR10 and CASIA-IrisV4. The\nachieved results show the efficacy of the proposal when compared to other\npopular metrics in the literature. The D-loss computation, besides being\nsimple, non-parametric and easy to implement, favors both the inter-class and\nintra-class scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1\">Pedro Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Gladston Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_V/0/1/0/all/0/1\">Vander Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Rodrigo Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luz_E/0/1/0/all/0/1\">Eduardo Luz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Unsupervised Deep-Learning Method for Fingerprint Classification: the CCAE Network and the Hybrid Clustering Strategy. (arXiv:2109.05526v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05526","description":"<p>The fingerprint classification is an important and effective method to\nquicken the process and improve the accuracy in the fingerprint matching\nprocess. Conventional supervised methods need a large amount of pre-labeled\ndata and thus consume immense human resources. In this paper, we propose a new\nand efficient unsupervised deep learning method that can extract fingerprint\nfeatures and classify fingerprint patterns automatically. In this approach, a\nnew model named constraint convolutional auto-encoder (CCAE) is used to extract\nfingerprint features and a hybrid clustering strategy is applied to obtain the\nfinal clusters. A set of experiments in the NIST-DB4 dataset shows that the\nproposed unsupervised method exhibits the efficient performance on fingerprint\nclassification. For example, the CCAE achieves an accuracy of 97.3% on only\n1000 unlabeled fingerprints in the NIST-DB4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yue-Jie Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zai-Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian-Hu/0/1/0/all/0/1\">Jian-Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao-Shen/0/1/0/all/0/1\">Yao-Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chi-Chun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSSL: Deep Surroundings-person Separation Learning for Text-based Person Retrieval. (arXiv:2109.05534v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05534","description":"<p>Many previous methods on text-based person retrieval tasks are devoted to\nlearning a latent common space mapping, with the purpose of extracting\nmodality-invariant features from both visual and textual modality.\nNevertheless, due to the complexity of high-dimensional data, the unconstrained\nmapping paradigms are not able to properly catch discriminative clues about the\ncorresponding person while drop the misaligned information. Intuitively, the\ninformation contained in visual data can be divided into person information\n(PI) and surroundings information (SI), which are mutually exclusive from each\nother. To this end, we propose a novel Deep Surroundings-person Separation\nLearning (DSSL) model in this paper to effectively extract and match person\ninformation, and hence achieve a superior retrieval accuracy. A\nsurroundings-person separation and fusion mechanism plays the key role to\nrealize an accurate and effective surroundings-person separation under a\nmutually exclusion constraint. In order to adequately utilize multi-modal and\nmulti-granular information for a higher retrieval accuracy, five diverse\nalignment paradigms are adopted. Extensive experiments are carried out to\nevaluate the proposed DSSL on CUHK-PEDES, which is currently the only\naccessible dataset for text-base person retrieval task. DSSL achieves the\nstate-of-the-art performance on CUHK-PEDES. To properly evaluate our proposed\nDSSL in the real scenarios, a Real Scenarios Text-based Person Reidentification\n(RSTPReid) dataset is constructed to benefit future research on text-based\nperson retrieval, which will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Aichun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xili Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fangqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v1 [cs.NE])","link":"http://arxiv.org/abs/2109.05539","description":"<p>Recent studies have shown that convolutional neural networks (CNNs) are not\nthe only feasible solution for image classification. Furthermore, weight\nsharing and backpropagation used in CNNs do not correspond to the mechanisms\npresent in the primate visual system. To propose a more biologically plausible\nsolution, we designed a locally connected spiking neural network (SNN) trained\nusing spike-timing-dependent plasticity (STDP) and its reward-modulated variant\n(R-STDP) learning rules. The use of spiking neurons and local connections along\nwith reinforcement learning (RL) led us to the nomenclature BioLCNet for our\nproposed architecture. Our network consists of a rate-coded input layer\nfollowed by a locally connected hidden layer and a decoding output layer. A\nspike population-based voting scheme is adopted for decoding in the output\nlayer. We used the MNIST dataset to obtain image classification accuracy and to\nassess the robustness of our rewarding system to varying target responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_E/0/1/0/all/0/1\">Erfan Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1\">Saeed Reza Kheradpisheh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptive Learning via Synthetic Data for Person Re-identification. (arXiv:2109.05542v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05542","description":"<p>Person re-identification (re-ID) has gained more and more attention due to\nits widespread applications in intelligent video surveillance. Unfortunately,\nthe mainstream deep learning methods still need a large quantity of labeled\ndata to train models, and annotating data is an expensive work in real-world\nscenarios. In addition, due to domain gaps between different datasets, the\nperformance is dramatically decreased when re-ID models pre-trained on\nlabel-rich datasets (source domain) are directly applied to other unlabeled\ndatasets (target domain). In this paper, we attempt to remedy these problems\nfrom two aspects, namely data and methodology. Firstly, we develop a data\ncollector to automatically generate synthetic re-ID samples in a computer game,\nand construct a data labeler to simultaneously annotate them, which free humans\nfrom heavy data collections and annotations. Based on them, we build two\nsynthetic person re-ID datasets with different scales, \"GSPR\" and \"mini-GSPR\"\ndatasets. Secondly, we propose a synthesis-based multi-domain collaborative\nrefinement (SMCR) network, which contains a synthetic pretraining module and\ntwo collaborative-refinement modules to implement sufficient learning for the\nvaluable knowledge from multiple domains. Extensive experiments show that our\nproposed framework obtains significant performance improvements over the\nstate-of-the-art methods on multiple unsupervised domain adaptation tasks of\nperson re-ID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Sikai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05565","description":"<p>This paper addresses the deep face recognition problem under an open-set\nprotocol, where ideal face features are expected to have smaller maximal\nintra-class distance than minimal inter-class distance under a suitably chosen\nmetric space. To this end, hyperspherical face recognition, as a promising line\nof research, has attracted increasing attention and gradually become a major\nfocus in face recognition research. As one of the earliest works in\nhyperspherical face recognition, SphereFace explicitly proposed to learn face\nembeddings with large inter-class angular margin. However, SphereFace still\nsuffers from severe training instability which limits its application in\npractice. In order to address this problem, we introduce a unified framework to\nunderstand large angular margin in hyperspherical face recognition. Under this\nframework, we extend the study of SphereFace and propose an improved variant\nwith substantially better training stability -- SphereFace-R. Specifically, we\npropose two novel ways to implement the multiplicative margin, and study\nSphereFace-R under three different feature normalization schemes (no feature\nnormalization, hard feature normalization and soft feature normalization). We\nalso propose an implementation strategy -- \"characteristic gradient detachment\"\n-- to stabilize training. Extensive experiments on SphereFace-R show that it is\nconsistently better than or competitive with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds. (arXiv:2109.05566v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05566","description":"<p>3D scene understanding from point clouds plays a vital role for various\nrobotic applications. Unfortunately, current state-of-the-art methods use\nseparate neural networks for different tasks like object detection or room\nlayout estimation. Such a scheme has two limitations: 1) Storing and running\nseveral networks for different tasks are expensive for typical robotic\nplatforms. 2) The intrinsic structure of separate outputs are ignored and\npotentially violated. To this end, we propose the first transformer\narchitecture that predicts 3D objects and layouts simultaneously, using point\ncloud inputs. Unlike existing methods that either estimate layout keypoints or\nedges, we directly parameterize room layout as a set of quads. As such, the\nproposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the\nnovel quad representation, we propose a tailored physical constraint loss\nfunction that discourages object-layout interference. The quantitative and\nqualitative evaluations on the public benchmark ScanNet show that the proposed\nPQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a\nquasi-real-time (8.91 FPS) rate without efficiency-oriented optimization.\nMoreover, the new physical constraint loss can improve strong baselines, and\nthe F1-score of the room layout is significantly promoted from 37.9% to 57.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoxue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya-Qin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MovieCuts: A New Dataset and Benchmark for Cut Type Recognition. (arXiv:2109.05569v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05569","description":"<p>Understanding movies and their structural patterns is a crucial task to\ndecode the craft of video editing. While previous works have developed tools\nfor general analysis such as detecting characters or recognizing cinematography\nproperties at the shot level, less effort has been devoted to understanding the\nmost basic video edit, the Cut. This paper introduces the cut type recognition\ntask, which requires modeling of multi-modal information. To ignite research in\nthe new task, we construct a large-scale dataset called MovieCuts, which\ncontains more than 170K videoclips labeled among ten cut types. We benchmark a\nseries of audio-visual approaches, including some that deal with the problem's\nmulti-modal and multi-label nature. Our best model achieves 45.7% mAP, which\nsuggests that the task is challenging and that attaining highly accurate cut\ntype recognition is an open research problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation. (arXiv:2109.05580v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05580","description":"<p>We present a joint graph convolution-image convolution neural network as our\nsubmission to the Brain Tumor Segmentation (BraTS) 2021 challenge. We model\neach brain as a graph composed of distinct image regions, which is initially\nsegmented by a graph neural network (GNN). Subsequently, the tumorous volume\nidentified by the GNN is further refined by a simple (voxel) convolutional\nneural network (CNN), which produces the final segmentation. This approach\ncaptures both global brain feature interactions via the graphical\nrepresentation and local image details through the use of convolutional\nfilters. We find that the GNN component by itself can effectively identify and\nsegment the brain tumors. The addition of the CNN further improves the median\nperformance of the model by 2 percent across all metrics evaluated. On the\nvalidation set, our joint GNN-CNN model achieves mean Dice scores of 0.89,\n0.81, 0.73 and mean Hausdorff distances (95th percentile) of 6.8, 12.6, 28.2mm\non the whole tumor, core tumor, and enhancing tumor, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saueressig_C/0/1/0/all/0/1\">Camillo Saueressig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berkley_A/0/1/0/all/0/1\">Adam Berkley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munbodh_R/0/1/0/all/0/1\">Reshma Munbodh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_R/0/1/0/all/0/1\">Ritambhara Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-Net Convolutional Network for Recognition of Vessels and Materials in Chemistry Lab. (arXiv:2109.05585v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05585","description":"<p>Convolutional networks have been widely applied for computer vision system.\nEncouraged by these results, a U-Net convolutional network was applied to\nrecognition of vessels and materials in chemistry lab using the recent\nVector-LabPics dataset, which contains 2187 images of materials within mostly\ntransparent vessels in a chemistry lab and other general settings, labeled with\n13 classes. By optimizing hyperparameters including learning rates and learning\nrate decays, 87% accuracy in vessel recognition was achieved. In the case of\nrelatively small training and test sets (relatively rare materials states, the\nnumber of training set samples less than 500 and the number of test set samples\nless than 100), a comprehensive improvement over 18% in IoU and 19% in accuracy\nfor the best model were achieved. Further improvements may be achievable by\nincorporating improved convolutional network structure into our models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1\">Zhihao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_D/0/1/0/all/0/1\">Di Bo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiresolution Deep Implicit Functions for 3D Shape Representation. (arXiv:2109.05591v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05591","description":"<p>We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical\nrepresentation that can recover fine geometry detail, while being able to\nperform global operations such as shape completion. Our model represents a\ncomplex 3D shape with a hierarchy of latent grids, which can be decoded into\ndifferent levels of detail and also achieve better accuracy. For shape\ncompletion, we propose latent grid dropout to simulate partial data in the\nlatent space and therefore defer the completing functionality to the decoder\nside. This along with our multires design significantly improves the shape\ncompletion quality under decoder-only latent optimization. To the best of our\nknowledge, MDIF is the first deep implicit function model that can at the same\ntime (1) represent different levels of detail and allow progressive decoding;\n(2) support both encoder-decoder inference and decoder-only latent\noptimization, and fulfill multiple applications; (3) perform detailed\ndecoder-only shape completion. Experiments demonstrate its superior performance\nagainst prior art in various 3D reconstruction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genova_K/0/1/0/all/0/1\">Kyle Genova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1\">Sean Fanello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haene_C/0/1/0/all/0/1\">Christian Haene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruofei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskin_C/0/1/0/all/0/1\">Cem Keskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Danhang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSGDD-cGAN: Multi-Scale Gradients Dual Discriminator Conditional Generative Adversarial Network. (arXiv:2109.05614v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05614","description":"<p>Conditional Generative Adversarial Networks (cGANs) have been used in many\nimage processing tasks. However, they still have serious problems maintaining\nthe balance between conditioning the output on the input and creating the\noutput with the desired distribution based on the corresponding ground truth.\nThe traditional cGANs, similar to most conventional GANs, suffer from vanishing\ngradients, which backpropagate from the discriminator to the generator.\nMoreover, the traditional cGANs are sensitive to architectural changes due to\npreviously mentioned gradient problems. Therefore, balancing the architecture\nof the cGANs is almost impossible. Recently MSG-GAN has been proposed to\nstabilize the performance of the GANs by applying multiple connections between\nthe generator and discriminator. In this work, we propose a method called\nMSGDD-cGAN, which first stabilizes the performance of the cGANs using\nmulti-connections gradients flow. Secondly, the proposed network architecture\nbalances the correlation of the output to input and the fitness of the output\non the target distribution. This balance is generated by using the proposed\ndual discrimination procedure. We tested our model by segmentation of fetal\nultrasound images. Our model shows a 3.18% increase in the F1 score comparing\nto the pix2pix version of cGANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naderi_M/0/1/0/all/0/1\">Mohammadreza Naderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabizadeh_Z/0/1/0/all/0/1\">Zahra Nabizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_N/0/1/0/all/0/1\">Nader Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirani_S/0/1/0/all/0/1\">Shahram Shirani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differential Diagnosis of Frontotemporal Dementia and Alzheimer's Disease using Generative Adversarial Network. (arXiv:2109.05627v1 [eess.IV])","link":"http://arxiv.org/abs/2109.05627","description":"<p>Frontotemporal dementia and Alzheimer's disease are two common forms of\ndementia and are easily misdiagnosed as each other due to their similar pattern\nof clinical symptoms. Differentiating between the two dementia types is crucial\nfor determining disease-specific intervention and treatment. Recent development\nof Deep-learning-based approaches in the field of medical image computing are\ndelivering some of the best performance for many binary classification tasks,\nalthough its application in differential diagnosis, such as neuroimage-based\ndifferentiation for multiple types of dementia, has not been explored. In this\nstudy, a novel framework was proposed by using the Generative Adversarial\nNetwork technique to distinguish FTD, AD and normal control subjects, using\nvolumetric features extracted at coarse-to-fine structural scales from Magnetic\nResonance Imaging scans. Experiments of 10-folds cross-validation on 1,954\nimages achieved high accuracy. With the proposed framework, we have\ndemonstrated that the combination of multi-scale structural features and\nsynthetic data augmentation based on generative adversarial network can improve\nthe performance of challenging tasks such as differentiating Dementia\nsub-types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Da_M/0/1/0/all/0/1\">Ma Da</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Donghuan_L/0/1/0/all/0/1\">Lu Donghuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karteek_P/0/1/0/all/0/1\">Popuri Karteek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faisal_B/0/1/0/all/0/1\">Beg Mirza Faisal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Datasets of 3D Garments with Sewing Patterns. (arXiv:2109.05633v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05633","description":"<p>Garments are ubiquitous in both real and many of the virtual worlds. They are\nhighly deformable objects, exhibit an immense variety of designs and shapes,\nand yet, most garments are created from a set of regularly shaped flat pieces.\nExploration of garment structure presents a peculiar case for an object\nstructure estimation task and might prove useful for downstream tasks of neural\n3D garment modeling and reconstruction by providing strong prior on garment\nshapes. To facilitate research in these directions, we propose a method for\ngenerating large synthetic datasets of 3D garment designs and their sewing\npatterns. Our method consists of a flexible description structure for\nspecifying parametric sewing pattern templates and the automatic generation\npipeline to produce garment 3D models with little-to-none manual intervention.\nTo add realism, the pipeline additionally creates corrupted versions of the\nfinal meshes that imitate artifacts of 3D scanning.\n</p>\n<p>With this pipeline, we created the first large-scale synthetic dataset of 3D\ngarment models with their sewing patterns. The dataset contains more than 20000\ngarment design variations produced from 19 different base types. Seven of these\ngarment types are specifically designed to target evaluation of the\ngeneralization across garment sewing pattern topologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korosteleva_M/0/1/0/all/0/1\">Maria Korosteleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v17 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1906.00460","description":"<p>Problems of interpolation, classification, and clustering are considered. In\nthe tenets of Radon--Nikodym approach $\\langle f(\\mathbf{x})\\psi^2 \\rangle /\n\\langle\\psi^2\\rangle$, where the $\\psi(\\mathbf{x})$ is a linear function on\ninput attributes, all the answers are obtained from a generalized eigenproblem\n$|f|\\psi^{[i]}\\rangle = \\lambda^{[i]} |\\psi^{[i]}\\rangle$. The solution to the\ninterpolation problem is a regular Radon-Nikodym derivative. The solution to\nthe classification problem requires prior and posterior probabilities that are\nobtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian\napproach new observations change only outcome probabilities, in the\nRadon-Nikodym approach not only outcome probabilities but also the probability\nspace $|\\psi^{[i]}\\rangle$ change with new observations. This is a remarkable\nfeature of the approach: both the probabilities and the probability space are\nconstructed from the data. The Lebesgue quadrature technique can be also\napplied to the optimal clustering problem. The problem is solved by\nconstructing a Gaussian quadrature on the Lebesgue measure. A distinguishing\nfeature of the Radon-Nikodym approach is the knowledge of the invariant group:\nall the answers are invariant relatively any non-degenerated linear transform\nof input vector $\\mathbf{x}$ components. A software product implementing the\nalgorithms of interpolation, classification, and optimal clustering is\navailable from the authors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malyshkin_V/0/1/0/all/0/1\">Vladislav Gennadievich Malyshkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction. (arXiv:1908.03918v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1908.03918","description":"<p>Dynamical models estimate and predict the temporal evolution of physical\nsystems. State Space Models (SSMs) in particular represent the system dynamics\nwith many desirable properties, such as being able to model uncertainty in both\nthe model and measurements, and optimal (in the Bayesian sense) recursive\nformulations e.g. the Kalman Filter. However, they require significant domain\nknowledge to derive the parametric form and considerable hand-tuning to\ncorrectly set all the parameters. Data driven techniques e.g. Recurrent Neural\nNetworks have emerged as compelling alternatives to SSMs with wide success\nacross a number of challenging tasks, in part due to their ability to extract\nrelevant features from rich inputs. They however lack interpretability and\nrobustness to unseen conditions. In this work, we present DynaNet, a hybrid\ndeep learning and time-varying state-space model which can be trained\nend-to-end. Our neural Kalman dynamical model allows us to exploit the relative\nmerits of each approach. We demonstrate state-of-the-art estimation and\nprediction on a number of physically challenging tasks, including visual\nodometry, sensor fusion for visual-inertial navigation and pendulum control. In\naddition we show how DynaNet can indicate failures through investigation of\nproperties such as the rate of innovation (Kalman Gain).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automatic Threat Detection: A Survey of Advances of Deep Learning within X-ray Security Imaging. (arXiv:2001.01293v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.01293","description":"<p>X-ray security screening is widely used to maintain aviation/transport\nsecurity, and its significance poses a particular interest in automated\nscreening systems. This paper aims to review computerised X-ray security\nimaging algorithms by taxonomising the field into conventional machine learning\nand contemporary deep learning applications. The first part briefly discusses\nthe classical machine learning approaches utilised within X-ray security\nimaging, while the latter part thoroughly investigates the use of modern deep\nlearning algorithms. The proposed taxonomy sub-categorises the use of deep\nlearning approaches into supervised, semi-supervised and unsupervised learning,\nwith a particular focus on object classification, detection, segmentation and\nanomaly detection tasks. The paper further explores well-established X-ray\ndatasets and provides a performance benchmark. Based on the current and future\ntrends in deep learning, the paper finally presents a discussion and future\ndirections for X-ray security imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby Breckon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Progress of Deep Learning for Visual Relational Concepts. (arXiv:2001.10857v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.10857","description":"<p>Convolutional Neural Networks (CNNs) have become the state of the art method\nfor image classification in the last ten years. Despite the fact that they\nachieve superhuman classification accuracy on many popular datasets, they often\nperform much worse on more abstract image classification tasks. We will show\nthat these difficult tasks are linked to relational concepts from cognitive\npsychology and that despite progress over the last few years, such relational\nreasoning tasks still remain difficult for current neural network\narchitectures.\n</p>\n<p>We will review deep learning research that is linked to relational concept\nlearning, even if it was not originally presented from this angle. Reviewing\nthe current literature, we will argue that some form of attention will be an\nimportant component of future systems to solve relational tasks.\n</p>\n<p>In addition, we will point out the shortcomings of currently used datasets,\nand we will recommend steps to make future datasets more relevant for testing\nsystems on relational reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stabinger_S/0/1/0/all/0/1\">Sebastian Stabinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_P/0/1/0/all/0/1\">Peer David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piater_J/0/1/0/all/0/1\">Justus Piater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Sanchez_A/0/1/0/all/0/1\">Antonio Rodr&#xed;guez-S&#xe1;nchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel-in-Pixel Net: Towards Efficient Facial Landmark Detection in the Wild. (arXiv:2003.03771v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.03771","description":"<p>Recently, heatmap regression models have become popular due to their superior\nperformance in locating facial landmarks. However, three major problems still\nexist among these models: (1) they are computationally expensive; (2) they\nusually lack explicit constraints on global shapes; (3) domain gaps are\ncommonly present. To address these problems, we propose Pixel-in-Pixel Net\n(PIPNet) for facial landmark detection. The proposed model is equipped with a\nnovel detection head based on heatmap regression, which conducts score and\noffset predictions simultaneously on low-resolution feature maps. By doing so,\nrepeated upsampling layers are no longer necessary, enabling the inference time\nto be largely reduced without sacrificing model accuracy. Besides, a simple but\neffective neighbor regression module is proposed to enforce local constraints\nby fusing predictions from neighboring landmarks, which enhances the robustness\nof the new detection head. To further improve the cross-domain generalization\ncapability of PIPNet, we propose self-training with curriculum. This training\nstrategy is able to mine more reliable pseudo-labels from unlabeled data across\ndomains by starting with an easier task, then gradually increasing the\ndifficulty to provide more precise labels. Extensive experiments demonstrate\nthe superiority of PIPNet, which obtains state-of-the-art results on three out\nof six popular benchmarks under the supervised setting. The results on two\ncross-domain test sets are also consistently improved compared to the\nbaselines. Notably, our lightweight version of PIPNet runs at 35.7 FPS and 200\nFPS on CPU and GPU, respectively, while still maintaining a competitive\naccuracy to state-of-the-art methods. The code of PIPNet is available at\nhttps://github.com/jhb86253817/PIPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection in Medical Imaging with Deep Perceptual Autoencoders. (arXiv:2006.13265v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.13265","description":"<p>Anomaly detection is the problem of recognizing abnormal inputs based on the\nseen examples of normal data. Despite recent advances of deep learning in\nrecognizing image anomalies, these methods still prove incapable of handling\ncomplex medical images, such as barely visible abnormalities in chest X-rays\nand metastases in lymph nodes. To address this problem, we introduce a new\npowerful method of image anomaly detection. It relies on the classical\nautoencoder approach with a re-designed training pipeline to handle\nhigh-resolution, complex images and a robust way of computing an image\nabnormality score. We revisit the very problem statement of fully unsupervised\nanomaly detection, where no abnormal examples at all are provided during the\nmodel setup. We propose to relax this unrealistic assumption by using a very\nsmall number of anomalies of confined variability merely to initiate the search\nof hyperparameters of the model. We evaluate our solution on natural image\ndatasets with a known benchmark, as well as on two medical datasets containing\nradiology and digital pathology images. The proposed approach suggests a new\nstrong baseline for image anomaly detection and outperforms state-of-the-art\napproaches in complex medical image analysis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shvetsova_N/0/1/0/all/0/1\">Nina Shvetsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakker_B/0/1/0/all/0/1\">Bart Bakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedulova_I/0/1/0/all/0/1\">Irina Fedulova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_H/0/1/0/all/0/1\">Heinrich Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stabilizing Deep Tomographic Reconstruction. (arXiv:2008.01846v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.01846","description":"<p>Tomographic image reconstruction with deep learning is an emerging field, but\na recent landmark study reveals that several deep reconstruction networks are\nunstable for computed tomography (CT) and magnetic resonance imaging (MRI).\nSpecifically, three kinds of instabilities were reported: (1) strong image\nartefacts from tiny perturbations, (2) small features missing in a deeply\nreconstructed image, and (3) decreased imaging performance with increased input\ndata. On the other hand, compressed sensing (CS) inspired reconstruction\nmethods do not suffer from these instabilities because of their built-in kernel\nawareness. For deep reconstruction to realize its full potential and become a\nmainstream approach for tomographic imaging, it is thus critically important to\nmeet this challenge by stabilizing deep reconstruction networks. Here we\npropose an Analytic Compressed Iterative Deep (ACID) framework to address this\nchallenge. ACID synergizes a deep reconstruction network trained on big data,\nkernel awareness from CS-inspired processing, and iterative refinement to\nminimize the data residual relative to real measurement. Our study demonstrates\nthat the deep reconstruction using ACID is accurate and stable, and sheds light\non the converging mechanism of the ACID iteration under a Bounded Relative\nError Norm (BREN) condition. In particular, the study shows that ACID-based\nreconstruction is resilient against adversarial attacks, superior to classic\nsparsity-regularized reconstruction alone, and eliminates the three kinds of\ninstabilities. We anticipate that this integrative data-driven approach will\nhelp promote development and translation of deep tomographic image\nreconstruction networks into clinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dianlin Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cong_W/0/1/0/all/0/1\">Wenxiang Cong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shaoyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vardhanabhuti_V/0/1/0/all/0/1\">Varut Vardhanabhuti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on 3D Pelvic CT Images. (arXiv:2009.09571v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.09571","description":"<p>Automated segmentation can assist radiotherapy treatment planning by saving\nmanual contouring efforts and reducing intra-observer and inter-observer\nvariations. The recent development of deep learning approaches has revoluted\nmedical data processing, including semantic segmentation, by dramatically\nimproving performance. However, training effective deep learning models usually\nrequire a large amount of high-quality labeled data, which are often costly to\ncollect. We developed a novel semi-supervised adversarial deep learning\napproach for 3D pelvic CT image semantic segmentation. Unlike supervised deep\nlearning methods, the new approach can utilize both annotated and un-annotated\ndata for training. It generates un-annotated synthetic data by a data\naugmentation scheme using generative adversarial networks (GANs). We applied\nthe new approach to segmenting multiple organs in male pelvic CT images, where\nCT images without annotations and GAN-synthesized un-annotated images were used\nin semi-supervised learning. Experimental results, evaluated by three metrics\n(Dice similarity coefficient, average Hausdorff distance, and average surface\nHausdorff distance), showed that the new method achieved either comparable\nperformance with substantially fewer annotated images or better performance\nwith the same amount of annotated data, outperforming the existing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuangzhuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gay_H/0/1/0/all/0/1\">Hiram Gay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baozhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2009.09780","description":"<p>COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging\nexams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,\nand uses less radiation. Here, we demonstrate the impact of lung segmentation\nin COVID-19 identification using CXR images and evaluate which contents of the\nimage influenced the most. Semantic segmentation was performed using a U-Net\nCNN architecture, and the classification using three CNN architectures (VGG,\nResNet, and Inception). Explainable Artificial Intelligence techniques were\nemployed to estimate the impact of segmentation. A three-classes database was\ncomposed: lung opacity (pneumonia), COVID-19, and normal. We assessed the\nimpact of creating a CXR image database from different sources, and the\nCOVID-19 generalization from one source to another. The segmentation achieved a\nJaccard distance of 0.034 and a Dice coefficient of 0.982. The classification\nusing segmented images achieved an F1-Score of 0.88 for the multi-class setup,\nand 0.83 for COVID-19 identification. In the cross-dataset scenario, we\nobtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for\nCOVID-19 identification using segmented images. Experiments support the\nconclusion that even after segmentation, there is a strong bias introduced by\nunderlying factors from different sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_L/0/1/0/all/0/1\">Lucas O. Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_R/0/1/0/all/0/1\">Rodolfo M. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertolini_D/0/1/0/all/0/1\">Diego Bertolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_L/0/1/0/all/0/1\">Luiz S. Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalcanti_G/0/1/0/all/0/1\">George D. C. Cavalcanti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Costa_Y/0/1/0/all/0/1\">Yandre M. G. Costa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Augmented ConvLSTM for Environment Prediction. (arXiv:2010.09662v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.09662","description":"<p>Safe and proactive planning in robotic systems generally requires accurate\npredictions of the environment. Prior work on environment prediction applied\nvideo frame prediction techniques to bird's-eye view environment\nrepresentations, such as occupancy grids. ConvLSTM-based frameworks used\npreviously often result in significant blurring and vanishing of moving\nobjects, thus hindering their applicability for use in safety-critical\napplications. In this work, we propose two extensions to the ConvLSTM to\naddress these issues. We present the Temporal Attention Augmented ConvLSTM\n(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks\nfor spatiotemporal occupancy prediction, and demonstrate improved performance\nover baseline architectures on the real-world KITTI and Waymo datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_B/0/1/0/all/0/1\">Bernard Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection in Video via Self-Supervised and Multi-Task Learning. (arXiv:2011.07491v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.07491","description":"<p>Anomaly detection in video is a challenging computer vision problem. Due to\nthe lack of anomalous events at training time, anomaly detection requires the\ndesign of learning methods without full supervision. In this paper, we approach\nanomalous event detection in video through self-supervised and multi-task\nlearning at the object level. We first utilize a pre-trained detector to detect\nobjects. Then, we train a 3D convolutional neural network to produce\ndiscriminative anomaly-specific information by jointly learning multiple proxy\ntasks: three self-supervised and one based on knowledge distillation. The\nself-supervised tasks are: (i) discrimination of forward/backward moving\nobjects (arrow of time), (ii) discrimination of objects in\nconsecutive/intermittent frames (motion irregularity) and (iii) reconstruction\nof object-specific appearance information. The knowledge distillation task\ntakes into account both classification and detection information, generating\nlarge prediction discrepancies between teacher and student models when\nanomalies occur. To the best of our knowledge, we are the first to approach\nanomalous event detection in video as a multi-task learning problem,\nintegrating multiple self-supervised and knowledge distillation proxy tasks in\na single architecture. Our lightweight architecture outperforms the\nstate-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD\nPed2. Additionally, we perform an ablation study demonstrating the importance\nof integrating self-supervised learning and normality-specific distillation in\na multi-task learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbalau_A/0/1/0/all/0/1\">Antonio Barbalau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking deinterlacing for early interlaced videos. (arXiv:2011.13675v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13675","description":"<p>With the rapid development of image restoration techniques, high-definition\nreconstruction of early videos has achieved impressive results. However, there\nare few studies about the interlacing artifacts that often appear in early\nvideos and significantly affect visual perception. Traditional deinterlacing\napproaches are mainly focused on early interlacing scanning systems and thus\ncannot handle the complex and complicated artifacts in real-world early\ninterlaced videos. Hence, this paper proposes a specific deinterlacing network\n(DIN), which is motivated by the traditional deinterlacing strategy. The\nproposed DIN consists of two stages, i.e., a cooperative vertical interpolation\nstage for split fields, and a merging stage that is applied to perceive\nmovements and remove ghost artifacts. Experimental results demonstrate that the\nproposed method can effectively remove complex artifacts in early interlaced\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ronggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.15081","description":"<p>We propose Deep Estimators of Features (DEFs), a learning-based framework for\npredicting sharp geometric features in sampled 3D shapes. Differently from\nexisting data-driven methods, which reduce this problem to feature\nclassification, we propose to regress a scalar field representing the distance\nfrom point samples to the closest feature line on local patches. Our approach\nis the first that scales to massive point clouds by fusing distance-to-feature\nestimates obtained on individual patches. We extensively evaluate our approach\nagainst five baselines on newly proposed synthetic and real-world 3D CAD model\nbenchmarks. Our approach not only outperforms the baselines (with improvements\nin Recall and False Positives Rates), but generalizes to real-world scans after\ntraining our model on synthetic data and fine-tuning it on a small dataset of\nscanned data. We demonstrate a downstream application, where we reconstruct an\nexplicit representation of straight and curved sharp feature lines from range\nscan data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matveev_A/0/1/0/all/0/1\">Albert Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhimov_R/0/1/0/all/0/1\">Ruslan Rakhimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobrovskikh_G/0/1/0/all/0/1\">Gleb Bobrovskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1\">Emil Bogomolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panozzo_D/0/1/0/all/0/1\">Daniele Panozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction. (arXiv:2012.00924v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.00924","description":"<p>Modeling the hand-object (HO) interaction not only requires estimation of the\nHO pose, but also pays attention to the contact due to their interaction.\nSignificant progress has been made in estimating hand and object separately\nwith deep learning methods, simultaneous HO pose estimation and contact\nmodeling has not yet been fully explored. In this paper, we present an explicit\ncontact representation namely Contact Potential Field (CPF), and a\nlearning-fitting hybrid framework namely MIHO to Modeling the Interaction of\nHand and Object. In CPF, we treat each contacting HO vertex pair as a\nspring-mass system. Hence the whole system forms a potential field with minimal\nelastic energy at the grasp position. Extensive experiments on the two commonly\nused benchmarks have demonstrated that our method can achieve state-of-the-art\nin several reconstruction metrics, and allow us to produce more physically\nplausible HO pose even when the ground-truth exhibits severe interpenetration\nor disjointedness. Our code is available at https://github.com/lixiny/CPF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02339","description":"<p>Image captioning models generally lack the capability to take into account\nuser interest, and usually default to global descriptions that try to balance\nreadability, informativeness, and information overload. On the other hand, VQA\nmodels generally lack the ability to provide long descriptive answers, while\nexpecting the textual question to be quite precise. We present a method to\ncontrol the concepts that an image caption should focus on, using an additional\ninput called the guiding text that refers to either groundable or ungroundable\nconcepts in the image. Our model consists of a Transformer-based multimodal\nencoder that uses the guiding text together with global and object-level image\nfeatures to derive early-fusion representations used to generate the guided\ncaption. While models trained on Visual Genome data have an in-domain advantage\nof fitting well when guided with automatic object labels, we find that guided\ncaptioning models trained on Conceptual Captions generalize better on\nout-of-domain images and guiding texts. Our human-evaluation results indicate\nthat attempting in-the-wild guided image captioning requires access to large,\nunrestricted-domain training datasets, and that increased style diversity (even\nwithout increasing the number of unique tokens) is a key factor for improved\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Pattern Mining for Nowcasting Extreme Earthquakes in Southern California. (arXiv:2012.14336v3 [physics.geo-ph] UPDATED)","link":"http://arxiv.org/abs/2012.14336","description":"<p>Geoscience and seismology have utilized the most advanced technologies and\nequipment to monitor seismic events globally from the past few decades. With\nthe enormous amount of data, modern GPU-powered deep learning presents a\npromising approach to analyze data and discover patterns. In recent years,\nthere are plenty of successful deep learning models for picking seismic waves.\nHowever, forecasting extreme earthquakes, which can cause disasters, is still\nan underdeveloped topic in history. Relevant research in spatiotemporal\ndynamics mining and forecasting has revealed some successful predictions, a\ncrucial topic in many scientific research fields. Most studies of them have\nmany successful applications of using deep neural networks. In Geology and\nEarth science studies, earthquake prediction is one of the world's most\nchallenging problems, about which cutting-edge deep learning technologies may\nhelp discover some valuable patterns. In this project, we propose a deep\nlearning modeling approach, namely \\tseqpre, to mine spatiotemporal patterns\nfrom data to nowcast extreme earthquakes by discovering visual dynamics in\nregional coarse-grained spatial grids over time. In this modeling approach, we\nuse synthetic deep learning neural networks with domain knowledge in geoscience\nand seismology to exploit earthquake patterns for prediction using\nconvolutional long short-term memory neural networks. Our experiments show a\nstrong correlation between location prediction and magnitude prediction for\nearthquakes in Southern California. Ablation studies and visualization validate\nthe effectiveness of the proposed modeling method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Feng_B/0/1/0/all/0/1\">Bo Feng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fox_G/0/1/0/all/0/1\">Geoffrey C. Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Temporal Dynamics from Cycles in Narrated Video. (arXiv:2101.02337v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.02337","description":"<p>Learning to model how the world changes as time elapses has proven a\nchallenging problem for the computer vision community. We propose a\nself-supervised solution to this problem using temporal cycle consistency\njointly in vision and language, training on narrated video. Our model learns\nmodality-agnostic functions to predict forward and backward in time, which must\nundo each other when composed. This constraint leads to the discovery of\nhigh-level transitions between moments in time, since such transitions are\neasily inverted and shared across modalities. We justify the design of our\nmodel with an ablation study on different configurations of the cycle\nconsistency problem. We then show qualitatively and quantitatively that our\napproach yields a meaningful, high-level model of the future and past. We apply\nthe learned dynamics model without further training to various tasks, such as\npredicting future action and temporally ordering sets of images. Project page:\nhttps://dave.ml/mmcc\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TypeNet: Deep Learning Keystroke Biometrics. (arXiv:2101.05570v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05570","description":"<p>We study the performance of Long Short-Term Memory networks for keystroke\nbiometric authentication at large scale in free-text scenarios. For this we\nexplore the performance of Long Short-Term Memory (LSTMs) networks trained with\na moderate number of keystrokes per identity and evaluated under different\nscenarios including: i) three learning approaches depending on the loss\nfunction (softmax, contrastive, and triplet loss); ii) different number of\ntraining samples and lengths of keystroke sequences; iii) four databases based\non two device types (physical vs touchscreen keyboard); and iv) comparison with\nexisting approaches based on both traditional statistical methods and deep\nlearning architectures. Our approach called TypeNet achieves state-of-the-art\nkeystroke biometric authentication performance with an Equal Error Rate of 2.2%\nand 9.2% for physical and touchscreen keyboards, respectively, significantly\noutperforming previous approaches. Our experiments demonstrate a moderate\nincrease in error with up to 100,000 subjects, demonstrating the potential of\nTypeNet to operate at an Internet scale. To the best of our knowledge, the\ndatabases used in this work are the largest existing free-text keystroke\ndatabases available for research with more than 136 million keystrokes from\n168,000 subjects in physical keyboards, and 60,000 subjects with more than 63\nmillion keystrokes acquired on mobile touchscreens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acien_A/0/1/0/all/0/1\">Alejandro Acien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monaco_J/0/1/0/all/0/1\">John V. Monaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channelized Axial Attention for Semantic Segmentation -- Considering Channel Relation within Spatial Attention for Semantic Segmentation. (arXiv:2101.07434v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07434","description":"<p>Spatial and channel attentions, modelling the semantic interdependencies in\nspatial and channel dimensions respectively, have recently been widely used for\nsemantic segmentation. However, computing spatial and channel attentions\nseparately sometimes causes errors, especially for those difficult cases. In\nthis paper, we propose Channelized Axial Attention (CAA) to seamlessly\nintegrate channel attention and spatial attention into a single operation with\nnegligible computation overhead. Specifically, we break down the dot-product\noperation of the spatial attention into two parts and insert channel relation\nin between, allowing for independently optimized channel attention on each\nspatial location. We further develop grouped vectorization, which allows our\nmodel to run with very little memory consumption without slowing down the\nrunning speed. Comparative experiments conducted on multiple benchmark\ndatasets, including Cityscapes, PASCAL Context, and COCO-Stuff, demonstrate\nthat our CAA outperforms many state-of-the-art segmentation models (including\ndual attention) on all tested datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ye Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenjing Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep One-Class Classification via Interpolated Gaussian Descriptor. (arXiv:2101.10043v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.10043","description":"<p>One-class classification (OCC) aims to learn an effective data description to\nenclose all normal training samples and detect anomalies based on the deviation\nfrom the data description. Current state-of-the-art OCC models learn a compact\nnormality description by hyper-sphere minimisation, but they often suffer from\noverfitting the training data, especially when the training set is small or\ncontaminated with anomalous samples. To address this issue, we introduce the\ninterpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a\none-class Gaussian anomaly classifier trained with adversarially interpolated\ntraining samples. The Gaussian anomaly classifier differentiates the training\nsamples based on their distance to the Gaussian centre and the standard\ndeviation of these distances, offering the model a discriminability w.r.t. the\ngiven samples during training. The adversarial interpolation is enforced to\nconsistently learn a smooth Gaussian descriptor, even when the training data is\nsmall or contaminated with anomalous samples. This enables our model to learn\nthe data description based on the representative normal samples rather than\nfringe or anomalous samples, resulting in significantly improved normality\ndescription. In extensive experiments on diverse popular benchmarks, including\nMNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves\nbetter detection accuracy than current state-of-the-art models. IGD also shows\nbetter robustness in problems with small or contaminated training sets. Code is\navailable at https://github.com/tianyu0207/IGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Memory Attention for Video Semantic Segmentation. (arXiv:2102.08643v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.08643","description":"<p>Video semantic segmentation requires to utilize the complex temporal\nrelations between frames of the video sequence. Previous works usually exploit\naccurate optical flow to leverage the temporal relations, which suffer much\nfrom heavy computational cost. In this paper, we propose a Temporal Memory\nAttention Network (TMANet) to adaptively integrate the long-range temporal\nrelations over the video sequence based on the self-attention mechanism without\nexhaustive optical flow prediction. Specially, we construct a memory using\nseveral past frames to store the temporal information of the current frame. We\nthen propose a temporal memory attention module to capture the relation between\nthe current frame and the memory to enhance the representation of the current\nframe. Our method achieves new state-of-the-art performances on two challenging\nvideo semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and\n76.5% mIoU on CamVid with ResNet-50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MosaicOS: A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection. (arXiv:2102.08884v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.08884","description":"<p>Many objects do not appear frequently enough in complex scenes (e.g., certain\nhandbags in living rooms) for training an accurate object detector, but are\noften found frequently by themselves (e.g., in product images). Yet, these\nobject-centric images are not effectively leveraged for improving object\ndetection in scene-centric images. In this paper, we propose Mosaic of\nObject-centric images as Scene-centric images (MosaicOS), a simple and novel\nframework that is surprisingly effective at tackling the challenges of\nlong-tailed object detection. Keys to our approach are three-fold: (i) pseudo\nscene-centric image construction from object-centric images for mitigating\ndomain differences, (ii) high-quality bounding box imputation using the\nobject-centric images' class labels, and (iii) a multi-stage training\nprocedure. On LVIS object detection (and instance segmentation), MosaicOS leads\nto a massive 60% (and 23%) relative improvement in average precision for rare\nobject categories. We also show that our framework can be compatibly used with\nother existing approaches to achieve even further gains. Our pre-trained models\nare publicly available at https://github.com/czhang0528/MosaicOS/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yandong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_D/0/1/0/all/0/1\">Dong Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Neural Network Subspaces. (arXiv:2102.10472v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.10472","description":"<p>Recent observations have advanced our understanding of the neural network\noptimization landscape, revealing the existence of (1) paths of high accuracy\ncontaining diverse solutions and (2) wider minima offering improved\nperformance. Previous methods observing diverse paths require multiple training\nruns. In contrast we aim to leverage both property (1) and (2) with a single\nmethod and in a single training run. With a similar computational cost as\ntraining one model, we learn lines, curves, and simplexes of high-accuracy\nneural networks. These neural network subspaces contain diverse solutions that\ncan be ensembled, approaching the ensemble performance of independently trained\nnetworks without the training cost. Moreover, using the subspace midpoint\nboosts accuracy, calibration, and robustness to label noise, outperforming\nStochastic Weight Averaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1\">Maxwell Horton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1\">Carlos Guestrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration. (arXiv:2103.00937v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00937","description":"<p>Point cloud registration is a key task in many computational fields. Previous\ncorrespondence matching based methods require the inputs to have distinctive\ngeometric structures to fit a 3D rigid transformation according to point-wise\nsparse feature matches. However, the accuracy of transformation heavily relies\non the quality of extracted features, which are prone to errors with respect to\npartiality and noise. In addition, they can not utilize the geometric knowledge\nof all the overlapping regions. On the other hand, previous global feature\nbased approaches can utilize the entire point cloud for the registration,\nhowever they ignore the negative effect of non-overlapping points when\naggregating global features. In this paper, we present OMNet, a global feature\nbased iterative network for partial-to-partial point cloud registration. We\nlearn overlapping masks to reject non-overlapping regions, which converts the\npartial-to-partial registration to the registration of the same shape.\nMoreover, the previously used data is sampled only once from the CAD models for\neach object, resulting in the same point clouds for the source and reference.\nWe propose a more practical manner of data generation where a CAD model is\nsampled twice for the source and reference, avoiding the previously prevalent\nover-fitting issue. Experimental results show that our method achieves\nstate-of-the-art performance compared to traditional and deep learning based\nmethods. Code is available at https://github.com/megvii-research/OMNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths. (arXiv:2103.01435v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01435","description":"<p>Quantizing deep networks with adaptive bit-widths is a promising technique\nfor efficient inference across many devices and resource constraints. In\ncontrast to static methods that repeat the quantization process and train\ndifferent models for different constraints, adaptive quantization enables us to\nflexibly adjust the bit-widths of a single deep network during inference for\ninstant adaptation in different scenarios. While existing research shows\nencouraging results on common image classification benchmarks, this paper\ninvestigates how to train such adaptive networks more effectively.\nSpecifically, we present two novel techniques for quantizing deep neural\nnetworks with adaptive bit-widths of weights and activations. First, we propose\na collaborative strategy to choose a high-precision teacher for transferring\nknowledge to the low-precision student while jointly optimizing the model with\nall bit-widths. Second, to effectively transfer knowledge, we develop a dynamic\nblock swapping method by randomly replacing the blocks in the lower-precision\nstudent network with the corresponding blocks in the higher-precision teacher\nnetwork. Extensive experiments on multiple image classification datasets\nincluding video classification benchmarks for the first time, well demonstrate\nthe efficacy of our approach over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Ximeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_B/0/1/0/all/0/1\">Bowen Pan Kailash Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans. (arXiv:2103.11161v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11161","description":"<p>We propose a novel method for reconstructing floor plans from noisy 3D point\nclouds. Our main contribution is a principled approach that relies on the Monte\nCarlo Tree Search (MCTS) algorithm to maximize a suitable objective function\nefficiently despite the complexity of the problem. Like previous work, we first\nproject the input point cloud to a top view to create a density map and extract\nroom proposals from it. Our method selects and optimizes the polygonal shapes\nof these room proposals jointly to fit the density map and outputs an accurate\nvectorized floor map even for large complex scenes. To do this, we adapted\nMCTS, an algorithm originally designed to learn to play games, to select the\nroom proposals by maximizing an objective function combining the fitness with\nthe density map as predicted by a deep network and regularizing terms on the\nroom shapes. We also introduce a refinement step to MCTS that adjusts the shape\nof the room proposals. For this step, we propose a novel differentiable method\nfor rendering the polygonal shapes of these proposals. We evaluate our method\non the recent and challenging Structured3D and Floor-SP datasets and show a\nsignificant improvement over the state-of-the-art, without imposing any hard\nconstraints nor assumptions on the floor plan configurations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stekovic_S/0/1/0/all/0/1\">Sinisa Stekovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rad_M/0/1/0/all/0/1\">Mahdi Rad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iMAP: Implicit Mapping and Positioning in Real-Time. (arXiv:2103.12352v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12352","description":"<p>We show for the first time that a multilayer perceptron (MLP) can serve as\nthe only scene representation in a real-time SLAM system for a handheld RGB-D\ncamera. Our network is trained in live operation without prior data, building a\ndense, scene-specific implicit 3D model of occupancy and colour which is also\nimmediately used for tracking.\n</p>\n<p>Achieving real-time SLAM via continual training of a neural network against a\nlive image stream requires significant innovation. Our iMAP algorithm uses a\nkeyframe structure and multi-processing computation flow, with dynamic\ninformation-guided pixel sampling for speed, with tracking at 10 Hz and global\nmap updating at 2 Hz. The advantages of an implicit MLP over standard dense\nSLAM techniques include efficient geometry representation with automatic detail\ncontrol and smooth, plausible filling-in of unobserved regions such as the back\nsurfaces of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sucar_E/0/1/0/all/0/1\">Edgar Sucar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shikun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-guided Automatic Natural Image Matting with Trimap Generation Network and Light-weight Non-local Attention. (arXiv:2103.17020v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17020","description":"<p>Natural image matting aims to precisely separate foreground objects from\nbackground using alpha matte. Fully automatic natural image matting without\nexternal annotation is challenging. Well-performed matting methods usually\nrequire accurate labor-intensive handcrafted trimap as extra input, while the\nperformance of automatic trimap generation method of dilating foreground\nsegmentation fluctuates with segmentation quality. Therefore, we argue that how\nto handle trade-off of additional information input is a major issue in\nautomatic matting. This paper presents a semantic-guided automatic natural\nimage matting pipeline with Trimap Generation Network and light-weight\nnon-local attention, which does not need trimap and background as input.\nSpecifically, guided by foreground segmentation, Trimap Generation Network\nestimates accurate trimap. Then, with estimated trimap as guidance, our\nlight-weight Non-local Matting Network with Refinement produces final alpha\nmatte, whose trimap-guided global aggregation attention block is equipped with\nstride downsampling convolution, reducing computation complexity and promoting\nperformance. Experimental results show that our matting algorithm has\ncompetitive performance with state-of-the-art methods in both trimap-free and\ntrimap-needed aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhongze Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise Estimation for Generative Diffusion Models. (arXiv:2104.02600v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.02600","description":"<p>Generative diffusion models have emerged as leading models in speech and\nimage generation. However, in order to perform well with a small number of\ndenoising steps, a costly tuning of the set of noise parameters is needed. In\nthis work, we present a simple and versatile learning scheme that can\nstep-by-step adjust those noise parameters, for any given number of steps,\nwhile the previous work needs to retune for each number separately.\nFurthermore, without modifying the weights of the diffusion model, we are able\nto significantly improve the synthesis results, for a small number of steps.\nOur approach comes at a negligible computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_Roman_R/0/1/0/all/0/1\">Robin San-Roman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jacobian Regularization for Mitigating Universal Adversarial Perturbations. (arXiv:2104.10459v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.10459","description":"<p>Universal Adversarial Perturbations (UAPs) are input perturbations that can\nfool a neural network on large sets of data. They are a class of attacks that\nrepresents a significant threat as they facilitate realistic, practical, and\nlow-cost attacks on neural networks. In this work, we derive upper bounds for\nthe effectiveness of UAPs based on norms of data-dependent Jacobians. We\nempirically verify that Jacobian regularization greatly increases model\nrobustness to UAPs by up to four times whilst maintaining clean performance.\nOur theoretical analysis also allows us to formulate a metric for the strength\nof shared adversarial perturbations between pairs of inputs. We apply this\nmetric to benchmark datasets and show that it is highly correlated with the\nactual observed robustness. This suggests that realistic and practical\nuniversal attacks can be reliably mitigated without sacrificing clean accuracy,\nwhich shows promise for the robustness of machine learning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1\">Kenneth T. Co</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_D/0/1/0/all/0/1\">David Martinez Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Medical Transformer for Medical Image Segmentation. (arXiv:2104.14702v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14702","description":"<p>Deep neural networks have been a prevailing technique in the field of medical\nimage processing. However, the most popular convolutional neural networks\n(CNNs) based methods for medical image segmentation are imperfect because they\nmodel long-range dependencies by stacking layers or enlarging filters.\nTransformers and the self-attention mechanism are recently proposed to\neffectively learn long-range dependencies by modeling all pairs of word-to-word\nattention regardless of their positions. The idea has also been extended to the\ncomputer vision field by creating and treating image patches as embeddings.\nConsidering the computation complexity for whole image self-attention, current\ntransformer-based models settle for a rigid partitioning scheme that\npotentially loses informative relations. Besides, current medical transformers\nmodel global context on full resolution images, leading to unnecessary\ncomputation costs. To address these issues, we developed a novel method to\nintegrate multi-scale attention and CNN feature extraction using a pyramidal\nnetwork architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans\ncaptured multi-range relations by working on multi-resolution images. An\nadaptive partitioning scheme was implemented to retain informative relations\nand to access different receptive fields efficiently. Experimental results on\nthree medical image datasets (gland segmentation, MoNuSeg, and HECKTOR\ndatasets) showed that PMTrans outperformed the latest CNN-based and\ntransformer-based models for medical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuangzhuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baozhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internet of Things (IoT) Based Video Analytics: a use case of Smart Doorbell. (arXiv:2105.06508v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06508","description":"<p>The vision of the internet of things (IoT) is a reality now. IoT devices are\ngetting cheaper, smaller. They are becoming more and more computationally and\nenergy-efficient. The global market of IoT-based video analytics has seen\nsignificant growth in recent years and it is expected to be a growing market\nsegment. For any IoT-based video analytics application, few key points\nrequired, such as cost-effectiveness, widespread use, flexible design, accurate\nscene detection, reusability of the framework. Video-based smart doorbell\nsystem is one such application domain for video analytics where many commercial\nofferings are available in the consumer market. However, such existing\nofferings are costly, monolithic, and proprietary. Also, there will be a\ntrade-off between accuracy and portability. To address the foreseen problems,\nI'm proposing a distributed framework for video analytics with a use case of a\nsmart doorbell system. The proposed framework uses AWS cloud services as a base\nplatform and to meet the price affordability constraint, the system was\nimplemented on affordable Raspberry Pi. The smart doorbell will be able to\nrecognize the known/unknown person with at most accuracy. The smart doorbell\nsystem is also having additional detection functionalities such as harmful\nweapon detection, noteworthy vehicle detection, animal/pet detection. An iOS\napplication is specifically developed for this implementation which can receive\nthe notification from the smart doorbell in real-time. Finally, the paper also\nmentions the classical approaches for video analytics, their feasibility in\nimplementing with this use-case, and comparative analysis in terms of accuracy\nand time required to detect an object in the frame is carried out. Results\nconclude that AWS cloud-based approach is worthy for this smart doorbell use\ncase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arya_S/0/1/0/all/0/1\">Shailesh Arya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Than Just Attention: Improving Cross-Modal Attentions with Contrastive Constraints for Image-Text Matching. (arXiv:2105.09597v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09597","description":"<p>Cross-modal attention mechanisms have been widely applied to the image-text\nmatching task and have achieved remarkable improvements thanks to its\ncapability of learning fine-grained relevance across different modalities.\nHowever, the cross-modal attention models of existing methods could be\nsub-optimal and inaccurate because there is no direct supervision provided\nduring the training process. In this work, we propose two novel training\nstrategies, namely Contrastive Content Re-sourcing (CCR) and Contrastive\nContent Swapping (CCS) constraints, to address such limitations. These\nconstraints supervise the training of cross-modal attention models in a\ncontrastive learning manner without requiring explicit attention annotations.\nThey are plug-in training strategies and can be easily integrated into existing\ncross-modal attention models. Additionally, we introduce three metrics\nincluding Attention Precision, Recall, and F1-Score to quantitatively measure\nthe quality of learned attention models. We evaluate the proposed constraints\nby incorporating them into four state-of-the-art cross-modal attention-based\nimage-text matching models. Experimental results on both Flickr30k and MS-COCO\ndatasets demonstrate that integrating these constraints improves the model\nperformance in terms of both retrieval performance and attention metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Rui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Domain Experts Collaborative Learning: Multi-Source Domain Generalization For Person Re-Identification. (arXiv:2105.12355v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12355","description":"<p>Recent years have witnessed significant progress in person re-identification\n(ReID). However, current ReID approaches still suffer from considerable\nperformance degradation when unseen testing domains exhibit different\ncharacteristics from the source training ones, known as the domain\ngeneralization problem. Given multiple source training domains, previous Domain\nGeneralizable ReID (DG-ReID) methods usually learn all domains together using a\nshared network, which can't learn sufficient knowledge from each domain. In\nthis paper, we propose a novel Multiple Domain Experts Collaborative Learning\n(MECL) framework for better exploiting all training domains, which benefits\nfrom the proposed Domain-Domain Collaborative Learning (DDCL) and\nUniversal-Domain Collaborative Learning (UDCL). DDCL utilizes domain-specific\nexperts for fully exploiting each domain, and prevents experts from\nover-fitting the corresponding domain using a meta-learning strategy. In UDCL,\na universal expert supervises the learning of domain experts and continuously\ngathers knowledge from all domain experts. Note, only the universal expert will\nbe used for inference. Extensive experiments on DG-ReID benchmarks demonstrate\nthe effectiveness of DDCL and UDCL, and show that the whole MECL framework\nsignificantly outperforms state-of-the-arts. Experimental results on\nDG-classification benchmarks also reveal the great potential of applying MECL\nto other DG tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shijie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dapeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haobin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinguo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration. (arXiv:2106.03479v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03479","description":"<p>Data association is important in the point cloud registration. In this work,\nwe propose to solve the partial-to-partial registration from a new perspective,\nby introducing multi-level feature interactions between the source and the\nreference clouds at the feature extraction stage, such that the registration\ncan be realized without the attentions or explicit mask estimation for the\noverlapping detection as adopted previously. Specifically, we present FINet, a\nfeature interaction-based structure with the capability to enable and\nstrengthen the information associating between the inputs at multiple stages.\nTo achieve this, we first split the features into two components, one for\nrotation and one for translation, based on the fact that they belong to\ndifferent solution spaces, yielding a dual branches structure. Second, we\ninsert several interaction modules at the feature extractor for the data\nassociation. Third, we propose a transformation sensitivity loss to obtain\nrotation-attentive and translation-attentive features. Experiments demonstrate\nthat our method performs higher precision and robustness compared to the\nstate-of-the-art traditional and learning-based methods. Code will be available\nat https://github.com/HaoXu-Work/FINet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nianjin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03776","description":"<p>Background modeling is a promising research area in video analysis with a\nvariety of video surveillance applications. Recent years have witnessed the\nproliferation of deep neural networks via effective learning-based approaches\nin motion analysis. However, these techniques only provide a limited\ndescription of the observed scenes' insufficient properties where a\nsingle-valued mapping is learned to approximate the temporal conditional\naverages of the target background. On the other hand, statistical learning in\nimagery domains has become one of the most prevalent approaches with high\nadaptation to dynamic context transformation, notably Gaussian Mixture Models,\ncombined with a foreground extraction step. In this work, we propose a novel,\ntwo-stage method of change detection with two convolutional neural networks.\nThe first architecture is grounded on the unsupervised Gaussian mixtures\nstatistical learning to describe the scenes' salient features. The second one\nimplements a light-weight pipeline of foreground detection. Our two-stage\nframework contains approximately 3.5K parameters in total but still maintains\nrapid convergence to intricate motion patterns. Our experiments on publicly\navailable datasets show that our proposed networks are not only capable of\ngeneralizing regions of moving objects in unseen cases with promising results\nbut also are competitive in performance efficiency and effectiveness regarding\nforeground segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuong_N/0/1/0/all/0/1\">Nguyen-Tien Cuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hung Ngoc Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Nhat Minh Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_P/0/1/0/all/0/1\">Phuong Hoai Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Synh Viet-Uyen Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06896","description":"<p>The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianbu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VSAC: Efficient and Accurate Estimator for H and F. (arXiv:2106.10240v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10240","description":"<p>We present VSAC, a RANSAC-type robust estimator with a number of novelties.\nIt benefits from the introduction of the concept of independent inliers that\nimproves significantly the efficacy of the dominant plane handling and, also,\nallows near error-free rejection of incorrect models, without false positives.\nThe local optimization process and its application is improved so that it is\nrun on average only once. Further technical improvements include adaptive\nsequential hypothesis verification and efficient model estimation via Gaussian\nelimination. Experiments on four standard datasets show that VSAC is\nsignificantly faster than all its predecessors and runs on average in 1-2 ms,\non a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,\nthe currently most accurate estimator of two-view geometry. In the repeated\nruns on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivashechkin_M/0/1/0/all/0/1\">Maksym Ivashechkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PERT: A Progressively Region-based Network for Scene Text Removal. (arXiv:2106.13029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13029","description":"<p>Scene text removal (STR) contains two processes: text localization and\nbackground reconstruction. Through integrating both processes into a single\nnetwork, previous methods provide an implicit erasure guidance by modifying all\npixels in the entire image. However, there exists two problems: 1) the implicit\nerasure guidance causes the excessive erasure to non-text areas; 2) the\none-stage erasure lacks the exhaustive removal of text region. In this paper,\nwe propose a ProgrEssively Region-based scene Text eraser (PERT), introducing\nan explicit erasure guidance and performing balanced multi-stage erasure for\naccurate and exhaustive text removal. Firstly, we introduce a new region-based\nmodification strategy (RegionMS) to explicitly guide the erasure process.\nDifferent from previous implicitly guided methods, RegionMS performs targeted\nand regional erasure on only text region, and adaptively perceives stroke-level\ninformation to improve the integrity of non-text areas with only bounding box\nlevel annotations. Secondly, PERT performs balanced multi-stage erasure with\nseveral progressive erasing stages. Each erasing stage takes an equal step\ntoward the text-erased image to ensure the exhaustive erasure of text regions.\nCompared with previous methods, PERT outperforms them by a large margin without\nthe need of adversarial loss, obtaining SOTA results with high speed (71 FPS)\nand at least 25% lower parameter complexity. Code is available at\nhttps://github.com/wangyuxin87/PERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hongtao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shancheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yadong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification. (arXiv:2107.02314v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02314","description":"<p>The BraTS 2021 challenge celebrates its 10th anniversary and is jointly\norganized by the Radiological Society of North America (RSNA), the American\nSociety of Neuroradiology (ASNR), and the Medical Image Computing and Computer\nAssisted Interventions (MICCAI) society. Since its inception, BraTS has been\nfocusing on being a common benchmarking venue for brain glioma segmentation\nalgorithms, with well-curated multi-institutional multi-parametric magnetic\nresonance imaging (mpMRI) data. Gliomas are the most common primary\nmalignancies of the central nervous system, with varying degrees of\naggressiveness and prognosis. The RSNA-ASNR-MICCAI BraTS 2021 challenge targets\nthe evaluation of computational algorithms assessing the same tumor\ncompartmentalization, as well as the underlying tumor's molecular\ncharacterization, in pre-operative baseline mpMRI data from 2,040 patients.\nSpecifically, the two tasks that BraTS 2021 focuses on are: a) the segmentation\nof the histologically distinct brain tumor sub-regions, and b) the\nclassification of the tumor's O[6]-methylguanine-DNA methyltransferase (MGMT)\npromoter methylation status. The performance evaluation of all participating\nalgorithms in BraTS 2021 will be conducted through the Sage Bionetworks Synapse\nplatform (Task 1) and Kaggle (Task 2), concluding in distributing to the top\nranked participants monetary awards of $60,000 collectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodasara_S/0/1/0/all/0/1\">Satyam Ghodasara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Suyash Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilello_M/0/1/0/all/0/1\">Michel Bilello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calabrese_E/0/1/0/all/0/1\">Evan Calabrese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colak_E/0/1/0/all/0/1\">Errol Colak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_F/0/1/0/all/0/1\">Felipe C. Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pati_S/0/1/0/all/0/1\">Sarthak Pati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevedello_L/0/1/0/all/0/1\">Luciano M. Prevedello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudie_J/0/1/0/all/0/1\">Jeffrey D. Rudie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sako_C/0/1/0/all/0/1\">Chiharu Sako</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinohara_R/0/1/0/all/0/1\">Russell T. Shinohara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergquist_T/0/1/0/all/0/1\">Timothy Bergquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_R/0/1/0/all/0/1\">Rong Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eddy_J/0/1/0/all/0/1\">James Eddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_J/0/1/0/all/0/1\">Julia Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reade_W/0/1/0/all/0/1\">Walter Reade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaffter_T/0/1/0/all/0/1\">Thomas Schaffter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Thomas Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moawad_A/0/1/0/all/0/1\">Ahmed W. Moawad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coelho_L/0/1/0/all/0/1\">Luiz Otavio Coelho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonnell_O/0/1/0/all/0/1\">Olivia McDonnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_E/0/1/0/all/0/1\">Elka Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moron_F/0/1/0/all/0/1\">Fanny E. Moron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswood_M/0/1/0/all/0/1\">Mark C. Oswood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_R/0/1/0/all/0/1\">Robert Y. Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siakallis_L/0/1/0/all/0/1\">Loizos Siakallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_Y/0/1/0/all/0/1\">Yulia Bronstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mason_J/0/1/0/all/0/1\">James R. Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1\">Anthony F. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_G/0/1/0/all/0/1\">Gagandeep Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aanchal Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besada_C/0/1/0/all/0/1\">Cristina H. Besada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derakhshan_J/0/1/0/all/0/1\">Jamal J. Derakhshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diogo_M/0/1/0/all/0/1\">Mariana C. Diogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_Dai_D/0/1/0/all/0/1\">Daniel D. Do-Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farage_L/0/1/0/all/0/1\">Luciano Farage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_J/0/1/0/all/0/1\">John L. Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadi_M/0/1/0/all/0/1\">Mohiuddin Hadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_V/0/1/0/all/0/1\">Virginia B. Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iv_M/0/1/0/all/0/1\">Michael Iv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joyner_D/0/1/0/all/0/1\">David Joyner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lincoln_C/0/1/0/all/0/1\">Christie Lincoln</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotan_E/0/1/0/all/0/1\">Eyal Lotan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyakoshi_A/0/1/0/all/0/1\">Asako Miyakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Montano_M/0/1/0/all/0/1\">Mariana Sanchez-Montano</a>, et al. (54 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.04795","description":"<p>Co-training, extended from self-training, is one of the frameworks for\nsemi-supervised learning. Without natural split of features, single-view\nco-training works at the cost of training extra classifiers, where the\nalgorithm should be delicately designed to prevent individual classifiers from\ncollapsing into each other. To remove these obstacles which deter the adoption\nof single-view co-training, we present a simple and efficient algorithm\nMulti-Head Co-Training. By integrating base learners into a multi-head\nstructure, the model is in a minimal amount of extra parameters. Every\nclassification head in the unified model interacts with its peers through a\n\"Weak and Strong Augmentation\" strategy, in which the diversity is naturally\nbrought by the strong data augmentation. Therefore, the proposed method\nfacilitates single-view co-training by 1). promoting diversity implicitly and\n2). only requiring a small extra computational overhead. The effectiveness of\nMulti-Head Co-Training is demonstrated in an empirical study on standard\nsemi-supervised learning benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shuwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Video Semantic Segmentation. (arXiv:2107.11052v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11052","description":"<p>Unsupervised Domain Adaptation for semantic segmentation has gained immense\npopularity since it can transfer knowledge from simulation to real (Sim2Real)\nby largely cutting out the laborious per pixel labeling efforts at real. In\nthis work, we present a new video extension of this task, namely Unsupervised\nDomain Adaptation for Video Semantic Segmentation. As it became easy to obtain\nlarge-scale video labels through simulation, we believe attempting to maximize\nSim2Real knowledge transferability is one of the promising directions for\nresolving the fundamental data-hungry issue in the video. To tackle this new\nproblem, we present a novel two-phase adaptation scheme. In the first step, we\nexhaustively distill source domain knowledge using supervised loss functions.\nSimultaneously, video adversarial training (VAT) is employed to align the\nfeatures from source to target utilizing video context. In the second step, we\napply video self-training (VST), focusing only on the target data. To construct\nrobust pseudo labels, we exploit the temporal information in the video, which\nhas been rarely explored in the previous image-based self-training approaches.\nWe set strong baseline scores on 'VIPER to CityscapeVPS' adaptation scenario.\nWe show that our proposals significantly outperform previous image-based UDA\nmethods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Inkyu Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kwanyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sanghyun Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13029","description":"<p>Zero-shot action recognition is the task of classifying action categories\nthat are not available in the training set. In this setting, the standard\nevaluation protocol is to use existing action recognition datasets(e.g. UCF101)\nand randomly split the classes into seen and unseen. However, most recent work\nbuilds on representations pre-trained on the Kinetics dataset, where classes\nlargely overlap with classes in the zero-shot evaluation datasets. As a result,\nclasses which are supposed to be unseen, are present during supervised\npre-training, invalidating the condition of the zero-shot setting. A similar\nconcern was previously noted several years ago for image based zero-shot\nrecognition but has not been considered by the zero-shot action recognition\ncommunity. In this paper, we propose a new split for true zero-shot action\nrecognition with no overlap between unseen test classes and training or\npre-training classes. We benchmark several recent approaches on the proposed\nTrue Zero-Shot(TruZe) Split for UCF101 and HMDB51, with zero-shot and\ngeneralized zero-shot evaluation. In our extensive analysis, we find that our\nTruZesplits are significantly harder than comparable random splits as nothing\nis leaking from pre-training, i.e. unseen performance is consistently lower,up\nto 8.9% for zero-shot action recognition. In an additional evaluation we also\nfind that similar issues exist in the splits used in few-shot action\nrecognition, here we see differences of up to 17.1%. We publish oursplits1and\nhope that our benchmark analysis will change how the field is evaluating zero-\nand few-shot action recognition moving forward.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle-Consistent Inverse GAN for Text-to-Image Synthesis. (arXiv:2108.01361v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01361","description":"<p>This paper investigates an open research task of text-to-image synthesis for\nautomatically generating or manipulating images from text descriptions.\nPrevailing methods mainly use the text as conditions for GAN generation, and\ntrain different models for the text-guided image generation and manipulation\ntasks. In this paper, we propose a novel unified framework of Cycle-consistent\nInverse GAN (CI-GAN) for both text-to-image generation and text-guided image\nmanipulation tasks. Specifically, we first train a GAN model without text\ninput, aiming to generate images with high diversity and quality. Then we learn\na GAN inversion model to convert the images back to the GAN latent space and\nobtain the inverted latent codes for each image, where we introduce the\ncycle-consistency training to learn more robust and consistent inverted latent\ncodes. We further uncover the latent space semantics of the trained GAN model,\nby learning a similarity model between text representations and the latent\ncodes. In the text-guided optimization module, we generate images with the\ndesired semantic attributes by optimizing the inverted latent codes. Extensive\nexperiments on the Recipe1M and CUB datasets validate the efficacy of our\nproposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02353","description":"<p>The vanilla GAN (Goodfellow et al. 2014) suffers from mode collapse deeply,\nwhich usually manifests as that the images generated by generators tend to have\na high similarity amongst them, even though their corresponding latent vectors\nhave been very different. In this paper, we introduce a pluggable diversity\npenalty module (DPM) to alleviate mode collapse of GANs. It reduces the\nsimilarity of image pairs in feature space, i.e., if two latent vectors are\ndifferent, then we enforce the generator to generate two images with different\nfeatures. The normalized Gram matrix is used to measure the similarity. We\ncompare the proposed method with Unrolled GAN (Metz et al. 2016), BourGAN\n(Xiao, Zhong, and Zheng 2018), PacGAN (Lin et al. 2018), VEEGAN (Srivastava et\nal. 2017) and ALI (Dumoulin et al. 2016) on 2D synthetic dataset, and results\nshow that the diversity penalty module can help GAN capture much more modes of\nthe data distribution. Further, in classification tasks, we apply this method\nas image data augmentation on MNIST, Fashion- MNIST and CIFAR-10, and the\nclassification testing accuracy is improved by 0.24%, 1.34% and 0.52% compared\nwith WGAN GP (Gulrajani et al. 2017), respectively. In domain translation,\ndiversity penalty module can help StarGAN (Choi et al. 2018) generate more\naccurate attention masks and accelarate the convergence process. Finally, we\nquantitatively evaluate the proposed method with IS and FID on CelebA,\nCIFAR-10, MNIST and Fashion-MNIST, and the results suggest GAN with diversity\npenalty module gets much higher IS and lower FID compared with some SOTA GAN\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard Yi Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Shiming Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation. (arXiv:2108.03372v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03372","description":"<p>In object re-identification (ReID), the development of deep learning\ntechniques often involves model updates and deployment. It is unbearable to\nre-embedding and re-index with the system suspended when deploying new models.\nTherefore, backward-compatible representation is proposed to enable \"new\"\nfeatures to be compared with \"old\" features directly, which means that the\ndatabase is active when there are both \"new\" and \"old\" features in it. Thus we\ncan scroll-refresh the database or even do nothing on the database to update.\n</p>\n<p>The existing backward-compatible methods either require a strong overlap\nbetween old and new training data or simply conduct constraints at the instance\nlevel. Thus they are difficult in handling complicated cluster structures and\nare limited in eliminating the impact of outliers in old embeddings, resulting\nin a risk of damaging the discriminative capability of new features. In this\nwork, we propose a Neighborhood Consensus Contrastive Learning (NCCL) method.\nWith no assumptions about the new training data, we estimate the sub-cluster\nstructures of old embeddings. A new embedding is constrained with multiple old\nembeddings in both embedding space and discrimination space at the sub-class\nlevel. The effect of outliers diminished, as the multiple samples serve as\n\"mean teachers\". Besides, we also propose a scheme to filter the old embeddings\nwith low credibility, further improving the compatibility robustness. Our\nmethod ensures backward compatibility without impairing the accuracy of the new\nmodel. And it can even improve the new model's accuracy in most scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lingyu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05015","description":"<p>Different from visible cameras which record intensity images frame by frame,\nthe biologically inspired event camera produces a stream of asynchronous and\nsparse events with much lower latency. In practice, the visible cameras can\nbetter perceive texture details and slow motion, while event cameras can be\nfree from motion blurs and have a larger dynamic range which enables them to\nwork well under fast motion and low illumination. Therefore, the two sensors\ncan cooperate with each other to achieve more reliable object tracking. In this\nwork, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to\nthe lack of a realistic and scaled dataset for this task. Our dataset consists\nof 820 video pairs captured under low illumination, high speed, and background\nclutter scenarios, and it is divided into a training and a testing subset, each\nof which contains 500 and 320 videos, respectively. Based on VisEvent, we\ntransform the event flows into event images and construct more than 30 baseline\nmethods by extending current single-modality trackers into dual-modality\nversions. More importantly, we further build a simple but effective tracking\nalgorithm by proposing a cross-modality transformer, to achieve more effective\nfeature fusion between visible and event data. Extensive experiments on the\nproposed VisEvent dataset, FE108, and two simulated datasets (i.e., OTB-DVS and\nVOT-DVS), validated the effectiveness of our model. The dataset and source code\nhave been released at our project page:\n\\url{https://sites.google.com/view/viseventtrack/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From Long-Tailed Data With Noisy Labels. (arXiv:2108.11096v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11096","description":"<p>Class imbalance and noisy labels are the norm rather than the exception in\nmany large-scale classification datasets. Nevertheless, most works in machine\nlearning typically assume balanced and clean data. There have been some recent\nattempts to tackle, on one side, the problem of learning from noisy labels and,\non the other side, learning from long-tailed data. Each group of methods make\nsimplifying assumptions about the other. Due to this separation, the proposed\nsolutions often underperform when both assumptions are violated. In this work,\nwe present a simple two-stage approach based on recent advances in\nself-supervised learning to treat both challenges simultaneously. It consists\nof, first, task-agnostic self-supervised pre-training, followed by\ntask-specific fine-tuning using an appropriate loss. Most significantly, we\nfind that self-supervised learning approaches are effectively able to cope with\nsevere class imbalance. In addition, the resulting learned representations are\nalso remarkably robust to label noise, when fine-tuned with an imbalance- and\nnoise-resistant loss function. We validate our claims with experiments on\nCIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as\nthe large-scale inherently noisy Clothing-1M dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1\">J&#xe9;rome Revaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1\">Boris Chidlovskii</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Adversarial Attacks on an Aerial Imagery Object Detector. (arXiv:2108.11765v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11765","description":"<p>Deep neural networks (DNNs) have become essential for processing the vast\namounts of aerial imagery collected using earth-observing satellite platforms.\nHowever, DNNs are vulnerable towards adversarial examples, and it is expected\nthat this weakness also plagues DNNs for aerial imagery. In this work, we\ndemonstrate one of the first efforts on physical adversarial attacks on aerial\nimagery, whereby adversarial patches were optimised, fabricated and installed\non or near target objects (cars) to significantly reduce the efficacy of an\nobject detector applied on overhead images. Physical adversarial attacks on\naerial images, particularly those captured from satellite platforms, are\nchallenged by atmospheric factors (lighting, weather, seasons) and the distance\nbetween the observer and target. To investigate the effects of these\nchallenges, we devised novel experiments and metrics to evaluate the efficacy\nof physical adversarial attacks against object detectors in aerial scenes. Our\nresults indicate the palpable threat posed by physical adversarial attacks\ntowards DNNs for processing satellite imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_A/0/1/0/all/0/1\">Andrew Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_Y/0/1/0/all/0/1\">Yee Wei Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasegaran_R/0/1/0/all/0/1\">Ramesh Rajasegaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dillon Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptually Optimized Deep High-Dynamic-Range Image Tone Mapping. (arXiv:2109.00180v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00180","description":"<p>We describe a deep high-dynamic-range (HDR) image tone mapping operator that\nis computationally efficient and perceptually optimized. We first decompose an\nHDR image into a normalized Laplacian pyramid, and use two deep neural networks\n(DNNs) to estimate the Laplacian pyramid of the desired tone-mapped image from\nthe normalized representation. We then end-to-end optimize the entire method\nover a database of HDR images by minimizing the normalized Laplacian pyramid\ndistance (NLPD), a recently proposed perceptual metric. Qualitative and\nquantitative experiments demonstrate that our method produces images with\nbetter visual quality, and runs the fastest among existing local tone mapping\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Chenyang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiebin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on the Joint Impact of Feature Selection and Data Re-sampling on Imbalance Classification. (arXiv:2109.00201v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.00201","description":"<p>In predictive tasks, real-world datasets often present different degrees of\nimbalanced (i.e., long-tailed or skewed) distributions. While the majority (the\nhead) classes have sufficient samples, the minority (the tail) classes can be\nunder-represented by a rather limited number of samples. Data pre-processing\nhas been shown to be very effective in dealing with such problems. On one hand,\ndata re-sampling is a common approach to tackling class imbalance. On the other\nhand, dimension reduction, which reduces the feature space, is a conventional\ntechnique for reducing noise and inconsistencies in a dataset. However, the\npossible synergy between feature selection and data re-sampling for\nhigh-performance imbalance classification has rarely been investigated before.\nTo address this issue, we carry out a comprehensive empirical study on the\njoint influence of feature selection and re-sampling on two-class imbalance\nclassification. Specifically, we study the performance of two opposite\npipelines for imbalance classification by applying feature selection before or\nafter data re-sampling. We conduct a large number of experiments, with a total\nof 9225 tests, on 52 publicly available datasets, using 9 feature selection\nmethods, 6 re-sampling approaches for class imbalance learning, and 3\nwell-known classification algorithms. Experimental results show that there is\nno constant winner between the two pipelines; thus both of them should be\nconsidered to derive the best performing model for imbalance classification. We\nfind that the performance of an imbalance classification model not only depends\non the classifier adopted and the ratio between the number of majority and\nminority samples, but also depends on the ratio between the number of samples\nand features. Overall, this study should provide new reference value for\nresearchers and practitioners in imbalance learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1\">Paolo Soda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jingjun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1\">Gaojuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almpanidis_G/0/1/0/all/0/1\">George Almpanidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1\">Salvador Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventPoint: Self-Supervised Local Descriptor Learning for Event Cameras. (arXiv:2109.00210v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00210","description":"<p>We proposes a method of extracting intrest points and descriptors using\nself-supervised learning method on frame-based event data, which is called\nEventPoint. Different from other feature extraction methods on event data, we\ntrain our model on real event-form driving dataset--DSEC with the\nself-supervised learning method we proposed, the training progress fully\nconsider the characteristics of event data.To verify the effectiveness of our\nwork,we conducted several complete evaluations: we emulated DART and carried\nout feature matching experiments on N-caltech101 dataset, the results shows\nthat the effect of EventPoint is better than DART; We use Vid2e tool provided\nby UZH to convert Oxford robotcar data into event-based format, and combined\nwith INS information provided to carry out the global pose estimation\nexperiment which is important in SLAM. As far as we know, this is the first\nwork to carry out this challenging task.Sufficient experimental data show that\nEventPoint can get better results while achieve real time on CPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ze Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Songzhi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Henry Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kevin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation. (arXiv:2109.01827v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01827","description":"<p>In this paper, we propose GOHOME, a method leveraging graph representations\nof the High Definition Map and sparse projections to generate a heatmap output\nrepresenting the future position probability distribution for a given agent in\na traffic scene. This heatmap output yields an unconstrained 2D grid\nrepresentation of agent future possible locations, allowing inherent\nmultimodality and a measure of the uncertainty of the prediction. Our\ngraph-oriented model avoids the high computation burden of representing the\nsurrounding context as squared images and processing it with classical CNNs,\nbut focuses instead only on the most probable lanes where the agent could end\nup in the immediate future. GOHOME reaches 3$rd$ on Argoverse Motion\nForecasting Benchmark on the MissRate$_6$ metric while achieving significant\nspeed-up and memory burden diminution compared to 1$^{st}$ place method HOME.\nWe also highlight that heatmap output enables multimodal ensembling and improve\n1$^{st}$ place MissRate$_6$ by more than 15$\\%$ with our best ensemble.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Fine-Grained Motion Embedding for Landscape Animation. (arXiv:2109.02216v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02216","description":"<p>In this paper we focus on landscape animation, which aims to generate\ntime-lapse videos from a single landscape image. Motion is crucial for\nlandscape animation as it determines how objects move in videos. Existing\nmethods are able to generate appealing videos by learning motion from real\ntime-lapse videos. However, current methods suffer from inaccurate motion\ngeneration, which leads to unrealistic video results. To tackle this problem,\nwe propose a model named FGLA to generate high-quality and realistic videos by\nlearning Fine-Grained motion embedding for Landscape Animation. Our model\nconsists of two parts: (1) a motion encoder which embeds time-lapse motion in a\nfine-grained way. (2) a motion generator which generates realistic motion to\nanimate input images. To train and evaluate on diverse time-lapse videos, we\nbuild the largest high-resolution Time-lapse video dataset with Diverse scenes,\nnamely Time-lapse-D, which includes 16,874 video clips with over 10 million\nframes. Quantitative and qualitative experimental results demonstrate the\nsuperiority of our method. In particular, our method achieves relative\nimprovements by 19% on LIPIS and 5.6% on FVD compared with state-of-the-art\nmethods on our dataset. A user study carried out with 700 human subjects shows\nthat our approach visually outperforms existing methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongwei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel Transformer for 3D Object Detection. (arXiv:2109.02497v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02497","description":"<p>We present Voxel Transformer (VoTr), a novel and effective voxel-based\nTransformer backbone for 3D object detection from point clouds. Conventional 3D\nconvolutional backbones in voxel-based 3D detectors cannot efficiently capture\nlarge context information, which is crucial for object recognition and\nlocalization, owing to the limited receptive fields. In this paper, we resolve\nthe problem by introducing a Transformer-based architecture that enables\nlong-range relationships between voxels by self-attention. Given the fact that\nnon-empty voxels are naturally sparse but numerous, directly applying standard\nTransformer on voxels is non-trivial. To this end, we propose the sparse voxel\nmodule and the submanifold voxel module, which can operate on the empty and\nnon-empty voxel positions effectively. To further enlarge the attention range\nwhile maintaining comparable computational overhead to the convolutional\ncounterparts, we propose two attention mechanisms for multi-head attention in\nthose two modules: Local Attention and Dilated Attention, and we further\npropose Fast Voxel Query to accelerate the querying process in multi-head\nattention. VoTr contains a series of sparse and submanifold voxel modules and\ncan be applied in most voxel-based detectors. Our proposed VoTr shows\nconsistent improvement over the convolutional baselines while maintaining\ncomputational efficiency on the KITTI dataset and the Waymo Open dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yujing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Minzhe Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoyue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02748","description":"<p>In a regular open set detection problem, samples of known classes (also\ncalled closed set classes) are used to train a special classifier. In testing,\nthe classifier can (1) classify the test samples of known classes to their\nrespective classes and (2) also detect samples that do not belong to any of the\nknown classes (we say they belong to some unknown or open set classes). This\npaper studies the problem of zero-shot open-set detection, which still performs\nthe same two tasks in testing but has no training except using the given known\nclass names. This paper proposes a novel and yet simple method (called ZO-CLIP)\nto solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot\nclassification through multi-modal representation learning. It first extends\nthe pre-trained multi-modal model CLIP by training a text-based image\ndescription generator on top of CLIP. In testing, it uses the extended model to\ngenerate some candidate unknown class names for each test sample and computes a\nconfidence score based on both the known class names and candidate unknown\nclass names for zero-shot open set detection. Experimental results on 5\nbenchmark datasets for open set detection confirm that ZO-CLIP outperforms the\nbaselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1\">Sepideh Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Journalistic Guidelines Aware News Image Captioning. (arXiv:2109.02865v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02865","description":"<p>The task of news article image captioning aims to generate descriptive and\ninformative captions for news article images. Unlike conventional image\ncaptions that simply describe the content of the image in general terms, news\nimage captions follow journalistic guidelines and rely heavily on named\nentities to describe the image content, often drawing context from the whole\narticle they are associated with. In this work, we propose a new approach to\nthis task, motivated by caption guidelines that journalists follow. Our\napproach, Journalistic Guidelines Aware News Image Captioning (JoGANIC),\nleverages the structure of captions to improve the generation quality and guide\nour representation design. Experimental results, including detailed ablation\nstudies, on two large-scale publicly available datasets show that JoGANIC\nsubstantially outperforms state-of-the-art methods both on caption generation\nand named entity related metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1\">Svebor Karaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alex Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.03082","description":"<p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach\nwith recurrent conditional generative adversarial network. In our approach, the\nrecurrent auto-encoder-based generator learns to fully explore the temporal\ncorrelation for compressing video. More importantly, we propose a recurrent\nconditional discriminator, which judges raw and compressed video conditioned on\nboth spatial and temporal information, including the latent representation,\ntemporal motion and hidden states in recurrent cells. This way, in the\nadversarial training, it pushes the generated video to be not only spatially\nphoto-realistic but also temporally consistent with groundtruth and coherent\namong video frames. The experimental results show that the proposed PLVC model\nlearns to compress video towards good perceptual quality at low bit-rate, and\noutperforms the previous traditional and learned approaches on several\nperceptual quality metrics. The user study further validates the outstanding\nperceptual performance of PLVC in comparison with the latest learned video\ncompression approaches and the official HEVC test model (HM 16.20). The codes\nwill be released at https://github.com/RenYang-home/PLVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal RoI Align for Video Object Recognition. (arXiv:2109.03495v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03495","description":"<p>Video object detection is challenging in the presence of appearance\ndeterioration in certain video frames. Therefore, it is a natural choice to\naggregate temporal information from other frames of the same video into the\ncurrent frame. However, RoI Align, as one of the most core procedures of video\ndetectors, still remains extracting features from a single-frame feature map\nfor proposals, making the extracted RoI features lack temporal information from\nvideos. In this work, considering the features of the same object instance are\nhighly similar among frames in a video, a novel Temporal RoI Align operator is\nproposed to extract features from other frames feature maps for current frame\nproposals by utilizing feature similarity. The proposed Temporal RoI Align\noperator can extract temporal information from the entire video for proposals.\nWe integrate it into single-frame video detectors and other state-of-the-art\nvideo detectors, and conduct quantitative experiments to demonstrate that the\nproposed Temporal RoI Align operator can consistently and significantly boost\nthe performance. Besides, the proposed Temporal RoI Align can also be applied\ninto video instance segmentation. Codes are available at\nhttps://github.com/open-mmlab/mmtracking\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huamin Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Recognizing Occluded Faces in the Wild. (arXiv:2109.03672v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03672","description":"<p>Facial appearance variations due to occlusion has been one of the main\nchallenges for face recognition systems. To facilitate further research in this\narea, it is necessary and important to have occluded face datasets collected\nfrom real-world, as synthetically generated occluded faces cannot represent the\nnature of the problem. In this paper, we present the Real World Occluded Faces\n(ROF) dataset, that contains faces with both upper face occlusion, due to\nsunglasses, and lower face occlusion, due to masks. We propose two evaluation\nprotocols for this dataset. Benchmark experiments on the dataset have shown\nthat no matter how powerful the deep face representation models are, their\nperformance degrades significantly when they are tested on real-world occluded\nfaces. It is observed that the performance drop is far less when the models are\ntested on synthetically generated occluded faces. The ROF dataset and the\nassociated evaluation protocols are publicly available at the following link\nhttps://github.com/ekremerakin/RealWorldOccludedFaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Erakin_M/0/1/0/all/0/1\">Mustafa Ekrem Erak&#x131;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">U&#x11f;ur Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic SegFormer. (arXiv:2109.03814v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03814","description":"<p>We present Panoptic SegFormer, a general framework for end-to-end panoptic\nsegmentation with Transformers. The proposed method extends Deformable DETR\nwith a unified mask prediction workflow for both things and stuff, making the\npanoptic segmentation pipeline concise and effective. With a ResNet-50\nbackbone, our method achieves 50.0\\% PQ on the COCO test-dev split, surpassing\nprevious state-of-the-art methods by significant margins without bells and\nwhistles. Using a more powerful PVTv2-B5 backbone, Panoptic-SegFormer achieves\na new record of 54.1\\%PQ and 54.4\\% PQ on the COCO val and test-dev splits with\nsingle scale input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss. (arXiv:2109.04290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04290","description":"<p>Employing large-scale pre-trained model CLIP to conduct video-text retrieval\ntask (VTR) has become a new trend, which exceeds previous VTR methods. Though,\ndue to the heterogeneity of structures and contents between video and text,\nprevious CLIP-based models are prone to overfitting in the training phase,\nresulting in relatively poor retrieval performance. In this paper, we propose a\nmulti-stream Corpus Alignment network with single gate Mixture-of-Experts\n(CAMoE) and a novel Dual Softmax Loss (DSL) to solve the two heterogeneity. The\nCAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video\nrepresentations, including action, entity, scene, etc., then align them with\nthe corresponding part of the text. In this stage, we conduct massive\nexplorations towards the feature extraction module and feature alignment\nmodule. DSL is proposed to avoid the one-way optimum-match which occurs in\nprevious contrastive methods. Introducing the intrinsic prior of each pair in a\nbatch, DSL serves as a reviser to correct the similarity matrix and achieves\nthe dual optimal match. DSL is easy to implement with only one-line code but\nimproves significantly. The results show that the proposed CAMoE and DSL are of\nstrong efficiency, and each of them is capable of achieving State-of-The-Art\n(SOTA) individually on various benchmarks such as MSR-VTT, MSVD, and LSMDC.\nFurther, with both of them, the performance is advanced to a big extend,\nsurpassing the previous SOTA methods for around 4.6\\% R@1 in MSR-VTT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hezheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dong Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Portrait Video Matting via Context Motion Network. (arXiv:2109.04598v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04598","description":"<p>Automatic portrait video matting is an under-constrained problem. Most\nstate-of-the-art methods only exploit the semantic information and process each\nframe individually. Their performance is compromised due to the lack of\ntemporal information between the frames. To solve this problem, we propose the\ncontext motion network to leverage semantic information and motion information.\nTo capture the motion information, we estimate the optical flow and design a\ncontext-motion updating operator to integrate features between frames\nrecurrently. Our experiments show that our network outperforms state-of-the-art\nmatting methods significantly on the Video240K SD dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiqi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Charlie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}