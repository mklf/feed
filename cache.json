{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Challenges in Generalization in Open Domain Question Answering. (arXiv:2109.01156v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01156","description":"<p>Recent work on Open Domain Question Answering has shown that there is a large\ndiscrepancy in model performance between novel test questions and those that\nlargely overlap with training questions. However, it is as of yet unclear which\naspects of novel questions that make them challenging. Drawing upon studies on\nsystematic generalization, we introduce and annotate questions according to\nthree categories that measure different levels and kinds of generalization:\ntraining set overlap, compositional generalization (comp-gen), and novel entity\ngeneralization (novel-entity). When evaluating six popular parametric and\nnon-parametric models, we find that for the established Natural Questions and\nTriviaQA datasets, even the strongest model performance for\ncomp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the\nfull test set -- indicating the challenge posed by these types of questions.\nFurthermore, we show that whilst non-parametric models can handle questions\ncontaining novel entities, they struggle with those requiring compositional\ngeneralization. Through thorough analysis we find that key question difficulty\nfactors are: cascading errors from the retrieval component, frequency of\nquestion pattern, and frequency of the entity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Conformer: Progressive Downsampling and Grouped Attention for Automatic Speech Recognition. (arXiv:2109.01163v1 [eess.AS])","link":"http://arxiv.org/abs/2109.01163","description":"<p>The recently proposed Conformer architecture has shown state-of-the-art\nperformances in Automatic Speech Recognition by combining convolution with\nattention to model both local and global dependencies. In this paper, we study\nhow to reduce the Conformer architecture complexity with a limited computing\nbudget, leading to a more efficient architecture design that we call Efficient\nConformer. We introduce progressive downsampling to the Conformer encoder and\npropose a novel attention mechanism named grouped attention, allowing us to\nreduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence\nlength $n$, hidden dimension $d$ and group size parameter $g$. We also\nexperiment the use of strided multi-head self-attention as a global\ndownsampling operation. Our experiments are performed on the LibriSpeech\ndataset with CTC and RNN-Transducer losses. We show that within the same\ncomputing budget, the proposed architecture achieves better performances with\nfaster training and decoding compared to the Conformer. Our 13M parameters CTC\nmodel achieves competitive WERs of 3.6\\%/9.0\\% without using a language model\nand 2.7\\%/6.7\\% with an external n-gram language model on the\ntest-clean/test-other sets while being 29\\% faster than our CTC Conformer\nbaseline at inference and 36\\% faster to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Burchi_M/0/1/0/all/0/1\">Maxime Burchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ranking Scientific Papers Using Preference Learning. (arXiv:2109.01190v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01190","description":"<p>Peer review is the main quality control mechanism in academia. Quality of\nscientific work has many dimensions; coupled with the subjective nature of the\nreviewing task, this makes final decision making based on the reviews and\nscores therein very difficult and time-consuming. To assist with this important\ntask, we cast it as a paper ranking problem based on peer review texts and\nreviewer scores. We introduce a novel, multi-faceted generic evaluation\nframework for making final decisions based on peer reviews that takes into\naccount effectiveness, efficiency and fairness of the evaluated system. We\npropose a novel approach to paper ranking based on Gaussian Process Preference\nLearning (GPPL) and evaluate it on peer review data from the ACL-2018\nconference. Our experiments demonstrate the superiority of our GPPL-based\napproach over prior work, while highlighting the importance of using both texts\nand review scores for paper ranking during peer review aggregation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dycke_N/0/1/0/all/0/1\">Nils Dycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_E/0/1/0/all/0/1\">Edwin Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Establishing Interlingua in Multilingual Language Models. (arXiv:2109.01207v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01207","description":"<p>Large multilingual language models show remarkable zero-shot cross-lingual\ntransfer performance on a range of tasks. Follow-up works hypothesized that\nthese models internally project representations of different languages into a\nshared interlingual space. However, they produced contradictory results. In\nthis paper, we correct %one of the previous works the famous prior work\nclaiming that \"BERT is not an Interlingua\" and show that with the proper choice\nof sentence representation different languages actually do converge to a shared\nspace in such language models. Furthermore, we demonstrate that this\nconvergence pattern is robust across four measures of correlation similarity\nand six mBERT-like models. We then extend our analysis to 28 diverse languages\nand find that the interlingual space exhibits a particular structure similar to\nthe linguistic relatedness of languages. We also highlight a few outlier\nlanguages that seem to fail to converge to the shared space. The code for\nreplicating our results is available at the following URL:\nhttps://github.com/maksym-del/interlingua.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Del_M/0/1/0/all/0/1\">Maksym Del</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishel_M/0/1/0/all/0/1\">Mark Fishel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Reproducibility in NLP and ML. (arXiv:2109.01211v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01211","description":"<p>Reproducibility has become an intensely debated topic in NLP and ML over\nrecent years, but no commonly accepted way of assessing reproducibility, let\nalone quantifying it, has so far emerged. The assumption has been that wider\nscientific reproducibility terminology and definitions are not applicable to\nNLP/ML, with the result that many different terms and definitions have been\nproposed, some diametrically opposed. In this paper, we test this assumption,\nby taking the standard terminology and definitions from metrology and applying\nthem directly to NLP/ML. We find that we are able to straightforwardly derive a\npractical framework for assessing reproducibility which has the desirable\nproperty of yielding a quantified degree of reproducibility that is comparable\nacross different reproduction studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1\">Anya Belz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements. (arXiv:2109.01226v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01226","description":"<p>More predictable words are easier to process - they are read faster and\nelicit smaller neural signals associated with processing difficulty, most\nnotably, the N400 component of the event-related brain potential. Thus, it has\nbeen argued that prediction of upcoming words is a key component of language\ncomprehension, and that studying the amplitude of the N400 is a valuable way to\ninvestigate the predictions that we make. In this study, we investigate whether\nthe linguistic predictions of computational language models or humans better\nreflect the way in which natural language stimuli modulate the amplitude of the\nN400. One important difference in the linguistic predictions of humans versus\ncomputational language models is that while language models base their\npredictions exclusively on the preceding linguistic context, humans may rely on\nother factors. We find that the predictions of three top-of-the-line\ncontemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more\nclosely than human predictions. This suggests that the predictive processes\nunderlying the N400 may be more sensitive to the surface-level statistics of\nlanguage than previously thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Conditionality for Natural Language Generation. (arXiv:2109.01229v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01229","description":"<p>Large scale pretrained language models have demonstrated state-of-the-art\nperformance in language understanding tasks. Their application has recently\nexpanded into multimodality learning, leading to improved representations\ncombining vision and language. However, progress in adapting language models\ntowards conditional Natural Language Generation (NLG) has been limited to a\nsingle modality, generally text. We propose MAnTiS, Multimodal Adaptation for\nText Synthesis, a general approach for multimodal conditionality in\ntransformer-based NLG models. In this method, we pass inputs from each modality\nthrough modality-specific encoders, project to textual token space, and finally\njoin to form a conditionality prefix. We fine-tune the pretrained language\nmodel and encoders with the conditionality prefix guiding the generation. We\napply MAnTiS to the task of product description generation, conditioning a\nnetwork on both product images and titles to generate descriptive text. We\ndemonstrate that MAnTiS outperforms strong baseline approaches on standard NLG\nscoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS\ncan generate human quality descriptions consistent with given multimodal\ninputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sollami_M/0/1/0/all/0/1\">Michael Sollami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aashish Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction. (arXiv:2109.01238v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01238","description":"<p>Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new\nsubtask of target-oriented sentiment analysis that aims to extract opinion\nwords for a given aspect in text. Current state-of-the-art methods leverage\nposition embeddings to capture the relative position of a word to the target.\nHowever, the performance of these methods depends on the ability to incorporate\nthis information into word representations. In this paper, we explore a variety\nof text encoders based on pretrained word embeddings or language models that\nleverage part-of-speech and position embeddings, aiming to examine the actual\ncontribution of each component in TOWE. We also adapt a graph convolutional\nnetwork (GCN) to enhance word representations by incorporating syntactic\ninformation. Our experimental results demonstrate that BiLSTM-based models can\neffectively encode position information into word representations while using a\nGCN only achieves marginal gains. Interestingly, our simple methods outperform\nseveral state-of-the-art complex neural structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mensah_S/0/1/0/all/0/1\">Samuel Mensah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Linking and Discovery via Arborescence-based Supervised Clustering. (arXiv:2109.01242v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01242","description":"<p>Previous work has shown promising results in performing entity linking by\nmeasuring not only the affinities between mentions and entities but also those\namongst mentions. In this paper, we present novel training and inference\nprocedures that fully utilize mention-to-mention affinities by building minimum\narborescences (i.e., directed spanning trees) over mentions and entities across\ndocuments in order to make linking decisions. We also show that this method\ngracefully extends to entity discovery, enabling the clustering of mentions\nthat do not have an associated entity in the knowledge base. We evaluate our\napproach on the Zero-Shot Entity Linking dataset and MedMentions, the largest\npublicly available biomedical dataset, and show significant improvements in\nperformance for both entity linking and discovery compared to identically\nparameterized models. We further show significant efficiency improvements with\nonly a small loss in accuracy over previous work, which use more\ncomputationally expensive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dhruv Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angell_R/0/1/0/all/0/1\">Rico Angell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Prompt-Based Models Really Understand the Meaning of their Prompts?. (arXiv:2109.01247v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01247","description":"<p>Recently, a boom of papers have shown extraordinary progress in few-shot\nlearning with various prompt-based models. Such success can give the impression\nthat prompts help models to learn faster in the same way that humans learn\nfaster when provided with task instructions expressed in natural language. In\nthis study, we experiment with over 30 prompts manually written for natural\nlanguage inference (NLI). We find that models learn just as fast with many\nprompts that are intentionally irrelevant or even pathologically misleading as\nthey do with instructively \"good\" prompts. Additionally, we find that model\nperformance is more dependent on the choice of the LM target words (a.k.a. the\n\"verbalizer\" that converts LM vocabulary prediction to class labels) than on\nthe text of the prompt itself. In sum, we find little evidence that suggests\nexisting prompt-based models truly understand the meaning of their given\nprompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Context-Aware Hierarchical BERT Fusion Network for Multi-turn Dialog Act Detection. (arXiv:2109.01267v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01267","description":"<p>The success of interactive dialog systems is usually associated with the\nquality of the spoken language understanding (SLU) task, which mainly\nidentifies the corresponding dialog acts and slot values in each turn. By\ntreating utterances in isolation, most SLU systems often overlook the semantic\ncontext in which a dialog act is expected. The act dependency between turns is\nnon-trivial and yet critical to the identification of the correct semantic\nrepresentations. Previous works with limited context awareness have exposed the\ninadequacy of dealing with complexity in multiproned user intents, which are\nsubject to spontaneous change during turn transitions. In this work, we propose\nto enhance SLU in multi-turn dialogs, employing a context-aware hierarchical\nBERT fusion Network (CaBERT-SLU) to not only discern context information within\na dialog but also jointly identify multiple dialog acts and slots in each\nutterance. Experimental results show that our approach reaches new\nstate-of-the-art (SOTA) performances in two complicated multi-turn dialogue\ndatasets with considerable improvements compared with previous methods, which\nonly consider single utterances for multiple intents and slot filling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting-Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruolin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juang_B/0/1/0/all/0/1\">Biing-Hwang Juang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Open-Source Dataset and A Multi-Task Model for Malay Named Entity Recognition. (arXiv:2109.01293v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01293","description":"<p>Named entity recognition (NER) is a fundamental task of natural language\nprocessing (NLP). However, most state-of-the-art research is mainly oriented to\nhigh-resource languages such as English and has not been widely applied to\nlow-resource languages. In Malay language, relevant NER resources are limited.\nIn this work, we propose a dataset construction framework, which is based on\nlabeled datasets of homologous languages and iterative optimization, to build a\nMalay NER dataset (MYNER) comprising 28,991 sentences (over 384 thousand\ntokens). Additionally, to better integrate boundary information for NER, we\npropose a multi-task (MT) model with a bidirectional revision (Bi-revision)\nmechanism for Malay NER task. Specifically, an auxiliary task, boundary\ndetection, is introduced to improve NER training in both explicit and implicit\nways. Furthermore, a gated ignoring mechanism is proposed to conduct\nconditional label transfer and alleviate error propagation by the auxiliary\ntask. Experimental results demonstrate that our model achieves comparable\nresults over baselines on MYNER. The dataset and the model in this paper would\nbe publicly released as a benchmark dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhihe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Symmetry Matters: A Modal-Alternating Propagation Network for Few-Shot Learning. (arXiv:2109.01295v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01295","description":"<p>Semantic information provides intra-class consistency and inter-class\ndiscriminability beyond visual concepts, which has been employed in Few-Shot\nLearning (FSL) to achieve further gains. However, semantic information is only\navailable for labeled samples but absent for unlabeled samples, in which the\nembeddings are rectified unilaterally by guiding the few labeled samples with\nsemantics. Therefore, it is inevitable to bring a cross-modal bias between\nsemantic-guided samples and nonsemantic-guided samples, which results in an\ninformation asymmetry problem. To address this problem, we propose a\nModal-Alternating Propagation Network (MAP-Net) to supplement the absent\nsemantic information of unlabeled samples, which builds information symmetry\namong all samples in both visual and semantic modalities. Specifically, the\nMAP-Net transfers the neighbor information by the graph propagation to generate\nthe pseudo-semantics for unlabeled samples guided by the completed visual\nrelationships and rectify the feature embeddings. In addition, due to the large\ndiscrepancy between visual and semantic modalities, we design a Relation\nGuidance (RG) strategy to guide the visual relation vectors via semantics so\nthat the propagated information is more beneficial. Extensive experimental\nresults on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011,\nSUN Attribute Database, and Oxford 102 Flower, have demonstrated that our\nproposed method achieves promising performance and outperforms the\nstate-of-the-art approaches, which indicates the necessity of information\nsymmetry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhishen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yanwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indexing Context-Sensitive Reachability. (arXiv:2109.01321v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01321","description":"<p>Many context-sensitive data flow analyses can be formulated as a variant of\nthe all-pairs Dyck-CFL reachability problem, which, in general, is of sub-cubic\ntime complexity and quadratic space complexity. Such high complexity\nsignificantly limits the scalability of context-sensitive data flow analysis\nand is not affordable for analyzing large-scale software. This paper presents\n\\textsc{Flare}, a reduction from the CFL reachability problem to the\nconventional graph reachability problem for context-sensitive data flow\nanalysis. This reduction allows us to benefit from recent advances in\nreachability indexing schemes, which often consume almost linear space for\nanswering reachability queries in almost constant time. We have applied our\nreduction to a context-sensitive alias analysis and a context-sensitive\ninformation-flow analysis for C/C++ programs. Experimental results on standard\nbenchmarks and open-source software demonstrate that we can achieve orders of\nmagnitude speedup at the cost of only moderate space to store the indexes. The\nimplementation of our approach is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qingkai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Charles Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Speaker Personas from Conversational Texts. (arXiv:2109.01330v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01330","description":"<p>Personas are useful for dialogue response prediction. However, the personas\nused in current studies are pre-defined and hard to obtain before a\nconversation. To tackle this issue, we study a new task, named Speaker Persona\nDetection (SPD), which aims to detect speaker personas based on the plain\nconversational text. In this task, a best-matched persona is searched out from\ncandidates given the conversational text. This is a many-to-many semantic\nmatching task because both contexts and personas in SPD are composed of\nmultiple sentences. The long-term dependency and the dynamic redundancy among\nthese sentences increase the difficulty of this task. We build a dataset for\nSPD, dubbed as Persona Match on Persona-Chat (PMPC). Furthermore, we evaluate\nseveral baseline models and propose utterance-to-profile (U2P) matching\nnetworks for this task. The U2P models operate at a fine granularity which\ntreat both contexts and personas as sets of multiple sequences. Then, each\nsequence pair is scored and an interpretable overall score is obtained for a\ncontext-persona pair through aggregation. Evaluation results show that the U2P\nmodels outperform their baseline counterparts significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT. (arXiv:2109.01396v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01396","description":"<p>Differently from the traditional statistical MT that decomposes the\ntranslation task into distinct separately learned components, neural machine\ntranslation uses a single neural network to model the entire translation\nprocess. Despite neural machine translation being de-facto standard, it is\nstill not clear how NMT models acquire different competences over the course of\ntraining, and how this mirrors the different models in traditional SMT. In this\nwork, we look at the competences related to three core SMT components and find\nthat during training, NMT first focuses on learning target-side language\nmodeling, then improves translation quality approaching word-by-word\ntranslation, and finally learns more complicated reordering patterns. We show\nthat this behavior holds for several models and language pairs. Additionally,\nwe explain how such an understanding of the training process can be useful in\npractice and, as an example, show how it can be used to improve vanilla\nnon-autoregressive neural machine translation by guiding teacher model\nselection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1\">Elena Voita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining. (arXiv:2109.01411v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01411","description":"<p>The Linked Open Data practice has led to a significant growth of structured\ndata on the Web in the last decade. Such structured data describe real-world\nentities in a machine-readable way, and have created an unprecedented\nopportunity for research in the field of Natural Language Processing. However,\nthere is a lack of studies on how such data can be used, for what kind of\ntasks, and to what extent they can be useful for these tasks. This work focuses\non the e-commerce domain to explore methods of utilising such structured data\nto create language resources that may be used for product classification and\nlinking. We process billions of structured data points in the form of RDF\nn-quads, to create multi-million words of product-related corpora that are\nlater used in three different ways for creating of language resources: training\nword embedding models, continued pre-training of BERT-like language models, and\ntraining Machine Translation models that are used as a proxy to generate\nproduct-related keywords. Our evaluation on an extensive set of benchmarks\nshows word embeddings to be the most reliable and consistent method to improve\nthe accuracy on both tasks (with up to 6.9 percentage points in macro-average\nF1 on some datasets). The other two methods however, are not as useful. Our\nanalysis shows that this could be due to a number of reasons, including the\nbiased domain representation in the structured data and lack of vocabulary\ncoverage. We share our datasets and discuss how our lessons learned could be\ntaken forward to inform future research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LG4AV: Combining Language Models and Graph Neural Networks for Author Verification. (arXiv:2109.01479v1 [cs.LG])","link":"http://arxiv.org/abs/2109.01479","description":"<p>The automatic verification of document authorships is important in various\nsettings. Researchers are for example judged and compared by the amount and\nimpact of their publications and public figures are confronted by their posts\non social media platforms. Therefore, it is important that authorship\ninformation in frequently used web services and platforms is correct. The\nquestion whether a given document is written by a given author is commonly\nreferred to as authorship verification (AV). While AV is a widely investigated\nproblem in general, only few works consider settings where the documents are\nshort and written in a rather uniform style. This makes most approaches\nunpractical for online databases and knowledge graphs in the scholarly domain.\nHere, authorships of scientific publications have to be verified, often with\njust abstracts and titles available. To this point, we present our novel\napproach LG4AV which combines language models and graph neural networks for\nauthorship verification. By directly feeding the available texts in a\npre-trained transformer architecture, our model does not need any hand-crafted\nstylometric features that are not meaningful in scenarios where the writing\nstyle is, at least to some extent, standardized. By the incorporation of a\ngraph neural network structure, our model can benefit from relations between\nauthors that are meaningful with respect to the verification process. For\nexample, scientific authors are more likely to write about topics that are\naddressed by their co-authors and twitter users tend to post about the same\nsubjects as people they follow. We experimentally evaluate our model and study\nto which extent the inclusion of co-authorships enhances verification decisions\nin bibliometric environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stubbemann_M/0/1/0/all/0/1\">Maximilian Stubbemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1\">Gerd Stumme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation. (arXiv:2109.01484v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01484","description":"<p>Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target\nsentence which conforms to the style of the given exemplar while encapsulating\nthe content information of the source sentence. In this paper, we propose a new\nmethod with the goal of learning a better representation of the style andthe\ncontent. This method is mainly motivated by the recent success of contrastive\nlearning which has demonstrated its power in unsupervised feature extraction\ntasks. The idea is to design two contrastive losses with respect to the content\nand the style by considering two problem characteristics during training. One\ncharacteristic is that the target sentence shares the same content with the\nsource sentence, and the second characteristic is that the target sentence\nshares the same style with the exemplar. These two contrastive losses are\nincorporated into the general encoder-decoder paradigm. Experiments on two\ndatasets, namely QQP-Pos and ParaNMT, demonstrate the effectiveness of our\nproposed constrastive losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical Data-to-Text Generation via Fine-Tuning Transformers. (arXiv:2109.01518v1 [cs.LG])","link":"http://arxiv.org/abs/2109.01518","description":"<p>Data-to-text (D2T) generation in the biomedical domain is a promising - yet\nmostly unexplored - field of research. Here, we apply neural models for D2T\ngeneration to a real-world dataset consisting of package leaflets of European\nmedicines. We show that fine-tuned transformers are able to generate realistic,\nmultisentence text from data in the biomedical domain, yet have important\nlimitations. We also release a new dataset (BioLeaflets) for benchmarking D2T\ngeneration models in the biomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yermakov_R/0/1/0/all/0/1\">Ruslan Yermakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drago_N/0/1/0/all/0/1\">Nicholas Drago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziletti_A/0/1/0/all/0/1\">Angelo Ziletti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01537","description":"<p>Dementia is a family of neurogenerative conditions affecting memory and\ncognition in an increasing number of individuals in our globally aging\npopulation. Automated analysis of language, speech and paralinguistic\nindicators have been gaining popularity as potential indicators of cognitive\ndecline. Here we propose a novel longitudinal multi-modal dataset collected\nfrom people with mild dementia and age matched controls over a period of\nseveral months in a natural setting. The multi-modal data consists of spoken\nconversations, a subset of which are transcribed, as well as typed and written\nthoughts and associated extra-linguistic information such as pen strokes and\nkeystrokes. We describe the dataset in detail and proceed to focus on a task\nusing the speech modality. The latter involves distinguishing controls from\npeople with dementia by exploiting the longitudinal nature of the data. Our\nexperiments showed significant differences in how the speech varied from\nsession to session in the control and dementia groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gkoumas_D/0/1/0/all/0/1\">Dimitris Gkoumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolters_M/0/1/0/all/0/1\">Maria Wolters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Neural Models for Natural Language Processing in the Face of Distributional Shift. (arXiv:2109.01558v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01558","description":"<p>The dominating NLP paradigm of training a strong neural predictor to perform\none task on a specific dataset has led to state-of-the-art performance in a\nvariety of applications (eg. sentiment classification, span-prediction based\nquestion answering or machine translation). However, it builds upon the\nassumption that the data distribution is stationary, ie. that the data is\nsampled from a fixed distribution both at training and test time. This way of\ntraining is inconsistent with how we as humans are able to learn from and\noperate within a constantly changing stream of information. Moreover, it is\nill-adapted to real-world use cases where the data distribution is expected to\nshift over the course of a model's lifetime.\n</p>\n<p>The first goal of this thesis is to characterize the different forms this\nshift can take in the context of natural language processing, and propose\nbenchmarks and evaluation metrics to measure its effect on current deep\nlearning architectures. We then proceed to take steps to mitigate the effect of\ndistributional shift on NLP models. To this end, we develop methods based on\nparametric reformulations of the distributionally robust optimization\nframework. Empirically, we demonstrate that these approaches yield more robust\nmodels as demonstrated on a selection of realistic problems. In the third and\nfinal part of this thesis, we explore ways of efficiently adapting existing\nmodels to new domains or tasks. Our contribution to this topic takes\ninspiration from information geometry to derive a new gradient update rule\nwhich alleviate catastrophic forgetting issues during adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Embeddings based Convolutional Neural Networks for Duplicate Question Identification. (arXiv:2109.01560v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01560","description":"<p>Question Paraphrase Identification (QPI) is a critical task for large-scale\nQuestion-Answering forums. The purpose of QPI is to determine whether a given\npair of questions are semantically identical or not. Previous approaches for\nthis task have yielded promising results, but have often relied on complex\nrecurrence mechanisms that are expensive and time-consuming in nature. In this\npaper, we propose a novel architecture combining a Bidirectional Transformer\nEncoder with Convolutional Neural Networks for the QPI task. We produce the\npredictions from the proposed architecture using two different inference\nsetups: Siamese and Matched Aggregation. Experimental results demonstrate that\nour model achieves state-of-the-art performance on the Quora Question Pairs\ndataset. We empirically prove that the addition of convolution layers to the\nmodel architecture improves the results in both inference setups. We also\ninvestigate the impact of partial and complete fine-tuning and analyze the\ntrade-off between computational power and accuracy in the process. Based on the\nobtained results, we conclude that the Matched-Aggregation setup consistently\noutperforms the Siamese setup. Our work provides insights into what\narchitecture combinations and setups are likely to produce better results for\nthe QPI task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakhrani_H/0/1/0/all/0/1\">Harsh Sakhrani</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Saloni Parekh</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ratadiya_P/0/1/0/all/0/1\">Pratik Ratadiya</a> (2) ((1) Pune Institute of Computer Technology, Maharashtra, India, (2) vCreaTek Consulting Services Pvt. Ltd., Maharashtra, India)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding. (arXiv:2109.01583v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01583","description":"<p>Lack of training data presents a grand challenge to scaling out spoken\nlanguage understanding (SLU) to low-resource languages. Although various data\naugmentation approaches have been proposed to synthesize training data in\nlow-resource target languages, the augmented data sets are often noisy, and\nthus impede the performance of SLU models. In this paper we focus on mitigating\nnoise in augmented data. We develop a denoising training approach. Multiple\nmodels are trained with data produced by various augmented methods. Those\nmodels provide supervision signals to each other. The experimental results show\nthat our method outperforms the existing state of the art by 3.05 and 4.24\npercentage points on two benchmark datasets, respectively. The code will be\nmade open sourced on github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yingmei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Training with Dense Retrieval for Document Retrieval. (arXiv:2109.01628v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01628","description":"<p>Dense retrieval has shown great success in passage ranking in English.\nHowever, its effectiveness in document retrieval for non-English languages\nremains unexplored due to the limitation in training resources. In this work,\nwe explore different transfer techniques for document ranking from English\nannotations to multiple non-English languages. Our experiments on the test\ncollections in six languages (Chinese, Arabic, French, Hindi, Bengali, Spanish)\nfrom diverse language families reveal that zero-shot model-based transfer using\nmBERT improves the search quality in non-English mono-lingual retrieval. Also,\nwe find that weakly-supervised target language transfer yields competitive\nperformances against the generation-based target language transfer that\nrequires external translators and query generators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01636","description":"<p>With the fast development of Deep Learning techniques, Named Entity\nRecognition (NER) is becoming more and more important in the information\nextraction task. The greatest difficulty that the NER task faces is to keep the\ndetectability even when types of NE and documents are unfamiliar. Realizing\nthat the specificity information may contain potential meanings of a word and\ngenerate semantic-related features for word embedding, we develop a\ndistribution-aware word embedding and implement three different methods to make\nuse of the distribution information in a NER framework. And the result shows\nthat the performance of NER will be improved if the word specificity is\nincorporated into existing NER methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuned Language Models Are Zero-Shot Learners. (arXiv:2109.01652v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01652","description":"<p>This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially boosts zero-shot performance on unseen tasks.\n</p>\n<p>We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof tasks and model scale are key components to the success of instruction\ntuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Y. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge. (arXiv:2109.01653v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01653","description":"<p>Most benchmark datasets targeting commonsense reasoning focus on everyday\nscenarios: physical knowledge like knowing that you could fill a cup under a\nwaterfall [Talmor et al., 2019], social knowledge like bumping into someone is\nawkward [Sap et al., 2019], and other generic situations. However, there is a\nrich space of commonsense inferences anchored to knowledge about specific\nentities: for example, deciding the truthfulness of a claim \"Harry Potter can\nteach classes on how to fly on a broomstick.\" Can models learn to combine\nentity knowledge with commonsense reasoning in this fashion? We introduce\nCREAK, a testbed for commonsense reasoning about entity knowledge, bridging\nfact-checking about entities (Harry Potter is a wizard and is skilled at riding\na broomstick) with commonsense inferences (if you're good at a skill you can\nteach others how to do it). Our dataset consists of 13k human-authored English\nclaims about entities that are either true or false, in addition to a small\ncontrast set. Crowdworkers can easily come up with these statements and human\nperformance on the dataset is high (high 90s); we argue that models should be\nable to blend entity knowledge and commonsense reasoning to do well here. In\nour experiments, we focus on the closed-book setting and observe that a\nbaseline model finetuned on existing fact verification benchmark struggles on\nCREAK. Training a model on CREAK improves accuracy by a substantial margin, but\nstill falls short of human performance. Our benchmark provides a unique probe\ninto natural language understanding models, testing both its ability to\nretrieve facts (e.g., who teaches at the University of Chicago?) and unstated\ncommonsense knowledge (e.g., butlers do not yell at guests).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1\">Yasumasa Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Michael J.Q. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?. (arXiv:1905.10617v10 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1905.10617","description":"<p>Exposure bias has been regarded as a central problem for auto-regressive\nlanguage models (LM). It claims that teacher forcing would cause the test-time\ngeneration to be incrementally distorted due to the training-generation\ndiscrepancy. Although a lot of algorithms have been proposed to avoid teacher\nforcing and therefore alleviate exposure bias, there is little work showing how\nserious the exposure bias problem actually is. In this work, we focus on the\ntask of open-ended language generation, propose metrics to quantify the impact\nof exposure bias in the aspects of quality, diversity, and consistency. Our key\nintuition is that if we feed ground-truth data prefixes (instead of prefixes\ngenerated by the model itself) into the model and ask it to continue the\ngeneration, the performance should become much better because the\ntraining-generation discrepancy in the prefix is removed. Both automatic and\nhuman evaluations are conducted in our experiments. On the contrary to the\npopular belief in exposure bias, we find that the the distortion induced by the\nprefix discrepancy is limited, and does not seem to be incremental during the\ngeneration. Moreover, our analysis reveals an interesting self-recovery ability\nof the LM, which we hypothesize to be countering the harmful effects from\nexposure bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v10 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adjusting for Confounders with Text: Challenges and an Empirical Evaluation Framework for Causal Inference. (arXiv:2009.09961v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.09961","description":"<p>Leveraging text, such as social media posts, for causal inferences requires\nthe use of NLP models to 'learn' and adjust for confounders, which could\notherwise impart bias. However, evaluating such models is challenging, as\nground truth is almost never available. We demonstrate the need for empirical\nevaluation frameworks for causal inference in natural language by showing that\nexisting, commonly used models regularly disagree with one another on real\nworld tasks. We contribute the first such framework, generalizing several\nchallenges across these real world tasks. Using this framework, we evaluate a\nlarge set of commonly used causal inference models based on propensity scores\nand identify their strengths and weaknesses to inform future improvements. We\nmake all tasks, data, and models public to inform applications and encourage\nadditional research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weld_G/0/1/0/all/0/1\">Galen Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glenski_M/0/1/0/all/0/1\">Maria Glenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbour_D/0/1/0/all/0/1\">David Arbour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation. (arXiv:2010.10333v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.10333","description":"<p>Growing interests have been attracted in Conversational Recommender Systems\n(CRS), which explore user preference through conversational interactions in\norder to make appropriate recommendation. However, there is still a lack of\nability in existing CRS to (1) traverse multiple reasoning paths over\nbackground knowledge to introduce relevant items and attributes, and (2)\narrange selected entities appropriately under current system intents to control\nresponse generation. To address these issues, we propose CR-Walker in this\npaper, a model that performs tree-structured reasoning on a knowledge graph,\nand generates informative dialog acts to guide language generation. The unique\nscheme of tree-structured reasoning views the traversed entity at each hop as\npart of dialog acts to facilitate language generation, which links how entities\nare selected and expressed. Automatic and human evaluations show that CR-Walker\ncan arrive at more accurate recommendation, and generate more informative and\nengaging responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenchang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1\">Ryuichi Takanobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where Are You? Localization from Embodied Dialog. (arXiv:2011.08277v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08277","description":"<p>We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans\n-- an Observer and a Locator -- complete a cooperative localization task. The\nObserver is spawned at random in a 3D environment and can navigate from\nfirst-person views while answering questions from the Locator. The Locator must\nlocalize the Observer in a detailed top-down map by asking questions and giving\ninstructions. Based on this dataset, we define three challenging tasks:\nLocalization from Embodied Dialog or LED (localizing the Observer from dialog\nhistory), Embodied Visual Dialog (modeling the Observer), and Cooperative\nLocalization (modeling both agents). In this paper, we focus on the LED task --\nproviding a strong baseline model with detailed ablations characterizing both\ndataset biases and the importance of various modeling choices. Our best model\nachieves 32.7% success at identifying the Observer's location within 3m in\nunseen buildings, vs. 70.4% for human Locators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Meera Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade. (arXiv:2012.14682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14682","description":"<p>Dynamic early exiting aims to accelerate the inference of pre-trained\nlanguage models (PLMs) by emitting predictions in internal layers without\npassing through the entire model. In this paper, we empirically analyze the\nworking mechanism of dynamic early exiting and find that it faces a performance\nbottleneck under high speed-up ratios. On one hand, the PLMs' representations\nin shallow layers lack high-level semantic information and thus are not\nsufficient for accurate predictions. On the other hand, the exiting decisions\nmade by internal classifiers are unreliable, leading to wrongly emitted early\npredictions. We instead propose a new framework for accelerating the inference\nof PLMs, CascadeBERT, which dynamically selects proper-sized and complete\nmodels in a cascading manner, providing comprehensive representations for\npredictions. We further devise a difficulty-aware objective, encouraging the\nmodel to output the class probability that reflects the real difficulty of each\ninstance for a more reliable cascading mechanism. Experimental results show\nthat CascadeBERT can achieve an overall 15\\% improvement under 4$\\times$\nspeed-up compared with existing dynamic early exiting methods on six\nclassification tasks, yielding more calibrated and accurate predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering. (arXiv:2101.00391v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00391","description":"<p>Many Question-Answering (QA) datasets contain unanswerable questions, but\ntheir treatment in QA systems remains primitive. Our analysis of the Natural\nQuestions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion\nof unanswerable questions ($\\sim$21%) can be explained based on the presence of\nunverifiable presuppositions. We discuss the shortcomings of current models in\nhandling such questions, and describe how an improved system could handle them.\nThrough a user preference study, we demonstrate that the oracle behavior of our\nproposed system that provides responses based on presupposition failure is\npreferred over the oracle behavior of existing QA systems. Then we discuss how\nour proposed system could be implemented, presenting a novel framework that\nbreaks down the problem into three steps: presupposition generation,\npresupposition verification and explanation generation. We report our progress\nin tackling each subproblem, and present a preliminary approach to integrating\nthese steps into an existing QA system. We find that adding presuppositions and\ntheir verifiability to an existing model yields modest gains in downstream\nperformance and unanswerability detection. The biggest bottleneck is the\nverification component, which needs to be substantially improved for the\nintegrated system to approach ideal behavior -- even transfer from the best\nentailment models currently falls short.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Najoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayan_B/0/1/0/all/0/1\">Burcu Karagol Ayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_D/0/1/0/all/0/1\">Deepak Ramachandran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDLM: Cross-Document Language Modeling. (arXiv:2101.00406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00406","description":"<p>We introduce a new pretraining approach geared for multi-document language\nmodeling, incorporating two key ideas into the masked language modeling\nself-supervised objective. First, instead of considering documents in\nisolation, we pretrain over sets of multiple related documents, encouraging the\nmodel to learn cross-document relationships. Second, we improve over recent\nlong-range transformers by introducing dynamic global attention that has access\nto the entire input to predict masked tokens. We release CDLM (Cross-Document\nLanguage Model), a new general language model for multi-document setting that\ncan be easily applied to downstream tasks. Our extensive analysis shows that\nboth ideas are essential for the success of CDLM, and work in synergy to set\nnew state-of-the-art results for several multi-text tasks. Code and models are\navailable at https://github.com/aviclu/CDLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04670","description":"<p>Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kristy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results on some few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces,\ni.e., templates, into the input and transform a classification task into a\nmasked language modeling problem. However, as for relation extraction,\ndetermining the appropriate prompt template requires domain expertise. Single\nlabel word handcrafted or auto-searched is cumbersome and time-consuming to\nverify their effectiveness in non-few-shot scenarios. Further, there exist\nabundant semantic knowledge among the entities and relation labels which cannot\nbe ignored. To this end, we focus on incorporating knowledge into prompt-tuning\nfor relation extraction and propose a knowledge-aware prompt-tuning with\nsynergistic optimization (KnowPrompt) approach. Specifically, we inject entity\nand relation knowledge into prompt construction with learnable virtual template\nwords and answer words and jointly optimize their representation with knowledge\nconstraints. Extensive experimental results on five datasets with standard and\nlow-resource settings demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sublanguage: A Serious Issue Affects Pretrained Models in Legal Domain. (arXiv:2104.07782v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07782","description":"<p>Legal English is a sublanguage that is important for everyone but not for\neveryone to understand. Pretrained models have become best practices among\ncurrent deep learning approaches for different problems. It would be a waste or\neven a danger if these models were applied in practice without knowledge of the\nsublanguage of the law. In this paper, we raise the issue and propose a trivial\nsolution by introducing BERTLaw a legal sublanguage pretrained model. The\npaper's experiments demonstrate the superior effectiveness of the method\ncompared to the baseline pretrained model\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding. (arXiv:2104.08145v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08145","description":"<p>Contextualized entity representations learned by state-of-the-art\ntransformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the\nattention mechanism to learn the data context from training data corpus.\nHowever, these models do not use the knowledge context. Knowledge context can\nbe understood as semantics about entities and their relationship with\nneighboring entities in knowledge graphs. We propose a novel and effective\ntechnique to infuse knowledge context from multiple knowledge graphs for\nconceptual and ambiguous entities into TLMs during fine-tuning. It projects\nknowledge graph embeddings in the homogeneous vector-space, introduces new\ntoken-types for entities, aligns entity position ids, and a selective attention\nmechanism. We take BERT as a baseline model and implement the\n\"Knowledge-Infused BERT\" by infusing knowledge context from ConceptNet and\nWordNet, which significantly outperforms BERT and other recent knowledge-aware\nBERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks\nof GLUE benchmark. The KI-BERT-base model even significantly outperforms\nBERT-large for domain-specific tasks like SciTail and academic subsets of QQP,\nQNLI, and MNLI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faldu_K/0/1/0/all/0/1\">Keyur Faldu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikani_P/0/1/0/all/0/1\">Prashant Kikani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1\">Hemang Akbari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation. (arXiv:2104.08724v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08724","description":"<p>Prior studies on text-to-text generation typically assume that the model\ncould figure out what to attend to in the input and what to include in the\noutput via seq2seq learning, with only the parallel training data and no\nadditional guidance. However, it remains unclear whether current models can\npreserve important concepts in the source input, as seq2seq learning does not\nhave explicit focus on the concepts and commonly used evaluation metrics also\ntreat concepts equally important as other tokens. In this paper, we present a\nsystematic analysis that studies whether current seq2seq models, especially\npre-trained language models, are good enough for preserving important input\nconcepts and to what extent explicitly guiding generation with the concepts as\nlexical constraints is beneficial. We answer the above questions by conducting\nextensive analytical experiments on four representative text-to-text generation\ntasks. Based on the observations, we then propose a simple yet effective\nframework to automatically extract, denoise, and enforce important input\nconcepts as lexical constraints. This new method performs comparably or better\nthan its unconstrained counterpart on automatic metrics, demonstrates higher\ncoverage for concept preservation, and receives better ratings in the human\nevaluation. Our code is available at https://github.com/morningmoni/EDE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenchang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_D/0/1/0/all/0/1\">Deren Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Verdi: Quality Estimation and Error Detection for Bilingual Corpora. (arXiv:2105.14878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14878","description":"<p>Translation Quality Estimation is critical to reducing post-editing efforts\nin machine translation and to cross-lingual corpus cleaning. As a research\nproblem, quality estimation (QE) aims to directly estimate the quality of\ntranslation in a given pair of source and target sentences, and highlight the\nwords that need corrections, without referencing to golden translations. In\nthis paper, we propose Verdi, a novel framework for word-level and\nsentence-level post-editing effort estimation for bilingual corpora. Verdi\nadopts two word predictors to enable diverse features to be extracted from a\npair of sentences for subsequent quality estimation, including a\ntransformer-based neural machine translation (NMT) model and a pre-trained\ncross-lingual language model (XLM). We exploit the symmetric nature of\nbilingual corpora and apply model-level dual learning in the NMT predictor,\nwhich handles a primal task and a dual task simultaneously with weight sharing,\nleading to stronger context prediction ability than single-direction NMT\nmodels. By taking advantage of the dual learning scheme, we further design a\nnovel feature to directly encode the translated target information without\nrelying on the source context. Extensive experiments conducted on WMT20 QE\ntasks demonstrate that our method beats the winner of the competition and\noutperforms other baseline methods by a great margin. We further use the\nsentence-level scores provided by Verdi to clean a parallel corpus and observe\nbenefits on both model performance and training efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haijiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoli Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Coreference Resolution with Harmonized Annotations. (arXiv:2107.12088v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12088","description":"<p>In this paper, we present coreference resolution experiments with a newly\ncreated multilingual corpus CorefUD. We focus on the following languages:\nCzech, Russian, Polish, German, Spanish, and Catalan. In addition to\nmonolingual experiments, we combine the training data in multilingual\nexperiments and train two joined models -- for Slavic languages and for all the\nlanguages together. We rely on an end-to-end deep learning model that we\nslightly adapted for the CorefUD corpus. Our results show that we can profit\nfrom harmonized annotations, and using joined models helps significantly for\nthe languages with smaller training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06314","description":"<p>Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and align them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes challenges in the aspect of both temporal understanding and temporal\nreasoning. We evaluate different SoTA long-document QA systems like BigBird and\nFiD on our dataset. The best-performing model FiD can only achieve 46\\%\naccuracy, still far behind the human performance of 87\\%. We demonstrate that\nthese models are still lacking the ability to perform consistent temporal\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\ndevelop NLP models more sensitive to temporal shift. The dataset and code are\nreleased in~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Decoding for Empathetic Response Generation. (arXiv:2108.08102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08102","description":"<p>Understanding speaker's feelings and producing appropriate responses with\nemotion connection is a key communicative skill for empathetic dialogue\nsystems. In this paper, we propose a simple technique called Affective Decoding\nfor empathetic response generation. Our method can effectively incorporate\nemotion signals during each decoding step, and can additionally be augmented\nwith an auxiliary dual emotion encoder, which learns separate embeddings for\nthe speaker and listener given the emotion base of the dialogue. Extensive\nempirical studies show that our models are perceived to be more empathetic by\nhuman evaluations, in comparison to several strong mainstream methods for\nempathetic responding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chengkun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on five public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work claims, our auxiliary experiments suggest that relation\nprediction is contributory to named entity prediction in a non-negligible way.\nThe source code can be found at https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications since the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current algorithms can tackle such\nproblem reliably in a realistic scenario where zero OOD training data is\navailable. In this study, we propose ProtoInfoMax, a new architecture that\nextends Prototypical Networks to simultaneously process In-Domain (ID) and OOD\nsentences via Mutual Information Maximization (InfoMax) objective. Experimental\nresults show that our proposed method can substantially improve performance up\nto 20% for OOD detection in low resource settings of text classification. We\nalso show that ProtoInfoMax is less prone to typical over-confidence Error of\nNeural Networks, leading to more reliable ID and OOD prediction outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEREL: A Russian Dataset with Nested Named Entities, Relations and Events. (arXiv:2108.13112v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13112","description":"<p>In this paper, we present NEREL, a Russian dataset for named entity\nrecognition and relation extraction. NEREL is significantly larger than\nexisting Russian datasets: to date it contains 56K annotated named entities and\n39K annotated relations. Its important difference from previous datasets is\nannotation of nested named entities, as well as relations within nested\nentities and at the discourse level. NEREL can facilitate development of novel\nmodels that can extract relations between nested named entities, as well as\nrelations on both sentence and document levels. NEREL also contains the\nannotation of events involving named entities and their roles in the events.\nThe NEREL collection is available via https://github.com/nerel-ds/NEREL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukachevitch_N/0/1/0/all/0/1\">Natalia Loukachevitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batura_T/0/1/0/all/0/1\">Tatiana Batura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_I/0/1/0/all/0/1\">Ilia Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_V/0/1/0/all/0/1\">Vladimir Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1\">Suresh Manandhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugachev_A/0/1/0/all/0/1\">Alexander Pugachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00627","description":"<p>Contextual knowledge is important for real-world automatic speech recognition\n(ASR) applications. In this paper, a novel tree-constrained pointer generator\n(TCPGen) component is proposed that incorporates such knowledge as a list of\nbiasing words into both attention-based encoder-decoder and transducer\nend-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing\nwords into an efficient prefix tree to serve as its symbolic input and creates\na neural shortcut between the tree and the final ASR output distribution to\nfacilitate recognising biasing words during decoding. Systems were trained and\nevaluated on the Librispeech corpus where biasing words were extracted at the\nscales of an utterance, a chapter, or a book to simulate different application\nscenarios. Experimental results showed that TCPGen consistently improved word\nerror rates (WERs) compared to the baselines, and in particular, achieved\nsignificant WER reductions on the biasing words. TCPGen is highly efficient: it\ncan handle 5,000 biasing words and distractors and only add a small overhead to\nmemory use and computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training. (arXiv:2109.00993v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00993","description":"<p>Large Transformer-based language models such as BERT have led to broad\nperformance improvements on many NLP tasks. Domain-specific variants of these\nmodels have demonstrated excellent performance on a variety of specialised\ntasks. In legal NLP, BERT-based models have led to new state-of-the-art results\non multiple tasks. The exploration of these models has demonstrated the\nimportance of capturing the specificity of the legal language and its\nvocabulary. However, such approaches suffer from high computational costs,\nleading to a higher ecological impact and lower accessibility. Our findings,\nfocusing on English language legal text, show that lightweight LSTM-based\nLanguage Models are able to capture enough information from a small legal text\npretraining corpus and achieve excellent performance on short legal text\nclassification tasks. This is achieved with a significantly reduced\ncomputational overhead compared to BERT-based models. However, our method also\nshows degraded performance on a more complex task, multi-label classification\nof longer documents, highlighting the limitations of this lightweight approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheewala_A/0/1/0/all/0/1\">Akshita Gheewala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briton_P/0/1/0/all/0/1\">Paul Briton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonsus_M/0/1/0/all/0/1\">Marc Alphonsus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laabiyad_R/0/1/0/all/0/1\">Rym Laabiyad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_F/0/1/0/all/0/1\">Francesco Piccoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Optimal Target Shape for LiDAR Pose Estimation. (arXiv:2109.01181v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01181","description":"<p>Targets are essential in problems such as object tracking in cluttered or\ntextureless environments, camera (and multi-sensor) calibration tasks, and\nsimultaneous localization and mapping (SLAM). Target shapes for these tasks\ntypically are symmetric (square, rectangular, or circular) and work well for\nstructured, dense sensor data such as pixel arrays (i.e., image). However,\nsymmetric shapes lead to pose ambiguity when using sparse sensor data such as\nLiDAR point clouds and suffer from the quantization uncertainty of the LiDAR.\nThis paper introduces the concept of optimizing target shape to remove pose\nambiguity for LiDAR point clouds. A target is designed to induce large\ngradients at edge points under rotation and translation relative to the LiDAR\nto ameliorate the quantization uncertainty associated with point cloud\nsparseness. Moreover, given a target shape, we present a means that leverages\nthe target's geometry to estimate the target's vertices while globally\nestimating the pose. Both the simulation and the experimental results (verified\nby a motion capture system) confirm that by using the optimal shape and the\nglobal solver, we achieve centimeter error in translation and a few degrees in\nrotation even when a partially illuminated target is placed 30 meters away. All\nthe implementations and datasets are available at\nhttps://github.com/UMich-BipedLab/optimal_shape_global_pose_estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiunn-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_W/0/1/0/all/0/1\">William Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1\">Jessy W. Grizzle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"roadscene2vec: A Tool for Extracting and Embedding Road Scene-Graphs. (arXiv:2109.01183v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01183","description":"<p>Recently, road scene-graph representations used in conjunction with graph\nlearning techniques have been shown to outperform state-of-the-art deep\nlearning techniques in tasks including action classification, risk assessment,\nand collision prediction. To enable the exploration of applications of road\nscene-graph representations, we introduce roadscene2vec: an open-source tool\nfor extracting and embedding road scene-graphs. The goal of roadscene2vec is to\nenable research into the applications and capabilities of road scene-graphs by\nproviding tools for generating scene-graphs, graph learning models to generate\nspatio-temporal scene-graph embeddings, and tools for visualizing and analyzing\nscene-graph-based methodologies. The capabilities of roadscene2vec include (i)\ncustomized scene-graph generation from either video clips or data from the\nCARLA simulator, (ii) multiple configurable spatio-temporal graph embedding\nmodels and baseline CNN-based models, (iii) built-in functionality for using\ngraph and sequence embeddings for risk assessment and collision prediction\napplications, (iv) tools for evaluating transfer learning, and (v) utilities\nfor visualizing scene-graphs and analyzing the explainability of graph learning\nmodels. We demonstrate the utility of roadscene2vec for these use cases with\nexperimental results and qualitative evaluations for both graph learning models\nand CNN-based models. roadscene2vec is available at\nhttps://github.com/AICPS/roadscene2vec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malawade_A/0/1/0/all/0/1\">Arnav Vaibhav Malawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shih-Yuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_B/0/1/0/all/0/1\">Brandon Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaeley_H/0/1/0/all/0/1\">Harsimrat Kaeley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karra_A/0/1/0/all/0/1\">Anurag Karra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remote Multilinear Compressive Learning with Adaptive Compression. (arXiv:2109.01184v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01184","description":"<p>Multilinear Compressive Learning (MCL) is an efficient signal acquisition and\nlearning paradigm for multidimensional signals. The level of signal compression\naffects the detection or classification performance of a MCL model, with higher\ncompression rates often associated with lower inference accuracy. However,\nhigher compression rates are more amenable to a wider range of applications,\nespecially those that require low operating bandwidth and minimal energy\nconsumption such as Internet-of-Things (IoT) applications. Many communication\nprotocols provide support for adaptive data transmission to maximize the\nthroughput and minimize energy consumption. By developing compressive sensing\nand learning models that can operate with an adaptive compression rate, we can\nmaximize the informational content throughput of the whole application. In this\npaper, we propose a novel optimization scheme that enables such a feature for\nMCL models. Our proposal enables practical implementation of adaptive\ncompressive signal acquisition and inference systems. Experimental results\ndemonstrated that the proposed approach can significantly reduce the amount of\ncomputations required during the training phase of remote learning systems but\nalso improve the informational content throughput via adaptive-rate sensing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dat Thanh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Reliable, Self-Adaptive Face Identification Framework via Lyapunov Optimization. (arXiv:2109.01212v1 [cs.DC])","link":"http://arxiv.org/abs/2109.01212","description":"<p>Realtime face identification (FID) from a video feed is highly\ncomputation-intensive, and may exhaust computation resources if performed on a\ndevice with a limited amount of resources (e.g., a mobile device). In general,\nFID performs better when images are sampled at a higher rate, minimizing false\nnegatives. However, performing it at an overwhelmingly high rate exposes the\nsystem to the risk of a queue overflow that hampers the system's reliability.\nThis paper proposes a novel, queue-aware FID framework that adapts the sampling\nrate to maximize the FID performance while avoiding a queue overflow by\nimplementing the Lyapunov optimization. A preliminary evaluation via a\ntrace-based simulation confirms the effectiveness of the framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dohyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1\">Jae young Bang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepTracks: Geopositioning Maritime Vehicles in Video Acquired from a Moving Platform. (arXiv:2109.01235v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01235","description":"<p>Geopositioning and tracking a moving boat at sea is a very challenging\nproblem, requiring boat detection, matching and estimating its GPS location\nfrom imagery with no common features. The problem can be stated as follows:\ngiven imagery from a camera mounted on a moving platform with known GPS\nlocation as the only valid sensor, we predict the geoposition of a target boat\nvisible in images. Our solution uses recent ML algorithms, the camera-scene\ngeometry and Bayesian filtering. The proposed pipeline first detects and tracks\nthe target boat's location in the image with the strategy of tracking by\ndetection. This image location is then converted to geoposition to the local\nsea coordinates referenced to the camera GPS location using plane projective\ngeometry. Finally, target boat local coordinates are transformed to global GPS\ncoordinates to estimate the geoposition. To achieve a smooth geotrajectory, we\napply unscented Kalman filter (UKF) which implicitly overcomes small detection\nerrors in the early stages of the pipeline. We tested the performance of our\napproach using GPS ground truth and show the accuracy and speed of the\nestimated geopositions. Our code is publicly available at\nhttps://github.com/JianliWei1995/AI-Track-at-Sea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jianli Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guanyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Shifts for Crop Mapping: Leveraging Aggregate Crop Statistics to Improve Satellite-based Maps in New Regions. (arXiv:2109.01246v1 [stat.AP])","link":"http://arxiv.org/abs/2109.01246","description":"<p>Crop type mapping at the field level is critical for a variety of\napplications in agricultural monitoring, and satellite imagery is becoming an\nincreasingly abundant and useful raw input from which to create crop type maps.\nStill, in many regions crop type mapping with satellite data remains\nconstrained by a scarcity of field-level crop labels for training supervised\nclassification models. When training data is not available in one region,\nclassifiers trained in similar regions can be transferred, but shifts in the\ndistribution of crop types as well as transformations of the features between\nregions lead to reduced classification accuracy. We present a methodology that\nuses aggregate-level crop statistics to correct the classifier by accounting\nfor these two types of shifts. To adjust for shifts in the crop type\ncomposition we present a scheme for properly reweighting the posterior\nprobabilities of each class that are output by the classifier. To adjust for\nshifts in features we propose a method to estimate and remove linear shifts in\nthe mean feature vector. We demonstrate that this methodology leads to\nsubstantial improvements in overall classification accuracy when using Linear\nDiscriminant Analysis (LDA) to map crop types in Occitanie, France and in\nWestern Province, Kenya. When using LDA as our base classifier, we found that\nin France our methodology led to percent reductions in misclassifications\nranging from 2.8% to 42.2% (mean = 21.9%) over eleven different training\ndepartments, and in Kenya the percent reductions in misclassification were\n6.6%, 28.4%, and 42.7% for three training regions. While our methodology was\nstatistically motivated by the LDA classifier, it can be applied to any type of\nclassifier. As an example, we demonstrate its successful application to improve\na Random Forest classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Kluger_D/0/1/0/all/0/1\">Dan M. Kluger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Sherrie Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lobell_D/0/1/0/all/0/1\">David B. Lobell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAP-Net: Correspondence-Aware Point-view Fusion Network for 3D Shape Analysis. (arXiv:2109.01291v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01291","description":"<p>Learning 3D representations by fusing point cloud and multi-view data has\nbeen proven to be fairly effective. While prior works typically focus on\nexploiting global features of the two modalities, in this paper we argue that\nmore discriminative features can be derived by modeling \"where to fuse\". To\ninvestigate this, we propose a novel Correspondence-Aware Point-view Fusion Net\n(CAPNet). The core element of CAP-Net is a module named Correspondence-Aware\nFusion (CAF) which integrates the local features of the two modalities based on\ntheir correspondence scores. We further propose to filter out correspondence\nscores with low values to obtain salient local correspondences, which reduces\nredundancy for the fusion process. In our CAP-Net, we utilize the CAF modules\nto fuse the multi-scale features of the two modalities both bidirectionally and\nhierarchically in order to obtain more informative features. Comprehensive\nevaluations on popular 3D shape benchmarks covering 3D object classification\nand retrieval show the superiority of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Silin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Symmetry Matters: A Modal-Alternating Propagation Network for Few-Shot Learning. (arXiv:2109.01295v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01295","description":"<p>Semantic information provides intra-class consistency and inter-class\ndiscriminability beyond visual concepts, which has been employed in Few-Shot\nLearning (FSL) to achieve further gains. However, semantic information is only\navailable for labeled samples but absent for unlabeled samples, in which the\nembeddings are rectified unilaterally by guiding the few labeled samples with\nsemantics. Therefore, it is inevitable to bring a cross-modal bias between\nsemantic-guided samples and nonsemantic-guided samples, which results in an\ninformation asymmetry problem. To address this problem, we propose a\nModal-Alternating Propagation Network (MAP-Net) to supplement the absent\nsemantic information of unlabeled samples, which builds information symmetry\namong all samples in both visual and semantic modalities. Specifically, the\nMAP-Net transfers the neighbor information by the graph propagation to generate\nthe pseudo-semantics for unlabeled samples guided by the completed visual\nrelationships and rectify the feature embeddings. In addition, due to the large\ndiscrepancy between visual and semantic modalities, we design a Relation\nGuidance (RG) strategy to guide the visual relation vectors via semantics so\nthat the propagated information is more beneficial. Extensive experimental\nresults on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011,\nSUN Attribute Database, and Oxford 102 Flower, have demonstrated that our\nproposed method achieves promising performance and outperforms the\nstate-of-the-art approaches, which indicates the necessity of information\nsymmetry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhishen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yanwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Taught Cross-Domain Few-Shot Learning with Weakly Supervised Object Localization and Task-Decomposition. (arXiv:2109.01302v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01302","description":"<p>The domain shift between the source and target domain is the main challenge\nin Cross-Domain Few-Shot Learning (CD-FSL). However, the target domain is\nabsolutely unknown during the training on the source domain, which results in\nlacking directed guidance for target tasks. We observe that since there are\nsimilar backgrounds in target domains, it can apply self-labeled samples as\nprior tasks to transfer knowledge onto target tasks. To this end, we propose a\ntask-expansion-decomposition framework for CD-FSL, called Self-Taught (ST)\napproach, which alleviates the problem of non-target guidance by constructing\ntask-oriented metric spaces. Specifically, Weakly Supervised Object\nLocalization (WSOL) and self-supervised technologies are employed to enrich\ntask-oriented samples by exchanging and rotating the discriminative regions,\nwhich generates a more abundant task set. Then these tasks are decomposed into\nseveral tasks to finish the task of few-shot recognition and rotation\nclassification. It helps to transfer the source knowledge onto the target tasks\nand focus on discriminative regions. We conduct extensive experiments under the\ncross-domain setting including 8 target domains: CUB, Cars, Places, Plantae,\nCropDieases, EuroSAT, ISIC, and ChestX. Experimental results demonstrate that\nthe proposed ST approach is applicable to various metric-based models, and\nprovides promising improvements in CD-FSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yanwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongfei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-centred Strong Augmentation via Contrastive Learning for Unsupervised Lesion Detection and Segmentation. (arXiv:2109.01303v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01303","description":"<p>The scarcity of high quality medical image annotations hinders the\nimplementation of accurate clinical applications for detecting and segmenting\nabnormal lesions. To mitigate this issue, the scientific community is working\non the development of unsupervised anomaly detection (UAD) systems that learn\nfrom a training set containing only normal (i.e., healthy) images, where\nabnormal samples (i.e., unhealthy) are detected and segmented based on how much\nthey deviate from the learned distribution of normal samples. One significant\nchallenge faced by UAD methods is how to learn effective low-dimensional image\nrepresentations that are sensitive enough to detect and segment abnormal\nlesions of varying size, appearance and shape. To address this challenge, we\npropose a novel self-supervised UAD pre-training algorithm, named Multi-centred\nStrong Augmentation via Contrastive Learning (MSACL). MSACL learns\nrepresentations by separating several types of strong and weak augmentations of\nnormal image samples, where the weak augmentations represent normal images and\nstrong augmentations denote synthetic abnormal images. To produce such strong\naugmentations, we introduce MedMix, a novel data augmentation strategy that\ncreates new training images with realistic looking lesions (i.e., anomalies) in\nnormal images. The pre-trained representations from MSACL are generic and can\nbe used to improve the efficacy of different types of off-the-shelf\nstate-of-the-art (SOTA) UAD models. Comprehensive experimental results show\nthat the use of MSACL largely improves these SOTA UAD models on four medical\nimaging datasets from diverse organs, namely colonoscopy, fundus screening and\ncovid-19 chest-ray datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W. Verjans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition. (arXiv:2109.01305v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01305","description":"<p>Human pose is a useful feature for fine-grained sports action understanding.\nHowever, pose estimators are often unreliable when run on sports video due to\ndomain shift and factors such as motion blur and occlusions. This leads to poor\naccuracy when downstream tasks, such as action recognition, depend on pose.\nEnd-to-end learning circumvents pose, but requires more labels to generalize.\n</p>\n<p>We introduce Video Pose Distillation (VPD), a weakly-supervised technique to\nlearn features for new video domains, such as individual sports that challenge\npose estimation. Under VPD, a student network learns to extract robust pose\nfeatures from RGB frames in the sports video, such that, whenever pose is\nconsidered reliable, the features match the output of a pretrained teacher pose\ndetector. Our strategy retains the best of both pose and end-to-end worlds,\nexploiting the rich visual patterns in raw video frames, while learning\nfeatures that agree with the athletes' pose and motion in the target video\ndomain to avoid over-fitting to patterns unrelated to athletes' motion.\n</p>\n<p>VPD features improve performance on few-shot, fine-grained action\nrecognition, retrieval, and detection tasks in four real-world sports video\ndatasets, without requiring additional ground-truth pose annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">James Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1\">Micha&#xeb;l Gharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatahalian_K/0/1/0/all/0/1\">Kayvon Fatahalian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised multi-latent space reinforcement learning framework for video summarization in ultrasound imaging. (arXiv:2109.01309v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01309","description":"<p>The COVID-19 pandemic has highlighted the need for a tool to speed up triage\nin ultrasound scans and provide clinicians with fast access to relevant\ninformation. The proposed video-summarization technique is a step in this\ndirection that provides clinicians access to relevant key-frames from a given\nultrasound scan (such as lung ultrasound) while reducing resource, storage and\nbandwidth requirements. We propose a new unsupervised reinforcement learning\n(RL) framework with novel rewards that facilitates unsupervised learning\navoiding tedious and impractical manual labelling for summarizing ultrasound\nvideos to enhance its utility as a triage tool in the emergency department (ED)\nand for use in telemedicine. Using an attention ensemble of encoders, the high\ndimensional image is projected into a low dimensional latent space in terms of:\na) reduced distance with a normal or abnormal class (classifier encoder), b)\nfollowing a topology of landmarks (segmentation encoder), and c) the distance\nor topology agnostic latent representation (convolutional autoencoders). The\ndecoder is implemented using a bi-directional long-short term memory (Bi-LSTM)\nwhich utilizes the latent space representation from the encoder. Our new\nparadigm for video summarization is capable of delivering classification labels\nand segmentation of key landmarks for each of the summarized keyframes.\nValidation is performed on lung ultrasound (LUS) dataset, that typically\nrepresent potential use cases in telemedicine and ED triage acquired from\ndifferent medical centers across geographies (India, Spain and Canada).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mathews_R/0/1/0/all/0/1\">Roshan P Mathews</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panicker_M/0/1/0/all/0/1\">Mahesh Raveendranatha Panicker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash R Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yale Tung Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buchanan_B/0/1/0/all/0/1\">Brian Buchanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayan_K/0/1/0/all/0/1\">Kiran Vishnu Narayan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+C_K/0/1/0/all/0/1\">Kesavadas C</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mathews_G/0/1/0/all/0/1\">Greeta Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation on VSPW Dataset through Aggregation of Transformer Models. (arXiv:2109.01316v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01316","description":"<p>Semantic segmentation is an important task in computer vision, from which\nsome important usage scenarios are derived, such as autonomous driving, scene\nparsing, etc. Due to the emphasis on the task of video semantic segmentation,\nwe participated in this competition. In this report, we briefly introduce the\nsolutions of team 'BetterThing' for the ICCV2021 - Video Scene Parsing in the\nWild Challenge. Transformer is used as the backbone for extracting video frame\nfeatures, and the final result is the aggregation of the output of two\nTransformer models, SWIN and VOLO. This solution achieves 57.3% mIoU, which is\nranked 3rd place in the Video Scene Parsing in the Wild Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junhong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaotao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Access Control Using Spatially Invariant Permutation of Feature Maps for Semantic Segmentation Models. (arXiv:2109.01332v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01332","description":"<p>In this paper, we propose an access control method that uses the spatially\ninvariant permutation of feature maps with a secret key for protecting semantic\nsegmentation models. Segmentation models are trained and tested by permuting\nselected feature maps with a secret key. The proposed method allows rightful\nusers with the correct key not only to access a model to full capacity but also\nto degrade the performance for unauthorized users. Conventional access control\nmethods have focused only on image classification tasks, and these methods have\nnever been applied to semantic segmentation tasks. In an experiment, the\nprotected models were demonstrated to allow rightful users to obtain almost the\nsame performance as that of non-protected models but also to be robust against\naccess by unauthorized users without a key. In addition, a conventional method\nwith block-wise transformations was also verified to have degraded performance\nunder semantic segmentation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1\">Hiroki Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AprilPyone_M/0/1/0/all/0/1\">MaungMaung AprilPyone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Camera Super-Resolution with Aligned Attention Modules. (arXiv:2109.01349v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01349","description":"<p>We present a novel approach to reference-based super-resolution (RefSR) with\nthe focus on dual-camera super-resolution (DCSR), which utilizes reference\nimages for high-quality and high-fidelity results. Our proposed method\ngeneralizes the standard patch-based feature matching with spatial alignment\noperations. We further explore the dual-camera super-resolution that is one\npromising application of RefSR, and build a dataset that consists of 146 image\npairs from the main and telephoto cameras in a smartphone. To bridge the domain\ngaps between real-world images and the training images, we propose a\nself-supervised domain adaptation strategy for real-world images. Extensive\nexperiments on our dataset and a public benchmark demonstrate clear improvement\nachieved by our method over state of the art in both quantitative evaluation\nand visual comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatially varying white balancing for mixed and non-uniform illuminants. (arXiv:2109.01350v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01350","description":"<p>In this paper, we propose a novel white balance adjustment, called \"spatially\nvarying white balancing,\" for single, mixed, and non-uniform illuminants. By\nusing n diagonal matrices along with a weight, the proposed method can reduce\nlighting effects on all spatially varying colors in an image under such\nillumination conditions. In contrast, conventional white balance adjustments do\nnot consider the correcting of all colors except under a single illuminant.\nAlso, multi-color balance adjustments can map multiple colors into\ncorresponding ground truth colors, although they may cause the rank deficiency\nproblem to occur as a non-diagonal matrix is used, unlike white balancing. In\nan experiment, the effectiveness of the proposed method is shown under mixed\nand non-uniform illuminants, compared with conventional white and multi-color\nbalancing. Moreover, under a single illuminant, the proposed method has almost\nthe same performance as the conventional white balancing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akazawa_T/0/1/0/all/0/1\">Teruaki Akazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1\">Yuma Kinoshita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MitoVis: A Visually-guided Interactive Intelligent System for Neuronal Mitochondria Analysis. (arXiv:2109.01351v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01351","description":"<p>Neurons have a polarized structure, including dendrites and axons, and\ncompartment-specific functions can be affected by dwelling mitochondria. It is\nknown that the morphology of mitochondria is closely related to the functions\nof neurons and neurodegenerative diseases. Even though several deep learning\nmethods have been developed to automatically analyze the morphology of\nmitochondria, the application of existing methods to actual analysis still\nencounters several difficulties. Since the performance of pre-trained deep\nlearning model may vary depending on the target data, re-training of the model\nis often required. Besides, even though deep learning has shown superior\nperformance under a constrained setup, there are always errors that need to be\ncorrected by humans in real analysis. To address these issues, we introduce\nMitoVis, a novel visualization system for end-to-end data processing and\ninteractive analysis of the morphology of neuronal mitochondria. MitoVis\nenables interactive fine-tuning of a pre-trained neural network model without\nthe domain knowledge of machine learning, which allows neuroscientists to\neasily leverage deep learning in their research. MitoVis also provides novel\nvisual guides and interactive proofreading functions so that the users can\nquickly identify and correct errors in the result with minimal effort. We\ndemonstrate the usefulness and efficacy of the system via a case study\nconducted by a neuroscientist on a real analysis scenario. The result shows\nthat MitoVis allows up to 15x faster analysis with similar accuracy compared to\nthe fully manual analysis method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">JunYoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hakjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Suyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Seok-Kyu Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1\">Won-Ki Jeong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-featured Graph Neural Architecture Search. (arXiv:2109.01356v1 [cs.LG])","link":"http://arxiv.org/abs/2109.01356","description":"<p>Graph neural networks (GNNs) have been successfully applied to learning\nrepresentation on graphs in many relational tasks. Recently, researchers study\nneural architecture search (NAS) to reduce the dependence of human expertise\nand explore better GNN architectures, but they over-emphasize entity features\nand ignore latent relation information concealed in the edges. To solve this\nproblem, we incorporate edge features into graph search space and propose\nEdge-featured Graph Neural Architecture Search to find the optimal GNN\narchitecture. Specifically, we design rich entity and edge updating operations\nto learn high-order representations, which convey more generic message passing\nmechanisms. Moreover, the architecture topology in our search space allows to\nexplore complex feature dependence of both entities and edges, which can be\nefficiently optimized by differentiable search strategy. Experiments at three\ngraph tasks on six datasets show EGNAS can search better GNNs with higher\nperformance than current state-of-the-art human-designed and searched-based\nGNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Learning Spatially Discriminative Feature Representations. (arXiv:2109.01359v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01359","description":"<p>The backbone of traditional CNN classifier is generally considered as a\nfeature extractor, followed by a linear layer which performs the\nclassification. We propose a novel loss function, termed as CAM-loss, to\nconstrain the embedded feature maps with the class activation maps (CAMs) which\nindicate the spatially discriminative regions of an image for particular\ncategories. CAM-loss drives the backbone to express the features of target\ncategory and suppress the features of non-target categories or background, so\nas to obtain more discriminative feature representations. It can be simply\napplied in any CNN architecture with neglectable additional parameters and\ncalculations. Experimental results show that CAM-loss is applicable to a\nvariety of network structures and can be combined with mainstream\nregularization methods to improve the performance of image classification. The\nstrong generalization ability of CAM-loss is validated in the transfer learning\nand few shot learning tasks. Based on CAM-loss, we also propose a novel\nCAAM-CAM matching knowledge distillation method. This method directly uses the\nCAM generated by the teacher network to supervise the CAAM generated by the\nstudent network, which effectively improves the accuracy and convergence rate\nof the student network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiayu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yizeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qisen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Fitness. (arXiv:2109.01376v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01376","description":"<p>We present Fitness tutor, an application for maintaining correct posture\nduring workout exercises or doing yoga. Current work on fitness focuses on\nsuggesting food supplements, accessing workouts, workout wearables does a great\njob in improving the fitness. Meanwhile, the current situation is making\ndifficult to monitor workouts by trainee. Inspired by healthcare innovations\nlike robotic surgery, we design a novel application Fitness tutor which can\nguide the workouts using pose estimation. Pose estimation can be deployed on\nthe reference image for gathering data and guide the user with the data. This\nallow Fitness tutor to guide the workouts (both exercise and yoga) in remote\nconditions with a single reference posture as image. We use posenet model in\ntensorflow with p5js for developing skeleton. Fitness tutor is an application\nof pose estimation model in bringing a realtime teaching experience in fitness.\nOur experiments shows that it can leverage potential of pose estimation models\nby providing guidance in realtime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+N_M/0/1/0/all/0/1\">Mahendran N</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of turbulent computational fluid dynamics simulations with unsupervised ensemble learning. (arXiv:2109.01381v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01381","description":"<p>Computer vision and machine learning tools offer an exciting new way for\nautomatically analyzing and categorizing information from complex computer\nsimulations. Here we design an ensemble machine learning framework that can\nindependently and robustly categorize and dissect simulation data output\ncontents of turbulent flow patterns into distinct structure catalogues. The\nsegmentation is performed using an unsupervised clustering algorithm, which\nsegments physical structures by grouping together similar pixels in simulation\nimages. The accuracy and robustness of the resulting segment region boundaries\nare enhanced by combining information from multiple simultaneously-evaluated\nclustering operations. The stacking of object segmentation evaluations is\nperformed using image mask combination operations. This statistically-combined\nensemble (SCE) of different cluster masks allows us to construct cluster\nreliability metrics for each pixel and for the associated segments without any\nprior user input. By comparing the similarity of different cluster occurrences\nin the ensemble, we can also assess the optimal number of clusters needed to\ndescribe the data. Furthermore, by relying on ensemble-averaged spatial segment\nregion boundaries, the SCE method enables reconstruction of more accurate and\nrobust region of interest (ROI) boundaries for the different image data\nclusters. We apply the SCE algorithm to 2-dimensional simulation data snapshots\nof magnetically-dominated fully-kinetic turbulent plasma flows where accurate\nROI boundaries are needed for geometrical measurements of intermittent flow\nstructures known as current sheets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bussov_M/0/1/0/all/0/1\">Maarja Bussov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nattila_J/0/1/0/all/0/1\">Joonas N&#xe4;ttil&#xe4;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion-Invariant Rotation-Equivariant Semi-Supervised Depth Based Cross-View Gait Pose Estimation. (arXiv:2109.01397v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01397","description":"<p>Accurate estimation of three-dimensional human skeletons from depth images\ncan provide important metrics for healthcare applications, especially for\nbiomechanical gait analysis. However, there exist inherent problems associated\nwith depth images captured from a single view. The collected data is greatly\naffected by occlusions where only partial surface data can be recorded.\nFurthermore, depth images of human body exhibit heterogeneous characteristics\nwith viewpoint changes, and the estimated poses under local coordinate systems\nare expected to go through equivariant rotations. Most existing pose estimation\nmodels are sensitive to both issues. To address this, we propose a novel\napproach for cross-view generalization with an occlusion-invariant\nsemi-supervised learning framework built upon a novel rotation-equivariant\nbackbone. Our model was trained with real-world data from a single view and\nunlabelled synthetic data from multiple views. It can generalize well on the\nreal-world data from all the other unseen views. Our approach has shown\nsuperior performance on gait analysis on our ICL-Gait dataset compared to other\nstate-of-the-arts and it can produce more convincing keypoints on ITOP dataset,\nthan its provided \"ground truth\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank Po Wen Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models. (arXiv:2109.01401v1 [cs.AI])","link":"http://arxiv.org/abs/2109.01401","description":"<p>We propose CX-ToM, short for counterfactual explanations with theory-of mind,\na new explainable AI (XAI) framework for explaining decisions made by a deep\nconvolutional neural network (CNN). In contrast to the current methods in XAI\nthat generate explanations as a single shot response, we pose explanation as an\niterative communication process, i.e. dialog, between the machine and human\nuser. More concretely, our CX-ToM framework generates sequence of explanations\nin a dialog by mediating the differences between the minds of machine and human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. Moreover, most state-of-the-art XAI\nframeworks provide attention (or heat map) based explanations. In our work, we\nshow that these attention based explanations are not sufficient for increasing\nhuman trust in the underlying CNN model. In CX-ToM, we instead use\ncounterfactual explanations called fault-lines which we define as follows:\ngiven an input image I for which a CNN classification model M predicts class\nc_pred, a fault-line identifies the minimal semantic-level features (e.g.,\nstripes on zebra, pointed ears of dog), referred to as explainable concepts,\nthat need to be added to or deleted from I in order to alter the classification\ncategory of I by M to another specified class c_alt. We argue that, due to the\niterative, conceptual and counterfactual nature of CX-ToM explanations, our\nframework is practical and more natural for both expert and non-expert users to\nunderstand the internal workings of complex deep learning models. Extensive\nquantitative and qualitative experiments verify our hypotheses, demonstrating\nthat our CX-ToM significantly outperforms the state-of-the-art explainable AI\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun R. Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changsong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saba_Sadiya_S/0/1/0/all/0/1\">Sari Saba-Sadiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongjing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorovic_S/0/1/0/all/0/1\">Sinisa Todorovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Approach for Hyperspectral Image Demosaicking, Spectral Correction and High-resolution RGB Reconstruction. (arXiv:2109.01403v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01403","description":"<p>Hyperspectral imaging is one of the most promising techniques for\nintraoperative tissue characterisation. Snapshot mosaic cameras, which can\ncapture hyperspectral data in a single exposure, have the potential to make a\nreal-time hyperspectral imaging system for surgical decision-making possible.\nHowever, optimal exploitation of the captured data requires solving an\nill-posed demosaicking problem and applying additional spectral corrections to\nrecover spatial and spectral information of the image. In this work, we propose\na deep learning-based image demosaicking algorithm for snapshot hyperspectral\nimages using supervised learning methods. Due to the lack of publicly available\nmedical images acquired with snapshot mosaic cameras, a synthetic image\ngeneration approach is proposed to simulate snapshot images from existing\nmedical image datasets captured by high-resolution, but slow, hyperspectral\nimaging devices. Image reconstruction is achieved using convolutional neural\nnetworks for hyperspectral image super-resolution, followed by cross-talk and\nleakage correction using a sensor-specific calibration matrix. The resulting\ndemosaicked images are evaluated both quantitatively and qualitatively, showing\nclear improvements in image quality compared to a baseline demosaicking method\nusing linear interpolation. Moreover, the fast processing time of~45\\,ms of our\nalgorithm to obtain super-resolved RGB or oxygenation saturation maps per image\nframe for a state-of-the-art snapshot mosaic camera demonstrates the potential\nfor its seamless integration into real-time surgical hyperspectral imaging\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Peichao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebner_M/0/1/0/all/0/1\">Michael Ebner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noonan_P/0/1/0/all/0/1\">Philip Noonan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horgan_C/0/1/0/all/0/1\">Conor Horgan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bahl_A/0/1/0/all/0/1\">Anisha Bahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shapey_J/0/1/0/all/0/1\">Jonathan Shapey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Foot Ulcer segmentation Using an Ensemble of Convolutional Neural Networks. (arXiv:2109.01408v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01408","description":"<p>Foot ulcer is a common complication of diabetes mellitus; it is associated\nwith substantial morbidity and mortality and remains a major risk factor for\nlower leg amputation. Extracting accurate morphological features from the foot\nwounds is crucial for proper treatment. Although visual and manual inspection\nby medical professionals is the common approach to extract the features, this\nmethod is subjective and error-prone. Computer-mediated approaches are the\nalternative solutions to segment the lesions and extract related morphological\nfeatures. Among various proposed computer-based approaches for image\nsegmentation, deep learning-based methods and more specifically convolutional\nneural networks (CNN) have shown excellent performances for various image\nsegmentation tasks including medical image segmentation. In this work, we\nproposed an ensemble approach based on two encoder-decoder-based CNN models,\nnamely LinkNet and UNet, to perform foot ulcer segmentation. To deal with\nlimited training samples, we used pre-trained weights (EfficientNetB1 for the\nLinkNet model and EfficientNetB2 for the UNet model) and further pre-training\nby the Medetec dataset. We also applied a number of morphological-based and\ncolour-based augmentation techniques to train the models. We integrated\nfive-fold cross-validation, test time augmentation and result fusion in our\nproposed ensemble approach to boost the segmentation performance. Applied on a\npublicly available foot ulcer segmentation dataset and the MICCAI 2021 Foot\nUlcer Segmentation (FUSeg) Challenge, our method achieved state-of-the-art\ndata-based Dice scores of 92.07% and 88.80%, respectively. Our developed method\nachieved the first rank in the FUSeg challenge leaderboard. The Dockerised\nguideline, inference codes and saved trained models are publicly available in\nthe published GitHub repository:\nhttps://github.com/masih4/Foot_Ulcer_Segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mahbod_A/0/1/0/all/0/1\">Amirreza Mahbod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ecker_R/0/1/0/all/0/1\">Rupert Ecker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ellinger_I/0/1/0/all/0/1\">Isabella Ellinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MitoDet: Simple and robust mitosis detection. (arXiv:2109.01485v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01485","description":"<p>Mitotic figure detection is a challenging task in digital pathology that has\na direct impact on therapeutic decisions. While automated methods often achieve\nacceptable results under laboratory conditions, they frequently fail in the\nclinical deployment phase. This problem can be mainly attributed to a\nphenomenon called domain shift. An important source of a domain shift is\nintroduced by different microscopes and their camera systems, which noticeably\nchange the color representation of digitized images. In this method description\nwe present our submitted algorithm for the Mitosis Domain Generalization\nChallenge, which employs a RetinaNet trained with strong data augmentation and\nachieves an F1 score of 0.7138 on the preliminary test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benz_M/0/1/0/all/0/1\">Michaela Benz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruns_V/0/1/0/all/0/1\">Volker Bruns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuritcyn_P/0/1/0/all/0/1\">Petr Kuritcyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1\">Thomas Wittenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Studying the Effects of Self-Attention for Medical Image Analysis. (arXiv:2109.01486v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01486","description":"<p>When the trained physician interprets medical images, they understand the\nclinical importance of visual features. By applying cognitive attention, they\napply greater focus onto clinically relevant regions while disregarding\nunnecessary features. The use of computer vision to automate the classification\nof medical images is widely studied. However, the standard convolutional neural\nnetwork (CNN) does not necessarily employ subconscious feature relevancy\nevaluation techniques similar to the trained medical specialist and evaluates\nfeatures more generally. Self-attention mechanisms enable CNNs to focus more on\nsemantically important regions or aggregated relevant context with long-range\ndependencies. By using attention, medical image analysis systems can\npotentially become more robust by focusing on more important clinical feature\nregions. In this paper, we provide a comprehensive comparison of various\nstate-of-the-art self-attention mechanisms across multiple medical image\nanalysis tasks. Through both quantitative and qualitative evaluations along\nwith a clinical user-centric survey study, we aim to provide a deeper\nunderstanding of the effects of self-attention in medical computer vision\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rao_A/0/1/0/all/0/1\">Adrit Rao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_S/0/1/0/all/0/1\">Sanghyun Woo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aalami_O/0/1/0/all/0/1\">Oliver Aalami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ghost Loss to Question the Reliability of Training Data. (arXiv:2109.01504v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01504","description":"<p>Supervised image classification problems rely on training data assumed to\nhave been correctly annotated; this assumption underpins most works in the\nfield of deep learning. In consequence, during its training, a network is\nforced to match the label provided by the annotator and is not given the\nflexibility to choose an alternative to inconsistencies that it might be able\nto detect. Therefore, erroneously labeled training images may end up\n``correctly'' classified in classes which they do not actually belong to. This\nmay reduce the performances of the network and thus incite to build more\ncomplex networks without even checking the quality of the training data. In\nthis work, we question the reliability of the annotated datasets. For that\npurpose, we introduce the notion of ghost loss, which can be seen as a regular\nloss that is zeroed out for some predicted values in a deterministic way and\nthat allows the network to choose an alternative to the given label without\nbeing penalized. After a proof of concept experiment, we use the ghost loss\nprinciple to detect confusing images and erroneously labeled images in\nwell-known training datasets (MNIST, Fashion-MNIST, SVHN, CIFAR10) and we\nprovide a new tool, called sanity matrix, for summarizing these confusions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deliege_A/0/1/0/all/0/1\">Adrien Deli&#xe8;ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cioppa_A/0/1/0/all/0/1\">Anthony Cioppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety-aware Motion Prediction with Unseen Vehicles for Autonomous Driving. (arXiv:2109.01510v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01510","description":"<p>Motion prediction of vehicles is critical but challenging due to the\nuncertainties in complex environments and the limited visibility caused by\nocclusions and limited sensor ranges. In this paper, we study a new task,\nsafety-aware motion prediction with unseen vehicles for autonomous driving.\nUnlike the existing trajectory prediction task for seen vehicles, we aim at\npredicting an occupancy map that indicates the earliest time when each location\ncan be occupied by either seen and unseen vehicles. The ability to predict\nunseen vehicles is critical for safety in autonomous driving. To tackle this\nchallenging task, we propose a safety-aware deep learning model with three new\nloss functions to predict the earliest occupancy map. Experiments on the\nlarge-scale autonomous driving nuScenes dataset show that our proposed model\nsignificantly outperforms the state-of-the-art baselines on the safety-aware\nmotion prediction task. To the best of our knowledge, our approach is the first\none that can predict the existence of unseen vehicles in most cases. Project\npage at {\\url{https://github.com/xrenaa/Safety-Aware-Motion-Prediction}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Erran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry. (arXiv:2109.01533v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01533","description":"<p>Extensive research efforts have been dedicated to deep learning based\nodometry. Nonetheless, few efforts are made on the unsupervised deep lidar\nodometry. In this paper, we design a novel framework for unsupervised lidar\nodometry with the IMU, which is never used in other deep methods. First, a pair\nof siamese LSTMs are used to obtain the initial pose from the linear\nacceleration and angular velocity of IMU. With the initial pose, we perform the\nrigid transform on the current frame and align it closer to the last frame.\nThen, we extract vertex and normal features from the transformed point clouds\nand its normals. Next a two-branches attention modules are proposed to estimate\nresidual rotation and translation from the extracted vertex and normal\nfeatures, respectively. Finally, our model outputs the sum of initial and\nresidual poses as the final pose. For unsupervised training, we introduce an\nunsupervised loss function which is employed on the voxelized point clouds. The\nproposed approach is evaluated on the KITTI odometry estimation benchmark and\nachieves comparable performances against other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yiming Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Based Parameter Optimization for Ground Texture Based Localization Methods. (arXiv:2109.01559v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01559","description":"<p>A promising approach to accurate positioning of robots is ground texture\nbased localization. It is based on the observation that visual features of\nground images enable fingerprint-like place recognition. We tackle the issue of\nefficient parametrization of such methods, deriving a prediction model for\nlocalization performance, which requires only a small collection of sample\nimages of an application area. In a first step, we examine whether the model\ncan predict the effects of changing one of the most important parameters of\nfeature-based localization methods: the number of extracted features. We\nexamine two localization methods, and in both cases our evaluation shows that\nthe predictions are sufficiently accurate. Since this model can be used to find\nsuitable values for any parameter, we then present a holistic parameter\noptimization framework, which finds suitable texture-specific parameter\nconfigurations, using only the model to evaluate the considered parameter\nconfigurations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmid_J/0/1/0/all/0/1\">Jan Fabian Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_S/0/1/0/all/0/1\">Stephan F. Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mester_R/0/1/0/all/0/1\">Rudolf Mester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ordinal Pooling. (arXiv:2109.01561v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01561","description":"<p>In the framework of convolutional neural networks, downsampling is often\nperformed with an average-pooling, where all the activations are treated\nequally, or with a max-pooling operation that only retains an element with\nmaximum activation while discarding the others. Both of these operations are\nrestrictive and have previously been shown to be sub-optimal. To address this\nissue, a novel pooling scheme, named\\emph{ ordinal pooling}, is introduced in\nthis work. Ordinal pooling rearranges all the elements of a pooling region in a\nsequence and assigns a different weight to each element based upon its order in\nthe sequence. These weights are used to compute the pooling operation as a\nweighted sum of the rearranged elements of the pooling region. They are learned\nvia a standard gradient-based training, allowing to learn a behavior anywhere\nin the spectrum of average-pooling to max-pooling in a differentiable manner.\nOur experiments suggest that it is advantageous for the networks to perform\ndifferent types of pooling operations within a pooling layer and that a hybrid\nbehavior between average- and max-pooling is often beneficial. More\nimportantly, they also demonstrate that ordinal pooling leads to consistent\nimprovements in the accuracy over average- or max-pooling operations while\nspeeding up the training and alleviating the issue of the choice of the pooling\noperations and activation functions to be used in the networks. In particular,\nordinal pooling mainly helps on lightweight or quantized deep learning\narchitectures, as typically considered e.g. for embedded applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deliege_A/0/1/0/all/0/1\">Adrien Deli&#xe8;ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Istasse_M/0/1/0/all/0/1\">Maxime Istasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashwani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1\">Christophe De Vleeschouwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Metric Learning for Ground Images. (arXiv:2109.01569v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01569","description":"<p>Ground texture based localization methods are potential prospects for\nlow-cost, high-accuracy self-localization solutions for robots. These methods\nestimate the pose of a given query image, i.e. the current observation of the\nground from a downward-facing camera, in respect to a set of reference images\nwhose poses are known in the application area. In this work, we deal with the\ninitial localization task, in which we have no prior knowledge about the\ncurrent robot positioning. In this situation, the localization method would\nhave to consider all available reference images. However, in order to reduce\ncomputational effort and the risk of receiving a wrong result, we would like to\nconsider only those reference images that are actually overlapping with the\nquery image. For this purpose, we propose a deep metric learning approach that\nretrieves the most similar reference images to the query image. In contrast to\nexisting approaches to image retrieval for ground images, our approach achieves\nsignificantly better recall performance and improves the localization\nperformance of a state-of-the-art ground texture based localization method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_R/0/1/0/all/0/1\">Raaghav Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_J/0/1/0/all/0/1\">Jan Fabian Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholz_R/0/1/0/all/0/1\">Randolf Scholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Topological Framework for the Design of Activation Function and Model Pruning in Deep Neural Networks. (arXiv:2109.01572v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01572","description":"<p>Success of deep neural networks in diverse tasks across domains of computer\nvision, speech recognition and natural language processing, has necessitated\nunderstanding the dynamics of training process and also working of trained\nmodels. Two independent contributions of this paper are 1) Novel activation\nfunction for faster training convergence 2) Systematic pruning of filters of\nmodels trained irrespective of activation function. We analyze the topological\ntransformation of the space of training samples as it gets transformed by each\nsuccessive layer during training, by changing the activation function. The\nimpact of changing activation function on the convergence during training is\nreported for the task of binary classification. A novel activation function\naimed at faster convergence for classification tasks is proposed. Here, Betti\nnumbers are used to quantify topological complexity of data. Results of\nexperiments on popular synthetic binary classification datasets with large\nBetti numbers(&gt;150) using MLPs are reported. Results show that the proposed\nactivation function results in faster convergence requiring fewer epochs by a\nfactor of 1.5 to 2, since Betti numbers reduce faster across layers with the\nproposed activation function. The proposed methodology was verified on\nbenchmark image datasets: fashion MNIST, CIFAR-10 and cat-vs-dog images, using\nCNNs. Based on empirical results, we propose a novel method for pruning a\ntrained model. The trained model was pruned by eliminating filters that\ntransform data to a topological space with large Betti numbers. All filters\nwith Betti numbers greater than 300 were removed from each layer without\nsignificant reduction in accuracy. This resulted in faster prediction time and\nreduced memory size of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kochar_Y/0/1/0/all/0/1\">Yogesh Kochar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vengalil_S/0/1/0/all/0/1\">Sunil Kumar Vengalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1\">Neelam Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Human Shape Style Transfer. (arXiv:2109.01587v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01587","description":"<p>We consider the problem of modifying/replacing the shape style of a real\nmoving character with those of an arbitrary static real source character.\nTraditional solutions follow a pose transfer strategy, from the moving\ncharacter to the source character shape, that relies on skeletal pose\nparametrization. In this paper, we explore an alternative approach that\ntransfers the source shape style onto the moving character. The expected\nbenefit is to avoid the inherently difficult pose to shape conversion required\nwith skeletal parametrization applied on real characters. To this purpose, we\nconsider image style transfer techniques and investigate how to adapt them to\n3D human shapes. Adaptive Instance Normalisation (AdaIN) and SPADE\narchitectures have been demonstrated to efficiently and accurately transfer the\nstyle of an image onto another while preserving the original image structure.\nWhere AdaIN contributes with a module to perform style transfer through the\nstatistics of the subjects and SPADE contribute with a residual block\narchitecture to refine the quality of the style transfer. We demonstrate that\nthese approaches are extendable to the 3D shape domain by proposing a\nconvolutional neural network that applies the same principle of preserving the\nshape structure (shape pose) while transferring the style of a new subject\nshape. The generated results are supervised through a discriminator module to\nevaluate the realism of the shape, whilst enforcing the decoder to synthesise\nplausible shapes and improve the style transfer for unseen subjects. Our\nexperiments demonstrate an average of $\\approx 56\\%$ qualitative and\nquantitative improvements over the baseline in shape transfer through\noptimization-based and learning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Regateiro_J/0/1/0/all/0/1\">Joao Regateiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Human Deformation Transfer. (arXiv:2109.01588v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01588","description":"<p>We consider the problem of human deformation transfer, where the goal is to\nretarget poses between different characters. Traditional methods that tackle\nthis problem require a clear definition of the pose, and use this definition to\ntransfer poses between characters. In this work, we take a different approach\nand transform the identity of a character into a new identity without modifying\nthe character's pose. This offers the advantage of not having to define\nequivalences between 3D human poses, which is not straightforward as poses tend\nto change depending on the identity of the character performing them, and as\ntheir meaning is highly contextual. To achieve the deformation transfer, we\npropose a neural encoder-decoder architecture where only identity information\nis encoded and where the decoder is conditioned on the pose. We use pose\nindependent representations, such as isometry-invariant shape characteristics,\nto represent identity features. Our model uses these features to supervise the\nprediction of offsets from the deformed pose to the result of the transfer. We\nshow experimentally that our method outperforms state-of-the-art methods both\nquantitatively and qualitatively, and generalises better to poses not seen\nduring training. We also introduce a fine-tuning step that allows to obtain\ncompetitive results for extreme identities, and allows to transfer simple\nclothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basset_J/0/1/0/all/0/1\">Jean Basset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1\">Adnane Boukhayma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuhrer_S/0/1/0/all/0/1\">Stefanie Wuhrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Multon_F/0/1/0/all/0/1\">Franck Multon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super Neurons. (arXiv:2109.01594v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01594","description":"<p>Operational Neural Networks (ONNs) are new generation network models that can\nperform any (non-linear) transformation with a proper combination of \"nodal\"\nand \"pool\" operators. However, they still have a certain restriction, which is\nthe sole usage of a single nodal operator for all (synaptic) connections of\neach neuron. The idea behind the \"generative neurons\" was born as a remedy for\nthis restriction where each nodal operator can be \"customized\" during the\ntraining in order to maximize the learning performance. Self-Organized ONNs\n(Self-ONNs) composed with the generative neurons can achieve an utmost level of\ndiversity even with a compact configuration; however, it still suffers from the\nlast property that was inherited from the CNNs: localized kernel operations\nwhich imposes a severe limitation to the information flow between layers. It\nis, therefore, desirable for the neurons to gather information from a larger\narea in the previous layer maps without increasing the kernel size. For certain\napplications, it might be even more desirable \"to learn\" the kernel locations\nof each connection during the training process along with the customized nodal\noperators so that both can be optimized simultaneously. This study introduces\nthe super (generative) neuron models that can accomplish this without altering\nthe kernel sizes and will enable a significant diversity in terms of\ninformation flow. The two models of super neurons proposed in this study vary\non the localization process of the kernels: i) randomly localized kernels\nwithin a bias range set for each layer, ii) optimized locations of each kernel\nduring the Back-Propagation (BP) training. The extensive set of comparative\nevaluations show that Self-ONNs with super-neurons can indeed achieve a\nsuperior learning and generalization capability without any significant rise of\nthe computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Junaid Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamac_M/0/1/0/all/0/1\">Mehmet Yamac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guldogan_E/0/1/0/all/0/1\">Esin Guldogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ince_T/0/1/0/all/0/1\">Turker Ince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representing Shape Collections with Alignment-Aware Linear Models. (arXiv:2109.01605v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01605","description":"<p>In this paper, we revisit the classical representation of 3D point clouds as\nlinear shape models. Our key insight is to leverage deep learning to represent\na collection of shapes as affine transformations of low-dimensional linear\nshape models. Each linear model is characterized by a shape prototype, a\nlow-dimensional shape basis and two neural networks. The networks take as input\na point cloud and predict the coordinates of a shape in the linear basis and\nthe affine transformation which best approximate the input. Both linear models\nand neural networks are learned end-to-end using a single reconstruction loss.\nThe main advantage of our approach is that, in contrast to many recent deep\napproaches which learn feature-based complex shape representations, our model\nis explicit and every operation occurs in 3D space. As a result, our linear\nshape models can be easily visualized and annotated, and failure cases can be\nvisually understood. While our main goal is to introduce a compact and\ninterpretable representation of shape collections, we show it leads to state of\nthe art results for few-shot segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loiseau_R/0/1/0/all/0/1\">Romain Loiseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monnier_T/0/1/0/all/0/1\">Tom Monnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Lo&#xef;c Landrieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wildfire smoke plume segmentation using geostationary satellite imagery. (arXiv:2109.01637v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01637","description":"<p>Wildfires have increased in frequency and severity over the past two decades,\nespecially in the Western United States. Beyond physical infrastructure damage\ncaused by these wildfire events, researchers have increasingly identified\nharmful impacts of particulate matter generated by wildfire smoke on\nrespiratory, cardiovascular, and cognitive health. This inference is difficult\ndue to the spatial and temporal uncertainty regarding how much particulate\nmatter is specifically attributable to wildfire smoke. One factor contributing\nto this challenge is the reliance on manually drawn smoke plume annotations,\nwhich are often noisy representations limited to the United States. This work\nuses deep convolutional neural networks to segment smoke plumes from\ngeostationary satellite imagery. We compare the performance of predicted plume\nsegmentations versus the noisy annotations using causal inference methods to\nestimate the amount of variation each explains in Environmental Protection\nAgency (EPA) measured surface level particulate matter &lt;2.5um in diameter\n($\\textrm{PM}_{2.5}$).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jeff Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instabilities in Plug-and-Play (PnP) algorithms from a learned denoiser. (arXiv:2109.01655v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01655","description":"<p>It's well-known that inverse problems are ill-posed and to solve them\nmeaningfully, one has to employ regularization methods. Traditionally, popular\nregularization methods are the penalized Variational approaches. In recent\nyears, the classical regularization approaches have been outclassed by the\nso-called plug-and-play (PnP) algorithms, which copy the proximal gradient\nminimization processes, such as ADMM or FISTA, but with any general denoiser.\nHowever, unlike the traditional proximal gradient methods, the theoretical\nunderpinnings, convergence, and stability results have been insufficient for\nthese PnP-algorithms. Hence, the results obtained from these algorithms, though\nempirically outstanding, can't always be completely trusted, as they may\ncontain certain instabilities or (hallucinated) features arising from the\ndenoiser, especially when using a pre-trained learned denoiser. In fact, in\nthis paper, we show that a PnP-algorithm can induce hallucinated features, when\nusing a pre-trained deep-learning-based (DnCNN) denoiser. We show that such\ninstabilities are quite different than the instabilities inherent to an\nill-posed problem. We also present methods to subdue these instabilities and\nsignificantly improve the recoveries. We compare the advantages and\ndisadvantages of a learned denoiser over a classical denoiser (here, BM3D), as\nwell as, the effectiveness of the FISTA-PnP algorithm vs. the ADMM-PnP\nalgorithm. In addition, we also provide an algorithm to combine these two\ndenoisers, the learned and the classical, in a weighted fashion to produce even\nbetter results. We conclude with numerical results which validate the developed\ntheories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1\">Abinash Nayak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it Raining Outside? Detection of Rainfall using General-Purpose Surveillance Cameras. (arXiv:1908.04034v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1908.04034","description":"<p>In integrated surveillance systems based on visual cameras, the mitigation of\nadverse weather conditions is an active research topic. Within this field, rain\nremoval algorithms have been developed that artificially remove rain streaks\nfrom images or video. In order to deploy such rain removal algorithms in a\nsurveillance setting, one must detect if rain is present in the scene. In this\npaper, we design a system for the detection of rainfall by the use of\nsurveillance cameras. We reimplement the former state-of-the-art method for\nrain detection and compare it against a modern CNN-based method by utilizing 3D\nconvolutions. The two methods are evaluated on our new AAU Visual Rain Dataset\n(VIRADA) that consists of 215 hours of general-purpose surveillance video from\ntwo traffic crossings. The results show that the proposed 3D CNN outperforms\nthe previous state-of-the-art method by a large margin on all metrics, for both\nof the traffic crossings. Finally, it is shown that the choice of\nregion-of-interest has a large influence on performance when trying to\ngeneralize the investigated methods. The AAU VIRADA dataset and our\nimplementation of the two rain detection algorithms are publicly available at\nhttps://bitbucket.org/aauvap/aau-virada.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1\">Joakim Bruslund Haurum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahnsen_C/0/1/0/all/0/1\">Chris H. Bahnsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Generalization: Joint Depth-Pose Learning without PoseNet. (arXiv:2004.01314v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.01314","description":"<p>In this work, we tackle the essential problem of scale inconsistency for\nself-supervised joint depth-pose learning. Most existing methods assume that a\nconsistent scale of depth and pose can be learned across all input samples,\nwhich makes the learning problem harder, resulting in degraded performance and\nlimited generalization in indoor environments and long-sequence visual odometry\napplication. To address this issue, we propose a novel system that explicitly\ndisentangles scale from the network estimation. Instead of relying on PoseNet\narchitecture, our method recovers relative pose by directly solving fundamental\nmatrix from dense optical flow correspondence and makes use of a two-view\ntriangulation module to recover an up-to-scale 3D structure. Then, we align the\nscale of the depth prediction with the triangulated point cloud and use the\ntransformed depth map for depth error computation and dense reprojection check.\nOur whole system can be jointly trained end-to-end. Extensive experiments show\nthat our system not only reaches state-of-the-art performance on KITTI depth\nand flow estimation, but also significantly improves the generalization ability\nof existing self-supervised depth-pose learning methods under a variety of\nchallenging scenarios, and achieves state-of-the-art results among\nself-supervised learning-based methods on KITTI Odometry and NYUv2 dataset.\nFurthermore, we present some interesting findings on the limitation of\nPoseNet-based relative pose estimation methods in terms of generalization\nability. Code is available at https://github.com/B1ueber2y/TrianFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yezhi Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Quantitative Approach for Optimizing Convolutional Neural Networks. (arXiv:2009.05236v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.05236","description":"<p>With the increasing popularity of deep learning, Convolutional Neural\nNetworks (CNNs) have been widely applied in various domains, such as image\nclassification and object detection, and achieve stunning success in terms of\ntheir high accuracy over the traditional statistical methods. To exploit the\npotential of CNN models, a huge amount of research and industry efforts have\nbeen devoted to optimizing CNNs. Among these endeavors, CNN architecture design\nhas attracted tremendous attention because of its great potential of improving\nmodel accuracy or reducing model complexity. However, existing work either\nintroduces repeated training overhead in the search process or lacks an\ninterpretable metric to guide the design. To clear these hurdles, we propose\nInformation Field (IF), an explainable and easy-to-compute metric, to estimate\nthe quality of a CNN architecture and guide the search process of designs. To\nvalidate the effectiveness of IF, we build a static optimizer to improve the\nCNN architectures at both the stage level and the kernel level. Our optimizer\nnot only provides a clear and reproducible procedure but also mitigates\nunnecessary training efforts in the architecture search process. Extensive\nexperiments and studies show that the models generated by our optimizer can\nachieve up to 5.47% accuracy improvement and up to 65.38% parameters deduction,\ncompared with state-of-the-art CNN structures like MobileNet and ResNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Boyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xueqiao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yufei Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ground-truth or DAER: Selective Re-query of Secondary Information. (arXiv:2009.07414v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.07414","description":"<p>Many vision tasks use secondary information at inference time -- a seed -- to\nassist a computer vision model in solving a problem. For example, an initial\nbounding box is needed to initialize visual object tracking. To date, all such\nwork makes the assumption that the seed is a good one. However, in practice,\nfrom crowdsourcing to noisy automated seeds, this is often not the case. We\nhence propose the problem of seed rejection -- determining whether to reject a\nseed based on the expected performance degradation when it is provided in place\nof a gold-standard seed. We provide a formal definition to this problem, and\nfocus on two meaningful subgoals: understanding causes of error and\nunderstanding the model's response to noisy seeds conditioned on the primary\ninput. With these goals in mind, we propose a novel training method and\nevaluation metrics for the seed rejection problem. We then use seeded versions\nof the viewpoint estimation and fine-grained classification tasks to evaluate\nthese contributions. In these experiments, we show our method can reduce the\nnumber of seeds that need to be reviewed for a target performance by over 23%\ncompared to strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemmer_S/0/1/0/all/0/1\">Stephan J. Lemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason J. Corso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation and Use of Uncertainty Obtained by Bayesian Neural Network Classifiers for Breast Histopathology Images. (arXiv:2010.12575v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12575","description":"<p>Despite the promise of Convolutional neural network (CNN) based\nclassification models for histopathological images, it is infeasible to\nquantify its uncertainties. Moreover, CNNs may suffer from overfitting when the\ndata is biased. We show that Bayesian-CNN can overcome these limitations by\nregularizing automatically and by quantifying the uncertainty. In addition, it\ncan perform much better than the state-of-the-art transfer learning CNN by\nreducing the false negative and false positive by 11% and 7.7% respectively. We\nhave developed a novel technique to utilize the uncertainties provided by the\nBayesian-CNN that significantly improves the performance on a large fraction of\nthe test data (about 6% improvement in accuracy on 77% of test data). Further,\nwe provide a novel explanation for the uncertainty by projecting the data into\na low dimensional space through a nonlinear dimensionality reduction technique.\nThis dimensionality reduction enables interpretation of the test data through\nvisualization and reveals the structure of the data in a low dimensional\nfeature space. Besides, we modify the Bayesian--CNN by introducing a stochastic\nadaptive activation function. The modified Bayesian-CNN performs slightly\nbetter than Bayesian-CNN on all performance metrics and significantly reduces\nthe number of false negatives and false positives (3% reduction for both). This\nwork shows the advantages of Bayesian-CNN against the state-of-the-art,\nexplains and utilizes the uncertainties for histopathological images. It should\nfind applications in various medical image classifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_P/0/1/0/all/0/1\">Ponkrshnan Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khairnar_P/0/1/0/all/0/1\">Pushkar Khairnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Susanta Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where Are You? Localization from Embodied Dialog. (arXiv:2011.08277v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08277","description":"<p>We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans\n-- an Observer and a Locator -- complete a cooperative localization task. The\nObserver is spawned at random in a 3D environment and can navigate from\nfirst-person views while answering questions from the Locator. The Locator must\nlocalize the Observer in a detailed top-down map by asking questions and giving\ninstructions. Based on this dataset, we define three challenging tasks:\nLocalization from Embodied Dialog or LED (localizing the Observer from dialog\nhistory), Embodied Visual Dialog (modeling the Observer), and Cooperative\nLocalization (modeling both agents). In this paper, we focus on the LED task --\nproviding a strong baseline model with detailed ablations characterizing both\ndataset biases and the importance of various modeling choices. Our best model\nachieves 32.7% success at identifying the Observer's location within 3m in\nunseen buildings, vs. 70.4% for human Locators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Meera Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Content and Style: Exploring Bias for Unsupervised Disentanglement. (arXiv:2102.10544v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10544","description":"<p>Content and style (C-S) disentanglement intends to decompose the underlying\nexplanatory factors of objects into two independent subspaces. From the\nunsupervised disentanglement perspective, we rethink content and style and\npropose a formulation for unsupervised C-S disentanglement based on our\nassumption that different factors are of different importance and popularity\nfor image reconstruction, which serves as a data bias. The corresponding model\ninductive bias is introduced by our proposed C-S disentanglement Module (C-S\nDisMo), which assigns different and independent roles to content and style when\napproximating the real data distributions. Specifically, each content embedding\nfrom the dataset, which encodes the most dominant factors for image\nreconstruction, is assumed to be sampled from a shared distribution across the\ndataset. The style embedding for a particular image, encoding the remaining\nfactors, is used to customize the shared distribution through an affine\ntransformation. The experiments on several popular datasets demonstrate that\nour method achieves the state-of-the-art unsupervised C-S disentanglement,\nwhich is comparable or even better than supervised methods. We verify the\neffectiveness of our method by downstream tasks: domain translation and\nsingle-view 3D reconstruction. Project page at\nhttps://github.com/xrenaa/CS-DisMo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The FaCells. An Exploratory Study about LSTM Layers on Face Sketches Classifiers. (arXiv:2102.11361v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.11361","description":"<p>Lines are human mental abstractions. A bunch of lines may form a drawing. A\nset of drawings can feed an LSTM network input layer, considering each draw as\na list of lines and a line a list of points. This paper proposes the pointless\nmotive to classify the gender of celebrities' portraits as an excuse for\nexploration in a broad, more artistic sense. Investigation results drove\ncompelling ideas here discussed. The experiments compared different ways to\nrepresent draws to be input in a network and showed that an absolute format of\ncoordinates (x, y) was a better performer than a relative one (Dx, Dy) with\nrespect to prior points, most frequent in the reviewed literature. Experiments\nalso showed that, due to the recurrent nature of LSTMs, the order of lines\nforming a drawing is a relevant factor for input in an LSTM classifier not\nstudied before. A minimum 'pencil' traveled length criteria for line ordering\nproved suitable, possible by reducing it to a TSP particular instance. The best\nconfiguration for gender classification appears with an LSTM layer that returns\nthe hidden state value for each input point step, followed by a global average\nlayer along the sequence, before the output dense layer. That result guided the\nidea of removing the average in the network pipeline and return a per-point\nattribute score just by adjusting tensors dimensions. With this trick, the\nmodel detects an attribute in a drawing and also recognizes the points linked\nto it. Moreover, by overlapping filtered lines of portraits, an attribute's\nvisual essence is depicted. Meet the FaCells.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_X/0/1/0/all/0/1\">Xavier Ignacio Gonz&#xe1;lez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic evaluation of human oocyte developmental potential from microscopy images. (arXiv:2103.00302v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.00302","description":"<p>Infertility is becoming an issue for an increasing number of couples. The\nmost common solution, in vitro fertilization, requires embryologists to\ncarefully examine light microscopy images of human oocytes to determine their\ndevelopmental potential. We propose an automatic system to improve the speed,\nrepeatability, and accuracy of this process. We first localize individual\noocytes and identify their principal components using CNN (U-Net) segmentation.\nNext, we calculate several descriptors based on geometry and texture. The final\nstep is an SVM classifier. Both the segmentation and classification training is\nbased on expert annotations. The presented approach leads to a classification\naccuracy of 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Barucic_D/0/1/0/all/0/1\">Denis Baru&#x10d;i&#x107;</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Kybic_J/0/1/0/all/0/1\">Jan Kybic</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Tepla_O/0/1/0/all/0/1\">Olga Tepl&#xe1;</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Topurko_Z/0/1/0/all/0/1\">Zinovij Topurko</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Kratochvilova_I/0/1/0/all/0/1\">Irena Kratochv&#xed;lov&#xe1;</a> (3) ((1) Czech Technical University in Prague, Czech Republic, (2) The First Faculty of Medicine and General Teaching Hospital, Czech Republic, (3) Institute of Physics of the Czech Academy of Sciences, Czech Republic)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness via Cross-Domain Ensembles. (arXiv:2103.10919v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10919","description":"<p>We present a method for making neural network predictions robust to shifts\nfrom the training data distribution. The proposed method is based on making\npredictions via a diverse set of cues (called 'middle domains') and ensembling\nthem into one strong prediction. The premise of the idea is that predictions\nmade via different cues respond differently to a distribution shift, hence one\nshould be able to merge them into one robust final prediction. We perform the\nmerging in a straightforward but principled manner based on the uncertainty\nassociated with each prediction. The evaluations are performed using multiple\ntasks and datasets (Taskonomy, Replica, ImageNet, CIFAR) under a wide range of\nadversarial and non-adversarial distribution shifts which demonstrate the\nproposed method is considerably more robust than its standard learning\ncounterpart, conventional deep ensembles, and several other baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeo_T/0/1/0/all/0/1\">Teresa Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_O/0/1/0/all/0/1\">O&#x11f;uzhan Fatih Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sax_A/0/1/0/all/0/1\">Alexander Sax</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1\">Amir Zamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.08756","description":"<p>It is no secret amongst deep learning researchers that finding the optimal\ndata augmentation strategy during training can mean the difference between\nstate-of-the-art performance and a run-of-the-mill result. To that end, the\ncommunity has seen many efforts to automate the process of finding the perfect\naugmentation procedure for any task at hand. Unfortunately, even recent\ncutting-edge methods bring massive computational overhead, requiring as many as\n100 full model trainings to settle on an ideal configuration. We show how to\nachieve equivalent performance in just 6: with Random Unidimensional\nAugmentation. Source code is available at https://github.com/fastestimator/RUA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaomeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1\">Michael Potter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gaurav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Chan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1\">V. Ratna Saripalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer Folding: Neural Network Depth Reduction using Activation Linearization. (arXiv:2106.09309v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09309","description":"<p>Despite the increasing prevalence of deep neural networks, their\napplicability in resource-constrained devices is limited due to their\ncomputational load. While modern devices exhibit a high level of parallelism,\nreal-time latency is still highly dependent on networks' depth. Although recent\nworks show that below a certain depth, the width of shallower networks must\ngrow exponentially, we presume that neural networks typically exceed this\nminimal depth to accelerate convergence and incrementally increase accuracy.\nThis motivates us to transform pre-trained deep networks that already exploit\nsuch advantages into shallower forms. We propose a method that learns whether\nnon-linear activations can be removed, allowing to fold consecutive linear\nlayers into one. We apply our method to networks pre-trained on CIFAR-10 and\nCIFAR-100 and find that they can all be transformed into shallower forms that\nshare a similar depth. Finally, we use our method to provide more efficient\nalternatives to MobileNetV2 and EfficientNet-Lite architectures on the ImageNet\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dror_A/0/1/0/all/0/1\">Amir Ben Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zehngut_N/0/1/0/all/0/1\">Niv Zehngut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_A/0/1/0/all/0/1\">Avraham Raviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artyomov_E/0/1/0/all/0/1\">Evgeny Artyomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitek_R/0/1/0/all/0/1\">Ran Vitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1\">Roy Jevnisek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hepatocellular Carcinoma Segmentation from Digital Subtraction Angiography Videos using Learnable Temporal Difference. (arXiv:2107.04306v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.04306","description":"<p>Automatic segmentation of hepatocellular carcinoma (HCC) in Digital\nSubtraction Angiography (DSA) videos can assist radiologists in efficient\ndiagnosis of HCC and accurate evaluation of tumors in clinical practice. Few\nstudies have investigated HCC segmentation from DSA videos. It shows great\nchallenging due to motion artifacts in filming, ambiguous boundaries of tumor\nregions and high similarity in imaging to other anatomical tissues. In this\npaper, we raise the problem of HCC segmentation in DSA videos, and build our\nown DSA dataset. We also propose a novel segmentation network called\nDSA-LTDNet, including a segmentation sub-network, a temporal difference\nlearning (TDL) module and a liver region segmentation (LRS) sub-network for\nproviding additional guidance. DSA-LTDNet is preferable for learning the latent\nmotion information from DSA videos proactively and boosting segmentation\nperformance. All of experiments are conducted on our self-collected dataset.\nExperimental results show that DSA-LTDNet increases the DICE score by nearly 4%\ncompared to the U-Net baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1\">Wenting Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Y/0/1/0/all/0/1\">Yicheng Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changmiao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Supervision Propagation for Weakly Supervised Semantic Segmentation on 3D Point Clouds. (arXiv:2107.11267v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11267","description":"<p>Semantic segmentation on 3D point clouds is an important task for 3D scene\nunderstanding. While dense labeling on 3D data is expensive and time-consuming,\nonly a few works address weakly supervised semantic point cloud segmentation\nmethods to relieve the labeling cost by learning from simpler and cheaper\nlabels. Meanwhile, there are still huge performance gaps between existing\nweakly supervised methods and state-of-the-art fully supervised methods. In\nthis paper, we train a semantic point cloud segmentation network with only a\nsmall portion of points being labeled. We argue that we can better utilize the\nlimited supervision information as we densely propagate the supervision signal\nfrom the labeled points to other points within and across the input samples.\nSpecifically, we propose a cross-sample feature reallocating module to transfer\nsimilar features and therefore re-route the gradients across two samples with\ncommon classes and an intra-sample feature redistribution module to propagate\nsupervision signals on unlabeled points across and within point cloud samples.\nWe conduct extensive experiments on public datasets S3DIS and ScanNet. Our\nweakly supervised method with only 10\\% and 1\\% of labels can produce\ncompatible results with the fully supervised counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiacheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_K/0/1/0/all/0/1\">Kim-Hui Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03151","description":"<p>Previous video object segmentation approaches mainly focus on using simplex\nsolutions between appearance and motion, limiting feature collaboration\nefficiency among and across these two cues. In this work, we study a novel and\nefficient full-duplex strategy network (FSNet) to address this issue, by\nconsidering a better mutual restraint scheme between motion and appearance in\nexploiting the cross-modal features from the fusion and decoding stage.\nSpecifically, we introduce the relational cross-attention module (RCAM) to\nachieve bidirectional message propagation across embedding sub-spaces. To\nimprove the model's robustness and update the inconsistent features from the\nspatial-temporal embeddings, we adopt the bidirectional purification module\n(BPM) after the RCAM. Extensive experiments on five popular benchmarks show\nthat our FSNet is robust to various challenging scenarios (e.g., motion blur,\nocclusion) and achieves favourable performance against existing cutting-edges\nboth in the video object segmentation and video salient object detection tasks.\nThe project is publicly available at: https://dpfan.net/FSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pattern Recognition in Vital Signs Using Spectrograms. (arXiv:2108.03168v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2108.03168","description":"<p>Spectrograms visualize the frequency components of a given signal which may\nbe an audio signal or even a time-series signal. Audio signals have higher\nsampling rate and high variability of frequency with time. Spectrograms can\ncapture such variations well. But, vital signs which are time-series signals\nhave less sampling frequency and low-frequency variability due to which,\nspectrograms fail to express variations and patterns. In this paper, we propose\na novel solution to introduce frequency variability using frequency modulation\non vital signs. Then we apply spectrograms on frequency modulated signals to\ncapture the patterns. The proposed approach has been evaluated on 4 different\nmedical datasets across both prediction and classification tasks. Significant\nresults are found showing the efficacy of the approach for vital sign signals.\nThe results from the proposed approach are promising with an accuracy of 91.55%\nand 91.67% in prediction and classification tasks respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sribhashyam_S/0/1/0/all/0/1\">Sidharth Srivatsav Sribhashyam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salekin_M/0/1/0/all/0/1\">Md Sirajus Salekin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldgof_D/0/1/0/all/0/1\">Dmitry Goldgof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Last_M/0/1/0/all/0/1\">Mark Last</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STN PLAD: A Dataset for Multi-Size Power Line Assets Detection in High-Resolution UAV Images. (arXiv:2108.07944v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07944","description":"<p>Many power line companies are using UAVs to perform their inspection\nprocesses instead of putting their workers at risk by making them climb high\nvoltage power line towers, for instance. A crucial task for the inspection is\nto detect and classify assets in the power transmission lines. However, public\ndata related to power line assets are scarce, preventing a faster evolution of\nthis area. This work proposes the Power Line Assets Dataset, containing\nhigh-resolution and real-world images of multiple high-voltage power line\ncomponents. It has 2,409 annotated objects divided into five classes:\ntransmission tower, insulator, spacer, tower plate, and Stockbridge damper,\nwhich vary in size (resolution), orientation, illumination, angulation, and\nbackground. This work also presents an evaluation with popular deep object\ndetection methods, showing considerable room for improvement. The STN PLAD\ndataset is publicly available at https://github.com/andreluizbvs/PLAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vieira_e_Silva_A/0/1/0/all/0/1\">Andr&#xe9; Luiz Buarque Vieira-e-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felix_H/0/1/0/all/0/1\">Heitor Felix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaves_T/0/1/0/all/0/1\">Thiago de Menezes Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1\">Francisco Paulo Magalh&#xe3;es Sim&#xf5;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1\">Veronica Teichrieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Michel Mozinho dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santiago_H/0/1/0/all/0/1\">Hemir da Cunha Santiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sgotti_V/0/1/0/all/0/1\">Virginia Ad&#xe9;lia Cordeiro Sgotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_H/0/1/0/all/0/1\">Henrique Baptista Duffles Teixeira Lott Neto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Discover Reflection Symmetry via Polar Matching Convolution. (arXiv:2108.12952v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12952","description":"<p>The task of reflection symmetry detection remains challenging due to\nsignificant variations and ambiguities of symmetry patterns in the wild.\nFurthermore, since the local regions are required to match in reflection for\ndetecting a symmetry pattern, it is hard for standard convolutional networks,\nwhich are not equivariant to rotation and reflection, to learn the task. To\naddress the issue, we introduce a new convolutional technique, dubbed the polar\nmatching convolution, which leverages a polar feature pooling, a\nself-similarity encoding, and a systematic kernel design for axes of different\nangles. The proposed high-dimensional kernel convolution network effectively\nlearns to discover symmetry patterns from real-world images, overcoming the\nlimitations of standard convolution. In addition, we present a new dataset and\nintroduce a self-supervised learning strategy by augmenting the dataset with\nsynthesizing images. Experiments demonstrate that our method outperforms\nstate-of-the-art methods in terms of accuracy and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_A/0/1/0/all/0/1\">Ahyun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_W/0/1/0/all/0/1\">Woohyeon Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Sample Generation: Pushing the Limit of Data-free Quantization. (arXiv:2109.00212v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00212","description":"<p>Recently, generative data-free quantization emerges as a practical approach\nthat compresses the neural network to low bit-width without access to real\ndata. It generates data to quantize the network by utilizing the batch\nnormalization (BN) statistics of its full-precision counterpart. However, our\nstudy shows that in practice, the synthetic data completely constrained by BN\nstatistics suffers severe homogenization at distribution and sample level,\nwhich causes serious accuracy degradation of the quantized network. This paper\npresents a generic Diverse Sample Generation (DSG) scheme for the generative\ndata-free post-training quantization and quantization-aware training, to\nmitigate the detrimental homogenization. In our DSG, we first slack the\nstatistics alignment for features in the BN layer to relax the distribution\nconstraint. Then we strengthen the loss impact of the specific BN layer for\ndifferent samples and inhibit the correlation among samples in the generation\nprocess, to diversify samples from the statistical and spatial perspective,\nrespectively. Extensive experiments show that for large-scale image\nclassification tasks, our DSG can consistently outperform existing data-free\nquantization methods on various neural architectures, especially under\nultra-low bit-width (e.g., 22% gain under W4A4 setting). Moreover, data\ndiversifying caused by our DSG brings a general gain in various quantization\nmethods, demonstrating diversity is an important property of high-quality\nsynthetic data for data-free quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly-Supervised Surface Crack Segmentation Method using Localisation with a Classifier and Thresholding. (arXiv:2109.00456v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00456","description":"<p>Surface cracks are a common sight on public infrastructure nowadays. Recent\nwork has been addressing this problem by supporting structural maintenance\nmeasures using machine learning methods which segment surface cracks from their\nbackground so that they are easy to localize. However, a common issue with\nthose methods is that to create a well functioning algorithm, the training data\nneeds to have detailed annotations of pixels that belong to cracks. Our work\nproposes a weakly supervised approach which leverages a CNN classifier to\ncreate surface crack segmentation maps. We use this classifier to create a\nrough crack localisation map by using its class activation maps and a patch\nbased classification approach and fuse this with a thresholding based approach\nto segment the mostly darker crack pixels. The classifier assists in\nsuppressing noise from the background regions, which commonly are incorrectly\nhighlighted as cracks by standard thresholding methods. We focus on the ease of\nimplementation of our method and it is shown to perform well on several surface\ncrack datasets, segmenting cracks efficiently even though the only data that\nwas used for training were simple classification labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konig_J/0/1/0/all/0/1\">Jacob K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_M/0/1/0/all/0/1\">Mark Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannion_M/0/1/0/all/0/1\">Mike Mannion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrie_P/0/1/0/all/0/1\">Peter Barrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morison_G/0/1/0/all/0/1\">Gordon Morison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse to Dense Motion Transfer for Face Image Animation. (arXiv:2109.00471v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00471","description":"<p>Face image animation from a single image has achieved remarkable progress.\nHowever, it remains challenging when only sparse landmarks are available as the\ndriving signal. Given a source face image and a sequence of sparse face\nlandmarks, our goal is to generate a video of the face imitating the motion of\nlandmarks. We develop an efficient and effective method for motion transfer\nfrom sparse landmarks to the face image. We then combine global and local\nmotion estimation in a unified model to faithfully transfer the motion. The\nmodel can learn to segment the moving foreground from the background and\ngenerate not only global motion, such as rotation and translation of the face,\nbut also subtle local motion such as the gaze change. We further improve face\nlandmark detection on videos. With temporally better aligned landmark sequences\nfor training, our method can generate temporally coherent videos with higher\nvisual quality. Experiments suggest we achieve results comparable to the\nstate-of-the-art image driven method on the same identity testing and better\nresults on cross identity testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo. (arXiv:2109.01129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01129","description":"<p>In this work, we present a new multi-view depth estimation method that\nutilizes both conventional SfM reconstruction and learning-based priors over\nthe recently proposed neural radiance fields (NeRF). Unlike existing neural\nnetwork based optimization method that relies on estimated correspondences, our\nmethod directly optimizes over implicit volumes, eliminating the challenging\nstep of matching pixels in indoor scenes. The key to our approach is to utilize\nthe learning-based priors to guide the optimization process of NeRF. Our system\nfirstly adapts a monocular depth network over the target scene by finetuning on\nits sparse SfM reconstruction. Then, we show that the shape-radiance ambiguity\nof NeRF still exists in indoor environments and propose to address the issue by\nemploying the adapted depth priors to monitor the sampling process of volume\nrendering. Finally, a per-pixel confidence map acquired by error computation on\nthe rendered image can be used to further improve the depth quality.\nExperiments show that our proposed framework significantly outperforms\nstate-of-the-art methods on indoor scenes, with surprising findings presented\non the effectiveness of correspondence-based optimization and NeRF-based\noptimization over the adapted depth priors. In addition, we show that the\nguided optimization scheme does not sacrifice the original synthesis capability\nof neural radiance fields, improving the rendering quality on both seen and\nnovel views. Code is available at https://github.com/weiyithu/NerfingMVS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}