{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.4","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Data Bootstrapping Recipe for Low Resource Multilingual Relation Classification. (arXiv:2110.09570v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09570","description":"<p>Relation classification (sometimes called 'extraction') requires trustworthy\ndatasets for fine-tuning large language models, as well as for evaluation. Data\ncollection is challenging for Indian languages, because they are syntactically\nand morphologically diverse, as well as different from resource-rich languages\nlike English. Despite recent interest in deep generative models for Indian\nlanguages, relation classification is still not well served by public data\nsets. In response, we present IndoRE, a dataset with 21K entity and relation\ntagged gold sentences in three Indian languages, plus English. We start with a\nmultilingual BERT (mBERT) based system that captures entity span positions and\ntype information and provides competitive monolingual relation classification.\nUsing this system, we explore and compare transfer mechanisms between\nlanguages. In particular, we study the accuracy efficiency tradeoff between\nexpensive gold instances vs. translated and aligned 'silver' instances. We\nrelease the dataset for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_A/0/1/0/all/0/1\">Arijit Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1\">Bidisha Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters. (arXiv:2110.09574v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09574","description":"<p>Adapter layers are lightweight, learnable units inserted between transformer\nlayers. Recent work explores using such layers for neural machine translation\n(NMT), to adapt pre-trained models to new domains or language pairs, training\nonly a small set of parameters for each new setting (language pair or domain).\nIn this work we study the compositionality of language and domain adapters in\nthe context of Machine Translation. We aim to study, 1) parameter-efficient\nadaptation to multiple domains and languages simultaneously (full-resource\nscenario) and 2) cross-lingual transfer in domains where parallel data is\nunavailable for certain language pairs (partial-resource scenario). We find\nthat in the partial resource scenario a naive combination of domain-specific\nand language-specific adapters often results in `catastrophic forgetting' of\nthe missing languages. We study other ways to combine the adapters to alleviate\nthis issue and maximize cross-lingual transfer. With our best adapter\ncombinations, we obtain improvements of 3-4 BLEU on average for source\nlanguages that do not have in-domain data. For target languages without\nin-domain data, we achieve a similar improvement by combining adapters with\nback-translation. Supplementary material is available at\nhttps://tinyurl.com/r66stbxj\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1\">Asa Cooper Stickland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre B&#xe9;rard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Descriptive Patterns and their Application to Characterizing Classification Errors. (arXiv:2110.09599v1 [cs.LG])","link":"http://arxiv.org/abs/2110.09599","description":"<p>State-of-the-art deep learning methods achieve human-like performance on many\ntasks, but make errors nevertheless. Characterizing these errors in easily\ninterpretable terms gives insight into whether a model is prone to making\nsystematic errors, but also gives a way to act and improve the model. In this\npaper we propose a method that allows us to do so for arbitrary classifiers by\nmining a small set of patterns that together succinctly describe the input data\nthat is partitioned according to correctness of prediction. We show this is an\ninstance of the more general label description problem, which we formulate in\nterms of the Minimum Description Length principle. To discover good pattern\nsets we propose the efficient and hyperparameter-free Premise algorithm, which\nthrough an extensive set of experiments we show on both synthetic and\nreal-world data performs very well in practice; unlike existing solutions it\nably recovers ground truth patterns, even on highly imbalanced data over many\nunique items, or where patterns are only weakly associated to labels. Through\ntwo real-world case studies we confirm that Premise gives clear and actionable\ninsight into the systematic errors made by modern NLP classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1\">Jonas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1\">Jilles Vreeken</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monotonic Simultaneous Translation with Chunk-wise Reordering and Refinement. (arXiv:2110.09646v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09646","description":"<p>Recent work in simultaneous machine translation is often trained with\nconventional full sentence translation corpora, leading to either excessive\nlatency or necessity to anticipate as-yet-unarrived words, when dealing with a\nlanguage pair whose word orders significantly differ. This is unlike human\nsimultaneous interpreters who produce largely monotonic translations at the\nexpense of the grammaticality of a sentence being translated. In this paper, we\nthus propose an algorithm to reorder and refine the target side of a full\nsentence translation corpus, so that the words/phrases between the source and\ntarget sentences are aligned largely monotonically, using word alignment and\nnon-autoregressive neural machine translation. We then train a widely used\nwait-k simultaneous translation model on this reordered-and-refined corpus. The\nproposed approach improves BLEU scores and resulting translations exhibit\nenhanced monotonicity with source sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">HyoJung Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Seokchan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yoonjung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_I/0/1/0/all/0/1\">Insoo Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble ALBERT on SQuAD 2.0. (arXiv:2110.09665v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09665","description":"<p>Machine question answering is an essential yet challenging task in natural\nlanguage processing. Recently, Pre-trained Contextual Embeddings (PCE) models\nlike Bidirectional Encoder Representations from Transformers (BERT) and A Lite\nBERT (ALBERT) have attracted lots of attention due to their great performance\nin a wide range of NLP tasks. In our Paper, we utilized the fine-tuned ALBERT\nmodels and implemented combinations of additional layers (e.g. attention layer,\nRNN layer) on top of them to improve model performance on Stanford Question\nAnswering Dataset (SQuAD 2.0). We implemented four different models with\ndifferent layers on top of ALBERT-base model, and two other models based on\nALBERT-xlarge and ALBERT-xxlarge. We compared their performance to our baseline\nmodel ALBERT-base-v2 + ALBERT-SQuAD-out with details. Our best-performing\nindividual model is ALBERT-xxlarge + ALBERT-SQuAD-out, which achieved an F1\nscore of 88.435 on the dev set. Furthermore, we have implemented three\ndifferent ensemble algorithms to boost overall performance. By passing in\nseveral best-performing models' results into our weighted voting ensemble\nalgorithm, our final result ranks first on the Stanford CS224N Test PCE SQuAD\nLeaderboard with F1 = 90.123.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shilun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Renee Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_V/0/1/0/all/0/1\">Veronica Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge. (arXiv:2110.09698v1 [cs.SD])","link":"http://arxiv.org/abs/2110.09698","description":"<p>End-to-end TTS suffers from high data requirements as it is difficult for\nboth costly speech corpora to cover all necessary knowledge and neural models\nto learn the knowledge, hence additional knowledge needs to be injected\nmanually. For example, to capture pronunciation knowledge on languages without\nregular orthography, a complicated grapheme-to-phoneme pipeline needs to be\nbuilt based on a structured, large pronunciation lexicon, leading to extra,\nsometimes high, costs to extend neural TTS to such languages. In this paper, we\npropose a framework to learn to extract knowledge from unstructured external\nresources using Token2Knowledge attention modules. The framework is applied to\nbuild a novel end-to-end TTS model named Neural Lexicon Reader that extracts\npronunciations from raw lexicon texts. Experiments support the potential of our\nframework that the model significantly reduces pronunciation errors in\nlow-resource, end-to-end Chinese TTS, and the lexicon-reading capability can be\ntransferred to other languages with a smaller amount of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mutian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingzhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1\">Frank K. Soong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A non-hierarchical attention network with modality dropout for textual response generation in multimodal dialogue systems. (arXiv:2110.09702v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09702","description":"<p>Existing text- and image-based multimodal dialogue systems use the\ntraditional Hierarchical Recurrent Encoder-Decoder (HRED) framework, which has\nan utterance-level encoder to model utterance representation and a\ncontext-level encoder to model context representation. Although pioneer efforts\nhave shown promising performances, they still suffer from the following\nchallenges: (1) the interaction between textual features and visual features is\nnot fine-grained enough. (2) the context representation can not provide a\ncomplete representation for the context. To address the issues mentioned above,\nwe propose a non-hierarchical attention network with modality dropout, which\nabandons the HRED framework and utilizes attention modules to encode each\nutterance and model the context representation. To evaluate our proposed model,\nwe conduct comprehensive experiments on a public multimodal dialogue dataset.\nAutomatic and human evaluation demonstrate that our proposed model outperforms\nthe existing methods and achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rongyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Borun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">YunBo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inter-Sense: An Investigation of Sensory Blending in Fiction. (arXiv:2110.09710v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09710","description":"<p>This study reports on the semantic organization of English sensory\ndescriptors of the five basic senses of sight, hearing, touch, taste, and smell\nin a large corpus of over 8,000 fiction books. We introduce a large-scale text\ndata-driven approach based on distributional-semantic word embeddings to\nidentify and extract these descriptors as well as analyze their mixing\ninterconnections in the resulting conceptual and sensory space. The findings\nare relevant for research on concept acquisition and representation, as well as\nfor applications that can benefit from a better understanding of perceptual\nspaces of sensory experiences, in fiction, in particular, and in language in\ngeneral.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girju_R/0/1/0/all/0/1\">Roxana Girju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambert_C/0/1/0/all/0/1\">Charlotte Lambert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Sensory Spaces of English Perceptual Verbs in Natural Language Data. (arXiv:2110.09721v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09721","description":"<p>In this study, we explore how language captures the meaning of words, in\nparticular meaning related to sensory experiences learned from statistical\ndistributions across texts. We focus on the most frequent perception verbs of\nEnglish analyzed from an and Agentive vs. Experiential distinction across the\nfive basic sensory modalities: Visual (to look vs. to see), Auditory (to listen\nvs. to hear), Tactile (to touch vs. to feel), Olfactory (to smell), and\nGustatory (to taste). In this study we report on a data-driven approach based\non distributional-semantic word embeddings and clustering models to identify\nand uncover the descriptor sensory spaces of the perception verbs. In the\nanalysis, we identified differences and similarities of the generated\ndescriptors based on qualitative and quantitative differences of the perceptual\nexperience they denote. For instance, our results show that while the\nperceptual spaces of the experiential verbs like to see, to hear show a more\ndetached, logical way of knowing and learning, their agentive counterparts (to\nlook, listen) provide a more intentional as well as more intimate and intuitive\nway of discovering and interacting with the world around us. We believe that\nsuch an approach has a high potential to expand our understanding and the\napplicability of such sensory spaces to different fields of social and cultural\nanalysis. Research on the semantic organization of sensory spaces for various\napplications might benefit from an the Agentive/Experiential account to address\nthe complexity of multiple senses wired with each other in still unexplored\nways.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girju_R/0/1/0/all/0/1\">Roxana Girju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">David Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory Prediction with Linguistic Representations. (arXiv:2110.09741v1 [cs.RO])","link":"http://arxiv.org/abs/2110.09741","description":"<p>Language allows humans to build mental models that interpret what is\nhappening around them resulting in more accurate long-term predictions. We\npresent a novel trajectory prediction model that uses linguistic intermediate\nrepresentations to forecast trajectories, and is trained using trajectory\nsamples with partially annotated captions. The model learns the meaning of each\nof the words without direct per-word supervision. At inference time, it\ngenerates a linguistic description of trajectories which captures maneuvers and\ninteractions over an extended time interval. This generated description is used\nto refine predictions of the trajectories of multiple agents. We train and\nvalidate our model on the Argoverse dataset, and demonstrate improved accuracy\nresults in trajectory prediction. In addition, our model is more interpretable:\nit presents part of its reasoning in plain language as captions, which can aid\nmodel development and can aid in building confidence in the model before\ndeploying it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1\">Yen-Ling Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbu_A/0/1/0/all/0/1\">Andrei Barbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGill_S/0/1/0/all/0/1\">Stephen G. McGill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_B/0/1/0/all/0/1\">Boris Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1\">John J. Leonard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1\">Guy Rosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Importance Estimation from Multiple Perspectives for Keyphrase Extraction. (arXiv:2110.09749v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09749","description":"<p>Keyphrase extraction is a fundamental task in Natural Language Processing,\nwhich usually contains two main parts: candidate keyphrase extraction and\nkeyphrase importance estimation. From the view of human understanding\ndocuments, we typically measure the importance of phrase according to its\nsyntactic accuracy, information saliency, and concept consistency\nsimultaneously. However, most existing keyphrase extraction approaches only\nfocus on the part of them, which leads to biased results. In this paper, we\npropose a new approach to estimate the importance of keyphrase from multiple\nperspectives (called as \\textit{KIEMP}) and further improve the performance of\nkeyphrase extraction. Specifically, \\textit{KIEMP} estimates the importance of\nphrase with three modules: a chunking module to measure its syntactic accuracy,\na ranking module to check its information saliency, and a matching module to\njudge the concept (i.e., topic) consistency between phrase and the whole\ndocument. These three modules are seamlessly jointed together via an end-to-end\nmulti-task learning model, which is helpful for three parts to enhance each\nother and balance the effects of three perspectives. Experimental results on\nsix benchmark datasets show that \\textit{KIEMP} outperforms the existing\nstate-of-the-art keyphrase extraction approaches in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lin Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Multimodal Transformer for Bi-directional Image and Text Generation. (arXiv:2110.09753v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09753","description":"<p>We study the joint learning of image-to-text and text-to-image generations,\nwhich are naturally bi-directional tasks. Typical existing works design two\nseparate task-specific models for each task, which impose expensive design\nefforts. In this work, we propose a unified image-and-text generative framework\nbased on a single multimodal model to jointly study the bi-directional tasks.\nWe adopt Transformer as our unified architecture for its strong performance and\ntask-agnostic design. Specifically, we formulate both tasks as sequence\ngeneration tasks, where we represent images and text as unified sequences of\ntokens, and the Transformer learns multimodal interactions to generate\nsequences. We further propose two-level granularity feature representations and\nsequence-level training to improve the Transformer-based unified framework.\nExperiments show that our approach significantly improves previous\nTransformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for\ntext-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for\nfine-tuned image-to-text generation on the MS-COCO dataset. Our code is\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongwei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation. (arXiv:2110.09756v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09756","description":"<p>A creative image-and-text generative AI system mimics humans' extraordinary\nabilities to provide users with diverse and comprehensive caption suggestions,\nas well as rich image creations. In this work, we demonstrate such an AI\ncreation system to produce both diverse captions and rich images. When users\nimagine an image and associate it with multiple captions, our system paints a\nrich image to reflect all captions faithfully. Likewise, when users upload an\nimage, our system depicts it with multiple diverse captions. We propose a\nunified multi-modal framework to achieve this goal. Specifically, our framework\njointly models image-and-text representations with a Transformer network, which\nsupports rich image creation by accepting multiple captions as input. We\nconsider the relations among input captions to encourage diversity in training\nand adopt a non-autoregressive decoding strategy to enable real-time inference.\nBased on these, our system supports both diverse captions and rich images\ngenerations. Our code is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-domain clarification question generation without question examples. (arXiv:2110.09779v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09779","description":"<p>An overarching goal of natural language processing is to enable machines to\ncommunicate seamlessly with humans. However, natural language can be ambiguous\nor unclear. In cases of uncertainty, humans engage in an interactive process\nknown as repair: asking questions and seeking clarification until their\nuncertainty is resolved. We propose a framework for building a visually\ngrounded question-asking model capable of producing polar (yes-no)\nclarification questions to resolve misunderstandings in dialogue. Our model\nuses an expected information gain objective to derive informative questions\nfrom an off-the-shelf image captioner without requiring any supervised\nquestion-answer data. We demonstrate our model's ability to pose questions that\nimprove communicative success in a goal-oriented 20 questions game with\nsynthetic and human answerers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Julia White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesia_G/0/1/0/all/0/1\">Gabriel Poesia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Pattern based Black-box Model Watermarking for Automatic Speech Recognition. (arXiv:2110.09814v1 [cs.SD])","link":"http://arxiv.org/abs/2110.09814","description":"<p>As an effective method for intellectual property (IP) protection, model\nwatermarking technology has been applied on a wide variety of deep neural\nnetworks (DNN), including speech classification models. However, how to design\na black-box watermarking scheme for automatic speech recognition (ASR) models\nis still an unsolved problem, which is a significant demand for protecting\nremote ASR Application Programming Interface (API) deployed in cloud servers.\nDue to conditional independence assumption and label-detection-based evasion\nattack risk of ASR models, the black-box model watermarking scheme for speech\nclassification models cannot apply to ASR models. In this paper, we propose the\nfirst black-box model watermarking framework for protecting the IP of ASR\nmodels. Specifically, we synthesize trigger audios by spreading the speech\nclips of model owners over the entire input audios and labeling the trigger\naudios with the stego texts, which hides the authorship information with\nlinguistic steganography. Experiments on the state-of-the-art open-source ASR\nsystem DeepSpeech demonstrate the feasibility of the proposed watermarking\nscheme, which is robust against five kinds of attacks and has little impact on\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haozhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kejiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AequeVox: Automated Fairness Testing of Speech Recognition Systems. (arXiv:2110.09843v1 [cs.LG])","link":"http://arxiv.org/abs/2110.09843","description":"<p>Automatic Speech Recognition (ASR) systems have become ubiquitous. They can\nbe found in a variety of form factors and are increasingly important in our\ndaily lives. As such, ensuring that these systems are equitable to different\nsubgroups of the population is crucial. In this paper, we introduce, AequeVox,\nan automated testing framework for evaluating the fairness of ASR systems.\nAequeVox simulates different environments to assess the effectiveness of ASR\nsystems for different populations. In addition, we investigate whether the\nchosen simulations are comprehensible to humans. We further propose a fault\nlocalization technique capable of identifying words that are not robust to\nthese varying environments. Both components of AequeVox are able to operate in\nthe absence of ground truth data.\n</p>\n<p>We evaluated AequeVox on speech from four different datasets using three\ndifferent commercial ASRs. Our experiments reveal that non-native English,\nfemale and Nigerian English speakers generate 109%, 528.5% and 156.9% more\nerrors, on average than native English, male and UK Midlands speakers,\nrespectively. Our user study also reveals that 82.9% of the simulations\n(employed through speech transformations) had a comprehensibility rating above\nseven (out of ten), with the lowest rating being 6.78. This further validates\nthe fairness violations discovered by AequeVox. Finally, we show that the\nnon-robust words, as predicted by the fault localization technique embodied in\nAequeVox, show 223.8% more errors than the predicted robust words across all\nASRs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1\">Sai Sathiesh Rajan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1\">Sakshi Udeshi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Sudipta Chattopadhyay</a> (1) ((1) Singapore University of Technology and Design)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage Voice Application Recommender System for Unhandled Utterances in Intelligent Personal Assistant. (arXiv:2110.09877v1 [cs.LG])","link":"http://arxiv.org/abs/2110.09877","description":"<p>Intelligent personal assistants (IPA) enable voice applications that\nfacilitate people's daily tasks. However, due to the complexity and ambiguity\nof voice requests, some requests may not be handled properly by the standard\nnatural language understanding (NLU) component. In such cases, a simple reply\nlike \"Sorry, I don't know\" hurts the user's experience and limits the\nfunctionality of IPA. In this paper, we propose a two-stage\nshortlister-reranker recommender system to match third-party voice applications\n(skills) to unhandled utterances. In this approach, a skill shortlister is\nproposed to retrieve candidate skills from the skill catalog by calculating\nboth lexical and semantic similarity between skills and user requests. We also\nillustrate how to build a new system by using observed data collected from a\nbaseline rule-based system, and how the exposure biases can generate\ndiscrepancy between offline and human metrics. Lastly, we present two\nrelabeling methods that can handle the incomplete ground truth, and mitigate\nexposure bias. We demonstrate the effectiveness of our proposed system through\nextensive offline experiments. Furthermore, we present online A/B testing\nresults that show a significant boost on user experience satisfaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_T/0/1/0/all/0/1\">Thahir Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xibin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1\">Radhika Arava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AbdelHady_M/0/1/0/all/0/1\">Mohamed AbdelHady</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Relation Extraction as Dependency Parsing in Visually Rich Documents. (arXiv:2110.09915v1 [cs.CL])","link":"http://arxiv.org/abs/2110.09915","description":"<p>Previous works on key information extraction from visually rich documents\n(VRDs) mainly focus on labeling the text within each bounding box (i.e.,\nsemantic entity), while the relations in-between are largely unexplored. In\nthis paper, we adapt the popular dependency parsing model, the biaffine parser,\nto this entity relation extraction task. Being different from the original\ndependency parsing model which recognizes dependency relations between words,\nwe identify relations between groups of words with layout information instead.\nWe have compared different representations of the semantic entity, different\nVRD encoders, and different relation decoders. The results demonstrate that our\nproposed model achieves 65.96% F1 score on the FUNSD dataset. As for the\nreal-world application, our model has been applied to the in-house customs\ndata, achieving reliable performance in the production setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Junjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zuyi Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Representation Learning Through Self-supervised Pretraining And Multi-task Finetuning. (arXiv:2110.09930v1 [eess.AS])","link":"http://arxiv.org/abs/2110.09930","description":"<p>Speech representation learning plays a vital role in speech processing. Among\nthem, self-supervised learning (SSL) has become an important research\ndirection. It has been shown that an SSL pretraining model can achieve\nexcellent performance in various downstream tasks of speech processing. On the\nother hand, supervised multi-task learning (MTL) is another representation\nlearning paradigm, which has been proven effective in computer vision (CV) and\nnatural language processing (NLP). However, there is no systematic research on\nthe general representation learning model trained by supervised MTL in speech\nprocessing. In this paper, we show that MTL finetuning can further improve SSL\npretraining. We analyze the generalizability of supervised MTL finetuning to\nexamine if the speech representation learned by MTL finetuning can generalize\nto unseen new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Cheng-Kuang Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEEPAG\\'E: Answering Questions in Portuguese about the Brazilian Environment. (arXiv:2110.10015v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10015","description":"<p>The challenge of climate change and biome conservation is one of the most\npressing issues of our time - particularly in Brazil, where key environmental\nreserves are located. Given the availability of large textual databases on\necological themes, it is natural to resort to question answering (QA) systems\nto increase social awareness and understanding about these topics. In this\nwork, we introduce multiple QA systems that combine in novel ways the BM25\nalgorithm, a sparse retrieval technique, with PTT5, a pre-trained\nstate-of-the-art language model. Our QA systems focus on the Portuguese\nlanguage, thus offering resources not found elsewhere in the literature. As\ntraining data, we collected questions from open-domain datasets, as well as\ncontent from the Portuguese Wikipedia and news from the press. We thus\ncontribute with innovative architectures and novel applications, attaining an\nF1-score of 36.2 with our best model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cacao_F/0/1/0/all/0/1\">Fl&#xe1;vio Nakasato Ca&#xe7;&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_M/0/1/0/all/0/1\">Marcos Menon Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Andr&#xe9; Seidel Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spindola_S/0/1/0/all/0/1\">Stefano Spindola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Anna Helena Reali Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozman_F/0/1/0/all/0/1\">F&#xe1;bio Gagliardi Cozman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Private Language Model Adaptation for Speech Recognition. (arXiv:2110.10026v1 [eess.AS])","link":"http://arxiv.org/abs/2110.10026","description":"<p>Speech model adaptation is crucial to handle the discrepancy between\nserver-side proxy training data and actual data received on users' local\ndevices. With the use of federated learning (FL), we introduce an efficient\napproach on continuously adapting neural network language models (NNLMs) on\nprivate devices with applications on automatic speech recognition (ASR). To\naddress the potential speech transcription errors in the on-device training\ncorpus, we perform empirical studies on comparing various strategies of\nleveraging token-level confidence scores to improve the NNLM quality in the FL\nsettings. Experiments show that compared with no model adaptation, the proposed\nmethod achieves relative 2.6% and 10.8% word error rate (WER) reductions on two\nspeech evaluation datasets, respectively. We also provide analysis in\nevaluating privacy guarantees of our presented procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakshi_S/0/1/0/all/0/1\">Shreyan Bakshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_F/0/1/0/all/0/1\">Fuchun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical Trial Information Extraction with BERT. (arXiv:2110.10027v1 [q-bio.QM])","link":"http://arxiv.org/abs/2110.10027","description":"<p>Natural language processing (NLP) of clinical trial documents can be useful\nin new trial design. Here we identify entity types relevant to clinical trial\ndesign and propose a framework called CT-BERT for information extraction from\nclinical trial text. We trained named entity recognition (NER) models to\nextract eligibility criteria entities by fine-tuning a set of pre-trained BERT\nmodels. We then compared the performance of CT-BERT with recent baseline\nmethods including attention-based BiLSTM and Criteria2Query. The results\ndemonstrate the superiority of CT-BERT in clinical trial NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1\">Xiong Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hersch_G/0/1/0/all/0/1\">Greg L. Hersch</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khalil_I/0/1/0/all/0/1\">Iya Khalil</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Devarakonda_M/0/1/0/all/0/1\">Murthy Devarakonda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Idiomatic Expression Identification using Semantic Compatibility. (arXiv:2110.10064v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10064","description":"<p>Idiomatic expressions are an integral part of natural language and constantly\nbeing added to a language. Owing to their non-compositionality and their\nability to take on a figurative or literal meaning depending on the sentential\ncontext, they have been a classical challenge for NLP systems. To address this\nchallenge, we study the task of detecting whether a sentence has an idiomatic\nexpression and localizing it. Prior art for this task had studied specific\nclasses of idiomatic expressions offering limited views of their\ngeneralizability to new idioms. We propose a multi-stage neural architecture\nwith the attention flow mechanism for identifying these expressions. The\nnetwork effectively fuses contextual and lexical information at different\nlevels using word and sub-word representations. Empirical evaluations on three\nof the largest benchmark datasets with idiomatic expressions of varied\nsyntactic patterns and degrees of non-compositionality show that our proposed\nmodel achieves new state-of-the-art results. A salient feature of the model is\nits ability to identify idioms unseen during training with gains from 1.4% to\n30.8% over competitive baselines on the largest dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1\">Suma Bhat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Networks Enable Systematic Generalization for Grounded Language Understanding. (arXiv:2008.02742v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.02742","description":"<p>Humans are remarkably flexible when understanding new sentences that include\ncombinations of concepts they have never encountered before. Recent work has\nshown that while deep networks can mimic some human language abilities when\npresented with novel sentences, systematic variation uncovers the limitations\nin the language-understanding abilities of networks. We demonstrate that these\nlimitations can be overcome by addressing the generalization challenges in the\ngSCAN dataset, which explicitly measures how well an agent is able to interpret\nnovel linguistic commands grounded in vision, e.g., novel pairings of\nadjectives and nouns. The key principle we employ is compositionality: that the\ncompositional structure of networks should reflect the compositional structure\nof the problem domain they address, while allowing other parameters to be\nlearned end-to-end. We build a general-purpose mechanism that enables agents to\ngeneralize their language understanding to compositional domains. Crucially,\nour network has the same state-of-the-art performance as prior work while\ngeneralizing its knowledge when prior work does not. Our network also provides\na level of interpretability that enables users to inspect what each part of\nnetworks learns. Robust grounded language understanding without dramatic\nfailures and without corner cases is critical to building safe and fair robots;\nwe demonstrate the significant role that compositionality can play in achieving\nthat goal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1\">Yen-Ling Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_B/0/1/0/all/0/1\">Boris Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbu_A/0/1/0/all/0/1\">Andrei Barbu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions. (arXiv:2010.10216v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.10216","description":"<p>Popular dialog datasets such as MultiWOZ are created by providing crowd\nworkers an instruction, expressed in natural language, that describes the task\nto be accomplished. Crowd workers play the role of a user and an agent to\ngenerate dialogs to accomplish tasks involving booking restaurant tables,\ncalling a taxi etc. In this paper, we present a data creation strategy that\nuses the pre-trained language model, GPT2, to simulate the interaction between\ncrowd workers by creating a user bot and an agent bot. We train the simulators\nusing a smaller percentage of actual crowd-generated conversations and their\ncorresponding instructions. We demonstrate that by using the simulated data, we\nachieve significant improvements in low-resource settings on two publicly\navailable datasets - the MultiWOZ dataset and the Persona chat dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_B/0/1/0/all/0/1\">Biswesh Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_G/0/1/0/all/0/1\">Gaurav Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for Enabling Hardware-agnostic Programming with True Performance Portability for Heterogeneous HPC. (arXiv:2011.10896v4 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2011.10896","description":"<p>This paper presents HALO 1.0, an open-ended extensible multi-agent software\nframework that implements a set of proposed hardware-agnostic accelerator\norchestration (HALO) principles. HALO implements a novel compute-centric\nmessage passing interface (C^2MPI) specification for enabling the\nperformance-portable execution of a hardware-agnostic host application across\nheterogeneous accelerators. The experiment results of evaluating eight widely\nused HPC subroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs,\nand NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified\ncontrol flow for host programs to run across all the computing devices with a\nconsistently top performance portability score, which is up to five orders of\nmagnitude higher than the OpenCL-based solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riera_M/0/1/0/all/0/1\">Michael Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_E/0/1/0/all/0/1\">Erfan Bank Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quraishi_M/0/1/0/all/0/1\">Masudul Hassan Quraishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Fengbo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer. (arXiv:2107.02681v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02681","description":"<p>Since visual perception can give rich information beyond text descriptions\nfor world understanding, there has been increasing interest in leveraging\nvisual grounding for language learning. Recently, vokenization (Tan and Bansal,\n2020) has attracted attention by using the predictions of a text-to-image\nretrieval model as labels for language model supervision. Despite its success,\nthe method suffers from approximation error of using finite image labels and\nthe lack of vocabulary diversity of a small image-text dataset. To overcome\nthese limitations, we present VidLanKD, a video-language knowledge distillation\nmethod for improving language understanding. We train a multi-modal teacher\nmodel on a video-text dataset, and then transfer its knowledge to a student\nlanguage model with a text dataset. To avoid approximation error, we propose to\nuse different knowledge distillation objectives. In addition, the use of a\nlarge-scale video-text dataset helps learn diverse and richer vocabularies. In\nour experiments, VidLanKD achieves consistent improvements over text-only\nlanguage models and vokenization models, on several downstream language\nunderstanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the\nimproved world knowledge, physical reasoning, and temporal reasoning\ncapabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and\nTRACIE datasets. Lastly, we present comprehensive ablation studies as well as\nvisualizations of the learned text-to-video grounding results of our teacher\nand student language models. Our code and models are available at:\nhttps://github.com/zinengtang/VidLanKD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution. (arXiv:2107.05612v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2107.05612","description":"<p>Natural language provides an accessible and expressive interface to specify\nlong-term tasks for robotic agents. However, non-experts are likely to specify\nsuch tasks with high-level instructions, which abstract over specific robot\nactions through several layers of abstraction. We propose that key to bridging\nthis gap between language and robot actions over long execution horizons are\npersistent representations. We propose a persistent spatial semantic\nrepresentation method, and show how it enables building an agent that performs\nhierarchical reasoning to effectively execute long-term tasks. We evaluate our\napproach on the ALFRED benchmark and achieve state-of-the-art results, despite\ncompletely avoiding the commonly used step-by-step instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blukis_V/0/1/0/all/0/1\">Valts Blukis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Break, Perturb, Build: Automatic Perturbation of Reasoning Paths Through Question Decomposition. (arXiv:2107.13935v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.13935","description":"<p>Recent efforts to create challenge benchmarks that test the abilities of\nnatural language understanding models have largely depended on human\nannotations. In this work, we introduce the \"Break, Perturb, Build\" (BPB)\nframework for automatic reasoning-oriented perturbation of question-answer\npairs. BPB represents a question by decomposing it into the reasoning steps\nthat are required to answer it, symbolically perturbs the decomposition, and\nthen generates new question-answer pairs. We demonstrate the effectiveness of\nBPB by creating evaluation sets for three reading comprehension (RC)\nbenchmarks, generating thousands of high-quality examples without human\nintervention. We evaluate a range of RC models on our evaluation sets, which\nreveals large performance gaps on generated examples compared to the original\ndata. Moreover, symbolic perturbations enable fine-grained analysis of the\nstrengths and limitations of models. Last, augmenting the training data with\nexamples generated by BPB helps close the performance gaps, without any drop on\nthe original data distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models. (arXiv:2108.04049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04049","description":"<p>Open-domain extractive question answering works well on textual data by first\nretrieving candidate texts and then extracting the answer from those\ncandidates. However, some questions cannot be answered by text alone but\nrequire information stored in tables. In this paper, we present an approach for\nretrieving both texts and tables relevant to a question by jointly encoding\ntexts, tables and questions into a single vector space. To this end, we create\na new multi-modal dataset based on text and table datasets from related work\nand compare the retrieval performance of different encoding schemata. We find\nthat dense vector embeddings of transformer models outperform sparse embeddings\non four out of six evaluation datasets. Comparing different dense embedding\nmodels, tri-encoders with one encoder for each question, text and table,\nincrease retrieval performance compared to bi-encoders with one encoder for the\nquestion and one for both text and tables. We release the newly created\nmulti-modal dataset to the community so that it can be used for training and\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_B/0/1/0/all/0/1\">Bogdan Kosti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning. (arXiv:2108.06743v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06743","description":"<p>To quantitatively and intuitively explore the generalization ability of\npre-trained language models (PLMs), we have designed several tasks of\narithmetic and logical reasoning. We both analyse how well PLMs generalize when\nthe test data is in the same distribution as the train data and when it is\ndifferent, for the latter analysis, we have also designed a cross-distribution\ntest set other than the in-distribution test set. We conduct experiments on one\nof the most advanced and publicly released generative PLM - BART. Our research\nfinds that the PLMs can easily generalize when the distribution is the same,\nhowever, it is still difficult for them to generalize out of the distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yuchen Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08597","description":"<p>Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique or doing this is to apply named entity\ndisambiguation (NED) systems to the question, and retrieve KB facts for the\ndisambiguated entities. This work presents CLOCQ, an efficient method that\nprunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses\na top-k query processor over score-ordered lists of KB items that combine\nsignals about lexical matching, relevance to the question, coherence among\ncandidate items, and connectivity in the KB graph. Experiments with two recent\nQA benchmarks for complex questions demonstrate the superiority of CLOCQ over\nstate-of-the-art baselines with respect to answer presence, size of the search\nspace, and runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04993","description":"<p>Pre-training visual and textual representations from large-scale image-text\npairs is becoming a standard approach for many downstream vision-language\ntasks. The transformer-based models learn inter and intra-modal attention\nthrough a list of self-supervised learning tasks. This paper proposes LAViTeR,\na novel architecture for visual and textual representation learning. The main\nmodule, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\nGAN-based image synthesis and Image Captioning. We also propose a new\nevaluation metric measuring the similarity between the learnt visual and\ntextual embedding. The experimental results on two public datasets, CUB and\nMS-COCO, demonstrate superior visual and textual representation alignment in\nthe joint feature embedding space\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP. (arXiv:2110.03618v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03618","description":"<p>The principle of independent causal mechanisms (ICM) states that generative\nprocesses of real world data consist of independent modules which do not\ninfluence or inform each other. While this idea has led to fruitful\ndevelopments in the field of causal inference, it is not widely-known in the\nNLP community. In this work, we argue that the causal direction of the data\ncollection process bears nontrivial implications that can explain a number of\npublished NLP findings, such as differences in semi-supervised learning (SSL)\nand domain adaptation (DA) performance across different settings. We categorize\ncommon NLP tasks according to their causal direction and empirically assay the\nvalidity of the ICM principle for text data using minimum description length.\nWe conduct an extensive meta-analysis of over 100 published SSL and 30 DA\nstudies, and find that the results are consistent with our expectations based\non causal insights. This work presents the first attempt to analyze the ICM\nprinciple in NLP, and provides constructive suggestions for future modeling\nchoices. Code available at https://github.com/zhijing-jin/icm4nlp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidhya_T/0/1/0/all/0/1\">Tejas Vaidhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushal_A/0/1/0/all/0/1\">Ayush Kaushal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Order Does Not Matter For Speech Recognition. (arXiv:2110.05994v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.05994","description":"<p>In this paper, we study training of automatic speech recognition system in a\nweakly supervised setting where the order of words in transcript labels of the\naudio training data is not known. We train a word-level acoustic model which\naggregates the distribution of all output frames using LogSumExp operation and\nuses a cross-entropy loss to match with the ground-truth words distribution.\nUsing the pseudo-labels generated from this model on the training set, we then\ntrain a letter-based acoustic model using Connectionist Temporal Classification\nloss. Our system achieves 2.3%/4.6% on test-clean/test-other subsets of\nLibriSpeech, which closely matches with the supervised baseline's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07342","description":"<p>Recent methods for embodied instruction following are typically trained\nend-to-end using imitation learning. This requires the use of expert\ntrajectories and low-level language instructions. Such approaches assume\nlearned hidden states will simultaneously integrate semantics from the language\nand vision to perform state tracking, spatial memory, exploration, and\nlong-term planning. In contrast, we propose a modular method with structured\nrepresentations that (1) builds a semantic map of the scene, and (2) performs\nexploration with a semantic search policy, to achieve the natural language\ngoal. Our modular method achieves SOTA performance (24.46%) with a substantial\n(8.17 % absolute) gap from previous work while using less data by eschewing\nboth expert trajectories and low-level instructions. Leveraging low-level\nlanguage, however, can further increase our performance (26.49%). Our findings\nsuggest that an explicit spatial memory and a semantic search policy can\nprovide a stronger and more general representation for state-tracking and\nguidance, even in the absence of expert trajectories or low-level instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">So Yeon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm. (arXiv:2110.08190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08190","description":"<p>Various pruning approaches have been proposed to reduce the footprint\nrequirements of Transformer-based language models. Conventional wisdom is that\npruning reduces the model expressiveness and thus is more likely to underfit\nthan overfit compared to the original model. However, under the trending\npretrain-and-finetune paradigm, we argue that pruning increases the risk of\noverfitting if pruning was performed at the fine-tuning phase, as it increases\nthe amount of information a model needs to learn from the downstream task,\nresulting in relative data deficiency. In this paper, we aim to address the\noverfitting issue under the pretrain-and-finetune paradigm to improve pruning\nperformance via progressive knowledge distillation (KD) and sparse pruning.\nFurthermore, to mitigate the interference between different strategies of\nlearning rate, pruning and distillation, we propose a three-stage learning\nframework. We show for the first time that reducing the risk of overfitting can\nhelp the effectiveness of pruning under the pretrain-and-finetune paradigm.\nExperiments on multiple datasets of GLUE benchmark show that our method\nachieves highly competitive pruning performance over the state-of-the-art\ncompetitors across different pruning ratio constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1\">Ian E.H. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sung-en Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mimi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Multimodal to Unimodal Attention in Transformers using Knowledge Distillation. (arXiv:2110.08270v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08270","description":"<p>Multimodal Deep Learning has garnered much interest, and transformers have\ntriggered novel approaches, thanks to the cross-attention mechanism. Here we\npropose an approach to deal with two key existing challenges: the high\ncomputational resource demanded and the issue of missing modalities. We\nintroduce for the first time the concept of knowledge distillation in\ntransformers to use only one modality at inference time. We report a full study\nanalyzing multiple student-teacher configurations, levels at which distillation\nis applied, and different methodologies. With the best configuration, we\nimproved the state-of-the-art accuracy by 3%, we reduced the number of\nparameters by 2.5 times and the inference time by 22%. Such\nperformance-computation tradeoff can be exploited in many applications and we\naim at opening a new research area where the deployment of complex models with\nlimited resources is demanded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dhruv Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_T/0/1/0/all/0/1\">Tanay Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_L/0/1/0/all/0/1\">Laura M. Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Fran&#xe7;ois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViraPart: A Text Refinement Framework for ASR and NLP Tasks in Persian. (arXiv:2110.09086v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.09086","description":"<p>The Persian language is an inflectional SOV language. This fact makes Persian\na more uncertain language. However, using techniques such as ZWNJ recognition,\npunctuation restoration, and Persian Ezafe construction will lead us to a more\nunderstandable and precise language. In most of the works in Persian, these\ntechniques are addressed individually. Despite that, we believe that for text\nrefinement in Persian, all of these tasks are necessary. In this work, we\nproposed a ViraPart framework that uses embedded ParsBERT in its core for text\nclarifications. First, used the BERT variant for Persian following by a\nclassifier layer for classification procedures. Next, we combined models\noutputs to output cleartext. In the end, the proposed model for ZWNJ\nrecognition, punctuation restoration, and Persian Ezafe construction performs\nthe averaged F1 macro scores of 96.90%, 92.13%, and 98.50%, respectively.\nExperimental results show that our proposed approach is very effective in text\nrefinement for the Persian language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farokhshad_N/0/1/0/all/0/1\">Narges Farokhshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molazadeh_M/0/1/0/all/0/1\">Milad Molazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamalabbasi_S/0/1/0/all/0/1\">Saman Jamalabbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giglou_H/0/1/0/all/0/1\">Hamed Babaei Giglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibak_S/0/1/0/all/0/1\">Saeed Bibak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"TransFusion: Cross-view Fusion with Transformer for 3D Human Pose Estimation. (arXiv:2110.09554v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09554","description":"<p>Estimating the 2D human poses in each view is typically the first step in\ncalibrated multi-view 3D pose estimation. But the performance of 2D pose\ndetectors suffers from challenging situations such as occlusions and oblique\nviewing angles. To address these challenges, previous works derive\npoint-to-point correspondences between different views from epipolar geometry\nand utilize the correspondences to merge prediction heatmaps or feature\nrepresentations. Instead of post-prediction merge/calibration, here we\nintroduce a transformer framework for multi-view 3D pose estimation, aiming at\ndirectly improving individual 2D predictors by integrating information from\ndifferent views. Inspired by previous multi-modal transformers, we design a\nunified transformer architecture, named TransFusion, to fuse cues from both\ncurrent views and neighboring views. Moreover, we propose the concept of\nepipolar field to encode 3D positional information into the transformer model.\nThe 3D position encoding guided by the epipolar field provides an efficient way\nof encoding correspondences between pixels of different views. Experiments on\nHuman 3.6M and Ski-Pose show that our method is more efficient and has\nconsistent improvements compared to other fusion methods. Specifically, we\nachieve 25.8 mm MPJPE on Human 3.6M with only 5M parameters on 256 x 256\nresolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deying Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiangyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yusheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shih-Yao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BGaitR-Net: Occluded Gait Sequence reconstructionwith temporally constrained model for gait recognition. (arXiv:2110.09564v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09564","description":"<p>Recent advancements in computational resources and Deep Learning\nmethodologies has significantly benefited development of intelligent\nvision-based surveillance applications. Gait recognition in the presence of\nocclusion is one of the challenging research topics in this area, and the\nsolutions proposed by researchers to date lack in robustness and also dependent\nof several unrealistic constraints, which limits their practical applicability.\nWe improve the state-of-the-art by developing novel deep learning-based\nalgorithms to identify the occluded frames in an input sequence and next\nreconstruct these occluded frames by exploiting the spatio-temporal information\npresent in the gait sequence. The multi-stage pipeline adopted in this work\nconsists of key pose mapping, occlusion detection and reconstruction, and\nfinally gait recognition. While the key pose mapping and occlusion detection\nphases are done %using Constrained KMeans Clustering and via a graph sorting\nalgorithm, reconstruction of occluded frames is done by fusing the key\npose-specific information derived in the previous step along with the\nspatio-temporal information contained in a gait sequence using a Bi-Directional\nLong Short Time Memory. This occlusion reconstruction model has been trained\nusing synthetically occluded CASIA-B and OU-ISIR data, and the trained model is\ntermed as Bidirectional Gait Reconstruction Network BGait-R-Net. Our LSTM-based\nmodel reconstructs occlusion and generates frames that are temporally\nconsistent with the periodic pattern of a gait cycle, while simultaneously\npreserving the body structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumara_S/0/1/0/all/0/1\">Somnath Sendhil Kumara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyaya_P/0/1/0/all/0/1\">Pratik Chattopadhyaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lipo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hands Off: A Handshake Interaction Detection and Localization Model for COVID-19 Threat Control. (arXiv:2110.09571v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09571","description":"<p>The COVID-19 outbreak has affected millions of people across the globe and is\ncontinuing to spread at a drastic scale. Out of the numerous steps taken to\ncontrol the spread of the virus, social distancing has been a crucial and\neffective practice. However, recent reports of social distancing violations\nsuggest the need for non-intrusive detection techniques to ensure safety in\npublic spaces. In this paper, a real-time detection model is proposed to\nidentify handshake interactions in a range of realistic scenarios with multiple\npeople in the scene and also detect multiple interactions in a single frame.\nThis is the first work that performs dyadic interaction localization in a\nmulti-person setting. The efficacy of the proposed model was evaluated across\ntwo different datasets on more than 3200 frames, thus enabling a robust\nlocalization model in different environments. The proposed model is the first\ndyadic interaction localizer in a multi-person setting, which enables it to be\nused in public spaces to identify handshake interactions and thereby identify\nand mitigate COVID-19 transmission.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1\">A.S. Jameel Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sritharan_S/0/1/0/all/0/1\">Suren Sritharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayatilaka_G/0/1/0/all/0/1\">Gihan Jayatilaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godaliyadda_R/0/1/0/all/0/1\">Roshan I. Godaliyadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_P/0/1/0/all/0/1\">Parakrama B. Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herath_V/0/1/0/all/0/1\">Vijitha Herath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_J/0/1/0/all/0/1\">Janaka B. Ekanayake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Feature Alignment for Semi-supervised Domain Adaptation. (arXiv:2110.09641v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09641","description":"<p>Most research on domain adaptation has focused on the purely unsupervised\nsetting, where no labeled examples in the target domain are available. However,\nin many real-world scenarios, a small amount of labeled target data is\navailable and can be used to improve adaptation. We address this\nsemi-supervised setting and propose to use dynamic feature alignment to address\nboth inter- and intra-domain discrepancy. Unlike previous approaches, which\nattempt to align source and target features within a mini-batch, we propose to\nalign the target features to a set of dynamically updated class prototypes,\nwhich we use both for minimizing divergence and pseudo-labeling. By updating\nbased on class prototypes, we avoid problems that arise in previous approaches\ndue to class imbalances. Our approach, which doesn't require extensive tuning\nor adversarial training, significantly improves the state of the art for\nsemi-supervised domain adaptation. We provide a quantitative evaluation on two\nstandard datasets, DomainNet and Office-Home, and performance analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Osteoporosis Prescreening using Panoramic Radiographs through a Deep Convolutional Neural Network with Attention Mechanism. (arXiv:2110.09662v1 [eess.IV])","link":"http://arxiv.org/abs/2110.09662","description":"<p>Objectives. The aim of this study was to investigate whether a deep\nconvolutional neural network (CNN) with an attention module can detect\nosteoporosis on panoramic radiographs.\n</p>\n<p>Study Design. A dataset of 70 panoramic radiographs (PRs) from 70 different\nsubjects of age between 49 to 60 was used, including 49 subjects with\nosteoporosis and 21 normal subjects. We utilized the leave-one-out\ncross-validation approach to generate 70 training and test splits.\nSpecifically, for each split, one image was used for testing and the remaining\n69 images were used for training. A deep convolutional neural network (CNN)\nusing the Siamese architecture was implemented through a fine-tuning process to\nclassify an PR image using patches extracted from eight representative\ntrabecula bone areas (Figure 1). In order to automatically learn the importance\nof different PR patches, an attention module was integrated into the deep CNN.\nThree metrics, including osteoporosis accuracy (OPA), non-osteoporosis accuracy\n(NOPA) and overall accuracy (OA), were utilized for performance evaluation.\n</p>\n<p>Results. The proposed baseline CNN approach achieved the OPA, NOPA and OA\nscores of 0.667, 0.878 and 0.814, respectively. With the help of the attention\nmodule, the OPA, NOPA and OA scores were further improved to 0.714, 0.939 and\n0.871, respectively.\n</p>\n<p>Conclusions. The proposed method obtained promising results using deep CNN\nwith an attention module, which might be applied to osteoporosis prescreening.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jiaxiang Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_Y/0/1/0/all/0/1\">Yi-Xian Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation. (arXiv:2110.09674v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09674","description":"<p>Knowledge Distillation is becoming one of the primary trends among neural\nnetwork compression algorithms to improve the generalization performance of a\nsmaller student model with guidance from a larger teacher model. This momentous\nrise in applications of knowledge distillation is accompanied by the\nintroduction of numerous algorithms for distilling the knowledge such as soft\ntargets and hint layers. Despite this advancement in different techniques for\ndistilling the knowledge, the aggregation of different paths for distillation\nhas not been studied comprehensively. This is of particular significance, not\nonly because different paths have different importance, but also due to the\nfact that some paths might have negative effects on the generalization\nperformance of the student model. Hence, we need to adaptively adjust the\nimportance of each path to maximize the impact of distillation on the student\nmodel. In this paper, we explore different approaches for aggregating these\ndifferent paths and introduce our proposed adaptive approach based on multitask\nlearning methods. We empirically demonstrate the effectiveness of the proposed\napproach over other baselines on the applications of knowledge distillation in\nclassification, semantic segmentation, and object detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chennupati_S/0/1/0/all/0/1\">Sumanth Chennupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamani_M/0/1/0/all/0/1\">Mohammad Mahdi Kamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhongwei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Vendor CT Image Data Harmonization Using CVH-CT. (arXiv:2110.09693v1 [eess.IV])","link":"http://arxiv.org/abs/2110.09693","description":"<p>While remarkable advances have been made in Computed Tomography (CT), most of\nthe existing efforts focus on imaging enhancement while reducing radiation\ndose. How to harmonize CT image data captured using different scanners is vital\nin cross-center large-scale radiomics studies but remains the boundary to\nexplore. Furthermore, the lack of paired training image problem makes it\ncomputationally challenging to adopt existing deep learning models. %developed\nfor CT image standardization. %this problem more challenging. We propose a\nnovel deep learning approach called CVH-CT for harmonizing CT images captured\nusing scanners from different vendors. The generator of CVH-CT uses a\nself-attention mechanism to learn the scanner-related information. We also\npropose a VGG feature-based domain loss to effectively extract texture\nproperties from unpaired image data to learn the scanner-based texture\ndistributions. The experimental results show that CVH-CT is clearly better than\nthe baselines because of the use of the proposed domain loss, and CVH-CT can\neffectively reduce the scanner-related variability in terms of radiomic\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Selim_M/0/1/0/all/0/1\">Md Selim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_B/0/1/0/all/0/1\">Baowei Fei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guo-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_G/0/1/0/all/0/1\">Gary Yeeming Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Quality Assessment in the Modern Age. (arXiv:2110.09699v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09699","description":"<p>This tutorial provides the audience with the basic theories, methodologies,\nand current progresses of image quality assessment (IQA). From an actionable\nperspective, we will first revisit several subjective quality assessment\nmethodologies, with emphasis on how to properly select visual stimuli. We will\nthen present in detail the design principles of objective quality assessment\nmodels, supplemented by an in-depth analysis of their advantages and\ndisadvantages. Both hand-engineered and (deep) learning-based methods will be\ncovered. Moreover, the limitations with the conventional model comparison\nmethodology for objective quality models will be pointed out, and novel\ncomparison methodologies such as those based on the theory of \"analysis by\nsynthesis\" will be introduced. We will last discuss the real-world multimedia\napplications of IQA, and give a list of open challenging problems, in the hope\nof encouraging more and more talented researchers and engineers devoting to\nthis exciting and rewarding research field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-aware IoU for Anchor Assignment in Real-time Instance Segmentation. (arXiv:2110.09734v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09734","description":"<p>This paper presents Mask-aware Intersection-over-Union (maIoU) for assigning\nanchor boxes as positives and negatives during training of instance\nsegmentation methods. Unlike conventional IoU or its variants, which only\nconsiders the proximity of two boxes; maIoU consistently measures the proximity\nof an anchor box with not only a ground truth box but also its associated\nground truth mask. Thus, additionally considering the mask, which, in fact,\nrepresents the shape of the object, maIoU enables a more accurate supervision\nduring training. We present the effectiveness of maIoU on a state-of-the-art\n(SOTA) assigner, ATSS, by replacing IoU operation by our maIoU and training\nYOLACT, a SOTA real-time instance segmentation method. Using ATSS with maIoU\nconsistently outperforms (i) ATSS with IoU by $\\sim 1$ mask AP, (ii) baseline\nYOLACT with fixed IoU threshold assigner by $\\sim 2$ mask AP over different\nimage sizes and (iii) decreases the inference time by $25 \\%$ owing to using\nless anchors. Then, exploiting this efficiency, we devise maYOLACT, a faster\nand $+6$ AP more accurate detector than YOLACT. Our best model achieves $37.7$\nmask AP at $25$ fps on COCO test-dev establishing a new state-of-the-art for\nreal-time instance segmentation. Code is available at\nhttps://github.com/kemaloksuz/Mask-aware-IoU\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1\">Kemal Oksuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cam_B/0/1/0/all/0/1\">Baris Can Cam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahraman_F/0/1/0/all/0/1\">Fehmi Kahraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltaci_Z/0/1/0/all/0/1\">Zeynep Sonat Baltaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1\">Sinan Kalkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Not to Reconstruct Anomalies. (arXiv:2110.09742v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09742","description":"<p>Video anomaly detection is often seen as one-class classification (OCC)\nproblem due to the limited availability of anomaly examples. Typically, to\ntackle this problem, an autoencoder (AE) is trained to reconstruct the input\nwith training set consisting only of normal data. At test time, the AE is then\nexpected to well reconstruct the normal data while poorly reconstructing the\nanomalous data. However, several studies have shown that, even with only normal\ndata training, AEs can often start reconstructing anomalies as well which\ndepletes the anomaly detection performance. To mitigate this problem, we\npropose a novel methodology to train AEs with the objective of reconstructing\nonly normal data, regardless of the input (i.e., normal or abnormal). Since no\nreal anomalies are available in the OCC settings, the training is assisted by\npseudo anomalies that are generated by manipulating normal data to simulate the\nout-of-normal-data distribution. We additionally propose two ways to generate\npseudo anomalies: patch and skip frame based. Extensive experiments on three\nchallenging video anomaly datasets demonstrate the effectiveness of our method\nin improving conventional AEs, achieving state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1\">Marcella Astrid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae-Yeong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Variability Augmented Sparse Unmixing of Hyperspectral Images. (arXiv:2110.09744v1 [eess.IV])","link":"http://arxiv.org/abs/2110.09744","description":"<p>Spectral unmixing (SU) expresses the mixed pixels existed in hyperspectral\nimages as the product of endmember and abundance, which has been widely used in\nhyperspectral imagery analysis. However, the influence of light, acquisition\nconditions and the inherent properties of materials, results in that the\nidentified endmembers can vary spectrally within a given image (construed as\nspectral variability). To address this issue, recent methods usually use a\npriori obtained spectral library to represent multiple characteristic spectra\nof the same object, but few of them extracted the spectral variability\nexplicitly. In this paper, a spectral variability augmented sparse unmixing\nmodel (SVASU) is proposed, in which the spectral variability is extracted for\nthe first time. The variable spectra are divided into two parts of intrinsic\nspectrum and spectral variability for spectral reconstruction, and modeled\nsynchronously in the SU model adding the regular terms restricting the sparsity\nof abundance and the generalization of the variability coefficient. It is noted\nthat the spectral variability library and the intrinsic spectral library are\nall constructed from the In-situ observed image. Experimental results over both\nsynthetic and real-world data sets demonstrate that the augmented decomposition\nby spectral variability significantly improves the unmixing performance than\nthe decomposition only by spectral library, as well as compared to\nstate-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_S/0/1/0/all/0/1\">Shaohui Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_M/0/1/0/all/0/1\">Mingyang Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Multimodal Transformer for Bi-directional Image and Text Generation. (arXiv:2110.09753v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09753","description":"<p>We study the joint learning of image-to-text and text-to-image generations,\nwhich are naturally bi-directional tasks. Typical existing works design two\nseparate task-specific models for each task, which impose expensive design\nefforts. In this work, we propose a unified image-and-text generative framework\nbased on a single multimodal model to jointly study the bi-directional tasks.\nWe adopt Transformer as our unified architecture for its strong performance and\ntask-agnostic design. Specifically, we formulate both tasks as sequence\ngeneration tasks, where we represent images and text as unified sequences of\ntokens, and the Transformer learns multimodal interactions to generate\nsequences. We further propose two-level granularity feature representations and\nsequence-level training to improve the Transformer-based unified framework.\nExperiments show that our approach significantly improves previous\nTransformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for\ntext-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for\nfine-tuned image-to-text generation on the MS-COCO dataset. Our code is\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongwei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation. (arXiv:2110.09756v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09756","description":"<p>A creative image-and-text generative AI system mimics humans' extraordinary\nabilities to provide users with diverse and comprehensive caption suggestions,\nas well as rich image creations. In this work, we demonstrate such an AI\ncreation system to produce both diverse captions and rich images. When users\nimagine an image and associate it with multiple captions, our system paints a\nrich image to reflect all captions faithfully. Likewise, when users upload an\nimage, our system depicts it with multiple diverse captions. We propose a\nunified multi-modal framework to achieve this goal. Specifically, our framework\njointly models image-and-text representations with a Transformer network, which\nsupports rich image creation by accepting multiple captions as input. We\nconsider the relations among input captions to encourage diversity in training\nand adopt a non-autoregressive decoding strategy to enable real-time inference.\nBased on these, our system supports both diverse captions and rich images\ngenerations. Our code is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Regularization Method to Improve Adversarial Robustness of Neural Networks for ECG Signal Classification. (arXiv:2110.09759v1 [cs.LG])","link":"http://arxiv.org/abs/2110.09759","description":"<p>Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor\nthe condition of the human heart. By using deep neural networks (DNNs),\ninterpretation of ECG signals can be fully automated for the identification of\npotential abnormalities in a patient's heart in a fraction of a second. Studies\nhave shown that given a sufficiently large amount of training data, DNN\naccuracy for ECG classification could reach human-expert cardiologist level.\nHowever, despite of the excellent performance in classification accuracy, DNNs\nare highly vulnerable to adversarial noises that are subtle changes in the\ninput of a DNN and may lead to a wrong class-label prediction. It is\nchallenging and essential to improve robustness of DNNs against adversarial\nnoises, which are a threat to life-critical applications. In this work, we\nproposed a regularization method to improve DNN robustness from the perspective\nof noise-to-signal ratio (NSR) for the application of ECG signal\nclassification. We evaluated our method on PhysioNet MIT-BIH dataset and\nCPSC2018 ECG dataset, and the results show that our method can substantially\nenhance DNN robustness against adversarial noises generated from adversarial\nattacks, with a minimal change in accuracy on clean data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linhai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Blurred Ground-based Sky/Cloud Images. (arXiv:2110.09764v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09764","description":"<p>Ground-based whole sky imagers (WSIs) are being used by researchers in\nvarious fields to study the atmospheric events. These ground-based sky cameras\ncapture visible-light images of the sky at regular intervals of time. Owing to\nthe atmospheric interference and camera sensor noise, the captured images often\nexhibit noise and blur. This may pose a problem in subsequent image processing\nstages. Therefore, it is important to accurately identify the blurred images.\nThis is a difficult task, as clouds have varying shapes, textures, and soft\nedges whereas the sky acts as a homogeneous and uniform background. In this\npaper, we propose an efficient framework that can identify the blurred\nsky/cloud images. Using a static external marker, our proposed methodology has\na detection accuracy of 94\\%. To the best of our knowledge, our approach is the\nfirst of its kind in the automatic identification of blurred images for\nground-based sky/cloud images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mayank Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Navya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yee Hui Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1\">Stefan Winkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Soumyabrata Dev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Augmented Deep Unfolding Network for Compressive Sensing. (arXiv:2110.09766v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09766","description":"<p>Mapping a truncated optimization method into a deep neural network, deep\nunfolding network (DUN) has attracted growing attention in compressive sensing\n(CS) due to its good interpretability and high performance. Each stage in DUNs\ncorresponds to one iteration in optimization. By understanding DUNs from the\nperspective of the human brain's memory processing, we find there exists two\nissues in existing DUNs. One is the information between every two adjacent\nstages, which can be regarded as short-term memory, is usually lost seriously.\nThe other is no explicit mechanism to ensure that the previous stages affect\nthe current stage, which means memory is easily forgotten. To solve these\nissues, in this paper, a novel DUN with persistent memory for CS is proposed,\ndubbed Memory-Augmented Deep Unfolding Network (MADUN). We design a\nmemory-augmented proximal mapping module (MAPMM) by combining two types of\nmemory augmentation mechanisms, namely High-throughput Short-term Memory (HSM)\nand Cross-stage Long-term Memory (CLM). HSM is exploited to allow DUNs to\ntransmit multi-channel short-term memory, which greatly reduces information\nloss between adjacent stages. CLM is utilized to develop the dependency of deep\ninformation across cascading stages, which greatly enhances network\nrepresentation capability. Extensive CS experiments on natural and MR images\nshow that with the strong ability to maintain and balance information our MADUN\noutperforms existing state-of-the-art methods by a large margin. The source\ncode is available at https://github.com/jianzhangcs/MADUN/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiechong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Temporal Anomaly Guided End-to-End Video Anomaly Detection. (arXiv:2110.09768v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09768","description":"<p>Due to the limited availability of anomaly examples, video anomaly detection\nis often seen as one-class classification (OCC) problem. A popular way to\ntackle this problem is by utilizing an autoencoder (AE) trained only on normal\ndata. At test time, the AE is then expected to reconstruct the normal input\nwell while reconstructing the anomalies poorly. However, several studies show\nthat, even with normal data only training, AEs can often start reconstructing\nanomalies as well which depletes their anomaly detection performance. To\nmitigate this, we propose a temporal pseudo anomaly synthesizer that generates\nfake-anomalies using only normal data. An AE is then trained to maximize the\nreconstruction loss on pseudo anomalies while minimizing this loss on normal\ndata. This way, the AE is encouraged to produce distinguishable reconstructions\nfor normal and anomalous frames. Extensive experiments and analysis on three\nchallenging video anomaly datasets demonstrate the effectiveness of our\napproach to improve the basic AEs in achieving superiority against several\nexisting state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1\">Marcella Astrid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry. (arXiv:2110.09772v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09772","description":"<p>This work studies learning from a synergy process of 3D Morphable Models\n(3DMM) and 3D facial landmarks to predict complete 3D facial geometry,\nincluding 3D alignment, face orientation, and 3D face modeling. Our synergy\nprocess leverages a representation cycle for 3DMM parameters and 3D landmarks.\n3D landmarks can be extracted and refined from face meshes built by 3DMM\nparameters. We next reverse the representation direction and show that\npredicting 3DMM parameters from sparse 3D landmarks improves the information\nflow. Together we create a synergy process that utilizes the relation between\n3D landmarks and 3DMM parameters, and they collaboratively contribute to better\nperformance. We extensively validate our contribution on full tasks of facial\ngeometry prediction and show our superior and robust performance on these tasks\nfor various scenarios. Particularly, we adopt only simple and widely-used\nnetwork operations to attain fast and accurate facial geometry prediction.\nCodes and data: https://choyingw.github.io/works/SynergyNet/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cho-Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aesthetic Photo Collage with Deep Reinforcement Learning. (arXiv:2110.09775v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09775","description":"<p>Photo collage aims to automatically arrange multiple photos on a given canvas\nwith high aesthetic quality. Existing methods are based mainly on handcrafted\nfeature optimization, which cannot adequately capture high-level human\naesthetic senses. Deep learning provides a promising way, but owing to the\ncomplexity of collage and lack of training data, a solution has yet to be\nfound. In this paper, we propose a novel pipeline for automatic generation of\naspect ratio specified collage and the reinforcement learning technique is\nintroduced in collage for the first time. Inspired by manual collages, we model\nthe collage generation as sequential decision process to adjust spatial\npositions, orientation angles, placement order and the global layout. To\ninstruct the agent to improve both the overall layout and local details, the\nreward function is specially designed for collage, considering subjective and\nobjective factors. To overcome the lack of training data, we pretrain our deep\naesthetic network on a large scale image aesthetic dataset (CPC) for general\naesthetic feature extraction and propose an attention fusion module for\nstructural collage feature representation. We test our model against competing\nmethods on two movie datasets and our results outperform others in aesthetic\nquality evaluation. Further user study is also conducted to demonstrate the\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mading Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Toxic and Narcotic Medication Detection with Rotated Object Detector. (arXiv:2110.09777v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09777","description":"<p>Recent years have witnessed the advancement of deep learning vision\ntechnologies and applications in the medical industry. Intelligent devices for\nspecial medication management are in great need of, which requires more precise\ndetection algorithms to identify the specifications and locations. In this\nwork, YOLO (You only look once) based object detectors are tailored for toxic\nand narcotic medications detection tasks. Specifically, a more flexible\nannotation with rotated degree ranging from $0^\\circ$ to $90^\\circ$ and a\nmask-mapping-based non-maximum suppression method are proposed to achieve a\nfeasible and efficient medication detector aiming at arbitrarily oriented\nbounding boxes. Extensive experiments demonstrate that the rotated YOLO\ndetectors are more suitable for identifying densely arranged drugs. The best\nshot mean average precision of the proposed network reaches 0.811 while the\ninference time is less than 300ms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhongqiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiying Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zichen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinghan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Transformer for 3D Point Cloud Sequences. (arXiv:2110.09783v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09783","description":"<p>Effective learning of spatial-temporal information within a point cloud\nsequence is highly important for many down-stream tasks such as 4D semantic\nsegmentation and 3D action recognition. In this paper, we propose a novel\nframework named Point Spatial-Temporal Transformer (PST2) to learn\nspatial-temporal representations from dynamic 3D point cloud sequences. Our\nPST2 consists of two major modules: a Spatio-Temporal Self-Attention (STSA)\nmodule and a Resolution Embedding (RE) module. Our STSA module is introduced to\ncapture the spatial-temporal context information across adjacent frames, while\nthe RE module is proposed to aggregate features across neighbors to enhance the\nresolution of feature maps. We test the effectiveness our PST2 with two\ndifferent tasks on point cloud sequences, i.e., 4D semantic segmentation and 3D\naction recognition. Extensive experiments on three benchmarks show that our\nPST2 outperforms existing methods on all datasets. The effectiveness of our\nSTSA and RE modules have also been justified with ablation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yimin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tingting Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis. (arXiv:2110.09788v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09788","description":"<p>The style-based GAN (StyleGAN) architecture achieved state-of-the-art results\nfor generating high-quality images, but it lacks explicit and precise control\nover camera poses. The recently proposed NeRF-based GANs made great progress\ntowards 3D-aware generators, but they are unable to generate high-quality\nimages yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that\nis composed of a shallow NeRF network and a deep implicit neural representation\n(INR) network. The generator synthesizes each pixel value independently without\nany spatial convolution or upsampling operation. In addition, we diagnose the\nproblem of mirror symmetry that implies a suboptimal solution and solve it by\nintroducing an auxiliary discriminator. Trained on raw, single-view images,\nCIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of\n6.97 for images at the $256\\times256$ resolution on FFHQ. We also demonstrate\nseveral interesting directions for CIPS-3D such as transfer learning and\n3D-aware face stylization. The synthesis results are best viewed as videos, so\nwe recommend the readers to check our github project at\nhttps://github.com/PeterouZh/CIPS-3D\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geo-DefakeHop: High-Performance Geographic Fake Image Detection. (arXiv:2110.09795v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09795","description":"<p>A robust fake satellite image detection method, called Geo-DefakeHop, is\nproposed in this work. Geo-DefakeHop is developed based on the parallel\nsubspace learning (PSL) methodology. PSL maps the input image space into\nseveral feature subspaces using multiple filter banks. By exploring response\ndifferences of different channels between real and fake images for a filter\nbank, Geo-DefakeHop learns the most discriminant channels and uses their soft\ndecision scores as features. Then, Geo-DefakeHop selects a few discriminant\nfeatures from each filter bank and ensemble them to make a final binary\ndecision. Geo-DefakeHop offers a light-weight high-performance solution to fake\nsatellite images detection. Its model size is analyzed, which ranges from 0.8\nto 62K parameters. Furthermore, it is shown by experimental results that it\nachieves an F1-score higher than 95\\% under various common image manipulations\nsuch as resizing, compression and noise corruption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong-Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaitai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent reweighting, an almost free improvement for GANs. (arXiv:2110.09803v1 [cs.LG])","link":"http://arxiv.org/abs/2110.09803","description":"<p>Standard formulations of GANs, where a continuous function deforms a\nconnected latent space, have been shown to be misspecified when fitting\ndifferent classes of images. In particular, the generator will necessarily\nsample some low-quality images in between the classes. Rather than modifying\nthe architecture, a line of works aims at improving the sampling quality from\npre-trained generators at the expense of increased computational cost. Building\non this, we introduce an additional network to predict latent importance\nweights and two associated sampling methods to avoid the poorest samples. This\nidea has several advantages: 1) it provides a way to inject disconnectedness\ninto any GAN architecture, 2) since the rejection happens in the latent space,\nit avoids going through both the generator and the discriminator, saving\ncomputation time, 3) this importance weights formulation provides a principled\nway to reduce the Wasserstein's distance to the target distribution. We\ndemonstrate the effectiveness of our method on several datasets, both synthetic\nand high-dimensional.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Issenhuth_T/0/1/0/all/0/1\">Thibaut Issenhuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1\">Ugo Tanielian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mary_J/0/1/0/all/0/1\">Jeremie Mary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Microstructure reconstruction via artificial neural networks: A combination of causal and non-causal approach. (arXiv:2110.09815v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2110.09815","description":"<p>We investigate the applicability of artificial neural networks (ANNs) in\nreconstructing a sample image of a sponge-like microstructure. We propose to\nreconstruct the image by predicting the phase of the current pixel based on its\ncausal neighbourhood, and subsequently, use a non-causal ANN model to smooth\nout the reconstructed image as a form of post-processing. We also consider the\nimpacts of different configurations of the ANN model (e.g. number of densely\nconnected layers, number of neurons in each layer, the size of both the causal\nand non-causal neighbourhood) on the models' predictive abilities quantified by\nthe discrepancy between the spatial statistics of the reference and the\nreconstructed sample.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Latka_K/0/1/0/all/0/1\">Kry&#x161;tof Latka</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Doskar_M/0/1/0/all/0/1\">Martin Do&#x161;k&#xe1;&#x159;</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zeman_J/0/1/0/all/0/1\">Jan Zeman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSTC: Boosting Atomic Action Detection with Long-Short-Term Context. (arXiv:2110.09819v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09819","description":"<p>In this paper, we place the atomic action detection problem into a Long-Short\nTerm Context (LSTC) to analyze how the temporal reliance among video signals\naffect the action detection results. To do this, we decompose the action\nrecognition pipeline into short-term and long-term reliance, in terms of the\nhypothesis that the two kinds of context are conditionally independent given\nthe objective action instance. Within our design, a local aggregation branch is\nutilized to gather dense and informative short-term cues, while a high order\nlong-term inference branch is designed to reason the objective action class\nfrom high-order interaction between actor and other person or person pairs.\nBoth branches independently predict the context-specific actions and the\nresults are merged in the end. We demonstrate that both temporal grains are\nbeneficial to atomic action recognition. On the mainstream benchmarks of atomic\naction detection, our design can bring significant performance gain from the\nexisting state-of-the-art pipeline. The code of this project can be found at\n[this url](https://github.com/TencentYoutuResearch/ActionDetection-LSTC)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Hidden Bias within Face Recognition via Racial Phenotypes. (arXiv:2110.09839v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09839","description":"<p>Recent work reports disparate performance for intersectional racial groups\nacross face recognition tasks: face verification and identification. However,\nthe definition of those racial groups has a significant impact on the\nunderlying findings of such racial bias analysis. Previous studies define these\ngroups based on either demographic information (e.g. African, Asian etc.) or\nskin tone (e.g. lighter or darker skins). The use of such sensitive or broad\ngroup definitions has disadvantages for bias investigation and subsequent\ncounter-bias solutions design. By contrast, this study introduces an\nalternative racial bias analysis methodology via facial phenotype attributes\nfor face recognition. We use the set of observable characteristics of an\nindividual face where a race-related facial phenotype is hence specific to the\nhuman face and correlated to the racial profile of the subject. We propose\ncategorical test cases to investigate the individual influence of those\nattributes on bias within face recognition tasks. We compare our\nphenotype-based grouping methodology with previous grouping strategies and show\nthat phenotype-based groupings uncover hidden bias without reliance upon any\npotentially protected attributes or ill-defined grouping strategies.\nFurthermore, we contribute corresponding phenotype attribute category labels\nfor two face recognition tasks: RFW for face verification and VGGFace2 (test\nset) for face identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yucer_S/0/1/0/all/0/1\">Seyma Yucer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tektas_F/0/1/0/all/0/1\">Furkan Tektas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cutting Voxel Projector a New Approach to Construct 3D Cone Beam CT Operator. (arXiv:2110.09841v1 [eess.IV])","link":"http://arxiv.org/abs/2110.09841","description":"<p>In this paper, we introduce a new class of projectors for 3D cone beam\ntomographic reconstruction. We find analytical formulas for the relationship\nbetween the voxel volume projected onto a given detector pixel and its\ncontribution to the extinction value detected on that pixel. Using this\napproach, we construct a near-exact projector and backprojector that can be\nused especially for algebraic reconstruction techniques. We have implemented\nthis cutting voxel projector and a less accurate, speed-optimized version of it\ntogether with two established projectors, a ray tracing projector based on\nSiddon's algorithm and a TT footprint projector. We show that the cutting voxel\nprojector achieves, especially for large cone beam angles, noticeably higher\naccuracy than the TT projector. Moreover, our implementation of the relaxed\nversion of the cutting voxel projector is significantly faster than current\nfootprint projector implementations. We further show that Siddon's algorithm\nwith comparable accuracy would be much slower than the cutting voxel projector.\nAll algorithms are implemented within an open source framework for algebraic\nreconstruction in OpenCL 1.2 and C++ and are optimized for GPU computation.\nThey are published as open-source software under the GNU GPL 3 license, see\nhttps://github.com/kulvait/KCT_cbct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kulvait_V/0/1/0/all/0/1\">Vojt&#x11b;ch Kulvait</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Rose_G/0/1/0/all/0/1\">Georg Rose</a> (1) ((1) Institute for Medical Engineering and Research Campus STIMULATE, University of Magdeburg, Magdeburg, Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Object Detection via Generative Image Synthesis. (arXiv:2110.09848v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09848","description":"<p>We present SSOD, the first end-to-end analysis-by synthesis framework with\ncontrollable GANs for the task of self-supervised object detection. We use\ncollections of real world images without bounding box annotations to learn to\nsynthesize and detect objects. We leverage controllable GANs to synthesize\nimages with pre-defined object properties and use them to train object\ndetectors. We propose a tight end-to-end coupling of the synthesis and\ndetection networks to optimally train our system. Finally, we also propose a\nmethod to optimally adapt SSOD to an intended target data without requiring\nlabels for it. For the task of car detection, on the challenging KITTI and\nCityscapes datasets, we show that SSOD outperforms the prior state-of-the-art\npurely image-based self-supervised object detection method Wetectron. Even\nwithout requiring any 3D CAD assets, it also surpasses the state-of-the-art\nrendering based method Meta-Sim2. Our work advances the field of\nself-supervised object detection by introducing a successful new paradigm of\nusing controllable GAN-based image synthesis for it and by significantly\nimproving the baseline accuracy of the task. We open-source our code at\nhttps://github.com/NVlabs/SSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mustikovela_S/0/1/0/all/0/1\">Siva Karthik Mustikovela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aayush Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Phuoc_T/0/1/0/all/0/1\">Thu Nguyen-Phuoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral-ViT for Robust Fovea Localization. (arXiv:2110.09860v1 [eess.IV])","link":"http://arxiv.org/abs/2110.09860","description":"<p>The fovea is an important anatomical landmark of the retina. Detecting the\nlocation of the fovea is essential for the analysis of many retinal diseases.\nHowever, robust fovea localization remains a challenging problem, as the fovea\nregion often appears fuzzy, and retina diseases may further obscure its\nappearance. This paper proposes a novel vision transformer (ViT) approach that\nintegrates information both inside and outside the fovea region to achieve\nrobust fovea localization. Our proposed network named\nBilateral-Vision-Transformer (Bilateral-ViT) consists of two network branches:\na transformer-based main network branch for integrating global context across\nthe entire fundus image and a vessel branch for explicitly incorporating the\nstructure of blood vessels. The encoded features from both network branches are\nsubsequently merged with a customized multi-scale feature fusion (MFF) module.\nOur comprehensive experiments demonstrate that the proposed approach is\nsignificantly more robust for diseased images and establishes the new state of\nthe arts on both Messidor and PALM datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_K/0/1/0/all/0/1\">Kang Dang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Q/0/1/0/all/0/1\">Qinji Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coenen_F/0/1/0/all/0/1\">Frans Coenen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xiaowei Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a self-supervised tone mapping operator via feature contrast masking loss. (arXiv:2110.09866v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09866","description":"<p>High Dynamic Range (HDR) content is becoming ubiquitous due to the rapid\ndevelopment of capture technologies. Nevertheless, the dynamic range of common\ndisplay devices is still limited, therefore tone mapping (TM) remains a key\nchallenge for image visualization. Recent work has demonstrated that neural\nnetworks can achieve remarkable performance in this task when compared to\ntraditional methods, however, the quality of the results of these\nlearning-based methods is limited by the training data. Most existing works use\nas training set a curated selection of best-performing results from existing\ntraditional tone mapping operators (often guided by a quality metric),\ntherefore, the quality of newly generated results is fundamentally limited by\nthe performance of such operators. This quality might be even further limited\nby the pool of HDR content that is used for training. In this work we propose a\nlearning-based self-supervised tone mapping operator that is trained at test\ntime specifically for each HDR image and does not need any data labeling. The\nkey novelty of our approach is a carefully designed loss function built upon\nfundamental knowledge on contrast perception that allows for directly comparing\nthe content in the HDR and tone mapped images. We achieve this goal by\nreformulating classic VGG feature maps into feature contrast maps that\nnormalize local feature differences by their average magnitude in a local\nneighborhood, allowing our loss to account for contrast masking effects. We\nperform extensive ablation studies and exploration of parameters and\ndemonstrate that our solution outperforms existing approaches with a single set\nof fixed parameters, as confirmed by both objective and subjective metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1\">Hans-Peter Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1\">Karol Myszkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_A/0/1/0/all/0/1\">Ana Serrano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HM-Net: A Regression Network for Object Center Detection and Tracking on Wide Area Motion Imagery. (arXiv:2110.09881v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09881","description":"<p>Wide Area Motion Imagery (WAMI) yields high resolution images with a large\nnumber of extremely small objects. Target objects have large spatial\ndisplacements throughout consecutive frames. This nature of WAMI images makes\nobject tracking and detection challenging. In this paper, we present our deep\nneural network-based combined object detection and tracking model, namely, Heat\nMap Network (HM-Net). HM-Net is significantly faster than state-of-the-art\nframe differencing and background subtraction-based methods, without\ncompromising detection and tracking performances. HM-Net follows object\ncenter-based joint detection and tracking paradigm. Simple heat map-based\npredictions support unlimited number of simultaneous detections. The proposed\nmethod uses two consecutive frames and the object detection heat map obtained\nfrom the previous frame as input, which helps HM-Net monitor spatio-temporal\nchanges between frames and keeps track of previously predicted objects.\nAlthough reuse of prior object detection heat map acts as a vital\nfeedback-based memory element, it can lead to unintended surge of false\npositive detections. To increase robustness of the method against false\npositives and to eliminate low confidence detections, HM-Net employs novel\nfeedback filters and advanced data augmentations. HM-Net outperforms\nstate-of-the-art WAMI moving object detection and tracking methods on WPAFB\ndataset with its 96.2% F1 and 94.4% mAP detection scores, while achieving a\n61.8% mAP tracking score on the same dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Motorcu_H/0/1/0/all/0/1\">Hakki Motorcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ates_H/0/1/0/all/0/1\">Hasan F. Ates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugurdag_H/0/1/0/all/0/1\">H. Fatih Ugurdag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunturk_B/0/1/0/all/0/1\">Bahadir Gunturk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unrestricted Adversarial Attacks on ImageNet Competition. (arXiv:2110.09903v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09903","description":"<p>Many works have investigated the adversarial attacks or defenses under the\nsettings where a bounded and imperceptible perturbation can be added to the\ninput. However in the real-world, the attacker does not need to comply with\nthis restriction. In fact, more threats to the deep model come from\nunrestricted adversarial examples, that is, the attacker makes large and\nvisible modifications on the image, which causes the model classifying\nmistakenly, but does not affect the normal observation in human perspective.\nUnrestricted adversarial attack is a popular and practical direction but has\nnot been studied thoroughly. We organize this competition with the purpose of\nexploring more effective unrestricted adversarial attack algorithm, so as to\naccelerate the academical research on the model robustness under stronger\nunbounded attacks. The competition is held on the TianChi platform\n(\\url{https://tianchi.aliyun.com/competition/entrance/531853/introduction}) as\none of the series of AI Security Challengers Program.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaofeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qi-An Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Haoran Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yidan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Taoyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huanqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Ying Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Bingyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yunfei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yekui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haorong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional De-Identification of 3D Magnetic Resonance Images. (arXiv:2110.09927v1 [eess.IV])","link":"http://arxiv.org/abs/2110.09927","description":"<p>Privacy protection of medical image data is challenging. Even if metadata is\nremoved, brain scans are vulnerable to attacks that match renderings of the\nface to facial image databases. Solutions have been developed to de-identify\ndiagnostic scans by obfuscating or removing parts of the face. However, these\nsolutions either fail to reliably hide the patient's identity or are so\naggressive that they impair further analyses. We propose a new class of\nde-identification techniques that, instead of removing facial features,\nremodels them. Our solution relies on a conditional multi-scale GAN\narchitecture. It takes a patient's MRI scan as input and generates a 3D volume\nconditioned on the patient's brain, which is preserved exactly, but where the\nface has been de-identified through remodeling. We demonstrate that our\napproach preserves privacy far better than existing techniques, without\ncompromising downstream medical analyses. Analyses were run on the OASIS-3 and\nADNI corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goten_L/0/1/0/all/0/1\">Lennart Alexander Van der Goten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_K/0/1/0/all/0/1\">Kevin Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralDiff: Segmenting 3D objects that move in egocentric videos. (arXiv:2110.09936v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09936","description":"<p>Given a raw video sequence taken from a freely-moving camera, we study the\nproblem of decomposing the observed 3D scene into a static background and a\ndynamic foreground containing the objects that move in the video sequence. This\ntask is reminiscent of the classic background subtraction problem, but is\nsignificantly harder because all parts of the scene, static and dynamic,\ngenerate a large apparent motion due to the camera large viewpoint change. In\nparticular, we consider egocentric videos and further separate the dynamic\ncomponent into objects and the actor that observes and moves them. We achieve\nthis factorization by reconstructing the video via a triple-stream neural\nrendering network that explains the different motions based on corresponding\ninductive biases. We demonstrate that our method can successfully separate the\ndifferent types of motion, outperforming recent neural rendering baselines at\nthis task, and can accurately segment moving objects. We do so by assessing the\nmethod empirically on challenging videos from the EPIC-KITCHENS dataset which\nwe augment with appropriate annotations to create a new benchmark for the task\nof dynamic object segmentation on unconstrained video sequences, for complex 3D\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tschernezki_V/0/1/0/all/0/1\">Vadim Tschernezki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking Head Generation with Audio and Speech Related Facial Action Units. (arXiv:2110.09951v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09951","description":"<p>The task of talking head generation is to synthesize a lip synchronized\ntalking head video by inputting an arbitrary face image and audio clips. Most\nexisting methods ignore the local driving information of the mouth muscles. In\nthis paper, we propose a novel recurrent generative network that uses both\naudio and speech-related facial action units (AUs) as the driving information.\nAU information related to the mouth can guide the movement of the mouth more\naccurately. Since speech is highly correlated with speech-related AUs, we\npropose an Audio-to-AU module in our system to predict the speech-related AU\ninformation from speech. In addition, we use AU classifier to ensure that the\ngenerated images contain correct AU information. Frame discriminator is also\nconstructed for adversarial training to improve the realism of the generated\nface. We verify the effectiveness of our model on the GRID dataset and\nTCD-TIMIT dataset. We also conduct an ablation study to verify the contribution\nof each component in our model. Quantitative and qualitative experiments\ndemonstrate that our method outperforms existing methods in both image quality\nand lip-sync accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhengxiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positional-Spectral-Temporal Attention in 3D Convolutional Neural Networks for EEG Emotion Recognition. (arXiv:2110.09955v1 [eess.SP])","link":"http://arxiv.org/abs/2110.09955","description":"<p>Recognizing the feelings of human beings plays a critical role in our daily\ncommunication. Neuroscience has demonstrated that different emotion states\npresent different degrees of activation in different brain regions, EEG\nfrequency bands and temporal stamps. In this paper, we propose a novel\nstructure to explore the informative EEG features for emotion recognition. The\nproposed module, denoted by PST-Attention, consists of Positional, Spectral and\nTemporal Attention modules to explore more discriminative EEG features.\nSpecifically, the Positional Attention module is to capture the activate\nregions stimulated by different emotions in the spatial dimension. The Spectral\nand Temporal Attention modules assign the weights of different frequency bands\nand temporal slices respectively. Our method is adaptive as well as efficient\nwhich can be fit into 3D Convolutional Neural Networks (3D-CNN) as a plug-in\nmodule. We conduct experiments on two real-world datasets. 3D-CNN combined with\nour module achieves promising results and demonstrate that the PST-Attention is\nable to capture stable patterns for emotion recognition from EEG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiyao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanxi Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_D/0/1/0/all/0/1\">Dongmei Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Three-dimensional Radial Visualization. (arXiv:2110.09971v1 [stat.ME])","link":"http://arxiv.org/abs/2110.09971","description":"<p>We develop methodology for three-dimensional (3D) radial visualization\n(RadViz) of multidimensional datasets. The classical two-dimensional (2D)\nRadViz visualizes multivariate data in the 2D plane by mapping every\nobservation to a point inside the unit circle. Our tool, RadViz3D, distributes\nanchor points uniformly on the 3D unit sphere. We show that this uniform\ndistribution provides the best visualization with minimal artificial visual\ncorrelation for data with uncorrelated variables. However, anchor points can be\nplaced exactly equi-distant from each other only for the five Platonic solids,\nso we provide equi-distant anchor points for these five settings, and\napproximately equi-distant anchor points via a Fibonacci grid for the other\ncases. Our methodology, implemented in the R package $radviz3d$, makes fully 3D\nRadViz possible and is shown to improve the ability of this nonlinear technique\nin more faithfully displaying simulated data as well as the crabs, olive oils\nand wine datasets. Additionally, because radial visualization is naturally\nsuited for compositional data, we use RadViz3D to illustrate (i) the chemical\ncomposition of Longquan celadon ceramics and their Jingdezhen imitation over\ncenturies, and (ii) US regional SARS-Cov-2 variants' prevalence in the Covid-19\npandemic during the summer 2021 surge of the Delta variant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifan Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dai_F/0/1/0/all/0/1\">Fan Dai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Optimal Correlational Object Search. (arXiv:2110.09991v1 [cs.RO])","link":"http://arxiv.org/abs/2110.09991","description":"<p>In realistic applications of object search, robots will need to locate target\nobjects in complex environments while coping with unreliable sensors,\nespecially for small or hard-to-detect objects. In such settings, correlational\ninformation can be valuable for planning efficiently: when looking for a fork,\nthe robot could start by locating the easier-to-detect refrigerator, since\nforks would probably be found nearby. Previous approaches to object search with\ncorrelational information typically resort to ad-hoc or greedy search\nstrategies. In this paper, we propose the Correlational Object Search POMDP\n(COS-POMDP), which can be solved to produce search strategies that use\ncorrelational information. COS-POMDPs contain a correlation-based observation\nmodel that allows us to avoid the exponential blow-up of maintaining a joint\nbelief about all objects, while preserving the optimal solution to this naive,\nexponential POMDP formulation. We propose a hierarchical planning algorithm to\nscale up COS-POMDP for practical domains. We conduct experiments using\nAI2-THOR, a realistic simulator of household environments, as well as YOLOv5, a\nwidely-used object detector. Our results show that, particularly for\nhard-to-detect objects, such as scrub brush and remote control, our method\noffers the most robust performance compared to baselines that ignore\ncorrelations as well as a greedy, next-best view approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1\">Rohan Chitnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yoonchang Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERQA: Edge-Restoration Quality Assessment for Video Super-Resolution. (arXiv:2110.09992v1 [eess.IV])","link":"http://arxiv.org/abs/2110.09992","description":"<p>Despite the growing popularity of video super-resolution (VSR), there is\nstill no good way to assess the quality of the restored details in upscaled\nframes. Some SR methods may produce the wrong digit or an entirely different\nface. Whether a method's results are trustworthy depends on how well it\nrestores truthful details. Image super-resolution can use natural distributions\nto produce a high-resolution image that is only somewhat similar to the real\none. VSR enables exploration of additional information in neighboring frames to\nrestore details from the original scene. The ERQA metric, which we propose in\nthis paper, aims to estimate a model's ability to restore real details using\nVSR. On the assumption that edges are significant for detail and character\nrecognition, we chose edge fidelity as the foundation for this metric.\nExperimental validation of our work is based on the MSU Video Super-Resolution\nBenchmark, which includes the most difficult patterns for detail restoration\nand verifies the fidelity of details from the original frame. Code for the\nproposed metric is publicly available at\nhttps://github.com/msu-video-group/ERQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kirillova_A/0/1/0/all/0/1\">Anastasia Kirillova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyapustin_E/0/1/0/all/0/1\">Eugene Lyapustin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitry Vatolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPFM: Deep Partial Functional Maps. (arXiv:2110.09994v1 [cs.CV])","link":"http://arxiv.org/abs/2110.09994","description":"<p>We consider the problem of computing dense correspondences between non-rigid\nshapes with potentially significant partiality. Existing formulations tackle\nthis problem through heavy manifold optimization in the spectral domain, given\nhand-crafted shape descriptors. In this paper, we propose the first learning\nmethod aimed directly at partial non-rigid shape correspondence. Our approach\nuses the functional map framework, can be trained in a supervised or\nunsupervised manner, and learns descriptors directly from the data, thus both\nimproving robustness and accuracy in challenging cases. Furthermore, unlike\nexisting techniques, our method is also applicable to partial-to-partial\nnon-rigid matching, in which the common regions on both shapes are unknown a\npriori. We demonstrate that the resulting method is data-efficient, and\nachieves state-of-the-art results on several benchmark datasets. Our code and\ndata can be found online: https://github.com/pvnieo/DPFM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attaiki_S/0/1/0/all/0/1\">Souhaib Attaiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_G/0/1/0/all/0/1\">Gautam Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-driven and Automatic Surface Texture Analysis Using Persistent Homology. (arXiv:2110.10005v1 [eess.SP])","link":"http://arxiv.org/abs/2110.10005","description":"<p>Surface roughness plays an important role in analyzing engineering surfaces.\nIt quantifies the surface topography and can be used to determine whether the\nresulting surface finish is acceptable or not. Nevertheless, while several\nexisting tools and standards are available for computing surface roughness,\nthese methods rely heavily on user input thus slowing down the analysis and\nincreasing manufacturing costs. Therefore, fast and automatic determination of\nthe roughness level is essential to avoid costs resulting from surfaces with\nunacceptable finish, and user-intensive analysis. In this study, we propose a\nTopological Data Analysis (TDA) based approach to classify the roughness level\nof synthetic surfaces using both their areal images and profiles. We utilize\npersistent homology from TDA to generate persistence diagrams that encapsulate\ninformation on the shape of the surface. We then obtain feature matrices for\neach surface or profile using Carlsson coordinates, persistence images, and\ntemplate functions. We compare our results to two widely used methods in the\nliterature: Fast Fourier Transform (FFT) and Gaussian filtering. The results\nshow that our approach yields mean accuracies as high as 97%. We also show\nthat, in contrast to existing surface analysis tools, our TDA-based approach is\nfully automatable and provides adaptive feature extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yesilli_M/0/1/0/all/0/1\">Melih C. Yesilli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khasawneh_F/0/1/0/all/0/1\">Firas A. Khasawneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference. (arXiv:2110.10031v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10031","description":"<p>Despite rapid advances in continual learning, a large body of research is\ndevoted to improving performance in the existing setups. While a handful of\nwork do propose new continual learning setups, they still lack practicality in\ncertain aspects. For better practicality, we first propose a novel continual\nlearning setup that is online, task-free, class-incremental, of blurry task\nboundaries and subject to inference queries at any moment. We additionally\npropose a new metric to better measure the performance of the continual\nlearning methods subject to inference queries at any moment. To address the\nchallenging setup and evaluation protocol, we propose an effective method that\nemploys a new memory management scheme and novel learning techniques. Our\nempirical validation demonstrates that the proposed method outperforms prior\narts by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1\">Hyunseo Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Tail-Class Representation with Centroid Contrastive Learning. (arXiv:2110.10048v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10048","description":"<p>In vision domain, large-scale natural datasets typically exhibit long-tailed\ndistribution which has large class imbalance between head and tail classes.\nThis distribution poses difficulty in learning good representations for tail\nclasses. Recent developments have shown good long-tailed model can be learnt by\ndecoupling the training into representation learning and classifier balancing.\nHowever, these works pay insufficient consideration on the long-tailed effect\non representation learning. In this work, we propose interpolative centroid\ncontrastive learning (ICCL) to improve long-tailed representation learning.\nICCL interpolates two images from a class-agnostic sampler and a class-aware\nsampler, and trains the model such that the representation of the interpolative\nimage can be used to retrieve the centroids for both source classes. We\ndemonstrate the effectiveness of our approach on multiple long-tailed image\nclassification benchmarks. Our result shows a significant accuracy gain of 2.8%\non the iNaturalist 2018 dataset with a real-world long-tailed distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiong_A/0/1/0/all/0/1\">Anthony Meng Huat Tiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Primal-Dual Deep Unrolling Networks for Imaging Inverse Problems. (arXiv:2110.10093v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10093","description":"<p>In this work we present a new type of efficient deep-unrolling networks for\nsolving imaging inverse problems. Classical deep-unrolling methods require full\nforward operator and its adjoint across each layer, and hence can be\ncomputationally more expensive than other end-to-end methods such as\nFBP-ConvNet, especially in 3D image reconstruction tasks. We propose a\nstochastic (ordered-subsets) extension of the Learned Primal-Dual (LPD) which\nis the state-of-the-art unrolling network. In our unrolling network, we only\nuse a subset of the forward and adjoint operator, to achieve computational\nefficiency. We consider 3 ways of training the proposed network to cope with\ndifferent scenarios of the availability of the training data, including (1)\nsupervised training on paired data, (2) unsupervised adversarial training which\nenable us to train the network without paired ground-truth data, (3)\nequivariant self-supervised training approach, which utilizes equivariant\nstructure which is prevalent in many imaging applications, and only requires\nmeasurement data. Our numerical results demonstrate the effectiveness of our\napproach in X-ray CT imaging task, showing that our networks achieve similar\nreconstruction accuracies as the full-batch LPD, while require only a fraction\nof the computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1\">Junqi Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition. (arXiv:2110.10101v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10101","description":"<p>First person action recognition is becoming an increasingly researched area\nthanks to the rising popularity of wearable cameras. This is bringing to light\ncross-domain issues that are yet to be addressed in this context. Indeed, the\ninformation extracted from learned representations suffers from an intrinsic\n\"environmental bias\". This strongly affects the ability to generalize to unseen\nscenarios, limiting the application of current methods to real settings where\nlabeled data are not available during training. In this work, we introduce the\nfirst domain generalization approach for egocentric activity recognition, by\nproposing a new audio-visual loss, called Relative Norm Alignment loss. It\nre-balances the contributions from the two modalities during training, over\ndifferent domains, by aligning their feature norm representations. Our approach\nleads to strong results in domain generalization on both EPIC-Kitchens-55 and\nEPIC-Kitchens-100, as demonstrated by extensive experiments, and can be\nextended to work also on domain adaptation settings with competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1\">Mirco Planamente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1\">Chiara Plizzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberti_E/0/1/0/all/0/1\">Emanuele Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BUSIS: A Benchmark for Breast Ultrasound Image Segmentation. (arXiv:1801.03182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1801.03182","description":"<p>Breast ultrasound (BUS) image segmentation is challenging and critical for\nBUS Comput-er-Aided Diagnosis (CAD) systems. Many BUS segmentation approaches\nhave been studied in the last two decades, but the performances of most\napproaches have been assessed using relatively small private datasets with\ndifferent quantitative metrics, which results in a discrepancy in performance\ncomparison. Therefore, there is a pressing need for building a benchmark to\ncompare existing methods using a public dataset objectively, to determine the\nperformance of the best breast tumor segmentation algorithm available today,\nand to investigate what segmentation strategies are valuable in clinical\npractice and theoretical study. In this work, a benchmark for B-mode breast\nultrasound image segmentation is presented. In the benchmark, 1) we collected\n562 breast ultrasound images, prepared a software tool, and involved four\nradiologists in obtaining accurate annotations through standardized procedures;\n2) we extensively compared the performance of sixteen state-of-the-art\nsegmentation methods and discussed their advantages and disadvantages; 3) we\nproposed a set of valuable quantitative metrics to evaluate both semi-automatic\nand fully automatic segmentation approaches; and 4) the successful segmentation\nstrategies and possible future improvements are discussed in details.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingtao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">H. D. Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jianrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1\">Chunping Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoScale: Learning to Scale for Crowd Counting and Localization. (arXiv:1912.09632v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.09632","description":"<p>Recent works on crowd counting mainly leverage CNNs to count by regressing\ndensity maps, and have achieved great progress. In the density map, each person\nis represented by a Gaussian blob, and the final count is obtained from the\nintegration of the whole map. However, it is difficult to accurately predict\nthe density map on dense regions. A major issue is that the density map on\ndense regions usually accumulates density values from a number of nearby\nGaussian blobs, yielding different large density values on a small set of\npixels. This makes the density map present variant patterns with significant\npattern shifts and brings a long-tailed distribution of pixel-wise density\nvalues. We propose a simple and effective Learning to Scale (L2S) module, which\nautomatically scales dense regions into reasonable closeness levels (reflecting\nimage-plane distance between neighboring people). L2S directly normalizes the\ncloseness in different patches such that it dynamically separates the\noverlapped blobs, decomposes the accumulated values in the ground-truth density\nmap, and thus alleviates the pattern shifts and long-tailed distribution of\ndensity values. This helps the model to better learn the density map. We also\nexplore the effectiveness of L2S in localizing people by finding the local\nminima of the quantized distance (w.r.t. person location map). To the best of\nour knowledge, such a localization method is also novel in localization-based\ncrowd counting. We further introduce a customized dynamic cross-entropy loss,\nsignificantly improving the localization-based model optimization. Extensive\nexperiments demonstrate that the proposed framework termed AutoScale improves\nupon some state-of-the-art methods in both regression and localization\nbenchmarks on three crowded datasets and achieves very competitive performance\non two sparse datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dingkang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro Hand Gestures and a Real-Time Recognition Framework. (arXiv:2003.00951v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.00951","description":"<p>The use of hand gestures provides a natural alternative to cumbersome\ninterface devices for Human-Computer Interaction (HCI) systems. However,\nreal-time recognition of dynamic micro hand gestures from video streams is\nchallenging for in-vehicle scenarios since (i) the gestures should be performed\nnaturally without distracting the driver, (ii) micro hand gestures occur within\nvery short time intervals at spatially constrained areas, (iii) the performed\ngesture should be recognized only once, and (iv) the entire architecture should\nbe designed lightweight as it will be deployed to an embedded system. In this\nwork, we propose an HCI system for dynamic recognition of driver micro hand\ngestures, which can have a crucial impact in automotive sector especially for\nsafety related issues. For this purpose, we initially collected a dataset named\nDriver Micro Hand Gestures (DriverMHG), which consists of RGB, depth and\ninfrared modalities. The challenges for dynamic recognition of micro hand\ngestures have been addressed by proposing a lightweight convolutional neural\nnetwork (CNN) based architecture which operates online efficiently with a\nsliding window approach. For the CNN model, several 3-dimensional resource\nefficient networks are applied and their performances are analyzed. Online\nrecognition of gestures has been performed with 3D-MobileNetV2, which provided\nthe best offline accuracy among the applied networks with similar computational\ncomplexities. The final architecture is deployed on a driver simulator\noperating in real-time. We make DriverMHG dataset and our source code publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1\">Okan K&#xf6;p&#xfc;kl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledwon_T/0/1/0/all/0/1\">Thomas Ledwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kose_N/0/1/0/all/0/1\">Neslihan Kose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking. (arXiv:2004.01888v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.01888","description":"<p>Multi-object tracking (MOT) is an important problem in computer vision which\nhas a wide range of applications. Formulating MOT as multi-task learning of\nobject detection and re-ID in a single network is appealing since it allows\njoint optimization of the two tasks and enjoys high computation efficiency.\nHowever, we find that the two tasks tend to compete with each other which need\nto be carefully addressed. In particular, previous works usually treat re-ID as\na secondary task whose accuracy is heavily affected by the primary detection\ntask. As a result, the network is biased to the primary detection task which is\nnot fair to the re-ID task. To solve the problem, we present a simple yet\neffective approach termed as FairMOT based on the anchor-free object detection\narchitecture CenterNet. Note that it is not a naive combination of CenterNet\nand re-ID. Instead, we present a bunch of detailed designs which are critical\nto achieve good tracking results by thorough empirical studies. The resulting\napproach achieves high accuracy for both detection and tracking. The approach\noutperforms the state-of-the-art methods by a large margin on several public\ndatasets. The source code and pre-trained models are released at\nhttps://github.com/ifzhang/FairMOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks. (arXiv:2006.06880v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2006.06880","description":"<p>Training neural networks with binary weights and activations is a challenging\nproblem due to the lack of gradients and difficulty of optimization over\ndiscrete weights. Many successful experimental results have been achieved with\nempirical straight-through (ST) approaches, proposing a variety of ad-hoc rules\nfor propagating gradients through non-differentiable activations and updating\ndiscrete weights. At the same time, ST methods can be truly derived as\nestimators in the stochastic binary network (SBN) model with Bernoulli weights.\nWe advance these derivations to a more complete and systematic study. We\nanalyze properties, estimation accuracy, obtain different forms of correct ST\nestimators for activations and weights, explain existing empirical approaches\nand their shortcomings, explain how latent weights arise from the mirror\ndescent method when optimizing over probabilities. This allows to reintroduce\nST methods, long known empirically, as sound approximations, apply them with\nclarity and develop further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Shekhovtsov_A/0/1/0/all/0/1\">Alexander Shekhovtsov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yanush_V/0/1/0/all/0/1\">Viktor Yanush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving. (arXiv:2008.11901v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.11901","description":"<p>We present an end-to-end method for object detection and trajectory\nprediction utilizing multi-view representations of LiDAR returns and camera\nimages. In this work, we recognize the strengths and weaknesses of different\nview representations, and we propose an efficient and generic fusing method\nthat aggregates benefits from all views. Our model builds on a state-of-the-art\nBird's-Eye View (BEV) network that fuses voxelized features from a sequence of\nhistorical LiDAR data as well as rasterized high-definition map to perform\ndetection and prediction tasks. We extend this model with additional LiDAR\nRange-View (RV) features that use the raw LiDAR information in its native,\nnon-quantized representation. The RV feature map is projected into BEV and\nfused with the BEV features computed from LiDAR and high-definition map. The\nfused features are then further processed to output the final detections and\ntrajectories, within a single end-to-end trainable network. In addition, the RV\nfusion of LiDAR and camera is performed in a straightforward and\ncomputationally efficient manner using this framework. The proposed multi-view\nfusion approach improves the state-of-the-art on proprietary large-scale\nreal-world data collected by a fleet of self-driving vehicles, as well as on\nthe public nuScenes data set with minimal increases on the computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fadadu_S/0/1/0/all/0/1\">Sudeep Fadadu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Shreyash Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_D/0/1/0/all/0/1\">Darshan Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_F/0/1/0/all/0/1\">Fang-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djuric_N/0/1/0/all/0/1\">Nemanja Djuric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallespi_Gonzalez_C/0/1/0/all/0/1\">Carlos Vallespi-Gonzalez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-LIBRA: Artificial intelligence method for robust quantification of breast density with independent validation in breast cancer risk assessment. (arXiv:2011.08001v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.08001","description":"<p>Breast density is an important risk factor for breast cancer that also\naffects the specificity and sensitivity of screening mammography. Current\nfederal legislation mandates reporting of breast density for all women\nundergoing breast screening. Clinically, breast density is assessed visually\nusing the American College of Radiology Breast Imaging Reporting And Data\nSystem (BI-RADS) scale. Here, we introduce an artificial intelligence (AI)\nmethod to estimate breast percentage density (PD) from digital mammograms. Our\nmethod leverages deep learning (DL) using two convolutional neural network\narchitectures to accurately segment the breast area. A machine-learning\nalgorithm combining superpixel generation, texture feature analysis, and\nsupport vector machine is then applied to differentiate dense from non-dense\ntissue regions, from which PD is estimated. Our method has been trained and\nvalidated on a multi-ethnic, multi-institutional dataset of 15,661 images\n(4,437 women), and then tested on an independent dataset of 6,368 digital\nmammograms (1,702 women; cases=414) for both PD estimation and discrimination\nof breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and\nan expert reader were strongly correlated (Spearman correlation coefficient =\n0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination\nperformance (area under the ROC curve, AUC = 0.611 [95% confidence interval\n(CI): 0.583, 0.639]) compared to four other widely-used research and commercial\nPD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong\nagreement of PD estimates between Deep-LIBRA and gold-standard assessment by an\nexpert reader, as well as improved performance in breast cancer risk assessment\nover state-of-the-art open-source and commercial methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maghsoudi_O/0/1/0/all/0/1\">Omid Haji Maghsoudi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gastounioti_A/0/1/0/all/0/1\">Aimilia Gastounioti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scott_C/0/1/0/all/0/1\">Christopher Scott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pantalone_L/0/1/0/all/0/1\">Lauren Pantalone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1\">Fang-Fang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_E/0/1/0/all/0/1\">Eric A. Cohen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Winham_S/0/1/0/all/0/1\">Stacey Winham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Conant_E/0/1/0/all/0/1\">Emily F. Conant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vachon_C/0/1/0/all/0/1\">Celine Vachon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kontos_D/0/1/0/all/0/1\">Despina Kontos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Cats and Dogs: Semi-supervised Classification of fuzzy labels with overclustering. (arXiv:2012.01768v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01768","description":"<p>A long-standing issue with deep learning is the need for large and\nconsistently labeled datasets. Although the current research in semi-supervised\nlearning can decrease the required amount of annotated data by a factor of 10\nor even more, this line of research still uses distinct classes like cats and\ndogs. However, in the real-world we often encounter problems where different\nexperts have different opinions, thus producing fuzzy labels. We propose a\nnovel framework for handling semi-supervised classifications of such fuzzy\nlabels. Our framework is based on the idea of overclustering to detect\nsubstructures in these fuzzy labels. We propose a novel loss to improve the\noverclustering capability of our framework and show on the common image\nclassification dataset STL-10 that it is faster and has better overclustering\nperformance than previous work. On a real-world plankton dataset, we illustrate\nthe benefit of overclustering for fuzzy labels and show that we beat previous\nstate-of-the-art semisupervised methods. Moreover, we acquire 5 to 10% more\nconsistent predictions of substructures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunger_J/0/1/0/all/0/1\">Johannes Br&#xfc;nger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1\">Rainer Kiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Models as Distributions of Functions. (arXiv:2102.04776v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.04776","description":"<p>Generative models are typically trained on grid-like data such as images. As\na result, the size of these models usually scales directly with the underlying\ngrid resolution. In this paper, we abandon discretized grids and instead\nparameterize individual data points by continuous functions. We then build\ngenerative models by learning distributions over such functions. By treating\ndata points as functions, we can abstract away from the specific type of data\nwe train on and construct models that are agnostic to discretization. To train\nour model, we use an adversarial approach with a discriminator that acts on\ncontinuous signals. Through experiments on a wide variety of data modalities\nincluding images, 3D shapes and climate data, we demonstrate that our model can\nlearn rich distributions of functions independently of data type and\nresolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_E/0/1/0/all/0/1\">Emilien Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Fully Convolutional Neural Networks with Intersection Over Union Loss for Crop Mapping from Multi-Temporal Satellite Images. (arXiv:2102.07280v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.07280","description":"<p>Information on cultivated crops is relevant for a large number of food\nsecurity studies. Different scientific efforts are dedicated to generating this\ninformation from remote sensing images by means of machine learning methods.\nUnfortunately, these methods do not take account of the spatial-temporal\nrelationships inherent in remote sensing images. In our paper, we explore the\ncapability of a 3D Fully Convolutional Neural Network (FCN) to map crop types\nfrom multi-temporal images. In addition, we propose the Intersection Over Union\n(IOU) loss function for increasing the overlap between the predicted classes\nand ground reference data. The proposed method was applied to identify soybean\nand corn from a study area situated in the US corn belt using multi-temporal\nLandsat images. The study shows that our method outperforms related methods,\nobtaining a Kappa coefficient of 91.8%. We conclude that using the IOU loss\nfunction provides a superior choice to learn individual crop types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1\">Sina Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belgiu_M/0/1/0/all/0/1\">Mariana Belgiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1\">Alfred Stein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning. (arXiv:2103.00370v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.00370","description":"<p>Visual search, recommendation, and contrastive similarity learning power a\nwide breadth of technologies that impact billions of users across the world.\nThe best-performing approaches are often complex and difficult to interpret,\nand there are several competing techniques one can use to explain a search\nengine's behavior. We show that the theory of fair credit assignment provides a\nunique axiomatic solution that generalizes several existing recommendation- and\nmetric-explainability techniques in the literature. Using this formalism, we\nare able to determine in what regimes existing approaches fall short of\nfairness and provide variations that are fair in more situations and handle\ncounterfactual information. More specifically, we show existing approaches\nimplicitly approximate second-order Shapley-Taylor indices and use this\nperspective to extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to\nsearch engines. These extensions can extract pairwise correspondences between\nimages from trained black-box models. We also introduce a fast kernel-based\nmethod for estimating Shapley-Taylor indices that require orders of magnitude\nfewer function evaluations to converge. Finally, we evaluate these methods and\nshow that these game-theoretic measures yield more consistent explanations for\nimage similarity architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_M/0/1/0/all/0/1\">Mark Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Stephanie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Application of Image-to-Image Translation: Chromosome Straightening Framework by Learning from a Single Image. (arXiv:2103.02835v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02835","description":"<p>In medical imaging, chromosome straightening plays a significant role in the\npathological study of chromosomes and in the development of cytogenetic maps.\nWhereas different approaches exist for the straightening task, typically\ngeometric algorithms are used whose outputs are characterized by jagged edges\nor fragments with discontinued banding patterns. To address the flaws in the\ngeometric algorithms, we propose a novel framework based on image-to-image\ntranslation to learn a pertinent mapping dependence for synthesizing\nstraightened chromosomes with uninterrupted banding patterns and preserved\ndetails. In addition, to avoid the pitfall of deficient input chromosomes, we\nconstruct an augmented dataset using only one single curved chromosome image\nfor training models. Based on this framework, we apply two popular\nimage-to-image translation architectures, U-shape networks and conditional\ngenerative adversarial networks, to assess its efficacy. Experiments on a\ndataset comprised of 642 real-world chromosomes demonstrate the superiority of\nour framework, as compared to the geometric method in straightening\nperformance, by rendering realistic and continued chromosome details.\nFurthermore, our straightened results improve the chromosome classification by\n0.98%-1.39% mean accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Daiyun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yalun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chunxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1\">Jia Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_F/0/1/0/all/0/1\">Frans Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Attention-in-Attention Model for Joint Rain Streak and Raindrop Removal. (arXiv:2103.07051v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.07051","description":"<p>Rain streaks and rain drops are two natural phenomena, which degrade image\ncapture in different ways. Currently, most existing deep deraining networks\ntake them as two distinct problems and individually address one, and thus\ncannot deal adequately with both simultaneously. To address this, we propose a\nDual Attention-in-Attention Model (DAiAM) which includes two DAMs for removing\nboth rain streaks and raindrops. Inside the DAM, there are two attentive maps -\neach of which attends to the heavy and light rainy regions, respectively, to\nguide the deraining process differently for applicable regions. In addition, to\nfurther refine the result, a Differential-driven Dual Attention-in-Attention\nModel (D-DAiAM) is proposed with a \"heavy-to-light\" scheme to remove rain via\naddressing the unsatisfying deraining regions. Extensive experiments on one\npublic raindrop dataset, one public rain streak and our synthesized joint rain\nstreak and raindrop (JRSRD) dataset have demonstrated that the proposed method\nnot only is capable of removing rain streaks and raindrops simultaneously, but\nalso achieves the state-of-the-art performance on both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Maximization Clustering via Multi-View Self-Labelling. (arXiv:2103.07368v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.07368","description":"<p>Image clustering is a particularly challenging computer vision task, which\naims to generate annotations without human supervision. Recent advances focus\non the use of self-supervised learning strategies in image clustering, by first\nlearning valuable semantics and then clustering the image representations.\nThese multiple-phase algorithms, however, increase the computational time and\ntheir final performance is reliant on the first stage. By extending the\nself-supervised approach, we propose a novel single-phase clustering method\nthat simultaneously learns meaningful representations and assigns the\ncorresponding annotations. This is achieved by integrating a discrete\nrepresentation into the self-supervised paradigm through a classifier net.\nSpecifically, the proposed clustering objective employs mutual information, and\nmaximizes the dependency between the integrated discrete representation and a\ndiscrete probability distribution. The discrete probability distribution is\nderived though the self-supervised process by comparing the learnt latent\nrepresentation with a set of trainable prototypes. To enhance the learning\nperformance of the classifier, we jointly apply the mutual information across\nmulti-crop views. Our empirical results show that the proposed framework\noutperforms state-of-the-art techniques with the average accuracy of 89.1% and\n49.0%, respectively, on CIFAR-10 and CIFAR-100/20 datasets. Finally, the\nproposed method also demonstrates attractive robustness to parameter settings,\nmaking it ready to be applicable to other datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ntelemis_F/0/1/0/all/0/1\">Foivos Ntelemis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Spencer A. Thomas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FakeMix Augmentation Improves Transparent Object Detection. (arXiv:2103.13279v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13279","description":"<p>Detecting transparent objects in natural scenes is challenging due to the low\ncontrast in texture, brightness and colors. Recent deep-learning-based works\nreveal that it is effective to leverage boundaries for transparent object\ndetection (TOD). However, these methods usually encounter boundary-related\nimbalance problem, leading to limited generation capability. Detailly, a kind\nof boundaries in the background, which share the same characteristics with\nboundaries of transparent objects but have much smaller amounts, usually hurt\nthe performance. To conquer the boundary-related imbalance problem, we propose\na novel content-dependent data augmentation method termed FakeMix. Considering\ncollecting these trouble-maker boundaries in the background is hard without\ncorresponding annotations, we elaborately generate them by appending the\nboundaries of transparent objects from other samples into the current image\nduring training, which adjusts the data space and improves the generalization\nof the models. Further, we present AdaptiveASPP, an enhanced version of ASPP,\nthat can capture multi-scale and cross-modality features dynamically. Extensive\nexperiments demonstrate that our methods clearly outperform the\nstate-of-the-art methods. We also show that our approach can also transfer well\non related tasks, in which the model meets similar troubles, such as mirror\ndetection, glass detection, and camouflaged object detection. Code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiangui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_J/0/1/0/all/0/1\">Jian Tuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13389","description":"<p>Given a large dataset for training, GANs can achieve remarkable performance\nfor the image synthesis task. However, training GANs in extremely low data\nregimes remains a challenge, as overfitting often occurs, leading to\nmemorization or training divergence. In this work, we introduce SIV-GAN, an\nunconditional generative model that can generate new scene compositions from a\nsingle training image or a single video clip. We propose a two-branch\ndiscriminator architecture, with content and layout branches designed to judge\ninternal content and scene layout realism separately from each other. This\ndiscriminator design enables synthesis of visually plausible, novel\ncompositions of a scene, with varying content and layout, while preserving the\ncontext of the original sample. Compared to previous single-image GANs, our\nmodel generates more diverse, higher quality images, while not being restricted\nto a single image setting. We show that SIV-GAN successfully deals with a new\nchallenging task of learning from a single video, for which prior GAN models\nfail to achieve synthesis of both high quality and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sushko_V/0/1/0/all/0/1\">Vadim Sushko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1\">Anna Khoreva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Broaden Your Views for Self-Supervised Video Learning. (arXiv:2103.16559v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16559","description":"<p>Most successful self-supervised learning methods are trained to align the\nrepresentations of two independent views from the data. State-of-the-art\nmethods in video are inspired by image techniques, where these two views are\nsimilarly extracted by cropping and augmenting the resulting crop. However,\nthese methods miss a crucial element in the video domain: time. We introduce\nBraVe, a self-supervised learning framework for video. In BraVe, one of the\nviews has access to a narrow temporal window of the video while the other view\nhas a broad access to the video content. Our models learn to generalise from\nthe narrow view to the general content of the video. Furthermore, BraVe\nprocesses the views with different backbones, enabling the use of alternative\naugmentations or modalities into the broad view such as optical flow, randomly\nconvolved RGB frames, audio or their combinations. We demonstrate that BraVe\nachieves state-of-the-art results in self-supervised representation learning on\nstandard video and audio classification benchmarks including UCF101, HMDB51,\nKinetics, ESC-50 and AudioSet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Recasens_A/0/1/0/all/0/1\">Adri&#xe0; Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luc_P/0/1/0/all/0/1\">Pauline Luc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Luyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemsley_R/0/1/0/all/0/1\">Ross Hemsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1\">Florian Strub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tallec_C/0/1/0/all/0/1\">Corentin Tallec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altche_F/0/1/0/all/0/1\">Florent Altch&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1\">Michal Valko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grill_J/0/1/0/all/0/1\">Jean-Bastien Grill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1\">A&#xe4;ron van den Oord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-Level or Object-Level? A Tale of Two Resampling Strategies for Long-Tailed Detection. (arXiv:2104.05702v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05702","description":"<p>Training on datasets with long-tailed distributions has been challenging for\nmajor recognition tasks such as classification and detection. To deal with this\nchallenge, image resampling is typically introduced as a simple but effective\napproach. However, we observe that long-tailed detection differs from\nclassification since multiple classes may be present in one image. As a result,\nimage resampling alone is not enough to yield a sufficiently balanced\ndistribution at the object level. We address object-level resampling by\nintroducing an object-centric memory replay strategy based on dynamic, episodic\nmemory banks. Our proposed strategy has two benefits: 1) convenient\nobject-level resampling without significant extra computation, and 2) implicit\nfeature-level augmentation from model updates. We show that image-level and\nobject-level resamplings are both important, and thus unify them with a joint\nresampling strategy (RIO). Our method outperforms state-of-the-art long-tailed\ndetection and segmentation methods on LVIS v0.5 across various backbones. Code\nis available at https://github.com/NVlabs/RIO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_N/0/1/0/all/0/1\">Nadine Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Permutation Equivariant Structure from Motion. (arXiv:2104.06703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06703","description":"<p>Existing deep methods produce highly accurate 3D reconstructions in stereo\nand multiview stereo settings, i.e., when cameras are both internally and\nexternally calibrated. Nevertheless, the challenge of simultaneous recovery of\ncamera poses and 3D scene structure in multiview settings with deep networks is\nstill outstanding. Inspired by projective factorization for Structure from\nMotion (SFM) and by deep matrix completion techniques, we propose a neural\nnetwork architecture that, given a set of point tracks in multiple images of a\nstatic scene, recovers both the camera parameters and a (sparse) scene\nstructure by minimizing an unsupervised reprojection loss. Our network\narchitecture is designed to respect the structure of the problem: the sought\noutput is equivariant to permutations of both cameras and scene points.\nNotably, our method does not require initialization of camera parameters or 3D\npoint locations. We test our architecture in two setups: (1) single scene\nreconstruction and (2) learning from multiple scenes. Our experiments,\nconducted on a variety of datasets in both internally calibrated and\nuncalibrated settings, indicate that our method accurately recovers pose and\nstructure, on par with classical state of the art methods. Additionally, we\nshow that a pre-trained network can be used to reconstruct novel scenes using\ninexpensive fine-tuning with no loss of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moran_D/0/1/0/all/0/1\">Dror Moran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koslowsky_H/0/1/0/all/0/1\">Hodaya Koslowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1\">Yoni Kasten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galun_M/0/1/0/all/0/1\">Meirav Galun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1\">Ronen Basri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Driven 3D Reconstruction of Dressed Humans From Sparse Views. (arXiv:2104.08013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08013","description":"<p>Recently, data-driven single-view reconstruction methods have shown great\nprogress in modeling 3D dressed humans. However, such methods suffer heavily\nfrom depth ambiguities and occlusions inherent to single view inputs. In this\npaper, we tackle this problem by considering a small set of input views and\ninvestigate the best strategy to suitably exploit information from these views.\nWe propose a data-driven end-to-end approach that reconstructs an implicit 3D\nrepresentation of dressed humans from sparse camera views. Specifically, we\nintroduce three key components: first a spatially consistent reconstruction\nthat allows for arbitrary placement of the person in the input views using a\nperspective camera model; second an attention-based fusion layer that learns to\naggregate visual information from several viewpoints; and third a mechanism\nthat encodes local 3D patterns under the multi-view context. In the\nexperiments, we show the proposed approach outperforms the state of the art on\nstandard data both quantitatively and qualitatively. To demonstrate the\nspatially consistent reconstruction, we apply our approach to dynamic scenes.\nAdditionally, we apply our method on real data acquired with a multi-camera\nplatform and demonstrate our approach can obtain results comparable to\nmulti-view stereo with dramatically less views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zins_P/0/1/0/all/0/1\">Pierre Zins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuhrer_S/0/1/0/all/0/1\">Stefanie Wuhrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeaDronesSee: A Maritime Benchmark for Detecting Humans in Open Water. (arXiv:2105.01922v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01922","description":"<p>Unmanned Aerial Vehicles (UAVs) are of crucial importance in search and\nrescue missions in maritime environments due to their flexible and fast\noperation capabilities. Modern computer vision algorithms are of great interest\nin aiding such missions. However, they are dependent on large amounts of\nreal-case training data from UAVs, which is only available for traffic\nscenarios on land. Moreover, current object detection and tracking data sets\nonly provide limited environmental information or none at all, neglecting a\nvaluable source of information. Therefore, this paper introduces a large-scaled\nvisual object detection and tracking benchmark (SeaDronesSee) aiming to bridge\nthe gap from land-based vision systems to sea-based ones. We collect and\nannotate over 54,000 frames with 400,000 instances captured from various\naltitudes and viewing angles ranging from 5 to 260 meters and 0 to 90 degrees\nwhile providing the respective meta information for altitude, viewing angle and\nother meta data. We evaluate multiple state-of-the-art computer vision\nalgorithms on this newly established benchmark serving as baselines. We provide\nan evaluation server where researchers can upload their prediction and compare\ntheir results on a central leaderboard\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1\">Leon Amadeus Varga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiefer_B/0/1/0/all/0/1\">Benjamin Kiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messmer_M/0/1/0/all/0/1\">Martin Messmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Human and Automated Identification of Wildlife Images. (arXiv:2105.02320v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02320","description":"<p>Camera trapping is increasingly used to monitor wildlife, but this technology\ntypically requires extensive data annotation. Recently, deep learning has\nsignificantly advanced automatic wildlife recognition. However, current methods\nare hampered by a dependence on large static data sets when wildlife data is\nintrinsically dynamic and involves long-tailed distributions. These two\ndrawbacks can be overcome through a hybrid combination of machine learning and\nhumans in the loop. Our proposed iterative human and automated identification\napproach is capable of learning from wildlife imagery data with a long-tailed\ndistribution. Additionally, it includes self-updating learning that facilitates\ncapturing the community dynamics of rapidly changing natural systems. Extensive\nexperiments show that our approach can achieve a ~90% accuracy employing only\n~20% of the human annotations of existing approaches. Our synergistic\ncollaboration of humans and machines transforms deep learning from a relatively\ninefficient post-annotation tool to a collaborative on-going annotation tool\nthat vastly relieves the burden of human annotation and enables efficient and\nconstant model updates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhongqi Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaynor_K/0/1/0/all/0/1\">Kaitlyn M. Gaynor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Meredith S. Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getz_W/0/1/0/all/0/1\">Wayne M. Getz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain Inspired Face Recognition: A Computational Framework. (arXiv:2105.07237v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07237","description":"<p>This paper presents a new proposal of an efficient computational model of\nface recognition which uses cues from the distributed face recognition\nmechanism of the brain, and by gathering engineering equivalent of these cues\nfrom existing literature. Three distinct and widely used features: Histogram of\nOriented Gradients (HOG), Local Binary Patterns (LBP), and Principal components\n(PCs) extracted from target images are used in a manner which is simple, and\nyet effective. The HOG and LBP features further undergo principal component\nanalysis for dimensionality reduction. Our model uses multi-layer perceptrons\n(MLP) to classify these three features and fuse them at the decision level\nusing sum rule. A computational theory is first developed by using concepts\nfrom the information processing mechanism of the brain. Extensive experiments\nare carried out using ten publicly available datasets to validate our proposed\nmodel's performance in recognizing faces with extreme variation of\nillumination, pose angle, expression, and background. Results obtained are\nextremely promising when compared with other face recognition algorithms\nincluding CNN and deep learning-based methods. This highlights that simple\ncomputational processes, if clubbed properly, can produce competing performance\nwith best algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_A/0/1/0/all/0/1\">Angad Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_N/0/1/0/all/0/1\">Nikhil Tyagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A parameter refinement method for Ptychography based on Deep Learning concepts. (arXiv:2105.08058v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.08058","description":"<p>X-ray Ptychography is an advanced computational microscopy technique which is\ndelivering exceptionally detailed quantitative imaging of biological and\nnanotechnology specimens. However coarse parametrisation in propagation\ndistance, position errors and partial coherence frequently menaces the\nexperiment viability. In this work we formally introduced these actors, solving\nthe whole reconstruction as an optimisation problem. A modern Deep Learning\nframework is used to correct autonomously the setup incoherences, thus\nimproving the quality of a ptychography reconstruction. Automatic procedures\nare indeed crucial to reduce the time for a reliable analysis, which has a\nsignificant impact on all the fields that use this kind of microscopy. We\nimplemented our algorithm in our software framework, SciComPty, releasing it as\nopen-source. We tested our system on both synthetic datasets and also on real\ndata acquired at the TwinMic beamline of the Elettra synchrotron facility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guzzi_F/0/1/0/all/0/1\">Francesco Guzzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kourousias_G/0/1/0/all/0/1\">George Kourousias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bille_F/0/1/0/all/0/1\">Fulvio Bill&#xe8;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pugliese_R/0/1/0/all/0/1\">Roberto Pugliese</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gianoncelli_A/0/1/0/all/0/1\">Alessandra Gianoncelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carrato_S/0/1/0/all/0/1\">Sergio Carrato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding. (arXiv:2105.13077v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13077","description":"<p>Single-image super-resolution (SR) and multi-frame SR are two ways to super\nresolve low-resolution images. Single-Image SR generally handles each image\nindependently, but ignores the temporal information implied in continuing\nframes. Multi-frame SR is able to model the temporal dependency via capturing\nmotion information. However, it relies on neighbouring frames which are not\nalways available in the real world. Meanwhile, slight camera shake easily\ncauses heavy motion blur on long-distance-shot low-resolution images. To\naddress these problems, a Blind Motion Deblurring Super-Reslution Networks,\nBMDSRNet, is proposed to learn dynamic spatio-temporal information from single\nstatic motion-blurred images. Motion-blurred images are the accumulation over\ntime during the exposure of cameras, while the proposed BMDSRNet learns the\nreverse process and uses three-streams to learn Bidirectional spatio-temporal\ninformation based on well designed reconstruction loss functions to recover\nclean high-resolution images. Extensive experiments demonstrate that the\nproposed BMDSRNet outperforms recent state-of-the-art methods, and has the\nability to simultaneously deal with image deblurring and SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wenjia Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroWaste Dataset: Towards Deformable Object Segmentation in Extreme Clutter. (arXiv:2106.02740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02740","description":"<p>Less than 35% of recyclable waste is being actually recycled in the US, which\nleads to increased soil and sea pollution and is one of the major concerns of\nenvironmental researchers as well as the common public. At the heart of the\nproblem are the inefficiencies of the waste sorting process (separating paper,\nplastic, metal, glass, etc.) due to the extremely complex and cluttered nature\nof the waste stream. Automated waste detection has great potential to enable\nmore efficient, reliable, and safe waste sorting practices, but it requires\nlabel-efficient detection of deformable objects in extremely cluttered scenes.\nThis challenging computer vision task currently lacks suitable datasets or\nmethods in the available literature. In this paper, we take a step towards\ncomputer-aided waste detection and present the first in-the-wild\nindustrial-grade waste detection and segmentation dataset, ZeroWaste. This\ndataset contains over 1800 fully segmented video frames collected from a real\nwaste sorting plant along with waste material labels for training and\nevaluation of the segmentation methods, as well as over 6000 unlabeled frames\nthat can be further used for semi-supervised and self-supervised learning\ntechniques, as well as frames of the conveyor belt before and after the sorting\nprocess, comprising a novel setup that can be used for weakly-supervised\nsegmentation. Our experimental results demonstrate that state-of-the-art\nsegmentation methods struggle to correctly detect and classify target objects\nwhich suggests the challenging nature of our proposed real-world task of\nfine-grained object detection in cluttered scenes. We believe that ZeroWaste\nwill catalyze research in object detection and semantic segmentation in extreme\nclutter as well as applications in the recycling domain.\n</p>\n<p>Our project page can be found at <a href=\"http://ai.bu.edu/zerowaste/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed Abdelfattah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziliang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akl_J/0/1/0/all/0/1\">James Akl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alladkani_F/0/1/0/all/0/1\">Fadi Alladkani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Ping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calli_B/0/1/0/all/0/1\">Berk Calli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1\">Sarah Adel Bargal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06959","description":"<p>The discovery of the disentanglement properties of the latent space in GANs\nmotivated a lot of research to find the semantically meaningful directions on\nit. In this paper, we suggest that the disentanglement property is closely\nrelated to the geometry of the latent space. In this regard, we propose an\nunsupervised method for finding the semantic-factorizing directions on the\nintermediate latent space of GANs based on the local geometry. Intuitively, our\nproposed method, called Local Basis, finds the principal variation of the\nlatent space in the neighborhood of the base latent variable. Experimental\nresults show that the local principal variation corresponds to the semantic\nfactorization and traversing along it provides strong robustness to image\ntraversal. Moreover, we suggest an explanation for the limited success in\nfinding the global traversal directions in the latent space, especially W-space\nof StyleGAN2. We show that W-space is warped globally by comparing the local\ngeometry, discovered from Local Basis, through the metric on Grassmannian\nManifold. The global warpage implies that the latent space is not well-aligned\nglobally and therefore the global traversal directions are bound to show\nlimited success on it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaewoong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1\">Changyeon Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jung Ho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Geonho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Super Resolution be used to improve Human Pose Estimation in Low Resolution Scenarios?. (arXiv:2107.02108v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02108","description":"<p>The results obtained from state of the art human pose estimation (HPE) models\ndegrade rapidly when evaluating people of a low resolution, but can super\nresolution (SR) be used to help mitigate this effect? By using various SR\napproaches we enhanced two low resolution datasets and evaluated the change in\nperformance of both an object and keypoint detector as well as end-to-end HPE\nresults. We remark the following observations. First we find that for people\nwho were originally depicted at a low resolution (segmentation area in pixels),\ntheir keypoint detection performance would improve once SR was applied. Second,\nthe keypoint detection performance gained is dependent on that persons pixel\ncount in the original image prior to any application of SR; keypoint detection\nperformance was improved when SR was applied to people with a small initial\nsegmentation area, but degrades as this becomes larger. To address this we\nintroduced a novel Mask-RCNN approach, utilising a segmentation area threshold\nto decide when to use SR during the keypoint detection step. This approach\nachieved the best results on our low resolution datasets for each HPE\nperformance metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardy_P/0/1/0/all/0/1\">Peter Hardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasmahapatra_S/0/1/0/all/0/1\">Srinandan Dasmahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hansung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer. (arXiv:2107.02681v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02681","description":"<p>Since visual perception can give rich information beyond text descriptions\nfor world understanding, there has been increasing interest in leveraging\nvisual grounding for language learning. Recently, vokenization (Tan and Bansal,\n2020) has attracted attention by using the predictions of a text-to-image\nretrieval model as labels for language model supervision. Despite its success,\nthe method suffers from approximation error of using finite image labels and\nthe lack of vocabulary diversity of a small image-text dataset. To overcome\nthese limitations, we present VidLanKD, a video-language knowledge distillation\nmethod for improving language understanding. We train a multi-modal teacher\nmodel on a video-text dataset, and then transfer its knowledge to a student\nlanguage model with a text dataset. To avoid approximation error, we propose to\nuse different knowledge distillation objectives. In addition, the use of a\nlarge-scale video-text dataset helps learn diverse and richer vocabularies. In\nour experiments, VidLanKD achieves consistent improvements over text-only\nlanguage models and vokenization models, on several downstream language\nunderstanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the\nimproved world knowledge, physical reasoning, and temporal reasoning\ncapabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and\nTRACIE datasets. Lastly, we present comprehensive ablation studies as well as\nvisualizations of the learned text-to-video grounding results of our teacher\nand student language models. Our code and models are available at:\nhttps://github.com/zinengtang/VidLanKD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments. (arXiv:2107.04174v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2107.04174","description":"<p>Augmented Reality (AR) as a platform has the potential to facilitate the\nreduction of the cocktail party effect. Future AR headsets could potentially\nleverage information from an array of sensors spanning many different\nmodalities. Training and testing signal processing and machine learning\nalgorithms on tasks such as beam-forming and speech enhancement require high\nquality representative data. To the best of the author's knowledge, as of\npublication there are no available datasets that contain synchronized\negocentric multi-channel audio and video with dynamic movement and\nconversations in a noisy environment. In this work, we describe, evaluate and\nrelease a dataset that contains over 5 hours of multi-modal data useful for\ntraining and testing algorithms for the application of improving conversations\nfor an AR glasses wearer. We provide speech intelligibility, quality and\nsignal-to-noise ratio improvement results for a baseline method and show\nimprovements across all tested metrics. The dataset we are releasing contains\nAR glasses egocentric multi-channel microphone array audio, wide field-of-view\nRGB video, speech source pose, headset microphone audio, annotated voice\nactivity, speech transcriptions, head bounding boxes, target of speech and\nsource identification labels. We have created and are releasing this dataset to\nfacilitate research in multi-modal AR solutions to the cocktail party problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1\">Jacob Donley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tourbabin_V/0/1/0/all/0/1\">Vladimir Tourbabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broyles_M/0/1/0/all/0/1\">Mark Broyles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1\">Vamsi Krishna Ithapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehra_R/0/1/0/all/0/1\">Ravish Mehra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution. (arXiv:2107.05612v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2107.05612","description":"<p>Natural language provides an accessible and expressive interface to specify\nlong-term tasks for robotic agents. However, non-experts are likely to specify\nsuch tasks with high-level instructions, which abstract over specific robot\nactions through several layers of abstraction. We propose that key to bridging\nthis gap between language and robot actions over long execution horizons are\npersistent representations. We propose a persistent spatial semantic\nrepresentation method, and show how it enables building an agent that performs\nhierarchical reasoning to effectively execute long-term tasks. We evaluate our\napproach on the ALFRED benchmark and achieve state-of-the-art results, despite\ncompletely avoiding the commonly used step-by-step instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blukis_V/0/1/0/all/0/1\">Valts Blukis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness Properties of Face Recognition and Obfuscation Systems. (arXiv:2108.02707v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02707","description":"<p>The proliferation of automated face recognition in various commercial and\ngovernment sectors has caused significant privacy concerns for individuals. A\nrecent, popular approach to address these privacy concerns is to employ evasion\nattacks against the metric embedding networks powering face recognition\nsystems. Face obfuscation systems generate imperceptible perturbations, when\nadded to an image, cause the face recognition system to misidentify the user.\nThe key to these approaches is the generation of perturbations using a\npre-trained metric embedding network followed by their application to an online\nsystem, whose model might be proprietary. This dependence of face obfuscation\non metric embedding networks, which are known to be unfair in the context of\nface recognition, surfaces the question of demographic fairness -- \\textit{are\nthere demographic disparities in the performance of face obfuscation systems?}\nTo address this question, we perform an analytical and empirical exploration of\nthe performance of recent face obfuscation systems that rely on deep embedding\nnetworks. We find that metric embedding networks are demographically aware;\nthey cluster faces in the embedding space based on their demographic\nattributes. We observe that this effect carries through to face obfuscation\nsystems: faces belonging to minority groups incur reduced utility compared to\nthose from majority groups. For example, the disparity in average obfuscation\nsuccess rate on the online Face++ API can reach up to 20 percentage points. We\npresent an intuitive analytical model to provide insights into these phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1\">Harrison Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Brian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1\">Kassem Fawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Cloud. (arXiv:2108.06230v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06230","description":"<p>While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D\nimages, its application to 3D data is still recent and scarce, with just a few\nmethods limited to classification. We present the first generative approach for\nboth ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both\nclassification and, for the first time, semantic segmentation. We show that it\nreaches or outperforms the state of the art on ModelNet40 classification for\nboth inductive ZSL and inductive GZSL. For semantic segmentation, we created\nthree benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and\nSemanticKITTI. Our experiments show that our method outperforms strong\nbaselines, which we additionally propose for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michele_B/0/1/0/all/0/1\">Bj&#xf6;rn Michele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1\">Gilles Puy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_M/0/1/0/all/0/1\">Maxime Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges. (arXiv:2108.06959v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06959","description":"<p>We introduce a novel dataset for architectural style classification,\nconsisting of 9,485 images of church buildings. Both images and style labels\nwere sourced from Wikipedia. The dataset can serve as a benchmark for various\nresearch fields, as it combines numerous real-world challenges: fine-grained\ndistinctions between classes based on subtle visual features, a comparatively\nsmall sample size, a highly imbalanced class distribution, a high variance of\nviewpoints, and a hierarchical organization of labels, where only some images\nare labeled at the most precise level. In addition, we provide 631 bounding box\nannotations of characteristic visual features for 139 churches from four major\ncategories. These annotations can, for example, be useful for research on\nfine-grained classification, where additional expert knowledge about\ndistinctive object parts is often available. Images and annotations are\navailable at: https://doi.org/10.5281/zenodo.5166987\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sk-Unet Model with Fourier Domain for Mitosis Detection. (arXiv:2109.00957v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.00957","description":"<p>Mitotic count is the most important morphological feature of breast cancer\ngrading. Many deep learning-based methods have been proposed but suffer from\ndomain shift. In this work, we construct a Fourier-based segmentation model for\nmitosis detection to address the problem. Swapping the low-frequency spectrum\nof source and target images is shown effective to alleviate the discrepancy\nbetween different scanners. Our Fourier-based segmentation method can achieve\nF1 with 0.7456 on the preliminary test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiyue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Points for Modeling Humans in Clothing. (arXiv:2109.01137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01137","description":"<p>Currently it requires an artist to create 3D human avatars with realistic\nclothing that can move naturally. Despite progress on 3D scanning and modeling\nof human bodies, there is still no technology that can easily turn a static\nscan into an animatable avatar. Automating the creation of such avatars would\nenable many applications in games, social networking, animation, and AR/VR to\nname a few. The key problem is one of representation. Standard 3D meshes are\nwidely used in modeling the minimally-clothed body but do not readily capture\nthe complex topology of clothing. Recent interest has shifted to implicit\nsurface models for this task but they are computationally heavy and lack\ncompatibility with existing 3D tools. What is needed is a 3D representation\nthat can capture varied topology at high resolution and that can be learned\nfrom data. We argue that this representation has been with us all along -- the\npoint cloud. Point clouds have properties of both implicit and explicit\nrepresentations that we exploit to model 3D garment geometry on a human body.\nWe train a neural network with a novel local clothing geometric feature to\nrepresent the shape of different outfits. The network is trained from 3D point\nclouds of many types of clothing, on many bodies, in many poses, and learns to\nmodel pose-dependent clothing deformations. The geometry feature can be\noptimized to fit a previously unseen scan of a person in clothing, enabling the\nscan to be reposed realistically. Our model demonstrates superior quantitative\nand qualitative results in both multi-outfit modeling and unseen outfit\nanimation. The code is available for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinlong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01745","description":"<p>The COVID-19 pandemic raises the problem of adapting face recognition systems\nto the new reality, where people may wear surgical masks to cover their noses\nand mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for\ntraining these systems were released before the pandemic, so they now seem\nunsuited due to the lack of examples of people wearing masks. We propose a\nmethod for enhancing data sets containing faces without masks by creating\nsynthetic masks and overlaying them on faces in the original images. Our method\nrelies on SparkAR Studio, a developer program made by Facebook that is used to\ncreate Instagram face filters. In our approach, we use 9 masks of different\ncolors, shapes and fabrics. We employ our method to generate a number of\n445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254\n(96.8%) masks for the CelebA data set, releasing the mask images at\nhttps://github.com/securifai/masked_faces. We show that our method produces\nsignificantly more realistic training examples of masks overlaid on faces by\nasking volunteers to qualitatively compare it to other methods or data sets\ndesigned for the same task. We also demonstrate the usefulness of our method by\nevaluating state-of-the-art face recognition systems (FaceNet, VGG-face,\nArcFace) trained on our enhanced data sets and showing that they outperform\nequivalent systems trained on original data sets (containing faces without\nmasks) or competing data sets (containing masks generated by related methods),\nwhen the test benchmarks contain masked faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mare_T/0/1/0/all/0/1\">Tudor Mare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duta_G/0/1/0/all/0/1\">Georgian Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1\">Bogdan Alexe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04993","description":"<p>Pre-training visual and textual representations from large-scale image-text\npairs is becoming a standard approach for many downstream vision-language\ntasks. The transformer-based models learn inter and intra-modal attention\nthrough a list of self-supervised learning tasks. This paper proposes LAViTeR,\na novel architecture for visual and textual representation learning. The main\nmodule, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\nGAN-based image synthesis and Image Captioning. We also propose a new\nevaluation metric measuring the similarity between the learnt visual and\ntextual embedding. The experimental results on two public datasets, CUB and\nMS-COCO, demonstrate superior visual and textual representation alignment in\nthe joint feature embedding space\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Context-Aware Network for Abdominal Multi-organ Segmentation. (arXiv:2109.10601v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.10601","description":"<p>The contextual information, presented in abdominal CT scan, is relative\nconsistent. In order to make full use of the overall 3D context, we develop a\nwhole-volume-based coarse-to-fine framework for efficient and effective\nabdominal multi-organ segmentation. We propose a new efficientSegNet network,\nwhich is composed of encoder,decoder and context block. For the decoder\nmodule,anisotropic convolution with a k*k*1 intra-slice convolution and a 1*1*k\ninter-slice convolution, is designed to reduce the computation burden. For the\ncontext block, we propose strip pooling module to capture anisotropic and\nlong-range contextual information, which exists in abdominal scene.\nQuantitative evaluation on the FLARE2021 validation cases, this method achieves\nthe average dice similarity coefficient (DSC) of 0.895 and average normalized\nsurface distance (NSD) of 0.775. This method won the 1st place on the\n2021-MICCAI-FLARE challenge. Codes and models are available at\nhttps://github.com/Shanghai-Aitrox-Technology/EfficientSegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hua Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05319","description":"<p>Seismic data fault detection has recently been regarded as a 3D image\nsegmentation task. The nature of fault structures in seismic image makes it\ndifficult to manually label faults. Manual labeling often has many false\nnegative labels (abnormal annotations), which will seriously jeopardize the\ntraining process. In this work, we find that region-based loss significantly\noutperforms distribution-based loss when dealing with false negative labels,\ntherefore we proposed Mask Dice loss (MD loss), which is the first reported\nregion-based loss function for training 3D image segmentation networks using\nsparse 2D slice labels. In addition, fault is an edge feature, and the current\nnetwork widely used for fault segmentation downsamples the features multiple\ntimes, which is not conducive to edge representation and thus requires many\nparameters and computational effort to preserve the features. We proposed\nFault-Net, which uses a high-resolution and shallow structure to propagate\nmulti-scale features in parallel, fully preserving edge features. Meanwhile, in\norder to efficiently fuse multiscale features, we decouple the convolution\nprocess into feature selection and channel fusion, and proposed a lightweight\nfeature fusion block, Multi-Scale Compression Fusion (MCF). Because the\nFault-Net always keeps the edge features during propagation, only few\nparameters and computation are required. Experimental results show that MD loss\ncan clearly weaken the effect of false negative labels. The Fault-Net parameter\nis only 0.42MB, support up to 528^3(1.5x10^8, Float32) size cuboid inference on\n16GB video ram, its inference speed on CPU and GPU is significantly faster than\nother networks. It works well on most of the open data seismic images, and the\nresult of our approach is state-ofthe-art in FORCE fault identification\ncompetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yimin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Timing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaoquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zongchao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers. (arXiv:2110.06990v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06990","description":"<p>Empirical science of neural scaling laws is a rapidly growing area of\nsignificant importance to the future of machine learning, particularly in the\nlight of recent breakthroughs achieved by large-scale pre-trained models such\nas GPT-3, CLIP and DALL-e. Accurately predicting the neural network performance\nwith increasing resources such as data, compute and model size provides a more\ncomprehensive evaluation of different approaches across multiple scales, as\nopposed to traditional point-wise comparisons of fixed-size models on\nfixed-size benchmarks, and, most importantly, allows for focus on the\nbest-scaling, and thus most promising in the future, approaches. In this work,\nwe consider a challenging problem of few-shot learning in image classification,\nespecially when the target data distribution in the few-shot phase is different\nfrom the source, training, data distribution, in a sense that it includes new\nimage classes not encountered during training. Our current main goal is to\ninvestigate how the amount of pre-training data affects the few-shot\ngeneralization performance of standard image classifiers. Our key observations\nare that (1) such performance improvements are well-approximated by power laws\n(linear log-log plots) as the training set size increases, (2) this applies to\nboth cases of target data coming from either the same or from a different\ndomain (i.e., new classes) as the training data, and (3) few-shot performance\non new classes converges at a faster rate than the standard classification\nperformance on previously seen classes. Our findings shed new light on the\nrelationship between scale and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prato_G/0/1/0/all/0/1\">Gabriele Prato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guiroy_S/0/1/0/all/0/1\">Simon Guiroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caballero_E/0/1/0/all/0/1\">Ethan Caballero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGAN: Adversarial Spatial-asymmetric Attention for Noisy Nona-Bayer Reconstruction. (arXiv:2110.08619v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.08619","description":"<p>Nona-Bayer colour filter array (CFA) pattern is considered one of the most\nviable alternatives to traditional Bayer patterns. Despite the substantial\nadvantages, such non-Bayer CFA patterns are susceptible to produce visual\nartefacts while reconstructing RGB images from noisy sensor data. This study\naddresses the challenges of learning RGB image reconstruction from noisy\nNona-Bayer CFA comprehensively. We propose a novel spatial-asymmetric attention\nmodule to jointly learn bi-direction transformation and large-kernel global\nattention to reduce the visual artefacts. We combine our proposed module with\nadversarial learning to produce plausible images from Nona-Bayer CFA. The\nfeasibility of the proposed method has been verified and compared with the\nstate-of-the-art image reconstruction method. The experiments reveal that the\nproposed method can reconstruct RGB images from noisy Nona-Bayer CFA without\nproducing any visually disturbing artefacts. Also, it can outperform the\nstate-of-the-art image reconstruction method in both qualitative and\nquantitative comparison. Code available:\nhttps://github.com/sharif-apu/SAGAN_BMVC21.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sharif_S/0/1/0/all/0/1\">S M A Sharif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naqvi_R/0/1/0/all/0/1\">Rizwan Ali Naqvi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biswas_M/0/1/0/all/0/1\">Mithun Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08733","description":"<p>Deep learning approaches have shown promising results in remote sensing high\nspatial resolution (HSR) land-cover mapping. However, urban and rural scenes\ncan show completely different geographical landscapes, and the inadequate\ngeneralizability of these algorithms hinders city-level or national-level\nmapping. Most of the existing HSR land-cover datasets mainly promote the\nresearch of learning semantic representation, thereby ignoring the model\ntransferability. In this paper, we introduce the Land-cOVEr Domain Adaptive\nsemantic segmentation (LoveDA) dataset to advance semantic and transferable\nlearning. The LoveDA dataset contains 5927 HSR images with 166768 annotated\nobjects from three different cities. Compared to the existing datasets, the\nLoveDA dataset encompasses two domains (urban and rural), which brings\nconsiderable challenges due to the: 1) multi-scale objects; 2) complex\nbackground samples; and 3) inconsistent class distributions. The LoveDA dataset\nis suitable for both land-cover semantic segmentation and unsupervised domain\nadaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on\neleven semantic segmentation methods and eight UDA methods. Some exploratory\nstudies including multi-scale architectures and strategies, additional\nbackground supervision, and pseudo-label analysis were also carried out to\naddress these challenges. The code are available at\nhttps://github.com/Junjue-Wang/LoveDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Ailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yanfei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alleviating Noisy-label Effects in Image Classification via Probability Transition Matrix. (arXiv:2110.08866v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08866","description":"<p>Deep-learning-based image classification frameworks often suffer from the\nnoisy label problem caused by the inter-observer variation. Recent studies\nemployed learning-to-learn paradigms (e.g., Co-teaching and JoCoR) to filter\nthe samples with noisy labels from the training set. However, most of them use\na simple cross-entropy loss as the criterion for noisy label identification.\nThe hard samples, which are beneficial for classifier learning, are often\nmistakenly treated as noises in such a setting since both the hard samples and\nones with noisy labels lead to a relatively larger loss value than the easy\ncases. In this paper, we propose a plugin module, namely noise ignoring block\n(NIB), consisting of a probability transition matrix and an inter-class\ncorrelation (IC) loss, to separate the hard samples from the mislabeled ones,\nand further boost the accuracy of image classification network trained with\nnoisy labels. Concretely, our IC loss is calculated as Kullback-Leibler\ndivergence between the network prediction and the accumulative soft label\ngenerated by the probability transition matrix. Such that, with the lower value\nof IC loss, the hard cases can be easily distinguished from mislabeled cases.\nExtensive experiments are conducted on natural and medical image datasets\n(CIFAR-10 and ISIC 2019). The experimental results show that our NIB module\nconsistently improves the performances of the state-of-the-art robust training\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongxin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09113","description":"<p>Salt and pepper noise removal is a common inverse problem in image\nprocessing, and it aims to restore image information with high quality.\nTraditional salt and pepper denoising methods have two limitations. First,\nnoise characteristics are often not described accurately. For example, the\nnoise location information is often ignored and the sparsity of the salt and\npepper noise is often described by L1 norm, which cannot illustrate the sparse\nvariables clearly. Second, conventional methods separate the contaminated image\ninto a recovered image and a noise part, thus resulting in recovering an image\nwith unsatisfied smooth parts and detail parts. In this study, we introduce a\nnoise detection strategy to determine the position of the noise, and a\nnon-convex sparsity regularization depicted by Lp quasi-norm is employed to\ndescribe the sparsity of the noise, thereby addressing the first limitation.\nThe morphological component analysis framework with stationary Framelet\ntransform is adopted to decompose the processed image into cartoon, texture,\nand noise parts to resolve the second limitation. In this framework, the\nstationary Framelet regularizations with different parameters control the\nrestoration of the cartoon and texture parts. In this way, the two parts are\nrecovered separately to avoid mutual interference. Then, the alternating\ndirection method of multipliers (ADMM) is employed to solve the proposed model.\nFinally, experiments are conducted to verify the proposed method and compare it\nwith some current state-of-the-art denoising methods. The experimental results\nshow that the proposed method can remove salt and pepper noise while preserving\nthe details of the processed image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingpin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huiying Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jianhua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Chaoqun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanping Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning multiplane images from single views with self-supervision. (arXiv:2110.09380v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09380","description":"<p>Generating static novel views from an already captured image is a hard task\nin computer vision and graphics, in particular when the single input image has\ndynamic parts such as persons or moving objects. In this paper, we tackle this\nproblem by proposing a new framework, called CycleMPI, that is capable of\nlearning a multiplane image representation from single images through a cyclic\ntraining strategy for self-supervision. Our framework does not require stereo\ndata for training, therefore it can be trained with massive visual data from\nthe Internet, resulting in a better generalization capability even for very\nchallenging cases. Although our method does not require stereo data for\nsupervision, it reaches results on stereo datasets comparable to the state of\nthe art in a zero-shot scenario. We evaluated our method on RealEstate10K and\nMannequin Challenge datasets for view synthesis and presented qualitative\nresults on Places II dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_G/0/1/0/all/0/1\">Gustavo Sutter P. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luvizon_D/0/1/0/all/0/1\">Diogo C. Luvizon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_A/0/1/0/all/0/1\">Antonio Joia Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_A/0/1/0/all/0/1\">Andre G. C. Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Penatti_O/0/1/0/all/0/1\">Otavio A. B. Penatti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}