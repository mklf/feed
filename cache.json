{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Graph Refinement for Coreference Resolution. (arXiv:2203.16574v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16574","description":"<p>The state-of-the-art models for coreference resolution are based on\nindependent mention pair-wise decisions. We propose a modelling approach that\nlearns coreference at the document-level and takes global decisions. For this\npurpose, we model coreference links in a graph structure where the nodes are\ntokens in the text, and the edges represent the relationship between them. Our\nmodel predicts the graph in a non-autoregressive manner, then iteratively\nrefines it based on previous predictions, allowing global dependencies between\ndecisions. The experimental results show improvements over various baselines,\nreinforcing the hypothesis that document-level information improves conference\nresolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miculicich_L/0/1/0/all/0/1\">Lesly Miculicich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Switched and Code Mixed Speech Recognition for Indic languages. (arXiv:2203.16578v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16578","description":"<p>Training multilingual automatic speech recognition (ASR) systems is\nchallenging because acoustic and lexical information is typically language\nspecific. Training multilingual system for Indic languages is even more tougher\ndue to lack of open source datasets and results on different approaches. We\ncompare the performance of end to end multilingual speech recognition system to\nthe performance of monolingual models conditioned on language identification\n(LID). The decoding information from a multilingual model is used for language\nidentification and then combined with monolingual models to get an improvement\nof 50% WER across languages. We also propose a similar technique to solve the\nCode Switched problem and achieve a WER of 21.77 and 28.27 over Hindi-English\nand Bengali-English respectively. Our work talks on how transformer based ASR\nespecially wav2vec 2.0 can be applied in developing multilingual ASR and code\nswitched ASR for Indic languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Speech Recognition for Indic Languages using Language Model. (arXiv:2203.16595v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16595","description":"<p>We study the effect of applying a language model (LM) on the output of\nAutomatic Speech Recognition (ASR) systems for Indic languages. We fine-tune\nwav2vec $2.0$ models for $18$ Indic languages and adjust the results with\nlanguage models trained on text derived from a variety of sources. Our findings\ndemonstrate that the average Character Error Rate (CER) decreases by over $28$\n\\% and the average Word Error Rate (WER) decreases by about $36$ \\% after\ndecoding with LM. We show that a large LM may not provide a substantial\nimprovement as compared to a diverse one. We also demonstrate that high quality\ntranscriptions can be obtained on domain-specific data without retraining the\nASR model and show results on biomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Word Error Rate a good evaluation metric for Speech Recognition in Indic Languages?. (arXiv:2203.16601v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16601","description":"<p>We propose a new method for the calculation of error rates in Automatic\nSpeech Recognition (ASR). This new metric is for languages that contain half\ncharacters and where the same character can be written in different forms. We\nimplement our methodology in Hindi which is one of the main languages from\nIndic context and we think this approach is scalable to other similar languages\ncontaining a large character set. We call our metrics Alternate Word Error Rate\n(AWER) and Alternate Character Error Rate (ACER).\n</p>\n<p>We train our ASR models using wav2vec 2.0\\cite{baevski2020wav2vec} for Indic\nlanguages. Additionally we use language models to improve our model\nperformance. Our results show a significant improvement in analyzing the error\nrates at word and character level and the interpretability of the ASR system is\nimproved upto $3$\\% in AWER and $7$\\% in ACER for Hindi. Our experiments\nsuggest that in languages which have complex pronunciation, there are multiple\nways of writing words without changing their meaning. In such cases AWER and\nACER will be more useful rather than WER and CER as metrics. Furthermore, we\nopen source a new benchmarking dataset of 21 hours for Hindi with the new\nmetric scripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Language Models without Positional Encodings Still Learn Positional Information. (arXiv:2203.16634v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16634","description":"<p>Transformers typically require some form of positional encoding, such as\npositional embeddings, to process natural language sequences. Surprisingly, we\nfind that transformer language models without any explicit positional encoding\nare still competitive with standard models, and that this phenomenon is robust\nacross different datasets, model sizes, and sequence lengths. Probing\nexperiments reveal that such models acquire an implicit notion of absolute\npositions throughout the network, effectively compensating for the missing\ninformation. We conjecture that causal attention enables the model to infer the\nnumber of predecessors that each token can attend to, thereby approximating its\nabsolute position.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haviv_A/0/1/0/all/0/1\">Adi Haviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ofir Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1\">Peter Izsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations. (arXiv:2203.16639v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16639","description":"<p>We present a meta-learning framework for learning new visual concepts\nquickly, from just one or a few examples, guided by multiple naturally\noccurring data streams: simultaneously looking at images, reading sentences\nthat describe the objects in the scene, and interpreting supplemental sentences\nthat relate the novel concept with other concepts. The learned concepts support\ndownstream applications, such as answering questions by reasoning about unseen\nimages. Our model, namely FALCON, represents individual visual concepts, such\nas colors and shapes, as axis-aligned boxes in a high-dimensional space (the\n\"box embedding space\"). Given an input image and its paired sentence, our model\nfirst resolves the referential expression in the sentence and associates the\nnovel concept with particular objects in the scene. Next, our model interprets\nsupplemental sentences to relate the novel concept with other known concepts,\nsuch as \"X has property Y\" or \"X is a kind of Y\". Finally, it infers an optimal\nbox embedding for the novel concept that jointly 1) maximizes the likelihood of\nthe observed instances in the image, and 2) satisfies the relationships between\nthe novel concepts and the known ones. We demonstrate the effectiveness of our\nmodel on both synthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_L/0/1/0/all/0/1\">Lingjie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiayuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of Speaker Representations Using Heterogeneous Training Batch Assembly. (arXiv:2203.16646v1 [cs.SD])","link":"http://arxiv.org/abs/2203.16646","description":"<p>In traditional speaker diarization systems, a well-trained speaker model is a\nkey component to extract representations from consecutive and partially\noverlapping segments in a long speech session. To be more consistent with the\nback-end segmentation and clustering, we propose a new CNN-based speaker\nmodeling scheme, which takes into account the heterogeneity of the speakers in\neach training segment and batch. We randomly and synthetically augment the\ntraining data into a set of segments, each of which contains more than one\nspeaker and some overlapping parts. A soft label is imposed on each segment\nbased on its speaker occupation ratio, and the standard cross entropy loss is\nimplemented in model training. In this way, the speaker model should have the\nability to generate a geometrically meaningful embedding for each multi-speaker\nsegment. Experimental results show that our system is superior to the baseline\nsystem using x-vectors in two speaker diarization tasks. In the CALLHOME task\ntrained on the NIST SRE and Switchboard datasets, our system achieves a\nrelative reduction of 12.93% in DER. In Track 2 of CHiME-6, our system provides\n13.24%, 12.60%, and 5.65% relative reductions in DER, JER, and WER,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Huai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pin-Tuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo. (arXiv:2203.16682v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16682","description":"<p>We present a debiased dataset for the Person-centric Visual Grounding (PCVG)\ntask first proposed by Cui et al. (2021) in the Who's Waldo dataset. Given an\nimage and a caption, PCVG requires pairing up a person's name mentioned in a\ncaption with a bounding box that points to the person in the image. We find\nthat the original Who's Waldo dataset compiled for this task contains a large\nnumber of biased samples that are solvable simply by heuristic methods; for\ninstance, in many cases the first name in the sentence corresponds to the\nlargest bounding box, or the sequence of names in the sentence corresponds to\nan exact left-to-right order in the image. Naturally, models trained on these\nbiased data lead to over-estimation of performance on the benchmark. To enforce\nmodels being correct for the correct reasons, we design automated tools to\nfilter and debias the original dataset by ruling out all examples of\ninsufficient context, such as those with no verb or with a long chain of\nconjunct names in their captions. Our experiments show that our new sub-sampled\ndataset contains less bias with much lowered heuristic performances and widened\ngaps between heuristic and supervised methods. We also demonstrate the same\nbenchmark model trained on our debiased training set outperforms that trained\non the original biased (and larger) training set on our debiased test set. We\nargue our debiased dataset offers the PCVG task a more practical baseline for\nreliable benchmarking and future improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yiran Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings. (arXiv:2203.16685v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16685","description":"<p>This paper presents a streaming speaker-attributed automatic speech\nrecognition (SA-ASR) model that can recognize \"who spoke what\" with low latency\neven when multiple people are speaking simultaneously. Our model is based on\ntoken-level serialized output training (t-SOT) which was recently proposed to\ntranscribe multi-talker speech in a streaming fashion. To further recognize\nspeaker identities, we propose an encoder-decoder based speaker embedding\nextractor that can estimate a speaker representation for each recognized token\nnot only from non-overlapping speech but also from overlapping speech. The\nproposed speaker embedding, named t-vector, is extracted synchronously with the\nt-SOT ASR model, enabling joint execution of speaker identification (SID) or\nspeaker diarization (SD) with the multi-talker transcription with low latency.\nWe evaluate the proposed model for a joint task of ASR and SID/SD by using\nLibriSpeechMix and LibriCSS corpora. The proposed model achieves substantially\nbetter accuracy than a prior streaming model and shows comparable or sometimes\neven superior results to the state-of-the-art offline SA-ASR model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAE-AST: Masked Autoencoding Audio Spectrogram Transformer. (arXiv:2203.16691v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16691","description":"<p>In this paper, we propose a simple yet powerful improvement over the recent\nSelf-Supervised Audio Spectrogram Transformer (SSAST) model for speech and\naudio classification. Specifically, we leverage the insight that the SSAST uses\na very high masking ratio (75%) during pretraining, meaning that the vast\nmajority of self-attention compute is performed on mask tokens. We address this\nby integrating the encoder-decoder architecture from Masked Autoencoders are\nScalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on\nonly unmasked input, and a shallow decoder operates on encoder outputs and mask\ntokens. We find that MAE-like pretraining can provide a 3x speedup and 2x\nmemory usage reduction over the vanilla SSAST using current audio pretraining\nstrategies with ordinary model and input sizes. When fine-tuning on downstream\ntasks, which only uses the encoder, we find that our approach outperforms the\nSSAST on a variety of downstream tasks. We further conduct comprehensive\nevaluations into different strategies of pretraining and explore differences in\nMAE-style pretraining between the visual and audio domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baade_A/0/1/0/all/0/1\">Alan Baade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Disentangled Variational Speech Representation Learning for Zero-shot Voice Conversion. (arXiv:2203.16705v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16705","description":"<p>Traditional studies on voice conversion (VC) have made progress with parallel\ntraining data and known speakers. Good voice conversion quality is obtained by\nexploring better alignment modules or expressive mapping functions. In this\nstudy, we investigate zero-shot VC from a novel perspective of self-supervised\ndisentangled speech representation learning. Specifically, we achieve the\ndisentanglement by balancing the information flow between global speaker\nrepresentation and time-varying content representation in a sequential\nvariational autoencoder (VAE). A zero-shot voice conversion is performed by\nfeeding an arbitrary speaker embedding and content embeddings to the VAE\ndecoder. Besides that, an on-the-fly data augmentation training strategy is\napplied to make the learned representation noise invariant. On TIMIT and VCTK\ndatasets, we achieve state-of-the-art performance on both objective evaluation,\ni.e., speaker verification (SV) on speaker embedding and content embedding, and\nsubjective evaluation, i.e., voice naturalness and similarity, and remains to\nbe robust even with noisy source/target utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lian_J/0/1/0/all/0/1\">Jiachen Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chunlei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Table Question Answering via Retrieval-Augmented Generation. (arXiv:2203.16714v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16714","description":"<p>Most existing end-to-end Table Question Answering (Table QA) models consist\nof a two-stage framework with a retriever to select relevant table candidates\nfrom a corpus and a reader to locate the correct answers from table candidates.\nEven though the accuracy of the reader models is significantly improved with\nthe recent transformer-based approaches, the overall performance of such\nframeworks still suffers from the poor accuracy of using traditional\ninformation retrieval techniques as retrievers. To alleviate this problem, we\nintroduce T-RAG, an end-to-end Table QA model, where a non-parametric dense\nvector index is fine-tuned jointly with BART, a parametric sequence-to-sequence\nmodel to generate answer tokens. Given any natural language question, T-RAG\nutilizes a unified pipeline to automatically search through a table corpus to\ndirectly locate the correct answer from the table cells. We apply T-RAG to\nrecent open-domain Table QA benchmarks and demonstrate that the fine-tuned\nT-RAG model is able to achieve state-of-the-art performance in both the\nend-to-end Table QA and the table retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feifei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1\">Mustafa Canim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1\">James Hendler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving speaker de-identification with functional data analysis of f0 trajectories. (arXiv:2203.16738v1 [cs.SD])","link":"http://arxiv.org/abs/2203.16738","description":"<p>Due to a constantly increasing amount of speech data that is stored in\ndifferent types of databases, voice privacy has become a major concern. To\nrespond to such concern, speech researchers have developed various methods for\nspeaker de-identification. The state-of-the-art solutions utilize deep learning\nsolutions which can be effective but might be unavailable or impractical to\napply for, for example, under-resourced languages. Formant modification is a\nsimpler, yet effective method for speaker de-identification which requires no\ntraining data. Still, remaining intonational patterns in formant-anonymized\nspeech may contain speaker-dependent cues. This study introduces a novel\nspeaker de-identification method, which, in addition to simple formant shifts,\nmanipulates f0 trajectories based on functional data analysis. The proposed\nspeaker de-identification method will conceal plausibly identifying pitch\ncharacteristics in a phonetically controllable manner and improve formant-based\nspeaker de-identification up to 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavi_L/0/1/0/all/0/1\">Lauri Tavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinnunen_T/0/1/0/all/0/1\">Tomi Kinnunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_R/0/1/0/all/0/1\">Rosa Gonz&#xe1;lez Hautam&#xe4;ki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis. (arXiv:2203.16747v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16747","description":"<p>Recently, there has been a trend to investigate the factual knowledge\ncaptured by Pre-trained Language Models (PLMs). Many works show the PLMs'\nability to fill in the missing factual words in cloze-style prompts such as\n\"Dante was born in [MASK].\" However, it is still a mystery how PLMs generate\nthe results correctly: relying on effective clues or shortcut patterns? We try\nto answer this question by a causal-inspired analysis that quantitatively\nmeasures and evaluates the word-level patterns that PLMs depend on to generate\nthe missing words. We check the words that have three typical associations with\nthe missing words: knowledge-dependent, positionally close, and highly\nco-occurred. Our analysis shows: (1) PLMs generate the missing factual words\nmore by the positionally close and highly co-occurred words than the\nknowledge-dependent words; (2) the dependence on the knowledge-dependent words\nis more effective than the positionally close and highly co-occurred words.\nAccordingly, we conclude that the PLMs capture the factual knowledge\nineffectively because of depending on the inadequate associations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhenhua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chengjie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhenzhou Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech Recognition: A Comparative Study. (arXiv:2203.16757v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16757","description":"<p>Recently, the end-to-end training approach for multi-channel ASR has shown\nits effectiveness, which usually consists of a beamforming front-end and a\nrecognition back-end. However, the end-to-end training becomes more difficult\ndue to the integration of multiple modules, particularly considering that\nmulti-channel speech data recorded in real environments are limited in size.\nThis raises the demand to exploit the single-channel data for multi-channel\nend-to-end ASR. In this paper, we systematically compare the performance of\nthree schemes to exploit external single-channel data for multi-channel\nend-to-end ASR, namely back-end pre-training, data scheduling, and data\nsimulation, under different settings such as the sizes of the single-channel\ndata and the choices of the front-end. Extensive experiments on CHiME-4 and\nAISHELL-4 datasets demonstrate that while all three methods improve the\nmulti-channel end-to-end speech recognition performance, data simulation\noutperforms the other two, at the cost of longer training time. Data scheduling\noutperforms back-end pre-training marginally but nearly consistently,\npresumably because that in the pre-training stage, the back-end tends to\noverfit on the single-channel data, especially when the single-channel data\nsize is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR. (arXiv:2203.16758v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16758","description":"<p>History and future contextual information are known to be important for\naccurate acoustic modeling. However, acquiring future context brings latency\nfor streaming ASR. In this paper, we propose a new framework - Chunking,\nSimulating Future Context and Decoding (CUSIDE) for streaming speech\nrecognition. A new simulation module is introduced to recursively simulate the\nfuture contextual frames, without waiting for future context. The simulation\nmodule is jointly trained with the ASR model using a self-supervised loss; the\nASR model is optimized with the usual ASR loss, e.g., CTC-CRF as used in our\nexperiments. Experiments show that, compared to using real future frames as\nright context, using simulated future context can drastically reduce latency\nwhile maintaining recognition accuracy. With CUSIDE, we obtain new\nstate-of-the-art streaming ASR results on the AISHELL-1 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Huahuan Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_H/0/1/0/all/0/1\">Hongyu Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Ke Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks. (arXiv:2203.16773v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16773","description":"<p>Speech representations learned from Self-supervised learning (SSL) models\nhave been found beneficial for various speech processing tasks. However,\nutilizing SSL representations usually requires fine-tuning the pre-trained\nmodels or designing task-specific downstream models and loss functions, causing\nmuch memory usage and human labor. On the other hand, prompting in Natural\nLanguage Processing (NLP) is an efficient and widely used technique to leverage\npre-trained language models (LMs). Nevertheless, such a paradigm is little\nstudied in the speech community. We report in this paper the first exploration\nof the prompt tuning paradigm for speech processing tasks based on Generative\nSpoken Language Model (GSLM). Experiment results show that the prompt tuning\ntechnique achieves competitive performance in speech classification tasks with\nfewer trainable parameters than fine-tuning specialized downstream models. We\nfurther study the technique in challenging sequence generation tasks. Prompt\ntuning also demonstrates its potential, while the limitation and possible\nresearch directions are discussed in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bangla hate speech detection on social media using attention-based recurrent neural network. (arXiv:2203.16775v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16775","description":"<p>Hate speech has spread more rapidly through the daily use of technology and,\nmost notably, by sharing your opinions or feelings on social media in a\nnegative aspect. Although numerous works have been carried out in detecting\nhate speeches in English, German, and other languages, very few works have been\ncarried out in the context of the Bengali language. In contrast, millions of\npeople communicate on social media in Bengali. The few existing works that have\nbeen carried out need improvements in both accuracy and interpretability. This\narticle proposed encoder decoder based machine learning model, a popular tool\nin NLP, to classify user's Bengali comments on Facebook pages. A dataset of\n7,425 Bengali comments, consisting of seven distinct categories of hate\nspeeches, was used to train and evaluate our model. For extracting and encoding\nlocal features from the comments, 1D convolutional layers were used. Finally,\nthe attention mechanism, LSTM, and GRU based decoders have been used for\npredicting hate speech categories. Among the three encoder decoder algorithms,\nthe attention-based decoder obtained the best accuracy (77%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amit Kumar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_A/0/1/0/all/0/1\">Abdullah Al Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Anik Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Nur Hossain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Language Model Integration for Transducer based Speech Recognition. (arXiv:2203.16776v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16776","description":"<p>Utilizing text-only data with an external language model (LM) in end-to-end\nRNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class\nof methods such as density ratio (DR) and ILM estimation (ILME) have been\ndeveloped, outperforming the classic shallow fusion (SF) method. The basic idea\nbehind these methods is that RNN-T posterior should first subtract the\nimplicitly learned ILM prior, in order to integrate the external LM. While\nrecent studies suggest that RNN-T only learns some low-order language model\ninformation, the DR method uses a well-trained ILM. We hypothesize that this\nsetting is appropriate and may deteriorate the performance of the DR method,\nand propose a low-order density ratio method (LODR) by training a low-order\nweak ILM for DR. Extensive empirical experiments are conducted on both\nin-domain and cross-domain scenarios on English LibriSpeech &amp; Tedlium-2 and\nChinese WenetSpeech &amp; AISHELL-1 datasets. It is shown that LODR consistently\noutperforms SF in all tasks, while performing generally close to ILME and\nbetter than DR in most tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Huahuan Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Ke Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misogynistic Meme Detection using Early Fusion Model with Graph Network. (arXiv:2203.16781v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16781","description":"<p>In recent years , there has been an upsurge in a new form of entertainment\nmedium called memes. These memes although seemingly innocuous have transcended\nonto the boundary of online harassment against women and created an unwanted\nbias against them . To help alleviate this problem , we propose an early fusion\nmodel for prediction and identification of misogynistic memes and its type in\nthis paper for which we participated in SemEval-2022 Task 5 . The model\nreceives as input meme image with its text transcription with a target vector.\nGiven that a key challenge with this task is the combination of different\nmodalities to predict misogyny, our model relies on pretrained contextual\nrepresentations from different state-of-the-art transformer-based language\nmodels and pretrained image pretrained models to get an effective image\nrepresentation. Our model achieved competitive results on both SubTask-A and\nSubTask-B with the other competition teams and significantly outperforms the\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices. (arXiv:2203.16788v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16788","description":"<p>Environmental, Social, and Governance (ESG) are non-financial factors that\nare garnering attention from investors as they increasingly look to apply these\nas part of their analysis to identify material risks and growth opportunities.\nSome of this attention is also driven by clients who, now more aware than ever,\nare demanding for their money to be managed and invested responsibly. As the\ninterest in ESG grows, so does the need for investors to have access to\nconsumable ESG information. Since most of it is in text form in reports,\ndisclosures, press releases, and 10-Q filings, we see a need for sophisticated\nNLP techniques for classification tasks for ESG text. We hypothesize that an\nESG domain-specific pre-trained model will help with such and study building of\nthe same in this paper. We explored doing this by fine-tuning BERTs pre-trained\nweights using ESG specific text and then further fine-tuning the model for a\nclassification task. We were able to achieve accuracy better than the original\nBERT and baseline models in environment-specific classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_S/0/1/0/all/0/1\">Srishti Mehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louka_R/0/1/0/all/0/1\">Robert Louka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMER: Multimodal Multi-task learning for Emotion Recognition in Spoken Utterances. (arXiv:2203.16794v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16794","description":"<p>Emotion Recognition (ER) aims to classify human utterances into different\nemotion categories. Based on early-fusion and self-attention-based multimodal\ninteraction between text and acoustic modalities, in this paper, we propose a\nmultimodal multitask learning approach for ER from individual utterances in\nisolation. Experiments on the IEMOCAP benchmark show that our proposed model\nperforms better than our re-implementation of state-of-the-art and achieves\nbetter performance than all other unimodal and multimodal approaches in\nliterature. In addition, strong baselines and ablation studies prove the\neffectiveness of our proposed approach. We make all our codes publicly\navailable on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Discourse Aware Sequence Learning Approach for Emotion Recognition in Conversations. (arXiv:2203.16799v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16799","description":"<p>The expression of emotions is a crucial part of daily human communication.\nModeling the conversational and sequential context has seen much success and\nplays a vital role in Emotion Recognition in Conversations (ERC). However,\nexisting approaches either model only one of the two or employ naive\nlate-fusion methodologies to obtain final utterance representations. This paper\nproposes a novel idea to incorporate both these contexts and better model the\nintrinsic structure within a conversation. More precisely, we propose a novel\narchitecture boosted by a modified LSTM cell, which we call DiscLSTM, that\nbetter captures the interaction between conversational and sequential context.\nDiscLSTM brings together the best of both worlds and provides a more intuitive\nand efficient way to model the information flow between individual utterances\nby better capturing long-distance conversational background through discourse\nrelations and sequential context through recurrence. We conduct experiments on\nfour benchmark datasets for ERC and show that our model achieves performance\ncompetitive to state-of-the-art and at times performs better than other\ngraph-based approaches in literature, with a conversational graph that is both\nsparse and avoids complicated edge relations like much of previous work. We\nmake all our codes publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BRIO: Bringing Order to Abstractive Summarization. (arXiv:2203.16804v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16804","description":"<p>Abstractive summarization models are commonly trained using maximum\nlikelihood estimation, which assumes a deterministic (one-point) target\ndistribution in which an ideal model will assign all the probability mass to\nthe reference summary. This assumption may lead to performance degradation\nduring inference, where the model needs to compare several system-generated\n(candidate) summaries that have deviated from the reference summary. To address\nthis problem, we propose a novel training paradigm which assumes a\nnon-deterministic distribution so that different candidate summaries are\nassigned probability mass according to their quality. Our method achieves a new\nstate-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07\nROUGE-1) datasets. Further analysis also shows that our model can estimate\nprobabilities of candidate summaries that are more correlated with their level\nof quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Pre-trained Wav2Vec2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications. (arXiv:2203.16822v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16822","description":"<p>Recent work on self-supervised pre-training focus on leveraging large-scale\nunlabeled speech data to build robust end-to-end (E2E) acoustic models (AM)\nthat can be later fine-tuned on downstream tasks e.g., automatic speech\nrecognition (ASR). Yet, few works investigated the impact on performance when\nthe data substantially differs between the pre-training and downstream\nfine-tuning phases (i.e., domain shift). We target this scenario by analyzing\nthe robustness of Wav2Vec2.0 and XLS-R models on downstream ASR for a\ncompletely unseen domain, i.e., air traffic control (ATC) communications. We\nbenchmark the proposed models on four challenging ATC test sets\n(signal-to-noise ratio varies between 5 to 20 dB). Relative word error rate\n(WER) reduction between 20% to 40% are obtained in comparison to hybrid-based\nstate-of-the-art ASR baselines by fine-tuning E2E acoustic models with a small\nfraction of labeled data. We also study the impact of fine-tuning data size on\nWERs, going from 5 minutes (few-shot) to 15 hours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kleinert_M/0/1/0/all/0/1\">Matthias Kleinert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhan_Q/0/1/0/all/0/1\">Qingran Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectiveness of text to speech pseudo labels for forced alignment and cross lingual pretrained models for low resource speech recognition. (arXiv:2203.16823v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16823","description":"<p>In the recent years end to end (E2E) automatic speech recognition (ASR)\nsystems have achieved promising results given sufficient resources. Even for\nlanguages where not a lot of labelled data is available, state of the art E2E\nASR systems can be developed by pretraining on huge amounts of high resource\nlanguages and finetune on low resource languages. For a lot of low resource\nlanguages the current approaches are still challenging, since in many cases\nlabelled data is not available in open domain. In this paper we present an\napproach to create labelled data for Maithili, Bhojpuri and Dogri by utilising\npseudo labels from text to speech for forced alignment. The created data was\ninspected for quality and then further used to train a transformer based\nwav2vec 2.0 ASR model. All data and models are available in open domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"indic-punct: An automatic punctuation restoration and inverse text normalization framework for Indic languages. (arXiv:2203.16825v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16825","description":"<p>Automatic Speech Recognition (ASR) generates text which is most of the times\ndevoid of any punctuation. Absence of punctuation is text can affect\nreadability. Also, down stream NLP tasks such as sentiment analysis, machine\ntranslation, greatly benefit by having punctuation and sentence boundary\ninformation. We present an approach for automatic punctuation of text using a\npretrained IndicBERT model. Inverse text normalization is done by hand writing\nweighted finite state transducer (WFST) grammars. We have developed this tool\nfor 11 Indic languages namely Hindi, Tamil, Telugu, Kannada, Gujarati, Marathi,\nOdia, Bengali, Assamese, Malayalam and Punjabi. All code and data is publicly.\navailable\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings. (arXiv:2203.16834v1 [cs.SD])","link":"http://arxiv.org/abs/2203.16834","description":"<p>In this paper, we conduct a comparative study on speaker-attributed automatic\nspeech recognition (SA-ASR) in the multi-party meeting scenario, a topic with\nincreasing attention in meeting rich transcription. Specifically, three\napproaches are evaluated in this study. The first approach, FD-SOT, consists of\na frame-level diarization model to identify speakers and a multi-talker ASR to\nrecognize utterances. The speaker-attributed transcriptions are obtained by\naligning the diarization results and recognized hypotheses. However, such an\nalignment strategy may suffer from erroneous timestamps due to the modular\nindependence, severely hindering the model performance. Therefore, we propose\nthe second approach, WD-SOT, to address alignment errors by introducing a\nword-level diarization model, which can get rid of such timestamp alignment\ndependency. To further mitigate the alignment issues, we propose the third\napproach, TS-ASR, which trains a target-speaker separation module and an ASR\nmodule jointly. By comparing various strategies for each SA-ASR approach,\nexperimental results on a real meeting scenario corpus, AliMeeting, reveal that\nthe WD-SOT approach achieves 10.7% relative reduction on averaged\nspeaker-dependent character error rate (SD-CER), compared with the FD-SOT\napproach. In addition, the TS-ASR approach also outperforms the FD-SOT approach\nand brings 16.5% relative average SD-CER reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhihao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuxiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset. (arXiv:2203.16844v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16844","description":"<p>This paper introduces a high-quality rich annotated Mandarin conversational\n(RAMC) speech dataset called MagicData-RAMC. The MagicData-RAMC corpus contains\n180 hours of conversational speech data recorded from native speakers of\nMandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs\nin MagicData-RAMC are classified into 15 diversified domains and tagged with\ntopic labels, ranging from science and technology to ordinary life. Accurate\ntranscription and precise speaker voice activity timestamps are manually\nlabeled for each sample. Speakers' detailed information is also provided. As a\nMandarin speech dataset designed for dialog scenarios with high quality and\nrich annotations, MagicData-RAMC enriches the data diversity in the Mandarin\nspeech community and allows extensive research on a series of speech-related\ntasks, including automatic speech recognition, speaker diarization, topic\ndetection, keyword search, text-to-speech, etc. We also conduct several\nrelevant tasks and provide experimental results to help evaluate the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Runyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lingxuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gaofeng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Ji Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yonghong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Efficient Training of RNN-Transducer with Sampled Softmax. (arXiv:2203.16868v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16868","description":"<p>RNN-Transducer has been one of promising architectures for end-to-end\nautomatic speech recognition. Although RNN-Transducer has many advantages\nincluding its strong accuracy and streaming-friendly property, its high memory\nconsumption during training has been a critical problem for development. In\nthis work, we propose to apply sampled softmax to RNN-Transducer, which\nrequires only a small subset of vocabulary during training thus saves its\nmemory consumption. We further extend sampled softmax to optimize memory\nconsumption for a minibatch, and employ distributions of auxiliary CTC losses\nfor sampling vocabulary to improve model accuracy. We present experimental\nresults on LibriSpeech, AISHELL-1, and CSJ-APS, where sampled softmax greatly\nreduces memory consumption and still maintains the accuracy of the baseline\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jaesong Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_L/0/1/0/all/0/1\">Lukas Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A bilingual approach to specialised adjectives through word embeddings in the karstology domain. (arXiv:2203.16885v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16885","description":"<p>We present an experiment in extracting adjectives which express a specific\nsemantic relation using word embeddings. The results of the experiment are then\nthoroughly analysed and categorised into groups of adjectives exhibiting formal\nor semantic similarity. The experiment and analysis are performed for English\nand Croatian in the domain of karstology using data sets and methods developed\nin the TermFrame project. The main original contributions of the article are\ntwofold: firstly, proposing a new and promising method of extracting\nsemantically related words relevant for terminology, and secondly, providing a\ndetailed evaluation of the output so that we gain a better understanding of the\ndomain-specific semantic structures on the one hand and the types of\nsimilarities extracted by word embeddings on the other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simeunovic_L/0/1/0/all/0/1\">Larisa Gr&#x10d;i&#x107; Simeunovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinc_M/0/1/0/all/0/1\">Matej Martinc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vintar_S/0/1/0/all/0/1\">&#x160;pela Vintar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey of neural models for the automatic analysis of conversation: Towards a better integration of the social sciences. (arXiv:2203.16891v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16891","description":"<p>Some exciting new approaches to neural architectures for the analysis of\nconversation have been introduced over the past couple of years. These include\nneural architectures for detecting emotion, dialogue acts, and sentiment\npolarity. They take advantage of some of the key attributes of contemporary\nmachine learning, such as recurrent neural networks with attention mechanisms\nand transformer-based approaches. However, while the architectures themselves\nare extremely promising, the phenomena they have been applied to to date are\nbut a small part of what makes conversation engaging. In this paper we survey\nthese neural architectures and what they have been applied to. On the basis of\nthe social science literature, we then describe what we believe to be the most\nfundamental and definitional feature of conversation, which is its\nco-construction over time by two or more interlocutors. We discuss how neural\narchitectures of the sort surveyed could profitably be applied to these more\nfundamental aspects of conversation, and what this buys us in terms of a better\nanalysis of conversation and even, in the longer term, a better way of\ngenerating conversation for a conversational system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassell_J/0/1/0/all/0/1\">Justine Cassell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Character-level Span-based Model for Mandarin Prosodic Structure Prediction. (arXiv:2203.16922v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16922","description":"<p>The accuracy of prosodic structure prediction is crucial to the naturalness\nof synthesized speech in Mandarin text-to-speech system, but now is limited by\nwidely-used sequence-to-sequence framework and error accumulation from previous\nword segmentation results. In this paper, we propose a span-based Mandarin\nprosodic structure prediction model to obtain an optimal prosodic structure\ntree, which can be converted to corresponding prosodic label sequence. Instead\nof the prerequisite for word segmentation, rich linguistic features are\nprovided by Chinese character-level BERT and sent to encoder with\nself-attention architecture. On top of this, span representation and label\nscoring are used to describe all possible prosodic structure trees, of which\neach tree has its corresponding score. To find the optimal tree with the\nhighest score for a given sentence, a bottom-up CKY-style algorithm is further\nused. The proposed method can predict prosodic labels of different levels at\nthe same time and accomplish the process directly from Chinese characters in an\nend-to-end manner. Experiment results on two real-world datasets demonstrate\nthe excellent performance of our span-based method over all\nsequence-to-sequence baseline approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xueyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changbin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aplica\\c{c}\\~ao de ros como ferramenta de ensino a rob\\'otica / using ros as a robotics teaching tool. (arXiv:2203.16923v1 [cs.RO])","link":"http://arxiv.org/abs/2203.16923","description":"<p>The study of robotic manipulators is the main goal of Industrial Robotics\nClass, part of Control Engineers training course. There is a difficulty in\npreparing academic practices and projects in the area of robotics due to the\nhigh cost of specific educational equipment. The practical classes and the\ndevelopment of projects are very important for engineers training, it is\nproposed to use simulation software in order to provide practical experience\nfor the students of the discipline. In this context, the present article aims\nto expose the use of the Robot Operation System (ROS) as a tool to develop a\nrobotic arm and implement the functionality of forward and inverse kinematics.\nSuch development could be used as an educational tool to increase the interest\nand learning of students in the robotics discipline and to expand research\nareas for the discipline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evangelista_D/0/1/0/all/0/1\">Daniel Maia Evangelista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavalcante_P/0/1/0/all/0/1\">Pedro Benevides Cavalcante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segundo_A/0/1/0/all/0/1\">Afonso Henriques Fontes Neto Segundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation for Sparse-Data Settings: What Do We Gain by Not Using Bert?. (arXiv:2203.16926v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16926","description":"<p>The practical success of much of NLP depends on the availability of training\ndata. However, in real-world scenarios, training data is often scarce, not\nleast because many application domains are restricted and specific. In this\nwork, we compare different methods to handle this problem and provide\nguidelines for building NLP applications when there is only a small amount of\nlabeled training data available for a specific domain. While transfer learning\nwith pre-trained language models outperforms other methods across tasks,\nalternatives do not perform much worse while requiring much less computational\neffort, thus significantly reducing monetary and environmental cost. We examine\nthe performance tradeoffs of several such alternatives, including models that\ncan be trained up to 175K times faster and do not require a single GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sedinkina_M/0/1/0/all/0/1\">Marina Sedinkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying PBL in the Development and Modeling of kinematics for Robotic Manipulators with Interdisciplinarity between Computer-Assisted Project, Robotics, and Microcontrollers. (arXiv:2203.16927v1 [cs.RO])","link":"http://arxiv.org/abs/2203.16927","description":"<p>Considering the difficulty of students in calculating the direct and inverse\nkinematics of a robotic manipulator using only conventional tools of a\nclassroom, this article proposes the application of Project Based Learning\n(ABP) through the design, development, mathematical modeling of a robotic\nmanipulator as an integrative project of the disciplines of Industrial\nRobotics, Microcontrollers and Computer Assisted Design with students of the\nControl and Automation Engineering of the University of Fortaleza. Once\ndesigned and machined, the manipulator arm was assembled using servo motors\nconnected to a microcontroled prototyping board, to then have its kinematics\ncalculated. At the end are presented the results that the project has brought\nto the learning of the disciplines on the optics of the tutor and students.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segundo_A/0/1/0/all/0/1\">Afonso Henriques Fontes Neto Segundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1\">Joel Sotero da Cunha Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_P/0/1/0/all/0/1\">Paulo Cirillo Souza Barbosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santana_R/0/1/0/all/0/1\">Raul Fontenele Santana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Search for Speech Emotion Recognition. (arXiv:2203.16928v1 [cs.SD])","link":"http://arxiv.org/abs/2203.16928","description":"<p>Deep neural networks have brought significant advancements to speech emotion\nrecognition (SER). However, the architecture design in SER is mainly based on\nexpert knowledge and empirical (trial-and-error) evaluations, which is\ntime-consuming and resource intensive. In this paper, we propose to apply\nneural architecture search (NAS) techniques to automatically configure the SER\nmodels. To accelerate the candidate architecture optimization, we propose a\nuniform path dropout strategy to encourage all candidate architecture\noperations to be equally optimized. Experimental results of two different\nneural structures on IEMOCAP show that NAS can improve SER performance (54.89\\%\nto 56.28\\%) while maintaining model parameter sizes. The proposed dropout\nstrategy also shows superiority over the previous approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shoukang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavThruVec: Latent speech representation as intermediate features for neural speech synthesis. (arXiv:2203.16930v1 [cs.SD])","link":"http://arxiv.org/abs/2203.16930","description":"<p>Recent advances in neural text-to-speech research have been dominated by\ntwo-stage pipelines utilizing low-level intermediate speech representation such\nas mel-spectrograms. However, such predetermined features are fundamentally\nlimited, because they do not allow to exploit the full potential of a\ndata-driven approach through learning hidden representations. For this reason,\nseveral end-to-end methods have been proposed. However, such models are harder\nto train and require a large number of high-quality recordings with\ntranscriptions. Here, we propose WavThruVec - a two-stage architecture that\nresolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as\nintermediate speech representation. Since these hidden activations provide\nhigh-level linguistic features, they are more robust to noise. That allows us\nto utilize annotated speech datasets of a lower quality to train the\nfirst-stage module. At the same time, the second-stage component can be trained\non large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are\ntime-aligned and speaker-independent. This results in an increased\ngeneralization capability to out-of-vocabulary words, as well as to a better\ngeneralization to unseen speakers. We show that the proposed model not only\nmatches the quality of state-of-the-art neural models, but also presents useful\nproperties enabling tasks like voice conversion or zero-shot synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siuzdak_H/0/1/0/all/0/1\">Hubert Siuzdak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dura_P/0/1/0/all/0/1\">Piotr Dura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-end Chinese Text Normalization Model based on Rule-guided Flat-Lattice Transformer. (arXiv:2203.16954v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16954","description":"<p>Text normalization, defined as a procedure transforming non standard words to\nspoken-form words, is crucial to the intelligibility of synthesized speech in\ntext-to-speech system. Rule-based methods without considering context can not\neliminate ambiguation, whereas sequence-to-sequence neural network based\nmethods suffer from the unexpected and uninterpretable errors problem. Recently\nproposed hybrid system treats rule-based model and neural model as two cascaded\nsub-modules, where limited interaction capability makes neural network model\ncannot fully utilize expert knowledge contained in the rules. Inspired by\nFlat-LAttice Transformer (FLAT), we propose an end-to-end Chinese text\nnormalization model, which accepts Chinese characters as direct input and\nintegrates expert knowledge contained in rules into the neural network, both\ncontribute to the superior performance of proposed model for the text\nnormalization task. We also release a first publicly accessible largescale\ndataset for Chinese text normalization. Our proposed model has achieved\nexcellent results on this dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenlin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Huashan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16965","description":"<p>While self-supervised speech representation learning (SSL) models serve a\nvariety of downstream tasks, these models have been observed to overfit to the\ndomain from which the unlabelled data originates. To alleviate this issue, we\npropose PADA (Pruning Assisted Domain Adaptation) and zero out redundant\nweights from models pre-trained on large amounts of out-of-domain (OOD) data.\nIntuitively, this helps to make space for the target-domain ASR finetuning. The\nredundant weights can be identified through various pruning strategies which\nhave been discussed in detail as a part of this work. Specifically, we\ninvestigate the effect of the recently discovered Task-Agnostic and Task-Aware\npruning on PADA and propose a new pruning paradigm based on the latter, which\nwe call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial\npruning mask from a well fine-tuned OOD model, which makes it starkly different\nfrom the rest of the pruning strategies discussed in the paper. Our proposed\nCD-TAW methodology achieves up to 20.6% relative WER improvement over our\nbaseline when fine-tuned on a 2-hour subset of Switchboard data without\nlanguage model (LM) decoding. Furthermore, we conduct a detailed analysis to\nhighlight the key design choices of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_L/0/1/0/all/0/1\">Lodagala V S V Durga Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16973","description":"<p>Self-supervised learning (SSL) to learn high-level speech representations has\nbeen a popular approach to building Automatic Speech Recognition (ASR) systems\nin low-resource settings. However, the common assumption made in literature is\nthat a considerable amount of unlabeled data is available for the same domain\nor language that can be leveraged for SSL pre-training, which we acknowledge is\nnot feasible in a real-world setting. In this paper, as part of the Interspeech\nGram Vaani ASR challenge, we try to study the effect of domain, language,\ndataset size, and other aspects of our upstream pre-training SSL data on the\nfinal performance low-resource downstream ASR task. We also build on the\ncontinued pre-training paradigm to study the effect of prior knowledge\npossessed by models trained using SSL. Extensive experiments and studies reveal\nthat the performance of ASR systems is susceptible to the data used for SSL\npre-training. Their performance improves with an increase in similarity and\nvolume of pre-training data. We believe our work will be helpful to the speech\ncommunity in building better ASR systems in low-resource settings and steer\nresearch towards improving generalization in SSL-based pre-training for speech\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_L/0/1/0/all/0/1\">Lodagala V S V Durga Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Coupling of Optimal Transport for Spoken Language Identification. (arXiv:2203.17036v1 [eess.AS])","link":"http://arxiv.org/abs/2203.17036","description":"<p>In order to reduce domain discrepancy to improve the performance of\ncross-domain spoken language identification (SLID) system, as an unsupervised\ndomain adaptation (UDA) method, we have proposed a joint distribution alignment\n(JDA) model based on optimal transport (OT). A discrepancy measurement based on\nOT was adopted for JDA between training and test data sets. In our previous\nstudy, it was supposed that the training and test sets share the same label\nspace. However, in real applications, the label space of the test set is only a\nsubset of that of the training set. Fully matching training and test domains\nfor distribution alignment may introduce negative domain transfer. In this\npaper, we propose an JDA model based on partial optimal transport (POT), i.e.,\nonly partial couplings of OT are allowed during JDA. Moreover, since the label\nof test data is unknown, in the POT, a soft weighting on the coupling based on\ntransport cost is adaptively set during domain alignment. Experiments were\ncarried out on a cross-domain SLID task to evaluate the proposed UDA. Results\nshowed that our proposed UDA significantly improved the performance due to the\nconsideration of the partial couplings in OT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xugang Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1\">Peng Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawai_H/0/1/0/all/0/1\">Hisashi Kawai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manipulation of oral cancer speech using neural articulatory synthesis. (arXiv:2203.17072v1 [cs.SD])","link":"http://arxiv.org/abs/2203.17072","description":"<p>We present an articulatory synthesis framework for the synthesis and\nmanipulation of oral cancer speech for clinical decision making and alleviation\nof patient stress. Objective and subjective evaluations demonstrate that the\nframework has acceptable naturalness and is worth further investigation. A\nsubsequent subjective vowel and consonant identification experiment showed that\nthe articulatory synthesis system can manipulate the articulatory trajectories\nso that the synthesised speech reproduces problems present in the ground truth\noral cancer speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halpern_B/0/1/0/all/0/1\">Bence Mark Halpern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebernik_T/0/1/0/all/0/1\">Teja Rebernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tienkamp_T/0/1/0/all/0/1\">Thomas Tienkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_R/0/1/0/all/0/1\">Rob van Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brekel_M/0/1/0/all/0/1\">Michiel van den Brekel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witjes_M/0/1/0/all/0/1\">Max Witjes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1\">Odette Scharenborg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific and Technological Text Knowledge Extraction Method of based on Word Mixing and GRU. (arXiv:2203.17079v1 [cs.CL])","link":"http://arxiv.org/abs/2203.17079","description":"<p>The knowledge extraction task is to extract triple relations (head\nentity-relation-tail entity) from unstructured text data. The existing\nknowledge extraction methods are divided into \"pipeline\" method and joint\nextraction method. The \"pipeline\" method is to separate named entity\nrecognition and entity relationship extraction and use their own modules to\nextract them. Although this method has better flexibility, the training speed\nis slow. The learning model of joint extraction is an end-to-end model\nimplemented by neural network to realize entity recognition and relationship\nextraction at the same time, which can well preserve the association between\nentities and relationships, and convert the joint extraction of entities and\nrelationships into a sequence annotation problem. In this paper, we propose a\nknowledge extraction method for scientific and technological resources based on\nword mixture and GRU, combined with word mixture vector mapping method and\nself-attention mechanism, to effectively improve the effect of text\nrelationship extraction for Chinese scientific and technological resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Suyu Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Junping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretation of Black Box NLP Models: A Survey. (arXiv:2203.17081v1 [cs.LG])","link":"http://arxiv.org/abs/2203.17081","description":"<p>An increasing number of machine learning models have been deployed in domains\nwith high stakes such as finance and healthcare. Despite their superior\nperformances, many models are black boxes in nature which are hard to explain.\nThere are growing efforts for researchers to develop methods to interpret these\nblack-box models. Post hoc explanations based on perturbations, such as LIME,\nare widely used approaches to interpret a machine learning model after it has\nbeen built. This class of methods has been shown to exhibit large instability,\nposing serious challenges to the effectiveness of the method itself and harming\nuser trust. In this paper, we propose S-LIME, which utilizes a hypothesis\ntesting framework based on central limit theorem for determining the number of\nperturbation points needed to guarantee stability of the resulting explanation.\nExperiments on both simulated and real world data sets are provided to\ndemonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Shivani Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_N/0/1/0/all/0/1\">Niladri Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Subir Kumar Saha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PANGUBOT: Efficient Generative Dialogue Pre-training from Pre-trained Language Model. (arXiv:2203.17090v1 [cs.CL])","link":"http://arxiv.org/abs/2203.17090","description":"<p>In this paper, we introduce PANGUBOT, a Chinese pre-trained open-domain\ndialogue generation model based on a large pre-trained language model (PLM)\nPANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue\nmodels trained over a massive amount of dialogue data from scratch, we aim to\nbuild a powerful dialogue model with relatively fewer data and computation\ncosts by inheriting valuable language capabilities and knowledge from PLMs. To\nthis end, we train PANGUBOT from the large PLM PANGU-alpha, which has been\nproven well-performed on a variety of Chinese natural language tasks. We\ninvestigate different aspects of responses generated by PANGUBOT, including\nresponse quality, knowledge, and safety. We show that PANGUBOT outperforms\nstate-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA\n(Zhou et al., 2021)) w.r.t. the above three aspects. We also demonstrate that\nPANGUBOT can be easily deployed to generate emotional responses without further\ntraining. Throughout our empirical analysis, we also point out that the\nPANGUBOT response quality, knowledge correctness, and safety are still far from\nperfect, and further explorations are indispensable to building reliable and\nsmart dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yulong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chuanfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$k$NN-NER: Named Entity Recognition with Nearest Neighbor Search. (arXiv:2203.17103v1 [cs.CL])","link":"http://arxiv.org/abs/2203.17103","description":"<p>Inspired by recent advances in retrieval augmented methods in\nNLP~\\citep{khandelwal2019generalization,khandelwal2020nearest,meng2021gnn}, in\nthis paper, we introduce a $k$ nearest neighbor NER ($k$NN-NER) framework,\nwhich augments the distribution of entity labels by assigning $k$ nearest\nneighbors retrieved from the training set. This strategy makes the model more\ncapable of handling long-tail cases, along with better few-shot learning\nabilities. $k$NN-NER requires no additional operation during the training\nphase, and by interpolating $k$ nearest neighbors search into the vanilla NER\nmodel, $k$NN-NER consistently outperforms its vanilla counterparts: we achieve\na new state-of-the-art F1-score of 72.03 (+1.25) on the Chinese Weibo dataset\nand improved results on a variety of widely used NER benchmarks. Additionally,\nwe show that $k$NN-NER can achieve comparable results to the vanilla NER model\nwith 40\\% less amount of training data. Code available at\n\\url{https://github.com/ShannonAI/KNN-NER}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_R/0/1/0/all/0/1\">Rongbin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Acoustic Noise on Alzheimer's Disease Detection from Speech: Should You Let Baby Cry?. (arXiv:2203.17110v1 [cs.SD])","link":"http://arxiv.org/abs/2203.17110","description":"<p>Research related to automatically detecting Alzheimer's disease (AD) is\nimportant, given the high prevalence of AD and the high cost of traditional\nmethods. Since AD significantly affects the acoustics of spontaneous speech,\nspeech processing and machine learning (ML) provide promising techniques for\nreliably detecting AD. However, speech audio may be affected by different types\nof background noise and it is important to understand how the noise influences\nthe accuracy of ML models detecting AD from speech. In this paper, we study the\neffect of fifteen types of noise from five different categories on the\nperformance of four ML models trained with three types of acoustic\nrepresentations. We perform a thorough analysis showing how ML models and\nacoustic features are affected by different types of acoustic noise. We show\nthat acoustic noise is not necessarily harmful - certain types of noise are\nbeneficial for AD detection models and help increasing accuracy by up to 4.8\\%.\nWe provide recommendations on how to utilize acoustic noise in order to achieve\nthe best performance results with the ML models deployed in real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1\">Jekaterina Novikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Contrast Stretching on Target Feature for Speech Enhancement. (arXiv:2203.17152v1 [cs.SD])","link":"http://arxiv.org/abs/2203.17152","description":"<p>Speech enhancement (SE) performance has improved considerably since the use\nof deep learning (DL) models as a base function. In this study, we propose a\nperceptual contrast stretching (PCS) approach to further improve SE\nperformance. PCS is derived based on the critical band importance function and\napplied to modify the targets of the SE model. Specifically, PCS stretches the\ncontract of target features according to perceptual importance, thereby\nimproving the overall SE performance. Compared to post-processing based\nimplementations, incorporating PCS into the training phase preserves\nperformance and reduces online computation. It is also worth noting that PCS\ncan be suitably combined with different SE model architectures and training\ncriteria. Meanwhile, PCS does not affect the causality or convergence of the SE\nmodel training. Experimental results on the VoiceBank-DEMAND dataset showed\nthat the proposed method can achieve state-of-the-art performance on both\ncausal (PESQ=3.07) and non-causal (PESQ=3.35) SE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chao_R/0/1/0/all/0/1\">Rong Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Szu-Wei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xugang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Evaluation of NLP-based Models for Software Engineering. (arXiv:2203.17166v1 [cs.SE])","link":"http://arxiv.org/abs/2203.17166","description":"<p>NLP-based models have been increasingly incorporated to address SE problems.\nThese models are either employed in the SE domain with little to no change, or\nthey are greatly tailored to source code and its unique characteristics. Many\nof these approaches are considered to be outperforming or complementing\nexisting solutions. However, an important question arises here: \"Are these\nmodels evaluated fairly and consistently in the SE community?\". To answer this\nquestion, we reviewed how NLP-based models for SE problems are being evaluated\nby researchers. The findings indicate that currently there is no consistent and\nwidely-accepted protocol for the evaluation of these models. While different\naspects of the same task are being assessed in different studies, metrics are\ndefined based on custom choices, rather than a system, and finally, answers are\ncollected and interpreted case by case. Consequently, there is a dire need to\nprovide a methodological way of evaluating NLP-based models to have a\nconsistent assessment and preserve the possibility of fair and efficient\ncomparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1\">Maliheh Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadabadi_M/0/1/0/all/0/1\">Matin Nili Ahmadabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$. (arXiv:2203.17189v1 [cs.LG])","link":"http://arxiv.org/abs/2203.17189","description":"<p>Recent neural network-based language models have benefited greatly from\nscaling up the size of training datasets and the number of parameters in the\nmodels themselves. Scaling can be complicated due to various factors including\nthe need to distribute computation on supercomputer clusters (e.g., TPUs),\nprevent bottlenecks when infeeding data, and ensure reproducible results. In\nthis work, we present two software libraries that ease these issues:\n$\\texttt{t5x}$ simplifies the process of building and training large language\nmodels at scale while maintaining ease of use, and $\\texttt{seqio}$ provides a\ntask-based API for simple creation of fast and reproducible training data and\nevaluation pipelines. These open-source libraries have been used to train\nmodels with hundreds of billions of parameters on datasets with multiple\nterabytes of training data.\n</p>\n<p>Along with the libraries, we release configurations and instructions for\nT5-like encoder-decoder models as well as GPT-like decoder-only architectures.\n</p>\n<p>$\\texttt{t5x}$ and $\\texttt{seqio}$ are open source and available at\nhttps://github.com/google-research/t5x and https://github.com/google/seqio,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levskaya_A/0/1/0/all/0/1\">Anselm Levskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1\">James Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andor_D/0/1/0/all/0/1\">Daniel Andor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaffney_C/0/1/0/all/0/1\">Colin Gaffney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohiuddin_A/0/1/0/all/0/1\">Afroz Mohiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1\">Curtis Hawthorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salcianu_A/0/1/0/all/0/1\">Alex Salcianu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zee_M/0/1/0/all/0/1\">Marc van Zee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1\">Sebastian Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_L/0/1/0/all/0/1\">Livio Baldini Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haitang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvyashchenko_S/0/1/0/all/0/1\">Sasha Tsvyashchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulian_J/0/1/0/all/0/1\">Jannis Bulian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Andrew Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenealy_K/0/1/0/all/0/1\">Kathleen Kenealy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stephan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_M/0/1/0/all/0/1\">Marvin Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1\">Alexandre Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maitin_Shepard_J/0/1/0/all/0/1\">Jeremy Maitin-Shepard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1\">Noah Fiedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omernick_M/0/1/0/all/0/1\">Mark Omernick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeta_B/0/1/0/all/0/1\">Brennan Saeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sepassi_R/0/1/0/all/0/1\">Ryan Sepassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiridonov_A/0/1/0/all/0/1\">Alexander Spiridonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newlan_J/0/1/0/all/0/1\">Joshua Newlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gesmundo_A/0/1/0/all/0/1\">Andrea Gesmundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech. (arXiv:2203.17190v1 [eess.AS])","link":"http://arxiv.org/abs/2203.17190","description":"<p>Recently, leveraging BERT pre-training to improve the phoneme encoder in text\nto speech (TTS) has drawn increasing attention. However, the works apply\npre-training with character-based units to enhance the TTS phoneme encoder,\nwhich is inconsistent with the TTS fine-tuning that takes phonemes as input.\nPre-training only with phonemes as input can alleviate the input mismatch but\nlack the ability to model rich representations and semantic information due to\nlimited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a\nnovel variant of the BERT model that uses mixed phoneme and sup-phoneme\nrepresentations to enhance the learning capability. Specifically, we merge the\nadjacent phonemes into sup-phonemes and combine the phoneme sequence and the\nmerged sup-phoneme sequence as the model input, which can enhance the model\ncapacity to learn rich contextual representations. Experiment results\ndemonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS\nperformance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The\nMixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to\nthe previous TTS pre-trained model PnG BERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_D/0/1/0/all/0/1\">Daxin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yuzi Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Tan Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CatIss: An Intelligent Tool for Categorizing Issues Reports using Transformers. (arXiv:2203.17196v1 [cs.SE])","link":"http://arxiv.org/abs/2203.17196","description":"<p>Users use Issue Tracking Systems to keep track and manage issue reports in\ntheir repositories. An issue is a rich source of software information that\ncontains different reports including a problem, a request for new features, or\nmerely a question about the software product. As the number of these issues\nincreases, it becomes harder to manage them manually. Thus, automatic\napproaches are proposed to help facilitate the management of issue reports.\n</p>\n<p>This paper describes CatIss, an automatic CATegorizer of ISSue reports which\nis built upon the Transformer-based pre-trained RoBERTa model. CatIss\nclassifies issue reports into three main categories of Bug reports,\nEnhancement/feature requests, and Questions. First, the datasets provided for\nthe NLBSE tool competition are cleaned and preprocessed. Then, the pre-trained\nRoBERTa model is fine-tuned on the preprocessed dataset. Evaluating CatIss on\nabout 80 thousand issue reports from GitHub, indicates that it performs very\nwell surpassing the competition baseline, TicketTagger, and achieving 87.2%\nF1-score (micro average). Additionally, as CatIss is trained on a wide set of\nrepositories, it is a generic prediction model, hence applicable for any unseen\nsoftware project or projects with little historical data. Scripts for cleaning\nthe datasets, training CatIss, and evaluating the model are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1\">Maliheh Izadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Wrap-Up Effects through an Information-Theoretic Lens. (arXiv:2203.17213v1 [cs.CL])","link":"http://arxiv.org/abs/2203.17213","description":"<p>Numerous analyses of reading time (RT) data have been implemented -- all in\nan effort to better understand the cognitive processes driving reading\ncomprehension. However, data measured on words at the end of a sentence -- or\neven at the end of a clause -- is often omitted due to the confounding factors\nintroduced by so-called \"wrap-up effects,\" which manifests as a skewed\ndistribution of RTs for these words. Consequently, the understanding of the\ncognitive processes that might be involved in these wrap-up effects is limited.\nIn this work, we attempt to learn more about these processes by examining the\nrelationship between wrap-up effects and information-theoretic quantities, such\nas word and context surprisals. We find that the distribution of information in\nprior contexts is often predictive of sentence- and clause-final RTs (while not\nof sentence-medial RTs). This lends support to several prior hypotheses about\nthe processes involved in wrap-up effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_T/0/1/0/all/0/1\">Thomas Hikaru Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the probability-quality paradox in language generation. (arXiv:2203.17217v1 [cs.CL])","link":"http://arxiv.org/abs/2203.17217","description":"<p>When generating natural language from neural probabilistic models, high\nprobability does not always coincide with high quality: It has often been\nobserved that mode-seeking decoding methods, i.e., those that produce\nhigh-probability text under the model, lead to unnatural language. On the other\nhand, the lower-probability text generated by stochastic methods is perceived\nas more human-like. In this note, we offer an explanation for this phenomenon\nby analyzing language generation through an information-theoretic lens.\nSpecifically, we posit that human-like language should contain an amount of\ninformation (quantified as negative log-probability) that is close to the\nentropy of the distribution over natural strings. Further, we posit that\nlanguage with substantially more (or less) information is undesirable. We\nprovide preliminary empirical evidence in favor of this hypothesis; quality\nratings of both human and machine-generated text -- covering multiple tasks and\ncommon decoding strategies -- suggest high-quality text has an information\ncontent significantly closer to the entropy than we would expect by chance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiher_G/0/1/0/all/0/1\">Gian Wiher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Readability Model for Cebuano. (arXiv:2203.17225v1 [cs.CL])","link":"http://arxiv.org/abs/2203.17225","description":"<p>In this study, we developed the first baseline readability model for the\nCebuano language. Cebuano is the second most-used native language in the\nPhilippines with about 27.5 million speakers. As the baseline, we extracted\ntraditional or surface-based features, syllable patterns based from Cebuano's\ndocumented orthography, and neural embeddings from the multilingual BERT model.\nResults show that the use of the first two handcrafted linguistic features\nobtained the best performance trained on an optimized Random Forest model with\napproximately 84\\% across all metrics. The feature sets and algorithm used also\nis similar to previous results in readability assessment for the Filipino\nlanguage showing potential of crosslingual application. To encourage more work\nfor readability assessment in Philippine languages such as Cebuano, we\nopen-sourced both code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reyes_L/0/1/0/all/0/1\">Lloyd Lois Antonie Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibanez_M/0/1/0/all/0/1\">Michael Antonio Iba&#xf1;ez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapinit_R/0/1/0/all/0/1\">Ranz Sapinit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1\">Mohammed Hussien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17247","description":"<p>Breakthroughs in transformer-based models have revolutionized not only the\nNLP field, but also vision and multimodal systems. However, although\nvisualization and interpretability tools have become available for NLP models,\ninternal mechanisms of vision and multimodal transformers remain largely\nopaque. With the success of these transformers, it is increasingly critical to\nunderstand their inner workings, as unraveling these black-boxes will lead to\nmore capable and trustworthy models. To contribute to this quest, we propose\nVL-InterpreT, which provides novel interactive visualizations for interpreting\nthe attentions and hidden representations in multimodal transformers.\nVL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety\nof statistics in attention heads throughout all layers for both vision and\nlanguage components, (2) visualizes cross-modal and intra-modal attentions\nthrough easily readable heatmaps, and (3) plots the hidden representations of\nvision and language tokens as they pass through the transformer layers. In this\npaper, we demonstrate the functionalities of VL-InterpreT through the analysis\nof KD-VLP, an end-to-end pretraining vision-language multimodal\ntransformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and\nWebQA, two visual question answering benchmarks. Furthermore, we also present a\nfew interesting findings about multimodal transformer behaviors that were\nlearned through our tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aflalo_E/0/1/0/all/0/1\">Estelle Aflalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Meng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_S/0/1/0/all/0/1\">Shao-Yen Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v1 [q-bio.NC])","link":"http://arxiv.org/abs/2203.17255","description":"<p>This theoretical article examines how to construct human-like working memory\nand thought processes within a computer. There should be two working memory\nstores, one analogous to sustained firing in association cortex, and one\nanalogous to synaptic potentiation in the cerebral cortex. These stores must be\nconstantly updated with new representations that arise from either\nenvironmental stimulation or internal processing. They should be updated\ncontinuously, and in an iterative fashion, meaning that, in the next state,\nsome items in the set of coactive items should always be retained. Thus, the\nset of concepts coactive in working memory will evolve gradually and\nincrementally over time. This makes each state is a revised iteration of the\npreceding state and causes successive states to overlap and blend with respect\nto the set of representations they contain. As new representations are added\nand old ones are subtracted, some remain active for several seconds over the\ncourse of these changes. This persistent activity, similar to that used in\nartificial recurrent neural networks, is used to spread activation energy\nthroughout the global workspace to search for the next associative update. The\nresult is a chain of associatively linked intermediate states that are capable\nof advancing toward a solution or goal. Iterative updating is conceptualized\nhere as an information processing strategy, a computational and\nneurophysiological determinant of the stream of thought, and an algorithm for\ndesigning and programming artificial intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1\">Jared Edward Reser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v5 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/1907.02704","description":"<p>A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2012.09966","description":"<p>Sender-receiver interactions, and specifically persuasion games, are widely\nresearched in economic modeling and artificial intelligence. However, in the\nclassic persuasion games setting, the messages sent from the expert to the\ndecision-maker (DM) are abstract or well-structured signals rather than natural\nlanguage messages. This paper addresses the use of natural language in\npersuasion games. For this purpose, we conduct an online repeated interaction\nexperiment. At each trial of the interaction, an informed expert aims to sell\nan uninformed decision-maker a vacation in a hotel, by sending her a review\nthat describes the hotel. While the expert is exposed to several scored\nreviews, the decision-maker observes only the single review sent by the expert,\nand her payoff in case she chooses to take the hotel is a random draw from the\nreview score distribution available to the expert only. We also compare the\nbehavioral patterns in this experiment to the equivalent patterns in similar\nexperiments where the communication is based on the numerical values of the\nreviews rather than the reviews' text, and observe substantial differences\nwhich can be explained through an equilibrium analysis of the game. We consider\na number of modeling approaches for our verbal communication setup, differing\nfrom each other in the model type (deep neural network vs. linear classifier),\nthe type of features used by the model (textual, behavioral or both) and the\nsource of the textual features (DNN-based vs. hand-crafted). Our results\ndemonstrate that given a prefix of the interaction sequence, our models can\npredict the future decisions of the decision-maker, particularly when a\nsequential modeling approach and hand-crafted textual features are applied.\nFurther analysis of the hand-crafted textual features allows us to make initial\nobservations about the aspects of text that drive decision making in our setup\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Apel_R/0/1/0/all/0/1\">Reut Apel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erev_I/0/1/0/all/0/1\">Ido Erev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1\">Moshe Tennenholtz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Speaker Adaptation for Text-to-Speech Synthesis. (arXiv:2103.14512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14512","description":"<p>Training a multi-speaker Text-to-Speech (TTS) model from scratch is\ncomputationally expensive and adding new speakers to the dataset requires the\nmodel to be re-trained. The naive solution of sequential fine-tuning of a model\nfor new speakers can lead to poor performance of older speakers. This\nphenomenon is known as catastrophic forgetting. In this paper, we look at TTS\nmodeling from a continual learning perspective, where the goal is to add new\nspeakers without forgetting previous speakers. Therefore, we first propose an\nexperimental setup and show that serial fine-tuning for new speakers can cause\nthe forgetting of the earlier speakers. Then we exploit two well-known\ntechniques for continual learning, namely experience replay and weight\nregularization. We reveal how one can mitigate the effect of degradation in\nspeech synthesis diversity in sequential training of new speakers using these\nmethods. Finally, we present a simple extension to experience replay to improve\nthe results in extreme setups where we have access to very small buffers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hemati_H/0/1/0/all/0/1\">Hamed Hemati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1\">Damian Borth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection. (arXiv:2106.04564v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04564","description":"<p>Pre-trained Transformer-based models were reported to be robust in intent\nclassification. In this work, we first point out the importance of in-domain\nout-of-scope detection in few-shot intent recognition tasks and then illustrate\nthe vulnerability of pre-trained Transformer-based models against samples that\nare in-domain but out-of-scope (ID-OOS). We construct two new datasets, and\nempirically show that pre-trained models do not perform well on both ID-OOS\nexamples and general out-of-scope examples, especially on fine-grained few-shot\nintent detection tasks. To figure out how the models mistakenly classify ID-OOS\nintents as in-scope intents, we further conduct analysis on confidence scores\nand the overlapping keywords, as well as point out several prospective\ndirections for future work. Resources are available on\nhttps://github.com/jianguoz/Few-Shot-Intent-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Speaking Styles in Conversational Text-to-Speech Synthesis with Graph-based Multi-modal Context Modeling. (arXiv:2106.06233v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.06233","description":"<p>Comparing with traditional text-to-speech (TTS) systems, conversational TTS\nsystems are required to synthesize speeches with proper speaking style\nconfirming to the conversational context. However, state-of-the-art context\nmodeling methods in conversational TTS only model the textual information in\ncontext with a recurrent neural network (RNN). Such methods have limited\nability in modeling the inter-speaker influence in conversations, and also\nneglect the speaking styles and the intra-speaker inertia inside each speaker.\nInspired by DialogueGCN and its superiority in modeling such conversational\ninfluences than RNN based approaches, we propose a graph-based multi-modal\ncontext modeling method and adopt it to conversational TTS to enhance the\nspeaking styles of synthesized speeches. Both the textual and speaking style\ninformation in the context are extracted and processed by DialogueGCN to model\nthe inter- and intra-speaker influence in conversations. The outputs of\nDialogueGCN are then summarized by attention mechanism, and converted to the\nenhanced speaking style for current utterance. An English conversation corpus\nis collected and annotated for our research and released to public. Experiment\nresults on this corpus demonstrate the effectiveness of our proposed approach,\nwhich outperforms the state-of-the-art context modeling method in\nconversational TTS in both MOS and ABX preference rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingbei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chao Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.11956","description":"<p>Automatically mining sentiment tendency contained in natural language is a\nfundamental research to some artificial intelligent applications, where\nsolutions alternate with challenges. Transfer learning and multi-task learning\ntechniques have been leveraged to mitigate the supervision sparsity and\ncollaborate multiple heterogeneous domains correspondingly. Recent years, the\nsensitive nature of users' private data raises another challenge for sentiment\nclassification, i.e., data privacy protection. In this paper, we resort to\nfederated learning for multiple domain sentiment classification under the\nconstraint that the corpora must be stored on decentralized devices. In view of\nthe heterogeneous semantics across multiple parties and the peculiarities of\nword embedding, we pertinently provide corresponding solutions. First, we\npropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for\nbetter model aggregation and personalization in federated sentiment\nclassification. Second, we propose KTEPS$^\\star$ with the consideration of the\nrich semantic and huge embedding size properties of word vectors, utilizing\nProjection-based Dimension Reduction (PDR) methods for privacy protection and\nefficient transmission simultaneously. We propose two federated sentiment\nclassification scenes based on public benchmarks, and verify the superiorities\nof our proposed methods with abundant experimental investigations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Chun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shaoming Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Structure Matters Most: Perturbation Study in NLU. (arXiv:2107.13955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.13955","description":"<p>Recent research analyzing the sensitivity of natural language understanding\nmodels to word-order perturbations has shown that neural models are\nsurprisingly insensitive to the order of words. In this paper, we investigate\nthis phenomenon by developing order-altering perturbations on the order of\nwords, subwords, and characters to analyze their effect on neural models'\nperformance on language understanding tasks. We experiment with measuring the\nimpact of perturbations to the local neighborhood of characters and global\nposition of characters in the perturbed texts and observe that perturbation\nfunctions found in prior literature only affect the global ordering while the\nlocal ordering remains relatively unperturbed. We empirically show that neural\nmodels, invariant of their inductive biases, pretraining scheme, or the choice\nof tokenization, mostly rely on the local structure of text to build\nunderstanding and make limited use of the global structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clouatre_L/0/1/0/all/0/1\">Louis Clouatre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1\">Amal Zouaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The paradox of the compositionality of natural language: a neural machine translation case study. (arXiv:2108.05885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05885","description":"<p>Obtaining human-like performance in NLP is often argued to require\ncompositional generalisation. Whether neural networks exhibit this ability is\nusually studied by training models on highly compositional synthetic data.\nHowever, compositionality in natural language is much more complex than the\nrigid, arithmetic-like version such data adheres to, and artificial\ncompositionality tests thus do not allow us to determine how neural models deal\nwith more realistic forms of compositionality. In this work, we re-instantiate\nthree compositionality tests from the literature and reformulate them for\nneural machine translation (NMT). Our results highlight that: i) unfavourably,\nmodels trained on more data are more compositional; ii) models are sometimes\nless compositional than expected, but sometimes more, exemplifying that\ndifferent levels of compositionality are required, and models are not always\nable to modulate between them correctly; iii) some of the non-compositional\nbehaviours are mistakes, whereas others reflect the natural variation in data.\nApart from an empirical study, our work is a call to action: we should rethink\nthe evaluation of compositionality in neural networks and develop benchmarks\nusing real data to evaluate compositionality on natural language, where\ncomposing meaning is not as straightforward as doing the math.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1\">Verna Dankers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1\">Elia Bruni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03910","description":"<p>In this paper, we leverage large language models (LMs) to perform zero-shot\ntext style transfer. We present a prompting method that we call augmented\nzero-shot learning, which frames style transfer as a sentence rewriting task\nand requires only a natural language instruction, without model fine-tuning or\nexemplars in the target style. Augmented zero-shot learning is simple and\ndemonstrates promising results not just on standard style transfer tasks such\nas sentiment, but also on arbitrary transformations such as \"make this\nmelodramatic\" or \"insert a metaphor.\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01661","description":"<p>Iterating with new and improved OCR solutions enforces decision making when\nit comes to targeting the right candidates for reprocessing. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those targeting decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. Through extension of this technique, a regression model,\nthat is able to take into account the enhancement potential of a new OCR\nengine, is also presented. They both mark promising approaches, especially for\ncultural institutions dealing with historical data of lower quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_Y/0/1/0/all/0/1\">Yves Maurer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications. (arXiv:2110.05781v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.05781","description":"<p>Automatic speech recognition (ASR) allows transcribing the communications\nbetween air traffic controllers (ATCOs) and aircraft pilots. The transcriptions\nare used later to extract ATC named entities e.g., aircraft callsigns, command\ntypes, or values. One common challenge is Speech Activity Detection (SAD) and\ndiarization system. If one of them fails then two or more single speaker\nsegments remain in the same recording, jeopardizing the overall system's\nperformance. We propose a system that combines the segmentation of a SAD module\nwith a BERT model that performs speaker change detection (SCD) and speaker role\ndetection (SRD) by chunking ASR transcripts i.e., diarization with a defined\nnumber of speakers together with SRD. The proposed model is evaluated on\nreal-life ATC test sets. It reaches up to 0.90/0.95 F1-score on ATCO/pilot SRD,\nwhich means a 27% relative improvement on diarization error rate (DER) compared\nto standard acoustic-based diarization. Results are measured on ASR transcripts\nof challenging ATC test sets with $\\sim$13\\% word error rate, and the\nrobustness of the system is even validated on noisy ASR transcripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Seyyed Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Approach to Mispronunciation Detection and Diagnosis with Acoustic, Phonetic and Linguistic (APL) Embeddings. (arXiv:2110.07274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07274","description":"<p>Many mispronunciation detection and diagnosis (MD&amp;D) research approaches try\nto exploit both the acoustic and linguistic features as input. Yet the\nimprovement of the performance is limited, partially due to the shortage of\nlarge amount annotated training data at the phoneme level. Phonetic embeddings,\nextracted from ASR models trained with huge amount of word level annotations,\ncan serve as a good representation of the content of input speech, in a\nnoise-robust and speaker-independent manner. These embeddings, when used as\nimplicit phonetic supplementary information, can alleviate the data shortage of\nexplicit phoneme annotations. We propose to utilize Acoustic, Phonetic and\nLinguistic (APL) embedding features jointly for building a more powerful MD&amp;D\nsystem. Experimental results obtained on the L2-ARCTIC database show the\nproposed approach outperforms the baseline by 9.93%, 10.13% and 6.17% on the\ndetection accuracy, diagnosis error rate and the F-measure, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wenxuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_J/0/1/0/all/0/1\">Jonathan Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. (arXiv:2110.07298v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07298","description":"<p>Existing approaches to lifelong language learning rely on plenty of labeled\ndata for learning a new task, which is hard to obtain in most real scenarios.\nConsidering that humans can continually learn new tasks from a handful of\nexamples, we expect the models also to be able to generalize well on new\nfew-shot tasks without forgetting the previous ones. In this work, we define\nthis more challenging yet practical problem as Lifelong Few-shot Language\nLearning (LFLL) and propose a unified framework for it based on prompt tuning\nof T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot\nlearning ability, and simultaneously trains the model as a task solver and a\ndata generator. Before learning a new domain of the same task type, LFPT5\ngenerates pseudo (labeled) samples of previously learned domains, and later\ngets trained on those samples to alleviate forgetting of previous knowledge as\nit learns the new domain. In addition, a KL divergence loss is minimized to\nachieve label consistency between the previous and the current model. While\nadapting to a new task type, LFPT5 includes and tunes additional prompt\nembeddings for the new task. With extensive experiments, we demonstrate that\nLFPT5 can be applied to various different types of tasks and significantly\noutperform previous methods in different LFLL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BI-RADS BERT & Using Section Segmentation to Understand Radiology Reports. (arXiv:2110.07552v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07552","description":"<p>Radiology reports are one of the main forms of communication between\nradiologists and other clinicians and contain important information for patient\ncare. In order to use this information for research and automated patient care\nprograms, it is necessary to convert the raw text into structured data suitable\nfor analysis. State-of-the-art natural language processing (NLP)\ndomain-specific contextual word embeddings have been shown to achieve\nimpressive accuracy for these tasks in medicine, but have yet to be utilized\nfor section structure segmentation. In this work, we pre-trained a contextual\nembedding BERT model using breast radiology reports and developed a classifier\nthat incorporated the embedding with auxiliary global textual features in order\nto perform section segmentation. This model achieved a 98% accuracy at\nsegregating free text reports sentence by sentence into sections of information\noutlined in the Breast Imaging Reporting and Data System (BI-RADS) lexicon, a\nsignificant improvement over the Classic BERT model without auxiliary\ninformation. We then evaluated whether using section segmentation improved the\ndownstream extraction of clinically relevant information such as\nmodality/procedure, previous cancer, menopausal status, the purpose of the\nexam, breast density, and breast MRI background parenchymal enhancement. Using\nthe BERT model pre-trained on breast radiology reports combined with section\nsegmentation resulted in an overall accuracy of 95.9% in the field extraction\ntasks. This is a 17% improvement compared to an overall accuracy of 78.9% for\nfield extraction with models using Classic BERT embeddings and not using\nsection segmentation. Our work shows the strength of using BERT in radiology\nreport analysis and the advantages of section segmentation in identifying key\nfeatures of patient factors recorded in breast radiology reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuling_G/0/1/0/all/0/1\">Grey Kuling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curpen_D/0/1/0/all/0/1\">Dr. Belinda Curpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER. (arXiv:2110.08454v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08454","description":"<p>Recent advances in prompt-based learning have shown strong results on\nfew-shot text classification by using cloze-style templates. Similar attempts\nhave been made on named entity recognition (NER) which manually design\ntemplates to predict entity types for every text span in a sentence. However,\nsuch methods may suffer from error propagation induced by entity span\ndetection, high cost due to enumeration of all possible text spans, and\nomission of inter-dependencies among token labels in a sentence. Here we\npresent a simple demonstration-based learning method for NER, which lets the\ninput be prefaced by task demonstrations for in-context learning. We perform a\nsystematic study on demonstration strategy regarding what to include (entity\nexamples, with or without surrounding context), how to select the examples, and\nwhat templates to use. Results on in-domain learning and domain adaptation show\nthat the model's performance in low-resource settings can be largely improved\nwith a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train\ninstances). We also find that good demonstration can save many labeled examples\nand consistency in demonstration contributes to better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadakia_A/0/1/0/all/0/1\">Akshen Kadakia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kangmin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mahak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibuya_T/0/1/0/all/0/1\">Takashi Shibuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitani_R/0/1/0/all/0/1\">Ryosuke Mitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekiya_T/0/1/0/all/0/1\">Toshiyuki Sekiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum Description Length Recurrent Neural Networks. (arXiv:2111.00600v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00600","description":"<p>We train neural networks to optimize a Minimum Description Length score,\ni.e., to balance between the complexity of the network and its accuracy at a\ntask. We show that networks optimizing this objective function master tasks\ninvolving memory challenges and go beyond context-free languages. These\nlearners master languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^{2n}$,\n$a^nb^mc^{n+m}$, and they perform addition. Moreover, they often do so with\n100% accuracy. The networks are small, and their inner workings are\ntransparent. We thus provide formal proofs that their perfect accuracy holds\nnot only on a given test set, but for any input sequence. To our knowledge, no\nother connectionist model has been shown to capture the underlying grammars for\nthese languages in full generality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_N/0/1/0/all/0/1\">Nur Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyer_M/0/1/0/all/0/1\">Michal Geyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemla_E/0/1/0/all/0/1\">Emmanuel Chemla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzir_R/0/1/0/all/0/1\">Roni Katzir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Transduction with Graph-based Supervision. (arXiv:2111.01272v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01272","description":"<p>The recurrent neural network transducer (RNN-T) objective plays a major role\nin building today's best automatic speech recognition (ASR) systems for\nproduction. Similarly to the connectionist temporal classification (CTC)\nobjective, the RNN-T loss uses specific rules that define how a set of\nalignments is generated to form a lattice for the full-sum training. However,\nit is yet largely unknown if these rules are optimal and do lead to the best\npossible ASR results. In this work, we present a new transducer objective\nfunction that generalizes the RNN-T loss to accept a graph representation of\nthe labels, thus providing a flexible and efficient framework to manipulate\ntraining lattices, e.g., for studying different transition rules, implementing\ndifferent transducer losses, or restricting alignments. We demonstrate that\ntransducer-based ASR with CTC-like lattice achieves better results compared to\nstandard RNN-T, while also ensuring a strictly monotonic alignment, which will\nallow better optimization of the decoding procedure. For example, the proposed\nCTC-like transducer achieves an improvement of 4.8% on the test-other condition\nof LibriSpeech relative to an equivalent RNN-T based system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moritz_N/0/1/0/all/0/1\">Niko Moritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic. (arXiv:2111.14447v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14447","description":"<p>Recent text-to-image matching models apply contrastive learning to large\ncorpora of uncurated pairs of images and sentences. While such models can\nprovide a powerful score for matching and subsequent zero-shot tasks, they are\nnot capable of generating caption given an image. In this work, we repurpose\nsuch models to generate a descriptive text given an image at inference time,\nwithout any further training or tuning steps. This is done by combining the\nvisual-semantic model with a large language model, benefiting from the\nknowledge in both web-scale models. The resulting captions are much less\nrestrictive than those obtained by supervised captioning methods. Moreover, as\na zero-shot learning method, it is extremely flexible and we demonstrate its\nability to perform image arithmetic in which the inputs can be either images or\ntext, and the output is a sentence. This enables novel high-level vision\ncapabilities such as comparing two images or solving visual analogy tests. Our\ncode is available at: https://github.com/YoadTew/zero-shot-image-to-text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tewel_Y/0/1/0/all/0/1\">Yoad Tewel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Y/0/1/0/all/0/1\">Yoav Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make It Move: Controllable Image-to-Video Generation with Text Descriptions. (arXiv:2112.02815v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02815","description":"<p>Generating controllable videos conforming to user intentions is an appealing\nyet challenging topic in computer vision. To enable maneuverable control in\nline with user intentions, a novel video generation task, named\nText-Image-to-Video generation (TI2V), is proposed. With both controllable\nappearance and motion, TI2V aims at generating videos from a static image and a\ntext description. The key challenges of TI2V task lie both in aligning\nappearance and motion from different modalities, and in handling uncertainty in\ntext descriptions. To address these challenges, we propose a Motion\nAnchor-based video GEnerator (MAGE) with an innovative motion anchor (MA)\nstructure to store appearance-motion aligned representation. To model the\nuncertainty and increase the diversity, it further allows the injection of\nexplicit condition and implicit randomness. Through three-dimensional axial\ntransformers, MA is interacted with given image to generate next frames\nrecursively with satisfying controllability and diversity. Accompanying the new\ntask, we build two new video-text paired datasets based on MNIST and CATER for\nevaluation. Experiments conducted on these datasets verify the effectiveness of\nMAGE and show appealing potentials of TI2V task. Source code for model and\ndatasets will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaosi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Semantic Concepts into End-to-End Image Captioning. (arXiv:2112.05230v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05230","description":"<p>Tremendous progress has been made in recent years in developing better image\ncaptioning models, yet most of them rely on a separate object detector to\nextract regional features. Recent vision-language studies are shifting towards\nthe detector-free trend by leveraging grid representations for more flexible\nmodel training and faster inference speed. However, such development is\nprimarily focused on image understanding tasks, and remains less investigated\nfor the caption generation task. In this paper, we are concerned with a\nbetter-performing detector-free image captioning model, and propose a pure\nvision transformer-based image captioning model, dubbed as ViTCAP, in which\ngrid representations are used without extracting the regional features. For\nimproved performance, we introduce a novel Concept Token Network (CTN) to\npredict the semantic concepts and then incorporate them into the end-to-end\ncaptioning. In particular, the CTN is built on the basis of a vision\ntransformer and is designed to predict the concept tokens through a\nclassification task, from which the rich semantic information contained greatly\nbenefits the captioning task. Compared with the previous detector-based models,\nViTCAP drastically simplifies the architectures and at the same time achieves\ncompetitive performance on various challenging image captioning datasets. In\nparticular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split,\n93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhiyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Abstractive Summarization with Adaptive Length Controlling. (arXiv:2112.07534v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07534","description":"<p>Document summarization, as a fundamental task in natural language generation,\naims to generate a short and coherent summary for a given document.\nControllable summarization, especially of the length, is an important issue for\nsome practical applications, especially how to trade-off the length constraint\nand information integrity. In this paper, we propose an \\textbf{A}daptive\n\\textbf{L}ength \\textbf{C}ontrolling \\textbf{O}ptimization (\\textbf{ALCO})\nmethod to leverage two-stage abstractive summarization model via reinforcement\nlearning. ALCO incorporates length constraint into the stage of sentence\nextraction to penalize the overlength extracted sentences. Meanwhile, a\nsaliency estimation mechanism is designed to preserve the salient information\nin the generated sentences. A series of experiments have been conducted on a\nwildly-used benchmark dataset \\textit{CNN/Daily Mail}. The results have shown\nthat ALCO performs better than the popular baselines in terms of length\ncontrollability and content preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcing Semantic-Symmetry for Document Summarization. (arXiv:2112.07583v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07583","description":"<p>Document summarization condenses a long document into a short version with\nsalient information and accurate semantic descriptions. The main issue is how\nto make the output summary semantically consistent with the input document. To\nreach this goal, recently, researchers have focused on supervised end-to-end\nhybrid approaches, which contain an extractor module and abstractor module.\nAmong them, the extractor identifies the salient sentences from the input\ndocument, and the abstractor generates a summary from the salient sentences.\nThis model successfully keeps the consistency between the generated summary and\nthe reference summary via various strategies (e.g., reinforcement learning).\nThere are two semantic gaps when training the hybrid model (one is between\ndocument and extracted sentences, and the other is between extracted sentences\nand summary). However, they are not explicitly considered in the existing\nmethods, which usually results in a semantic bias of summary. To mitigate the\nabove issue, in this paper, a new \\textbf{r}einforcing\ns\\textbf{e}mantic-\\textbf{sy}mmetry learning \\textbf{m}odel is proposed for\ndocument summarization (\\textbf{ReSyM}). ReSyM introduces a\nsemantic-consistency reward in the extractor to bridge the first gap. A\nsemantic dual-reward is designed to bridge the second gap in the abstractor.\nThe whole document summarization process is implemented via reinforcement\nlearning with a hybrid reward mechanism (combining the above two rewards).\nMoreover, a comprehensive sentence representation learning method is presented\nto sufficiently capture the information from the original document. A series of\nexperiments have been conducted on two wildly used benchmark datasets CNN/Daily\nMail and BigPatent. The results have shown the superiority of ReSyM by\ncomparing it with the state-of-the-art baselines in terms of various evaluation\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Aware Encoding for Extractive Summarization. (arXiv:2112.09572v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09572","description":"<p>Document summarization provides an instrument for faster understanding the\ncollection of text documents and has several real-life applications. With the\ngrowth of online text data, numerous summarization models have been proposed\nrecently. The Sequence-to-Sequence (Seq2Seq) based neural summarization model\nis the most widely used in the summarization field due to its high performance.\nThis is because semantic information and structure information in the text is\nadequately considered when encoding. However, the existing extractive\nsummarization models pay little attention to and use the central topic\ninformation to assist the generation of summaries, which leads to models not\nensuring the generated summary under the primary topic. A lengthy document can\nspan several topics, and a single summary cannot do justice to all the topics.\nTherefore, the key to generating a high-quality summary is determining the\ncentral topic and building a summary based on it, especially for a long\ndocument. We propose a topic-aware encoding for document summarization to deal\nwith this issue. This model effectively combines syntactic-level and\ntopic-level information to build a comprehensive sentence representation.\nSpecifically, a neural topic model is added in the neural-based sentence-level\nrepresentation learning to adequately consider the central topic information\nfor capturing the critical content in the original document. The experimental\nresults on three public datasets show that our model outperforms the\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02732","description":"<p>Conversational recommender systems (CRS) aim to recommend suitable items to\nusers through natural language conversations. For developing effective CRSs, a\nmajor technical issue is how to accurately infer user preference from very\nlimited conversation context. To address issue, a promising solution is to\nincorporate external data for enriching the context information. However, prior\nstudies mainly focus on designing fusion models tailored for some specific type\nof external data, which is not general to model and utilize multi-type external\ndata.\n</p>\n<p>To effectively leverage multi-type external data, we propose a novel\ncoarse-to-fine contrastive learning framework to improve data semantic fusion\nfor CRS. In our approach, we first extract and represent multi-grained semantic\nunits from different data signals, and then align the associated multi-type\nsemantic units in a coarse-to-fine way. To implement this framework, we design\nboth coarse-grained and fine-grained procedures for modeling user preference,\nwhere the former focuses on more general, coarse-grained semantic fusion and\nthe latter focuses on more specific, fine-grained semantic fusion. Such an\napproach can be extended to incorporate more kinds of external data. Extensive\nexperiments on two public CRS datasets have demonstrated the effectiveness of\nour approach in both recommendation and conversation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">He Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Learning for Short Text Classification. (arXiv:2202.11345v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.11345","description":"<p>In the short text, the extremely short length, feature sparsity, and high\nambiguity pose huge challenges to classification tasks. Recently, as an\neffective method for tuning Pre-trained Language Models for specific downstream\ntasks, prompt-learning has attracted a vast amount of attention and research.\nThe main intuition behind the prompt-learning is to insert the template into\nthe input and convert the text classification tasks into equivalent cloze-style\ntasks. However, most prompt-learning methods expand label words manually or\nonly consider the class name for knowledge incorporating in cloze-style\nprediction, which will inevitably incur omissions and bias in short text\nclassification tasks. In this paper, we propose a simple short text\nclassification approach that makes use of prompt-learning based on\nknowledgeable expansion. Taking the special characteristics of short text into\nconsideration, the method can consider both the short text itself and class\nname during expanding label words space. Specifically, the top $N$ concepts\nrelated to the entity in the short text are retrieved from the open Knowledge\nGraph like Probase, and we further refine the expanded label words by the\ndistance calculation between selected concepts and class labels. Experimental\nresults show that our approach obtains obvious improvement compared with other\nfine-tuning, prompt-learning, and knowledgeable prompt-tuning methods,\noutperforming the state-of-the-art by up to 6 Accuracy points on three\nwell-known datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinke Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xindong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaNames: A Massively Multilingual Entity Name Corpus. (arXiv:2202.14035v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.14035","description":"<p>This preprint describes work in progress on ParaNames, a multilingual\nparallel name resource consisting of names for approximately 14 million\nentities. The included names span over 400 languages, and almost all entities\nare mapped to standardized entity types (PER/LOC/ORG). Using Wikidata as a\nsource, we create the largest resource of this type to-date. We describe our\napproach to filtering and standardizing the data to provide the best quality\npossible. ParaNames is useful for multilingual language processing, both in\ndefining tasks for name translation/transliteration and as supplementary data\nfor tasks such as named entity recognition and linking. We demonstrate an\napplication of ParaNames by training a multilingual model for canonical name\ntranslation to and from English. Our resource is released at\n\\url{https://github.com/bltlab/paranames} under a Creative Commons license (CC\nBY 4.0).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleva_J/0/1/0/all/0/1\">Jonne S&#xe4;lev&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01322","description":"<p>Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1\">Venkat Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding. (arXiv:2203.01515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01515","description":"<p>Automatic ICD coding is defined as assigning disease codes to electronic\nmedical records (EMRs). Existing methods usually apply label attention with\ncode representations to match related text snippets. Unlike these works that\nmodel the label with the code hierarchy or description, we argue that the code\nsynonyms can provide more comprehensive knowledge based on the observation that\nthe code expressions in EMRs vary from their descriptions in ICD. By aligning\ncodes to concepts in UMLS, we collect synonyms of every code. Then, we propose\na multiple synonyms matching network to leverage synonyms for better code\nrepresentation learning, and finally help the code classification. Experiments\non the MIMIC-III dataset show that our proposed method outperforms previous\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History. (arXiv:2203.02459v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02459","description":"<p>Simultaneous Machine Translation is the task of incrementally translating an\ninput sentence before it is fully available. Currently, simultaneous\ntranslation is carried out by translating each sentence independently of the\npreviously translated text. More generally, Streaming MT can be understood as\nan extension of Simultaneous MT to the incremental translation of a continuous\ninput text stream. In this work, a state-of-the-art simultaneous sentence-level\nMT system is extended to the streaming setup by leveraging the streaming\nhistory. Extensive empirical results are reported on IWSLT Translation Tasks,\nshowing that leveraging the streaming history leads to significant quality\ngains. In particular, the proposed system proves to compare favorably to the\nbest performing systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Javier Iranzo-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Jorge Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_A/0/1/0/all/0/1\">Alfons Juan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking. (arXiv:2203.03123v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03123","description":"<p>Dialogue state tracking (DST) aims to extract essential information from\nmulti-turn dialogue situations and take appropriate actions. A belief state,\none of the core pieces of information, refers to the subject and its specific\ncontent, and appears in the form of domain-slot-value. The trained model\npredicts \"accumulated\" belief states in every turn, and joint goal accuracy and\nslot accuracy are mainly used to evaluate the prediction; however, we specify\nthat the current evaluation metrics have a critical limitation when evaluating\nbelief states accumulated as the dialogue proceeds, especially in the most used\nMultiWOZ dataset. Additionally, we propose relative slot accuracy to complement\nexisting metrics. Relative slot accuracy does not depend on the number of\npredefined slots, and allows intuitive evaluation by assigning relative scores\naccording to the turn of each dialogue. This study also encourages not solely\nthe reporting of joint goal accuracy, but also various complementary metrics in\nDST tasks for the sake of a realistic evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Takyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hoonsang Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yukyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Misuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05598","description":"<p>The study of the applicability of the BERTScore metric was conducted to\ntranslation quality assessment at the sentence level for English -&gt; Russian\ndirection. Experiments were performed with a pre-trained Multilingual BERT as\nwell as with a pair of Monolingual BERT models. To align monolingual\nembeddings, an orthogonal transformation based on anchor tokens was used. It\nwas demonstrated that such transformation helps to prevent mismatching issue\nand shown that this approach gives better results than using embeddings of the\nMultilingual model. To improve the token matching process it is proposed to\ncombine all incomplete WorkPiece tokens into meaningful words and use simple\naveraging of corresponding vectors and to calculate BERTScore based on anchor\ntokens only. Such modifications allowed us to achieve a better correlation of\nthe model predictions with human judgments. In addition to evaluating machine\ntranslation, several versions of human translation were evaluated as well, the\nproblems of this approach were listed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_A/0/1/0/all/0/1\">A.A. Vetrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorn_E/0/1/0/all/0/1\">E.A. Gorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Pre-training for AMR Parsing and Generation. (arXiv:2203.07836v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07836","description":"<p>Abstract meaning representation (AMR) highlights the core semantic\ninformation of text in a graph structure. Recently, pre-trained language models\n(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,\nrespectively. However, PLMs are typically pre-trained on textual data, thus are\nsub-optimal for modeling structural knowledge. To this end, we investigate\ngraph self-supervised training to improve the structure awareness of PLMs over\nAMR graphs. In particular, we introduce two graph auto-encoding strategies for\ngraph-to-graph pre-training and four tasks to integrate text and graph\ninformation during pre-training. We further design a unified framework to\nbridge the gap between pre-training and fine-tuning tasks. Experiments on both\nAMR parsing and AMR-to-text generation show the superiority of our model. To\nour knowledge, we are the first to consider pre-training on semantic graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Converse: A Tree-Based Modular Task-Oriented Dialogue System. (arXiv:2203.12187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12187","description":"<p>Creating a system that can have meaningful conversations with humans to help\naccomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).\nIt has defined the meaning of AI since the beginning. A lot has been\naccomplished in this area recently, with voice assistant products entering our\ndaily lives and chat bot systems becoming commonplace in customer service. At\nfirst glance there seems to be no shortage of options for dialogue systems.\nHowever, the frequently deployed dialogue systems today seem to all struggle\nwith a critical weakness - they are hard to build and harder to maintain. At\nthe core of the struggle is the need to script every single turn of\ninteractions between the bot and the human user. This makes the dialogue\nsystems more difficult to maintain as the tasks become more complex and more\ntasks are added to the system. In this paper, we propose Converse, a flexible\ntree-based modular task-oriented dialogue system. Converse uses an and-or tree\nstructure to represent tasks and offers powerful multi-task dialogue\nmanagement. Converse supports task dependency and task switching, which are\nunique features compared to other open-source dialogue frameworks. At the same\ntime, Converse aims to make the bot building process easy and simple, for both\nprofessional and non-professional software developers. The code is available at\nhttps://github.com/salesforce/Converse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Angela S. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Young Mo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1\">Richard Socher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2203.13366","description":"<p>For a long period, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language grounding is a powerful medium to describe and\nrepresent various problems or tasks, we present a flexible and unified\ntext-to-text paradigm called \"Pretrain, Personalized Prompt, and Predict\nParadigm\" (P5) for recommendation, which unifies various recommendation tasks\nin a shared framework. In P5, all data such as user-item interactions, item\nmetadata, and user reviews are converted to a common format -- natural language\nsequences. The rich information from natural language assist P5 to capture\ndeeper semantics for recommendation. P5 learns different tasks with the same\nlanguage modeling objective during pretraining. Thus, it possesses the\npotential to serve as the foundation model for downstream recommendation tasks,\nallows easy integration with other modalities, and enables instruction-based\nrecommendation, which will revolutionize the technical form of recommender\nsystem towards universal recommendation engine. With adaptive personalized\nprompt for different users, P5 is able to make predictions in a zero-shot or\nfew-shot manner and largely reduces the necessity for extensive fine-tuning. On\nseveral recommendation benchmarks, we conduct experiments to show the\neffectiveness of our generative approach. We will release our prompts and\npretrained P5 language model to help advance future research on Recommendation\nas Language Processing (RLP) and Personalized Foundation Models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation. (arXiv:2203.13560v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13560","description":"<p>Applying existing methods to emotional support conversation -- which provides\nvaluable assistance to people who are in need -- has two major limitations: (a)\nthey generally employ a conversation-level emotion label, which is too\ncoarse-grained to capture user's instant mental state; (b) most of them focus\non expressing empathy in the response(s) rather than gradually reducing user's\ndistress. To address the problems, we propose a novel model \\textbf{MISC},\nwhich firstly infers the user's fine-grained emotional status, and then\nresponds skillfully using a mixture of strategy. Experimental results on the\nbenchmark dataset demonstrate the effectiveness of our method and reveal the\nbenefits of fine-grained emotion understanding as well as mixed-up strategy\nmodeling. Our code and data could be found in\n\\url{https://github.com/morecry/MISC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Q/0/1/0/all/0/1\">Quan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16487","description":"<p>In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel\napproach to accelerating autoregressive translation with no quality loss,\nthrough the collaboration of autoregressive and non-autoregressive translation\n(NAT) of the Transformer. At each decoding iteration, GAD aggressively decodes\na number of tokens in parallel as a draft through NAT and then verifies them in\nthe autoregressive manner, where only the tokens that pass the verification are\nkept as decoded tokens. GAD can achieve the same performance as autoregressive\ntranslation but much more efficiently because both NAT drafting and\nautoregressive verification are fast due to parallel computing. We conduct\nexperiments in the WMT14 English-German translation task and confirm that the\nvanilla GAD yields exactly the same results as greedy decoding with an around\n3x speedup, and that its variant (GAD++) with an advanced verification strategy\nnot only outperforms the greedy translation and even achieves the comparable\ntranslation quality with the beam search result, but also further improves the\ndecoding speed, resulting in an around 5x speedup over autoregressive\ntranslation. Our models and codes are available at\nhttps://github.com/hemingkx/Generalized-Aggressive-Decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Heming Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese Codex. (arXiv:2203.15173v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2203.15173","description":"<p>Word embedding is a modern distributed word representations approach widely\nused in many natural language processing tasks. Converting the vocabulary in a\nlegal document into a word embedding model facilitates subjecting legal\ndocuments to machine learning, deep learning, and other algorithms and\nsubsequently performing the downstream tasks of natural language processing\nvis-\\`a-vis, for instance, document classification, contract review, and\nmachine translation. The most common and practical approach of accuracy\nevaluation with the word embedding model uses a benchmark set with linguistic\nrules or the relationship between words to perform analogy reasoning via\nalgebraic calculation. This paper proposes establishing a 1,134 Legal\nAnalogical Reasoning Questions Set (LARQS) from the 2,388 Chinese Codex corpus\nusing five kinds of legal relations, which are then used to evaluate the\naccuracy of the Chinese word embedding model. Moreover, we discovered that\nlegal relations might be ubiquitous in the word embedding model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chun-Hsien Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pu-Jen Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v1 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2203.15937","description":"<p>Current leading mispronunciation detection and diagnosis (MDD) systems\nachieve promising performance via end-to-end phoneme recognition. One challenge\nof such end-to-end solutions is the scarcity of human-annotated phonemes on\nnatural L2 speech. In this work, we leverage unlabeled L2 speech via a\npseudo-labeling (PL) procedure and extend the fine-tuning approach based on\npre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec\n2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples\nplus the created pseudo-labeled L2 speech samples. Our pseudo labels are\ndynamic and are produced by an ensemble of the online model on-the-fly, which\nensures that our model is robust to pseudo label noise. We show that\nfine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and\n2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning\nbaseline. The proposed PL method is also shown to outperform conventional\noffline PL methods. Compared to the state-of-the-art MDD systems, our MDD\nsolution achieves a more accurate and consistent phonetic error diagnosis. In\naddition, we conduct an open test on a separate UTD-4Accents dataset, where our\nsystem recognition outputs show a strong correlation with human perception,\nbased on accentedness and intelligibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirschi_K/0/1/0/all/0/1\">Kevin Hirschi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Looney_S/0/1/0/all/0/1\">Stephen D. Looney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_O/0/1/0/all/0/1\">Okim Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_J/0/1/0/all/0/1\">John H. L. Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards Multimodal Depth Estimation from Light Fields. (arXiv:2203.16542v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16542","description":"<p>Light field applications, especially light field rendering and depth\nestimation, developed rapidly in recent years. While state-of-the-art light\nfield rendering methods handle semi-transparent and reflective objects well,\ndepth estimation methods either ignore these cases altogether or only deliver a\nweak performance. We argue that this is due current methods only considering a\nsingle \"true\" depth, even when multiple objects at different depths contributed\nto the color of a single pixel. Based on the simple idea of outputting a\nposterior depth distribution instead of only a single estimate, we develop and\nexplore several different deep-learning-based approaches to the problem.\nAdditionally, we contribute the first \"multimodal light field depth dataset\"\nthat contains the depths of all objects which contribute to the color of a\npixel. This allows us to supervise the multimodal depth prediction and also\nvalidate all methods by measuring the KL divergence of the predicted\nposteriors. With our thorough analysis and novel dataset, we aim to start a new\nline of depth estimation research that overcomes some of the long-standing\nlimitations of this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leistner_T/0/1/0/all/0/1\">Titus Leistner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackowiak_R/0/1/0/all/0/1\">Radek Mackowiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardizzone_L/0/1/0/all/0/1\">Lynton Ardizzone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1\">Ullrich K&#xf6;the</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COSMOS: Cross-Modality Unsupervised Domain Adaptation for 3D Medical Image Segmentation based on Target-aware Domain Translation and Iterative Self-Training. (arXiv:2203.16557v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16557","description":"<p>Recent advances in deep learning-based medical image segmentation studies\nachieve nearly human-level performance when in fully supervised condition.\nHowever, acquiring pixel-level expert annotations is extremely expensive and\nlaborious in medical imaging fields. Unsupervised domain adaptation can\nalleviate this problem, which makes it possible to use annotated data in one\nimaging modality to train a network that can successfully perform segmentation\non target imaging modality with no labels. In this work, we propose a\nself-training based unsupervised domain adaptation framework for 3D medical\nimage segmentation named COSMOS and validate it with automatic segmentation of\nVestibular Schwannoma (VS) and cochlea on high-resolution T2 Magnetic Resonance\nImages (MRI). Our target-aware contrast conversion network translates source\ndomain annotated T1 MRI to pseudo T2 MRI to enable segmentation training on\ntarget domain, while preserving important anatomical features of interest in\nthe converted images. Iterative self-training is followed to incorporate\nunlabeled data to training and incrementally improve the quality of\npseudo-labels, thereby leading to improved performance of segmentation. COSMOS\nwon the 1\\textsuperscript{st} place in the Cross-Modality Domain Adaptation\n(crossMoDA) challenge held in conjunction with the 24th International\nConference on Medical Image Computing and Computer Assisted Intervention\n(MICCAI 2021). It achieves mean Dice score and Average Symmetric Surface\nDistance of 0.871(0.063) and 0.437(0.270) for VS, and 0.842(0.020) and\n0.152(0.030) for cochlea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shin_H/0/1/0/all/0/1\">Hyungseob Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hyeongyu Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sewon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jun_Y/0/1/0/all/0/1\">Yohan Jun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eo_T/0/1/0/all/0/1\">Taejoon Eo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dosik Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Cycle-Consistent Learning for Instruction Following and Generation in Vision-Language Navigation. (arXiv:2203.16586v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16586","description":"<p>Since the rise of vision-language navigation (VLN), great progress has been\nmade in instruction following -- building a follower to navigate environments\nunder the guidance of instructions. However, far less attention has been paid\nto the inverse task: instruction generation -- learning a speaker~to generate\ngrounded descriptions for navigation routes. Existing VLN methods train a\nspeaker independently and often treat it as a data augmentation tool to\nstrengthen the follower while ignoring rich cross-task relations. Here we\ndescribe an approach that learns the two tasks simultaneously and exploits\ntheir intrinsic correlations to boost the training of each: the follower judges\nwhether the speaker-created instruction explains the original navigation route\ncorrectly, and vice versa. Without the need of aligned instruction-path pairs,\nsuch cycle-consistent learning scheme is complementary to task-specific\ntraining targets defined on labeled data, and can also be applied over\nunlabeled paths (sampled without paired instructions). Another agent,\ncalled~creator is added to generate counterfactual environments. It greatly\nchanges current scenes yet leaves novel items -- which are vital for the\nexecution of original instructions -- unchanged. Thus more informative training\nscenes are synthesized and the three agents compose a powerful VLN learning\nsystem. Extensive experiments on a standard benchmark show that our approach\nimproves the performance of various follower models and produces accurate\nnavigation instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Wei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Few-shot Class-incremental Learning. (arXiv:2203.16588v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16588","description":"<p>Continually learning new classes from fresh data without forgetting previous\nknowledge of old classes is a very challenging research problem. Moreover, it\nis imperative that such learning must respect certain memory and computational\nconstraints such as (i) training samples are limited to only a few per class,\n(ii) the computational cost of learning a novel class remains constant, and\n(iii) the memory footprint of the model grows at most linearly with the number\nof classes observed. To meet the above constraints, we propose C-FSCIL, which\nis architecturally composed of a frozen meta-learned feature extractor, a\ntrainable fixed-size fully connected layer, and a rewritable dynamically\ngrowing memory that stores as many vectors as the number of encountered\nclasses. C-FSCIL provides three update modes that offer a trade-off between\naccuracy and compute-memory cost of learning novel classes. C-FSCIL exploits\nhyperdimensional embedding that allows to continually express many more classes\nthan the fixed dimensions in the vector space, with minimal interference. The\nquality of class vector representations is further improved by aligning them\nquasi-orthogonally to each other by means of novel loss functions. Experiments\non the CIFAR100, miniImageNet, and Omniglot datasets show that C-FSCIL\noutperforms the baselines with remarkable accuracy and compression. It also\nscales up to the largest problem size ever tried in this few-shot setting by\nlearning 423 novel classes on top of 1200 base classes with less than 1.6%\naccuracy drop. Our code is available at\nhttps://github.com/IBM/constrained-FSCIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hersche_M/0/1/0/all/0/1\">Michael Hersche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karunaratne_G/0/1/0/all/0/1\">Geethan Karunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherubini_G/0/1/0/all/0/1\">Giovanni Cherubini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1\">Abu Sebastian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Abbas Rahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Local Displacements for Point Cloud Completion. (arXiv:2203.16600v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16600","description":"<p>We propose a novel approach aimed at object and semantic scene completion\nfrom a partial scan represented as a 3D point cloud. Our architecture relies on\nthree novel layers that are used successively within an encoder-decoder\nstructure and specifically developed for the task at hand. The first one\ncarries out feature extraction by matching the point features to a set of\npre-trained local descriptors. Then, to avoid losing individual descriptors as\npart of standard operations such as max-pooling, we propose an alternative\nneighbor-pooling operation that relies on adopting the feature vectors with the\nhighest activations. Finally, up-sampling in the decoder modifies our feature\nextraction in order to increase the output dimension. While this model is\nalready able to achieve competitive results with the state of the art, we\nfurther propose a way to increase the versatility of our approach to process\npoint clouds. To this aim, we introduce a second model that assembles our\nlayers within a transformer architecture. We evaluate both architectures on\nobject and indoor scene completion tasks, achieving state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1\">David Joseph Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Cancer Prediction in Challenging Screen-Detected Incident Lung Nodules Using Time-Series Deep Learning. (arXiv:2203.16606v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16606","description":"<p>Lung cancer is the leading cause of cancer-related mortality worldwide. Lung\ncancer screening (LCS) using annual low-dose computed tomography (CT) scanning\nhas been proven to significantly reduce lung cancer mortality by detecting\ncancerous lung nodules at an earlier stage. Improving risk stratification of\nmalignancy risk in lung nodules can be enhanced using machine/deep learning\nalgorithms. However most existing algorithms: a) have primarily assessed single\ntime-point CT data alone thereby failing to utilize the inherent advantages\ncontained within longitudinal imaging datasets; b) have not integrated into\ncomputer models pertinent clinical data that might inform risk prediction; c)\nhave not assessed algorithm performance on the spectrum of nodules that are\nmost challenging for radiologists to interpret and where assistance from\nanalytic tools would be most beneficial.\n</p>\n<p>Here we show the performance of our time-series deep learning model\n(DeepCAD-NLM-L) which integrates multi-model information across three\nlongitudinal data domains: nodule-specific, lung-specific, and clinical\ndemographic data. We compared our time-series deep learning model to a)\nradiologist performance on CTs from the National Lung Screening Trial enriched\nwith the most challenging nodules for diagnosis; b) a nodule management\nalgorithm from a North London LCS study (SUMMIT). Our model demonstrated\ncomparable and complementary performance to radiologists when interpreting\nchallenging lung nodules and showed improved performance (AUC=88\\%) against\nmodels utilizing single time-point data only. The results emphasise the\nimportance of time-series, multi-modal analysis when interpreting malignancy\nrisk in LCS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aslani_S/0/1/0/all/0/1\">Shahab Aslani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alluri_P/0/1/0/all/0/1\">Pavan Alluri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gudmundsson_E/0/1/0/all/0/1\">Eyjolfur Gudmundsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandy_E/0/1/0/all/0/1\">Edward Chandy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McCabe_J/0/1/0/all/0/1\">John McCabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Devaraj_A/0/1/0/all/0/1\">Anand Devaraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horst_C/0/1/0/all/0/1\">Carolyn Horst</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Janes_S/0/1/0/all/0/1\">Sam M Janes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakkara_R/0/1/0/all/0/1\">Rahul Chakkara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nair_A/0/1/0/all/0/1\">Arjun Nair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+consortium_S/0/1/0/all/0/1\">SUMMIT consortium</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacob_J/0/1/0/all/0/1\">Joseph Jacob</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v1 [cs.AI])","link":"http://arxiv.org/abs/2203.16616","description":"<p>Knowledge-based entity prediction (KEP) is a novel task that aims to improve\nmachine perception in autonomous systems. KEP leverages relational knowledge\nfrom heterogeneous sources in predicting potentially unrecognized entities. In\nthis paper, we provide a formal definition of KEP as a knowledge completion\ntask. Three potential solutions are then introduced, which employ several\nmachine learning and data mining techniques. Finally, the applicability of KEP\nis demonstrated on two autonomous systems from different domains; namely,\nautonomous driving and smart manufacturing. We argue that in complex real-world\nsystems, the use of KEP would significantly improve machine perception while\npushing the current technology one step closer to achieving the full autonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wickramarachchi_R/0/1/0/all/0/1\">Ruwan Wickramarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henson_C/0/1/0/all/0/1\">Cory Henson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Document Recognition and Understanding with Dessurt. (arXiv:2203.16618v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16618","description":"<p>We introduce Dessurt, a relatively simple document understanding transformer\ncapable of being fine-tuned on a greater variety of document tasks than prior\nmethods. It receives a document image and task string as input and generates\narbitrary text autoregressively as output. Because Dessurt is an end-to-end\narchitecture that performs text recognition in addition to the document\nunderstanding, it does not require an external recognition model as prior\nmethods do, making it easier to fine-tune to new visual domains. We show that\nthis model is effective at 9 different dataset-task combinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1\">Brian Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morse_B/0/1/0/all/0/1\">Bryan Morse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Bryan Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tensmeyer_C/0/1/0/all/0/1\">Chris Tensmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wigington_C/0/1/0/all/0/1\">Curtis Wigington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad Morariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TR-MOT: Multi-Object Tracking by Reference. (arXiv:2203.16621v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16621","description":"<p>Multi-object Tracking (MOT) generally can be split into two sub-tasks, i.e.,\ndetection and association. Many previous methods follow the tracking by\ndetection paradigm, which first obtain detections at each frame and then\nassociate them between adjacent frames. Though with an impressive performance\nby utilizing a strong detector, it will degrade their detection and association\nperformance under scenes with many occlusions and large motion if not using\ntemporal information. In this paper, we propose a novel Reference Search (RS)\nmodule to provide a more reliable association based on the deformable\ntransformer structure, which is natural to learn the feature alignment for each\nobject among frames. RS takes previous detected results as references to\naggregate the corresponding features from the combined features of the adjacent\nframes and makes a one-to-one track state prediction for each reference in\nparallel. Therefore, RS can attain a reliable association coping with\nunexpected motions by leveraging visual temporal features while maintaining the\nstrong detection performance by decoupling from the detector. Our RS module can\nalso be compatible with the structure of the other tracking by detection\nframeworks. Furthermore, we propose a joint training strategy and an effective\nmatching pipeline for our online MOT framework with the RS module. Our method\nachieves competitive results on MOT17 and MOT20 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning for the Classification of Tumor Infiltrating Lymphocytes. (arXiv:2203.16622v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16622","description":"<p>We evaluate the performance of federated learning (FL) in developing deep\nlearning models for analysis of digitized tissue sections. A classification\napplication was considered as the example use case, on quantifiying the\ndistribution of tumor infiltrating lymphocytes within whole slide images\n(WSIs). A deep learning classification model was trained using 50*50 square\nmicron patches extracted from the WSIs. We simulated a FL environment in which\na dataset, generated from WSIs of cancer from numerous anatomical sites\navailable by The Cancer Genome Atlas repository, is partitioned in 8 different\nnodes. Our results show that the model trained with the federated training\napproach achieves similar performance, both quantitatively and qualitatively,\nto that of a model trained with all the training data pooled at a centralized\nlocation. Our study shows that FL has tremendous potential for enabling\ndevelopment of more robust and accurate models for histopathology image\nanalysis without having to collect large and diverse training data at a single\nlocation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pati_S/0/1/0/all/0/1\">Sarthak Pati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin M. Kurc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_R/0/1/0/all/0/1\">Rajarsi Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bremer_E/0/1/0/all/0/1\">Erich Bremer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abousamra_S/0/1/0/all/0/1\">Shahira Abousamra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thakur_S/0/1/0/all/0/1\">Siddhesh P. Thakur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saltz_J/0/1/0/all/0/1\">Joel H. Saltz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DDNeRF: Depth Distribution Neural Radiance Fields. (arXiv:2203.16626v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16626","description":"<p>In recent years, the field of implicit neural representation has progressed\nsignificantly. Models such as neural radiance fields (NeRF), which uses\nrelatively small neural networks, can represent high-quality scenes and achieve\nstate-of-the-art results for novel view synthesis. Training these types of\nnetworks, however, is still computationally very expensive. We present depth\ndistribution neural radiance field (DDNeRF), a new method that significantly\nincreases sampling efficiency along rays during training while achieving\nsuperior results for a given sampling budget. DDNeRF achieves this by learning\na more accurate representation of the density distribution along rays. More\nspecifically, we train a coarse model to predict the internal distribution of\nthe transparency of an input volume in addition to the volume's total density.\nThis finer distribution then guides the sampling procedure of the fine model.\nThis method allows us to use fewer samples during training while reducing\ncomputational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dadon_D/0/1/0/all/0/1\">David Dadon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hel_Or_Y/0/1/0/all/0/1\">Yacov Hel-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Augmentations for Video Representation Learning. (arXiv:2203.16632v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16632","description":"<p>This paper focuses on self-supervised video representation learning. Most\nexisting approaches follow the contrastive learning pipeline to construct\npositive and negative pairs by sampling different clips. However, this\nformulation tends to bias to static background and have difficulty establishing\nglobal temporal structures. The major reason is that the positive pairs, i.e.,\ndifferent clips sampled from the same video, have limited temporal receptive\nfield, and usually share similar background but differ in motions. To address\nthese problems, we propose a framework to jointly utilize local clips and\nglobal videos to learn from detailed region-level correspondence as well as\ngeneral long-term temporal relations. Based on a set of controllable\naugmentations, we achieve accurate appearance and motion pattern alignment\nthrough soft spatio-temporal region contrast. Our formulation is able to avoid\nthe low-level redundancy shortcut by mutual information minimization to improve\nthe generalization. We also introduce local-global temporal order dependency to\nfurther bridge the gap between clip-level and video-level representations for\nrobust temporal modeling. Extensive experiments demonstrate that our framework\nis superior on three video benchmarks in action recognition and video\nretrieval, capturing more accurate temporal dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations. (arXiv:2203.16639v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16639","description":"<p>We present a meta-learning framework for learning new visual concepts\nquickly, from just one or a few examples, guided by multiple naturally\noccurring data streams: simultaneously looking at images, reading sentences\nthat describe the objects in the scene, and interpreting supplemental sentences\nthat relate the novel concept with other concepts. The learned concepts support\ndownstream applications, such as answering questions by reasoning about unseen\nimages. Our model, namely FALCON, represents individual visual concepts, such\nas colors and shapes, as axis-aligned boxes in a high-dimensional space (the\n\"box embedding space\"). Given an input image and its paired sentence, our model\nfirst resolves the referential expression in the sentence and associates the\nnovel concept with particular objects in the scene. Next, our model interprets\nsupplemental sentences to relate the novel concept with other known concepts,\nsuch as \"X has property Y\" or \"X is a kind of Y\". Finally, it infers an optimal\nbox embedding for the novel concept that jointly 1) maximizes the likelihood of\nthe observed instances in the image, and 2) satisfies the relationships between\nthe novel concepts and the known ones. We demonstrate the effectiveness of our\nmodel on both synthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_L/0/1/0/all/0/1\">Lingjie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiayuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination. (arXiv:2203.16669v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16669","description":"<p>In Heterogeneous Face Recognition (HFR), the objective is to match faces\nacross two different domains such as visible and thermal. Large domain\ndiscrepancy makes HFR a difficult problem. Recent methods attempting to fill\nthe gap via synthesis have achieved promising results, but their performance is\nstill limited by the scarcity of paired training data. In practice, large-scale\nheterogeneous face data are often inaccessible due to the high cost of\nacquisition and annotation process as well as privacy regulations. In this\npaper, we propose a new face hallucination paradigm for HFR, which not only\nenables data-efficient synthesis but also allows to scale up model training\nwithout breaking any privacy policy. Unlike existing methods that learn face\nsynthesis entirely from scratch, our approach is particularly designed to take\nadvantage of rich and diverse facial priors from visible domain for more\nfaithful hallucination. On the other hand, large-scale training is enabled by\nintroducing a new federated learning scheme to allow institution-wise\ncollaborations while avoiding explicit data sharing. Extensive experiments\ndemonstrate the advantages of our approach in tackling HFR under current data\nlimitations. In a unified framework, our method yields the state-of-the-art\nhallucination results on multiple HFR datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1\">Yiqun Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition. (arXiv:2203.16670v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16670","description":"<p>Intrinsic image decomposition is the process of recovering the image\nformation components (reflectance and shading) from an image. Previous methods\nemploy either explicit priors to constrain the problem or implicit constraints\nas formulated by their losses (deep learning). These methods can be negatively\ninfluenced by strong illumination conditions causing shading-reflectance\nleakages.\n</p>\n<p>Therefore, in this paper, an end-to-end edge-driven hybrid CNN approach is\nproposed for intrinsic image decomposition. Edges correspond to illumination\ninvariant gradients. To handle hard negative illumination transitions, a\nhierarchical approach is taken including global and local refinement layers. We\nmake use of attention layers to further strengthen the learning process.\n</p>\n<p>An extensive ablation study and large scale experiments are conducted showing\nthat it is beneficial for edge-driven hybrid IID networks to make use of\nillumination invariant descriptors and that separating global and local cues\nhelps in improving the performance of the network. Finally, it is shown that\nthe proposed method obtains state of the art performance and is able to\ngeneralise well to real world images. The project page with pretrained models,\nfinetuned models and network code can be found at\nhttps://ivi.fnwi.uva.nl/cv/pienet/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Partha Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaoglu_S/0/1/0/all/0/1\">Sezer Karaoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1\">Theo Gevers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Spreader: Learning Facial Action Unit Dynamics with Extremely Limited Labels. (arXiv:2203.16678v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16678","description":"<p>Recent studies on the automatic detection of facial action unit (AU) have\nextensively relied on large-sized annotations. However, manually AU labeling is\ndifficult, time-consuming, and costly. Most existing semi-supervised works\nignore the informative cues from the temporal domain, and are highly dependent\non densely annotated videos, making the learning process less efficient. To\nalleviate these problems, we propose a deep semi-supervised framework\nKnowledge-Spreader (KS), which differs from conventional methods in two\naspects. First, rather than only encoding human knowledge as constraints, KS\nalso learns the Spatial-Temporal AU correlation knowledge in order to\nstrengthen its out-of-distribution generalization ability. Second, we approach\nKS by applying consistency regularization and pseudo-labeling in multiple\nstudent networks alternately and dynamically. It spreads the spatial knowledge\nfrom labeled frames to unlabeled data, and completes the temporal information\nof partially labeled video clips. Thus, the design allows KS to learn AU\ndynamics from video clips with only one label allocated, which significantly\nreduce the requirements of using annotations. Extensive experiments demonstrate\nthat the proposed KS achieves competitive performance as compared to the state\nof the arts under the circumstances of using only 2% labels on BP4D and 5%\nlabels on DISFA. In addition, we test it on our newly developed large-scale\ncomprehensive emotion database, which contains considerable samples across\nwell-synchronized and aligned sensor modalities for easing the scarcity issue\nof annotations and identities in human affective computing. The new database\nwill be released to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Taoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lijun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Effect of Registration Hyperparameters with HyperMorph. (arXiv:2203.16680v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16680","description":"<p>We introduce HyperMorph, a framework that facilitates efficient\nhyperparameter tuning in learning-based deformable image registration.\nClassical registration algorithms perform an iterative pair-wise optimization\nto compute a deformation field that aligns two images. Recent learning-based\napproaches leverage large image datasets to learn a function that rapidly\nestimates a deformation for a given image pair. In both strategies, the\naccuracy of the resulting spatial correspondences is strongly influenced by the\nchoice of certain hyperparameter values. However, an effective hyperparameter\nsearch consumes substantial time and human effort as it often involves training\nmultiple models for different fixed hyperparameter values and may lead to\nsuboptimal registration. We propose an amortized hyperparameter learning\nstrategy to alleviate this burden by learning the impact of hyperparameters on\ndeformation fields. We design a meta network, or hypernetwork, that predicts\nthe parameters of a registration network for input hyperparameters, thereby\ncomprising a single model that generates the optimal deformation field\ncorresponding to given hyperparameter values. This strategy enables fast,\nhigh-resolution hyperparameter search at test-time, reducing the inefficiency\nof traditional approaches while increasing flexibility. We also demonstrate\nadditional benefits of HyperMorph, including enhanced robustness to model\ninitialization and the ability to rapidly identify optimal hyperparameter\nvalues specific to a dataset, image contrast, task, or even anatomical region,\nall without the need to retrain models. We make our code publicly available at\n<a href=\"http://hypermorph.voxelmorph.net.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoopes_A/0/1/0/all/0/1\">Andrew Hoopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1\">Malte Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greve_D/0/1/0/all/0/1\">Douglas N. Greve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1\">John Guttag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Relighting with Geometrically Consistent Shadows. (arXiv:2203.16681v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16681","description":"<p>Most face relighting methods are able to handle diffuse shadows, but struggle\nto handle hard shadows, such as those cast by the nose. Methods that propose\ntechniques for handling hard shadows often do not produce geometrically\nconsistent shadows since they do not directly leverage the estimated face\ngeometry while synthesizing them. We propose a novel differentiable algorithm\nfor synthesizing hard shadows based on ray tracing, which we incorporate into\ntraining our face relighting model. Our proposed algorithm directly utilizes\nthe estimated face geometry to synthesize geometrically consistent hard\nshadows. We demonstrate through quantitative and qualitative experiments on\nMulti-PIE and FFHQ that our method produces more geometrically consistent\nshadows than previous face relighting methods while also achieving\nstate-of-the-art face relighting performance under directional lighting. In\naddition, we demonstrate that our differentiable hard shadow modeling improves\nthe quality of the estimated face geometry over diffuse shading models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_A/0/1/0/all/0/1\">Andrew Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkis_M/0/1/0/all/0/1\">Michel Sarkis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1\">Ning Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yiying Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo. (arXiv:2203.16682v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16682","description":"<p>We present a debiased dataset for the Person-centric Visual Grounding (PCVG)\ntask first proposed by Cui et al. (2021) in the Who's Waldo dataset. Given an\nimage and a caption, PCVG requires pairing up a person's name mentioned in a\ncaption with a bounding box that points to the person in the image. We find\nthat the original Who's Waldo dataset compiled for this task contains a large\nnumber of biased samples that are solvable simply by heuristic methods; for\ninstance, in many cases the first name in the sentence corresponds to the\nlargest bounding box, or the sequence of names in the sentence corresponds to\nan exact left-to-right order in the image. Naturally, models trained on these\nbiased data lead to over-estimation of performance on the benchmark. To enforce\nmodels being correct for the correct reasons, we design automated tools to\nfilter and debias the original dataset by ruling out all examples of\ninsufficient context, such as those with no verb or with a long chain of\nconjunct names in their captions. Our experiments show that our new sub-sampled\ndataset contains less bias with much lowered heuristic performances and widened\ngaps between heuristic and supervised methods. We also demonstrate the same\nbenchmark model trained on our debiased training set outperforms that trained\non the original biased (and larger) training set on our debiased test set. We\nargue our debiased dataset offers the PCVG task a more practical baseline for\nreliable benchmarking and future improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yiran Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Adaptive Parameter Sharing for Multi-Task Learning. (arXiv:2203.16708v1 [cs.LG])","link":"http://arxiv.org/abs/2203.16708","description":"<p>Adapting pre-trained models with broad capabilities has become standard\npractice for learning a wide range of downstream tasks. The typical approach of\nfine-tuning different models for each task is performant, but incurs a\nsubstantial memory cost. To efficiently learn multiple downstream tasks we\nintroduce Task Adaptive Parameter Sharing (TAPS), a general method for tuning a\nbase model to a new task by adaptively modifying a small, task-specific subset\nof layers. This enables multi-task learning while minimizing resources used and\ncompetition between tasks. TAPS solves a joint optimization problem which\ndetermines which layers to share with the base model and the value of the\ntask-specific weights. Further, a sparsity penalty on the number of active\nlayers encourages weight sharing with the base model. Compared to other\nmethods, TAPS retains high accuracy on downstream tasks while introducing few\ntask-specific parameters. Moreover, TAPS is agnostic to the model architecture\nand requires only minor changes to the training scheme. We evaluate our method\non a suite of fine-tuning tasks and architectures (ResNet, DenseNet, ViT) and\nshow that it achieves state-of-the-art performance while being simple to\nimplement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1\">Matthew Wallingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Explainable Metrics for Augmented SGD. (arXiv:2203.16723v1 [cs.LG])","link":"http://arxiv.org/abs/2203.16723","description":"<p>Explaining the generalization characteristics of deep learning is an emerging\ntopic in advanced machine learning. There are several unanswered questions\nabout how learning under stochastic optimization really works and why certain\nstrategies are better than others. In this paper, we address the following\nquestion: \\textit{can we probe intermediate layers of a deep neural network to\nidentify and quantify the learning quality of each layer?} With this question\nin mind, we propose new explainability metrics that measure the redundant\ninformation in a network's layers using a low-rank factorization framework and\nquantify a complexity measure that is highly correlated with the generalization\nperformance of a given optimizer, network, and dataset. We subsequently exploit\nthese metrics to augment the Stochastic Gradient Descent (SGD) optimizer by\nadaptively adjusting the learning rate in each layer to improve in\ngeneralization performance. Our augmented SGD -- dubbed RMSGD -- introduces\nminimal computational overhead compared to SOTA methods and outperforms them by\nexhibiting strong generalization characteristics across application,\narchitecture, and dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1\">Mathieu Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Image Aesthetics Assessment with Rich Attributes. (arXiv:2203.16754v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16754","description":"<p>Personalized image aesthetics assessment (PIAA) is challenging due to its\nhighly subjective nature. People's aesthetic tastes depend on diversified\nfactors, including image characteristics and subject characters. The existing\nPIAA databases are limited in terms of annotation diversity, especially the\nsubject aspect, which can no longer meet the increasing demands of PIAA\nresearch. To solve the dilemma, we conduct so far, the most comprehensive\nsubjective study of personalized image aesthetics and introduce a new\nPersonalized image Aesthetics database with Rich Attributes (PARA), which\nconsists of 31,220 images with annotations by 438 subjects. PARA features\nwealthy annotations, including 9 image-oriented objective attributes and 4\nhuman-oriented subjective attributes. In addition, desensitized subject\ninformation, such as personality traits, is also provided to support study of\nPIAA and user portraits. A comprehensive analysis of the annotation data is\nprovided and statistic study indicates that the aesthetic preferences can be\nmirrored by proposed subjective attributes. We also propose a conditional PIAA\nmodel by utilizing subject information as conditional prior. Experimental\nresults indicate that the conditional PIAA model can outperform the control\ngroup, which is also the first attempt to demonstrate how image aesthetics and\nsubject characters interact to produce the intricate personalized tastes on\nimage aesthetics. We believe the database and the associated analysis would be\nuseful for conducting next-generation PIAA study. The project page of PARA can\nbe found at: https://cv-datasets.institutecv.com/#/data-sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuzhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Leida Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_N/0/1/0/all/0/1\">Nan Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models. (arXiv:2203.16755v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16755","description":"<p>We propose a memory efficient method, named Stochastic Backpropagation (SBP),\nfor training deep neural networks on videos. It is based on the finding that\ngradients from incomplete execution for backpropagation can still effectively\ntrain the models with minimal accuracy loss, which attributes to the high\nredundancy of video. SBP keeps all forward paths but randomly and independently\nremoves the backward paths for each network layer in each training step. It\nreduces the GPU memory cost by eliminating the need to cache activation values\ncorresponding to the dropped backward paths, whose amount can be controlled by\nan adjustable keep-ratio. Experiments show that SBP can be applied to a wide\nrange of models for video tasks, leading to up to 80.0% GPU memory saving and\n10% training speedup with less than 1% accuracy drop on action recognition and\ntemporal action detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Feng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Casual 6-DoF: free-viewpoint panorama using a handheld 360 camera. (arXiv:2203.16756v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16756","description":"<p>Six degrees-of-freedom (6-DoF) video provides telepresence by enabling users\nto move around in the captured scene with a wide field of regard. Compared to\nmethods requiring sophisticated camera setups, the image-based rendering method\nbased on photogrammetry can work with images captured with any poses, which is\nmore suitable for casual users. However, existing image-based rendering methods\nare based on perspective images. When used to reconstruct 6-DoF views, it often\nrequires capturing hundreds of images, making data capture a tedious and\ntime-consuming process. In contrast to traditional perspective images,\n360{\\deg} images capture the entire surrounding view in a single shot, thus,\nproviding a faster capturing process for 6-DoF view reconstruction. This paper\npresents a novel method to provide 6-DoF experiences over a wide area using an\nunstructured collection of 360{\\deg} panoramas captured by a conventional\n360{\\deg} camera. Our method consists of 360{\\deg} data capturing, novel depth\nestimation to produce a high-quality spherical depth panorama, and\nhigh-fidelity free-viewpoint generation. We compared our method against\nstate-of-the-art methods, using data captured in various environments. Our\nmethod shows better visual quality and robustness in the tested scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rongsen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fang-Lue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnie_S/0/1/0/all/0/1\">Simon Finnie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalmers_A/0/1/0/all/0/1\">Andrew Chalmers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_T/0/1/0/all/0/1\">Teahyun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeMOT: Multi-Object Tracking with Memory. (arXiv:2203.16761v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16761","description":"<p>We propose an online tracking algorithm that performs the object detection\nand data association under a common framework, capable of linking objects after\na long time span. This is realized by preserving a large spatio-temporal memory\nto store the identity embeddings of the tracked objects, and by adaptively\nreferencing and aggregating useful information from the memory as needed. Our\nmodel, called MeMOT, consists of three main modules that are all\nTransformer-based: 1) Hypothesis Generation that produce object proposals in\nthe current video frame; 2) Memory Encoding that extracts the core information\nfrom the memory for each tracked object; and 3) Memory Decoding that solves the\nobject detection and data association tasks simultaneously for multi-object\ntracking. When evaluated on widely adopted MOT benchmark datasets, MeMOT\nobserves very competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREATE: A Benchmark for Chinese Short Video Retrieval and Title Generation. (arXiv:2203.16763v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16763","description":"<p>Previous works of video captioning aim to objectively describe the video's\nactual content, which lacks subjective and attractive expression, limiting its\npractical application scenarios. Video titling is intended to achieve this\ngoal, but there is a lack of a proper benchmark. In this paper, we propose to\nCREATE, the first large-scale Chinese shoRt vidEo retrievAl and Title\ngEneration benchmark, to facilitate research and application in video titling\nand video retrieval in Chinese. CREATE consists of a high-quality labeled 210K\ndataset and two large-scale 3M/10M pre-training datasets, covering 51\ncategories, 50K+ tags, 537K manually annotated titles and captions, and 10M+\nshort videos. Based on CREATE, we propose a novel model ALWIG which combines\nvideo retrieval and video titling tasks to achieve the purpose of multi-modal\nALignment WIth Generation with the help of video tags and a GPT pre-trained\nmodel. CREATE opens new directions for facilitating future research and\napplications on video titling and video retrieval in the field of Chinese short\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhongang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunfeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpatioTemporal Focus for Skeleton-based Action Recognition. (arXiv:2203.16767v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16767","description":"<p>Graph convolutional networks (GCNs) are widely adopted in skeleton-based\naction recognition due to their powerful ability to model data topology. We\nargue that the performance of recent proposed skeleton-based action recognition\nmethods is limited by the following factors. First, the predefined graph\nstructures are shared throughout the network, lacking the flexibility and\ncapacity to model the multi-grain semantic information. Second, the relations\namong the global joints are not fully exploited by the graph local convolution,\nwhich may lose the implicit joint relevance. For instance, actions such as\nrunning and waving are performed by the co-movement of body parts and joints,\ne.g., legs and arms, however, they are located far away in physical connection.\nInspired by the recent attention mechanism, we propose a multi-grain contextual\nfocus module, termed MCF, to capture the action associated relation information\nfrom the body joints and parts. As a result, more explainable representations\nfor different skeleton action sequences can be obtained by MCF. In this study,\nwe follow the common practice that the dense sample strategy of the input\nskeleton sequences is adopted and this brings much redundancy since number of\ninstances has nothing to do with actions. To reduce the redundancy, a temporal\ndiscrimination focus module, termed TDF, is developed to capture the local\nsensitive points of the temporal dynamics. MCF and TDF are integrated into the\nstandard GCN network to form a unified architecture, named STF-Net. It is noted\nthat STF-Net provides the capability to capture robust movement patterns from\nthese skeleton topology structures, based on multi-grain context aggregation\nand temporal dependency. Extensive experimental results show that our STF-Net\nsignificantly achieves state-of-the-art results on three challenging benchmarks\nNTU RGB+D 60, NTU RGB+D 120, and Kinetics-skeleton.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReSTR: Convolution-free Referring Image Segmentation Using Transformers. (arXiv:2203.16768v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16768","description":"<p>Referring image segmentation is an advanced semantic segmentation task where\ntarget is not a predefined class but is described in natural language. Most of\nexisting methods for this task rely heavily on convolutional neural networks,\nwhich however have trouble capturing long-range dependencies between entities\nin the language expression and are not flexible enough for modeling\ninteractions between the two different modalities. To address these issues, we\npresent the first convolution-free model for referring image segmentation using\ntransformers, dubbed ReSTR. Since it extracts features of both modalities\nthrough transformer encoders, it can capture long-range dependencies between\nentities within each modality. Also, ReSTR fuses features of the two modalities\nby a self-attention encoder, which enables flexible and adaptive interactions\nbetween the two modalities in the fusion process. The fused features are fed to\na segmentation module, which works adaptively according to the image and\nlanguage expression in hand. ReSTR is evaluated and compared with previous work\non all public benchmarks, where it outperforms all existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Namyup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAKe-Net: Topology-Aware Point Cloud Completion by Localizing Aligned Keypoints. (arXiv:2203.16771v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16771","description":"<p>Point cloud completion aims at completing geometric and topological shapes\nfrom a partial observation. However, some topology of the original shape is\nmissing, existing methods directly predict the location of complete points,\nwithout predicting structured and topological information of the complete\nshape, which leads to inferior performance. To better tackle the missing\ntopology part, we propose LAKe-Net, a novel topology-aware point cloud\ncompletion model by localizing aligned keypoints, with a novel\nKeypoints-Skeleton-Shape prediction manner. Specifically, our method completes\nmissing topology using three steps: 1) Aligned Keypoint Localization. An\nasymmetric keypoint locator, including an unsupervised multi-scale keypoint\ndetector and a complete keypoint generator, is proposed for localizing aligned\nkeypoints from complete and partial point clouds. We theoretically prove that\nthe detector can capture aligned keypoints for objects within a sub-category.\n2) Surface-skeleton Generation. A new type of skeleton, named Surface-skeleton,\nis generated from keypoints based on geometric priors to fully represent the\ntopological information captured from keypoints and better recover the local\ndetails. 3) Shape Refinement. We design a refinement subnet where multi-scale\nsurface-skeletons are fed into each recursive skeleton-assisted refinement\nmodule to assist the completion process. Experimental results show that our\nmethod achieves the state-of-the-art performance on point cloud completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Junshu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhijun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask Atari for Deep Reinforcement Learning as POMDP Benchmarks. (arXiv:2203.16777v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16777","description":"<p>We present Mask Atari, a new benchmark to help solve partially observable\nMarkov decision process (POMDP) problems with Deep Reinforcement Learning\n(DRL)-based approaches. To achieve a simulation environment for the POMDP\nproblems, Mask Atari is constructed based on Atari 2600 games with\ncontrollable, moveable, and learnable masks as the observation area for the\ntarget agent, especially with the active information gathering (AIG) setting in\nPOMDPs. Given that one does not yet exist, Mask Atari provides a challenging,\nefficient benchmark for evaluating the methods that focus on the above problem.\nMoreover, the mask operation is a trial for introducing the receptive field in\nthe human vision system into a simulation environment for an agent, which means\nthe evaluations are not biased from the sensing ability and purely focus on the\ncognitive performance of the methods when compared with the human baseline. We\ndescribe the challenges and features of our benchmark and evaluate several\nbaselines with Mask Atari.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yang Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Quan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_T/0/1/0/all/0/1\">Tadayuki Matsumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuji_T/0/1/0/all/0/1\">Taiki Fuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_K/0/1/0/all/0/1\">Kiyoto Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizuno_H/0/1/0/all/0/1\">Hiroyuki Mizuno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval. (arXiv:2203.16778v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16778","description":"<p>Visual appearance is considered to be the most important cue to understand\nimages for cross-modal retrieval, while sometimes the scene text appearing in\nimages can provide valuable information to understand the visual semantics.\nMost of existing cross-modal retrieval approaches ignore the usage of scene\ntext information and directly adding this information may lead to performance\ndegradation in scene text free scenarios. To address this issue, we propose a\nfull transformer architecture to unify these cross-modal retrieval scenarios in\na single $\\textbf{Vi}$sion and $\\textbf{S}$cene $\\textbf{T}$ext\n$\\textbf{A}$ggregation framework (ViSTA). Specifically, ViSTA utilizes\ntransformer blocks to directly encode image patches and fuse scene text\nembedding to learn an aggregated visual representation for cross-modal\nretrieval. To tackle the modality missing problem of scene text, we propose a\nnovel fusion token based transformer aggregation approach to exchange the\nnecessary scene text information only through the fusion token and concentrate\non the most important features in each modality. To further strengthen the\nvisual modality, we develop dual contrastive learning losses to embed both\nimage-text pairs and fusion-text pairs into a common cross-modal space.\nCompared to existing methods, ViSTA enables to aggregate relevant scene text\nsemantics with visual appearance, and hence improve results under both scene\ntext free and scene text aware scenarios. Experimental results show that ViSTA\noutperforms other methods by at least $\\bf{8.4}\\%$ at Recall@1 for scene text\naware retrieval task. Compared with state-of-the-art scene text free retrieval\nmethods, ViSTA can achieve better accuracy on Flicker30K and MSCOCO while\nrunning at least three times faster during the inference stage, which validates\nthe effectiveness of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mengjun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiongwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guoli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Patch Label Inference Networks for Efficient Pavement Distress Detection and Recognition in the Wild. (arXiv:2203.16782v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16782","description":"<p>Automatic image-based pavement distress detection and recognition are vital\nfor pavement maintenance and management. However, existing deep learning-based\nmethods largely omit the specific characteristics of pavement images, such as\nhigh image resolution and low distress area ratio, and are not end-to-end\ntrainable. In this paper, we present a series of simple yet effective\nend-to-end deep learning approaches named Weakly Supervised Patch Label\nInference Networks (WSPLIN) for efficiently addressing these tasks under\nvarious application settings. To fully exploit the resolution and scale\ninformation, WSPLIN first divides the pavement image under different scales\ninto patches with different collection strategies and then employs a Patch\nLabel Inference Network (PLIN) to infer the labels of these patches. Notably,\nwe design a patch label sparsity constraint based on the prior knowledge of\ndistress distribution, and leverage the Comprehensive Decision Network (CDN) to\nguide the training of PLIN in a weakly supervised way. Therefore, the patch\nlabels produced by PLIN provide interpretable intermediate information, such as\nthe rough location and the type of distress. We evaluate our method on a\nlarge-scale bituminous pavement distress dataset named CQU-BPDD. Extensive\nresults demonstrate the superiority of our method over baselines in both\nperformance and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wenhao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guixin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huangfu_L/0/1/0/all/0/1\">Luwen Huangfu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-Text Representation Learning via Differentiable Weak Temporal Alignment. (arXiv:2203.16784v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16784","description":"<p>Learning generic joint representations for video and text by a supervised\nmethod requires a prohibitively substantial amount of manually annotated video\ndatasets. As a practical alternative, a large-scale but uncurated and narrated\nvideo dataset, HowTo100M, has recently been introduced. But it is still\nchallenging to learn joint embeddings of video and text in a self-supervised\nmanner, due to its ambiguity and non-sequential alignment. In this paper, we\npropose a novel multi-modal self-supervised framework Video-Text Temporally\nWeak Alignment-based Contrastive Learning (VT-TWINS) to capture significant\ninformation from noisy and weakly correlated data using a variant of Dynamic\nTime Warping (DTW). We observe that the standard DTW inherently cannot handle\nweakly correlated data and only considers the globally optimal alignment path.\nTo address these problems, we develop a differentiable DTW which also reflects\nlocal information with weak temporal alignment. Moreover, our proposed model\napplies a contrastive learning scheme to learn feature representations on\nweakly correlated data. Our extensive experiments demonstrate that VT-TWINS\nattains significant improvements in multi-modal representation learning and\noutperforms various challenging downstream tasks. Code is available at\nhttps://github.com/mlvlab/VT-TWINS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_D/0/1/0/all/0/1\">Dohwan Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Joonmyung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Juyeon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1\">Shinyeong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+On_K/0/1/0/all/0/1\">Kyoung-Woon On</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eun-Sol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo J. Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflection and Rotation Symmetry Detection via Equivariant Learning. (arXiv:2203.16787v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16787","description":"<p>The inherent challenge of detecting symmetries stems from arbitrary\norientations of symmetry patterns; a reflection symmetry mirrors itself against\nan axis with a specific orientation while a rotation symmetry matches its\nrotated copy with a specific orientation. Discovering such symmetry patterns\nfrom an image thus benefits from an equivariant feature representation, which\nvaries consistently with reflection and rotation of the image. In this work, we\nintroduce a group-equivariant convolutional network for symmetry detection,\ndubbed EquiSym, which leverages equivariant feature maps with respect to a\ndihedral group of reflection and rotation. The proposed network is built\nend-to-end with dihedrally-equivariant layers and trained to output a spatial\nmap for reflection axes or rotation centers. We also present a new dataset,\nDENse and DIverse symmetry (DENDI), which mitigates limitations of existing\nbenchmarks for reflection and rotation symmetry detection. Experiments show\nthat our method achieves the state of the arts in symmetry detection on LDRS\nand DENDI datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_A/0/1/0/all/0/1\">Ahyun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byungjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable Video Transformer. (arXiv:2203.16795v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16795","description":"<p>Video transformers have recently emerged as an effective alternative to\nconvolutional networks for action classification. However, most prior video\ntransformers adopt either global space-time attention or hand-defined\nstrategies to compare patches within and across frames. These fixed attention\nschemes not only have high computational cost but, by comparing patches at\npredetermined locations, they neglect the motion dynamics in the video. In this\npaper, we introduce the Deformable Video Transformer (DVT), which dynamically\npredicts a small subset of video patches to attend for each query location\nbased on motion information, thus allowing the model to decide where to look in\nthe video based on correspondences across frames. Crucially, these motion-based\ncorrespondences are obtained at zero-cost from information stored in the\ncompressed format of the video. Our deformable attention mechanism is optimised\ndirectly with respect to classification performance, thus eliminating the need\nfor suboptimal hand-design of attention strategies. Experiments on four\nlarge-scale video benchmarks (Kinetics-400, Something-Something-V2,\nEPIC-KITCHENS and Diving-48) demonstrate that, compared to existing video\ntransformers, our model achieves higher accuracy at the same or lower\ncomputational cost, and it attains state-of-the-art results on these four\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ternary and Binary Quantization for Improved Classification. (arXiv:2203.16798v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16798","description":"<p>Dimension reduction and data quantization are two important methods for\nreducing data complexity. In the paper, we study the methodology of first\nreducing data dimension by random projection and then quantizing the\nprojections to ternary or binary codes, which has been widely applied in\nclassification. Usually, the quantization will seriously degrade the accuracy\nof classification due to high quantization errors. Interestingly, however, we\nobserve that the quantization could provide comparable and often superior\naccuracy, as the data to be quantized are sparse features generated with common\nfilters. Furthermore, this quantization property could be maintained in the\nrandom projections of sparse features, if both the features and random\nprojection matrices are sufficiently sparse. By conducting extensive\nexperiments, we validate and analyze this intriguing property.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weizhi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization. (arXiv:2203.16800v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16800","description":"<p>We target at the task of weakly-supervised action localization (WSAL), where\nonly video-level action labels are available during model training. Despite the\nrecent progress, existing methods mainly embrace a\nlocalization-by-classification paradigm and overlook the fruitful fine-grained\ntemporal distinctions between video sequences, thus suffering from severe\nambiguity in classification learning and classification-to-localization\nadaption. This paper argues that learning by contextually comparing\nsequence-to-sequence distinctions offers an essential inductive bias in WSAL\nand helps identify coherent action instances. Specifically, under a\ndifferentiable dynamic programming formulation, two complementary contrastive\nobjectives are designed, including Fine-grained Sequence Distance (FSD)\ncontrasting and Longest Common Subsequence (LCS) contrasting, where the first\none considers the relations of various action/background proposals by using\nmatch, insert, and delete operators and the second one mines the longest common\nsubsequences between two videos. Both contrasting modules can enhance each\nother and jointly enjoy the merits of discriminative action-background\nseparation and alleviated task gap between classification and localization.\nExtensive experiments show that our method achieves state-of-the-art\nperformance on two popular benchmarks. Our code is available at\nhttps://github.com/MengyuanChen21/CVPR2022-FTCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Portrait Matting with Privacy Preserving. (arXiv:2203.16828v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16828","description":"<p>Recently, there has been an increasing concern about the privacy issue raised\nby using personally identifiable information in machine learning. However,\nprevious portrait matting methods were all based on identifiable portrait\nimages. To fill the gap, we present P3M-10k in this paper, which is the first\nlarge-scale anonymized benchmark for Privacy-Preserving Portrait Matting (P3M).\nP3M-10k consists of 10,000 high-resolution face-blurred portrait images along\nwith high-quality alpha mattes. We systematically evaluate both trimap-free and\ntrimap-based matting methods on P3M-10k and find that existing matting methods\nshow different generalization abilities under the privacy preserving training\nsetting, i.e., training only on face-blurred images while testing on arbitrary\nimages. Based on the gained insights, we propose a unified matting model named\nP3M-Net consisting of three carefully designed integration modules that can\nperform privacy-insensitive semantic perception and detail-reserved matting\nsimultaneously. We further design multiple variants of P3M-Net with different\nCNN and transformer backbones and identify the difference in their\ngeneralization abilities. To further mitigate this issue, we devise a simple\nyet effective Copy and Paste strategy (P3M-CP) that can borrow facial\ninformation from public celebrity images without privacy concerns and direct\nthe network to reacquire the face context at both data and feature levels.\nP3M-CP only brings a few additional computations during training, while\nenabling the matting model to process both face-blurred and normal images\nwithout extra effort during inference. Extensive experiments on P3M-10k\ndemonstrate the superiority of P3M-Net over state-of-the-art methods and the\neffectiveness of P3M-CP in improving the generalization ability of P3M-Net,\nimplying a great significance of P3M for future research and real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sihan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Scene Understanding via Disentangled Instance Mesh Reconstruction. (arXiv:2203.16832v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16832","description":"<p>Semantic scene reconstruction from point cloud is an essential and\nchallenging task for 3D scene understanding. This task requires not only to\nrecognize each instance in the scene, but also to recover their geometries\nbased on the partial observed point cloud. Existing methods usually attempt to\ndirectly predict occupancy values of the complete object based on incomplete\npoint cloud proposals from a detection-based backbone. However, this framework\nalways fails to reconstruct high fidelity mesh due to the obstruction of\nvarious detected false positive object proposals and the ambiguity of\nincomplete point observations for learning occupancy values of complete\nobjects. To circumvent the hurdle, we propose a Disentangled Instance Mesh\nReconstruction (DIMR) framework for effective point scene understanding. A\nsegmentation-based backbone is applied to reduce false positive object\nproposals, which further benefits our exploration on the relationship between\nrecognition and reconstruction. Based on the accurate proposals, we leverage a\nmesh-aware latent code space to disentangle the processes of shape completion\nand mesh generation, relieving the ambiguity caused by the incomplete point\nobservations. Furthermore, with access to the CAD model pool at test time, our\nmodel can also be used to improve the reconstruction quality by performing mesh\nretrieval without extra training. We thoroughly evaluate the reconstructed mesh\nquality with multiple metrics, and demonstrate the superiority of our method on\nthe challenging ScanNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaxiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Extraction with Co-Speech Gestures Cue. (arXiv:2203.16840v1 [eess.AS])","link":"http://arxiv.org/abs/2203.16840","description":"<p>Speaker extraction seeks to extract the clean speech of a target speaker from\na multi-talker mixture speech. There have been studies to use a pre-recorded\nspeech sample or face image of the target speaker as the speaker cue. In human\ncommunication, co-speech gestures that are naturally timed with speech also\ncontribute to speech perception. In this work, we explore the use of co-speech\ngestures sequence, e.g. hand and body movements, as the speaker cue for speaker\nextraction, which could be easily obtained from low-resolution video\nrecordings, thus more available than face recordings. We propose two networks\nusing the co-speech gestures cue to perform attentive listening on the target\nspeaker, one that implicitly fuses the co-speech gestures cue in the speaker\nextraction process, the other performs speech separation first, followed by\nexplicitly using the co-speech gestures cue to associate a separated speech to\nthe target speaker. The experimental results show that the co-speech gestures\ncue is informative in associating the target speaker, and the quality of the\nextracted speech shows significant improvements over the unprocessed mixture\nspeech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_Z/0/1/0/all/0/1\">Zexu Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_X/0/1/0/all/0/1\">Xinyuan Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Driving-Oriented Metric for Lane Detection Models. (arXiv:2203.16851v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16851","description":"<p>After the 2017 TuSimple Lane Detection Challenge, its dataset and evaluation\nbased on accuracy and F1 score have become the de facto standard to measure the\nperformance of lane detection methods. While they have played a major role in\nimproving the performance of lane detection methods, the validity of this\nevaluation method in downstream tasks has not been adequately researched. In\nthis study, we design 2 new driving-oriented metrics for lane detection:\nEnd-to-End Lateral Deviation metric (E2E-LD) is directly formulated based on\nthe requirements of autonomous driving, a core downstream task of lane\ndetection; Per-frame Simulated Lateral Deviation metric (PSLD) is a lightweight\nsurrogate metric of E2E-LD. To evaluate the validity of the metrics, we conduct\na large-scale empirical study with 4 major types of lane detection approaches\non the TuSimple dataset and our newly constructed dataset Comma2k19-LD. Our\nresults show that the conventional metrics have strongly negative correlations\n($\\leq$-0.55) with E2E-LD, meaning that some recent improvements purely\ntargeting the conventional metrics may not have led to meaningful improvements\nin autonomous driving, but rather may actually have made it worse by\noverfitting to the conventional metrics. As autonomous driving is a\nsecurity/safety-critical system, the underestimation of robustness hinders the\nsound development of practical lane detection models. We hope that our study\nwill help the community achieve more downstream task-aware evaluations for lane\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Takami Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Alfred Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Modality Bias in Audio Visual Video Parsing. (arXiv:2203.16860v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16860","description":"<p>We focus on the audio-visual video parsing (AVVP) problem that involves\ndetecting audio and visual event labels with temporal boundaries. The task is\nespecially challenging since it is weakly supervised with only event labels\navailable as a bag of labels for each video. An existing state-of-the-art model\nfor AVVP uses a hybrid attention network (HAN) to generate cross-modal features\nfor both audio and visual modalities, and an attentive pooling module that\naggregates predicted audio and visual segment-level event probabilities to\nyield video-level event probabilities. We provide a detailed analysis of\nmodality bias in the existing HAN architecture, where a modality is completely\nignored during prediction. We also propose a variant of feature aggregation in\nHAN that leads to an absolute gain in F-scores of about 2% and 1.6% for visual\nand audio-visual events at both segment-level and event-level, in comparison to\nthe existing HAN model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasi_P/0/1/0/all/0/1\">Piyush Singh Pasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemani_S/0/1/0/all/0/1\">Shubham Nemani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16875","description":"<p>There has been rapid progress recently on 3D human rendering, including novel\nview synthesis and pose animation, based on the advances of neural radiance\nfields (NeRF). However, most existing methods focus on person-specific training\nand their training typically requires multi-view videos. This paper deals with\na new challenging task -- rendering novel views and novel poses for a person\nunseen in training, using only multiview images as input. For this task, we\npropose a simple yet effective method to train a generalizable NeRF with\nmultiview images as conditional input. The key ingredient is a dedicated\nrepresentation combining a canonical NeRF and a volume deformation scheme.\nUsing a canonical space enables our method to learn shared properties of human\nand easily generalize to different people. Volume deformation is used to\nconnect the canonical space with input and target images and query image\nfeatures for radiance and density prediction. We leverage the parametric 3D\nhuman model fitted on the input images to derive the deformation, which works\nquite well in practice when combined with our canonical NeRF. The experiments\non both real and synthetic data with the novel view synthesis and pose\nanimation tasks collectively demonstrate the efficacy of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiangjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongyoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds. (arXiv:2203.16895v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16895","description":"<p>Point cloud scene flow estimation is of practical importance for dynamic\nscene navigation in autonomous driving. Since scene flow labels are hard to\nobtain, current methods train their models on synthetic data and transfer them\nto real scenes. However, large disparities between existing synthetic datasets\nand real scenes lead to poor model transfer. We make two major contributions to\naddress that. First, we develop a point cloud collector and scene flow\nannotator for GTA-V engine to automatically obtain diverse realistic training\nsamples without human intervention. With that, we develop a large-scale\nsynthetic scene flow dataset GTA-SF. Second, we propose a mean-teacher-based\ndomain adaptation framework that leverages self-generated pseudo-labels of the\ntarget domain. It also explicitly incorporates shape deformation regularization\nand surface correspondence refinement to address distortions and misalignments\nin domain transfer. Through extensive experiments, we show that our GTA-SF\ndataset leads to a consistent boost in model generalization to three real\ndatasets (i.e., Waymo, Lyft and KITTI) as compared to the most widely used FT3D\ndataset. Moreover, our framework achieves superior adaptation performance on\nsix source-target dataset pairs, remarkably closing the average domain gap by\n60%. Data and codes are available at https://github.com/leolyj/DCA-SRSFE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow. (arXiv:2203.16896v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16896","description":"<p>Optical flow estimation aims to find the 2D motion field by identifying\ncorresponding pixels between two images. Despite the tremendous progress of\ndeep learning-based optical flow methods, it remains a challenge to accurately\nestimate large displacements with motion blur. This is mainly because the\ncorrelation volume, the basis of pixel matching, is computed as the dot product\nof the convolutional features of the two images. The locality of convolutional\nfeatures makes the computed correlations susceptible to various noises. On\nlarge displacements with motion blur, noisy correlations could cause severe\nerrors in the estimated flow. To overcome this challenge, we propose a new\narchitecture \"CRoss-Attentional Flow Transformer\" (CRAFT), aiming to revitalize\nthe correlation volume computation. In CRAFT, a Semantic Smoothing Transformer\nlayer transforms the features of one frame, making them more global and\nsemantically stable. In addition, the dot-product correlations are replaced\nwith transformer Cross-Frame Attention. This layer filters out feature noises\nthrough the Query and Key projections, and computes more accurate correlations.\nOn Sintel (Final) and KITTI (foreground) benchmarks, CRAFT has achieved new\nstate-of-the-art performance. Moreover, to test the robustness of different\nmodels on large motions, we designed an image shifting attack that shifts input\nimages to generate large artificial motions. Under this attack, CRAFT performs\nmuch more robustly than two representative methods, RAFT and GMA. The code of\nCRAFT is is available at https://github.com/askerlee/craft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1\">Xiuchao Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaohua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xue Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyuan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Granularity Alignment Domain Adaptation for Object Detection. (arXiv:2203.16897v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16897","description":"<p>Domain adaptive object detection is challenging due to distinctive data\ndistribution between source domain and target domain. In this paper, we propose\na unified multi-granularity alignment based object detection framework towards\ndomain-invariant feature learning. To this end, we encode the dependencies\nacross different granularity perspectives including pixel-, instance-, and\ncategory-levels simultaneously to align two domains. Based on pixel-level\nfeature maps from the backbone network, we first develop the omni-scale gated\nfusion module to aggregate discriminative representations of instances by\nscale-aware convolutions, leading to robust multi-scale object detection.\nMeanwhile, the multi-granularity discriminators are proposed to identify which\ndomain different granularities of samples(i.e., pixels, instances, and\ncategories) come from. Notably, we leverage not only the instance\ndiscriminability in different categories but also the category consistency\nbetween two domains. Extensive experiments are carried out on multiple domain\nadaptation scenarios, demonstrating the effectiveness of our framework over\nstate-of-the-art algorithms on top of anchor-free FCOS and anchor-based Faster\nRCNN detectors with different backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenzhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tiejian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanjun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis. (arXiv:2203.16898v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16898","description":"<p>Recent years have witnessed substantial progress in semantic image synthesis,\nit is still challenging in synthesizing photo-realistic images with rich\ndetails. Most previous methods focus on exploiting the given semantic map,\nwhich just captures an object-level layout for an image. Obviously, a\nfine-grained part-level semantic layout will benefit object details generation,\nand it can be roughly inferred from an object's shape. In order to exploit the\npart-level layouts, we propose a Shape-aware Position Descriptor (SPD) to\ndescribe each pixel's positional feature, where object shape is explicitly\nencoded into the SPD feature. Furthermore, a Semantic-shape Adaptive Feature\nModulation (SAFM) block is proposed to combine the given semantic map and our\npositional features to produce adaptively modulated features. Extensive\nexperiments demonstrate that the proposed SPD and SAFM significantly improve\nthe generation of objects with rich details. Moreover, our method performs\nfavorably against the SOTA methods in terms of quantitative and qualitative\nevaluation. The source code and model are available at\nhttps://github.com/cszy98/SAFM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhengyao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps. (arXiv:2203.16910v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16910","description":"<p>In this paper, we aim to forecast a future trajectory distribution of a\nmoving agent in the real world, given the social scene images and historical\ntrajectories. Yet, it is a challenging task because the ground-truth\ndistribution is unknown and unobservable, while only one of its samples can be\napplied for supervising model learning, which is prone to bias. Most recent\nworks focus on predicting diverse trajectories in order to cover all modes of\nthe real distribution, but they may despise the precision and thus give too\nmuch credit to unrealistic predictions. To address the issue, we learn the\ndistribution with symmetric cross-entropy using occupancy grid maps as an\nexplicit and scene-compliant approximation to the ground-truth distribution,\nwhich can effectively penalize unlikely predictions. In specific, we present an\ninverse reinforcement learning based multi-modal trajectory distribution\nforecasting framework that learns to plan by an approximate value iteration\nnetwork in an end-to-end manner. Besides, based on the predicted distribution,\nwe generate a small set of representative trajectories through a differentiable\nTransformer-based network, whose attention mechanism helps to model the\nrelations of trajectories. In experiments, our method achieves state-of-the-art\nperformance on the Stanford Drone Dataset and Intersection Drone Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Ke Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques. (arXiv:2203.16915v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16915","description":"<p>A dataset of street light images is presented. Our dataset consists of\n$\\sim350\\textrm{k}$ images, taken from 140 UMBRELLA nodes installed in the\nSouth Gloucestershire region in the UK. Each UMBRELLA node is installed on the\npole of a lamppost and is equipped with a Raspberry Pi Camera Module v1 facing\nupwards towards the sky and lamppost light bulb. Each node collects an image at\nhourly intervals for 24h every day. The data collection spans for a period of\nsix months.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mavromatis_I/0/1/0/all/0/1\">Ioannis Mavromatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanoev_A/0/1/0/all/0/1\">Aleksandar Stanoev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carnelli_P/0/1/0/all/0/1\">Pietro Carnelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yichao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sooriyabandara_M/0/1/0/all/0/1\">Mahesh Sooriyabandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Aftab Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond. (arXiv:2203.16931v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16931","description":"<p>Rain removal aims to remove rain streaks from images/videos and reduce the\ndisruptive effects caused by rain. It not only enhances image/video visibility\nbut also allows many computer vision algorithms to function properly. This\npaper makes the first attempt to conduct a comprehensive study on the\nrobustness of deep learning-based rain removal methods against adversarial\nattacks. Our study shows that, when the image/video is highly degraded, rain\nremoval methods are more vulnerable to the adversarial attacks as small\ndistortions/perturbations become less noticeable or detectable. In this paper,\nwe first present a comprehensive empirical evaluation of various methods at\ndifferent levels of attacks and with various losses/targets to generate the\nperturbations from the perspective of human perception and machine analysis\ntasks. A systematic evaluation of key modules in existing methods is performed\nin terms of their robustness against adversarial attacks. From the insights of\nour analysis, we construct a more robust deraining method by integrating these\neffective modules. Finally, we examine various types of adversarial attacks\nthat are specific to deraining problems and their effects on both human and\nmachine vision tasks, including 1) rain region attacks, adding perturbations\nonly in the rain regions to make the perturbations in the attacked rain images\nless visible; 2) object-sensitive attacks, adding perturbations only in regions\nnear the given objects. Code is available at\nhttps://github.com/yuyi-sd/Robust_Rain_Removal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yap-Peng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contributions to interframe coding. (arXiv:2203.16934v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16934","description":"<p>Advanced motion models (4 or 6 parameters) are needed for a good\nrepresentation of the motion experimented by the different objects contained in\na sequence of images. If the image is split in very small blocks, then an\naccurate description of complex movements can be achieved with only 2\nparameters. This alternative implies a large set of vectors per image. We\npropose a new approach to reduce the number of vectors, using different block\nsizes as a function of the local characteristics of the image, without\nincreasing the error accepted with the smallest blocks. A second algorithm is\nproposed for an inter/intraframe coder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallverdu_Bayes_F/0/1/0/all/0/1\">Francesc Vallverdu-Bayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarres_Ruiz_F/0/1/0/all/0/1\">Francesc Tarres-Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Pose Verification for Outdoor Visual Localization with Self-supervised Contrastive Learning. (arXiv:2203.16945v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16945","description":"<p>Any city-scale visual localization system has to overcome long-term\nappearance changes, such as varying illumination conditions or seasonal changes\nbetween query and database images. Since semantic content is more robust to\nsuch changes, we exploit semantic information to improve visual localization.\nIn our scenario, the database consists of gnomonic views generated from\npanoramic images (e.g. Google Street View) and query images are collected with\na standard field-of-view camera at a different time. To improve localization,\nwe check the semantic similarity between query and database images, which is\nnot trivial since the position and viewpoint of the cameras do not exactly\nmatch. To learn similarity, we propose training a CNN in a self-supervised\nfashion with contrastive learning on a dataset of semantically segmented\nimages. With experiments we showed that this semantic similarity estimation\napproach works better than measuring the similarity at pixel-level. Finally, we\nused the semantic similarity scores to verify the retrievals obtained by a\nstate-of-the-art visual localization method and observed that contrastive\nlearning-based pose verification increases top-1 recall value to 0.90 which\ncorresponds to a 2% improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_S/0/1/0/all/0/1\">Semih Orhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_J/0/1/0/all/0/1\">Jose J. Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastanlar_Y/0/1/0/all/0/1\">Yalin Bastanlar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Fusion Transformer for Remote Sensing Image Classification. (arXiv:2203.16952v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16952","description":"<p>Vision transformer (ViT) has been trending in image classification tasks due\nto its promising performance when compared to convolutional neural networks\n(CNNs). As a result, many researchers have tried to incorporate ViT models in\nhyperspectral image (HSI) classification tasks, but without achieving\nsatisfactory performance. To this paper, we introduce a new multimodal fusion\ntransformer (MFT) network for HSI land-cover classification, which utilizes\nother sources of multimodal data in addition to HSI. Instead of using\nconventional feature fusion techniques, other multimodal data are used as an\nexternal classification (CLS) token in the transformer encoder, which helps\nachieving better generalization. ViT and other similar transformer models use a\nrandomly initialized external classification token {and fail to generalize\nwell}. However, the use of a feature embedding derived from other sources of\nmultimodal data, such as light detection and ranging (LiDAR), offers the\npotential to improve those models by means of a CLS. The concept of\ntokenization is used in our work to generate CLS and HSI patch tokens, helping\nto learn key features in a reduced feature space. We also introduce a new\nattention mechanism for improving the exchange of information between HSI\ntokens and the CLS (e.g., LiDAR) token. Extensive experiments are carried out\non widely used and benchmark datasets i.e., the University of Houston, Trento,\nUniversity of Southern Mississippi Gulfpark (MUUFL), and Augsburg. In the\nresults section, we compare the proposed MFT model with other state-of-the-art\ntransformer models, classical CNN models, as well as conventional classifiers.\nThe superior performance achieved by the proposed model is due to the use of\nmultimodal information as external classification tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Swalpa Kumar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deria_A/0/1/0/all/0/1\">Ankur Deria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1\">Behnood Rasti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1\">Antonio Plaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Instance Segmentation and Tracking via Data Association and Single-stage Detector. (arXiv:2203.16966v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16966","description":"<p>Human video instance segmentation plays an important role in computer\nunderstanding of human activities and is widely used in video processing, video\nsurveillance, and human modeling in virtual reality. Most current VIS methods\nare based on Mask-RCNN framework, where the target appearance and motion\ninformation for data matching will increase computational cost and have an\nimpact on segmentation real-time performance; on the other hand, the existing\ndatasets for VIS focus less on all the people appearing in the video. In this\npaper, to solve the problems, we develop a new method for human video instance\nsegmentation based on single-stage detector. To tracking the instance across\nthe video, we have adopted data association strategy for matching the same\ninstance in the video sequence, where we jointly learn target instance\nappearances and their affinities in a pair of video frames in an end-to-end\nfashion. We have also adopted the centroid sampling strategy for enhancing the\nembedding extraction ability of instance, which is to bias the instance\nposition to the inside of each instance mask with heavy overlap condition. As a\nresult, even there exists a sudden change in the character activity, the\ninstance position will not move out of the mask, so that the problem that the\nsame instance is represented by two different instances can be alleviated.\nFinally, we collect PVIS dataset by assembling several video instance\nsegmentation datasets to fill the gap of the current lack of datasets dedicated\nto human video segmentation. Extensive simulations based on such dataset has\nbeen conduct. Simulation results verify the effectiveness and efficiency of the\nproposed work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification. (arXiv:2203.16983v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16983","description":"<p>Self-supervised learning (SSL) has drawn increasing attention in pathological\nimage analysis in recent years. However, the prevalent contrastive SSL is\nsuboptimal in feature representation under this scenario due to the homogeneous\nvisual appearance. Alternatively, masked autoencoders (MAE) build SSL from a\ngenerative paradigm. They are more friendly to pathological image modeling. In\nthis paper, we firstly introduce MAE to pathological image analysis. A novel\nSD-MAE model is proposed to enable a self-distillation augmented SSL on top of\nthe raw MAE. Besides the reconstruction loss on masked image patches, SD-MAE\nfurther imposes the self-distillation loss on visible patches. It guides the\nencoder to perceive high-level semantics that benefit downstream tasks. We\napply SD-MAE to the image classification task on two pathological and one\nnatural image datasets. Experiments demonstrate that SD-MAE performs highly\ncompetitive when compared with leading contrastive SSL methods. The results,\nwhich are pre-trained using a moderate size of pathological images, are also\ncomparable to the method pre-trained with two orders of magnitude more images.\nOur code will be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhineng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xieping Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring hand use in the home after cervical spinal cord injury using egocentric video. (arXiv:2203.16996v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16996","description":"<p>Background: Egocentric video has recently emerged as a potential solution for\nmonitoring hand function in individuals living with tetraplegia in the\ncommunity, especially for its ability to detect functional use in the home\nenvironment. Objective: To develop and validate a wearable vision-based system\nfor measuring hand use in the home among individuals living with tetraplegia.\nMethods: Several deep learning algorithms for detecting functional hand-object\ninteractions were developed and compared. The most accurate algorithm was used\nto extract measures of hand function from 65 hours of unscripted video recorded\nat home by 20 participants with tetraplegia. These measures were: the\npercentage of interaction time over total recording time (Perc); the average\nduration of individual interactions (Dur); the number of interactions per hour\n(Num). To demonstrate the clinical validity of the technology, egocentric\nmeasures were correlated with validated clinical assessments of hand function\nand independence (Graded Redefined Assessment of Strength, Sensibility and\nPrehension - GRASSP, Upper Extremity Motor Score - UEMS, and Spinal Cord\nIndependent Measure - SCIM). Results: Hand-object interactions were\nautomatically detected with a median F1-score of 0.80 (0.67-0.87). Our results\ndemonstrated that higher UEMS and better prehension were related to greater\ntime spent interacting, whereas higher SCIM and better hand sensation resulted\nin a higher number of interactions performed during the egocentric video\nrecordings. Conclusions: For the first time, measures of hand function\nautomatically estimated in an unconstrained environment in individuals with\ntetraplegia have been validated against internationally accepted measures of\nhand function. Future work will necessitate a formal evaluation of the\nreliability and responsiveness of the egocentric-based performance measures for\nhand use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bandini_A/0/1/0/all/0/1\">Andrea Bandini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dousty_M/0/1/0/all/0/1\">Mehdy Dousty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hitzig_S/0/1/0/all/0/1\">Sander L. Hitzig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Craven_B/0/1/0/all/0/1\">B. Catharine Craven</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalsi_Ryan_S/0/1/0/all/0/1\">Sukhvinder Kalsi-Ryan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zariffa_J/0/1/0/all/0/1\">Jos&#xe9; Zariffa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher. (arXiv:2203.17008v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17008","description":"<p>Model quantization is considered as a promising method to greatly reduce the\nresource requirements of deep neural networks. To deal with the performance\ndrop induced by quantization errors, a popular method is to use training data\nto fine-tune quantized networks. In real-world environments, however, such a\nmethod is frequently infeasible because training data is unavailable due to\nsecurity, privacy, or confidentiality concerns. Zero-shot quantization\naddresses such problems, usually by taking information from the weights of a\nfull-precision teacher network to compensate the performance drop of the\nquantized networks. In this paper, we first analyze the loss surface of\nstate-of-the-art zero-shot quantization techniques and provide several\nfindings. In contrast to usual knowledge distillation problems, zero-shot\nquantization often suffers from 1) the difficulty of optimizing multiple loss\nterms together, and 2) the poor generalization capability due to the use of\nsynthetic samples. Furthermore, we observe that many weights fail to cross the\nrounding threshold during training the quantized networks even when it is\nnecessary to do so for better performance. Based on the observations, we\npropose AIT, a simple yet powerful technique for zero-shot quantization, which\naddresses the aforementioned two problems in the following way: AIT i) uses a\nKL distance loss only without a cross-entropy loss, and ii) manipulates\ngradients to guarantee that a certain portion of weights are properly updated\nafter crossing the rounding thresholds. Experiments show that AIT outperforms\nthe performance of many existing methods by a great margin, taking over the\noverall state-of-the-art position in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kanghyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hye Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Deokki Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Joonsang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinho Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Temporal Learning Approach to Inpainting Endoscopic Specularities and Its effect on Image Correspondence. (arXiv:2203.17013v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17013","description":"<p>Video streams are utilised to guide minimally-invasive surgery and diagnostic\nprocedures in a wide range of procedures, and many computer assisted techniques\nhave been developed to automatically analyse them. These approaches can provide\nadditional information to the surgeon such as lesion detection, instrument\nnavigation, or anatomy 3D shape modeling. However, the necessary image features\nto recognise these patterns are not always reliably detected due to the\npresence of irregular light patterns such as specular highlight reflections. In\nthis paper, we aim at removing specular highlights from endoscopic videos using\nmachine learning. We propose using a temporal generative adversarial network\n(GAN) to inpaint the hidden anatomy under specularities, inferring its\nappearance spatially and from neighbouring frames where they are not present in\nthe same location. This is achieved using in-vivo data of gastric endoscopy\n(Hyper-Kvasir) in a fully unsupervised manner that relies on automatic\ndetection of specular highlights. System evaluations show significant\nimprovements to traditional methods through direct comparison as well as other\nmachine learning techniques through an ablation study that depicts the\nimportance of the network's temporal and transfer learning components. The\ngeneralizability of our system to different surgical setups and procedures was\nalso evaluated qualitatively on in-vivo data of gastric endoscopy and ex-vivo\nporcine data (SERV-CT, SCARED). We also assess the effect of our method in\ncomputer vision tasks that underpin 3D reconstruction and camera motion\nestimation, namely stereo disparity, optical flow, and sparse point feature\nmatching. These are evaluated quantitatively and qualitatively and results show\na positive effect of specular highlight inpainting on these tasks in a novel\ncomprehensive analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daher_R/0/1/0/all/0/1\">Rema Daher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logit Normalization for Long-tail Object Detection. (arXiv:2203.17020v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17020","description":"<p>Real-world data exhibiting skewed distributions pose a serious challenge to\nexisting object detectors. Moreover, the samplers in detectors lead to shifted\ntraining label distributions, while the tremendous proportion of background to\nforeground samples severely harms foreground classification. To mitigate these\nissues, in this paper, we propose Logit Normalization (LogN), a simple\ntechnique to self-calibrate the classified logits of detectors in a similar way\nto batch normalization. In general, our LogN is training- and tuning-free (i.e.\nrequire no extra training and tuning process), model- and label\ndistribution-agnostic (i.e. generalization to different kinds of detectors and\ndatasets), and also plug-and-play (i.e. direct application without any bells\nand whistles). Extensive experiments on the LVIS dataset demonstrate superior\nperformance of LogN to state-of-the-art methods with various detectors and\nbackbones. We also provide in-depth studies on different aspects of our LogN.\nFurther experiments on ImageNet-LT reveal its competitiveness and\ngeneralizability. Our LogN can serve as a strong baseline for long-tail object\ndetection and is expected to inspire future research in this field. Code and\ntrained models will be publicly available at https://github.com/MCG-NJU/LogN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yao Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks. (arXiv:2203.17030v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17030","description":"<p>New classes arise frequently in our ever-changing world, e.g., emerging\ntopics in social media and new types of products in e-commerce. A model should\nrecognize new classes and meanwhile maintain discriminability over old classes.\nUnder severe circumstances, only limited novel instances are available to\nincrementally update the model. The task of recognizing few-shot new classes\nwithout forgetting old classes is called few-shot class-incremental learning\n(FSCIL). In this work, we propose a new paradigm for FSCIL based on\nmeta-learning by LearnIng Multi-phase Incremental Tasks (LIMIT), which\nsynthesizes fake FSCIL tasks from the base dataset. The data format of fake\ntasks is consistent with the `real' incremental tasks, and we can build a\ngeneralizable feature space for the unseen tasks through meta-learning.\nBesides, LIMIT also constructs a calibration module based on transformer, which\ncalibrates the old class classifiers and new class prototypes into the same\nscale and fills in the semantic gap. The calibration module also adaptively\ncontextualizes the instance-specific embedding with a set-to-set function.\nLIMIT efficiently adapts to new classes and meanwhile resists forgetting over\nold classes. Experiments on three benchmark datasets (CIFAR100, miniImageNet,\nand CUB200) and large-scale dataset, i.e., ImageNet ILSVRC2012 validate that\nLIMIT achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection. (arXiv:2203.17054v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17054","description":"<p>Single frame data contains finite information which limits the performance of\nthe existing vision-based multi-camera 3D object detection paradigms. For\nfundamentally pushing the performance boundary in this area, BEVDet4D is\nproposed to lift the scalable BEVDet paradigm from the spatial-only 3D space to\nthe spatial-temporal 4D space. We upgrade the framework with a few\nmodifications just for fusing the feature from the previous frame with the\ncorresponding one in the current frame. In this way, with negligible extra\ncomputing budget, we enable the algorithm to access the temporal cues by\nquerying and comparing the two candidate features. Beyond this, we also\nsimplify the velocity learning task by removing the factors of ego-motion and\ntime, which equips BEVDet4D with robust generalization performance and reduces\nthe velocity error by 52.8%. This makes vision-based methods, for the first\ntime, become comparable with those relied on LiDAR or radar in this aspect. On\nchallenge benchmark nuScenes, we report a new record of 51.5% NDS with the\nhigh-performance configuration dubbed BEVDet4D-Base, which surpasses the\nprevious leading method BEVDet by +4.3% NDS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Learning of Graph Representations using Radar Point Cloud for Long-Range Gesture Recognition. (arXiv:2203.17066v1 [eess.SP])","link":"http://arxiv.org/abs/2203.17066","description":"<p>Gesture recognition is one of the most intuitive ways of interaction and has\ngathered particular attention for human computer interaction. Radar sensors\npossess multiple intrinsic properties, such as their ability to work in low\nillumination, harsh weather conditions, and being low-cost and compact, making\nthem highly preferable for a gesture recognition solution. However, most\nliterature work focuses on solutions with a limited range that is lower than a\nmeter. We propose a novel architecture for a long-range (1m - 2m) gesture\nrecognition solution that leverages a point cloud-based cross-learning approach\nfrom camera point cloud to 60-GHz FMCW radar point cloud, which allows learning\nbetter representations while suppressing noise. We use a variant of Dynamic\nGraph CNN (DGCNN) for the cross-learning, enabling us to model relationships\nbetween the points at a local and global level and to model the temporal\ndynamics a Bi-LSTM network is employed. In the experimental results section, we\ndemonstrate our model's overall accuracy of 98.4% for five gestures and its\ngeneralization capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hazra_S/0/1/0/all/0/1\">Souvik Hazra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_H/0/1/0/all/0/1\">Hao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiprit_G/0/1/0/all/0/1\">Gamze Naz Kiprit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stephan_M/0/1/0/all/0/1\">Michael Stephan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Servadei_L/0/1/0/all/0/1\">Lorenzo Servadei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wille_R/0/1/0/all/0/1\">Robert Wille</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weigel_R/0/1/0/all/0/1\">Robert Weigel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santra_A/0/1/0/all/0/1\">Avik Santra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CADG: A Model Based on Cross Attention for Domain Generalization. (arXiv:2203.17067v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17067","description":"<p>In Domain Generalization (DG) tasks, models are trained by using only\ntraining data from the source domains to achieve generalization on an unseen\ntarget domain, this will suffer from the distribution shift problem. So it's\nimportant to learn a classifier to focus on the common representation which can\nbe used to classify on multi-domains, so that this classifier can achieve a\nhigh performance on an unseen target domain as well. With the success of cross\nattention in various cross-modal tasks, we find that cross attention is a\npowerful mechanism to align the features come from different distributions. So\nwe design a model named CADG (cross attention for domain generalization),\nwherein cross attention plays a important role, to address distribution shift\nproblem. Such design makes the classifier can be adopted on multi-domains, so\nthe classifier will generalize well on an unseen domain. Experiments show that\nour proposed method achieves state-of-the-art performance on a variety of\ndomain generalization benchmarks compared with other single model and can even\nachieve a better performance than some ensemble-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1\">Cheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Donglin Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Hyperspectral Unmixing using Transformer Network. (arXiv:2203.17076v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17076","description":"<p>Currently, this paper is under review in IEEE. Transformers have intrigued\nthe vision research community with their state-of-the-art performance in\nnatural language processing. With their superior performance, transformers have\nfound their way in the field of hyperspectral image classification and achieved\npromising results. In this article, we harness the power of transformers to\nconquer the task of hyperspectral unmixing and propose a novel deep unmixing\nmodel with transformers. We aim to utilize the ability of transformers to\nbetter capture the global feature dependencies in order to enhance the quality\nof the endmember spectra and the abundance maps. The proposed model is a\ncombination of a convolutional autoencoder and a transformer. The hyperspectral\ndata is encoded by the convolutional encoder. The transformer captures\nlong-range dependencies between the representations derived from the encoder.\nThe data are reconstructed using a convolutional decoder. We applied the\nproposed unmixing model to three widely used unmixing datasets, i.e., Samson,\nApex, and Washington DC mall and compared it with the state-of-the-art in terms\nof root mean squared error and spectral angle distance. The source code for the\nproposed model will be made publicly available at\n\\url{https://github.com/preetam22n/DeepTrans-HSU}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Preetam Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Swalpa Kumar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koirala_B/0/1/0/all/0/1\">Bikram Koirala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1\">Behnood Rasti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheunders_P/0/1/0/all/0/1\">Paul Scheunders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AEGNN: Asynchronous Event-based Graph Neural Networks. (arXiv:2203.17149v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17149","description":"<p>The best performing learning algorithms devised for event cameras work by\nfirst converting events into dense representations that are then processed\nusing standard CNNs. However, these steps discard both the sparsity and high\ntemporal resolution of events, leading to high computational burden and\nlatency. For this reason, recent works have adopted Graph Neural Networks\n(GNNs), which process events as \"static\" spatio-temporal graphs, which are\ninherently \"sparse\". We take this trend one step further by introducing\nAsynchronous, Event-based Graph Neural Networks (AEGNNs), a novel\nevent-processing paradigm that generalizes standard GNNs to process events as\n\"evolving\" spatio-temporal graphs. AEGNNs follow efficient update rules that\nrestrict recomputation of network activations only to the nodes affected by\neach new event, thereby significantly reducing both computation and latency for\nevent-by-event processing. AEGNNs are easily trained on synchronous inputs and\ncan be converted to efficient, \"asynchronous\" networks at test time. We\nthoroughly validate our method on object classification and detection tasks,\nwhere we show an up to a 200-fold reduction in computational complexity\n(FLOPs), with similar or even better performance than state-of-the-art\nasynchronous methods. This reduction in computation directly translates to an\n8-fold reduction in computational latency when compared to standard GNNs, which\nopens the door to low-latency event-based processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_S/0/1/0/all/0/1\">Simon Schaefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Mean-Residue Loss for Robust Facial Age Estimation. (arXiv:2203.17156v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17156","description":"<p>Automated facial age estimation has diverse real-world applications in\nmultimedia analysis, e.g., video surveillance, and human-computer interaction.\nHowever, due to the randomness and ambiguity of the aging process, age\nassessment is challenging. Most research work over the topic regards the task\nas one of age regression, classification, and ranking problems, and cannot well\nleverage age distribution in representing labels with age ambiguity. In this\nwork, we propose a simple yet effective loss function for robust facial age\nestimation via distribution learning, i.e., adaptive mean-residue loss, in\nwhich, the mean loss penalizes the difference between the estimated age\ndistribution's mean and the ground-truth age, whereas the residue loss\npenalizes the entropy of age probability out of dynamic top-K in the\ndistribution. Experimental results in the datasets FG-NET and CLAP2016 have\nvalidated the effectiveness of the proposed loss. Our code is available at\nhttps://github.com/jacobzhaoziyuan/AMR-Loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peisheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yubo Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Equivariant Graph Implicit Functions. (arXiv:2203.17178v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17178","description":"<p>In recent years, neural implicit representations have made remarkable\nprogress in modeling of 3D shapes with arbitrary topology. In this work, we\naddress two key limitations of such representations, in failing to capture\nlocal 3D geometric fine details, and to learn from and generalize to shapes\nwith unseen 3D transformations. To this end, we introduce a novel family of\ngraph implicit functions with equivariant layers that facilitates modeling fine\nlocal details and guaranteed robustness to various groups of geometric\ntransformations, through local $k$-NN graph embeddings with sparse point set\nobservations at multiple resolutions. Our method improves over the existing\nrotation-equivariant implicit function from 0.69 to 0.89 (IoU) on the ShapeNet\nreconstruction task. We also show that our equivariant implicit function can be\nextended to other types of similarity transformations and generalizes to unseen\ntranslations and scaling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunlu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow and Multi-scale Fusion. (arXiv:2203.17191v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17191","description":"<p>Recently, video frame interpolation using a combination of frame- and\nevent-based cameras has surpassed traditional image-based methods both in terms\nof performance and memory efficiency. However, current methods still suffer\nfrom (i) brittle image-level fusion of complementary interpolation results,\nthat fails in the presence of artifacts in the fused image, (ii) potentially\ntemporally inconsistent and inefficient motion estimation procedures, that run\nfor every inserted frame and (iii) low contrast regions that do not trigger\nevents, and thus cause events-only motion estimation to generate artifacts.\nMoreover, previous methods were only tested on datasets consisting of planar\nand faraway scenes, which do not capture the full complexity of the real world.\nIn this work, we address the above problems by introducing multi-scale\nfeature-level fusion and computing one-shot non-linear inter-frame motion from\nevents and images, which can be efficiently sampled for image warping. We also\ncollect the first large-scale events and frames dataset consisting of more than\n100 challenging scenes with depth variations, captured with a new experimental\nsetup based on a beamsplitter. We show that our method improves the\nreconstruction quality by up to 0.2 dB in terms of PSNR and up to 15% in LPIPS\nscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Stepan Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bochicchio_A/0/1/0/all/0/1\">Alfredo Bochicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy. (arXiv:2203.17205v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17205","description":"<p>Self-supervised learning (SSL) methods aim to learn view-invariant\nrepresentations by maximizing the similarity between the features extracted\nfrom different crops of the same image regardless of cropping size and content.\nIn essence, this strategy ignores the fact that two crops may truly contain\ndifferent image information, e.g., background and small objects, and thus tends\nto restrain the diversity of the learned representations. %To this end, the\nexisting strategies typically employ loss functions that enforces the networks\nto discard part of valuable information, e.g. background and small objects, and\nsacrifices the diversity of representation. In this work, we address this issue\nby introducing a new self-supervised learning strategy, LoGo, that explicitly\nreasons about {\\bf Lo}cal and {\\bf G}l{\\bf o}bal crops. To achieve view\ninvariance, LoGo encourages similarity between global crops from the same\nimage, as well as between a global and a local crop. However, to correctly\nencode the fact that the content of smaller crops may differ entirely, LoGo\npromotes two local crops to have dissimilar representations, while being close\nto global crops. Our LoGo strategy can easily be applied to existing SSL\nmethods. Our extensive experiments on a variety of datasets and using different\nself-supervised learning frameworks validate its superiority over existing\napproaches. Noticeably, we achieve better results than supervised models on\ntransfer learning when using only $1/10$ of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Congpei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1\">Wei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimVQA: Exploring Simulated Environments for Visual Question Answering. (arXiv:2203.17219v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17219","description":"<p>Existing work on VQA explores data augmentation to achieve better\ngeneralization by perturbing the images in the dataset or modifying the\nexisting questions and answers. While these methods exhibit good performance,\nthe diversity of the questions and answers are constrained by the available\nimage set. In this work we explore using synthetic computer-generated data to\nfully control the visual and language space, allowing us to provide more\ndiverse scenarios. We quantify the effect of synthetic data in real-world VQA\nbenchmarks and to which extent it produces results that generalize to real\ndata. By exploiting 3D and physics simulation platforms, we provide a pipeline\nto generate synthetic data to expand and replace type-specific questions and\nanswers without risking the exposure of sensitive or personal data that might\nbe present in real images. We offer a comprehensive analysis while expanding\nexisting hyper-realistic datasets to be used for VQA. We also propose Feature\nSwapping (F-SWAP) -- where we randomly switch object-level features during\ntraining to make a VQA model more domain invariant. We show that F-SWAP is\neffective for enhancing a currently existing VQA dataset of real images without\ncompromising on the accuracy to answer existing questions in the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1\">Paola Cascante-Bonilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Letao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions. (arXiv:2203.17234v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17234","description":"<p>We present a method that can recognize new objects and estimate their 3D pose\nin RGB images even under partial occlusions. Our method requires neither a\ntraining phase on these objects nor real images depicting them, only their CAD\nmodels. It relies on a small set of training objects to learn local object\nrepresentations, which allow us to locally match the input image to a set of\n\"templates\", rendered images of the CAD models for the new objects. In contrast\nwith the state-of-the-art methods, the new objects on which our method is\napplied can be very different from the training objects. As a result, we are\nthe first to show generalization without retraining on the LINEMOD and\nOcclusion-LINEMOD datasets. Our analysis of the failure modes of previous\ntemplate-based approaches further confirms the benefits of local features for\ntemplate matching. We outperform the state-of-the-art template matching methods\non the LINEMOD, Occlusion-LINEMOD and T-LESS datasets. Our source code and data\nare publicly available at https://github.com/nv-nguyen/template-pose\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinlin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImpDet: Exploring Implicit Fields for 3D Object Detection. (arXiv:2203.17240v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17240","description":"<p>Conventional 3D object detection approaches concentrate on bounding boxes\nrepresentation learning with several parameters, i.e., localization, dimension,\nand orientation. Despite its popularity and universality, such a\nstraightforward paradigm is sensitive to slight numerical deviations,\nespecially in localization. By exploiting the property that point clouds are\nnaturally captured on the surface of objects along with accurate location and\nintensity information, we introduce a new perspective that views bounding box\nregression as an implicit function. This leads to our proposed framework,\ntermed Implicit Detection or ImpDet, which leverages implicit field learning\nfor 3D object detection. Our ImpDet assigns specific values to points in\ndifferent local 3D spaces, thereby high-quality boundaries can be generated by\nclassifying points inside or outside the boundary. To solve the problem of\nsparsity on the object surface, we further present a simple yet efficient\nvirtual sampling strategy to not only fill the empty region, but also learn\nrich semantic features to help refine the boundaries. Extensive experimental\nresults on KITTI and Waymo benchmarks demonstrate the effectiveness and\nrobustness of unifying implicit fields into object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuelin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17247","description":"<p>Breakthroughs in transformer-based models have revolutionized not only the\nNLP field, but also vision and multimodal systems. However, although\nvisualization and interpretability tools have become available for NLP models,\ninternal mechanisms of vision and multimodal transformers remain largely\nopaque. With the success of these transformers, it is increasingly critical to\nunderstand their inner workings, as unraveling these black-boxes will lead to\nmore capable and trustworthy models. To contribute to this quest, we propose\nVL-InterpreT, which provides novel interactive visualizations for interpreting\nthe attentions and hidden representations in multimodal transformers.\nVL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety\nof statistics in attention heads throughout all layers for both vision and\nlanguage components, (2) visualizes cross-modal and intra-modal attentions\nthrough easily readable heatmaps, and (3) plots the hidden representations of\nvision and language tokens as they pass through the transformer layers. In this\npaper, we demonstrate the functionalities of VL-InterpreT through the analysis\nof KD-VLP, an end-to-end pretraining vision-language multimodal\ntransformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and\nWebQA, two visual question answering benchmarks. Furthermore, we also present a\nfew interesting findings about multimodal transformer behaviors that were\nlearned through our tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aflalo_E/0/1/0/all/0/1\">Estelle Aflalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Meng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_S/0/1/0/all/0/1\">Shao-Yen Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Scene Representations for Embodied AI. (arXiv:2203.17251v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17251","description":"<p>We propose Continuous Scene Representations (CSR), a scene representation\nconstructed by an embodied agent navigating within a space, where objects and\ntheir relationships are modeled by continuous valued embeddings. Our method\ncaptures feature relationships between objects, composes them into a graph\nstructure on-the-fly, and situates an embodied agent within the representation.\nOur key insight is to embed pair-wise relationships between objects in a latent\nspace. This allows for a richer representation compared to discrete relations\n(e.g., [support], [next-to]) commonly used for building scene representations.\nCSR can track objects as the agent moves in a scene, update the representation\naccordingly, and detect changes in room configurations. Using CSR, we\noutperform state-of-the-art approaches for the challenging downstream task of\nvisual room rearrangement, without any task specific training. Moreover, we\nshow the learned embeddings capture salient spatial details of the scene and\nshow applicability to real world data. A summery video and code is available at\nhttps://prior.allenai.org/projects/csr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v1 [q-bio.NC])","link":"http://arxiv.org/abs/2203.17255","description":"<p>This theoretical article examines how to construct human-like working memory\nand thought processes within a computer. There should be two working memory\nstores, one analogous to sustained firing in association cortex, and one\nanalogous to synaptic potentiation in the cerebral cortex. These stores must be\nconstantly updated with new representations that arise from either\nenvironmental stimulation or internal processing. They should be updated\ncontinuously, and in an iterative fashion, meaning that, in the next state,\nsome items in the set of coactive items should always be retained. Thus, the\nset of concepts coactive in working memory will evolve gradually and\nincrementally over time. This makes each state is a revised iteration of the\npreceding state and causes successive states to overlap and blend with respect\nto the set of representations they contain. As new representations are added\nand old ones are subtracted, some remain active for several seconds over the\ncourse of these changes. This persistent activity, similar to that used in\nartificial recurrent neural networks, is used to spread activation energy\nthroughout the global workspace to search for the next associative update. The\nresult is a chain of associatively linked intermediate states that are capable\nof advancing toward a solution or goal. Iterative updating is conceptualized\nhere as an information processing strategy, a computational and\nneurophysiological determinant of the stream of thought, and an algorithm for\ndesigning and programming artificial intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1\">Jared Edward Reser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Video Salient Object Ranking. (arXiv:2203.17257v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17257","description":"<p>Salient Object Ranking (SOR) involves ranking the degree of saliency of\nmultiple salient objects in an input image. Most recently, a method is proposed\nfor ranking salient objects in an input video based on a predicted fixation\nmap. It relies solely on the density of the fixations within the salient\nobjects to infer their saliency ranks, which is incompatible with human\nperception of saliency ranking. In this work, we propose to explicitly learn\nthe spatial and temporal relations between different salient objects to produce\nthe saliency ranks. To this end, we propose an end-to-end method for video\nsalient object ranking (VSOR), with two novel modules: an intra-frame adaptive\nrelation (IAR) module to learn the spatial relation among the salient objects\nin the same frame locally and globally, and an inter-frame dynamic relation\n(IDR) module to model the temporal relation of saliency across different\nframes. In addition, to address the limited video types (just sports and\nmovies) and scene diversity in the existing VSOR dataset, we propose a new\ndataset that covers different video types and diverse scenes on a large scale.\nExperimental results demonstrate that our method outperforms state-of-the-art\nmethods in relevant fields. We will make the source code and our proposed\ndataset available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Huankang Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating High Fidelity Data from Low-density Regions using Diffusion Models. (arXiv:2203.17260v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17260","description":"<p>Our work focuses on addressing sample deficiency from low-density regions of\ndata manifold in common image datasets. We leverage diffusion process based\ngenerative models to synthesize novel images from low-density regions. We\nobserve that uniform sampling from diffusion models predominantly samples from\nhigh-density regions of the data manifold. Therefore, we modify the sampling\nprocess to guide it towards low-density regions while simultaneously\nmaintaining the fidelity of synthetic data. We rigorously demonstrate that our\nprocess successfully generates novel high fidelity samples from low-density\nregions. We further examine generated samples and show that the model does not\nmemorize low-density data and indeed learns to generate novel samples from\nlow-density regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1\">Vikash Sehwag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazirbas_C/0/1/0/all/0/1\">Caner Hazirbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordo_A/0/1/0/all/0/1\">Albert Gordo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgenel_F/0/1/0/all/0/1\">Firat Ozgenel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17261","description":"<p>Recent research explosion on Neural Radiance Field (NeRF) shows the\nencouraging potential to represent complex scenes with neural networks. One\nmajor drawback of NeRF is its prohibitive inference time: Rendering a single\npixel requires querying the NeRF network hundreds of times. To resolve it,\nexisting efforts mainly attempt to reduce the number of required sampled\npoints. However, the problem of iterative sampling still exists. On the other\nhand, Neural Light Field (NeLF) presents a more straightforward representation\nover NeRF in novel view synthesis -- the rendering of a pixel amounts to one\nsingle forward pass without ray-marching. In this work, we present a deep\nresidual MLP network (88 layers) to effectively learn the light field. We show\nthe key to successfully learning such a deep NeLF network is to have sufficient\ndata, for which we transfer the knowledge from a pre-trained NeRF model via\ndata distillation. Extensive experiments on both synthetic and real-world\nscenes show the merits of our method over other counterpart algorithms. On the\nsynthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x\nruntime speedup, meanwhile delivering significantly better (1.4-2.8 dB average\nPSNR improvement) rendering quality than NeRF without any customized\nimplementation tricks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement by Re-Synthesis. (arXiv:2203.17263v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17263","description":"<p>Since facial actions such as lip movements contain significant information\nabout speech content, it is not surprising that audio-visual speech enhancement\nmethods are more accurate than their audio-only counterparts. Yet,\nstate-of-the-art approaches still struggle to generate clean, realistic speech\nwithout noise artifacts and unnatural distortions in challenging acoustic\nenvironments. In this paper, we propose a novel audio-visual speech enhancement\nframework for high-fidelity telecommunications in AR/VR. Our approach leverages\naudio-visual speech cues to generate the codes of a neural speech codec,\nenabling efficient synthesis of clean, realistic speech from noisy signals.\nGiven the importance of speaker-specific cues in speech, we focus on developing\npersonalized models that work well for individual speakers. We demonstrate the\nefficacy of our approach on a new audio-visual speech dataset collected in an\nunconstrained, large vocabulary setting, as well as existing audio-visual\ndatasets, outperforming speech enhancement baselines on both quantitative\nmetrics and human evaluation studies. Please see the supplemental video for\nqualitative results at\nhttps://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Karren Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1\">Dejan Markovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krenn_S/0/1/0/all/0/1\">Steven Krenn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_V/0/1/0/all/0/1\">Vasu Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1\">Alexander Richard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing. (arXiv:2203.17266v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17266","description":"<p>Recent advances like StyleGAN have promoted the growth of controllable facial\nediting. To address its core challenge of attribute decoupling in a single\nlatent space, attempts have been made to adopt dual-space GAN for better\ndisentanglement of style and content representations. Nonetheless, these\nmethods are still incompetent to obtain plausible editing results with high\ncontrollability, especially for complicated attributes. In this study, we\nhighlight the importance of interaction in a dual-space GAN for more\ncontrollable editing. We propose TransEditor, a novel Transformer-based\nframework to enhance such interaction. Besides, we develop a new dual-space\nediting and inversion strategy to provide additional editing flexibility.\nExtensive experiments demonstrate the superiority of the proposed framework in\nimage quality and editing capability, suggesting the effectiveness of\nTransEditor for highly controllable facial editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yueqin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chengyao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v1 [cs.LG])","link":"http://arxiv.org/abs/2203.17269","description":"<p>Continual learning describes a setting where machine learning models learn\nnovel concepts from continuously shifting training data, while simultaneously\navoiding degradation of knowledge on previously seen classes (a phenomenon\nknown as the catastrophic forgetting problem) which may disappear from the\ntraining data for extended periods of time. Current approaches for continual\nlearning of a single expanding task (aka class-incremental continual learning)\nrequire extensive rehearsal of previously seen data to avoid this degradation\nof knowledge. Unfortunately, rehearsal comes at a sharp cost to memory and\ncomputation, and it may also violate data-privacy. Instead, we explore\ncombining knowledge distillation and parameter regularization in new ways to\nachieve strong continual learning performance without rehearsal. Specifically,\nwe take a deep dive into common continual learning techniques: prediction\ndistillation, feature distillation, L2 parameter regularization, and EWC\nparameter regularization. We first disprove the common assumption that\nparameter regularization techniques fail for rehearsal-free continual learning\nof a single, expanding task. Next, we explore how to leverage knowledge from a\npre-trained model in rehearsal-free continual learning and find that vanilla L2\nparameter regularization outperforms EWC parameter regularization and feature\ndistillation. We then highlight the impact of the rehearsal-free continual\nlearning settings with a classifier expansion benchmark, showing that a\nstrategy based on our findings combined with a positive/negative label\nbalancing heuristic can close the performance gap between the upper bound and\nthe existing strategies by up to roughly 50%. Finally, we show that a simple\nmethod consisting of pre-training, L2 regularization, and prediction\ndistillation can even outperform rehearsal-based methods on the common\nCIFAR-100 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Seale Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers. (arXiv:2203.17270v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17270","description":"<p>3D visual perception tasks, including 3D detection and map segmentation based\non multi-camera images, are essential for autonomous driving systems. In this\nwork, we present a new framework termed BEVFormer, which learns unified BEV\nrepresentations with spatiotemporal transformers to support multiple autonomous\ndriving perception tasks. In a nutshell, BEVFormer exploits both spatial and\ntemporal information by interacting with spatial and temporal space through\npredefined grid-shaped BEV queries. To aggregate spatial information, we design\na spatial cross-attention that each BEV query extracts the spatial features\nfrom the regions of interest across camera views. For temporal information, we\npropose a temporal self-attention to recurrently fuse the history BEV\ninformation. Our approach achieves the new state-of-the-art 56.9\\% in terms of\nNDS metric on the nuScenes test set, which is 9.0 points higher than previous\nbest arts and on par with the performance of LiDAR-based baselines. We further\nshow that BEVFormer remarkably improves the accuracy of velocity estimation and\nrecall of objects under low visibility conditions. The code will be released at\nhttps://github.com/zhiqi-li/BEVFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sima_C/0/1/0/all/0/1\">Chonghao Sima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Vision-Language Pretrained Models Learn Primitive Concepts?. (arXiv:2203.17271v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17271","description":"<p>Vision-language pretrained models have achieved impressive performance on\nmultimodal reasoning and zero-shot recognition tasks. Many of these VL models\nare pretrained on unlabeled image and caption pairs from the internet. In this\npaper, we study whether the notion of primitive concepts, such as color and\nshape attributes, emerges automatically from these pretrained VL models. We\npropose to learn compositional derivations that map primitive concept\nactivations into composite concepts, a task which we demonstrate to be\nstraightforward given true primitive concept annotations. This compositional\nderivation learning (CompDL) framework allows us to quantitively measure the\nusefulness and interpretability of the learned derivations, by jointly\nconsidering the entire set of candidate primitive concepts. Our study reveals\nthat state-of-the-art VL pretrained models learn primitive concepts that are\nhighly useful as visual descriptors, as demonstrated by their strong\nperformance on fine-grained visual recognition tasks, but those concepts\nstruggle to provide interpretable compositional derivations, which highlights\nlimitations of existing VL models. Code and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1\">Tian Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalla_U/0/1/0/all/0/1\">Usha Bhalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MyStyle: A Personalized Generative Prior. (arXiv:2203.17272v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17272","description":"<p>We introduce MyStyle, a personalized deep generative prior trained with a few\nshots of an individual. MyStyle allows to reconstruct, enhance and edit images\nof a specific person, such that the output is faithful to the person's key\nfacial characteristics. Given a small reference set of portrait images of a\nperson (~100), we tune the weights of a pretrained StyleGAN face generator to\nform a local, low-dimensional, personalized manifold in the latent space. We\nshow that this manifold constitutes a personalized region that spans latent\ncodes associated with diverse portrait images of the individual. Moreover, we\ndemonstrate that we obtain a personalized generative prior, and propose a\nunified approach to apply it to various ill-posed image enhancement problems,\nsuch as inpainting and super-resolution, as well as semantic editing. Using the\npersonalized generative prior we obtain outputs that exhibit high-fidelity to\nthe input images and are also faithful to the key facial characteristics of the\nindividual in the reference set. We demonstrate our method with fair-use images\nof numerous widely recognizable individuals for whom we have the prior\nknowledge for a qualitative evaluation of the expected outcome. We evaluate our\napproach against few-shots baselines and show that our personalized prior,\nquantitatively and qualitatively, outperforms state-of-the-art alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1\">Yotam Nitzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qiurui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liba_O/0/1/0/all/0/1\">Orly Liba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarom_M/0/1/0/all/0/1\">Michal Yarom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1\">Yossi Gandelsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1\">Inbar Mosseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1\">Yael Pritch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_or_D/0/1/0/all/0/1\">Daniel Cohen-or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FindIt: Generalized Localization with Natural Language Queries. (arXiv:2203.17273v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17273","description":"<p>We propose FindIt, a simple and versatile framework that unifies a variety of\nvisual grounding and localization tasks including referring expression\ncomprehension, text-based localization, and object detection. Key to our\narchitecture is an efficient multi-scale fusion module that unifies the\ndisparate localization requirements across the tasks. In addition, we discover\nthat a standard object detector is surprisingly effective in unifying these\ntasks without a need for task-specific design, losses, or pre-computed\ndetections. Our end-to-end trainable framework responds flexibly and accurately\nto a wide range of referring expression, localization or detection queries for\nzero, one, or multiple objects. Jointly trained on these tasks, FindIt\noutperforms the state of the art on both referring expression and text-based\nlocalization, and shows competitive performance on object detection. Finally,\nFindIt generalizes better to out-of-distribution data and novel categories\ncompared to strong single-task baselines. All of these are accomplished by a\nsingle, unified and efficient model. The code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertsch_F/0/1/0/all/0/1\">Fred Bertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1\">Mohammad Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models. (arXiv:2203.17274v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17274","description":"<p>Prompting has recently become a popular paradigm for adapting language models\nto downstream tasks. Rather than fine-tuning model parameters or adding\ntask-specific heads, this approach steers a model to perform a new task simply\nby adding a text prompt to the model's inputs. In this paper, we explore the\nquestion: can we create prompts with pixels instead? In other words, can\npre-trained vision models be adapted to a new task solely by adding pixels to\ntheir inputs? We introduce visual prompting, which learns a task-specific image\nperturbation such that a frozen pre-trained model prompted with this\nperturbation performs a new task. We discover that changing only a few pixels\nis enough to adapt models to new tasks and datasets, and performs on par with\nlinear probing, the current de facto approach to lightweight adaptation. The\nsurprising effectiveness of visual prompting provides a new perspective on how\nto adapt pre-trained models in vision, and opens up the possibility of adapting\nmodels solely through their inputs, which, unlike model parameters or outputs,\nare typically under an end-user's control. Code is available at\n<a href=\"http://hjbahng.github.io/visual_prompting\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahng_H/0/1/0/all/0/1\">Hyojin Bahng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanian_A/0/1/0/all/0/1\">Ali Jahanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_S/0/1/0/all/0/1\">Swami Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools. (arXiv:2203.17275v1 [cs.LG])","link":"http://arxiv.org/abs/2203.17275","description":"<p>We consider the problem of sequential robotic manipulation of deformable\nobjects using tools. Previous works have shown that differentiable physics\nsimulators provide gradients to the environment state and help trajectory\noptimization to converge orders of magnitude faster than model-free\nreinforcement learning algorithms for deformable object manipulation. However,\nsuch gradient-based trajectory optimization typically requires access to the\nfull simulator states and can only solve short-horizon, single-skill tasks due\nto local optima. In this work, we propose a novel framework, named DiffSkill,\nthat uses a differentiable physics simulator for skill abstraction to solve\nlong-horizon deformable object manipulation tasks from sensory observations. In\nparticular, we first obtain short-horizon skills using individual tools from a\ngradient-based optimizer, using the full state information in a differentiable\nsimulator; we then learn a neural skill abstractor from the demonstration\ntrajectories which takes RGBD images as input. Finally, we plan over the skills\nby finding the intermediate goals and then solve long-horizon tasks. We show\nthe advantages of our method in a new set of sequential deformable object\nmanipulation tasks compared to previous reinforcement learning algorithms and\ncompared to the trajectory optimizer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xingyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing Old Films Back to Life. (arXiv:2203.17276v1 [cs.CV])","link":"http://arxiv.org/abs/2203.17276","description":"<p>We present a learning-based framework, recurrent transformer network (RTN),\nto restore heavily degraded old films. Instead of performing frame-wise\nrestoration, our method is based on the hidden knowledge learned from adjacent\nframes that contain abundant information about the occlusion, which is\nbeneficial to restore challenging artifacts of each frame while ensuring\ntemporal coherency. Moreover, contrasting the representation of the current\nframe and the hidden knowledge makes it possible to infer the scratch position\nin an unsupervised manner, and such defect localization generalizes well to\nreal-world degradations. To better resolve mixed degradation and compensate for\nthe flow estimation error during frame alignment, we propose to leverage more\nexpressive transformer blocks for spatial restoration. Experiments on both\nsynthetic dataset and real-world old films demonstrate the significant\nsuperiority of the proposed RTN over existing solutions. In addition, the same\nframework can effectively propagate the color from keyframes to the whole\nvideo, ultimately yielding compelling restored films. The implementation and\nmodel will be released at\nhttps://github.com/raywzy/Bringing-Old-Films-Back-to-Life.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Ziyu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Part Mining for Fine-grained Image Classification. (arXiv:1902.09941v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1902.09941","description":"<p>Fine-grained image classification remains challenging due to the large\nintra-class variance and small inter-class variance. Since the subtle visual\ndifferences are only in local regions of discriminative parts among\nsubcategories, part localization is a key issue for fine-grained image\nclassification. Most existing approaches localize object or parts in an image\nwith object or part annotations, which are expensive and labor-consuming. To\ntackle this issue, we propose a fully unsupervised part mining (UPM) approach\nto localize the discriminative parts without even image-level annotations,\nwhich largely improves the fine-grained classification performance. We first\nutilize pattern mining techniques to discover frequent patterns, i.e.,\nco-occurrence highlighted regions, in the feature maps extracted from a\npre-trained convolutional neural network (CNN) model. Inspired by the fact that\nthese relevant meaningful patterns typically hold appearance and spatial\nconsistency, we then cluster the mined regions to obtain the cluster centers\nand the discriminative parts surrounding the cluster centers are generated.\nImportantly, any annotations and sophisticated training procedures are not used\nin our proposed part localization approach. Finally, a multi-stream\nclassification network is built for aggregating the original, object-level and\npart-level features simultaneously. Compared with other state-of-the-art\napproaches, our UPM approach achieves the competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Runsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zhang_j/0/1/0/all/0/1\">jian zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qi Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v5 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/1907.02704","description":"<p>A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDN: Salient Object Detection via Extremely-Downsampled Network. (arXiv:2012.13093v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13093","description":"<p>Recent progress on salient object detection (SOD) mainly benefits from\nmulti-scale learning, where the high-level and low-level features collaborate\nin locating salient objects and discovering fine details, respectively.\nHowever, most efforts are devoted to low-level feature learning by fusing\nmulti-scale features or enhancing boundary representations. High-level\nfeatures, which although have long proven effective for many other tasks, yet\nhave been barely studied for SOD. In this paper, we tap into this gap and show\nthat enhancing high- level features is essential for SOD as well. To this end,\nwe introduce an Extremely-Downsampled Network (EDN), which employs an extreme\ndownsampling technique to effectively learn a global view of the whole image,\nleading to accurate salient object localization. To accomplish better\nmulti-level feature fusion, we construct the Scale-Correlated Pyramid\nConvolution (SCPC) to build an elegant decoder for recovering object details\nfrom the above extreme downsampling. Extensive experiments demonstrate that EDN\nachieves state-of-the-art performance with real-time speed. Our efficient\nEDN-Lite also achieves competitive performance with a speed of 316fps. Hence,\nthis work is expected to spark some new thinking in SOD. Code is available at\nhttps://github.com/yuhuan-wu/EDN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localization Distillation for Dense Object Detection. (arXiv:2102.12252v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.12252","description":"<p>Knowledge distillation (KD) has witnessed its powerful capability in learning\ncompact models in object detection. Previous KD methods for object detection\nmostly focus on imitating deep features within the imitation regions instead of\nmimicking classification logit due to its inefficiency in distilling\nlocalization information and trivial improvement. In this paper, by\nreformulating the knowledge distillation process on localization, we present a\nnovel localization distillation (LD) method which can efficiently transfer the\nlocalization knowledge from the teacher to the student. Moreover, we also\nheuristically introduce the concept of valuable localization region that can\naid to selectively distill the semantic and localization knowledge for a\ncertain region. Combining these two new components, for the first time, we show\nthat logit mimicking can outperform feature imitation and localization\nknowledge distillation is more important and efficient than semantic knowledge\nfor distilling object detectors. Our distillation scheme is simple as well as\neffective and can be easily applied to different dense object detectors.\nExperiments show that our LD can boost the AP score of GFocal-ResNet-50 with a\nsingle-scale 1x training schedule from 40.1 to 42.1 on the COCO benchmark\nwithout any sacrifice on the inference speed. Our source code and trained\nmodels are publicly available at https://github.com/HikariTJU/LD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaohui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rongguang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cloth-Changing Person Re-identification from A Single Image with Gait Prediction and Regularization. (arXiv:2103.15537v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15537","description":"<p>Cloth-Changing person re-identification (CC-ReID) aims at matching the same\nperson across different locations over a long-duration, e.g., over days, and\ntherefore inevitably meets challenge of changing clothing. In this paper, we\nfocus on handling well the CC-ReID problem under a more challenging setting,\ni.e., just from a single image, which enables high-efficiency and latency-free\npedestrian identify for real-time surveillance applications. Specifically, we\nintroduce Gait recognition as an auxiliary task to drive the Image ReID model\nto learn cloth-agnostic representations by leveraging personal unique and\ncloth-independent gait information, we name this framework as GI-ReID. GI-ReID\nadopts a two-stream architecture that consists of a image ReID-Stream and an\nauxiliary gait recognition stream (Gait-Stream). The Gait-Stream, that is\ndiscarded in the inference for high computational efficiency, acts as a\nregulator to encourage the ReID-Stream to capture cloth-invariant biometric\nmotion features during the training. To get temporal continuous motion cues\nfrom a single image, we design a Gait Sequence Prediction (GSP) module for\nGait-Stream to enrich gait information. Finally, a high-level semantics\nconsistency over two streams is enforced for effective knowledge\nregularization. Experiments on multiple image-based Cloth-Changing ReID\nbenchmarks, e.g., LTCC, PRCC, Real28, and VC-Clothes, demonstrate that GI-ReID\nperforms favorably against the state-of-the-arts. Codes are available at\nhttps://github.com/jinx-USTC/GI-ReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhiheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DINE: Domain Adaptation from Single and Multiple Black-box Predictors. (arXiv:2104.01539v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01539","description":"<p>To ease the burden of labeling, unsupervised domain adaptation (UDA) aims to\ntransfer knowledge in previous and related labeled datasets (sources) to a new\nunlabeled dataset (target). Despite impressive progress, prior methods always\nneed to access the raw source data and develop data-dependent alignment\napproaches to recognize the target samples in a transductive learning manner,\nwhich may raise privacy concerns from source individuals. Several recent\nstudies resort to an alternative solution by exploiting the well-trained\nwhite-box model from the source domain, yet, it may still leak the raw data\nthrough generative adversarial learning. This paper studies a practical and\ninteresting setting for UDA, where only black-box source models (i.e., only\nnetwork predictions are available) are provided during adaptation in the target\ndomain. To solve this problem, we propose a new two-step knowledge adaptation\nframework called DIstill and fine-tuNE (DINE). Taking into consideration the\ntarget data structure, DINE first distills the knowledge from the source\npredictor to a customized target model, then fine-tunes the distilled model to\nfurther fit the target domain. Besides, neural networks are not required to be\nidentical across domains in DINE, even allowing effective adaptation on a\nlow-resource device. Empirical results on three UDA scenarios (i.e.,\nsingle-source, multi-source, and partial-set) confirm that DINE achieves highly\ncompetitive performance compared to state-of-the-art data-dependent approaches.\nCode is available at \\url{https://github.com/tim-learn/DINE/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dapeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart. (arXiv:2105.14785v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14785","description":"<p>Correctly classifying adversarial examples is an essential but challenging\nrequirement for safely deploying machine learning models. As reported in\nRobustBench, even the state-of-the-art adversarially trained models struggle to\nexceed 67% robust test accuracy on CIFAR-10, which is far from practical. A\ncomplementary way towards robustness is to introduce a rejection option,\nallowing the model to not return predictions on uncertain inputs, where\nconfidence is a commonly used certainty proxy. Along with this routine, we find\nthat confidence and a rectified confidence (R-Con) can form two coupled\nrejection metrics, which could provably distinguish wrongly classified inputs\nfrom correctly classified ones. This intriguing property sheds light on using\ncoupling strategies to better detect and reject adversarial examples. We\nevaluate our rectified rejection (RR) module on CIFAR-10, CIFAR-10-C, and\nCIFAR-100 under several attacks including adaptive ones, and demonstrate that\nthe RR module is compatible with different adversarial training frameworks on\nimproving robustness, with little extra computation. The code is available at\nhttps://github.com/P2333/Rectified-Rejection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RegionViT: Regional-to-Local Attention for Vision Transformers. (arXiv:2106.02689v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02689","description":"<p>Vision transformer (ViT) has recently shown its strong capability in\nachieving comparable results to convolutional neural networks (CNNs) on image\nclassification. However, vanilla ViT simply inherits the same architecture from\nthe natural language processing directly, which is often not optimized for\nvision applications. Motivated by this, in this paper, we propose a new\narchitecture that adopts the pyramid structure and employ a novel\nregional-to-local attention rather than global self-attention in vision\ntransformers. More specifically, our model first generates regional tokens and\nlocal tokens from an image with different patch sizes, where each regional\ntoken is associated with a set of local tokens based on the spatial location.\nThe regional-to-local attention includes two steps: first, the regional\nself-attention extract global information among all regional tokens and then\nthe local self-attention exchanges the information among one regional token and\nthe associated local tokens via self-attention. Therefore, even though local\nself-attention confines the scope in a local region but it can still receive\nglobal information. Extensive experiments on four vision tasks, including image\nclassification, object and keypoint detection, semantics segmentation and\naction recognition, show that our approach outperforms or is on par with\nstate-of-the-art ViT variants including many concurrent works. Our source codes\nand models are available at https://github.com/ibm/regionvit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Quanfu Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Flows with Invertible Attentions. (arXiv:2106.03959v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03959","description":"<p>Flow-based generative models have shown an excellent ability to explicitly\nlearn the probability density function of data via a sequence of invertible\ntransformations. Yet, learning attentions in generative flows remains\nunderstudied, while it has made breakthroughs in other domains. To fill the\ngap, this paper introduces two types of invertible attention mechanisms, i.e.,\nmap-based and transformer-based attentions, for both unconditional and\nconditional generative flows. The key idea is to exploit a masked scheme of\nthese two attentions to learn long-range data dependencies in the context of\ngenerative flows. The masked scheme allows for invertible attention modules\nwith tractable Jacobian determinants, enabling its seamless integration at any\npositions of the flow-based models. The proposed attention mechanisms lead to\nmore efficient generative flows, due to their capability of modeling the\nlong-term data dependencies. Evaluation on multiple image synthesis tasks shows\nthat the proposed attention flows result in efficient models and compare\nfavorably against the state-of-the-art unconditional and conditional generative\nflows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1\">Rhea Sanjay Sukthanker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09212","description":"<p>Video transformers have recently emerged as a competitive alternative to 3D\nCNNs for video understanding. However, due to their large number of parameters\nand reduced inductive biases, these models require supervised pretraining on\nlarge-scale image datasets to achieve top performance. In this paper, we\nempirically demonstrate that self-supervised pretraining of video transformers\non video-only datasets can lead to action recognition results that are on par\nor better than those obtained with supervised pretraining on large-scale image\ndatasets, even massive ones such as ImageNet-21K. Since transformer-based\nmodels are effective at capturing dependencies over extended temporal spans, we\npropose a simple learning procedure that forces the model to match a long-term\nview to a short-term view of the same video. Our approach, named Long-Short\nTemporal Contrastive Learning (LSTCL), enables video transformers to learn an\neffective clip-level representation by predicting temporal context captured\nfrom a longer temporal extent. To demonstrate the generality of our findings,\nwe implement and validate our approach under three different self-supervised\ncontrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct\nvideo-transformer architectures, including an improved variant of the Swin\nTransformer augmented with space-time attention. We conduct a thorough ablation\nstudy and show that LSTCL achieves competitive performance on multiple video\nbenchmarks and represents a convincing alternative to supervised image-based\npretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Du Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10811","description":"<p>Shape implicit neural representations (INRs) have recently shown to be\neffective in shape analysis and reconstruction tasks. Existing INRs require\npoint coordinates to learn the implicit level sets of the shape. When a normal\nvector is available for each point, a higher fidelity representation can be\nlearned, however normal vectors are often not provided as raw data.\nFurthermore, the method's initialization has been shown to play a crucial role\nfor surface reconstruction. In this paper, we propose a divergence guided shape\nrepresentation learning approach that does not require normal vectors as input.\nWe show that incorporating a soft constraint on the divergence of the distance\nfunction favours smooth solutions that reliably orients gradients to match the\nunknown normal at each point, in some cases even better than approaches that\nuse ground truth normal vectors directly. Additionally, we introduce a novel\ngeometric initialization method for sinusoidal INRs that further improves\nconvergence to the desired solution. We evaluate the effectiveness of our\napproach on the task of surface reconstruction and shape space learning and\nshow SOTA performance compared to other unoriented methods. Code and model\nparameters available at our project page https://chumbyte.github.io/DiGS-Site/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Shabat_Y/0/1/0/all/0/1\">Yizhak Ben-Shabat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koneputugodage_C/0/1/0/all/0/1\">Chamin Hewa Koneputugodage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2106.11821","description":"<p>In this paper we study data augmentation for opcode sequence based Android\nmalware detection. Data augmentation has been successfully used in many areas\nof deep-learning to significantly improve model performance. Typically, data\naugmentation simulates realistic variations in data to increase the apparent\ndiversity of the training-set. However, for opcode-based malware analysis it is\nnot immediately clear how to apply data augmentation. Hence we first study the\nuse of fixed transformations, then progress to adaptive methods. We propose a\nnovel data augmentation method -- Self-Embedding Language Model Augmentation --\nthat uses a malware detection network's own opcode embedding layer to measure\nopcode similarity for adaptive augmentation. To the best of our knowledge this\nis the first paper to carry out a systematic study of different augmentation\nmethods for opcode sequence based Android malware classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1\">Niall McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1\">Jesus Martinez del Rincon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.04488","description":"<p>Convolutional Neural Networks (CNNs) models are one of the most frequently\nused deep learning networks, and extensively used in both academia and\nindustry. Recent studies demonstrated that adversarial attacks against such\nmodels can maintain their effectiveness even when used on models other than the\none targeted by the attacker. This major property is known as transferability,\nand makes CNNs ill-suited for security applications. In this paper, we provide\nthe first comprehensive study which assesses the robustness of CNN-based models\nfor computer networks against adversarial transferability. Furthermore, we\ninvestigate whether the transferability property issue holds in computer\nnetworks applications. In our experiments, we first consider five different\nattacks: the Iterative Fast Gradient Method (I-FGSM), the Jacobian-based\nSaliency Map (JSMA), the Limited-memory Broyden Fletcher Goldfarb Shanno BFGS\n(L- BFGS), the Projected Gradient Descent (PGD), and the DeepFool attack. Then,\nwe perform these attacks against three well- known datasets: the Network-based\nDetection of IoT (N-BaIoT) dataset, the Domain Generating Algorithms (DGA)\ndataset, and the RIPE Atlas dataset. Our experimental results show clearly that\nthe transferability happens in specific use cases for the I- FGSM, the JSMA,\nand the LBFGS attack. In such scenarios, the attack success rate on the target\nnetwork range from 63.00% to 100%. Finally, we suggest two shielding strategies\nto hinder the attack transferability, by considering the Most Powerful Attacks\n(MPAs), and the mismatch LSTM architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekdad_Y/0/1/0/all/0/1\">Yassine Mekdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berenjestanaki_M/0/1/0/all/0/1\">Mohammad Hajian Berenjestanaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergougui_A/0/1/0/all/0/1\">Abdeslam EL Fergougui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Rounding for Image Interpolation and Scan Conversion. (arXiv:2110.12983v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2110.12983","description":"<p>The stochastic rounding (SR) function is proposed to evaluate and demonstrate\nthe effects of stochastically rounding row and column subscripts in image\ninterpolation and scan conversion. The proposed SR function is based on a\npseudorandom number, enabling the pseudorandom rounding up or down any\nnon-integer row and column subscripts. Also, the SR function exceptionally\nenables rounding up any possible cases of subscript inputs that are inferior to\na pseudorandom number. The algorithm of interest is the nearest-neighbor\ninterpolation (NNI) which is traditionally based on the deterministic rounding\n(DR) function. Experimental simulation results are provided to demonstrate the\nperformance of NNI-SR and NNI-DR algorithms before and after applying smoothing\nand sharpening filters of interest. Additional results are also provided to\ndemonstrate the performance of NNI-SR and NNI-DR interpolated scan conversion\nalgorithms in cardiac ultrasound videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Samuel Emil Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable image registration with deep network priors: a study on longitudinal PET images. (arXiv:2111.11873v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.11873","description":"<p>Longitudinal image registration is challenging and has not yet benefited from\nmajor performance improvements thanks to deep-learning. Inspired by Deep Image\nPrior, this paper introduces a different use of deep architectures as\nregularizers to tackle the image registration question. We propose a\nsubject-specific deformable registration method called MIRRBA, relying on a\ndeep pyramidal architecture to be the prior parametric model constraining the\ndeformation field. Diverging from the supervised learning paradigm, MIRRBA does\nnot require a learning database, but only the pair of images to be registered\nto optimize the network's parameters and provide a deformation field. We\ndemonstrate the regularizing power of deep architectures and present new\nelements to understand the role of the architecture in deep learning methods\nfor registration. Hence, to study the impact of the network parameters, we ran\nour method with different architectural configurations on a private dataset of\n110 metastatic breast cancer full-body PET images with manual segmentations of\nthe brain, bladder and metastatic lesions. We compared it against conventional\niterative registration approaches and supervised deep learning-based models.\nGlobal and local registration accuracies were evaluated using the detection\nrate and the Dice score respectively, while registration realism was evaluated\nusing the Jacobian's determinant. Moreover, we computed the ability of the\ndifferent methods to shrink vanishing lesions with the disappearing rate.\nMIRRBA significantly improves the organ and lesion Dice scores of supervised\nmodels. Regarding the disappearing rate, MIRRBA more than doubles the best\nperforming conventional approach SyNCC score. Our work therefore proposes an\nalternative way to bridge the performance gap between conventional and deep\nlearning-based methods and demonstrates the regularizing power of deep\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fourcade_C/0/1/0/all/0/1\">Constance Fourcade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrer_L/0/1/0/all/0/1\">Ludovic Ferrer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreau_N/0/1/0/all/0/1\">Noemie Moreau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santini_G/0/1/0/all/0/1\">Gianmarco Santini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brennan_A/0/1/0/all/0/1\">Aishlinn Brennan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rousseau_C/0/1/0/all/0/1\">Caroline Rousseau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lacombe_M/0/1/0/all/0/1\">Marie Lacombe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fleury_V/0/1/0/all/0/1\">Vincent Fleury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colombie_M/0/1/0/all/0/1\">Mathilde Colombi&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jezequel_P/0/1/0/all/0/1\">Pascal J&#xe9;z&#xe9;quel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campone_M/0/1/0/all/0/1\">Mario Campone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubeaux_M/0/1/0/all/0/1\">Mathieu Rubeaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mateus_D/0/1/0/all/0/1\">Diana Mateus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferability Metrics for Selecting Source Model Ensembles. (arXiv:2111.13011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13011","description":"<p>We address the problem of ensemble selection in transfer learning: Given a\nlarge pool of source models we want to select an ensemble of models which,\nafter fine-tuning on the target training set, yields the best performance on\nthe target test set. Since fine-tuning all possible ensembles is\ncomputationally prohibitive, we aim at predicting performance on the target\ndataset using a computationally efficient transferability metric. We propose\nseveral new transferability metrics designed for this task and evaluate them in\na challenging and realistic transfer learning setup for semantic segmentation:\nwe create a large and diverse pool of source models by considering 17 source\ndatasets covering a wide variety of image domain, two different architectures,\nand two pre-training schemes. Given this pool, we then automatically select a\nsubset to form an ensemble performing well on a given target dataset. We\ncompare the ensemble selected by our method to two baselines which select a\nsingle source model, either (1) from the same pool as our method; or (2) from a\npool containing large source models, each with similar capacity as an ensemble.\nAveraged over 17 target datasets, we outperform these baselines by 6.0% and\n2.5% relative mean IoU, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agostinelli_A/0/1/0/all/0/1\">Andrea Agostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uijlings_J/0/1/0/all/0/1\">Jasper Uijlings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Implicit Values of A Good Hand Shake: Handheld Multi-Frame Neural Depth Refinement. (arXiv:2111.13738v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13738","description":"<p>Modern smartphones can continuously stream multi-megapixel RGB images at\n60Hz, synchronized with high-quality 3D pose information and low-resolution\nLiDAR-driven depth estimates. During a snapshot photograph, the natural\nunsteadiness of the photographer's hands offers millimeter-scale variation in\ncamera pose, which we can capture along with RGB and depth in a circular\nbuffer. In this work we explore how, from a bundle of these measurements\nacquired during viewfinding, we can combine dense micro-baseline parallax cues\nwith kilopixel LiDAR depth to distill a high-fidelity depth map. We take a\ntest-time optimization approach and train a coordinate MLP to output\nphotometrically and geometrically consistent depth estimates at the continuous\ncoordinates along the path traced by the photographer's natural hand shake.\nWith no additional hardware, artificial hand motion, or user interaction beyond\nthe press of a button, our proposed method brings high-resolution depth\nestimates to point-and-shoot \"tabletop\" photography -- textured objects at\nclose range.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chugunov_I/0/1/0/all/0/1\">Ilya Chugunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuaner/0/1/0/all/0/1\">Xuaner</a> (Cecilia) <a href=\"http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1\">Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning. (arXiv:2111.14213v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.14213","description":"<p>Federated learning (FL) is a promising strategy for performing\nprivacy-preserving, distributed learning with a network of clients (i.e., edge\ndevices). However, the data distribution among clients is often non-IID in\nnature, making efficient optimization difficult. To alleviate this issue, many\nFL algorithms focus on mitigating the effects of data heterogeneity across\nclients by introducing a variety of proximal terms, some incurring considerable\ncompute and/or memory overheads, to restrain local updates with respect to the\nglobal model. Instead, we consider rethinking solutions to data heterogeneity\nin FL with a focus on local learning generality rather than proximal\nrestriction. To this end, we first present a systematic study informed by\nsecond-order indicators to better understand algorithm effectiveness in FL.\nInterestingly, we find that standard regularization methods are surprisingly\nstrong performers in mitigating data heterogeneity effects. Based on our\nfindings, we further propose a simple and effective method, FedAlign, to\novercome data heterogeneity and the pitfalls of previous methods. FedAlign\nachieves competitive accuracy with state-of-the-art FL methods across a variety\nof settings while minimizing computation and memory overhead. Code is available\nat https://github.com/mmendiet/FedAlign\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1\">Matias Mendieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Taojiannan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic. (arXiv:2111.14447v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14447","description":"<p>Recent text-to-image matching models apply contrastive learning to large\ncorpora of uncurated pairs of images and sentences. While such models can\nprovide a powerful score for matching and subsequent zero-shot tasks, they are\nnot capable of generating caption given an image. In this work, we repurpose\nsuch models to generate a descriptive text given an image at inference time,\nwithout any further training or tuning steps. This is done by combining the\nvisual-semantic model with a large language model, benefiting from the\nknowledge in both web-scale models. The resulting captions are much less\nrestrictive than those obtained by supervised captioning methods. Moreover, as\na zero-shot learning method, it is extremely flexible and we demonstrate its\nability to perform image arithmetic in which the inputs can be either images or\ntext, and the output is a sentence. This enables novel high-level vision\ncapabilities such as comparing two images or solving visual analogy tests. Our\ncode is available at: https://github.com/YoadTew/zero-shot-image-to-text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tewel_Y/0/1/0/all/0/1\">Yoad Tewel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Y/0/1/0/all/0/1\">Yoav Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDR-NeRF: High Dynamic Range Neural Radiance Fields. (arXiv:2111.14451v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14451","description":"<p>We present High Dynamic Range Neural Radiance Fields (HDR-NeRF) to recover an\nHDR radiance field from a set of low dynamic range (LDR) views with different\nexposures. Using the HDR-NeRF, we are able to generate both novel HDR views and\nnovel LDR views under different exposures. The key to our method is to model\nthe physical imaging process, which dictates that the radiance of a scene point\ntransforms to a pixel value in the LDR image with two implicit functions: a\nradiance field and a tone mapper. The radiance field encodes the scene radiance\n(values vary from 0 to +infty), which outputs the density and radiance of a ray\nby giving corresponding ray origin and ray direction. The tone mapper models\nthe mapping process that a ray hitting on the camera sensor becomes a pixel\nvalue. The color of the ray is predicted by feeding the radiance and the\ncorresponding exposure time into the tone mapper. We use the classic volume\nrendering technique to project the output radiance, colors, and densities into\nHDR and LDR images, while only the input LDR images are used as the\nsupervision. We collect a new forward-facing HDR dataset to evaluate the\nproposed method. Experimental results on synthetic and real-world scenes\nvalidate that our method can not only accurately control the exposures of\nsynthesized views but also render views with a high dynamic range.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Ying Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image. (arXiv:2112.02753v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02753","description":"<p>In this work, we propose a framework for single-view hand mesh\nreconstruction, which can simultaneously achieve high reconstruction accuracy,\nfast inference speed, and temporal coherence. Specifically, for 2D encoding, we\npropose lightweight yet effective stacked structures. Regarding 3D decoding, we\nprovide an efficient graph operator, namely depth-separable spiral convolution.\nMoreover, we present a novel feature lifting module for bridging the gap\nbetween 2D and 3D representations. This module begins with a map-based position\nregression (MapReg) block to integrate the merits of both heatmap encoding and\nposition regression paradigms for improved 2D accuracy and temporal coherence.\nFurthermore, MapReg is followed by pose pooling and pose-to-vertex lifting\napproaches, which transform 2D pose encodings to semantic features of 3D\nvertices. Overall, our hand reconstruction framework, called MobRecon,\ncomprises affordable computational costs and miniature model size, which\nreaches a high inference speed of 83FPS on Apple A14 CPU. Extensive experiments\non popular datasets such as FreiHAND, RHD, and HO3Dv2 demonstrate that our\nMobRecon achieves superior performance on reconstruction accuracy and temporal\ncoherence. Our code is publicly available at\nhttps://github.com/SeanChenxy/HandMesh.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yufeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yajiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yanmin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make It Move: Controllable Image-to-Video Generation with Text Descriptions. (arXiv:2112.02815v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02815","description":"<p>Generating controllable videos conforming to user intentions is an appealing\nyet challenging topic in computer vision. To enable maneuverable control in\nline with user intentions, a novel video generation task, named\nText-Image-to-Video generation (TI2V), is proposed. With both controllable\nappearance and motion, TI2V aims at generating videos from a static image and a\ntext description. The key challenges of TI2V task lie both in aligning\nappearance and motion from different modalities, and in handling uncertainty in\ntext descriptions. To address these challenges, we propose a Motion\nAnchor-based video GEnerator (MAGE) with an innovative motion anchor (MA)\nstructure to store appearance-motion aligned representation. To model the\nuncertainty and increase the diversity, it further allows the injection of\nexplicit condition and implicit randomness. Through three-dimensional axial\ntransformers, MA is interacted with given image to generate next frames\nrecursively with satisfying controllability and diversity. Accompanying the new\ntask, we build two new video-text paired datasets based on MNIST and CATER for\nevaluation. Experiments conducted on these datasets verify the effectiveness of\nMAGE and show appealing potentials of TI2V task. Source code for model and\ndatasets will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaosi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling. (arXiv:2112.04148v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04148","description":"<p>In this paper, we propose Neural Points, a novel point cloud representation\nand apply it to the arbitrary-factored upsampling task. Different from\ntraditional point cloud representation where each point only represents a\nposition or a local plane in the 3D space, each point in Neural Points\nrepresents a local continuous geometric shape via neural fields. Therefore,\nNeural Points contain more shape information and thus have a stronger\nrepresentation ability. Neural Points is trained with surface containing rich\ngeometric details, such that the trained model has enough expression ability\nfor various shapes. Specifically, we extract deep local features on the points\nand construct neural fields through the local isomorphism between the 2D\nparametric domain and the 3D local patch. In the final, local neural fields are\nintegrated together to form the global surface. Experimental results show that\nNeural Points has powerful representation ability and demonstrate excellent\nrobustness and generalization ability. With Neural Points, we can resample\npoint cloud with arbitrary resolutions, and it outperforms the state-of-the-art\npoint cloud upsampling methods. Code is available at\nhttps://github.com/WanquanF/NeuralPoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wanquan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongrui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaonan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Semantic Concepts into End-to-End Image Captioning. (arXiv:2112.05230v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05230","description":"<p>Tremendous progress has been made in recent years in developing better image\ncaptioning models, yet most of them rely on a separate object detector to\nextract regional features. Recent vision-language studies are shifting towards\nthe detector-free trend by leveraging grid representations for more flexible\nmodel training and faster inference speed. However, such development is\nprimarily focused on image understanding tasks, and remains less investigated\nfor the caption generation task. In this paper, we are concerned with a\nbetter-performing detector-free image captioning model, and propose a pure\nvision transformer-based image captioning model, dubbed as ViTCAP, in which\ngrid representations are used without extracting the regional features. For\nimproved performance, we introduce a novel Concept Token Network (CTN) to\npredict the semantic concepts and then incorporate them into the end-to-end\ncaptioning. In particular, the CTN is built on the basis of a vision\ntransformer and is designed to predict the concept tokens through a\nclassification task, from which the rich semantic information contained greatly\nbenefits the captioning task. Compared with the previous detector-based models,\nViTCAP drastically simplifies the architectures and at the same time achieves\ncompetitive performance on various challenging image captioning datasets. In\nparticular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split,\n93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhiyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Erasing and Diffusion Network for Occluded Person Re-Identification. (arXiv:2112.08740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08740","description":"<p>Occluded person re-identification (ReID) aims at matching occluded person\nimages to holistic ones across different camera views. Target Pedestrians (TP)\nare usually disturbed by Non-Pedestrian Occlusions (NPO) and NonTarget\nPedestrians (NTP). Previous methods mainly focus on increasing model's\nrobustness against NPO while ignoring feature contamination from NTP. In this\npaper, we propose a novel Feature Erasing and Diffusion Network (FED) to\nsimultaneously handle NPO and NTP. Specifically, NPO features are eliminated by\nour proposed Occlusion Erasing Module (OEM), aided by the NPO augmentation\nstrategy which simulates NPO on holistic pedestrian images and generates\nprecise occlusion masks. Subsequently, we Subsequently, we diffuse the\npedestrian representations with other memorized features to synthesize NTP\ncharacteristics in the feature space which is achieved by a novel Feature\nDiffusion Module (FDM) through a learnable cross attention mechanism. With the\nguidance of the occlusion scores from OEM, the feature diffusion process is\nmainly conducted on visible body parts, which guarantees the quality of the\nsynthesized NTP characteristics. By jointly optimizing OEM and FDM in our\nproposed FED network, we can greatly improve the model's perception ability\ntowards TP and alleviate the influence of NPO and NTP. Furthermore, the\nproposed FDM only works as an auxiliary module for training and will be\ndiscarded in the inference phase, thus introducing little inference\ncomputational overhead. Experiments on occluded and holistic person ReID\nbenchmarks demonstrate the superiority of FED over state-of-the-arts, where FED\nachieves 86.3% Rank-1 accuracy on Occluded-REID, surpassing others by at least\n4.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhikang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lihuo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiangning Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScanQA: 3D Question Answering for Spatial Scene Understanding. (arXiv:2112.10482v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10482","description":"<p>We propose a new 3D spatial understanding task of 3D Question Answering\n(3D-QA). In the 3D-QA task, models receive visual information from the entire\n3D scene of the rich RGB-D indoor scan and answer the given textual questions\nabout the 3D scene. Unlike the 2D-question answering of VQA, the conventional\n2D-QA models suffer from problems with spatial understanding of object\nalignment and directions and fail the object identification from the textual\nquestions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model,\nwhere the model learns a fused descriptor from 3D object proposals and encoded\nsentence embeddings. This learned descriptor correlates the language\nexpressions with the underlying geometric features of the 3D scan and\nfacilitates the regression of 3D bounding boxes to determine described objects\nin textual questions and outputs correct answers. We collected human-edited\nquestion-answer pairs with free-form answers that are grounded to 3D objects in\neach 3D scene. Our new ScanQA dataset contains over 40K question-answer pairs\nfrom the 800 indoor scenes drawn from the ScanNet dataset. To the best of our\nknowledge, the proposed 3D-QA task is the first large-scale effort to perform\nobject-grounded question-answering in 3D environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azuma_D/0/1/0/all/0/1\">Daichi Azuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyanishi_T/0/1/0/all/0/1\">Taiki Miyanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurita_S/0/1/0/all/0/1\">Shuhei Kurita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanabe_M/0/1/0/all/0/1\">Motoki Kawanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View. (arXiv:2112.11790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11790","description":"<p>Autonomous driving perceives its surroundings for decision making, which is\none of the most complex scenarios in visual perception. The success of paradigm\ninnovation in solving the 2D object detection task inspires us to seek an\nelegant, feasible, and scalable paradigm for fundamentally pushing the\nperformance boundary in this area. To this end, we contribute the BEVDet\nparadigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View\n(BEV), where most target values are defined and route planning can be handily\nperformed. We merely reuse existing modules to build its framework but\nsubstantially develop its performance by constructing an exclusive data\naugmentation strategy and upgrading the Non-Maximum Suppression strategy. In\nthe experiment, BEVDet offers an excellent trade-off between accuracy and\ntime-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS\non the nuScenes val set. It is comparable with FCOS3D, but requires just 11%\ncomputational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS.\nAnother high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2%\nNDS, significantly exceeding all published results. With a comparable inference\nspeed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The\ncode will be released to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dalong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12970","description":"<p>Scene Graph Generation (SGG) remains a challenging visual understanding task\ndue to its compositional property. Most previous works adopt a bottom-up\ntwo-stage or a point-based one-stage approach, which often suffers from high\ntime complexity or sub-optimal designs. In this work, we propose a novel SGG\nmethod to address the aforementioned issues, formulating the task as a\nbipartite graph construction problem. To solve the problem, we develop a\ntransformer-based end-to-end framework that first generates the entity and\npredicate proposal set, followed by inferring directed edges to form the\nrelation triplets. In particular, we develop a new entity-aware predicate\nrepresentation based on a structural predicate generator that leverages the\ncompositional property of relationships. Moreover, we design a graph assembling\nmodule to infer the connectivity of the bipartite scene graph based on our\nentity-aware structure, enabling us to generate the scene graph in an\nend-to-end manner. Extensive experimental results show that our design is able\nto achieve the state-of-the-art or comparable performance on two challenging\nbenchmarks, surpassing most of the existing approaches and enjoying higher\nefficiency in inference. We hope our model can serve as a strong baseline for\nthe Transformer-based scene graph generation. Code is available:\nhttps://github.com/Scarecrow0/SGTR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Cross-dataset Generalization in License Plate Recognition. (arXiv:2201.00267v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00267","description":"<p>Automatic License Plate Recognition (ALPR) systems have shown remarkable\nperformance on license plates (LPs) from multiple regions due to advances in\ndeep learning and the increasing availability of datasets. The evaluation of\ndeep ALPR systems is usually done within each dataset; therefore, it is\nquestionable if such results are a reliable indicator of generalization\nability. In this paper, we propose a traditional-split versus\nleave-one-dataset-out experimental setup to empirically assess the\ncross-dataset generalization of 12 Optical Character Recognition (OCR) models\napplied to LP recognition on nine publicly available datasets with a great\nvariety in several aspects (e.g., acquisition settings, image resolution, and\nLP layouts). We also introduce a public dataset for end-to-end ALPR that is the\nfirst to contain images of vehicles with Mercosur LPs and the one with the\nhighest number of motorcycle images. The experimental results shed light on the\nlimitations of the traditional-split protocol for evaluating approaches in the\nALPR context, as there are significant drops in performance for most datasets\nwhen training and testing the models in a leave-one-dataset-out fashion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_E/0/1/0/all/0/1\">Everton V. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucio_D/0/1/0/all/0/1\">Diego R. Lucio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estevam_V/0/1/0/all/0/1\">Valter Estevam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arbitrary Handwriting Image Style Transfer. (arXiv:2201.05346v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05346","description":"<p>This paper proposed a method to imitate handwriting style by style transfer.\nWe proposed an neural network model based on conditional generative adversarial\nnetworks (cGAN) for handwriting style transfer. This paper improved the loss\nfunction on the basis of the GAN. Compared with other handwriting imitation\nmethods, the handwriting style transfer's effect and efficiency have been\nsignificantly improved. The experiments showed that the shape of the generated\nChinese characters is clear and the analysis of experimental data showed the\nGenerative adversarial networks showed excellent performance in handwriting\nstyle transfer. The generated text image is closer to the real handwriting and\nachieved a better performance in term of handwriting imitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaoman Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huihuang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omnivore: A Single Model for Many Visual Modalities. (arXiv:2201.08377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08377","description":"<p>Prior work has studied different visual modalities in isolation and developed\nseparate architectures for recognition of images, videos, and 3D data. Instead,\nin this paper, we propose a single model which excels at classifying images,\nvideos, and single-view 3D data using exactly the same model parameters. Our\n'Omnivore' model leverages the flexibility of transformer-based architectures\nand is trained jointly on classification tasks from different modalities.\nOmnivore is simple to train, uses off-the-shelf standard datasets, and performs\nat-par or better than modality-specific models of the same size. A single\nOmnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN\nRGB-D. After finetuning, our models outperform prior work on a variety of\nvision tasks and generalize across modalities. Omnivore's shared visual\nrepresentation naturally enables cross-modal recognition without access to\ncorrespondences between modalities. We hope our results motivate researchers to\nmodel visual modalities together.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1\">Nikhila Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint. (arXiv:2202.00886v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.00886","description":"<p>Multi-perspective cameras are quickly gaining importance in many applications\nsuch as smart vehicles and virtual or augmented reality. However, a large\nsystem size or absence of overlap in neighbouring fields-of-view often\ncomplicate their calibration. We present a novel solution which relies on the\navailability of an external motion capture system. Our core contribution\nconsists of an extension to the hand-eye calibration problem which jointly\nsolves multi-eye-to-base problems in closed form. We furthermore demonstrate\nits equivalence to the multi-eye-in-hand problem. The practical validity of our\napproach is supported by our experiments, indicating that the method is highly\nefficient and accurate, and outperforms existing closed-form alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">S&#xf6;ren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06484","description":"<p>In the field of domain adaptation, a trade-off exists between the model\nperformance and the number of target domain annotations. Active learning,\nmaximizing model performance with few informative labeled data, comes in handy\nfor such a scenario. In this work, we present D2ADA, a general active domain\nadaptation framework for semantic segmentation. To adapt the model to the\ntarget domain with minimum queried labels, we propose acquiring labels of the\nsamples with high probability density in the target domain yet with low\nprobability density in the source domain, complementary to the existing source\ndomain labeled data. To further facilitate labeling efficiency, we design a\ndynamic scheduling policy to adjust the labeling budgets between domain\nexploration and model uncertainty over time. Extensive experiments show that\nour method outperforms existing active learning and domain adaptation baselines\non two benchmarks, GTA5 -&gt; Cityscapes and SYNTHIA -&gt; Cityscapes. With less than\n5% target domain annotations, our method reaches comparable results with that\nof full supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1\">Yi-Syuan Liou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shao-Ji Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tung-I Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Chih Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Satellite Imagery using Deep Learning for the Sensor To Shooter Timeline. (arXiv:2203.00116v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00116","description":"<p>The sensor to shooter timeline is affected by two main variables: satellite\npositioning and asset positioning. Speeding up satellite positioning by adding\nmore sensors or by decreasing processing time is important only if there is a\nprepared shooter, otherwise the main source of time is getting the shooter into\nposition. However, the intelligence community should work towards the\nexploitation of sensors to the highest speed and effectiveness possible.\nAchieving a high effectiveness while keeping speed high is a tradeoff that must\nbe considered in the sensor to shooter timeline. In this paper we investigate\ntwo main ideas, increasing the effectiveness of satellite imagery through image\nmanipulation and how on-board image manipulation would affect the sensor to\nshooter timeline. We cover these ideas in four scenarios: Discrete Event\nSimulation of onboard processing versus ground station processing, quality of\ninformation with cloud cover removal, information improvement with super\nresolution, and data reduction with image to caption. This paper will show how\nimage manipulation techniques such as Super Resolution, Cloud Removal, and\nImage to Caption will improve the quality of delivered information in addition\nto showing how those processes effect the sensor to shooter timeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1\">Matthew Ciolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hambrick_D/0/1/0/all/0/1\">Dominick Hambrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01322","description":"<p>Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1\">Venkat Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Attention Network: Transformer Meets U-Net. (arXiv:2203.01932v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01932","description":"<p>Currently, convolutional neural networks (CNN) (e.g., U-Net) have become the\nde facto standard and attained immense success in medical image segmentation.\nHowever, as a downside, CNN based methods are a double-edged sword as they fail\nto build long-range dependencies and global context connections due to the\nlimited receptive field that stems from the intrinsic characteristics of the\nconvolution operation. Hence, recent articles have exploited Transformer\nvariants for medical image segmentation tasks which open up great opportunities\ndue to their innate capability of capturing long-range correlations through the\nattention mechanism. Although being feasibly designed, most of the cohort\nstudies incur prohibitive performance in capturing local information, thereby\nresulting in less lucidness of boundary areas. In this paper, we propose a\ncontextual attention network to tackle the aforementioned limitations. The\nproposed method uses the strength of the Transformer module to model the\nlong-range contextual dependency. Simultaneously, it utilizes the CNN encoder\nto capture local semantic information. In addition, an object-level\nrepresentation is included to model the regional interaction map. The extracted\nhierarchical features are then fed to the contextual attention module to\nadaptively recalibrate the representation space using the local information.\nThen, they emphasize the informative regions while taking into account the\nlong-range contextual dependency derived by the Transformer module. We validate\nour method on several large-scale public medical image segmentation datasets\nand achieve state-of-the-art performance. We have provided the implementation\ncode in https://github.com/rezazad68/TMUnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heidari_M/0/1/0/all/0/1\">Moein Heidari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yuli Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merhof_D/0/1/0/all/0/1\">Dorit Merhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Context Matters: Enhancing Single Image Prediction with Disease Progression Representations. (arXiv:2203.01933v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01933","description":"<p>Clinical outcome or severity prediction from medical images has largely\nfocused on learning representations from single-timepoint or snapshot scans. It\nhas been shown that disease progression can be better characterized by temporal\nimaging. We therefore hypothesized that outcome predictions can be improved by\nutilizing the disease progression information from sequential images. We\npresent a deep learning approach that leverages temporal progression\ninformation to improve clinical outcome predictions from single-timepoint\nimages. In our method, a self-attention based Temporal Convolutional Network\n(TCN) is used to learn a representation that is most reflective of the disease\ntrajectory. Meanwhile, a Vision Transformer is pretrained in a self-supervised\nfashion to extract features from single-timepoint images. The key contribution\nis to design a recalibration module that employs maximum mean discrepancy loss\n(MMD) to align distributions of the above two contextual representations. We\ntrain our system to predict clinical outcomes and severity grades from\nsingle-timepoint images. Experiments on chest and osteoarthritis radiography\ndatasets demonstrate that our approach outperforms other state-of-the-art\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Konwer_A/0/1/0/all/0/1\">Aishik Konwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPAL: Occlusion Pattern Aware Loss for Unsupervised Light Field Disparity Estimation. (arXiv:2203.02231v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02231","description":"<p>Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables: i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiayin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingyao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Self-Supervised Category-Level Object Pose and Size Estimation. (arXiv:2203.02884v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02884","description":"<p>In this work, we tackle the challenging problem of category-level object pose\nand size estimation from a single depth image. Although previous\nfully-supervised works have demonstrated promising performance, collecting\nground-truth pose labels is generally time-consuming and labor-intensive.\nInstead, we propose a label-free method that learns to enforce the geometric\nconsistency between category template mesh and observed object point cloud\nunder a self-supervision manner. Specifically, our method consists of three key\ncomponents: differentiable shape deformation, registration, and rendering. In\nparticular, shape deformation and registration are applied to the template mesh\nto eliminate the differences in shape, pose and scale. A differentiable\nrenderer is then deployed to enforce geometric consistency between point clouds\nlifted from the rendered depth and the observed scene for self-supervision. We\nevaluate our approach on real-world datasets and find that our approach\noutperforms the simple traditional baseline by large margins while being\ncompetitive with some fully-supervised approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yisheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A study on joint modeling and data augmentation of multi-modalities for audio-visual scene classification. (arXiv:2203.04114v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2203.04114","description":"<p>In this paper, we propose two techniques, namely joint modeling and data\naugmentation, to improve system performances for audio-visual scene\nclassification (AVSC). We employ pre-trained networks trained only on image\ndata sets to extract video embedding; whereas for audio embedding models, we\ndecide to train them from scratch. We explore different neural network\narchitectures for joint modeling to effectively combine the video and audio\nmodalities. Moreover, data augmentation strategies are investigated to increase\naudio-visual training set size. For the video modality the effectiveness of\nseveral operations in RandAugment is verified. An audio-video joint mixup\nscheme is proposed to further improve AVSC performances. Evaluated on the\ndevelopment set of TAU Urban Audio Visual Scenes 2021, our final system can\nachieve the best accuracy of 94.2% among all single AVSC systems submitted to\nDCASE 2021 Task 1b.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment. (arXiv:2203.04121v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04121","description":"<p>Training a generative adversarial network (GAN) with limited data has been a\nchallenging task. A feasible solution is to start with a GAN well-trained on a\nlarge scale source domain and adapt it to the target domain with a few samples,\ntermed as few shot generative model adaption. However, existing methods are\nprone to model overfitting and collapse in extremely few shot setting (less\nthan 10). To solve this problem, we propose a relaxed spatial structural\nalignment method to calibrate the target generative models during the adaption.\nWe design a cross-domain spatial structural consistency loss comprising the\nself-correlation and disturbance correlation consistency loss. It helps align\nthe spatial structural information between the synthesis image pairs of the\nsource and target domains. To relax the cross-domain alignment, we compress the\noriginal latent space of generative models to a subspace. Image pairs generated\nfrom the subspace are pulled closer. Qualitative and quantitative experiments\nshow that our method consistently surpasses the state-of-the-art methods in few\nshot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiayu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection. (arXiv:2203.06398v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06398","description":"<p>Domain Adaptive Object Detection (DAOD) leverages a labeled domain to learn\nan object detector generalizing to a novel domain free of annotations. Recent\nadvances align class-conditional distributions by narrowing down cross-domain\nprototypes (class centers). Though great success,they ignore the significant\nwithin-class variance and the domain-mismatched semantics within the training\nbatch, leading to a sub-optimal adaptation. To overcome these challenges, we\npropose a novel SemantIc-complete Graph MAtching (SIGMA) framework for DAOD,\nwhich completes mismatched semantics and reformulates the adaptation with graph\nmatching. Specifically, we design a Graph-embedded Semantic Completion module\n(GSC) that completes mismatched semantics through generating hallucination\ngraph nodes in missing categories. Then, we establish cross-image graphs to\nmodel class-conditional distributions and learn a graph-guided memory bank for\nbetter semantic completion in turn. After representing the source and target\ndata as graphs, we reformulate the adaptation as a graph matching problem,\ni.e., finding well-matched node pairs across graphs to reduce the domain gap,\nwhich is solved with a novel Bipartite Graph Matching adaptor (BGM). In a\nnutshell, we utilize graph nodes to establish semantic-aware node affinity and\nleverage graph edges as quadratic constraints in a structure-aware matching\nloss, achieving fine-grained adaptation with a node-to-node graph matching.\nExtensive experiments verify that SIGMA outperforms existing works\nsignificantly. Our code is available at\nhttps://github.com/CityU-AIM-Group/SIGMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs. (arXiv:2203.08516v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08516","description":"<p>The discovery of interpretable directions in the latent spaces of pre-trained\nGAN models has recently become a popular topic. In particular, StyleGAN2 has\nenabled various image generation and manipulation tasks due to its rich and\ndisentangled latent spaces. The discovery of such directions is typically done\neither in a supervised manner, which requires annotated data for each desired\nmanipulation or in an unsupervised manner, which requires a manual effort to\nidentify the directions. As a result, existing work typically finds only a\nhandful of directions in which controllable edits can be made. In this study,\nwe design a novel submodular framework that finds the most representative and\ndiverse subset of directions in the latent space of StyleGAN2. Our approach\ntakes advantage of the latent space of channel-wise style parameters, so-called\nstyle space, in which we cluster channels that perform similar manipulations\ninto groups. Our framework promotes diversity by using the notion of clusters\nand can be efficiently solved with a greedy optimization scheme. We evaluate\nour framework with qualitative and quantitative experiments and show that our\nmethod finds more diverse and disentangled directions. Our project page can be\nfound at <a href=\"http://catlab-team.github.io/fantasticstyles.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simsar_E/0/1/0/all/0/1\">Enis Simsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocasari_U/0/1/0/all/0/1\">Umut Kocasari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_E/0/1/0/all/0/1\">Ezgi G&#xfc;lperi Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanardag_P/0/1/0/all/0/1\">Pinar Yanardag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scribble-Supervised LiDAR Semantic Segmentation. (arXiv:2203.08537v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08537","description":"<p>Densely annotating LiDAR point clouds remains too expensive and\ntime-consuming to keep up with the ever growing volume of data. While current\nliterature focuses on fully-supervised performance, developing efficient\nmethods that take advantage of realistic weak supervision have yet to be\nexplored. In this paper, we propose using scribbles to annotate LiDAR point\nclouds and release ScribbleKITTI, the first scribble-annotated dataset for\nLiDAR semantic segmentation. Furthermore, we present a pipeline to reduce the\nperformance gap that arises when using such weak annotations. Our pipeline\ncomprises of three stand-alone contributions that can be combined with any\nLiDAR semantic segmentation model to achieve up to 95.7% of the\nfully-supervised performance while using only 8% labeled points. Our scribble\nannotations and code are available at github.com/ouenal/scribblekitti.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unal_O/0/1/0/all/0/1\">Ozan Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Feedback Synthesis Towards Multimodal Text and Image Data. (arXiv:2203.12692v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2203.12692","description":"<p>In this paper, we have defined a novel task of affective feedback synthesis\nthat deals with generating feedback for input text &amp; corresponding image in a\nsimilar way as humans respond towards the multimodal data. A feedback synthesis\nsystem has been proposed and trained using ground-truth human comments along\nwith image-text input. We have also constructed a large-scale dataset\nconsisting of image, text, Twitter user comments, and the number of likes for\nthe comments by crawling the news articles through Twitter feeds. The proposed\nsystem extracts textual features using a transformer-based textual encoder\nwhile the visual features have been extracted using a Faster region-based\nconvolutional neural networks model. The textual and visual features have been\nconcatenated to construct the multimodal features using which the decoder\nsynthesizes the feedback. We have compared the results of the proposed system\nwith the baseline models using quantitative and qualitative measures. The\ngenerated feedbacks have been analyzed using automatic and human evaluation.\nThey have been found to be semantically similar to the ground-truth comments\nand relevant to the given text-image input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Puneet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_G/0/1/0/all/0/1\">Gaurav Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingle_O/0/1/0/all/0/1\">Omkar Ingle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1\">Daksh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_B/0/1/0/all/0/1\">Balasubramanian Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13032","description":"<p>In this paper, we briefly introduce our submission to the Valence-Arousal\nEstimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)\ncompetition. Our method utilizes the multi-modal information, i.e., the visual\nand audio information, and employs a temporal encoder to model the temporal\ncontext in the videos. Besides, a smooth processor is applied to get more\nreasonable predictions, and a model ensemble strategy is used to improve the\nperformance of our proposed method. The experiment results show that our method\nachieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation\nset of the Aff-Wild2 dataset, which prove the effectiveness of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Liyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaopei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Compression and Actionable Intelligence With Deep Neural Networks. (arXiv:2203.13686v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13686","description":"<p>If a unit cannot receive intelligence from a source due to external factors,\nwe consider them disadvantaged users. We categorize this as a preoccupied unit\nworking on a low connectivity device on the edge. This case requires that we\nuse a different approach to deliver intelligence, particularly satellite\nimagery information, than normally employed. To address this, we propose a\nsurvey of information reduction techniques to deliver the information from a\nsatellite image in a smaller package. We investigate four techniques to aid in\nthe reduction of delivered information: traditional image compression, neural\nnetwork image compression, object detection image cutout, and image to caption.\nEach of these mechanisms have their benefits and tradeoffs when considered for\na disadvantaged user.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1\">Matthew Ciolino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14542","description":"<p>Supervised deep learning methods require a large repository of annotated\ndata; hence, label noise is inevitable. Training with such noisy data\nnegatively impacts the generalization performance of deep neural networks. To\ncombat label noise, recent state-of-the-art methods employ some sort of sample\nselection mechanism to select a possibly clean subset of data. Next, an\noff-the-shelf semi-supervised learning method is used for training where\nrejected samples are treated as unlabeled data. Our comprehensive analysis\nshows that current selection methods disproportionately select samples from\neasy (fast learnable) classes while rejecting those from relatively harder\nones. This creates class imbalance in the selected clean set and in turn,\ndeteriorates performance under high label noise. In this work, we propose\nUNICON, a simple yet effective sample selection method which is robust to high\nlabel noise. To address the disproportionate selection of easy and hard\nsamples, we introduce a Jensen-Shannon divergence based uniform selection\nmechanism which does not require any probabilistic modeling and hyperparameter\ntuning. We complement our selection method with contrastive learning to further\ncombat the memorization of noisy labels. Extensive experimentation on multiple\nbenchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%\nimprovement over the current state-of-the-art on CIFAR100 dataset with a 90%\nnoise rate. Our code is publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1\">Mamshad Nayeem Rizve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15536","description":"<p>Our goal is to recover the 3D shape and pose of dogs from a single image.\nThis is a challenging task because dogs exhibit a wide range of shapes and\nappearances, and are highly articulated. Recent work has proposed to directly\nregress the SMAL animal model, with additional limb scale parameters, from\nimages. Our method, called BARC (Breed-Augmented Regression using\nClassification), goes beyond prior work in several important ways. First, we\nmodify the SMAL shape space to be more appropriate for representing dog shape.\nBut, even with a better shape model, the problem of regressing dog shape from\nan image is still challenging because we lack paired images with 3D ground\ntruth. To compensate for the lack of paired data, we formulate novel losses\nthat exploit information about dog breeds. In particular, we exploit the fact\nthat dogs of the same breed have similar body shapes. We formulate a novel\nbreed similarity loss consisting of two parts: One term encourages the shape of\ndogs from the same breed to be more similar than dogs of different breeds. The\nsecond one, a breed classification loss, helps to produce recognizable\nbreed-specific shapes. Through ablation studies, we find that our breed losses\nsignificantly improve shape accuracy over a baseline without them. We also\ncompare BARC qualitatively to WLDO with a perceptual study and find that our\napproach produces dogs that are significantly more realistic. This work shows\nthat a-priori information about genetic similarity can help to compensate for\nthe lack of 3D training data. This concept may be applicable to other animal\nspecies or groups of species. Our code is publicly available for research\npurposes at https://barc.is.tue.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rueegg_N/0/1/0/all/0/1\">Nadine Rueegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1\">Silvia Zuffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15547","description":"<p>Convolutional Neural Networks need the construction of informative features,\nwhich are determined by channel-wise and spatial-wise information at the\nnetwork's layers. In this research, we focus on bringing in a novel solution\nthat uses sophisticated optimization for enhancing both the spatial and channel\ncomponents inside each layer's receptive field. Capsule Networks were used to\nunderstand the spatial association between features in the feature map.\nStandalone capsule networks have shown good results on comparatively simple\ndatasets than on complex datasets as a result of the inordinate amount of\nfeature information. Thus, to tackle this issue, we have proposed ME-CapsNet by\nintroducing deeper convolutional layers to extract important features before\npassing through modules of capsule layers strategically to improve the\nperformance of the network significantly. The deeper convolutional layer\nincludes blocks of Squeeze-Excitation networks which use a stochastic sampling\napproach for progressively reducing the spatial size thereby dynamically\nrecalibrating the channels by reconstructing their interdependencies without\nmuch loss of important feature information. Extensive experimentation was done\nusing commonly used datasets demonstrating the efficiency of the proposed\nME-CapsNet, which clearly outperforms various research works by achieving\nhigher accuracy with minimal model complexity in complex datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bright_J/0/1/0/all/0/1\">Jerrin Bright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajkumar_S/0/1/0/all/0/1\">Suryaprakash Rajkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doss_A/0/1/0/all/0/1\">Arockia Selvakumar Arockia Doss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.15588","description":"<p>The rapid development of diagnostic technologies in healthcare is leading to\nhigher requirements for physicians to handle and integrate the heterogeneous,\nyet complementary data that are produced during routine practice. For instance,\nthe personalized diagnosis and treatment planning for a single cancer patient\nrelies on the various images (e.g., radiological, pathological, and camera\nimages) and non-image data (e.g., clinical data and genomic data). However,\nsuch decision-making procedures can be subjective, qualitative, and have large\ninter-subject variabilities. With the recent advances in multi-modal deep\nlearning technologies, an increasingly large number of efforts have been\ndevoted to a key question: how do we extract and aggregate multi-modal\ninformation to ultimately provide more objective, quantitative computer-aided\nclinical decision making? This paper reviews the recent studies on dealing with\nsuch a question. Briefly, this review will include the (1) overview of current\nmulti-modal learning workflows, (2) summarization of multi-modal fusion\nmethods, (3) discussion of the performance, (4) applications in disease\ndiagnosis and prognosis, and (5) challenges and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coburn_L/0/1/0/all/0/1\">Lori A. Coburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_K/0/1/0/all/0/1\">Keith T. Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proactive Image Manipulation Detection. (arXiv:2203.15880v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15880","description":"<p>Image manipulation detection algorithms are often trained to discriminate\nbetween images manipulated with particular Generative Models (GMs) and\ngenuine/real images, yet generalize poorly to images manipulated with GMs\nunseen in the training. Conventional detection algorithms receive an input\nimage passively. By contrast, we propose a proactive scheme to image\nmanipulation detection. Our key enabling technique is to estimate a set of\ntemplates which when added onto the real image would lead to more accurate\nmanipulation detection. That is, a template protected real image, and its\nmanipulated version, is better discriminated compared to the original real\nimage vs. its manipulated one. These templates are estimated using certain\nconstraints based on the desired properties of templates. For image\nmanipulation detection, our proposed approach outperforms the prior work by an\naverage precision of 16% for CycleGAN and 32% for GauGAN. Our approach is\ngeneralizable to a variety of GMs showing an improvement over prior work by an\naverage precision of 10% averaged across 12 GMs. Our code is available at\nhttps://www.github.com/vishal3477/proactive_IMD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1\">Vishal Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1\">Tal Hassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16297","description":"<p>Object detection and forecasting are fundamental components of embodied\nperception. These two problems, however, are largely studied in isolation by\nthe community. In this paper, we propose an end-to-end approach for detection\nand motion forecasting based on raw sensor measurement as opposed to ground\ntruth tracks. Instead of predicting the current frame locations and forecasting\nforward in time, we directly predict future object locations and backcast to\ndetermine where each trajectory began. Our approach not only improves overall\naccuracy compared to other modular or end-to-end baselines, it also prompts us\nto rethink the role of explicit tracking for embodied perception. Additionally,\nby linking future and current locations in a many-to-one manner, our approach\nis able to reason about multiple futures, a capability that was previously\nconsidered difficult for end-to-end approaches. We conduct extensive\nexperiments on the popular nuScenes dataset and demonstrate the empirical\neffectiveness of our approach. In addition, we investigate the appropriateness\nof reusing standard forecasting metrics for an end-to-end setup, and find a\nnumber of limitations which allow us to build simple baselines to game these\nmetrics. We address this issue with a novel set of joint forecasting and\ndetection metrics that extend the commonly used AP metrics from the detection\ncommunity to measuring forecasting accuracy. Our code is available at\nhttps://github.com/neeharperi/FutureDet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1\">Neehar Peri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luiten_J/0/1/0/all/0/1\">Jonathon Luiten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengtian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljo&#x161;a O&#x161;ep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The impact of using voxel-level segmentation metrics on evaluating multifocal prostate cancer localisation. (arXiv:2203.16415v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.16415","description":"<p>Dice similarity coefficient (DSC) and Hausdorff distance (HD) are widely used\nfor evaluating medical image segmentation. They have also been criticised, when\nreported alone, for their unclear or even misleading clinical interpretation.\nDSCs may also differ substantially from HDs, due to boundary smoothness or\nmultiple regions of interest (ROIs) within a subject. More importantly, either\nmetric can also have a nonlinear, non-monotonic relationship with outcomes\nbased on Type 1 and 2 errors, designed for specific clinical decisions that use\nthe resulting segmentation. Whilst cases causing disagreement between these\nmetrics are not difficult to postulate. This work first proposes a new\nasymmetric detection metric, adapting those used in object detection, for\nplanning prostate cancer procedures. The lesion-level metrics is then compared\nwith the voxel-level DSC and HD, whereas a 3D UNet is used for segmenting\nlesions from multiparametric MR (mpMR) images. Based on experimental results we\nreport pairwise agreement and correlation 1) between DSC and HD, and 2) between\nvoxel-level DSC and recall-controlled precision at lesion-level, with Cohen's\n[0.49, 0.61] and Pearson's [0.66, 0.76] (p-values}&lt;0.001) at varying cut-offs.\nHowever, the differences in false-positives and false-negatives, between the\nactual errors and the perceived counterparts if DSC is used, can be as high as\n152 and 154, respectively, out of the 357 test set lesions. We therefore\ncarefully conclude that, despite of the significant correlations, voxel-level\nmetrics such as DSC can misrepresent lesion-level detection accuracy for\nevaluating localisation of multifocal prostate cancer and should be interpreted\nwith caution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yan_W/0/1/0/all/0/1\">Wen Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Syer_T/0/1/0/all/0/1\">Tom Syer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Min_Z/0/1/0/all/0/1\">Zhe Min</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Punwani_S/0/1/0/all/0/1\">Shonit Punwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emberton_M/0/1/0/all/0/1\">Mark Emberton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_B/0/1/0/all/0/1\">Bernard Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast, Accurate and Memory-Efficient Partial Permutation Synchronization. (arXiv:2203.16505v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16505","description":"<p>Previous partial permutation synchronization (PPS) algorithms, which are\ncommonly used for multi-object matching, often involve computation-intensive\nand memory-demanding matrix operations. These operations become intractable for\nlarge scale structure-from-motion datasets. For pure permutation\nsynchronization, the recent Cycle-Edge Message Passing (CEMP) framework\nsuggests a memory-efficient and fast solution. Here we overcome the restriction\nof CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for\nestimating the corruption levels of the observed partial permutations. It\nallows us to subsequently implement a nonconvex weighted projected power method\nwithout the need of spectral initialization. The resulting new PPS algorithm,\nMatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse\nmatrix operations, and thus enjoys lower time and space complexities in\ncomparison to previous PPS algorithms. We prove that under adversarial\ncorruption, though without additive noise and with certain assumptions,\nCEMP-Partial is able to exactly classify corrupted and clean partial\npermutations. We demonstrate the state-of-the-art accuracy, speed and memory\nefficiency of our method on both synthetic and real datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunpeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_G/0/1/0/all/0/1\">Gilad Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaMixer: A Fast-Converging Query-Based Object Detector. (arXiv:2203.16507v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16507","description":"<p>Traditional object detectors employ the dense paradigm of scanning over\nlocations and scales in an image. The recent query-based object detectors break\nthis convention by decoding image features with a set of learnable queries.\nHowever, this paradigm still suffers from slow convergence, limited\nperformance, and design complexity of extra networks between backbone and\ndecoder. In this paper, we find that the key to these issues is the\nadaptability of decoders for casting queries to varying objects. Accordingly,\nwe propose a fast-converging query-based detector, named AdaMixer, by improving\nthe adaptability of query-based decoding processes in two aspects. First, each\nquery adaptively samples features over space and scales based on estimated\noffsets, which allows AdaMixer to efficiently attend to the coherent regions of\nobjects. Then, we dynamically decode these sampled features with an adaptive\nMLP-Mixer under the guidance of each query. Thanks to these two critical\ndesigns, AdaMixer enjoys architectural simplicity without requiring dense\nattentional encoders or explicit pyramid networks. On the challenging MS COCO\nbenchmark, AdaMixer with ResNet-50 as the backbone, with 12 training epochs,\nreaches up to 45.0 AP on the validation set along with 27.9 APs in detecting\nsmall objects. With the longer training scheme, AdaMixer with ResNeXt-101-DCN\nand Swin-S reaches 49.5 and 51.3 AP. Our work sheds light on a simple,\naccurate, and fast converging architecture for query-based object detectors.\nThe code is made available at https://github.com/MCG-NJU/AdaMixer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Ziteng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. (arXiv:2112.03028v2 [cs.RO] CROSS LISTED)","link":"http://arxiv.org/abs/2112.03028","description":"<p>We introduce the dynamic grasp synthesis task: given an object with a known\n6D pose and a grasp reference, our goal is to generate motions that move the\nobject to a target 6D pose. This is challenging, because it requires reasoning\nabout the complex articulation of the human hand and the intricate physical\ninteraction with the object. We propose a novel method that frames this problem\nin the reinforcement learning framework and leverages a physics simulation,\nboth to learn and to evaluate such dynamic interactions. A hierarchical\napproach decomposes the task into low-level grasping and high-level motion\nsynthesis. It can be used to generate novel hand sequences that approach,\ngrasp, and move an object to a desired location, while retaining\nhuman-likeness. We show that our approach leads to stable grasps and generates\na wide range of motions. Furthermore, even imperfect labels can be corrected by\nour method to generate dynamic interaction sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christen_S/0/1/0/all/0/1\">Sammy Christen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksan_E/0/1/0/all/0/1\">Emre Aksan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwangbo_J/0/1/0/all/0/1\">Jemin Hwangbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2203.01522","description":"<p>Despite the success of deep neural networks, there are still many challenges\nin deep representation learning due to the data scarcity issues such as data\nimbalance, unseen distribution, and domain shift. To address the\nabove-mentioned issues, a variety of methods have been devised to explore the\nsample relationships in a vanilla way (i.e., from the perspectives of either\nthe input or the loss function), failing to explore the internal structure of\ndeep neural networks for learning with sample relationships. Inspired by this,\nwe propose to enable deep neural networks themselves with the ability to learn\nthe sample relationships from each mini-batch. Specifically, we introduce a\nbatch transformer module or BatchFormer, which is then applied into the batch\ndimension of each mini-batch to implicitly explore sample relationships during\ntraining. By doing this, the proposed method enables the collaboration of\ndifferent samples, e.g., the head-class samples can also contribute to the\nlearning of the tail classes for long-tailed recognition. Furthermore, to\nmitigate the gap between training and testing, we share the classifier between\nwith or without the BatchFormer during training, which can thus be removed\nduring testing. We perform extensive experiments on over ten datasets and the\nproposed method achieves significant improvements on different data scarcity\napplications without any bells and whistles, including the tasks of long-tailed\nrecognition, compositional zero-shot learning, domain generalization, and\ncontrastive learning. Code will be made publicly available at\nhttps://github.com/zhihou7/BatchFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}