{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Improving In-Context Few-Shot Learning via Self-Supervised Training. (arXiv:2205.01703v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01703","description":"<p>Self-supervised pretraining has made few-shot learning possible for many NLP\ntasks. But the pretraining objectives are not typically adapted specifically\nfor in-context few-shot learning. In this paper, we propose to use\nself-supervision in an intermediate training stage between pretraining and\ndownstream few-shot usage with the goal to teach the model to perform\nin-context few shot learning. We propose and evaluate four self-supervised\nobjectives on two benchmarks. We find that the intermediate self-supervision\nstage produces models that outperform strong baselines. Ablation study shows\nthat several factors affect the downstream performance, such as the amount of\ntraining data and the diversity of the self-supervised objectives.\nHuman-annotated cross-task supervision and self-supervision are complementary.\nQualitative analysis suggests that the self-supervised-trained models are\nbetter at following task requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srini Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Veselin Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozareva_Z/0/1/0/all/0/1\">Zornitsa Kozareva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't sweat the small stuff, classify the rest: Sample Shielding to protect text classifiers against adversarial attacks. (arXiv:2205.01714v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01714","description":"<p>Deep learning (DL) is being used extensively for text classification.\nHowever, researchers have demonstrated the vulnerability of such classifiers to\nadversarial attacks. Attackers modify the text in a way which misleads the\nclassifier while keeping the original meaning close to intact. State-of-the-art\n(SOTA) attack algorithms follow the general principle of making minimal changes\nto the text so as to not jeopardize semantics. Taking advantage of this we\npropose a novel and intuitive defense strategy called Sample Shielding. It is\nattacker and classifier agnostic, does not require any reconfiguration of the\nclassifier or external resources and is simple to implement. Essentially, we\nsample subsets of the input text, classify them and summarize these into a\nfinal decision. We shield three popular DL text classifiers with Sample\nShielding, test their resilience against four SOTA attackers across three\ndatasets in a realistic threat setting. Even when given the advantage of\nknowing about our shielding strategy the adversary's attack success rate is\n&lt;=10% with only one exception and often &lt; 5%. Additionally, Sample Shielding\nmaintains near original accuracy when applied to original texts. Crucially, we\nshow that the `make minimal changes' approach of SOTA attackers leads to\ncritical vulnerabilities that can be defended against with an intuitive\nsampling strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rusert_J/0/1/0/all/0/1\">Jonathan Rusert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Padmini Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quiz Design Task: Helping Teachers Create Quizzes with Automated Question Generation. (arXiv:2205.01730v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01730","description":"<p>Question generation (QGen) models are often evaluated with standardized NLG\nmetrics that are based on n-gram overlap. In this paper, we measure whether\nthese metric improvements translate to gains in a practical setting, focusing\non the use case of helping teachers automate the generation of reading\ncomprehension quizzes. In our study, teachers building a quiz receive question\nsuggestions, which they can either accept or refuse with a reason. Even though\nwe find that recent progress in QGen leads to a significant increase in\nquestion acceptance rates, there is still large room for improvement, with the\nbest model having only 68.4% of its questions accepted by the ten teachers who\nparticipated in our study. We then leverage the annotations we collected to\nanalyze standard NLG metrics and find that model performance has reached\nprojected upper-bounds, suggesting new automatic metrics are needed to guide\nQGen research forward.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakhovska_L/0/1/0/all/0/1\">Lidiya Murakhovs&#x27;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Themes of Revenge: Automatic Identification of Vengeful Content in Textual Data. (arXiv:2205.01731v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01731","description":"<p>Revenge is a powerful motivating force reported to underlie the behavior of\nvarious solo perpetrators, from school shooters to right wing terrorists. In\nthis paper, we develop an automated methodology for identifying vengeful themes\nin textual data. Testing the model on four datasets (vengeful texts from social\nmedia, school shooters, Right Wing terrorist and Islamic terrorists), we\npresent promising results, even when the methodology is tested on extremely\nimbalanced datasets. The paper not only presents a simple and powerful\nmethodology that may be used for the screening of solo perpetrators but also\nvalidate the simple theoretical model of revenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neuman_Y/0/1/0/all/0/1\">Yair Neuman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erez_E/0/1/0/all/0/1\">Eden Shalom Erez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschantret_J/0/1/0/all/0/1\">Joshua Tschantret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_H/0/1/0/all/0/1\">Hayden Weiss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-effects transformers for hierarchical adaptation. (arXiv:2205.01749v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01749","description":"<p>Language use differs dramatically from context to context. To some degree,\nmodern language models like GPT-3 are able to account for such variance by\nconditioning on a string of previous input text, or prompt. Yet prompting is\nineffective when contexts are sparse, out-of-sample, or extra-textual; for\ninstance, accounting for when and where the text was produced or who produced\nit. In this paper, we introduce the mixed-effects transformer (MET), a novel\napproach for learning hierarchically-structured prefixes -- lightweight modules\nprepended to the input -- to account for structured variation. Specifically, we\nshow how the popular class of mixed-effects models may be extended to\ntransformer-based architectures using a regularized prefix-tuning procedure\nwith dropout. We evaluate this approach on several domain-adaptation\nbenchmarks, finding that it efficiently adapts to novel contexts with minimal\ndata while still effectively generalizing to unseen contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Julia White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert Hawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training. (arXiv:2205.01751v1 [cs.SD])","link":"http://arxiv.org/abs/2205.01751","description":"<p>In this paper, we explore an improved framework to train a monoaural neural\nenhancement model for robust speech recognition. The designed training\nframework extends the existing mixture invariant training criterion to exploit\nboth unpaired clean speech and real noisy data. It is found that the unpaired\nclean speech is crucial to improve quality of separated speech from real noisy\nspeech. The proposed method also performs remixing of processed and unprocessed\nsignals to alleviate the processing artifacts. Experiments on the\nsingle-channel CHiME-3 real test sets show that the proposed method improves\nsignificantly in terms of speech recognition performance over the enhancement\nsystem trained either on the mismatched simulated data in a supervised fashion\nor on the matched real data in an unsupervised fashion. Between 16% and 39%\nrelative WER reduction has been achieved by the proposed system compared to the\nunprocessed signal using end-to-end and hybrid acoustic models without\nretraining on distorted data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jisi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorila_C/0/1/0/all/0/1\">Catalin Zorila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1\">Jon Barker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLTime: A Cross-Lingual Knowledge Transfer Framework for Temporal Expression Extraction. (arXiv:2205.01757v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01757","description":"<p>Temporal Expression Extraction (TEE) is essential for understanding time in\nnatural language. It has applications in Natural Language Processing (NLP)\ntasks such as question answering, information retrieval, and causal inference.\nTo date, work in this area has mostly focused on English as there is a scarcity\nof labeled data for other languages. We propose XLTime, a novel framework for\nmultilingual TEE. XLTime works on top of pre-trained language models and\nleverages multi-task learning to prompt cross-language knowledge transfer both\nfrom English and within the non-English languages. XLTime alleviates problems\ncaused by a shortage of data in the target language. We apply XLTime with\ndifferent language models and show that it outperforms the previous automatic\nSOTA methods on French, Spanish, Portuguese, and Basque, by large margins.\nXLTime also closes the gap considerably on the handcrafted HeidelTime method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuwei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groves_W/0/1/0/all/0/1\">William Groves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_T/0/1/0/all/0/1\">Tanay Kumar Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel R. Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alex Jaimes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explain and Conquer: Personalised Text-based Reviews to Achieve Transparency. (arXiv:2205.01759v1 [cs.LG])","link":"http://arxiv.org/abs/2205.01759","description":"<p>There are many contexts where dyadic data is present. Social networking is a\nwell-known example, where transparency has grown on importance. In these\ncontexts, pairs of items are linked building a network where interactions play\na crucial role. Explaining why these relationships are established is core to\naddress transparency. These explanations are often presented using text, thanks\nto the spread of the natural language understanding tasks.\n</p>\n<p>We have focused on the TripAdvisor platform, considering the applicability to\nother dyadic data contexts. The items are a subset of users and restaurants and\nthe interactions the reviews posted by these users. Our aim is to represent and\nexplain pairs (user, restaurant) established by agents (e.g., a recommender\nsystem or a paid promotion mechanism), so that personalisation is taken into\naccount. We propose the PTER (Personalised TExt-based Reviews) model. We\npredict, from the available reviews for a given restaurant, those that fit to\nthe specific user interactions.\n</p>\n<p>PTER leverages the BERT (Bidirectional Encoders Representations from\nTransformers) language model. We customised a deep neural network following the\nfeature-based approach. The performance metrics show the validity of our\nlabelling proposal. We defined an evaluation framework based on a clustering\nprocess to assess our personalised representation. PTER clearly outperforms the\nproposed adversary in 5 of the 6 datasets, with a minimum ratio improvement of\n4%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botana_I/0/1/0/all/0/1\">I&#xf1;igo L&#xf3;pez-Riob&#xf3;o Botana</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bolon_Canedo_V/0/1/0/all/0/1\">Ver&#xf3;nica Bol&#xf3;n-Canedo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1\">Bertha Guijarro-Berdi&#xf1;as</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Betanzos_A/0/1/0/all/0/1\">Amparo Alonso-Betanzos</a> (1) ((1) University of A Coru&#xf1;a - Research Center on Information and Communication Technologies (CITIC))"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific Explanation and Natural Language: A Unified Epistemological-Linguistic Perspective for Explainable AI. (arXiv:2205.01809v1 [cs.AI])","link":"http://arxiv.org/abs/2205.01809","description":"<p>A fundamental research goal for Explainable AI (XAI) is to build models that\nare capable of reasoning through the generation of natural language\nexplanations. However, the methodologies to design and evaluate\nexplanation-based inference models are still poorly informed by theoretical\naccounts on the nature of explanation. As an attempt to provide an\nepistemologically grounded characterisation for XAI, this paper focuses on the\nscientific domain, aiming to bridge the gap between theory and practice on the\nnotion of a scientific explanation. Specifically, the paper combines a detailed\nsurvey of the modern accounts of scientific explanation in Philosophy of\nScience with a systematic analysis of corpora of natural language explanations,\nclarifying the nature and function of explanatory arguments from both a\ntop-down (categorical) and a bottom-up (corpus-based) perspective. Through a\nmixture of quantitative and qualitative methodologies, the presented study\nallows deriving the following main conclusions: (1) Explanations cannot be\nentirely characterised in terms of inductive or deductive arguments as their\nmain function is to perform unification; (2) An explanation must cite causes\nand mechanisms that are responsible for the occurrence of the event to be\nexplained; (3) While natural language explanations possess an intrinsic\ncausal-mechanistic nature, they are not limited to causes and mechanisms, also\naccounting for pragmatic elements such as definitions, properties and taxonomic\nrelations (4) Patterns of unification naturally emerge in corpora of\nexplanations even if not intentionally modelled; (5) Unification is realised\nthrough a process of abstraction, whose function is to provide the inference\nsubstrate for subsuming the event to be explained under recurring patterns and\nhigh-level regularities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Holistic Framework for Analyzing the COVID-19 Vaccine Debate. (arXiv:2205.01817v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01817","description":"<p>The Covid-19 pandemic has led to infodemic of low quality information leading\nto poor health decisions. Combating the outcomes of this infodemic is not only\na question of identifying false claims, but also reasoning about the decisions\nindividuals make. In this work we propose a holistic analysis framework\nconnecting stance and reason analysis, and fine-grained entity level moral\nsentiment analysis. We study how to model the dependencies between the\ndifferent level of analysis and incorporate human insights into the learning\nprocess. Experiments show that our framework provides reliable predictions even\nin the low-supervision settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_M/0/1/0/all/0/1\">Maria Leonor Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_M/0/1/0/all/0/1\">Monal Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shor_A/0/1/0/all/0/1\">Andrey Shor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Ming Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"i-Code: An Integrative and Composable Multimodal Learning Framework. (arXiv:2205.01818v1 [cs.LG])","link":"http://arxiv.org/abs/2205.01818","description":"<p>Human intelligence is multimodal; we integrate visual, linguistic, and\nacoustic signals to maintain a holistic worldview. Most current pretraining\nmethods, however, are limited to one or two modalities. We present i-Code, a\nself-supervised pretraining framework where users may flexibly combine the\nmodalities of vision, speech, and language into unified and general-purpose\nvector representations. In this framework, data from each modality are first\ngiven to pretrained single-modality encoders. The encoder outputs are then\nintegrated with a multimodal fusion network, which uses novel attention\nmechanisms and other architectural innovations to effectively combine\ninformation from the different modalities. The entire system is pretrained\nend-to-end with new objectives including masked modality unit modeling and\ncross-modality contrastive learning. Unlike previous research using only video\nfor pretraining, the i-Code framework can dynamically process single, dual, and\ntriple-modality data during training and inference, flexibly projecting\ndifferent combinations of modalities into a single representation space.\nExperimental results demonstrate how i-Code can outperform state-of-the-art\ntechniques on five video understanding tasks and the GLUE NLP benchmark,\nimproving by as much as 11% and demonstrating the power of integrative\nmultimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gmyr_R/0/1/0/all/0/1\">Robert Gmyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuedong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features. (arXiv:2205.01821v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01821","description":"<p>Poetry generation, and creative language generation in general, usually\nsuffers from the lack of large training data. In this paper, we present a novel\nframework to generate sonnets that does not require training on poems. We\ndesign a hierarchical framework which plans the poem sketch before decoding.\nSpecifically, a content planning module is trained on non-poetic texts to\nobtain discourse-level coherence; then a rhyme module generates rhyme words and\na polishing module introduces imagery and similes for aesthetics purposes.\nFinally, we design a constrained decoding algorithm to impose the\nmeter-and-rhyme constraint of the generated sonnets. Automatic and human\nevaluation show that our multi-stage approach without training on poem corpora\ngenerates more coherent, poetic, and creative sonnets than several strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AmbiPun: Generating Humorous Puns with Ambiguous Context. (arXiv:2205.01825v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01825","description":"<p>In this paper, we propose a simple yet effective way to generate pun\nsentences that does not require any training on existing puns. Our approach is\ninspired by humor theories that ambiguity comes from the context rather than\nthe pun word itself. Given a pair of definitions of a pun word, our model first\nproduces a list of related concepts through a reverse dictionary. We then\nutilize one-shot GPT3 to generate context words and then generate puns\nincorporating context words from both concepts. Human evaluation shows that our\nmethod successfully generates pun 52\\% of the time, outperforming well-crafted\nbaselines and the state-of-the-art models by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anirudh Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Semantic Typing with Meaningful Label Inference. (arXiv:2205.01826v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01826","description":"<p>Semantic typing aims at classifying tokens or spans of interest in a textual\ncontext into semantic categories such as relations, entity types, and event\ntypes. The inferred labels of semantic categories meaningfully interpret how\nmachines understand components of text. In this paper, we present UniST, a\nunified framework for semantic typing that captures label semantics by\nprojecting both inputs and labels into a joint semantic embedding space. To\nformulate different lexical and relational semantic typing tasks as a unified\ntask, we incorporate task descriptions to be jointly encoded with the input,\nallowing UniST to be adapted to different tasks without introducing\ntask-specific model components. UniST optimizes a margin ranking loss such that\nthe semantic relatedness of the input and labels is reflected from their\nembedding similarity. Our experiments demonstrate that UniST achieves strong\nperformance across three semantic typing tasks: entity typing, relation\nclassification and event typing. Meanwhile, UniST effectively transfers\nsemantic knowledge of labels and substantially improves generalizability on\ninferring rarely seen and unseen types. In addition, multiple semantic typing\ntasks can be jointly trained within the unified framework, leading to a single\ncompact multi-tasking model that performs comparably to dedicated single-task\nmodels, while offering even better transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">James Y. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bangzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Great Truths are Always Simple: A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models. (arXiv:2205.01841v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01841","description":"<p>Commonsense reasoning in natural language is a desired ability of artificial\nintelligent systems. For solving complex commonsense reasoning tasks, a typical\nsolution is to enhance pre-trained language models~(PTMs) with a\nknowledge-aware graph neural network~(GNN) encoder that models a commonsense\nknowledge graph~(CSKG). Despite the effectiveness, these approaches are built\non heavy architectures, and can't clearly explain how external knowledge\nresources improve the reasoning capacity of PTMs. Considering this issue, we\nconduct a deep empirical analysis, and find that it is indeed relation features\nfrom CSKGs (but not node features) that mainly contribute to the performance\nimprovement of PTMs. Based on this finding, we design a simple MLP-based\nknowledge encoder that utilizes statistical relation paths as features.\nExtensive experiments conducted on five benchmarks demonstrate the\neffectiveness of our approach, which also largely reduces the parameters for\nencoding CSKGs. Our codes and data are publicly available at\nhttps://github.com/RUCAIBox/SAFE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds. (arXiv:2205.01845v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01845","description":"<p>Discovering latent topics from text corpora has been studied for decades.\nMany existing topic models adopt a fully unsupervised setting, and their\ndiscovered topics may not cater to users' particular interests due to their\ninability of leveraging user guidance. Although there exist seed-guided topic\ndiscovery approaches that leverage user-provided seeds to discover\ntopic-representative terms, they are less concerned with two factors: (1) the\nexistence of out-of-vocabulary seeds and (2) the power of pre-trained language\nmodels (PLMs). In this paper, we generalize the task of seed-guided topic\ndiscovery to allow out-of-vocabulary seeds. We propose a novel framework, named\nSeeTopic, wherein the general knowledge of PLMs and the local semantics learned\nfrom the input corpus can mutually benefit each other. Experiments on three\nreal datasets from different domains demonstrate the effectiveness of SeeTopic\nin terms of topic coherence, accuracy, and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Commonsense in Pretrained Unimodal and Multimodal Models. (arXiv:2205.01850v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01850","description":"<p>Our commonsense knowledge about objects includes their typical visual\nattributes; we know that bananas are typically yellow or green, and not purple.\nText and image corpora, being subject to reporting bias, represent this\nworld-knowledge to varying degrees of faithfulness. In this paper, we\ninvestigate to what degree unimodal (language-only) and multimodal (image and\nlanguage) models capture a broad range of visually salient attributes. To that\nend, we create the Visual Commonsense Tests (ViComTe) dataset covering 5\nproperty types (color, shape, material, size, and visual co-occurrence) for\nover 5000 subjects. We validate this dataset by showing that our grounded color\ndata correlates much better than ungrounded text-only data with crowdsourced\ncolor judgments provided by Paik et al. (2021). We then use our dataset to\nevaluate pretrained unimodal models and multimodal models. Our results indicate\nthat multimodal models better reconstruct attribute distributions, but are\nstill subject to reporting bias. Moreover, increasing model size does not\nenhance performance, suggesting that the key to visual commonsense lies in the\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provably Confidential Language Modelling. (arXiv:2205.01863v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01863","description":"<p>Large language models are shown to memorize privacy information such as\nsocial security numbers in training data. Given the sheer scale of the training\ncorpus, it is challenging to screen and filter these privacy data, either\nmanually or automatically. In this paper, we propose Confidentially Redacted\nTraining (CRT), a method to train language generation models while protecting\nthe confidential segments. We borrow ideas from differential privacy (which\nsolves a related but distinct problem) and show that our method is able to\nprovably prevent unintended memorization by randomizing parts of the training\nprocess. Moreover, we show that redaction with an approximately correct\nscreening policy amplifies the confidentiality guarantee. We implement the\nmethod for both LSTM and GPT language models. Our experimental results show\nthat the models trained by CRT obtain almost the same perplexity while\npreserving strong confidentiality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Entity Interactions for Few-Shot Relation Learning (Student Abstract). (arXiv:2205.01878v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01878","description":"<p>Few-shot relation learning refers to infer facts for relations with a limited\nnumber of observed triples. Existing metric-learning methods for this problem\nmostly neglect entity interactions within and between triples. In this paper,\nwe explore this kind of fine-grained semantic meanings and propose our model\nTransAM. Specifically, we serialize reference entities and query entities into\nsequence and apply transformer structure with local-global attention to capture\nboth intra- and inter-triple entity interactions. Experiments on two public\nbenchmark datasets NELL-One and Wiki-One with 1-shot setting prove the\neffectiveness of TransAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">YI Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You May Need for VQA are Image Captions. (arXiv:2205.01883v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01883","description":"<p>Visual Question Answering (VQA) has benefited from increasingly sophisticated\nmodels, but has not enjoyed the same level of engagement in terms of data\ncreation. In this paper, we propose a method that automatically derives VQA\nexamples at volume, by leveraging the abundance of existing image-caption\nannotations combined with neural models for textual question generation. We\nshow that the resulting data is of high-quality. VQA models trained on our data\nimprove state-of-the-art zero-shot accuracy by double digits and achieve a\nlevel of robustness that lacks in the same model trained on human-annotated VQA\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukliansky_D/0/1/0/all/0/1\">Doron Kukliansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P$^3$ Ranker: Mitigating the Gaps between Pre-training and Ranking Fine-tuning with Prompt-based Learning and Pre-finetuning. (arXiv:2205.01886v1 [cs.IR])","link":"http://arxiv.org/abs/2205.01886","description":"<p>Compared to other language tasks, applying pre-trained language models (PLMs)\nfor search ranking often requires more nuances and training signals. In this\npaper, we identify and study the two mismatches between pre-training and\nranking fine-tuning: the training schema gap regarding the differences in\ntraining objectives and model architectures, and the task knowledge gap\nconsidering the discrepancy between the knowledge needed in ranking and that\nlearned during pre-training. To mitigate these gaps, we propose Pre-trained,\nPrompt-learned and Pre-finetuned Neural Ranker (P$^3$ Ranker). P$^3$ Ranker\nleverages prompt-based learning to convert the ranking task into a pre-training\nlike schema and uses pre-finetuning to initialize the model on intermediate\nsupervised tasks. Experiments on MS MARCO and Robust04 show the superior\nperformances of P$^3$ Ranker in few-shot ranking. Analyses reveal that P$^3$\nRanker is able to better accustom to the ranking task through prompt-based\nlearning and retrieve necessary ranking-oriented knowledge gleaned in\npre-finetuning, resulting in data-efficient PLM adaptation. Our code is\navailable at \\url{https://github.com/NEUIR/P3Ranker}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaomeng Hu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shi Yu</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Ge Yu</a> (1) ((1) Northeastern University, (2) Tsinghua University, (3) Microsoft Research)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness. (arXiv:2205.01889v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01889","description":"<p>A notable challenge in Multi-Document Summarization (MDS) is the\nextremely-long length of the input. In this paper, we present an\nextract-then-abstract Transformer framework to overcome the problem.\nSpecifically, we leverage pre-trained language models to construct a\nhierarchical extractor for salient sentence selection across documents and an\nabstractor for rewriting the selected contents as summaries. However, learning\nsuch a framework is challenging since the optimal contents for the abstractor\nare generally unknown. Previous works typically create pseudo extraction oracle\nto enable the supervised learning for both the extractor and the abstractor.\nNevertheless, we argue that the performance of such methods could be restricted\ndue to the insufficient information for prediction and inconsistent objectives\nbetween training and testing. To this end, we propose a loss weighting\nmechanism that makes the model aware of the unequal importance for the\nsentences not in the pseudo extraction oracle, and leverage the fine-tuned\nabstractor to generate summary references as auxiliary signals for learning the\nextractor. Moreover, we propose a reinforcement learning method that can\nefficiently apply to the extractor for harmonizing the optimization between\ntraining and testing. Experiment results show that our framework substantially\noutperforms strong baselines with comparable model sizes and achieves the best\nresults on the Multi-News, Multi-XScience, and WikiCatSum corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yun-Zhu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Syuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts. (arXiv:2205.01898v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01898","description":"<p>Stories or narratives are comprised of a sequence of events. To compose\ninteresting stories, professional writers often leverage a creative writing\ntechnique called flashback that inserts past events into current storylines as\nwe commonly observe in novels and plays. However, it is challenging for\nmachines to generate flashback as it requires a solid understanding of event\ntemporal order (e.g. \"feeling hungry\" before \"eat,\" not vice versa), and the\ncreativity to arrange storylines so that earlier events do not always appear\nfirst in narrative order. Two major issues in existing systems that exacerbate\nthe challenges: 1) temporal bias in pertaining and story datasets that leads to\nmonotonic event temporal orders; 2) lack of explicit guidance that helps\nmachines decide where to insert flashbacks. We propose to address these issues\nusing structured storylines to encode events and their pair-wise temporal\nrelations (before, after and vague) as temporal prompts that guide how stories\nshould unfold temporally. We leverage a Plan-and-Write framework enhanced by\nreinforcement learning to generate storylines and stories end-to-end.\nEvaluation results show that the proposed method can generate more interesting\nstories with flashbacks while maintaining textual diversity, fluency, and\ntemporal coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Word Embeddings in Hyperbolic Space. (arXiv:2205.01907v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01907","description":"<p>Cross-lingual word embeddings can be applied to several natural language\nprocessing applications across multiple languages. Unlike prior works that use\nword embeddings based on the Euclidean space, this short paper presents a\nsimple and effective cross-lingual Word2Vec model that adapts to the Poincar\\'e\nball model of hyperbolic space to learn unsupervised cross-lingual word\nrepresentations from a German-English parallel corpus. It has been shown that\nhyperbolic embeddings can capture and preserve hierarchical relationships. We\nevaluate the model on both hypernymy and analogy tasks. The proposed model\nachieves comparable performance with the vanilla Word2Vec model on the\ncross-lingual analogy task, the hypernymy task shows that the cross-lingual\nPoincar\\'e Word2Vec model can capture latent hierarchical structure from free\ntext across languages, which are absent from the Euclidean-based Word2Vec\nrepresentations. Our results show that by preserving the latent hierarchical\ninformation, hyperbolic spaces can offer better representations for\ncross-lingual embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Mudit Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Task Interactions in Document-Level Joint Entity and Relation Extraction. (arXiv:2205.01909v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01909","description":"<p>We target on the document-level relation extraction in an end-to-end setting,\nwhere the model needs to jointly perform mention extraction, coreference\nresolution (COREF) and relation extraction (RE) at once, and gets evaluated in\nan entity-centric way. Especially, we address the two-way interaction between\nCOREF and RE that has not been the focus by previous work, and propose to\nintroduce explicit interaction namely Graph Compatibility (GC) that is\nspecifically designed to leverage task characteristics, bridging decisions of\ntwo tasks for direct task interference. Our experiments are conducted on DocRED\nand DWIE; in addition to GC, we implement and compare different multi-task\nsettings commonly adopted in previous work, including pipeline, shared\nencoders, graph propagation, to examine the effectiveness of different\ninteractions. The result shows that GC achieves the best performance by up to\n2.3/5.1 F1 improvement over the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexical Knowledge Internalization for Neural Dialog Generation. (arXiv:2205.01941v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01941","description":"<p>We propose knowledge internalization (KI), which aims to complement the\nlexical knowledge into neural dialog models. Instead of further conditioning\nthe knowledge-grounded dialog (KGD) models on externally retrieved knowledge,\nwe seek to integrate knowledge about each input token internally into the\nmodel's parameters. To tackle the challenge due to the large scale of lexical\nknowledge, we adopt the contrastive learning approach and create an effective\ntoken-level lexical knowledge retriever that requires only weak supervision\nmined from Wikipedia. We demonstrate the effectiveness and general\napplicability of our approach on various datasets and diversified model\nstructures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_B/0/1/0/all/0/1\">Ben Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem. (arXiv:2205.01954v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01954","description":"<p>Word embeddings are one of the most fundamental technologies used in natural\nlanguage processing. Existing word embeddings are high-dimensional and consume\nconsiderable computational resources. In this study, we propose WordTour,\nunsupervised one-dimensional word embeddings. To achieve the challenging goal,\nwe propose a decomposition of the desiderata of word embeddings into two parts,\ncompleteness and soundness, and focus on soundness in this paper. Owing to the\nsingle dimensionality, WordTour is extremely efficient and provides a minimal\nmeans to handle word embeddings. We experimentally confirmed the effectiveness\nof the proposed method via user study and document classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Autoregressive Machine Translation: It's Not as Fast as it Seems. (arXiv:2205.01966v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01966","description":"<p>Efficient machine translation models are commercially important as they can\nincrease inference speeds, and reduce costs and carbon emissions. Recently,\nthere has been much interest in non-autoregressive (NAR) models, which promise\nfaster translation. In parallel to the research on NAR models, there have been\nsuccessful attempts to create optimized autoregressive models as part of the\nWMT shared task on efficient translation. In this paper, we point out flaws in\nthe evaluation methodology present in the literature on NAR models and we\nprovide a fair comparison between a state-of-the-art NAR model and the\nautoregressive submissions to the shared task. We make the case for consistent\nevaluation of NAR models, and also for the importance of comparing NAR models\nwith other widely used methods for improving efficiency. We run experiments\nwith a connectionist-temporal-classification-based (CTC) NAR model implemented\nin C++ and compare it with AR models using wall clock times. Our results show\nthat, although NAR models are faster on GPUs, with small batch sizes, they are\nalmost always slower under more realistic usage conditions. We call for more\nrealistic and extensive evaluation of NAR models in future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helcl_J/0/1/0/all/0/1\">Jind&#x159;ich Helcl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning to Social Norms and Values in Interactive Narratives. (arXiv:2205.01975v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01975","description":"<p>We focus on creating agents that act in alignment with socially beneficial\nnorms and values in interactive narratives or text-based games -- environments\nwherein an agent perceives and interacts with a world through natural language.\nSuch interactive agents are often trained via reinforcement learning to\noptimize task performance, even when such rewards may lead to agent behaviors\nthat violate societal norms -- causing harm either to the agent itself or other\nentities in the environment. Social value alignment refers to creating agents\nwhose behaviors conform to expected moral and social norms for a given context\nand group of people -- in our case, it means agents that behave in a manner\nthat is less harmful and more beneficial for themselves and others.\n</p>\n<p>We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25\nannotated interactive narratives containing thousands of morally salient\nscenarios covering everything from theft and bodily harm to altruism. We\nintroduce the GALAD (Game-value ALignment through Action Distillation) agent\nthat uses the social commonsense knowledge present in specially trained\nlanguage models to contextually restrict its action space to only those actions\nthat are aligned with socially beneficial values. An experimental study shows\nthat the GALAD agent makes decisions efficiently enough to improve\nstate-of-the-art task performance by 4% while reducing the frequency of\nsocially harmful behaviors by 25% compared to strong contemporary value\nalignment approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ON-TRAC Consortium Systems for the IWSLT 2022 Dialect and Low-resource Speech Translation Tasks. (arXiv:2205.01987v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01987","description":"<p>This paper describes the ON-TRAC Consortium translation systems developed for\ntwo challenge tracks featured in the Evaluation Campaign of IWSLT 2022:\nlow-resource and dialect speech translation. For the Tunisian Arabic-English\ndataset (low-resource and dialect tracks), we build an end-to-end model as our\njoint primary submission, and compare it against cascaded models that leverage\na large fine-tuned wav2vec 2.0 model for ASR. Our results show that in our\nsettings pipeline approaches are still very competitive, and that with the use\nof transfer learning, they can outperform end-to-end models for speech\ntranslation (ST). For the Tamasheq-French dataset (low-resource track) our\nprimary submission leverages intermediate representations from a wav2vec 2.0\nmodel trained on 234 hours of Tamasheq audio, while our contrastive model uses\na French phonetic transcription of the Tamasheq audio as input in a Conformer\nspeech translation architecture jointly trained on automatic speech\nrecognition, ST and machine translation losses. Our results highlight that\nself-supervised models trained on smaller sets of target data are more\neffective to low-resource end-to-end ST fine-tuning, compared to large\noff-the-shelf models. Results also illustrate that even approximate phonetic\ntranscriptions can improve ST scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riguidel_H/0/1/0/all/0/1\">Hugo Riguidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1\">Antoine Laurent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougares_F/0/1/0/all/0/1\">Fethi Bougares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaabani_F/0/1/0/all/0/1\">Firas Chaabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbier_F/0/1/0/all/0/1\">Florentin Barbier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gahbiche_S/0/1/0/all/0/1\">Souhir Gahbiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-Claims: A Dataset for Multimodal Claim Detection in Social Media. (arXiv:2205.01989v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01989","description":"<p>In recent years, the problem of misinformation on the web has become\nwidespread across languages, countries, and various social media platforms.\nAlthough there has been much work on automated fake news detection, the role of\nimages and their variety are not well explored. In this paper, we investigate\nthe roles of image and text at an earlier stage of the fake news detection\npipeline, called claim detection. For this purpose, we introduce a novel\ndataset, MM-Claims, which consists of tweets and corresponding images over\nthree topics: COVID-19, Climate Change and broadly Technology. The dataset\ncontains roughly 86000 tweets, out of which 3400 are labeled manually by\nmultiple annotators for the training and evaluation of multimodal models. We\ndescribe the dataset in detail, evaluate strong unimodal and multimodal\nbaselines, and analyze the potential and drawbacks of current models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1\">Gullal S. Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sittar_A/0/1/0/all/0/1\">Abdul Sittar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1\">Eric M&#xfc;ller-Budack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otto_C/0/1/0/all/0/1\">Christian Otto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoBank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis. (arXiv:2205.01996v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01996","description":"<p>We describe EmoBank, a corpus of 10k English sentences balancing multiple\ngenres, which we annotated with dimensional emotion metadata in the\nValence-Arousal-Dominance (VAD) representation format. EmoBank excels with a\nbi-perspectival and bi-representational design. On the one hand, we distinguish\nbetween writer's and reader's emotions, on the other hand, a subset of the\ncorpus complements dimensional VAD annotations with categorical ones based on\nBasic Emotions. We find evidence for the supremacy of the reader's perspective\nin terms of IAA and rating intensity, and achieve close-to-human performance\nwhen mapping between dimensional and categorical formats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buechel_S/0/1/0/all/0/1\">Sven Buechel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_U/0/1/0/all/0/1\">Udo Hahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design of a novel Korean learning application for efficient pronunciation correction. (arXiv:2205.02001v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02001","description":"<p>The Korean wave, which denotes the global popularity of South Korea's\ncultural economy, contributes to the increasing demand for the Korean language.\nHowever, as there does not exist any application for foreigners to learn\nKorean, this paper suggested a design of a novel Korean learning application.\nSpeech recognition, speech-to-text, and speech-to-waveform are the three key\nsystems in the proposed system. The Google API and the librosa library will\ntransform the user's voice into a sentence and MFCC. The software will then\ndisplay the user's phrase and answer, with mispronounced elements highlighted\nin red, allowing users to more easily recognize the incorrect parts of their\npronunciation. Furthermore, the Siamese network might utilize those translated\nspectrograms to provide a similarity score, which could subsequently be used to\noffer feedback to the user. Despite the fact that we were unable to collect\nsufficient foreigner data for this research, it is notable that we presented a\nnovel Korean pronunciation correction method for foreigners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheon_M/0/1/0/all/0/1\">Minjong Cheon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hanseon Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework to Generate High-Quality Datapoints for Multiple Novel Intent Detection. (arXiv:2205.02005v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02005","description":"<p>Systems like Voice-command based conversational agents are characterized by a\npre-defined set of skills or intents to perform user specified tasks. In the\ncourse of time, newer intents may emerge requiring retraining. However, the\nnewer intents may not be explicitly announced and need to be inferred\ndynamically. Thus, there are two important tasks at hand (a). identifying\nemerging new intents, (b). annotating data of the new intents so that the\nunderlying classifier can be retrained efficiently. The tasks become specially\nchallenging when a large number of new intents emerge simultaneously and there\nis a limited budget of manual annotation. In this paper, we propose MNID\n(Multiple Novel Intent Detection) which is a cluster based framework to detect\nmultiple novel intents with budgeted human annotation cost. Empirical results\non various benchmark datasets (of different sizes) demonstrate that MNID, by\nintelligently using the budget for annotation, outperforms the baseline methods\nin terms of accuracy and F1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1\">Ankan Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Sukannya Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Inflection for Scientific Discovery. (arXiv:2205.02007v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02007","description":"<p>We stand at the foot of a significant inflection in the trajectory of\nscientific discovery. As society continues on its fast-paced digital\ntransformation, so does humankind's collective scientific knowledge and\ndiscourse. We now read and write papers in digitized form, and a great deal of\nthe formal and informal processes of science are captured digitally --\nincluding papers, preprints and books, code and datasets, conference\npresentations, and interactions in social networks and communication platforms.\nThe transition has led to the growth of a tremendous amount of information,\nopening exciting opportunities for computational models and systems that\nanalyze and harness it. In parallel, exponential growth in data processing\npower has fueled remarkable advances in AI, including self-supervised neural\nmodels capable of learning powerful representations from large-scale\nunstructured text without costly human supervision. The confluence of societal\nand computational trends suggests that computer science is poised to ignite a\nrevolution in the scientific process itself.\n</p>\n<p>However, the explosion of scientific data, results and publications stands in\nstark contrast to the constancy of human cognitive capacity. While scientific\nknowledge is expanding with rapidity, our minds have remained static, with\nsevere limitations on the capacity for finding, assimilating and manipulating\ninformation. We propose a research agenda of task-guided knowledge retrieval,\nin which systems counter humans' bounded capacity by ingesting corpora of\nscientific knowledge and retrieving inspirations, explanations, solutions and\nevidence synthesized to directly augment human performance on salient tasks in\nscientific endeavors. We present initial progress on methods and prototypes,\nand lay out important opportunities and challenges ahead with computational\napproaches that have the potential to revolutionize science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etzioni_O/0/1/0/all/0/1\">Oren Etzioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Continual Model Refinement in Out-of-Distribution Data Streams. (arXiv:2205.02014v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02014","description":"<p>Real-world natural language processing (NLP) models need to be continually\nupdated to fix the prediction errors in out-of-distribution (OOD) data streams\nwhile overcoming catastrophic forgetting. However, existing continual learning\n(CL) problem setups cannot cover such a realistic and complex scenario. In\nresponse to this, we propose a new CL problem formulation dubbed continual\nmodel refinement (CMR). Compared to prior CL settings, CMR is more practical\nand introduces unique challenges (boundary-agnostic and non-stationary\ndistribution shift, diverse mixtures of multiple OOD data clusters,\nerror-centric streams, etc.). We extend several existing CL approaches to the\nCMR setting and evaluate them extensively. For benchmarking and analysis, we\npropose a general sampling algorithm to obtain dynamic OOD data streams with\ncontrollable non-stationarity, as well as a suite of metrics measuring various\naspects of online performance. Our experiments and detailed analysis reveal the\npromise and challenges of the CMR problem, supporting that studying CMR in\ndynamic OOD streams can benefit the longevity of deployed NLP models in\nproduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation. (arXiv:2205.02022v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02022","description":"<p>Recent advances in the pre-training of language models leverage large-scale\ndatasets to create multilingual models. However, low-resource languages are\nmostly left out in these datasets. This is primarily because many widely spoken\nlanguages are not well represented on the web and therefore excluded from the\nlarge-scale crawls used to create datasets. Furthermore, downstream users of\nthese models are restricted to the selection of languages originally chosen for\npre-training. This work investigates how to optimally leverage existing\npre-trained models to create low-resource translation systems for 16 African\nlanguages. We focus on two questions: 1) How can pre-trained models be used for\nlanguages not included in the initial pre-training? and 2) How can the\nresulting translation models effectively transfer to new domains? To answer\nthese questions, we create a new African news corpus covering 16 languages, of\nwhich eight languages are not part of any existing evaluation dataset. We\ndemonstrate that the most effective strategy for transferring both to\nadditional languages and to additional domains is to fine-tune large\npre-trained models on small quantities of high-quality translation data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Oluwadara Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabende_P/0/1/0/all/0/1\">Peter Nabende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ernie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sackey_F/0/1/0/all/0/1\">Freshia Sackey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beukman_M/0/1/0/all/0/1\">Michael Beukman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarso_G/0/1/0/all/0/1\">Guyo Dub Jarso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousuf_O/0/1/0/all/0/1\">Oreen Yousuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubungo_A/0/1/0/all/0/1\">Andre Niyongabo Rubungo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1\">Gilles Hacheme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wairagala_E/0/1/0/all/0/1\">Eric Peter Wairagala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1\">Muhammad Umair Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajibade_B/0/1/0/all/0/1\">Benjamin Ayoade Ajibade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajayi_T/0/1/0/all/0/1\">Tunde Oluwaseyi Ajayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gitau_Y/0/1/0/all/0/1\">Yvonne Wambui Gitau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_J/0/1/0/all/0/1\">Jade Abbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Mohamed Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_M/0/1/0/all/0/1\">Millicent Ochieng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogayo_P/0/1/0/all/0/1\">Perez Ogayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukiibi_J/0/1/0/all/0/1\">Jonathan Mukiibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabore_F/0/1/0/all/0/1\">Fatoumata Ouoba Kabore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalipe_G/0/1/0/all/0/1\">Godson Koffi Kalipe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mbaye_D/0/1/0/all/0/1\">Derguene Mbaye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapo_A/0/1/0/all/0/1\">Allahsera Auguste Tapo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koagne_V/0/1/0/all/0/1\">Victoire Memdjokam Koagne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munkoh_Buabeng_E/0/1/0/all/0/1\">Edwin Munkoh-Buabeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_V/0/1/0/all/0/1\">Valencia Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1\">Ayodele Awokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzaaba_H/0/1/0/all/0/1\">Happy Buzaaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sibanda_B/0/1/0/all/0/1\">Blessing Sibanda</a>, et al. (2 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models. (arXiv:2205.02023v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02023","description":"<p>The success of multilingual pre-trained models is underpinned by their\nability to learn representations shared by multiple languages even in absence\nof any explicit supervision. However, it remains unclear how these models learn\nto generalise across languages. In this work, we conjecture that multilingual\npre-trained models can derive language-universal abstractions about grammar. In\nparticular, we investigate whether morphosyntactic information is encoded in\nthe same subset of neurons in different languages. We conduct the first\nlarge-scale empirical study over 43 languages and 14 morphosyntactic categories\nwith a state-of-the-art neuron-level probe. Our findings show that the\ncross-lingual overlap between neurons is significant, but its extent may vary\nacross categories and depends on language proximity and pre-training data size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training. (arXiv:2205.02029v1 [cs.PL])","link":"http://arxiv.org/abs/2205.02029","description":"<p>Recent years have witnessed increasing interest in code representation\nlearning, which aims to represent the semantics of source code into distributed\nvectors. Currently, various works have been proposed to represent the complex\nsemantics of source code from different views, including plain text, Abstract\nSyntax Tree (AST), and several kinds of code graphs (e.g., Control/Data Flow\nGraph). However, most of them only consider a single view of source code\nindependently, ignoring the correspondences among different views. In this\npaper, we propose to integrate different views with the natural-language\ndescription of source code into a unified framework with Multi-View contrastive\nPre-training, and name our model as CODE-MVP. Specifically, we first extract\nmultiple code views using compiler tools, and learn the complementary\ninformation among them under a contrastive learning framework. Inspired by the\ntype checking in compilation, we also design a fine-grained type inference\nobjective in the pre-training. Experiments on three downstream tasks over five\ndatasets demonstrate the superiority of CODE-MVP when compared with several\nstate-of-the-art baselines. For example, we achieve 2.4/2.3/1.1 gain in terms\nof MRR/MAP/Accuracy metrics on natural language code retrieval, code\nsimilarity, and code defect detection tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Summarization to Generate Factually Inconsistent Summaries for Improved Factual Consistency Checking. (arXiv:2205.02035v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02035","description":"<p>Despite the recent advances in abstractive summarization systems, it is still\ndifficult to determine whether a generated summary is factual consistent with\nthe source text. To this end, the latest approach is to train a factual\nconsistency classifier on factually consistent and inconsistent summaries.\nLuckily, the former is readily available as reference summaries in existing\nsummarization datasets. However, generating the latter remains a challenge, as\nthey need to be factually inconsistent, yet closely relevant to the source text\nto be effective. In this paper, we propose to generate factually inconsistent\nsummaries using source texts and reference summaries with key information\nmasked. Experiments on seven benchmark datasets demonstrate that factual\nconsistency classifiers trained on summaries generated using our method\ngenerally outperform existing models and show a competitive correlation with\nhuman judgments. We also analyze the characteristics of the summaries generated\nusing our method. We will release the pre-trained model and the code at\nhttps://github.com/hwanheelee1993/MFMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joonsuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperbolic Relevance Matching for Neural Keyphrase Extraction. (arXiv:2205.02047v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02047","description":"<p>Keyphrase extraction is a fundamental task in natural language processing and\ninformation retrieval that aims to extract a set of phrases with important\ninformation from a source document. Identifying important keyphrase is the\ncentral component of the keyphrase extraction task, and its main challenge is\nhow to represent information comprehensively and discriminate importance\naccurately. In this paper, to address these issues, we design a new hyperbolic\nmatching model (HyperMatch) to represent phrases and documents in the same\nhyperbolic space and explicitly estimate the phrase-document relevance via the\nPoincar\\'e distance as the important score of each phrase. Specifically, to\ncapture the hierarchical syntactic and semantic structure information,\nHyperMatch takes advantage of the hidden representations in multiple layers of\nRoBERTa and integrates them as the word embeddings via an adaptive mixing\nlayer. Meanwhile, considering the hierarchical structure hidden in the\ndocument, HyperMatch embeds both phrases and documents in the same hyperbolic\nspace via a hyperbolic phrase encoder and a hyperbolic document encoder. This\nstrategy can further enhance the estimation of phrase-document relevance due to\nthe good properties of hyperbolic space. In this setting, the keyphrase\nextraction can be taken as a matching problem and effectively implemented by\nminimizing a hyperbolic margin-based triplet loss. Extensive experiments are\nconducted on six benchmarks and demonstrate that HyperMatch outperforms the\nstate-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Document-Level Relation Extraction. (arXiv:2205.02048v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02048","description":"<p>We present FREDo, a few-shot document-level relation extraction (FSDLRE)\nbenchmark. As opposed to existing benchmarks which are built on sentence-level\nrelation extraction corpora, we argue that document-level corpora provide more\nrealism, particularly regarding none-of-the-above (NOTA) distributions.\nTherefore, we propose a set of FSDLRE tasks and construct a benchmark based on\ntwo existing supervised learning data sets, DocRED and sciERC. We adapt the\nstate-of-the-art sentence-level method MNAV to the document-level and develop\nit further for improved domain adaptation. We find FSDLRE to be a challenging\nsetting with interesting new characteristics such as the ability to sample NOTA\ninstances from the support set. The data, code, and trained models are\navailable online (https://github.com/nicpopovic/FREDo).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nicholas Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_M/0/1/0/all/0/1\">Michael F&#xe4;rber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment. (arXiv:2205.02054v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02054","description":"<p>In text-to-SQL tasks -- as in much of NLP -- compositional generalization is\na major challenge: neural networks struggle with compositional generalization\nwhere training and test distributions differ. However, most recent attempts to\nimprove this are based on word-level synthetic data or specific dataset splits\nto generate compositional biases. In this work, we propose a clause-level\ncompositional example generation method. We first split the sentences in the\nSpider text-to-SQL dataset into sub-sentences, annotating each sub-sentence\nwith its corresponding SQL clause, resulting in a new dataset Spider-SS. We\nthen construct a further dataset, Spider-CG, by composing Spider-SS\nsub-sentences in different combinations, to test the ability of models to\ngeneralize compositionally. Experiments show that existing models suffer\nsignificant performance degradation when evaluated on Spider-CG, even though\nevery sub-sentence is seen during training. To deal with this problem, we\nmodify a number of state-of-the-art models to train on the segmented data of\nSpider-SS, and we show that this method improves the generalization\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yujian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Task-Oriented Parsing as Abstractive Question Answering. (arXiv:2205.02068v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02068","description":"<p>Task-oriented parsing (TOP) aims to convert natural language into\nmachine-readable representations of specific tasks, such as setting an alarm. A\npopular approach to TOP is to apply seq2seq models to generate linearized parse\ntrees. A more recent line of work argues that pretrained seq2seq models are\nbetter at generating outputs that are themselves natural language, so they\nreplace linearized parse trees with canonical natural-language paraphrases that\ncan then be easily translated into parse trees, resulting in so-called\nnaturalized parsers. In this work we continue to explore naturalized semantic\nparsing by presenting a general reduction of TOP to abstractive question\nanswering that overcomes some limitations of canonical paraphrasing.\nExperimental results show that our QA-based technique outperforms\nstate-of-the-art methods in full-data settings while achieving dramatic\nimprovements in few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Discourse Dependency Parsing with Contextualized Representations. (arXiv:2205.02090v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02090","description":"<p>Recent works show that discourse analysis benefits from modeling intra- and\ninter-sentential levels separately, where proper representations for text units\nof different granularities are desired to capture both the meaning of text\nunits and their relations to the context. In this paper, we propose to take\nadvantage of transformers to encode contextualized representations of units of\ndifferent levels to dynamically capture the information required for discourse\ndependency analysis on intra- and inter-sentential levels. Motivated by the\nobservation of writing patterns commonly shared across articles, we propose a\nnovel method that treats discourse relation identification as a sequence\nlabelling task, which takes advantage of structural information from the\ncontext of extracted discourse trees, and substantially outperforms traditional\ndirect-classification methods. Experiments show that our model achieves\nstate-of-the-art results on both English and Chinese datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yifei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are All the Datasets in Benchmark Necessary? A Pilot Study of Dataset Evaluation for Text Classification. (arXiv:2205.02129v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02129","description":"<p>In this paper, we ask the research question of whether all the datasets in\nthe benchmark are necessary. We approach this by first characterizing the\ndistinguishability of datasets when comparing different systems. Experiments on\n9 datasets and 36 systems show that several existing benchmark datasets\ncontribute little to discriminating top-scoring systems, while those less used\ndatasets exhibit impressive discriminative power. We further, taking the text\nclassification task as a case study, investigate the possibility of predicting\ndataset discrimination based on its properties (e.g., average sentence length).\nOur preliminary experiments promisingly show that given a sufficient number of\ntraining experimental records, a meaningful predictor can be learned to\nestimate dataset discrimination over unseen datasets. We released all datasets\nwith features explored in this work on DataLab:\n\\url{https://datalab.nlpedia.ai}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Limits of Word Level Differential Privacy. (arXiv:2205.02130v1 [cs.CR])","link":"http://arxiv.org/abs/2205.02130","description":"<p>As the issues of privacy and trust are receiving increasing attention within\nthe research community, various attempts have been made to anonymize textual\ndata. A significant subset of these approaches incorporate differentially\nprivate mechanisms to perturb word embeddings, thus replacing individual words\nin a sentence. While these methods represent very important contributions, have\nvarious advantages over other techniques and do show anonymization\ncapabilities, they have several shortcomings. In this paper, we investigate\nthese weaknesses and demonstrate significant mathematical constraints\ndiminishing the theoretical privacy guarantee as well as major practical\nshortcomings with regard to the protection against deanonymization attacks, the\npreservation of content of the original sentences as well as the quality of the\nlanguage output. Finally, we propose a new method for text anonymization based\non transformer based language models fine-tuned for paraphrasing that\ncircumvents most of the identified weaknesses and also offers a formal privacy\nguarantee. We evaluate the performance of our method via thorough\nexperimentation and demonstrate superior performance over the discussed\nmechanisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mattern_J/0/1/0/all/0/1\">Justus Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weggenmann_B/0/1/0/all/0/1\">Benjamin Weggenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1\">Florian Kerschbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion-Cause Pair Extraction. (arXiv:2205.02132v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02132","description":"<p>The Emotion-Cause Pair Extraction (ECPE) task aims to extract emotions and\ncauses as pairs from documents. We observe that the relative distance\ndistribution of emotions and causes is extremely imbalanced in the typical ECPE\ndataset. Existing methods have set a fixed size window to capture relations\nbetween neighboring clauses. However, they neglect the effective semantic\nconnections between distant clauses, leading to poor generalization ability\ntowards position-insensitive data. To alleviate the problem, we propose a novel\n\\textbf{M}ulti-\\textbf{G}ranularity \\textbf{S}emantic \\textbf{A}ware\n\\textbf{G}raph model (MGSAG) to incorporate fine-grained and coarse-grained\nsemantic features jointly, without regard to distance limitation. In\nparticular, we first explore semantic dependencies between clauses and keywords\nextracted from the document that convey fine-grained semantic features,\nobtaining keywords enhanced clause representations. Besides, a clause graph is\nalso established to model coarse-grained semantic relations between clauses.\nExperimental results indicate that MGSAG surpasses the existing\nstate-of-the-art ECPE models. Especially, MGSAG outperforms other models\nsignificantly in the condition of position-insensitive data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yinan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianwen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Few-Shot Fine-Tuning for Opinion Summarization. (arXiv:2205.02170v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02170","description":"<p>Abstractive summarization models are typically pre-trained on large amounts\nof generic texts, then fine-tuned on tens or hundreds of thousands of annotated\nsamples. However, in opinion summarization, large annotated datasets of reviews\npaired with reference summaries are not available and would be expensive to\ncreate. This calls for fine-tuning methods robust to overfitting on small\ndatasets. In addition, generically pre-trained models are often not accustomed\nto the specifics of customer reviews and, after fine-tuning, yield summaries\nwith disfluencies and semantic mistakes. To address these problems, we utilize\nan efficient few-shot method based on adapters which, as we show, can easily\nstore in-domain knowledge. Instead of fine-tuning the entire model, we add\nadapters and pre-train them in a task-specific way on a large corpus of\nunannotated customer reviews, using held-out reviews as pseudo summaries. Then,\nfine-tune the adapters on the small available human-annotated dataset. We show\nthat this self-supervised adapter pre-training improves summary quality over\nstandard fine-tuning by 2.0 and 1.3 ROUGE-L points on the Amazon and Yelp\ndatasets, respectively. Finally, for summary personalization, we condition on\naspect keyword queries, automatically created from generic datasets. In the\nsame vein, we pre-train the adapters in a query-based manner on customer\nreviews and then fine-tune them on annotated datasets. This results in\nbetter-organized summary content reflected in improved coherence and fewer\nredundancies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1\">Arthur Bra&#x17e;inskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using virtual edges to extract keywords from texts modeled as complex networks. (arXiv:2205.02172v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02172","description":"<p>Detecting keywords in texts is important for many text mining applications.\nGraph-based methods have been commonly used to automatically find the key\nconcepts in texts, however, relevant information provided by embeddings has not\nbeen widely used to enrich the graph structure. Here we modeled texts\nco-occurrence networks, where nodes are words and edges are established either\nby contextual or semantical similarity. We compared two embedding approaches --\nWord2vec and BERT -- to check whether edges created via word embeddings can\nimprove the quality of the keyword extraction method. We found that, in fact,\nthe use of virtual edges can improve the discriminability of co-occurrence\nnetworks. The best performance was obtained when we considered low percentages\nof addition of virtual (embedding) edges. A comparative analysis of structural\nand dynamical network metrics revealed the degree, PageRank, and accessibility\nare the metrics displaying the best performance in the model enriched with\nvirtual edges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tohalino_J/0/1/0/all/0/1\">Jorge A. V. Tohalino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1\">Thiago C. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1\">Diego R. Amancio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducibility Beyond the Research Community: Experience from NLP Beginners. (arXiv:2205.02182v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02182","description":"<p>As NLP research attracts public attention and excitement, it becomes\nincreasingly important for it to be accessible to a broad audience. As the\nresearch community works to democratize NLP, it remains unclear whether\nbeginners to the field can easily apply the latest developments. To understand\ntheir needs, we conducted a study with 93 students in an introductory NLP\ncourse, where students reproduced results of recent NLP papers. Surprisingly,\nour results suggest that their technical skill (i.e., programming experience)\nhas limited impact on their effort spent completing the exercise. Instead, we\nfind accessibility efforts by research authors to be key to a successful\nexperience, including thorough documentation and easy access to required models\nand datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Keunwoo Peter Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User-Centric Gender Rewriting. (arXiv:2205.02211v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02211","description":"<p>In this paper, we define the task of gender rewriting in contexts involving\ntwo users (I and/or You) - first and second grammatical persons with\nindependent grammatical gender preferences. We focus on Arabic, a\ngender-marking morphologically rich language. We develop a multi-step system\nthat combines the positive aspects of both rule-based and neural rewriting\nmodels. Our results successfully demonstrate the viability of this approach on\na recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5\non a blind test set. Our proposed system improves over previous work on the\nfirst-person-only version of this task, by 3.05 absolute increase in M2 F0.5.\nWe demonstrate a use case of our gender rewriting system by using it to\npost-edit the output of a commercial MT system to provide personalized outputs\nbased on the users' grammatical gender preferences. We make our code, data, and\nmodels publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised learning approaches for predicting South African political sentiment for local government elections. (arXiv:2205.02223v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02223","description":"<p>This study aims to understand the South African political context by\nanalysing the sentiments shared on Twitter during the local government\nelections. An emphasis on the analysis was placed on understanding the\ndiscussions led around four predominant political parties ANC, DA, EFF and\nActionSA. A semi-supervised approach by means of a graph-based technique to\nlabel the vast accessible Twitter data for the classification of tweets into\nnegative and positive sentiment was used. The tweets expressing negative\nsentiment were further analysed through latent topic extraction to uncover\nhidden topics of concern associated with each of the political parties. Our\nfindings demonstrated that the general sentiment across South African Twitter\nusers is negative towards all four predominant parties with the worst negative\nsentiment among users projected towards the current ruling party, ANC, relating\nto concerns cantered around corruption, incompetence and loadshedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ledwaba_M/0/1/0/all/0/1\">Mashadi Ledwaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction. (arXiv:2205.02225v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02225","description":"<p>Unsupervised relation extraction aims to extract the relationship between\nentities from natural language sentences without prior information on\nrelational scope or distribution. Existing works either utilize self-supervised\nschemes to refine relational feature signals by iteratively leveraging adaptive\nclustering and classification that provoke gradual drift problems, or adopt\ninstance-wise contrastive learning which unreasonably pushes apart those\nsentence pairs that are semantically similar. To overcome these defects, we\npropose a novel contrastive learning framework named HiURE, which has the\ncapability to derive hierarchical signals from relational feature space using\ncross hierarchy attention and effectively optimize relation representation of\nsentences under exemplar-wise contrastive learning. Experimental results on two\npublic datasets demonstrate the advanced effectiveness and robustness of HiURE\non unsupervised relation extraction when compared with state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu`ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adjusting for Confounders with Text: Challenges and an Empirical Evaluation Framework for Causal Inference. (arXiv:2009.09961v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.09961","description":"<p>Leveraging text, such as social media posts, for causal inferences requires\nthe use of NLP models to 'learn' and adjust for confounders, which could\notherwise impart bias. However, evaluating such models is challenging, as\nground truth is almost never available. We demonstrate the need for empirical\nevaluation frameworks for causal inference in natural language by showing that\nexisting, commonly used models regularly disagree with one another on real\nworld tasks. We contribute the first such framework, generalizing several\nchallenges across these real world tasks. Using this framework, we evaluate a\nlarge set of commonly used causal inference models based on propensity scores\nand identify their strengths and weaknesses to inform future improvements. We\nmake all tasks, data, and models public to inform applications and encourage\nadditional research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weld_G/0/1/0/all/0/1\">Galen Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glenski_M/0/1/0/all/0/1\">Maria Glenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbour_D/0/1/0/all/0/1\">David Arbour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering. (arXiv:2012.14610v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14610","description":"<p>We study open-domain question answering with structured, unstructured and\nsemi-structured knowledge sources, including text, tables, lists and knowledge\nbases. Departing from prior work, we propose a unifying approach that\nhomogenizes all sources by reducing them to text and applies the\nretriever-reader model which has so far been limited to text sources only. Our\napproach greatly improves the results on knowledge-base QA tasks by 11 points,\ncompared to latest graph-based methods. More importantly, we demonstrate that\nour unified knowledge (UniK-QA) model is a simple and yet effective way to\ncombine heterogeneous sources of knowledge, advancing the state-of-the-art\nresults on two popular question answering benchmarks, NaturalQuestions and\nWebQuestions, by 3.5 and 2.6 points, respectively.\n</p>\n<p>The code of UniK-QA is available at:\nhttps://github.com/facebookresearch/UniK-QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1\">Scott Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnosing Vision-and-Language Navigation: What Really Matters. (arXiv:2103.16561v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16561","description":"<p>Vision-and-language navigation (VLN) is a multimodal task where an agent\nfollows natural language instructions and navigates in visual environments.\nMultiple setups have been proposed, and researchers apply new model\narchitectures or training techniques to boost navigation performance. However,\nthere still exist non-negligible gaps between machines' performance and human\nbenchmarks. Moreover, the agents' inner mechanisms for navigation decisions\nremain unclear. To the best of our knowledge, how the agents perceive the\nmultimodal input is under-studied and needs investigation. In this work, we\nconduct a series of diagnostic experiments to unveil agents' focus during\nnavigation. Results show that indoor navigation agents refer to both object and\ndirection tokens when making decisions. In contrast, outdoor navigation agents\nheavily rely on direction tokens and poorly understand the object tokens.\nTransformer-based agents acquire a better cross-modal understanding of objects\nand display strong numerical reasoning ability than non-Transformer-based\nagents. When it comes to vision-and-language alignments, many models claim that\nthey can align object tokens with specific visual targets. We find unbalanced\nattention on the vision and text input and doubt the reliability of such\ncross-modal alignments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayana_P/0/1/0/all/0/1\">Pradyumna Narayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sone_K/0/1/0/all/0/1\">Kazoo Sone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sugato Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thematic Fit Bits: Annotation Quality and Quantity Interplay for Event Participant Representation. (arXiv:2105.06097v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06097","description":"<p>Modeling thematic fit (a verb--argument compositional semantics task)\ncurrently requires a very large burden of labeled data. We take a\nlinguistically machine-annotated large corpus and replace corpus layers with\noutput from higher-quality, more modern taggers. We compare the old and new\ncorpus versions' impact on a verb--argument fit modeling task, using a\nhigh-performing neural approach. We discover that higher annotation quality\ndramatically reduces our data requirement while demonstrating better supervised\npredicate-argument classification. But in applying the model to\npsycholinguistic tasks outside the training objective, we see clear gains at\nscale, but only in one of two thematic fit estimation tasks, and no clear gains\non the other. We also see that quality improves with training size, but perhaps\nplateauing or even declining in one task. Last, we tested the effect of role\nset size. All this suggests that the quality/quantity interplay is not all you\nneed. We replicate previous studies while modifying certain role representation\ndetails and set a new state-of-the-art in event modeling, using a fraction of\nthe data. We make the new corpus version public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marton_Y/0/1/0/all/0/1\">Yuval Marton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayeed_A/0/1/0/all/0/1\">Asad Sayeed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08043","description":"<p>Causal inference is the process of estimating the effect or impact of a\ntreatment on an outcome with other covariates as potential confounders (and\nmediators) that may need to be controlled. The vast majority of existing\nmethods and systems for causal inference assume that all variables under\nconsideration are categorical or numerical (e.g., gender, price, enrollment).\nIn this paper, we present CausalNLP, a toolkit for inferring causality with\nobservational data that includes text in addition to traditional numerical and\ncategorical variables. CausalNLP employs the use of meta learners for treatment\neffect estimation and supports using raw text and its linguistic properties as\na treatment, an outcome, or a \"controlled-for\" variable (e.g., confounder). The\nlibrary is open source and available at: https://github.com/amaiya/causalnlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Language to Learn Program Abstractions and Search Heuristics. (arXiv:2106.11053v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11053","description":"<p>Inductive program synthesis, or inferring programs from examples of desired\nbehavior, offers a general paradigm for building interpretable, robust, and\ngeneralizable machine learning systems. Effective program synthesis depends on\ntwo key ingredients: a strong library of functions from which to build\nprograms, and an efficient search strategy for finding programs that solve a\ngiven task. We introduce LAPS (Language for Abstraction and Program Search), a\ntechnique for using natural language annotations to guide joint learning of\nlibraries and neurally-guided search models for synthesis. When integrated into\na state-of-the-art library learning system (DreamCoder), LAPS produces\nhigher-quality libraries and improves search efficiency and generalization on\nthree domains -- string editing, image composition, and abstract reasoning\nabout scenes -- even when no natural language hints are available at test time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Catherine Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1\">Kevin Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Political Ideology and Polarization of Policy Positions: A Multi-dimensional Approach. (arXiv:2106.14387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14387","description":"<p>Analyzing ideology and polarization is of critical importance in advancing\nour grasp of modern politics. Recent research has made great strides towards\nunderstanding the ideological bias (i.e., stance) of news media along the\nleft-right spectrum. In this work, we instead take a novel and more nuanced\napproach for the study of ideology based on its left or right positions on the\nissue being discussed. Aligned with the theoretical accounts in political\nscience, we treat ideology as a multi-dimensional construct, and introduce the\nfirst diachronic dataset of news articles whose ideological positions are\nannotated by trained political scientists and linguists at the paragraph level.\nWe showcase that, by controlling for the author's stance, our method allows for\nthe quantitative and temporal measurement and analysis of polarization as a\nmultidimensional ideological distance. We further present baseline models for\nideology prediction, outlining a challenging task distinct from stance\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinno_B/0/1/0/all/0/1\">Barea Sinno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oviedo_B/0/1/0/all/0/1\">Bernardo Oviedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atwell_K/0/1/0/all/0/1\">Katherine Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid deep learning methods for phenotype prediction from clinical notes. (arXiv:2108.10682v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10682","description":"<p>Identifying patient cohorts from clinical notes in secondary electronic\nhealth records is a fundamental task in clinical information management.\nHowever, with the growing number of clinical notes, it becomes challenging to\nanalyze the data manually for phenotype detection. Automatic extraction of\nclinical concepts would helps to identify the patient phenotypes correctly.\nThis paper proposes a novel hybrid model for automatically extracting patient\nphenotypes using natural language processing and deep learning models to\ndetermine the patient phenotypes without dictionaries and human intervention.\nThe model is based on a neural bidirectional sequence model (BiLSTM or BiGRU)\nand a CNN layer for phenotypes identification. An extra CNN layer is run\nparallel to the hybrid model to extract more features related to each\nphenotype. We used pre-trained embeddings such as FastText and Word2vec\nseparately as the input layers to evaluate other embedding's performance.\nExperimental results using MIMIC III database in internal comparison\ndemonstrate that the proposed model achieved significant performance\nimprovement over existing models. The enhanced version of our model with an\nextra CNN layer obtained a relatively higher F1-score than the original hybrid\nmodel. We also showed that BiGRU layer with FastText embedding had better\nperformance than BiLSTM layer to identify patient phenotypes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalafi_S/0/1/0/all/0/1\">Sahar Khalafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_N/0/1/0/all/0/1\">Nasser Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEGREE: A Data-Efficient Generation-Based Event Extraction Model. (arXiv:2108.12724v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12724","description":"<p>Event extraction requires high-quality expert human annotations, which are\nusually expensive. Therefore, learning a data-efficient event extraction model\nthat can be trained with only a few labeled examples has become a crucial\nchallenge. In this paper, we focus on low-resource end-to-end event extraction\nand propose DEGREE, a data-efficient model that formulates event extraction as\na conditional generation problem. Given a passage and a manually designed\nprompt, DEGREE learns to summarize the events mentioned in the passage into a\nnatural sentence that follows a predefined pattern. The final event predictions\nare then extracted from the generated sentence with a deterministic algorithm.\nDEGREE has three advantages to learn well with less training data. First, our\ndesigned prompts provide semantic guidance for DEGREE to leverage DEGREE and\nthus better capture the event arguments. Moreover, DEGREE is capable of using\nadditional weakly-supervised information, such as the description of events\nencoded in the prompts. Finally, DEGREE learns triggers and arguments jointly\nin an end-to-end manner, which encourages the model to better utilize the\nshared knowledge and dependencies among them. Our experimental results\ndemonstrate the strong performance of DEGREE for low-resource event extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Length Divergence Bias in Textual Matching Models. (arXiv:2109.02431v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02431","description":"<p>Despite the remarkable success deep models have achieved in Textual Matching\n(TM) tasks, it still remains unclear whether they truly understand language or\nmeasure the semantic similarity of texts by exploiting statistical bias in\ndatasets. In this work, we provide a new perspective to study this issue -- via\nthe length divergence bias. We find the length divergence heuristic widely\nexists in prevalent TM datasets, providing direct cues for prediction. To\ndetermine whether TM models have adopted such heuristic, we introduce an\nadversarial evaluation scheme which invalidates the heuristic. In this\nadversarial setting, all TM models perform worse, indicating they have indeed\nadopted this heuristic. Through a well-designed probing experiment, we\nempirically validate that the bias of TM models can be attributed in part to\nextracting the text length information during training. To alleviate the length\ndivergence bias, we propose an adversarial training method. The results\ndemonstrate we successfully improve the robustness and generalization ability\nof models at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_T/0/1/0/all/0/1\">Tianshu Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Meng Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xiaoyong Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOVER: Mask, Over-generate and Rank for Hyperbole Generation. (arXiv:2109.07726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07726","description":"<p>Despite being a common figure of speech, hyperbole is under-researched in\nFigurative Language Processing. In this paper, we tackle the challenging task\nof hyperbole generation to transfer a literal sentence into its hyperbolic\nparaphrase. To address the lack of available hyperbolic sentences, we construct\nHYPO-XL, the first large-scale English hyperbole corpus containing 17,862\nhyperbolic sentences in a non-trivial way. Based on our corpus, we propose an\nunsupervised method for hyperbole generation that does not require parallel\nliteral-hyperbole pairs. During training, we fine-tune BART to infill masked\nhyperbolic spans of sentences from HYPO-XL. During inference, we mask part of\nan input literal sentence and over-generate multiple possible hyperbolic\nversions. Then a BERT-based ranker selects the best candidate by hyperbolicity\nand paraphrase quality. Automatic and human evaluation results show that our\nmodel is effective at generating hyperbolic paraphrase sentences and\noutperforms several baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding. (arXiv:2109.13582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13582","description":"<p>Large language models (LM) based on Transformers allow to generate plausible\nlong texts. In this paper, we explore how this generation can be further\ncontrolled at decoding time to satisfy certain constraints (e.g. being\nnon-toxic, conveying certain emotions, using a specific writing style, etc.)\nwithout fine-tuning the LM. Precisely, we formalize constrained generation as a\ntree exploration process guided by a discriminator that indicates how well the\nassociated sequence respects the constraint. This approach, in addition to\nbeing easier and cheaper to train than fine-tuning the LM, allows to apply the\nconstraint more finely and dynamically. We propose several original methods to\nsearch this generation tree, notably the Monte Carlo Tree Search (MCTS) which\nprovides theoretical guarantees on the search efficiency, but also simpler\nmethods based on re-ranking a pool of diverse sequences using the discriminator\nscores. These methods are evaluated, with automatic and human-based metrics, on\ntwo types of constraints and languages: review polarity and emotion control in\nFrench and English. We show that discriminator-guided MCTS decoding achieves\nstate-of-the-art results without having to tune the language model, in both\ntasks and languages. We also demonstrate that other proposed decoding methods\nbased on re-ranking can be really effective when diversity among the generated\npropositions is encouraged.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1\">Antoine Chaffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claveau_V/0/1/0/all/0/1\">Vincent Claveau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1\">Ewa Kijak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis. (arXiv:2110.00135v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.00135","description":"<p>Global models are trained to be as generalizable as possible, with user\ninvariance considered desirable since the models are shared across multitudes\nof users. As such, these models are often unable to produce personalized\nresponses for individual users, based on their data. Contrary to widely-used\npersonalization techniques based on few-shot learning, we propose\nUserIdentifier, a novel scheme for training a single shared model for all\nusers. Our approach produces personalized responses by adding fixed,\nnon-trainable user identifiers to the input data. We empirically demonstrate\nthat this proposed method outperforms the prefix-tuning based state-of-the-art\napproach by up to 13%, on a suite of sentiment analysis datasets. We also show\nthat, unlike prior work, this method needs neither any additional model\nparameters nor any extra rounds of few-shot fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1\">Vaishnavi Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1\">Robert Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1\">Dimitrios Dimitriadis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of Reading Comprehension Models to Entity Renaming. (arXiv:2110.08555v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08555","description":"<p>We study the robustness of machine reading comprehension (MRC) models to\nentity renaming -- do models make more wrong predictions when the same\nquestions are asked about an entity whose name has been changed? Such failures\nimply that models overly rely on entity information to answer questions, and\nthus may generalize poorly when facts about the world change or questions are\nasked about novel entities. To systematically audit this issue, we present a\npipeline to automatically generate test examples at scale, by replacing entity\nnames in the original test sample with names from a variety of sources, ranging\nfrom names in the same test set, to common names in life, to arbitrary strings.\nAcross five datasets and three pretrained model architectures, MRC models\nconsistently perform worse when entities are renamed, with particularly large\naccuracy drops on datasets constructed via distant supervision. We also find\nlarge differences between models: SpanBERT, which is pretrained with span-level\nmasking, is more robust than RoBERTa, despite having similar accuracy on\nunperturbed test data. We further experiment with different masking strategies\nas the continual pretraining objective and find that entity-based masking can\nimprove the robustness of MRC models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sagnik Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNN-LM: Language Modeling based on Global Contexts via GNN. (arXiv:2110.08743v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08743","description":"<p>Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in\nthis work, we introduce GNN-LM, which extends the vanilla neural language model\n(LM) by allowing to reference similar contexts in the entire training corpus.\nWe build a directed heterogeneous graph between an input context and its\nsemantically related neighbors selected from the training corpus, where nodes\nare tokens in the input context and retrieved neighbor contexts, and edges\nrepresent connections between nodes. Graph neural networks (GNNs) are\nconstructed upon the graph to aggregate information from similar contexts to\ndecode the token. This learning paradigm provides direct access to the\nreference contexts and helps improve a model's generalization ability. We\nconduct comprehensive experiments to validate the effectiveness of the GNN-LM:\nGNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a\n3.9 point improvement over its counterpart of the vanilla LM model), and shows\nsubstantial improvement on One Billion Word and Enwiki8 datasets against strong\nbaselines. In-depth ablation studies are performed to understand the mechanics\nof GNN-LM. \\footnote{The code can be found at\nhttps://github.com/ShannonAI/GNN-LM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer. (arXiv:2110.14782v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14782","description":"<p>While recent work on multilingual language models has demonstrated their\ncapacity for cross-lingual zero-shot transfer on downstream tasks, there is a\nlack of consensus in the community as to what shared properties between\nlanguages enable such transfer. Analyses involving pairs of natural languages\nare often inconclusive and contradictory since languages simultaneously differ\nin many linguistic aspects. In this paper, we perform a large-scale empirical\nstudy to isolate the effects of various linguistic properties by measuring\nzero-shot transfer between four diverse natural languages and their\ncounterparts constructed by modifying aspects such as the script, word order,\nand syntax. Among other things, our experiments show that the absence of\nsub-word overlap significantly affects zero-shot transfer when languages differ\nin their word order, and there is a strong correlation between transfer\nperformance and word embedding alignment between languages (e.g., R=0.94 on the\ntask of NLI). Our results call for focus in multilingual models on explicitly\nimproving word embedding alignment between languages rather than relying on its\nimplicit emergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity. (arXiv:2111.08366v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08366","description":"<p>We present a new scientific document similarity model based on matching\nfine-grained aspects of texts. To train our model, we exploit a\nnaturally-occurring source of supervision: sentences in the full-text of papers\nthat cite multiple papers together (co-citations). Such co-citations not only\nreflect close paper relatedness, but also provide textual descriptions of how\nthe co-cited papers are related. This novel form of textual supervision is used\nfor learning to match aspects across papers. We develop multi-vector\nrepresentations where vectors correspond to sentence-level aspects of\ndocuments, and present two methods for aspect matching: (1) A fast method that\nonly matches single aspects, and (2) a method that makes sparse multiple\nmatches with an Optimal Transport mechanism that computes an Earth Mover's\nDistance between aspects. Our approach improves performance on document\nsimilarity tasks in four datasets. Further, our fast single-match method\nachieves competitive results, paving the way for applying fine-grained\nsimilarity to large scientific corpora. Code, data, and models available at:\nhttps://github.com/allenai/aspire\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoBERTuito: a pre-trained language model for social media text in Spanish. (arXiv:2111.09453v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.09453","description":"<p>Since BERT appeared, Transformer language models and transfer learning have\nbecome state-of-the-art for Natural Language Understanding tasks. Recently,\nsome works geared towards pre-training specially-crafted models for particular\ndomains, such as scientific papers, medical documents, user-generated texts,\namong others. These domain-specific models have been shown to improve\nperformance significantly in most tasks. However, for languages other than\nEnglish such models are not widely available.\n</p>\n<p>In this work, we present RoBERTuito, a pre-trained language model for\nuser-generated text in Spanish, trained on over 500 million tweets. Experiments\non a benchmark of tasks involving user-generated text showed that RoBERTuito\noutperformed other pre-trained language models in Spanish. In addition to this,\nour model achieves top results for some English-Spanish tasks of the Linguistic\nCode-Switching Evaluation benchmark (LinCE) and has also competitive\nperformance against monolingual models in English tasks. To facilitate further\nresearch, we make RoBERTuito publicly available at the HuggingFace model hub\ntogether with the dataset used to pre-train it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan Manuel P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furman_D/0/1/0/all/0/1\">Dami&#xe1;n A. Furman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemany_L/0/1/0/all/0/1\">Laura Alonso Alemany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_F/0/1/0/all/0/1\">Franco Luque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTMap: A BERT-based Ontology Alignment System. (arXiv:2112.02682v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2112.02682","description":"<p>Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in\nknowledge integration. Owing to the success of machine learning in many\ndomains, it has been applied in OM. However, the existing methods, which often\nadopt ad-hoc feature engineering or non-contextual word embeddings, have not\nyet outperformed rule-based systems especially in an unsupervised setting. In\nthis paper, we propose a novel OM system named BERTMap which can support both\nunsupervised and semi-supervised settings. It first predicts mappings using a\nclassifier based on fine-tuning the contextual embedding model BERT on text\nsemantics corpora extracted from ontologies, and then refines the mappings\nthrough extension and repair by utilizing the ontology structure and logic. Our\nevaluation with three alignment tasks on biomedical ontologies demonstrates\nthat BERTMap can often perform better than the leading OM systems LogMap and\nAML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonyrajah_D/0/1/0/all/0/1\">Denvar Antonyrajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models. (arXiv:2112.06598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06598","description":"<p>Large pretrained language models (LMs) have become the central building block\nof many NLP applications. Training these models requires ever more\ncomputational resources and most of the existing models are trained on English\ntext only. It is exceedingly expensive to train these models in other\nlanguages. To alleviate this problem, we introduce a novel method -- called\nWECHSEL -- to efficiently and effectively transfer pretrained LMs to new\nlanguages. WECHSEL can be applied to any model which uses subword-based\ntokenization and learns an embedding for each subword. The tokenizer of the\nsource model (in English) is replaced with a tokenizer in the target language\nand token embeddings are initialized such that they are semantically similar to\nthe English tokens by utilizing multilingual static word embeddings covering\nEnglish and the target language. We use WECHSEL to transfer the English RoBERTa\nand GPT-2 models to four languages (French, German, Chinese and Swahili). We\nalso study the benefits of our method on very low-resource languages. WECHSEL\nimproves over proposed methods for cross-lingual parameter transfer and\noutperforms models of comparable size trained from scratch with up to 64x less\ntraining effort. Our method makes training large language models for new\nlanguages more accessible and less damaging to the environment. We make our\ncode and models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minixhofer_B/0/1/0/all/0/1\">Benjamin Minixhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paischer_F/0/1/0/all/0/1\">Fabian Paischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Local Attentions Remain Competitive for Long-Context Tasks. (arXiv:2112.07210v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07210","description":"<p>Many NLP tasks require processing long contexts beyond the length limit of\npretrained models. In order to scale these models to longer text sequences,\nmany efficient long-range attention variants have been proposed. Despite the\nabundance of research along this direction, it is still difficult to gauge the\nrelative effectiveness of these models in practical use cases, e.g., if we\napply these models following the pretrain-and-finetune paradigm. In this work,\nwe aim to conduct a thorough analysis of these emerging models with large-scale\nand controlled experiments. For each attention variant, we pretrain large-size\nmodels using the same long-doc corpus and then finetune these models for\nreal-world long-context tasks. Our findings reveal pitfalls of an existing\nwidely-used long-range benchmark and show none of the tested efficient\nattentions can beat a simple local window attention under standard pretraining\nparadigms. Further analysis on local attention variants suggests that even the\ncommonly used attention-window overlap is not necessary to achieve good\ndownstream results -- using disjoint local attentions, we are able to build a\nsimpler and more efficient long-doc QA model that matches the performance of\nLongformer~\\citep{longformer} with half of its pretraining compute.\n</p>\n<p>The code to replicate our experiments can be found at\nhttps://github.com/pytorch/fairseq/tree/main/examples/xformers\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liskovich_D/0/1/0/all/0/1\">Diana Liskovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cross-Lingual IR from an English Retriever. (arXiv:2112.08185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08185","description":"<p>We present DR.DECR (Dense Retrieval with Distillation-Enhanced Cross-Lingual\nRepresentation), a new cross-lingual information retrieval (CLIR) system\ntrained using multi-stage knowledge distillation (KD). The teacher of DR.DECR\nrelies on a highly effective but computationally expensive two-stage inference\nprocess consisting of query translation and monolingual IR, while the student,\nDR.DECR, executes a single CLIR step. We teach DR.DECR powerful multilingual\nrepresentations as well as CLIR by optimizing two corresponding KD objectives.\nLearning useful representations of non-English text from an English-only\nretriever is accomplished through a cross-lingual token alignment algorithm\nthat relies on the representation capabilities of the underlying multilingual\nencoders. In both in-domain and zero-shot out-of-domain evaluation, DR.DECR\ndemonstrates far superior accuracy over direct fine-tuning with labeled CLIR\ndata. It is also the best single-model retriever on the XOR-TyDi benchmark at\nthe time of this writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franz_M/0/1/0/all/0/1\">Martin Franz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_B/0/1/0/all/0/1\">Bhavani Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts. (arXiv:2112.08348v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08348","description":"<p>Fine-tuning continuous prompts for target tasks has recently emerged as a\ncompact alternative to full model fine-tuning. Motivated by these promising\nresults, we investigate the feasibility of extracting a discrete (textual)\ninterpretation of continuous prompts that is faithful to the problem they\nsolve. In practice, we observe a \"wayward\" behavior between the task solved by\ncontinuous prompts and their nearest neighbor discrete projections: We can find\ncontinuous prompts that solve a task while being projected to an arbitrary text\n(e.g., definition of a different or even a contradictory task), while being\nwithin a very small (2%) margin of the best continuous prompt of the same size\nfor the task. We provide intuitions behind this odd and surprising behavior, as\nwell as extensive empirical analyses quantifying the effect of various\nparameters. For instance, for larger model sizes we observe higher waywardness,\ni.e, we can find prompts that more closely map to any arbitrary text with a\nsmaller drop in accuracy. These findings have important implications relating\nto the difficulty of faithfully interpreting continuous prompts and their\ngeneralization across models and tasks, providing guidance for future progress\nin prompting language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shane Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ErAConD : Error Annotated Conversational Dialog Dataset for Grammatical Error Correction. (arXiv:2112.08466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08466","description":"<p>Currently available grammatical error correction (GEC) datasets are compiled\nusing well-formed written text, limiting the applicability of these datasets to\nother domains such as informal writing and dialog. In this paper, we present a\nnovel parallel GEC dataset drawn from open-domain chatbot conversations; this\ndataset is, to our knowledge, the first GEC dataset targeted to a\nconversational setting. To demonstrate the utility of the dataset, we use our\nannotated data to fine-tune a state-of-the-art GEC model, resulting in a 16\npoint increase in model precision. This is of particular importance in a GEC\nmodel, as model precision is considered more important than recall in GEC tasks\nsince false positives could lead to serious confusion in language learners. We\nalso present a detailed annotation scheme which ranks errors by perceived\nimpact on comprehensibility, making our dataset both reproducible and\nextensible. Experimental results show the effectiveness of our data in\nimproving GEC model performance in conversational scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_D/0/1/0/all/0/1\">Derek Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1\">Sam Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding. (arXiv:2112.10728v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.10728","description":"<p>Recently, there has been an increasing interest in building question\nanswering (QA) models that reason across multiple modalities, such as text and\nimages. However, QA using images is often limited to just picking the answer\nfrom a pre-defined set of options. In addition, images in the real world,\nespecially in news, have objects that are co-referential to the text, with\ncomplementary information from both modalities. In this paper, we present a new\nQA evaluation benchmark with 1,384 questions over news articles that require\ncross-media grounding of objects in images onto text. Specifically, the task\ninvolves multi-hop questions that require reasoning over image-caption pairs to\nidentify the grounded visual object being referred to and then predicting a\nspan from the news body text to answer the question. In addition, we introduce\na novel multimedia data augmentation framework, based on cross-media knowledge\nextraction and synthetic question-answer generation, to automatically augment\ndata that can provide weak supervision for this task. We evaluate both\npipeline-based and end-to-end pretraining-based multimedia QA models on our\nbenchmark, and show that they achieve promising performance, while considerably\nlagging behind human performance hence leaving large room for future work on\nthis challenging new task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1\">Xilin Rui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Haoyang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Variant Consistency based Self-supervised Learning for Robust Automatic Speech Recognition. (arXiv:2112.12522v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.12522","description":"<p>Automatic speech recognition (ASR) has shown rapid advances in recent years\nbut still degrades significantly in far-field and noisy environments. The\nrecent development of self-supervised learning (SSL) technology can improve the\nASR performance by pre-training the model with additional unlabeled speech and\nthe SSL pre-trained model has achieved the state-of-the-art result on several\nspeech benchmarks. Nevertheless, most of the previous SSL methods ignore the\ninfluence of the background noise or reverberation, which is crucial to\ndeploying ASR systems in real-world speech applications. This study addresses\nthe robust ASR by introducing a multi-variant consistency (MVC) based SSL\nmethod that adapts to different environments. The MVC-SSL is a robust SSL\npre-training method designed for noisy and distant-talking speech in real-world\napplications. Compared to the previous SSL method, the MVC-SSL can calculate\nthe contrastive loss among audios from different acoustic conditions or\nchannels and can learn invariant representations with the change in the\nenvironment or the recording equipment. We also explore different SSL training\npipelines to balance the noisy distant-talking speech and extra high resource\nclean speech. We evaluate the proposed method on the commercially-motivated\ndataset, CHiME-4, and the meeting dataset, AMI. With the help of the MVC-SSL\nand appropriate training pipeline, we can achieve up to 30% relative word error\nrate reductions over the baseline wav2vec2.0, one of the most successful SSL\nmethods for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gaofeng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anticipation-Free Training for Simultaneous Machine Translation. (arXiv:2201.12868v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12868","description":"<p>Simultaneous machine translation (SimulMT) speeds up the translation process\nby starting to translate before the source sentence is completely available. It\nis difficult due to limited context and word order difference between\nlanguages. Existing methods increase latency or introduce adaptive read-write\npolicies for SimulMT models to handle local reordering and improve translation\nquality. However, the long-distance reordering would make the SimulMT models\nlearn translation mistakenly. Specifically, the model may be forced to predict\ntarget tokens when the corresponding source tokens have not been read. This\nleads to aggressive anticipation during inference, resulting in the\nhallucination phenomenon. To mitigate this problem, we propose a new framework\nthat decompose the translation process into the monotonic translation step and\nthe reordering step, and we model the latter by the auxiliary sorting network\n(ASN). The ASN rearranges the hidden states to match the order in the target\nlanguage, so that the SimulMT model could learn to translate more reasonably.\nThe entire model is optimized end-to-end and does not rely on external aligners\nor data. During inference, ASN is removed to achieve streaming. Experiments\nshow the proposed framework could outperform previous methods with less\nlatency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chih-Chiang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-Po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Attention for Language Models. (arXiv:2202.02093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02093","description":"<p>Pretrained language models based on the transformer architecture have shown\ngreat success in NLP. Textual training data often comes from the web and is\nthus tagged with time-specific information, but most language models ignore\nthis information. They are trained on the textual data alone, limiting their\nability to generalize temporally. In this work, we extend the key component of\nthe transformer architecture, i.e., the self-attention mechanism, and propose\ntemporal attention - a time-aware self-attention mechanism. Temporal attention\ncan be applied to any transformer model and requires the input texts to be\naccompanied with their relevant time points. It allows the transformer to\ncapture this temporal information and create time-specific contextualized word\nrepresentations. We leverage these representations for the task of semantic\nchange detection; we apply our proposed mechanism to BERT and experiment on\nthree datasets in different languages (English, German, and Latin) that also\nvary in time, size, and genre. Our proposed model achieves state-of-the-art\nresults on all the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_G/0/1/0/all/0/1\">Guy D. Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Evaluation of Large Language Models of Code. (arXiv:2202.13169v3 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2202.13169","description":"<p>Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities. (arXiv:2203.05325v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05325","description":"<p>In this paper, we present an end-to-end joint entity and relation extraction\napproach based on transformer-based language models. We apply the model to the\ntask of linking mathematical symbols to their descriptions in LaTeX documents.\nIn contrast to existing approaches, which perform entity and relation\nextraction in sequence, our system incorporates information from relation\nextraction into entity extraction. This means that the system can be trained\neven on data sets where only a subset of all valid entity spans is annotated.\nWe provide an extensive evaluation of the proposed system and its strengths and\nweaknesses. Our approach, which can be scaled dynamically in computational\ncomplexity at inference time, produces predictions with high precision and\nreaches 3rd place in the leaderboard of SemEval-2022 Task 12. For inputs in the\ndomain of physics and math, it achieves high relation extraction macro F1\nscores of 95.43% and 79.17%, respectively. The code used for training and\nevaluating our models is available at: https://github.com/nicpopovic/RE1st\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nicholas Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurito_W/0/1/0/all/0/1\">Walter Laurito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_M/0/1/0/all/0/1\">Michael F&#xe4;rber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Pre-training for AMR Parsing and Generation. (arXiv:2203.07836v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07836","description":"<p>Abstract meaning representation (AMR) highlights the core semantic\ninformation of text in a graph structure. Recently, pre-trained language models\n(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,\nrespectively. However, PLMs are typically pre-trained on textual data, thus are\nsub-optimal for modeling structural knowledge. To this end, we investigate\ngraph self-supervised training to improve the structure awareness of PLMs over\nAMR graphs. In particular, we introduce two graph auto-encoding strategies for\ngraph-to-graph pre-training and four tasks to integrate text and graph\ninformation during pre-training. We further design a unified framework to\nbridge the gap between pre-training and fine-tuning tasks. Experiments on both\nAMR parsing and AMR-to-text generation show the superiority of our model. To\nour knowledge, we are the first to consider pre-training on semantic graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Pre-trained Model for Personality and Interpersonal Reactivity Prediction. (arXiv:2203.12481v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12481","description":"<p>This paper describes our proposed method for the Workshop on Computational\nApproaches to Subjectivity, Sentiment &amp; Social Media Analysis (WASSA) 2022\nshared task on Personality Prediction (PER) and Reactivity Index Prediction\n(IRI). In this paper, we adopt the prompt-based learning method with the\npre-trained language model to accomplish these tasks. Specifically, the prompt\nis designed to provide knowledge of the extra personalized information for\nenhancing the pre-trained model. Data augmentation and model ensemble are\nadopted for obtaining better results. Moreover, we also provided the online\nsoftware demonstration and the codes of the software for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks. (arXiv:2203.16773v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.16773","description":"<p>Speech representations learned from Self-supervised learning (SSL) models can\nbenefit various speech processing tasks. However, utilizing SSL representations\nusually requires fine-tuning the pre-trained models or designing task-specific\ndownstream models and loss functions, causing much memory usage and human\nlabor. Recently, prompting in Natural Language Processing (NLP) has been found\nto be an efficient technique to leverage pre-trained language models (LMs).\nSpecifically, prompt tuning optimizes a limited number of task-specific\nparameters with a fixed pre-trained model; as a result, only a small set of\nparameters is needed to be stored for each task. Prompt tuning improves\ncomputation and memory efficiency by leveraging the pre-trained LM's prediction\nability. Nevertheless, such a paradigm is little studied in the speech\ncommunity. We report in this paper the first exploration of the prompt tuning\nparadigm for speech processing tasks based on Generative Spoken Language Model\n(GSLM). Experiment results show that the prompt tuning technique achieves\ncompetitive performance in speech classification tasks with fewer trainable\nparameters than fine-tuning specialized downstream models. We further study the\ntechnique in challenging sequence generation tasks. Prompt tuning also\ndemonstrates its potential, while the limitation and possible research\ndirections are discussed in this paper. The source code is available on\nhttps://github.com/ga642381/SpeechPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quick Starting Dialog Systems with Paraphrase Generation. (arXiv:2204.02546v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02546","description":"<p>Acquiring training data to improve the robustness of dialog systems can be a\npainstakingly long process. In this work, we propose a method to reduce the\ncost and effort of creating new conversational agents by artificially\ngenerating more data from existing examples, using paraphrase generation. Our\nproposed approach can kick-start a dialog system with little human effort, and\nbrings its performance to a level satisfactory enough for allowing actual\ninteractions with real end-users. We experimented with two neural paraphrasing\napproaches, namely Neural Machine Translation and a Transformer-based seq2seq\nmodel. We present the results obtained with two datasets in English and in\nFrench:~a crowd-sourced public intent classification dataset and our own\ncorporate dialog system dataset. We show that our proposed approach increased\nthe generalization capabilities of the intent classification model on both\ndatasets, reducing the effort required to initialize a new dialog system and\nhelping to deploy this technology at scale within an organization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marceau_L/0/1/0/all/0/1\">Louis Marceau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belbahar_R/0/1/0/all/0/1\">Raouf Belbahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queudot_M/0/1/0/all/0/1\">Marc Queudot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naji_N/0/1/0/all/0/1\">Nada Naji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_E/0/1/0/all/0/1\">Eric Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meurs_M/0/1/0/all/0/1\">Marie-Jean Meurs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language. (arXiv:2204.04487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04487","description":"<p>Spurious correlations are a threat to the trustworthiness of natural language\nprocessing systems, motivating research into methods for identifying and\neliminating them. However, addressing the problem of spurious correlations\nrequires more clarity on what they are and how they arise in language data.\nGardner et al (2021) argue that the compositional nature of language implies\nthat \\emph{all} correlations between labels and individual \"input features\" are\nspurious. This paper analyzes this proposal in the context of a toy example,\ndemonstrating three distinct conditions that can give rise to feature-label\ncorrelations in a simple PCFG. Linking the toy example to a structured causal\nmodel shows that (1) feature-label correlations can arise even when the label\nis invariant to interventions on the feature, and (2) feature-label\ncorrelations may be absent even when the label is sensitive to interventions on\nthe feature. Because input features will be individually correlated with labels\nin all but very rare circumstances, domain knowledge must be applied to\nidentify spurious correlations that pose genuine robustness threats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding. (arXiv:2204.06283v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06283","description":"<p>In the age of large transformer language models, linguistic evaluation play\nan important role in diagnosing models' abilities and limitations on natural\nlanguage understanding. However, current evaluation methods show some\nsignificant shortcomings. In particular, they do not provide insight into how\nwell a language model captures distinct linguistic skills essential for\nlanguage understanding and reasoning. Thus they fail to effectively map out the\naspects of language understanding that remain challenging to existing models,\nwhich makes it hard to discover potential limitations in models and datasets.\nIn this paper, we introduce Curriculum as a new format of NLI benchmark for\nevaluation of broad-coverage linguistic phenomena. Curriculum contains a\ncollection of datasets that covers 36 types of major linguistic phenomena and\nan evaluation procedure for diagnosing how well a language model captures\nreasoning skills for distinct types of linguistic phenomena. We show that this\nlinguistic-phenomena-driven benchmark can serve as an effective tool for\ndiagnosing model behavior and verifying model learning quality. In addition,\nour experiments provide insight into the limitation of existing benchmark\ndatasets and state-of-the-art models that may encourage future research on\nre-designing datasets, model architectures, and learning objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiyue Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection. (arXiv:2204.06389v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06389","description":"<p>The last decade has witnessed a surge in the interaction of people through\nsocial networking platforms. While there are several positive aspects of these\nsocial platforms, the proliferation has led them to become the breeding ground\nfor cyber-bullying and hate speech. Recent advances in NLP have often been used\nto mitigate the spread of such hateful content. Since the task of hate speech\ndetection is usually applicable in the context of social networks, we introduce\nCRUSH, a framework for hate speech detection using user-anchored\nself-supervision and contextual regularization. Our proposed approach secures ~\n1-12% improvement in test set metrics over best performing previous approaches\non two types of tasks and multiple popular english social media datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Souvic Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1\">Parag Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sumegh Roychowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shedding New Light on the Language of the Dark Web. (arXiv:2204.06885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06885","description":"<p>The hidden nature and the limited accessibility of the Dark Web, combined\nwith the lack of public datasets in this domain, make it difficult to study its\ninherent characteristics such as linguistic properties. Previous works on text\nclassification of Dark Web domain have suggested that the use of deep neural\nmodels may be ineffective, potentially due to the linguistic differences\nbetween the Dark and Surface Webs. However, not much work has been done to\nuncover the linguistic characteristics of the Dark Web. This paper introduces\nCoDA, a publicly available Dark Web dataset consisting of 10000 web documents\ntailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a\nthorough linguistic analysis of the Dark Web and examine the textual\ndifferences between the Dark Web and the Surface Web. We also assess the\nperformance of various methods of Dark Web page classification. Finally, we\ncompare CoDA with an existing public Dark Web dataset and evaluate their\nsuitability for various use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngjin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">Eugene Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yongjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungwon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jin-Woo Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction. (arXiv:2204.10994v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10994","description":"<p>This paper presents MuCGEC, a multi-reference multi-source evaluation dataset\nfor Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences\ncollected from three Chinese-as-a-Second-Language (CSL) learner sources. Each\nsentence is corrected by three annotators, and their corrections are carefully\nreviewed by a senior annotator, resulting in 2.3 references per sentence. We\nconduct experiments with two mainstream CGEC models, i.e., the\nsequence-to-sequence model and the sequence-to-edit model, both enhanced with\nlarge pretrained language models, achieving competitive benchmark performance\non previous and our datasets. We also discuss CGEC evaluation methodologies,\nincluding the effect of multiple references and using a char-based metric. Our\nannotation guidelines, data, and code are available at\n\\url{https://github.com/HillZhang1999/MuCGEC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zuyi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Hypergraph-based Nested Named Entity Recognition as Query-based Sequence Labeling. (arXiv:2204.11467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11467","description":"<p>There has been a growing academic interest in the recognition of nested named\nentities in many domains. We tackle the task with a novel local\nhypergraph-based method: We first propose start token candidates and generate\ncorresponding queries with their surrounding context, then use a query-based\nsequence labeling module to form a local hypergraph for each candidate. An end\ntoken estimator is used to correct the hypergraphs and get the final\npredictions. Compared to span-based approaches, our method is free of the high\ncomputation cost of span sampling and the risk of losing long entities.\nSequential prediction makes it easier to leverage information in word order\ninside nested structures, and richer representations are built with a local\nhypergraph. Experiments show that our proposed method outperforms all the\nprevious hypergraph-based and sequence labeling approaches with large margins\non all four nested datasets. It achieves a new state-of-the-art F1 score on the\nACE 2004 dataset and competitive F1 scores with previous state-of-the-art\nmethods on three other nested NER datasets: ACE 2005, GENIA, and KBP 2017.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yukun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sen Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Rationalization Improve Robustness?. (arXiv:2204.11790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11790","description":"<p>A growing line of work has investigated the development of neural NLP models\nthat can produce rationales--subsets of input that can explain their model\npredictions. In this paper, we ask whether such rationale models can also\nprovide robustness to adversarial attacks in addition to their interpretable\nnature. Since these models need to first generate rationales (\"rationalizer\")\nbefore making predictions (\"predictor\"), they have the potential to ignore\nnoise or adversarially added text by simply masking it out of the generated\nrationale. To this end, we systematically generate various types of 'AddText'\nattacks for both token and sentence-level rationalization tasks, and perform an\nextensive empirical evaluation of state-of-the-art rationale models across five\ndifferent tasks. Our experiments reveal that the rationale models show the\npromise to improve robustness, while they struggle in certain scenarios--when\nthe rationalizer is sensitive to positional bias or lexical choices of attack\ntext. Further, leveraging human rationale as supervision does not always\ntranslate to better performance. Our study is a first step towards exploring\nthe interplay between interpretability and robustness in the\nrationalize-then-predict framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Howard Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jacqueline He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPERA:Operation-Pivoted Discrete Reasoning over Text. (arXiv:2204.14166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14166","description":"<p>Machine reading comprehension (MRC) that requires discrete reasoning\ninvolving symbolic operations, e.g., addition, sorting, and counting, is a\nchallenging task. According to this nature, semantic parsing-based methods\npredict interpretable but complex logical forms. However, logical form\ngeneration is nontrivial and even a little perturbation in a logical form will\nlead to wrong answers. To alleviate this issue, multi-predictor -based methods\nare proposed to directly predict different types of answers and achieve\nimprovements. However, they ignore the utilization of symbolic operations and\nencounter a lack of reasoning ability and interpretability. To inherit the\nadvantages of these two types of methods, we propose OPERA, an\noperation-pivoted discrete reasoning framework, where lightweight symbolic\noperations (compared with logical forms) as neural modules are utilized to\nfacilitate the reasoning ability and interpretability. Specifically, operations\nare first selected and then softly executed to simulate the answer reasoning\nprocedure. Extensive experiments on both DROP and RACENum datasets show the\nreasoning ability of OPERA. Moreover, further analysis verifies its\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chaoqun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Naturalized Semantic Parsers with Very Little Data. (arXiv:2204.14243v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14243","description":"<p>Semantic parsing is an important NLP problem, particularly for voice\nassistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic\nparsers are seq2seq architectures based on large language models that have been\npretrained on vast amounts of text. To better leverage that pretraining, recent\nwork has explored a reformulation of semantic parsing whereby the output\nsequences are themselves natural language sentences, but in a controlled\nfragment of natural language. This approach delivers strong results,\nparticularly for few-shot semantic parsing, which is of key importance in\npractice and the focus of our paper. We push this line of work forward by\nintroducing an automated methodology that delivers very significant additional\nimprovements by utilizing modest amounts of unannotated data, which is\ntypically easy to obtain. Our method is based on a novel synthesis of four\ntechniques: joint training with auxiliary unsupervised tasks; constrained\ndecoding; self-training; and paraphrasing. We show that this method delivers\nnew SOTA few-shot performance on the Overnight dataset, particularly in very\nlow-resource settings, and very compelling few-shot results on a new semantic\nparsing dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rongali_S/0/1/0/all/0/1\">Subendhu Rongali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubino_M/0/1/0/all/0/1\">Melanie Rubino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. (arXiv:2205.00001v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.00001","description":"<p>Having a rich multimodal inner language is an important component of human\nintelligence that enables several necessary core cognitive functions such as\nmultimodal prediction, translation, and generation. Building upon the Conscious\nTuring Machine (CTM), a machine model for consciousness proposed by Blum and\nBlum (2021), we describe the desiderata of a multimodal language called\nBrainish, comprising words, images, audio, and sensations combined in\nrepresentations that the CTM's processors use to communicate with each other.\nWe define the syntax and semantics of Brainish before operationalizing this\nlanguage through the lens of multimodal artificial intelligence, a vibrant\nresearch area studying the computational tools necessary for processing and\nrelating information from heterogeneous signals. Our general framework for\nlearning Brainish involves designing (1) unimodal encoders to segment and\nrepresent unimodal data, (2) a coordinated representation space that relates\nand composes unimodal features to derive holistic meaning across multimodal\ninputs, and (3) decoders to map multimodal representations into predictions\n(for fusion) or raw data (for translation or generation). Through discussing\nhow Brainish is crucial for communication and coordination in order to achieve\nconsciousness in the CTM, and by implementing a simple version of Brainish and\nevaluating its capability of demonstrating intelligence on multimodal\nprediction and retrieval tasks on several real-world image, text, and audio\ndatasets, we argue that such an inner language will be important for advances\nin machine models of intelligence and consciousness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do we Really Know about State of the Art NER?. (arXiv:2205.00034v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00034","description":"<p>Named Entity Recognition (NER) is a well researched NLP task and is widely\nused in real world NLP scenarios. NER research typically focuses on the\ncreation of new ways of training NER, with relatively less emphasis on\nresources and evaluation. Further, state of the art (SOTA) NER models, trained\non standard datasets, typically report only a single performance measure\n(F-score) and we don't really know how well they do for different entity types\nand genres of text, or how robust are they to new, unseen entities. In this\npaper, we perform a broad evaluation of NER using a popular dataset, that takes\ninto consideration various text genres and sources constituting the dataset at\nhand. Additionally, we generate six new adversarial test sets through small\nperturbations in the original test set, replacing select entities while\nretaining the context. We also train and test our models on randomly generated\ntrain/dev/test splits followed by an experiment where the models are trained on\na select set of genres but tested genres not seen in training. These\ncomprehensive evaluation strategies were performed using three SOTA NER models.\nBased on our results, we recommend some useful reporting practices for NER\nresearchers, that could help in providing a better understanding of a SOTA\nmodel's performance in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vajjala_S/0/1/0/all/0/1\">Sowmya Vajjala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_R/0/1/0/all/0/1\">Ramya Balasubramaniam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Bias in Masked Language Models for Multiple Languages. (arXiv:2205.00551v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00551","description":"<p>Masked Language Models (MLMs) pre-trained by predicting masked tokens on\nlarge corpora have been used successfully in natural language processing tasks\nfor a variety of languages. Unfortunately, it was reported that MLMs also learn\ndiscriminative biases regarding attributes such as gender and race. Because\nmost studies have focused on MLMs in English, the bias of MLMs in other\nlanguages has rarely been investigated. Manual annotation of evaluation data\nfor languages other than English has been challenging due to the cost and\ndifficulty in recruiting annotators. Moreover, the existing bias evaluation\nmethods require the stereotypical sentence pairs consisting of the same context\nwith attribute words (e.g. He/She is a nurse). We propose Multilingual Bias\nEvaluation (MBE) score, to evaluate bias in various languages using only\nEnglish attribute word lists and parallel corpora between the target language\nand English without requiring manually annotated data. We evaluated MLMs in\neight languages using the MBE and confirmed that gender-related biases are\nencoded in MLMs for all those languages. We manually created datasets for\ngender bias in Japanese and Russian to evaluate the validity of the MBE. The\nresults show that the bias scores reported by the MBE significantly correlates\nwith that computed from the above manually created datasets and the existing\nEnglish datasets for gender bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imankulova_A/0/1/0/all/0/1\">Aizhan Imankulova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas. (arXiv:2205.00872v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00872","description":"<p>Maintaining a consistent persona is essential for building a human-like\nconversational model. However, the lack of attention to the partner makes the\nmodel more egocentric: they tend to show their persona by all means such as\ntwisting the topic stiffly, pulling the conversation to their own interests\nregardless, and rambling their persona with little curiosity to the partner. In\nthis work, we propose COSPLAY(COncept Set guided PersonaLized dialogue\ngeneration Across both partY personas) that considers both parties as a \"team\":\nexpressing self-persona while keeping curiosity toward the partner, leading\nresponses around mutual personas, and finding the common ground. Specifically,\nwe first represent self-persona, partner persona and mutual dialogue all in the\nconcept sets. Then, we propose the Concept Set framework with a suite of\nknowledge-enhanced operations to process them such as set algebras, set\nexpansion, and set distance. Based on these operations as medium, we train the\nmodel by utilizing 1) concepts of both party personas, 2) concept relationship\nbetween them, and 3) their relationship to the future dialogue. Extensive\nexperiments on a large public dataset, Persona-Chat, demonstrate that our model\noutperforms state-of-the-art baselines for generating less egocentric, more\nhuman-like, and higher quality responses in both automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chuangbai Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding patterns in Knowledge Attribution for Transformers. (arXiv:2205.01366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01366","description":"<p>We analyze the Knowledge Neurons framework for the attribution of factual and\nrelational knowledge to particular neurons in the transformer network. We use a\n12-layer multi-lingual BERT model for our experiments. Our study reveals\nvarious interesting phenomena. We observe that mostly factual knowledge can be\nattributed to middle and higher layers of the network($\\ge 6$). Further\nanalysis reveals that the middle layers($6-9$) are mostly responsible for\nrelational information, which is further refined into actual factual knowledge\nor the \"correct answer\" in the last few layers($10-12$). Our experiments also\nshow that the model handles prompts in different languages, but representing\nthe same fact, similarly, providing further evidence for effectiveness of\nmulti-lingual pre-training. Applying the attribution scheme for grammatical\nknowledge, we find that grammatical knowledge is far more dispersed among the\nneurons than factual knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1\">Jeevesh Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Ritu Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exact Paired-Permutation Testing for Structured Test Statistics. (arXiv:2205.01416v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01416","description":"<p>Significance testing -- especially the paired-permutation test -- has played\na vital role in developing NLP systems to provide confidence that the\ndifference in performance between two systems (i.e., the test statistic) is not\ndue to luck. However, practitioners rely on Monte Carlo approximation to\nperform this test due to a lack of a suitable exact algorithm. In this paper,\nwe provide an efficient exact algorithm for the paired-permutation test for a\nfamily of structured test statistics. Our algorithm runs in $\\mathcal{O}(GN\n(\\log GN )(\\log N ))$ time where $N$ is the dataset size and $G$ is the range\nof the test statistic. We found that our exact algorithm was $10$x faster than\nthe Monte Carlo approximation with $20000$ samples on a common dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1\">Ran Zmigrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Training for High-Stakes Reliability. (arXiv:2205.01663v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.01663","description":"<p>In the future, powerful AI systems may be deployed in high-stakes settings,\nwhere a single failure could be catastrophic. One technique for improving AI\nsafety in high-stakes settings is adversarial training, which uses an adversary\nto generate examples to train on in order to achieve better worst-case\nperformance.\n</p>\n<p>In this work, we used a language generation task as a testbed for achieving\nhigh reliability through adversarial training. We created a series of\nadversarial training techniques -- including a tool that assists human\nadversaries -- to find and eliminate failures in a classifier that filters text\ncompletions suggested by a generator. In our simple \"avoid injuries\" task, we\ndetermined that we can set very conservative classifier thresholds without\nsignificantly impacting the quality of the filtered outputs. With our chosen\nthresholds, filtering with our baseline classifier decreases the rate of unsafe\ncompletions from about 2.4% to 0.003% on in-distribution data, which is near\nthe limit of our ability to measure. We found that adversarial training\nsignificantly increased robustness to the adversarial attacks that we trained\non, without affecting in-distribution performance. We hope to see further work\nin the high-stakes reliability setting, including more powerful tools for\nenhancing human adversaries and better ways to measure high levels of\nreliability, until we can confidently rule out the possibility of catastrophic\ndeployment-time failures of powerful models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nix_S/0/1/0/all/0/1\">Seraphina Nix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1\">Lawrence Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauman_T/0/1/0/all/0/1\">Tim Bauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Nielsen_P/0/1/0/all/0/1\">Peter Schmidt-Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherlis_A/0/1/0/all/0/1\">Adam Scherlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabeshima_N/0/1/0/all/0/1\">Noa Nabeshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinstein_Raun_B/0/1/0/all/0/1\">Ben Weinstein-Raun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haas_D/0/1/0/all/0/1\">Daniel de Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1\">Buck Shlegeris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_N/0/1/0/all/0/1\">Nate Thomas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIB's Visual Analytics Group at MediaEval '20: Detecting Fake News on Corona Virus and 5G Conspiracy. (arXiv:2101.03529v1 [cs.SI] CROSS LISTED)","link":"http://arxiv.org/abs/2101.03529","description":"<p>Fake news on social media has become a hot topic of research as it negatively\nimpacts the discourse of real news in the public. Specifically, the ongoing\nCOVID-19 pandemic has seen a rise of inaccurate and misleading information due\nto the surrounding controversies and unknown details at the beginning of the\npandemic. The FakeNews task at MediaEval 2020 tackles this problem by creating\na challenge to automatically detect tweets containing misinformation based on\ntext and structure from Twitter follower network. In this paper, we present a\nsimple approach that uses BERT embeddings and a shallow neural network for\nclassifying tweets using only text, and discuss our findings and limitations of\nthe approach in text-based misinformation detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1\">Gullal S. Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The scope for AI-augmented interpretation of building blueprints in commercial and industrial property insurance. (arXiv:2205.01671v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01671","description":"<p>This report, commissioned by the WTW research network, investigates the use\nof AI in property risk assessment. It (i) reviews existing work on risk\nassessment in commercial and industrial properties and automated information\nextraction from building blueprints; and (ii) presents an exploratory 'proof-of\nconcept-solution' exploring the feasibility of using machine learning for the\nautomated extraction of information from building blueprints to support\ninsurance risk assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milne_A/0/1/0/all/0/1\">Alistair Milne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillier_J/0/1/0/all/0/1\">John Hillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oglesby_F/0/1/0/all/0/1\">Frances Oglesby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning-based Integrated Framework for Quality-aware Undersampled Cine Cardiac MRI Reconstruction and Analysis. (arXiv:2205.01673v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01673","description":"<p>Cine cardiac magnetic resonance (CMR) imaging is considered the gold standard\nfor cardiac function evaluation. However, cine CMR acquisition is inherently\nslow and in recent decades considerable effort has been put into accelerating\nscan times without compromising image quality or the accuracy of derived\nresults. In this paper, we present a fully-automated, quality-controlled\nintegrated framework for reconstruction, segmentation and downstream analysis\nof undersampled cine CMR data. The framework enables active acquisition of\nradial k-space data, in which acquisition can be stopped as soon as acquired\ndata are sufficient to produce high quality reconstructions and segmentations.\nThis results in reduced scan times and automated analysis, enabling robust and\naccurate estimation of functional biomarkers. To demonstrate the feasibility of\nthe proposed approach, we perform realistic simulations of radial k-space\nacquisitions on a dataset of subjects from the UK Biobank and present results\non in-vivo cine CMR k-space data collected from healthy subjects. The results\ndemonstrate that our method can produce quality-controlled images in a mean\nscan time reduced from 12 to 4 seconds per slice, and that image quality is\nsufficient to allow clinically relevant parameters to be automatically\nestimated to within 5% mean absolute difference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Machado_I/0/1/0/all/0/1\">In&#xea;s P. Machado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Ant&#xf3;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1\">Gast&#xe3;o Cruz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ugurlu_D/0/1/0/all/0/1\">Devran Ugurlu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Olakorede_I/0/1/0/all/0/1\">Ihsane Olakorede</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oksuz_I/0/1/0/all/0/1\">Ilkay Oksuz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castelo_Branco_M/0/1/0/all/0/1\">Miguel Castelo-Branco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1\">Alistair A. Young</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer. (arXiv:2205.01674v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01674","description":"<p>Robust self-training (RST) can augment the adversarial robustness of image\nclassification models without significantly sacrificing models'\ngeneralizability. However, RST and other state-of-the-art defense approaches\nfailed to preserve the generalizability and reproduce their good adversarial\nrobustness on small medical image sets. In this work, we propose the\nMulti-instance RST with a drop-max layer, namely MIRST-DM, which involves a\nsequence of iteratively generated adversarial instances during training to\nlearn smoother decision boundaries on small datasets. The proposed drop-max\nlayer eliminates unstable features and helps learn representations that are\nrobust to image perturbations. The proposed approach was validated using a\nsmall breast ultrasound dataset with 1,190 images. The results demonstrate that\nthe proposed approach achieves state-of-the-art adversarial robustness against\nthree prevalent attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1\">Shoukun Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghanem_H/0/1/0/all/0/1\">Hossny Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Framework for Real-time Fetal Brain Segmentation in MRI. (arXiv:2205.01675v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01675","description":"<p>Fetal brain segmentation is an important first step for slice-level motion\ncorrection and slice-to-volume reconstruction in fetal MRI. Fast and accurate\nsegmentation of the fetal brain on fetal MRI is required to achieve real-time\nfetal head pose estimation and motion tracking for slice re-acquisition and\nsteering. To address this critical unmet need, in this work we analyzed the\nspeed-accuracy performance of a variety of deep neural network models, and\ndevised a symbolically small convolutional neural network that combines spatial\ndetails at high resolution with context features extracted at lower\nresolutions. We used multiple branches with skip connections to maintain high\naccuracy while devising a parallel combination of convolution and pooling\noperations as an input downsampling module to further reduce inference time. We\ntrained our model as well as eight alternative, state-of-the-art networks with\nmanually-labeled fetal brain MRI slices and tested on two sets of normal and\nchallenging test cases. Experimental results show that our network achieved the\nhighest accuracy and lowest inference time among all of the compared\nstate-of-the-art real-time segmentation methods. We achieved average Dice\nscores of 97.99\\% and 84.04\\% on the normal and challenging test sets,\nrespectively, with an inference time of 3.36 milliseconds per image on an\nNVIDIA GeForce RTX 2080 Ti. Code, data, and the trained models are available at\nhttps://github.com/bchimagine/real_time_fetal_brain_segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Faghihpirayesh_R/0/1/0/all/0/1\">Razieh Faghihpirayesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1\">Davood Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholipour_A/0/1/0/all/0/1\">Ali Gholipour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FundusQ-Net: a Regression Quality Assessment Deep Learning Algorithm for Fundus Images Quality Grading. (arXiv:2205.01676v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01676","description":"<p>Objective: Ophthalmological pathologies such as glaucoma, diabetic\nretinopathy and age-related macular degeneration are major causes of blindness\nand vision impairment. There is a need for novel decision support tools that\ncan simplify and speed up the diagnosis of these pathologies. A key step in\nthis process is to automatically estimate the quality of the fundus images to\nmake sure these are interpretable by a human operator or a machine learning\nmodel. We present a novel fundus image quality scale and deep learning (DL)\nmodel that can estimate fundus image quality relative to this new scale.\n</p>\n<p>Methods: A total of 1,245 images were graded for quality by two\nophthalmologists within the range 1-10, with a resolution of 0.5. A DL\nregression model was trained for fundus image quality assessment. The\narchitecture used was Inception-V3. The model was developed using a total of\n89,947 images from 6 databases, of which 1,245 were labeled by the specialists\nand the remaining 88,702 images were used for pre-training and semi-supervised\nlearning. The final DL model was evaluated on an internal test set (n=209) as\nwell as an external test set (n=194).\n</p>\n<p>Results: The final DL model, denoted FundusQ-Net, achieved a mean absolute\nerror of 0.61 (0.54-0.68) on the internal test set. When evaluated as a binary\nclassification model on the public DRIMDB database as an external test set the\nmodel obtained an accuracy of 99%.\n</p>\n<p>Significance: the proposed algorithm provides a new robust tool for automated\nquality grading of fundus images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abramovich_O/0/1/0/all/0/1\">Or Abramovich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pizem_H/0/1/0/all/0/1\">Hadas Pizem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eijgen_J/0/1/0/all/0/1\">Jan Van Eijgen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stalmans_I/0/1/0/all/0/1\">Ingeborg Stalmans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumenthal_E/0/1/0/all/0/1\">Eytan Blumenthal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Behar_J/0/1/0/all/0/1\">Joachim A. Behar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics to the Rescue: Deep Non-line-of-sight Reconstruction for High-speed Imaging. (arXiv:2205.01679v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01679","description":"<p>Computational approach to imaging around the corner, or non-line-of-sight\n(NLOS) imaging, is becoming a reality thanks to major advances in imaging\nhardware and reconstruction algorithms. A recent development towards practical\nNLOS imaging, Nam et al. demonstrated a high-speed non-confocal imaging system\nthat operates at 5Hz, 100x faster than the prior art. This enormous gain in\nacquisition rate, however, necessitates numerous approximations in light\ntransport, breaking many existing NLOS reconstruction methods that assume an\nidealized image formation model. To bridge the gap, we present a novel deep\nmodel that incorporates the complementary physics priors of wave propagation\nand volume rendering into a neural network for high-quality and robust NLOS\nreconstruction. This orchestrated design regularizes the solution space by\nrelaxing the image formation model, resulting in a deep model that generalizes\nwell on real captures despite being exclusively trained on synthetic data.\nFurther, we devise a unified learning framework that enables our model to be\nflexibly trained using diverse supervision signals, including target intensity\nimages or even raw NLOS transient measurements. Once trained, our model renders\nboth intensity and depth images at inference time in a single forward pass,\ncapable of processing more than 5 captures per second on a high-end GPU.\nThrough extensive qualitative and quantitative experiments, we show that our\nmethod outperforms prior physics and learning based approaches on both\nsynthetic and real measurements. We anticipate that our method along with the\nfast capturing system will accelerate future development of NLOS imaging for\nreal world applications that require high-speed imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mu_F/0/1/0/all/0/1\">Fangzhou Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mo_S/0/1/0/all/0/1\">Sicheng Mo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_J/0/1/0/all/0/1\">Jiayong Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaochun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nam_J/0/1/0/all/0/1\">Ji Hyun Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raghavan_S/0/1/0/all/0/1\">Siddeshwar Raghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Velten_A/0/1/0/all/0/1\">Andreas Velten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpineNetV2: Automated Detection, Labelling and Radiological Grading Of Clinical MR Scans. (arXiv:2205.01683v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01683","description":"<p>This technical report presents SpineNetV2, an automated tool which: (i)\ndetects and labels vertebral bodies in clinical spinal magnetic resonance (MR)\nscans across a range of commonly used sequences; and (ii) performs radiological\ngrading of lumbar intervertebral discs in T2-weighted scans for a range of\ncommon degenerative changes. SpineNetV2 improves over the original SpineNet\nsoftware in two ways: (1) The vertebral body detection stage is significantly\nfaster, more accurate and works across a range of fields-of-view (as opposed to\njust lumbar scans). (2) Radiological grading adopts a more powerful\narchitecture, adding several new grading schemes without loss in performance. A\ndemo of the software is available at the project website:\n<a href=\"http://zeus.robots.ox.ac.uk/spinenet2/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Windsor_R/0/1/0/all/0/1\">Rhydian Windsor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Random Histogram Equalization on Breast Calcification Analysis Using Deep Learning. (arXiv:2205.01684v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01684","description":"<p>Early detection and analysis of calcifications in mammogram images is crucial\nin a breast cancer diagnosis workflow. Management of calcifications that\nrequire immediate follow-up and further analyzing its benignancy or malignancy\ncan result in a better prognosis. Recent studies have shown that deep\nlearning-based algorithms can learn robust representations to analyze\nsuspicious calcifications in mammography. In this work, we demonstrate that\nrandomly equalizing the histograms of calcification patches as a data\naugmentation technique can significantly improve the classification performance\nfor analyzing suspicious calcifications. We validate our approach by using the\nCBIS-DDSM dataset for two classification tasks. The results on both the tasks\nshow that the proposed methodology gains more than 1% mean accuracy and\nF1-score when equalizing the data with a probability of 0.4 when compared to\nnot using histogram equalization. This is further supported by the t-tests,\nwhere we obtain a p-value of p&lt;0.0001, thus showing the statistical\nsignificance of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Panambur_A/0/1/0/all/0/1\">Adarsh Bhandary Panambur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madhu_P/0/1/0/all/0/1\">Prathmesh Madhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart City Intersections: Intelligence Nodes for Future Metropolises. (arXiv:2205.01686v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01686","description":"<p>Traffic intersections are the most suitable locations for the deployment of\ncomputing, communications, and intelligence services for smart cities of the\nfuture. The abundance of data to be collected and processed, in combination\nwith privacy and security concerns, motivates the use of the edge-computing\nparadigm which aligns well with physical intersections in metropolises. This\npaper focuses on high-bandwidth, low-latency applications, and in that context\nit describes: (i) system design considerations for smart city intersection\nintelligence nodes; (ii) key technological components including sensors,\nnetworking, edge computing, low latency design, and AI-based intelligence; and\n(iii) applications such as privacy preservation, cloud-connected vehicles, a\nreal-time \"radar-screen\", traffic management, and monitoring of pedestrian\nbehavior during pandemics. The results of the experimental studies performed on\nthe COSMOS testbed located in New York City are illustrated. Future challenges\nin designing human-centered smart city intersections are summarized.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zoran Kosti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angus_A/0/1/0/all/0/1\">Alex Angus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhuoxu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seskar_I/0/1/0/all/0/1\">Ivan Seskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zussman_G/0/1/0/all/0/1\">Gil Zussman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1\">Dipankar Raychaudhuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End2End Multi-View Feature Matching using Differentiable Pose Optimization. (arXiv:2205.01694v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01694","description":"<p>Learning-based approaches have become indispensable for camera pose\nestimation. However, feature detection, description, matching, and pose\noptimization are often approached in an isolated fashion. In particular,\nerroneous feature matches have severe impact on subsequent camera pose\nestimation and often require additional measures such as outlier rejection. Our\nmethod tackles this challenge by addressing feature matching and pose\noptimization jointly: first, we integrate information from multiple views into\nthe matching by spanning a graph attention network across multiple frames to\npredict their matches all at once. Second, the resulting matches along with\ntheir predicted confidences are used for robust pose optimization with a\ndifferentiable Gauss-Newton solver. End-to-end training combined with\nmulti-view feature matching boosts the pose estimation metrics compared to\nSuperGlue by 8.9% on ScanNet and 10.7% on MegaDepth on average. Our approach\nimproves both pose estimation and matching accuracy over state-of-the-art\nmatching networks. Training feature matching across multiple views with\ngradients from pose optimization naturally learns to disregard outliers,\nthereby rendering additional outlier handling unnecessary, which is highly\ndesirable for pose estimation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roessle_B/0/1/0/all/0/1\">Barbara Roessle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Class Aware Video Anomaly Detection through Image Translation. (arXiv:2205.01706v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01706","description":"<p>Semi-supervised video anomaly detection (VAD) methods formulate the task of\nanomaly detection as detection of deviations from the learned normal patterns.\nPrevious works in the field (reconstruction or prediction-based methods) suffer\nfrom two drawbacks: 1) They focus on low-level features, and they (especially\nholistic approaches) do not effectively consider the object classes. 2)\nObject-centric approaches neglect some of the context information (such as\nlocation). To tackle these challenges, this paper proposes a novel two-stream\nobject-aware VAD method that learns the normal appearance and motion patterns\nthrough image translation tasks. The appearance branch translates the input\nimage to the target semantic segmentation map produced by Mask-RCNN, and the\nmotion branch associates each frame with its expected optical flow magnitude.\nAny deviation from the expected appearance or motion in the inference stage\nshows the degree of potential abnormality. We evaluated our proposed method on\nthe ShanghaiTech, UCSD-Ped1, and UCSD-Ped2 datasets and the results show\ncompetitive performance compared with state-of-the-art works. Most importantly,\nthe results show that, as significant improvements to previous methods,\ndetections by our method are completely explainable and anomalies are localized\naccurately in the frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_M/0/1/0/all/0/1\">Mohammad Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergevin_R/0/1/0/all/0/1\">Robert Bergevin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Defense of Image Pre-Training for Spatiotemporal Recognition. (arXiv:2205.01721v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01721","description":"<p>Image pre-training, the current de-facto paradigm for a wide range of visual\ntasks, is generally less favored in the field of video recognition. By\ncontrast, a common strategy is to directly train with spatiotemporal\nconvolutional neural networks (CNNs) from scratch. Nonetheless, interestingly,\nby taking a closer look at these from-scratch learned CNNs, we note there exist\ncertain 3D kernels that exhibit much stronger appearance modeling ability than\nothers, arguably suggesting appearance information is already well disentangled\nin learning. Inspired by this observation, we hypothesize that the key to\neffectively leveraging image pre-training lies in the decomposition of learning\nspatial and temporal features, and revisiting image pre-training as the\nappearance prior to initializing 3D kernels. In addition, we propose\nSpatial-Temporal Separable (STS) convolution, which explicitly splits the\nfeature channels into spatial and temporal groups, to further enable a more\nthorough decomposition of spatiotemporal features for fine-tuning 3D CNNs. Our\nexperiments show that simply replacing 3D convolution with STS notably improves\na wide range of 3D CNNs without increasing parameters and computation on both\nKinetics-400 and Something-Something V2. Moreover, this new training pipeline\nconsistently achieves better results on video recognition with significant\nspeedup. For instance, we achieve +0.6% top-1 of Slowfast on Kinetics-400 over\nthe strong 256-epoch 128-GPU baseline while fine-tuning for only 50 epochs with\n4 GPUs. The code and models are available at\nhttps://github.com/UCSC-VLAA/Image-Pretraining-for-Video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jieru Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"License Plate Privacy in Collaborative Visual Analysis of Traffic Scenes. (arXiv:2205.01724v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01724","description":"<p>Traffic scene analysis is important for emerging technologies such as smart\ntraffic management and autonomous vehicles. However, such analysis also poses\npotential privacy threats. For example, a system that can recognize license\nplates may construct patterns of behavior of the corresponding vehicles' owners\nand use that for various illegal purposes. In this paper we present a system\nthat enables traffic scene analysis while at the same time preserving license\nplate privacy. The system is based on a multi-task model whose latent space is\nselectively compressed depending on the amount of information the specific\nfeatures carry about analysis tasks and private information. Effectiveness of\nthe proposed method is illustrated by experiments on the Cityscapes dataset,\nfor which we also provide license plate annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alvar_S/0/1/0/all/0/1\">Saeed Ranjbar Alvar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyanik_K/0/1/0/all/0/1\">Korcan Uyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of belief functions to medical image segmentation: A review. (arXiv:2205.01733v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01733","description":"<p>Belief function theory, a formal framework for uncertainty analysis and\nmultiple evidence fusion, has made significant contributions in the medical\ndomain, especially since the development of deep learning. Medical image\nsegmentation with belief function theory has shown significant benefits in\nclinical diagnosis and medical image research. In this paper, we provide a\nreview of medical image segmentation methods using belief function theory. We\nclassify the methods according to the fusion step and explain how information\nwith uncertainty or imprecision is modeled and fused with belief function\ntheory. In addition, we discuss the challenges and limitations of present\nbelief function-based medical image segmentation and propose orientations for\nfuture research. Future research could investigate both belief function theory\nand deep learning to achieve more promising and reliable segmentation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of CoModGANs, LaMa and GLIDE for Art Inpainting- Completing M.C Escher's Print Gallery. (arXiv:2205.01741v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01741","description":"<p>Digital art restoration has benefited from inpainting models to correct the\ndegradation or missing sections of a painting. This work compares three current\nstate-of-the art models for inpainting of large missing regions. We provide\nqualitative and quantitative comparison of the performance by CoModGANs, LaMa\nand GLIDE in inpainting of blurry and missing sections of images. We use\nEscher's incomplete painting Print Gallery as our test study since it presents\nseveral of the challenges commonly present in restorative inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cipolina_Kun_L/0/1/0/all/0/1\">Lucia Cipolina-Kun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caenazzo_S/0/1/0/all/0/1\">Simone Caenazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzei_G/0/1/0/all/0/1\">Gaston Mazzei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Consistent Non-Cartesian Deep Subspace Learning for Efficient Dynamic MR Image Reconstruction. (arXiv:2205.01770v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01770","description":"<p>Non-Cartesian sampling with subspace-constrained image reconstruction is a\npopular approach to dynamic MRI, but slow iterative reconstruction limits its\nclinical application. Data-consistent (DC) deep learning can accelerate\nreconstruction with good image quality, but has not been formulated for\nnon-Cartesian subspace imaging. In this study, we propose a DC non-Cartesian\ndeep subspace learning framework for fast, accurate dynamic MR image\nreconstruction. Four novel DC formulations are developed and evaluated: two\ngradient decent approaches, a directly solved approach, and a conjugate\ngradient approach. We applied a U-Net model with and without DC layers to\nreconstruct T1-weighted images for cardiac MR Multitasking (an advanced\nmultidimensional imaging method), comparing our results to the iteratively\nreconstructed reference. Experimental results show that the proposed framework\nsignificantly improves reconstruction accuracy over the U-Net model without DC,\nwhile significantly accelerating the reconstruction over conventional iterative\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zihao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1\">Yibin Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1\">Debiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christodoulou_A/0/1/0/all/0/1\">Anthony G. Christodoulou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Os Dados dos Brasileiros sob Risco na Era da Intelig\\^encia Artificial?. (arXiv:2205.01772v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01772","description":"<p>Advances in image processing and analysis as well as machine learning\ntechniques have contributed to the use of biometric recognition systems in\ndaily people tasks. These tasks range from simple access to mobile devices to\ntagging friends in photos shared on social networks and complex financial\noperations on self-service devices for banking transactions. In China, the use\nof these systems goes beyond personal use becoming a country's government\npolicy with the objective of monitoring the behavior of its population. On July\n05th 2021, the Brazilian government announced acquisition of a biometric\nrecognition system to be used nationwide. In the opposite direction to China,\nEurope and some American cities have already started the discussion about the\nlegality of using biometric systems in public places, even banning this\npractice in their territory. In order to open a deeper discussion about the\nrisks and legality of using these systems, this work exposes the\nvulnerabilities of biometric recognition systems, focusing its efforts on the\nface modality. Furthermore, it shows how it is possible to fool a biometric\nsystem through a well-known presentation attack approach in the literature\ncalled morphing. Finally, a list of ten concerns was created to start the\ndiscussion about the security of citizen data and data privacy law in the Age\nof Artificial Intelligence (AI).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teixeira_R/0/1/0/all/0/1\">Raoni F. da S. Teixeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Januzi_R/0/1/0/all/0/1\">Rafael B. Januzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faria_F/0/1/0/all/0/1\">Fabio A. Faria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multi-Scale U-Net Architecture and Noise-Robust Training Strategies for Histopathological Image Segmentation. (arXiv:2205.01777v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01777","description":"<p>Although the U-Net architecture has been extensively used for segmentation of\nmedical images, we address two of its shortcomings in this work. Firstly, the\naccuracy of vanilla U-Net degrades when the target regions for segmentation\nexhibit significant variations in shape and size. Even though the U-Net already\npossesses some capability to analyze features at various scales, we propose to\nexplicitly add multi-scale feature maps in each convolutional module of the\nU-Net encoder to improve segmentation of histology images. Secondly, the\naccuracy of a U-Net model also suffers when the annotations for supervised\nlearning are noisy or incomplete. This can happen due to the inherent\ndifficulty for a human expert to identify and delineate all instances of\nspecific pathology very precisely and accurately. We address this challenge by\nintroducing auxiliary confidence maps that emphasize less on the boundaries of\nthe given target regions. Further, we utilize the bootstrapping properties of\nthe deep network to address the missing annotation problem intelligently. In\nour experiments on a private dataset of breast cancer lymph nodes, where the\nprimary task was to segment germinal centres and sinus histiocytosis, we\nobserved substantial improvement over a U-Net baseline based on the two\nproposed augmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kurian_N/0/1/0/all/0/1\">Nikhil Cherian Kurian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lohan_A/0/1/0/all/0/1\">Amit Lohan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verghese_G/0/1/0/all/0/1\">Gregory Verghese</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dharamshi_N/0/1/0/all/0/1\">Nimish Dharamshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meena_S/0/1/0/all/0/1\">Swati Meena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mengyuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fangfang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gillet_C/0/1/0/all/0/1\">Cheryl Gillet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rane_S/0/1/0/all/0/1\">Swapnil Rane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grigoriadis_A/0/1/0/all/0/1\">Anita Grigoriadis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition. (arXiv:2205.01782v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01782","description":"<p>The activations of Facial Action Units (AUs) mutually influence one another.\nWhile the relationship between a pair of AUs can be complex and unique,\nexisting approaches fail to specifically and explicitly represent such cues for\neach pair of AUs in each facial display. This paper proposes an AU relationship\nmodelling approach that deep learns a unique graph to explicitly describe the\nrelationship between each pair of AUs of the target facial display. Our\napproach first encodes each AU's activation status and its association with\nother AUs into a node feature. Then, it learns a pair of multi-dimensional edge\nfeatures to describe multiple task-specific relationship cues between each pair\nof AUs. During both node and edge feature learning, our approach also considers\nthe influence of the unique facial display on AUs' relationship by taking the\nfull face representation as an input. Experimental results on BP4D and DISFA\ndatasets show that both node and edge feature learning modules provide large\nperformance improvements for CNN and transformer-based backbones, with our best\nsystems achieving the state-of-the-art AU recognition results. Our approach not\nonly has a strong capability in modelling relationship cues for AU recognition\nbut also can be easily incorporated into various backbones. Our PyTorch code is\nmade available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Siyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weicheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1\">Hatice Gunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pre-study on Data Processing Pipelines for Roadside Object Detection Systems Towards Safer Road Infrastructure. (arXiv:2205.01783v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01783","description":"<p>Single-vehicle accidents are the most common type of fatal accidents in\nSweden, where a car drives off the road and runs into hazardous roadside\nobjects. Proper installation and maintenance of protective objects, such as\ncrash cushions and guard rails, may reduce the chance and severity of such\naccidents. Moreover, efficient detection and management of hazardous roadside\nobjects also plays an important role in improving road safety. To better\nunderstand the state-of-the-art and system requirements, in this pre-study, we\ninvestigate the feasibility, implementation, limitations and scaling up of data\nprocessing pipelines for roadside object detection. In particular, we divide\nour investigation into three parts: the target of interest, the sensors of\nchoice and the algorithm design. The data sources we consider in this study\ncover two common setups: 1) road surveying fleet - annual scans conducted by\nTrafikverket, the Swedish Transport Administration, and 2) consumer vehicle -\ndata collected using a research vehicle from the laboratory of Resource for\nvehicle research at Chalmers (REVERE). The goal of this report is to\ninvestigate how to implement a scalable roadside object detection system\ntowards safe road infrastructure and Sweden's Vision Zero.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yinan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheidegger_S/0/1/0/all/0/1\">Samuel Scheidegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gronvall_J/0/1/0/all/0/1\">John-Fredrik Gr&#xf6;nvall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palm_M/0/1/0/all/0/1\">Magnus Palm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svanberg_E/0/1/0/all/0/1\">Erik Svanberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wennerby_J/0/1/0/all/0/1\">Johan Amoruso Wennerby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakker_J/0/1/0/all/0/1\">J&#xf6;rg Bakker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesized Speech Detection Using Convolutional Transformer-Based Spectrogram Analysis. (arXiv:2205.01800v1 [cs.SD])","link":"http://arxiv.org/abs/2205.01800","description":"<p>Synthesized speech is common today due to the prevalence of virtual\nassistants, easy-to-use tools for generating and modifying speech signals, and\nremote work practices. Synthesized speech can also be used for nefarious\npurposes, including creating a purported speech signal and attributing it to\nsomeone who did not speak the content of the signal. We need methods to detect\nif a speech signal is synthesized. In this paper, we analyze speech signals in\nthe form of spectrograms with a Compact Convolutional Transformer (CCT) for\nsynthesized speech detection. A CCT utilizes a convolutional layer that\nintroduces inductive biases and shared weights into a network, allowing a\ntransformer architecture to perform well with fewer data samples used for\ntraining. The CCT uses an attention mechanism to incorporate information from\nall parts of a signal under analysis. Trained on both genuine human voice\nsignals and synthesized human voice signals, we demonstrate that our CCT\napproach successfully differentiates between genuine and synthesized speech\nsignals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_E/0/1/0/all/0/1\">Emily R. Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Splicing Detection and Localization In Satellite Imagery Using Conditional GANs. (arXiv:2205.01805v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01805","description":"<p>The widespread availability of image editing tools and improvements in image\nprocessing techniques allow image manipulation to be very easy. Oftentimes,\neasy-to-use yet sophisticated image manipulation tools yields\ndistortions/changes imperceptible to the human observer. Distribution of forged\nimages can have drastic ramifications, especially when coupled with the speed\nand vastness of the Internet. Therefore, verifying image integrity poses an\nimmense and important challenge to the digital forensic community. Satellite\nimages specifically can be modified in a number of ways, including the\ninsertion of objects to hide existing scenes and structures. In this paper, we\ndescribe the use of a Conditional Generative Adversarial Network (cGAN) to\nidentify the presence of such spliced forgeries within satellite images.\nAdditionally, we identify their locations and shapes. Trained on pristine and\nfalsified images, our method achieves high success on these detection and\nlocalization objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_E/0/1/0/all/0/1\">Emily R. Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarlagadda_S/0/1/0/all/0/1\">Sri Kalyan Yarlagadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guera_D/0/1/0/all/0/1\">David G&#xfc;era</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing M. Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency Domain-Based Detection of Generated Audio. (arXiv:2205.01806v1 [cs.SD])","link":"http://arxiv.org/abs/2205.01806","description":"<p>Attackers may manipulate audio with the intent of presenting falsified\nreports, changing an opinion of a public figure, and winning influence and\npower. The prevalence of inauthentic multimedia continues to rise, so it is\nimperative to develop a set of tools that determines the legitimacy of media.\nWe present a method that analyzes audio signals to determine whether they\ncontain real human voices or fake human voices (i.e., voices generated by\nneural acoustic and waveform models). Instead of analyzing the audio signals\ndirectly, the proposed approach converts the audio signals into spectrogram\nimages displaying frequency, intensity, and temporal content and evaluates them\nwith a Convolutional Neural Network (CNN). Trained on both genuine human voice\nsignals and synthesized voice signals, we show our approach achieves high\naccuracy on this classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_E/0/1/0/all/0/1\">Emily R. Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Dataset Bias in Computer Vision. (arXiv:2205.01811v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01811","description":"<p>A biased dataset is a dataset that generally has attributes with an uneven\nclass distribution. These biases have the tendency to propagate to the models\nthat train on them, often leading to a poor performance in the minority class.\nIn this project, we will explore the extent to which various data augmentation\nmethods alleviate intrinsic biases within the dataset. We will apply several\naugmentation techniques on a sample of the UTKFace dataset, such as\nundersampling, geometric transformations, variational autoencoders (VAEs), and\ngenerative adversarial networks (GANs). We then trained a classifier for each\nof the augmented datasets and evaluated their performance on the native test\nset and on external facial recognition datasets. We have also compared their\nperformance to the state-of-the-art attribute classifier trained on the\nFairFace dataset. Through experimentation, we were able to find that training\nthe model on StarGAN-generated images led to the best overall performance. We\nalso found that training on geometrically transformed images lead to a similar\nperformance with a much quicker training time. Additionally, the best\nperforming models also exhibit a uniform performance across the classes within\neach attribute. This signifies that the model was also able to mitigate the\nbiases present in the baseline model that was trained on the original training\nset. Finally, we were able to show that our model has a better overall\nperformance and consistency on age and ethnicity classification on multiple\ndatasets when compared with the FairFace model. Our final model has an accuracy\non the UTKFace test set of 91.75%, 91.30%, and 87.20% for the gender, age, and\nethnicity attribute respectively, with a standard deviation of less than 0.1\nbetween the accuracies of the classes of each attribute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deviyani_A/0/1/0/all/0/1\">Athiya Deviyani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Image Captioning with Grounded Style. (arXiv:2205.01813v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01813","description":"<p>Stylized image captioning as presented in prior work aims to generate\ncaptions that reflect characteristics beyond a factual description of the scene\ncomposition, such as sentiments. Such prior work relies on given sentiment\nidentifiers, which are used to express a certain global style in the caption,\ne.g. positive or negative, however without taking into account the stylistic\ncontent of the visual scene. To address this shortcoming, we first analyze the\nlimitations of current stylized captioning datasets and propose COCO\nattribute-based augmentations to obtain varied stylized captions from COCO\nannotations. Furthermore, we encode the stylized information in the latent\nspace of a Variational Autoencoder; specifically, we leverage extracted image\nattributes to explicitly structure its sequential latent space according to\ndifferent localized style characteristics. Our experiments on the Senticap and\nCOCO datasets show the ability of our approach to generate accurate captions\nwith diversity in styles that are grounded in the image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klein_F/0/1/0/all/0/1\">Franz Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1\">Shweta Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1\">Stefan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"i-Code: An Integrative and Composable Multimodal Learning Framework. (arXiv:2205.01818v1 [cs.LG])","link":"http://arxiv.org/abs/2205.01818","description":"<p>Human intelligence is multimodal; we integrate visual, linguistic, and\nacoustic signals to maintain a holistic worldview. Most current pretraining\nmethods, however, are limited to one or two modalities. We present i-Code, a\nself-supervised pretraining framework where users may flexibly combine the\nmodalities of vision, speech, and language into unified and general-purpose\nvector representations. In this framework, data from each modality are first\ngiven to pretrained single-modality encoders. The encoder outputs are then\nintegrated with a multimodal fusion network, which uses novel attention\nmechanisms and other architectural innovations to effectively combine\ninformation from the different modalities. The entire system is pretrained\nend-to-end with new objectives including masked modality unit modeling and\ncross-modality contrastive learning. Unlike previous research using only video\nfor pretraining, the i-Code framework can dynamically process single, dual, and\ntriple-modality data during training and inference, flexibly projecting\ndifferent combinations of modalities into a single representation space.\nExperimental results demonstrate how i-Code can outperform state-of-the-art\ntechniques on five video understanding tasks and the GLUE NLP benchmark,\nimproving by as much as 11% and demonstrating the power of integrative\nmultimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gmyr_R/0/1/0/all/0/1\">Robert Gmyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuedong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedMix: Mixed Supervised Federated Learning for Medical Image Segmentation. (arXiv:2205.01840v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01840","description":"<p>The purpose of federated learning is to enable multiple clients to jointly\ntrain a machine learning model without sharing data. However, the existing\nmethods for training an image segmentation model have been based on an\nunrealistic assumption that the training set for each local client is annotated\nin a similar fashion and thus follows the same image supervision level. To\nrelax this assumption, in this work, we propose a label-agnostic unified\nfederated learning framework, named FedMix, for medical image segmentation\nbased on mixed image labels. In FedMix, each client updates the federated model\nby integrating and effectively making use of all available labeled data ranging\nfrom strong pixel-level labels, weak bounding box labels, to weakest\nimage-level class labels. Based on these local models, we further propose an\nadaptive weight assignment procedure across local clients, where each client\nlearns an aggregation weight during the global model update. Compared to the\nexisting methods, FedMix not only breaks through the constraint of a single\nlevel of image supervision, but also can dynamically adjust the aggregation\nweight of each local client, achieving rich yet discriminative feature\nrepresentations. To evaluate its effectiveness, experiments have been carried\nout on two challenging medical image segmentation tasks, i.e., breast tumor\nsegmentation and skin lesion segmentation. The results validate that our\nproposed FedMix outperforms the state-of-the-art method by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wicaksana_J/0/1/0/all/0/1\">Jeffry Wicaksana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huimin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Commonsense in Pretrained Unimodal and Multimodal Models. (arXiv:2205.01850v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01850","description":"<p>Our commonsense knowledge about objects includes their typical visual\nattributes; we know that bananas are typically yellow or green, and not purple.\nText and image corpora, being subject to reporting bias, represent this\nworld-knowledge to varying degrees of faithfulness. In this paper, we\ninvestigate to what degree unimodal (language-only) and multimodal (image and\nlanguage) models capture a broad range of visually salient attributes. To that\nend, we create the Visual Commonsense Tests (ViComTe) dataset covering 5\nproperty types (color, shape, material, size, and visual co-occurrence) for\nover 5000 subjects. We validate this dataset by showing that our grounded color\ndata correlates much better than ungrounded text-only data with crowdsourced\ncolor judgments provided by Paik et al. (2021). We then use our dataset to\nevaluate pretrained unimodal models and multimodal models. Our results indicate\nthat multimodal models better reconstruct attribute distributions, but are\nstill subject to reporting bias. Moreover, increasing model size does not\nenhance performance, suggesting that the key to visual commonsense lies in the\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCL-Dehaze: Towards Real-world Image Dehazing via Unsupervised Contrastive Learning. (arXiv:2205.01871v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01871","description":"<p>While the wisdom of training an image dehazing model on synthetic hazy data\ncan alleviate the difficulty of collecting real-world hazy/clean image pairs,\nit brings the well-known domain shift problem. From a different yet new\nperspective, this paper explores contrastive learning with an adversarial\ntraining effort to leverage unpaired real-world hazy and clean images, thus\nbridging the gap between synthetic and real-world haze is avoided. We propose\nan effective unsupervised contrastive learning paradigm for image dehazing,\ndubbed UCL-Dehaze. Unpaired real-world clean and hazy images are easily\ncaptured, and will serve as the important positive and negative samples\nrespectively when training our UCL-Dehaze network. To train the network more\neffectively, we formulate a new self-contrastive perceptual loss function,\nwhich encourages the restored images to approach the positive samples and keep\naway from the negative samples in the embedding space. Besides the overall\nnetwork architecture of UCL-Dehaze, adversarial training is utilized to align\nthe distributions between the positive samples and the dehazed images. Compared\nwith recent image dehazing works, UCL-Dehaze does not require paired data\nduring training and utilizes unpaired positive/negative data to better enhance\nthe dehazing performance. We conduct comprehensive experiments to evaluate our\nUCL-Dehaze and demonstrate its superiority over the state-of-the-arts, even\nonly 1,800 unpaired real-world images are used to train our network. Source\ncode has been available at https://github.com/yz-wang/UCL-Dehaze.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xuefeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fu Lee Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Image Compression and Denoising via Latent-Space Scalability. (arXiv:2205.01874v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01874","description":"<p>When it comes to image compression in digital cameras, denoising is\ntraditionally performed prior to compression. However, there are applications\nwhere image noise may be necessary to demonstrate the trustworthiness of the\nimage, such as court evidence and image forensics. This means that noise itself\nneeds to be coded, in addition to the clean image itself. In this paper, we\npresent a learnt image compression framework where image denoising and\ncompression are performed jointly. The latent space of the image codec is\norganized in a scalable manner such that the clean image can be decoded from a\nsubset of the latent space at a lower rate, while the noisy image is decoded\nfrom the full latent space at a higher rate. The proposed codec is compared\nagainst established compression and denoising benchmarks, and the experiments\nreveal considerable bitrate savings of up to 80% compared to cascade\ncompression and denoising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alvar_S/0/1/0/all/0/1\">Saeed Ranjbar Alvar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ulhaq_M/0/1/0/all/0/1\">Mateen Ulhaq</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_H/0/1/0/all/0/1\">Hyomin Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You May Need for VQA are Image Captions. (arXiv:2205.01883v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01883","description":"<p>Visual Question Answering (VQA) has benefited from increasingly sophisticated\nmodels, but has not enjoyed the same level of engagement in terms of data\ncreation. In this paper, we propose a method that automatically derives VQA\nexamples at volume, by leveraging the abundance of existing image-caption\nannotations combined with neural models for textual question generation. We\nshow that the resulting data is of high-quality. VQA models trained on our data\nimprove state-of-the-art zero-shot accuracy by double digits and achieve a\nlevel of robustness that lacks in the same model trained on human-annotated VQA\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukliansky_D/0/1/0/all/0/1\">Doron Kukliansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation Learning for Hierarchical Infant Pose Recognition with Synthetic Data. (arXiv:2205.01892v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01892","description":"<p>The Alberta Infant Motor Scale (AIMS) is a well-known assessment scheme that\nevaluates the gross motor development of infants by recording the number of\nspecific poses achieved. With the aid of the image-based pose recognition\nmodel, the AIMS evaluation procedure can be shortened and automated, providing\nearly diagnosis or indicator of potential developmental disorder. Due to\nlimited public infant-related datasets, many works use the SMIL-based method to\ngenerate synthetic infant images for training. However, this domain mismatch\nbetween real and synthetic training samples often leads to performance\ndegradation during inference. In this paper, we present a CNN-based model which\ntakes any infant image as input and predicts the coarse and fine-level pose\nlabels. The model consists of an image branch and a pose branch, which\nrespectively generates the coarse-level logits facilitated by the unsupervised\ndomain adaptation and the 3D keypoints using the HRNet with SMPLify\noptimization. Then the outputs of these branches will be sent into the\nhierarchical pose recognition module to estimate the fine-level pose labels. We\nalso collect and label a new AIMS dataset, which contains 750 real and 4000\nsynthetic infants images with AIMS pose labels. Our experimental results show\nthat the proposed method can significantly align the distribution of synthetic\nand real-world datasets, thus achieving accurate performance on fine-grained\ninfant pose recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shih-Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jang-Hee Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pik-Fix: Restoring and Colorizing Old Photo. (arXiv:2205.01902v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01902","description":"<p>Restoring and inpainting the visual memories that are present, but often\nimpaired, in old photos remains an intriguing but unsolved research topic.\nDecades-old photos often suffer from severe and commingled degradation such as\ncracks, defocus, and color-fading, which are difficult to treat individually\nand harder to repair when they interact. Deep learning presents a plausible\navenue, but the lack of large-scale datasets of old photos makes addressing\nthis restoration task very challenging. Here we present a novel reference-based\nend-to-end learning framework that is able to both repair and colorize old and\ndegraded pictures. Our proposed framework consists of three modules: a\nrestoration sub-network that conducts restoration from degradations, a\nsimilarity sub-network that performs color histogram matching and color\ntransfer, and a colorization subnet that learns to predict the chroma elements\nof images that have been conditioned on chromatic reference signals. The\noverall system makes uses of color histogram priors from reference images,\nwhich greatly reduces the need for large-scale training data. We have also\ncreated a first-of-a-kind public dataset of real old photos that are paired\nwith ground truth \"pristine\" photos that have been that have been manually\nrestored by PhotoShop experts. We conducted extensive experiments on this\ndataset and synthetic datasets, and found that our method significantly\noutperforms previous state-of-the-art models using both qualitative comparisons\nand quantitative measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zibo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Taught Metric Learning without Labels. (arXiv:2205.01903v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01903","description":"<p>We present a novel self-taught framework for unsupervised metric learning,\nwhich alternates between predicting class-equivalence relations between data\nthrough a moving average of an embedding model and learning the model with the\npredicted relations as pseudo labels. At the heart of our framework lies an\nalgorithm that investigates contexts of data on the embedding space to predict\ntheir class-equivalence relations as pseudo labels. The algorithm enables\nefficient end-to-end training since it demands no off-the-shelf module for\npseudo labeling. Also, the class-equivalence relations provide rich supervisory\nsignals for learning an embedding space. On standard benchmarks for metric\nlearning, it clearly outperforms existing unsupervised learning methods and\nsometimes even beats supervised learning models using the same backbone\nnetwork. It is also applied to semi-supervised metric learning as a way of\nexploiting additional unlabeled data, and achieves the state of the art by\nboosting performance of supervised learning substantially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Knowledge Distillation via Relationship Matching. (arXiv:2205.01915v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01915","description":"<p>The knowledge of a well-trained deep neural network (a.k.a. the \"teacher\") is\nvaluable for learning similar tasks. Knowledge distillation extracts knowledge\nfrom the teacher and integrates it with the target model (a.k.a. the\n\"student\"), which expands the student's knowledge and improves its learning\nefficacy. Instead of enforcing the teacher to work on the same task as the\nstudent, we borrow the knowledge from a teacher trained from a general label\nspace -- in this \"Generalized Knowledge Distillation (GKD)\", the classes of the\nteacher and the student may be the same, completely different, or partially\noverlapped. We claim that the comparison ability between instances acts as an\nessential factor threading knowledge across tasks, and propose the RElationship\nFacIlitated Local cLassifiEr Distillation (REFILLED) approach, which decouples\nthe GKD flow of the embedding and the top-layer classifier. In particular,\ndifferent from reconciling the instance-label confidence between models,\nREFILLED requires the teacher to reweight the hard tuples pushed forward by the\nstudent and then matches the similarity comparison levels between instances. An\nembedding-induced classifier based on the teacher model supervises the\nstudent's classification confidence and adaptively emphasizes the most related\nsupervision from the teacher. REFILLED demonstrates strong discriminative\nability when the classes of the teacher vary from the same to a fully\nnon-overlapped set w.r.t. the student. It also achieves state-of-the-art\nperformance on standard knowledge distillation, one-step incremental learning,\nand few-shot learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Su Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCa: Contrastive Captioners are Image-Text Foundation Models. (arXiv:2205.01917v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01917","description":"<p>Exploring large-scale pretrained foundation models is of significant interest\nin computer vision because these models can be quickly transferred to many\ndownstream tasks. This paper presents Contrastive Captioner (CoCa), a\nminimalist design to pretrain an image-text encoder-decoder foundation model\njointly with contrastive loss and captioning loss, thereby subsuming model\ncapabilities from contrastive approaches like CLIP and generative methods like\nSimVLM. In contrast to standard encoder-decoder transformers where all decoder\nlayers attend to encoder outputs, CoCa omits cross-attention in the first half\nof decoder layers to encode unimodal text representations, and cascades the\nremaining decoder layers which cross-attend to the image encoder for multimodal\nimage-text representations. We apply a contrastive loss between unimodal image\nand text embeddings, in addition to a captioning loss on the multimodal decoder\noutputs which predicts text tokens autoregressively. By sharing the same\ncomputational graph, the two training objectives are computed efficiently with\nminimal overhead. CoCa is pretrained end-to-end and from scratch on both\nweb-scale alt-text data and annotated images by treating all labels simply as\ntext, seamlessly unifying natural language supervision for representation\nlearning. Empirically, CoCa achieves state-of-the-art performance with\nzero-shot transfer or minimal task-specific adaptation on a broad range of\ndownstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700,\nMoments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal\nunderstanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps).\nNotably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1\naccuracy, 90.6% with a frozen encoder and learned classification head, and new\nstate-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_L/0/1/0/all/0/1\">Legg Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyedhosseini_M/0/1/0/all/0/1\">Mojtaba Seyedhosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Clustering Based Pseudo-labeling Strategy for Multi-modal Aerial View Object Classification. (arXiv:2205.01920v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01920","description":"<p>Multi-modal aerial view object classification (MAVOC) in Automatic target\nrecognition (ATR), although an important and challenging problem, has been\nunder studied. This paper firstly finds that fine-grained data, class imbalance\nand various shooting conditions preclude the representational ability of\ngeneral image classification. Moreover, the MAVOC dataset has scene aggregation\ncharacteristics. By exploiting these properties, we propose Scene Clustering\nBased Pseudo-labeling Strategy (SCP-Label), a simple yet effective method to\nemploy in post-processing. The SCP-Label brings greater accuracy by assigning\nthe same label to objects within the same scene while also mitigating bias and\nconfusion with model ensembles. Its performance surpasses the official baseline\nby a large margin of +20.57% Accuracy on Track 1 (SAR), and +31.86% Accuracy on\nTrack 2 (SAR+EO), demonstrating the potential of SCP-Label as post-processing.\nFinally, we win the championship both on Track1 and Track2 in the CVPR 2022\nPerception Beyond the Visible Spectrum (PBVS) Workshop MAVOC Challenge. Our\ncode is available at https://github.com/HowieChangchn/SCP-Label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keda Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shenshen Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Episode Few-Shot Contrastive Predictive Coding: Solving intelligence tests without prior training. (arXiv:2205.01924v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01924","description":"<p>Video prediction models often combine three components: an encoder from pixel\nspace to a small latent space, a latent space prediction model, and a\ngenerative model back to pixel space. However, the large and unpredictable\npixel space makes training such models difficult, requiring many training\nexamples. We argue that finding a predictive latent variable and using it to\nevaluate the consistency of a future image enables data-efficient predictions\nbecause it precludes the necessity of a generative model training. To\ndemonstrate it, we created sequence completion intelligence tests in which the\ntask is to identify a predictably changing feature in a sequence of images and\nuse this prediction to select the subsequent image. We show that a\none-dimensional Markov Contrastive Predictive Coding (M-CPC_1D) model solves\nthese tests efficiently, with only five examples. Finally, we demonstrate the\nusefulness of M-CPC_1D in solving two tasks without prior training: anomaly\ndetection and stochastic movement video prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barak_T/0/1/0/all/0/1\">T. Barak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loewenstein_Y/0/1/0/all/0/1\">Y. Loewenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised learning unveils morphological clusters behind lung cancer types and prognosis. (arXiv:2205.01931v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01931","description":"<p>Histopathological images of tumors contain abundant information about how\ntumors grow and how they interact with their micro-environment. Characterizing\nand improving our understanding of phenotypes could reveal factors related to\ntumor progression and their underpinning biological processes, ultimately\nimproving diagnosis and treatment. In recent years, the field of histological\ndeep learning applications has seen great progress, yet most of these\napplications focus on a supervised approach, relating tissue and associated\nsample annotations. Supervised approaches have their impact limited by two\nfactors. Firstly, high-quality labels are expensive in time and effort, which\nmakes them not easily scalable. Secondly, these methods focus on predicting\nannotations from histological images, fundamentally restricting the discovery\nof new tissue phenotypes. These limitations emphasize the importance of using\nnew methods that can characterize tissue by the features enclosed in the image,\nwithout pre-defined annotation or supervision. We present Phenotype\nRepresentation Learning (PRL), a methodology to extract histomorphological\nphenotypes through self-supervised learning and community detection. PRL\ncreates phenotype clusters by identifying tissue patterns that share common\nmorphological and cellular features, allowing to describe whole slide images\nthrough compositional representations of cluster contributions. We used this\nframework to analyze histopathology slides of LUAD and LUSC lung cancer\nsubtypes from TCGA and NYU cohorts. We show that PRL achieves a robust lung\nsubtype prediction providing statistically relevant phenotypes for each lung\nsubtype. We further demonstrate the significance of these phenotypes in lung\nadenocarcinoma overall and recurrence free survival, relating clusters with\npatient outcomes, cell types, grown patterns, and omic-based immune signatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1\">Adalberto Claudio Quiros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1\">Nicolas Coudray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1\">Anna Yeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiriboga_L/0/1/0/all/0/1\">Luis Chiriboga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimkhan_A/0/1/0/all/0/1\">Afreen Karimkhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narula_N/0/1/0/all/0/1\">Navneet Narula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pass_H/0/1/0/all/0/1\">Harvey Pass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_A/0/1/0/all/0/1\">Andre L. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quesne_J/0/1/0/all/0/1\">John Le Quesne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1\">Aristotelis Tsirigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Ke Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homography-Based Loss Function for Camera Pose Regression. (arXiv:2205.01937v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01937","description":"<p>Some recent visual-based relocalization algorithms rely on deep learning\nmethods to perform camera pose regression from image data. This paper focuses\non the loss functions that embed the error between two poses to perform deep\nlearning based camera pose regression. Existing loss functions are either\ndifficult-to-tune multi-objective functions or present unstable reprojection\nerrors that rely on ground truth 3D scene points and require a two-step\ntraining. To deal with these issues, we introduce a novel loss function which\nis based on a multiplane homography integration. This new function does not\nrequire prior initialization and only depends on physically interpretable\nhyperparameters. Furthermore, the experiments carried out on well established\nrelocalization datasets show that it minimizes best the mean square\nreprojection error during training when compared with existing loss functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boittiaux_C/0/1/0/all/0/1\">Cl&#xe9;mentin Boittiaux</a> (IFREMER), <a href=\"http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1\">Ricard Marxer</a> (LIS), <a href=\"http://arxiv.org/find/cs/1/au:+Dune_C/0/1/0/all/0/1\">Claire Dune</a> (COSMER), <a href=\"http://arxiv.org/find/cs/1/au:+Arnaubec_A/0/1/0/all/0/1\">Aur&#xe9;lien Arnaubec</a> (IFREMER), <a href=\"http://arxiv.org/find/cs/1/au:+Hugel_V/0/1/0/all/0/1\">Vincent Hugel</a> (COSMER)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EllSeg-Gen, towards Domain Generalization for head-mounted eyetracking. (arXiv:2205.01947v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01947","description":"<p>The study of human gaze behavior in natural contexts requires algorithms for\ngaze estimation that are robust to a wide range of imaging conditions. However,\nalgorithms often fail to identify features such as the iris and pupil centroid\nin the presence of reflective artifacts and occlusions. Previous work has shown\nthat convolutional networks excel at extracting gaze features despite the\npresence of such artifacts. However, these networks often perform poorly on\ndata unseen during training. This work follows the intuition that jointly\ntraining a convolutional network with multiple datasets learns a generalized\nrepresentation of eye parts. We compare the performance of a single model\ntrained with multiple datasets against a pool of models trained on individual\ndatasets. Results indicate that models tested on datasets in which eye images\nexhibit higher appearance variability benefit from multiset training. In\ncontrast, dataset-specific models generalize better onto eye images with lower\nappearance variability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Rakshit S. Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_R/0/1/0/all/0/1\">Reynold J. Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelz_J/0/1/0/all/0/1\">Jeff B. Pelz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_G/0/1/0/all/0/1\">Gabriel J. Diaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequencer: Deep LSTM for Image Classification. (arXiv:2205.01972v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01972","description":"<p>In recent computer vision research, the advent of the Vision Transformer\n(ViT) has rapidly revolutionized various architectural design efforts: ViT\nachieved state-of-the-art image classification performance using self-attention\nfound in natural language processing, and MLP-Mixer achieved competitive\nperformance using simple multi-layer perceptrons. In contrast, several studies\nhave also suggested that carefully redesigned convolutional neural networks\n(CNNs) can achieve advanced performance comparable to ViT without resorting to\nthese new ideas. Against this background, there is growing interest in what\ninductive bias is suitable for computer vision. Here we propose Sequencer, a\nnovel and competitive architecture alternative to ViT that provides a new\nperspective on these issues. Unlike ViTs, Sequencer models long-range\ndependencies using LSTMs rather than self-attention layers. We also propose a\ntwo-dimensional version of Sequencer module, where an LSTM is decomposed into\nvertical and horizontal LSTMs to enhance performance. Despite its simplicity,\nseveral experiments demonstrate that Sequencer performs impressively well:\nSequencer2D-L, with 54M parameters, realizes 84.6\\% top-1 accuracy on only\nImageNet-1K. Not only that, we show that it has good transferability and the\nrobust resolution adaptability on double resolution-band.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1\">Yuki Tatsunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-Claims: A Dataset for Multimodal Claim Detection in Social Media. (arXiv:2205.01989v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01989","description":"<p>In recent years, the problem of misinformation on the web has become\nwidespread across languages, countries, and various social media platforms.\nAlthough there has been much work on automated fake news detection, the role of\nimages and their variety are not well explored. In this paper, we investigate\nthe roles of image and text at an earlier stage of the fake news detection\npipeline, called claim detection. For this purpose, we introduce a novel\ndataset, MM-Claims, which consists of tweets and corresponding images over\nthree topics: COVID-19, Climate Change and broadly Technology. The dataset\ncontains roughly 86000 tweets, out of which 3400 are labeled manually by\nmultiple annotators for the training and evaluation of multimodal models. We\ndescribe the dataset in detail, evaluate strong unimodal and multimodal\nbaselines, and analyze the potential and drawbacks of current models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1\">Gullal S. Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sittar_A/0/1/0/all/0/1\">Abdul Sittar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1\">Eric M&#xfc;ller-Budack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otto_C/0/1/0/all/0/1\">Christian Otto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of a DCT-driven Loss in Attention-based Knowledge-Distillation for Scene Recognition. (arXiv:2205.01997v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01997","description":"<p>Knowledge Distillation (KD) is a strategy for the definition of a set of\ntransferability gangways to improve the efficiency of Convolutional Neural\nNetworks. Feature-based Knowledge Distillation is a subfield of KD that relies\non intermediate network representations, either unaltered or depth-reduced via\nmaximum activation maps, as the source knowledge. In this paper, we propose and\nanalyse the use of a 2D frequency transform of the activation maps before\ntransferring them. We pose that\\textemdash by using global image cues rather\nthan pixel estimates, this strategy enhances knowledge transferability in tasks\nsuch as scene recognition, defined by strong spatial and contextual\nrelationships between multiple and varied concepts. To validate the proposed\nmethod, an extensive evaluation of the state-of-the-art in scene recognition is\npresented. Experimental results provide strong evidences that the proposed\nstrategy enables the student network to better focus on the relevant image\nareas learnt by the teacher network, hence leading to better descriptive\nfeatures and higher transferred performance than every other state-of-the-art\nalternative. We publicly release the training and evaluation framework used\nalong this paper at\n<a href=\"http://www-vpu.eps.uam.es/publications/DCTBasedKDForSceneRecognition.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Cifuentes_A/0/1/0/all/0/1\">Alejandro L&#xf3;pez-Cifuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escudero_Vinolo_M/0/1/0/all/0/1\">Marcos Escudero-Vi&#xf1;olo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bescos_J/0/1/0/all/0/1\">Jes&#xfa;s Besc&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+SanMiguel_J/0/1/0/all/0/1\">Juan C. SanMiguel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition. (arXiv:2205.02028v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02028","description":"<p>Recognizing transformation types applied to a video clip (RecogTrans) is a\nlong-established paradigm for self-supervised video representation learning,\nwhich achieves much inferior performance compared to instance discrimination\napproaches (InstDisc) in recent works. However, based on a thorough comparison\nof representative RecogTrans and InstDisc methods, we observe the great\npotential of RecogTrans on both semantic-related and temporal-related\ndownstream tasks. Based on hard-label classification, existing RecogTrans\napproaches suffer from noisy supervision signals in pre-training. To mitigate\nthis problem, we developed TransRank, a unified framework for recognizing\nTransformations in a Ranking formulation. TransRank provides accurate\nsupervision signals by recognizing transformations relatively, consistently\noutperforming the classification-based formulation. Meanwhile, the unified\nframework can be instantiated with an arbitrary set of temporal or spatial\ntransformations, demonstrating good generality. With a ranking-based\nformulation and several empirical practices, we achieve competitive performance\non video retrieval and action recognition. Under the same setting, TransRank\nsurpasses the previous state-of-the-art method by 6.4% on UCF101 and 8.3% on\nHMDB51 for action recognition (Top1 Acc); improves video retrieval on UCF101 by\n20.4% (R@1). The promising results validate that RecogTrans is still a worth\nexploring paradigm for video self-supervised learning. Codes will be released\nat https://github.com/kennymckormick/TransRank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haodong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Nanxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Super-Resolution for Multi-Exposure Push-Frame Satellites. (arXiv:2205.02031v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02031","description":"<p>Modern Earth observation satellites capture multi-exposure bursts of\npush-frame images that can be super-resolved via computational means. In this\nwork, we propose a super-resolution method for such multi-exposure sequences, a\nproblem that has received very little attention in the literature. The proposed\nmethod can handle the signal-dependent noise in the inputs, process sequences\nof any length, and be robust to inaccuracies in the exposure times.\nFurthermore, it can be trained end-to-end with self-supervision, without\nrequiring ground truth high resolution frames, which makes it especially suited\nto handle real data. Central to our method are three key contributions: i) a\nbase-detail decomposition for handling errors in the exposure times, ii) a\nnoise-level-aware feature encoding for improved fusion of frames with varying\nsignal-to-noise ratio and iii) a permutation invariant fusion strategy by\ntemporal pooling operators. We evaluate the proposed method on synthetic and\nreal data and show that it outperforms by a significant margin existing\nsingle-exposure approaches that we adapted to the multi-exposure case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Long Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anger_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Anger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davy_A/0/1/0/all/0/1\">Axel Davy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_P/0/1/0/all/0/1\">Pablo Arias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1\">Gabriele Facciolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images. (arXiv:2205.02049v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02049","description":"<p>Self-Supervised learning (SSL) has become the new state-of-art in several\ndomain classification and segmentation tasks. Of these, one popular category in\nSSL is distillation networks such as BYOL. This work proposes RSDnet, which\napplies the distillation network (BYOL) in the remote sensing (RS) domain where\ndata is non-trivially different from natural RGB images. Since Multi-spectral\n(MS) and synthetic aperture radar (SAR) sensors provide varied spectral and\nspatial resolution information, we utilised them as an implicit augmentation to\nlearn invariant feature embeddings. In order to learn RS based invariant\nfeatures with SSL, we trained RSDnet in two ways, i.e., single channel feature\nlearning and three channel feature learning. This work explores the usefulness\nof single channel feature learning from random MS and SAR bands compared to the\ncommon notion of using three or more bands. In our linear evaluation, these\nsingle channel features reached a 0.92 F1 score on the EuroSAT classification\ntask and 59.6 mIoU on the DFC segmentation task for certain single bands. We\nalso compared our results with ImageNet weights and showed that the RS based\nSSL model outperforms the supervised ImageNet based model. We further explored\nthe usefulness of multi-modal data compared to single modality data, and it is\nshown that utilising MS and SAR data learn better invariant representations\nthan utilising only MS data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Pallavi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoen_Phelan_B/0/1/0/all/0/1\">Bianca Schoen-Phelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_R/0/1/0/all/0/1\">Robert Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVTS: Scalable Video-to-Speech Synthesis. (arXiv:2205.02058v1 [cs.SD])","link":"http://arxiv.org/abs/2205.02058","description":"<p>Video-to-speech synthesis (also known as lip-to-speech) refers to the\ntranslation of silent lip movements into the corresponding audio. This task has\nreceived an increasing amount of attention due to its self-supervised nature\n(i.e., can be trained without manual labelling) combined with the ever-growing\ncollection of audio-visual data available online. Despite these strong\nmotivations, contemporary video-to-speech works focus mainly on small- to\nmedium-sized corpora with substantial constraints in both vocabulary and\nsetting. In this work, we introduce a scalable video-to-speech framework\nconsisting of two components: a video-to-spectrogram predictor and a\npre-trained neural vocoder, which converts the mel-frequency spectrograms into\nwaveform audio. We achieve state-of-the art results for GRID and considerably\noutperform previous approaches on LRW. More importantly, by focusing on\nspectrogram prediction using a simple feedforward model, we can efficiently and\neffectively scale our method to very large and unconstrained datasets: To the\nbest of our knowledge, we are the first to show intelligible results on the\nchallenging LRS3 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mira_R/0/1/0/all/0/1\">Rodrigo Mira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haliassos_A/0/1/0/all/0/1\">Alexandros Haliassos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation. (arXiv:2205.02065v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02065","description":"<p>Spacecraft pose estimation is an essential computer vision application that\ncan improve the autonomy of in-orbit operations. An ESA/Stanford competition\nbrought out solutions that seem hardly compatible with the constraints imposed\non spacecraft onboard computers. URSONet is among the best in the competition\nfor its generalization capabilities but at the cost of a tremendous number of\nparameters and high computational complexity. In this paper, we propose\nMobile-URSONet: a spacecraft pose estimation convolutional neural network with\n178 times fewer parameters while degrading accuracy by no more than four times\ncompared to URSONet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Posso_J/0/1/0/all/0/1\">Julien Posso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bois_G/0/1/0/all/0/1\">Guy Bois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savaria_Y/0/1/0/all/0/1\">Yvon Savaria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Branch Neural Network for Sea Fog Detection in Geostationary Ocean Color Imager. (arXiv:2205.02069v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02069","description":"<p>Sea fog significantly threatens the safety of maritime activities. This paper\ndevelops a sea fog dataset (SFDD) and a dual branch sea fog detection network\n(DB-SFNet). We investigate all the observed sea fog events in the Yellow Sea\nand the Bohai Sea (118.1{\\deg}E-128.1{\\deg}E, 29.5{\\deg}N-43.8{\\deg}N) from\n2010 to 2020, and collect the sea fog images for each event from the\nGeostationary Ocean Color Imager (GOCI) to comprise the dataset SFDD. The\nlocation of the sea fog in each image in SFDD is accurately marked. The\nproposed dataset is characterized by a long-time span, large number of samples,\nand accurate labeling, that can substantially improve the robustness of various\nsea fog detection models. Furthermore, this paper proposes a dual branch sea\nfog detection network to achieve accurate and holistic sea fog detection. The\npoporsed DB-SFNet is composed of a knowledge extraction module and a dual\nbranch optional encoding decoding module. The two modules jointly extracts\ndiscriminative features from both visual and statistical domain. Experiments\nshow promising sea fog detection results with an F1-score of 0.77 and a\ncritical success index of 0.63. Compared with existing advanced deep learning\nnetworks, DB-SFNet is superior in detection performance and stability,\nparticularly in the mixed cloud and fog areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaofeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPortraitDrawing: Generating Human Body Images from Freehand Sketches. (arXiv:2205.02070v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02070","description":"<p>Researchers have explored various ways to generate realistic images from\nfreehand sketches, e.g., for objects and human faces. However, how to generate\nrealistic human body images from sketches is still a challenging problem. It\nis, first because of the sensitivity to human shapes, second because of the\ncomplexity of human images caused by body shape and pose changes, and third\nbecause of the domain gap between realistic images and freehand sketches. In\nthis work, we present DeepPortraitDrawing, a deep generative framework for\nconverting roughly drawn sketches to realistic human body images. To encode\ncomplicated body shapes under various poses, we take a local-to-global\napproach. Locally, we employ semantic part auto-encoders to construct\npart-level shape spaces, which are useful for refining the geometry of an input\npre-segmented hand-drawn sketch. Globally, we employ a cascaded spatial\ntransformer network to refine the structure of body parts by adjusting their\nspatial locations and relative proportions. Finally, we use a global synthesis\nnetwork for the sketch-to-image translation task, and a face refinement network\nto enhance facial details. Extensive experiments have shown that given roughly\nsketched human portraits, our method produces more realistic images than the\nstate-of-the-art sketch-to-image synthesis techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANUBIS: Review and Benchmark Skeleton-Based Action Recognition Methods with a New Dataset. (arXiv:2205.02071v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02071","description":"<p>Skeleton-based action recognition, as a subarea of action recognition, is\nswiftly accumulating attention and popularity. The task is to recognize actions\nperformed by human articulation points. Compared with other data modalities, 3D\nhuman skeleton representations have extensive unique desirable characteristics,\nincluding succinctness, robustness, racial-impartiality, and many more. We aim\nto provide a roadmap for new and existing researchers a on the landscapes of\nskeleton-based action recognition for new and existing researchers. To this\nend, we present a review in the form of a taxonomy on existing works of\nskeleton-based action recognition. We partition them into four major\ncategories: (1) datasets; (2) extracting spatial features; (3) capturing\ntemporal patterns; (4) improving signal quality. For each method, we provide\nconcise yet informatively-sufficient descriptions. To promote more fair and\ncomprehensive evaluation on existing approaches of skeleton-based action\nrecognition, we collect ANUBIS, a large-scale human skeleton dataset. Compared\nwith previously collected dataset, ANUBIS are advantageous in the following\nfour aspects: (1) employing more recently released sensors; (2) containing\nnovel back view; (3) encouraging high enthusiasm of subjects; (4) including\nactions of the COVID pandemic era. Using ANUBIS, we comparably benchmark\nperformance of current skeleton-based action recognizers. At the end of this\npaper, we outlook future development of skeleton-based action recognition by\nlisting several new technical problems. We believe they are valuable to solve\nin order to commercialize skeleton-based action recognition in the near future.\nThe dataset of ANUBIS is available at:\n<a href=\"http://hcc-workshop.anu.edu.au/webs/anu101/home.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Madhawa Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDF-based RGB-D Camera Tracking in Neural Scene Representations. (arXiv:2205.02079v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02079","description":"<p>We consider the problem of tracking the 6D pose of a moving RGB-D camera in a\nneural scene representation. Different such representations have recently\nemerged, and we investigate the suitability of them for the task of camera\ntracking. In particular, we propose to track an RGB-D camera using a signed\ndistance field-based representation and show that compared to density-based\nrepresentations, tracking can be sped up, which enables more robust and\naccurate pose estimates when computation time is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruns_L/0/1/0/all/0/1\">Leonard Bruns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zangeneh_F/0/1/0/all/0/1\">Fereidoon Zangeneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensfelt_P/0/1/0/all/0/1\">Patric Jensfelt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Extrapolationin Space and Time. (arXiv:2205.02084v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02084","description":"<p>Novel view synthesis (NVS) and video prediction (VP) are typically considered\ndisjoint tasks in computer vision. However, they can both be seen as ways to\nobserve the spatial-temporal world: NVS aims to synthesize a scene from a new\npoint of view, while VP aims to see a scene from a new point of time. These two\ntasks provide complementary signals to obtain a scene representation, as\nviewpoint changes from spatial observations inform depth, and temporal\nobservations inform the motion of cameras and individual objects. Inspired by\nthese observations, we propose to study the problem of Video Extrapolation in\nSpace and Time (VEST). We propose a model that leverages the self-supervision\nand the complementary cues from both tasks, while existing methods can only\nsolve one of them. Experiments show that our method achieves performance better\nthan or comparable to several state-of-the-art NVS and VP methods on indoor and\noutdoor real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wu_J/0/1/0/all/0/1\">Jiajun wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypercomplex Image-to-Image Translation. (arXiv:2205.02087v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02087","description":"<p>Image-to-image translation (I2I) aims at transferring the content\nrepresentation from an input domain to an output one, bouncing along different\ntarget domains. Recent I2I generative models, which gain outstanding results in\nthis task, comprise a set of diverse deep networks each with tens of million\nparameters. Moreover, images are usually three-dimensional being composed of\nRGB channels and common neural models do not take dimensions correlation into\naccount, losing beneficial information. In this paper, we propose to leverage\nhypercomplex algebra properties to define lightweight I2I generative models\ncapable of preserving pre-existing relations among image dimensions, thus\nexploiting additional input information. On manifold I2I benchmarks, we show\nhow the proposed Quaternion StarGANv2 and parameterized hypercomplex StarGANv2\n(PHStarGANv2) reduce parameters and storage memory amount while ensuring high\ndomain translation performance and good image quality as measured by FID and\nLPIPS scores. Full code is available at: https://github.com/ispamm/HI2I.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigillo_L/0/1/0/all/0/1\">Luigi Sigillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Fully Annotated Thermal Infrared Face Dataset: Recorded in Various Environment Conditions and Distances From The Camera. (arXiv:2205.02093v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02093","description":"<p>Facial thermography is one of the most popular research areas in infrared\nthermal imaging, with diverse applications in medical, surveillance, and\nenvironmental monitoring. However, in contrast to facial imagery in the visual\nspectrum, the lack of public datasets on facial thermal images is an obstacle\nto research improvement in this area. Thermal face imagery is still a\nrelatively new research area to be evaluated and studied in different\ndomains.The current thermal face datasets are limited in regards to the\nsubjects' distance from the camera, the ambient temperature variation, and\nfacial landmarks' localization. We address these gaps by presenting a new\nfacial thermography dataset. This article makes two main contributions to the\nbody of knowledge. First, it presents a comprehensive review and comparison of\ncurrent public datasets in facial thermography. Second, it introduces and\nstudies a novel public dataset on facial thermography, which we call it\nCharlotte-ThermalFace. Charlotte-ThermalFace contains more than10000 infrared\nthermal images in varying thermal conditions, several distances from the\ncamera, and different head positions. The data is fully annotated with the\nfacial landmarks, ambient temperature, relative humidity, the air speed of the\nroom, distance to the camera, and subject thermal sensation at the time of\ncapturing each image. Our dataset is the first publicly available thermal\ndataset annotated with the thermal sensation of each subject in different\nthermal conditions and one of the few datasets in raw 16-bit format. Finally,\nwe present a preliminary analysis of the dataset to show the applicability and\nimportance of the thermal conditions in facial thermography. The full dataset,\nincluding annotations, are freely available for research purpose at\nhttps://github.com/TeCSAR-UNCC/UNCC-ThermalFace\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashrafi_R/0/1/0/all/0/1\">Roshanak Ashrafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarbayjania_M/0/1/0/all/0/1\">Mona Azarbayjania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Sparse R-CNN. (arXiv:2205.02101v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02101","description":"<p>Sparse R-CNN is a recent strong object detection baseline by set prediction\non sparse, learnable proposal boxes and proposal features. In this work, we\npropose to improve Sparse R-CNN with two dynamic designs. First, Sparse R-CNN\nadopts a one-to-one label assignment scheme, where the Hungarian algorithm is\napplied to match only one positive sample for each ground truth. Such\none-to-one assignment may not be optimal for the matching between the learned\nproposal boxes and ground truths. To address this problem, we propose dynamic\nlabel assignment (DLA) based on the optimal transport algorithm to assign\nincreasing positive samples in the iterative training stages of Sparse R-CNN.\nWe constrain the matching to be gradually looser in the sequential stages as\nthe later stage produces the refined proposals with improved precision. Second,\nthe learned proposal boxes and features remain fixed for different images in\nthe inference process of Sparse R-CNN. Motivated by dynamic convolution, we\npropose dynamic proposal generation (DPG) to assemble multiple proposal experts\ndynamically for providing better initial proposal boxes and features for the\nconsecutive training stages. DPG thereby can derive sample-dependent proposal\nboxes and features for inference. Experiments demonstrate that our method,\nnamed Dynamic Sparse R-CNN, can boost the strong Sparse R-CNN baseline with\ndifferent backbones for object detection. Particularly, Dynamic Sparse R-CNN\nreaches the state-of-the-art 47.2% AP on the COCO 2017 validation set,\nsurpassing Sparse R-CNN by 2.2% AP with the same ResNet-50 backbone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qinghang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Activation Vectors for Generating User-Defined 3D Shapes. (arXiv:2205.02102v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02102","description":"<p>We explore the interpretability of 3D geometric deep learning models in the\ncontext of Computer-Aided Design (CAD). The field of parametric CAD can be\nlimited by the difficulty of expressing high-level design concepts in terms of\na few numeric parameters. In this paper, we use a deep learning architectures\nto encode high dimensional 3D shapes into a vectorized latent representation\nthat can be used to describe arbitrary concepts. Specifically, we train a\nsimple auto-encoder to parameterize a dataset of complex shapes. To understand\nthe latent encoded space, we use the idea of Concept Activation Vectors (CAV)\nto reinterpret the latent space in terms of user-defined concepts. This allows\nmodification of a reference design to exhibit more or fewer characteristics of\na chosen concept or group of concepts. We also test the statistical\nsignificance of the identified concepts and determine the sensitivity of a\nphysical quantity of interest across the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Druc_S/0/1/0/all/0/1\">Stefan Druc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1\">Aditya Balu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wooldridge_P/0/1/0/all/0/1\">Peter Wooldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Adarsh Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumik Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuroevolutionary Multi-objective approaches to Trajectory Prediction in Autonomous Vehicles. (arXiv:2205.02105v1 [cs.NE])","link":"http://arxiv.org/abs/2205.02105","description":"<p>The incentive for using Evolutionary Algorithms (EAs) for the automated\noptimization and training of deep neural networks (DNNs), a process referred to\nas neuroevolution, has gained momentum in recent years. The configuration and\ntraining of these networks can be posed as optimization problems. Indeed, most\nof the recent works on neuroevolution have focused their attention on\nsingle-objective optimization. Moreover, from the little research that has been\ndone at the intersection of neuroevolution and evolutionary multi-objective\noptimization (EMO), all the research that has been carried out has focused\npredominantly on the use of one type of DNN: convolutional neural networks\n(CNNs), using well-established standard benchmark problems such as MNIST. In\nthis work, we make a leap in the understanding of these two areas\n(neuroevolution and EMO), regarded in this work as neuroevolutionary\nmulti-objective, by using and studying a rich DNN composed of a CNN and\nLong-short Term Memory network. Moreover, we use a robust and challenging\nvehicle trajectory prediction problem. By using the well-known Non-dominated\nSorting Genetic Algorithm-II, we study the effects of five different\nobjectives, tested in categories of three, allowing us to show how these\nobjectives have either a positive or detrimental effect in neuroevolution for\ntrajectory prediction in autonomous vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_F/0/1/0/all/0/1\">Fergal Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galvan_E/0/1/0/all/0/1\">Edgar Galv&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of fish location by combining fisheries data and sea bottom temperature forecasting. (arXiv:2205.02107v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02107","description":"<p>This paper combines fisheries dependent data and environmental data to be\nused in a machine learning pipeline to predict the spatio-temporal abundance of\ntwo species (plaice and sole) commonly caught by the Belgian fishery in the\nNorth Sea. By combining fisheries related features with environmental data, sea\nbottom temperature derived from remote sensing, a higher accuracy can be\nachieved. In a forecast setting, the predictive accuracy is further improved by\npredicting, using a recurrent deep neural network, the sea bottom temperature\nup to four days in advance instead of relying on the last previous temperature\nmeasurement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ospici_M/0/1/0/all/0/1\">Matthieu Ospici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sys_K/0/1/0/all/0/1\">Klaas Sys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guegan_Marat_S/0/1/0/all/0/1\">Sophie Guegan-Marat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Orientation Estimation and Detection with Hybrid Object Detection Networks for Automotive Radar. (arXiv:2205.02111v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02111","description":"<p>This paper presents novel hybrid architectures that combine grid- and\npoint-based processing to improve the detection performance and orientation\nestimation of radar-based object detection networks. Purely grid-based\ndetection models operate on a bird's-eye-view (BEV) projection of the input\npoint cloud. These approaches suffer from a loss of detailed information\nthrough the discrete grid resolution. This applies in particular to radar\nobject detection, where relatively coarse grid resolutions are commonly used to\naccount for the sparsity of radar point clouds. In contrast, point-based models\nare not affected by this problem as they continuously process point clouds.\nHowever, they generally exhibit worse detection performances than grid-based\nmethods.\n</p>\n<p>We show that a point-based model can extract neighborhood features,\nleveraging the exact relative positions of points, before grid rendering. This\nhas significant benefits for a following convolutional detection backbone. In\nexperiments on the public nuScenes dataset our hybrid architecture achieves\nimprovements in terms of detection performance and orientation estimates over\nnetworks from previous literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_M/0/1/0/all/0/1\">Michael Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1\">Sascha Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_D/0/1/0/all/0/1\">Daniel K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niederlohner_D/0/1/0/all/0/1\">Daniel Niederl&#xf6;hner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faion_F/0/1/0/all/0/1\">Florian Faion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaser_C/0/1/0/all/0/1\">Claudius Gl&#xe4;ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blume_H/0/1/0/all/0/1\">Holger Blume</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Engineering deep learning methods on automatic detection of damage in infrastructure due to extreme events. (arXiv:2205.02125v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02125","description":"<p>This paper presents a few comprehensive experimental studies for automated\nStructural Damage Detection (SDD) in extreme events using deep learning methods\nfor processing 2D images. In the first study, a 152-layer Residual network\n(ResNet) is utilized to classify multiple classes in eight SDD tasks, which\ninclude identification of scene levels, damage levels, material types, etc. The\nproposed ResNet achieved high accuracy for each task while the positions of the\ndamage are not identifiable. In the second study, the existing ResNet and a\nsegmentation network (U-Net) are combined into a new pipeline, cascaded\nnetworks, for categorizing and locating structural damage. The results show\nthat the accuracy of damage detection is significantly improved compared to\nonly using a segmentation network. In the third and fourth studies, end-to-end\nnetworks are developed and tested as a new solution to directly detect cracks\nand spalling in the image collections of recent large earthquakes. One of the\nproposed networks can achieve an accuracy above 67.6% for all tested images at\nvarious scales and resolutions, and shows its robustness for these human-free\ndetection tasks. As a preliminary field study, we applied the proposed method\nto detect damage in a concrete structure that was tested to study its\nprogressive collapse performance. The experiments indicate that these solutions\nfor automatic detection of structural damage using deep learning methods are\nfeasible and promising. The training datasets and codes will be made available\nfor the public upon the publication of this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yongsheng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_B/0/1/0/all/0/1\">Bing Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezen_H/0/1/0/all/0/1\">Halil Sezen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domino Saliency Metrics: Improving Existing Channel Saliency Metrics with Structural Information. (arXiv:2205.02131v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02131","description":"<p>Channel pruning is used to reduce the number of weights in a Convolutional\nNeural Network (CNN). Channel pruning removes slices of the weight tensor so\nthat the convolution layer remains dense. The removal of these weight slices\nfrom a single layer causes mismatching number of feature maps between layers of\nthe network. A simple solution is to force the number of feature map between\nlayers to match through the removal of weight slices from subsequent layers.\nThis additional constraint becomes more apparent in DNNs with branches where\nmultiple channels need to be pruned together to keep the network dense. Popular\npruning saliency metrics do not factor in the structural dependencies that\narise in DNNs with branches. We propose Domino metrics (built on existing\nchannel saliency metrics) to reflect these structural constraints. We test\nDomino saliency metrics against the baseline channel saliency metrics on\nmultiple networks with branches. Domino saliency metrics improved pruning rates\nin most tested networks and up to 25% in AlexNet on CIFAR-10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1\">Kaveena Persand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RecipeSnap -- a lightweight image-to-recipe model. (arXiv:2205.02141v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02141","description":"<p>In this paper we want to address the problem of automation for recognition of\nphotographed cooking dishes and generating the corresponding food recipes.\nCurrent image-to-recipe models are computation expensive and require powerful\nGPUs for model training and implementation. High computational cost prevents\nthose existing models from being deployed on portable devices, like smart\nphones. To solve this issue we introduce a lightweight image-to-recipe\nprediction model, RecipeSnap, that reduces memory cost and computational cost\nby more than 90% while still achieving 2.0 MedR, which is in line with the\nstate-of-the-art model. A pre-trained recipe encoder was used to compute recipe\nembeddings. Recipes from Recipe1M dataset and corresponding recipe embeddings\nare collected as a recipe library, which are used for image encoder training\nand image query later. We use MobileNet-V2 as image encoder backbone, which\nmakes our model suitable to portable devices. This model can be further\ndeveloped into an application for smart phones with a few effort. A comparison\nof the performance between this lightweight model to other heavy models are\npresented in this paper. Code, data and models are publicly accessible on\ngithub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianfa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Generative Methods for Multiple Image Inpainting. (arXiv:2205.02146v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02146","description":"<p>Image inpainting refers to the restoration of an image with missing regions\nin a way that is not detectable by the observer. The inpainting regions can be\nof any size and shape. This is an ill-posed inverse problem that does not have\na unique solution. In this work, we focus on learning-based image completion\nmethods for multiple and diverse inpainting which goal is to provide a set of\ndistinct solutions for a given damaged image. These methods capitalize on the\nprobabilistic nature of certain generative models to sample various solutions\nthat coherently restore the missing content. Along the chapter, we will analyze\nthe underlying theory and analyze the recent proposals for multiple inpainting.\nTo investigate the pros and cons of each method, we present quantitative and\nqualitative comparisons, on common datasets, regarding both the quality and the\ndiversity of the set of inpainted solutions. Our analysis allows us to identify\nthe most successful generative strategies in both inpainting quality and\ninpainting diversity. This task is closely related to the learning of an\naccurate probability distribution of images. Depending on the dataset in use,\nthe challenges that entail the training of such a model will be discussed\nthrough the analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ballester_C/0/1/0/all/0/1\">Coloma Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugeau_A/0/1/0/all/0/1\">Aurelie Bugeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurault_S/0/1/0/all/0/1\">Samuel Hurault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisotto_S/0/1/0/all/0/1\">Simone Parisotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitoria_P/0/1/0/all/0/1\">Patricia Vitoria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Cross-Attention Learning for Fine-Grained Visual Categorization and Object Re-Identification. (arXiv:2205.02151v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02151","description":"<p>Recently, self-attention mechanisms have shown impressive performance in\nvarious NLP and CV tasks, which can help capture sequential characteristics and\nderive global information. In this work, we explore how to extend\nself-attention modules to better learn subtle feature embeddings for\nrecognizing fine-grained objects, e.g., different bird species or person\nidentities. To this end, we propose a dual cross-attention learning (DCAL)\nalgorithm to coordinate with self-attention learning. First, we propose\nglobal-local cross-attention (GLCA) to enhance the interactions between global\nimages and local high-response regions, which can help reinforce the\nspatial-wise discriminative clues for recognition. Second, we propose pair-wise\ncross-attention (PWCA) to establish the interactions between image pairs. PWCA\ncan regularize the attention learning of an image by treating another image as\ndistractor and will be removed during inference. We observe that DCAL can\nreduce misleading attentions and diffuse the attention response to discover\nmore complementary parts for recognition. We conduct extensive evaluations on\nfine-grained visual categorization and object re-identification. Experiments\ndemonstrate that DCAL performs on par with state-of-the-art methods and\nconsistently improves multiple self-attention baselines, e.g., surpassing\nDeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haowei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1\">Wenjing Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2 segmentation models. (arXiv:2205.02152v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02152","description":"<p>Recent studies indicate that detecting radiographic patterns on CT scans can\nyield high sensitivity and specificity for COVID-19 localization. In this\npaper, we investigate the appropriateness of deep learning models\ntransferability, for semantic segmentation of pneumonia-infected areas in CT\nimages. Transfer learning allows for the fast initialization/ reutilization of\ndetection models, given that large volumes of training are not available. Our\nwork explores the efficacy of using pre-trained U-Net architectures, on a\nspecific CT data set, for identifying Covid-19 side-effects over images from\ndifferent datasets. Experimental results indicate improvement in the\nsegmentation accuracy of identifying COVID-19 infected regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maganaris_C/0/1/0/all/0/1\">Constantine Maganaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakalos_N/0/1/0/all/0/1\">Nikolaos Bakalos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalogeras_D/0/1/0/all/0/1\">Dimitris Kalogeras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angeli_A/0/1/0/all/0/1\">Aikaterini Angeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnrealNAS: Can We Search Neural Architectures with Unreal Data?. (arXiv:2205.02162v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02162","description":"<p>Neural architecture search (NAS) has shown great success in the automatic\ndesign of deep neural networks (DNNs). However, the best way to use data to\nsearch network architectures is still unclear and under exploration. Previous\nwork [19, 46] has analyzed the necessity of having ground-truth labels in NAS\nand inspired broad interest. In this work, we take a further step to question\nwhether real data is necessary for NAS to be effective. The answer to this\nquestion is important for applications with limited amount of accessible data,\nand can help people improve NAS by leveraging the extra flexibility of data\ngeneration. To explore if NAS needs real data, we construct three types of\nunreal datasets using: 1) randomly labeled real images; 2) generated images and\nlabels; and 3) generated Gaussian noise with random labels. These datasets\nfacilitate to analyze the generalization and expressivity of the searched\narchitectures. We study the performance of architectures searched on these\nconstructed datasets using popular differentiable NAS methods. Extensive\nexperiments on CIFAR, ImageNet and CheXpert [12] show that the searched\narchitectures can achieve promising results compared with those derived from\nthe conventional NAS pipeline with real labeled data, suggesting the\nfeasibility of performing NAS with unreal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaicheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingfei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compound virtual screening by learning-to-rank with gradient boosting decision tree and enrichment-based cumulative gain. (arXiv:2205.02169v1 [q-bio.BM])","link":"http://arxiv.org/abs/2205.02169","description":"<p>Learning-to-rank, a machine learning technique widely used in information\nretrieval, has recently been applied to the problem of ligand-based virtual\nscreening, to accelerate the early stages of new drug development. Ranking\nprediction models learn based on ordinal relationships, making them suitable\nfor integrating assay data from various environments. Existing studies of rank\nprediction in compound screening have generally used a learning-to-rank method\ncalled RankSVM. However, they have not been compared with or validated against\nthe gradient boosting decision tree (GBDT)-based learning-to-rank methods that\nhave gained popularity recently. Furthermore, although the ranking metric\ncalled Normalized Discounted Cumulative Gain (NDCG) is widely used in\ninformation retrieval, it only determines whether the predictions are better\nthan those of other models. In other words, NDCG is incapable of recognizing\nwhen a prediction model produces worse than random results. Nevertheless, NDCG\nis still used in the performance evaluation of compound screening using\nlearning-to-rank. This study used the GBDT model with ranking loss functions,\ncalled lambdarank and lambdaloss, for ligand-based virtual screening; results\nwere compared with existing RankSVM methods and GBDT models using regression.\nWe also proposed a new ranking metric, Normalized Enrichment Discounted\nCumulative Gain (NEDCG), which aims to properly evaluate the goodness of\nranking predictions. Results showed that the GBDT model with learning-to-rank\noutperformed existing regression methods using GBDT and RankSVM on diverse\ndatasets. Moreover, NEDCG showed that predictions by regression were comparable\nto random predictions in multi-assay, multi-family datasets, demonstrating its\nusefulness for a more direct assessment of compound screening performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Furui_K/0/1/0/all/0/1\">Kairi Furui</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ohue_M/0/1/0/all/0/1\">Masahito Ohue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COOPERNAUT: End-to-End Driving with Cooperative Perception for Networked Vehicles. (arXiv:2205.02222v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02222","description":"<p>Optical sensors and learning algorithms for autonomous vehicles have\ndramatically advanced in the past few years. Nonetheless, the reliability of\ntoday's autonomous vehicles is hindered by the limited line-of-sight sensing\ncapability and the brittleness of data-driven methods in handling extreme\nsituations. With recent developments of telecommunication technologies,\ncooperative perception with vehicle-to-vehicle communications has become a\npromising paradigm to enhance autonomous driving in dangerous or emergency\nsituations. We introduce COOPERNAUT, an end-to-end learning model that uses\ncross-vehicle perception for vision-based cooperative driving. Our model\nencodes LiDAR information into compact point-based representations that can be\ntransmitted as messages between vehicles via realistic wireless channels. To\nevaluate our model, we develop AutoCastSim, a network-augmented driving\nsimulation framework with example accident-prone scenarios. Our experiments on\nAutoCastSim suggest that our cooperative perception driving models lead to a\n40% improvement in average success rate over egocentric driving models in these\nchallenging driving situations and a 5 times smaller bandwidth requirement than\nprior work V2VNet. COOPERNAUT and AUTOCASTSIM are available at\nhttps://ut-austin-rpl.github.io/Coopernaut/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiaxun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Hang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Similarity Attention. (arXiv:1911.07381v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.07381","description":"<p>While there has been substantial progress in learning suitable distance\nmetrics, these techniques in general lack transparency and decision reasoning,\ni.e., explaining why the input set of images is similar or dissimilar. In this\nwork, we solve this key problem by proposing the first method to generate\ngeneric visual similarity explanations with gradient-based attention. We\ndemonstrate that our technique is agnostic to the specific similarity model\ntype, e.g., we show applicability to Siamese, triplet, and quadruplet models.\nFurthermore, we make our proposed similarity attention a principled part of the\nlearning process, resulting in a new paradigm for learning similarity\nfunctions. We demonstrate that our learning mechanism results in more\ngeneralizable, as well as explainable, similarity models. Finally, we\ndemonstrate the generality of our framework by means of experiments on a\nvariety of tasks, including image retrieval, person re-identification, and\nlow-shot semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1\">Richard J. Radke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking. (arXiv:2007.09600v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.09600","description":"<p>Ellipse fitting, an essential component in pupil or iris tracking based video\noculography, is performed on previously segmented eye parts generated using\nvarious computer vision techniques. Several factors, such as occlusions due to\neyelid shape, camera position or eyelashes, frequently break ellipse fitting\nalgorithms that rely on well-defined pupil or iris edge segments. In this work,\nwe propose training a convolutional neural network to directly segment entire\nelliptical structures and demonstrate that such a framework is robust to\nocclusions and offers superior pupil and iris tracking performance (at least\n10$\\%$ and 24$\\%$ increase in pupil and iris center detection rate respectively\nwithin a two-pixel error margin) compared to using standard eye parts\nsegmentation for multiple publicly available synthetic segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Rakshit S. Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aayush K. Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_R/0/1/0/all/0/1\">Reynold J. Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelz_J/0/1/0/all/0/1\">Jeff B. Pelz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_G/0/1/0/all/0/1\">Gabriel J. Diaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Representations of Positive Functions via First and Second-Order Pseudo-Mirror Descent. (arXiv:2011.07142v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2011.07142","description":"<p>We consider expected risk minimization problems when the range of the\nestimator is required to be nonnegative, motivated by the settings of maximum\nlikelihood estimation (MLE) and trajectory optimization. To facilitate\nnonlinear interpolation, we hypothesize that the search space is a Reproducing\nKernel Hilbert Space (RKHS). We develop first and second-order variants of\nstochastic mirror descent employing (i) \\emph{pseudo-gradients} and (ii)\ncomplexity-reducing projections. Compressive projection in the first-order\nscheme is executed via kernel orthogonal matching pursuit (KOMP), which\novercomes the fact that the vanilla RKHS parameterization grows unbounded with\nthe iteration index in the stochastic setting. Moreover, pseudo-gradients are\nneeded when gradient estimates for cost are only computable up to some\nnumerical error, which arise in, e.g., integral approximations. Under constant\nstep-size and compression budget, we establish tradeoffs between the radius of\nconvergence of the expected sub-optimality and the projection budget parameter,\nas well as non-asymptotic bounds on the model complexity. To refine the\nsolution's precision, we develop a second-order extension which employs\nrecursively averaged pseudo-gradient outer-products to approximate the Hessian\ninverse, whose convergence in mean is established under an additional\neigenvalue decay condition on the Hessian of the optimal RKHS element, which is\nunique to this work. Experiments demonstrate favorable performance on\ninhomogeneous Poisson Process intensity estimation in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Chakraborty_A/0/1/0/all/0/1\">Abhishek Chakraborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Two-Stream CNN for Multi-Modal Age-related Macular Degeneration Categorization. (arXiv:2012.01879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01879","description":"<p>This paper tackles automated categorization of Age-related Macular\nDegeneration (AMD), a common macular disease among people over 50. Previous\nresearch efforts mainly focus on AMD categorization with a single-modal input,\nlet it be a color fundus photograph (CFP) or an OCT B-scan image. By contrast,\nwe consider AMD categorization given a multi-modal input, a direction that is\nclinically meaningful yet mostly unexplored. Contrary to the prior art that\ntakes a traditional approach of feature extraction plus classifier training\nthat cannot be jointly optimized, we opt for end-to-end multi-modal\nConvolutional Neural Networks (MM-CNN). Our MM-CNN is instantiated by a\ntwo-stream CNN, with spatially-invariant fusion to combine information from the\nCFP and OCT streams. In order to visually interpret the contribution of the\nindividual modalities to the final prediction, we extend the class activation\nmapping (CAM) technique to the multi-modal scenario. For effective training of\nMM-CNN, we develop two data augmentation methods. One is GAN-based CFP/OCT\nimage synthesis, with our novel use of CAMs as conditional input of a\nhigh-resolution image-to-image translation GAN. The other method is Loose\nPairing, which pairs a CFP image and an OCT image on the basis of their classes\ninstead of eye identities. Experiments on a clinical dataset consisting of\n1,094 CFP images and 1,289 OCT images acquired from 1,093 distinct eyes show\nthat the proposed solution obtains better F1 and Accuracy than multiple\nbaselines for multi-modal AMD categorization. Code and data are available at\nhttps://github.com/li-xirong/mmc-amd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianchun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Dayong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Youxin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Generation of Medical Images via Disentangled Adversarial Inference. (arXiv:2012.04764v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.04764","description":"<p>Synthetic medical image generation has a huge potential for improving\nhealthcare through many applications, from data augmentation for training\nmachine learning systems to preserving patient privacy. Conditional Adversarial\nGenerative Networks (cGANs) use a conditioning factor to generate images and\nhave shown great success in recent years. Intuitively, the information in an\nimage can be divided into two parts: 1) content which is presented through the\nconditioning vector and 2) style which is the undiscovered information missing\nfrom the conditioning vector. Current practices in using cGANs for medical\nimage generation, only use a single variable for image generation (i.e.,\ncontent) and therefore, do not provide much flexibility nor control over the\ngenerated image. In this work we propose a methodology to learn from the image\nitself, disentangled representations of style and content, and use this\ninformation to impose control over the generation process. In this framework,\nstyle is learned in a fully unsupervised manner, while content is learned\nthrough both supervised learning (using the conditioning vector) and\nunsupervised learning (with the inference mechanism). We undergo two novel\nregularization steps to ensure content-style disentanglement. First, we\nminimize the shared information between content and style by introducing a\nnovel application of the gradient reverse layer (GRL); second, we introduce a\nself-supervised regularization method to further separate information in the\ncontent and style variables. We show that in general, two latent variable\nmodels achieve better performance and give more control over the generated\nimage. We also show that our proposed model (DRAI) achieves the best\ndisentanglement score and has the best overall performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Havaei_M/0/1/0/all/0/1\">Mohammad Havaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_X/0/1/0/all/0/1\">Ximeng Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yiping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lao_Q/0/1/0/all/0/1\">Qicheng Lao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Universal Deep Learning Framework for Real-Time Denoising of Ultrasound Images. (arXiv:2101.09122v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.09122","description":"<p>Ultrasound images are widespread in medical diagnosis for muscle-skeletal,\ncardiac, and obstetrical diseases, due to the efficiency and non-invasiveness\nof the acquisition methodology. However, ultrasound acquisition introduces\nnoise in the signal, which corrupts the resulting image and affects further\nprocessing steps, e.g., segmentation and quantitative analysis. We define a\nnovel deep learning framework for the real-time denoising of ultrasound images.\nFirstly, we compare state-of-the-art methods for denoising (e.g., spectral,\nlow-rank methods) and select WNNM (Weighted Nuclear Norm Minimisation) as the\nbest denoising in terms of accuracy, preservation of anatomical features, and\nedge enhancement. Then, we propose a tuned version of WNNM (tuned-WNNM) that\nimproves the quality of the denoised images and extends its applicability to\nultrasound images. Through a deep learning framework, the tuned-WNNM\nqualitatively and quantitatively replicates WNNM results in real-time. Finally,\nour approach is general in terms of its building blocks and parameters of the\ndeep learning and high-performance computing framework; in fact, we can select\ndifferent denoising algorithms and deep learning architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cammarasana_S/0/1/0/all/0/1\">Simone Cammarasana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nicolardi_P/0/1/0/all/0/1\">Paolo Nicolardi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patane_G/0/1/0/all/0/1\">Giuseppe Patan&#xe8;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Purified Feature Representations from Task-irrelevant Labels. (arXiv:2102.10955v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.10955","description":"<p>Learning an empirically effective model with generalization using limited\ndata is a challenging task for deep neural networks. In this paper, we propose\na novel learning framework called PurifiedLearning to exploit task-irrelevant\nfeatures extracted from task-irrelevant labels when training models on\nsmall-scale datasets. Particularly, we purify feature representations by using\nthe expression of task-irrelevant information, thus facilitating the learning\nprocess of classification. Our work is built on solid theoretical analysis and\nextensive experiments, which demonstrate the effectiveness of PurifiedLearning.\nAccording to the theory we proved, PurifiedLearning is model-agnostic and\ndoesn't have any restrictions on the model needed, so it can be combined with\nany existing deep neural networks with ease to achieve better performance. The\nsource code of this paper will be available in the future for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangning_L/0/1/0/all/0/1\">Li Yangning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnosing Vision-and-Language Navigation: What Really Matters. (arXiv:2103.16561v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16561","description":"<p>Vision-and-language navigation (VLN) is a multimodal task where an agent\nfollows natural language instructions and navigates in visual environments.\nMultiple setups have been proposed, and researchers apply new model\narchitectures or training techniques to boost navigation performance. However,\nthere still exist non-negligible gaps between machines' performance and human\nbenchmarks. Moreover, the agents' inner mechanisms for navigation decisions\nremain unclear. To the best of our knowledge, how the agents perceive the\nmultimodal input is under-studied and needs investigation. In this work, we\nconduct a series of diagnostic experiments to unveil agents' focus during\nnavigation. Results show that indoor navigation agents refer to both object and\ndirection tokens when making decisions. In contrast, outdoor navigation agents\nheavily rely on direction tokens and poorly understand the object tokens.\nTransformer-based agents acquire a better cross-modal understanding of objects\nand display strong numerical reasoning ability than non-Transformer-based\nagents. When it comes to vision-and-language alignments, many models claim that\nthey can align object tokens with specific visual targets. We find unbalanced\nattention on the vision and text input and doubt the reliability of such\ncross-modal alignments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayana_P/0/1/0/all/0/1\">Pradyumna Narayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sone_K/0/1/0/all/0/1\">Kazoo Sone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sugato Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving. (arXiv:2105.00373v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.00373","description":"<p>The past few years have witnessed an increasing interest in improving the\nperception performance of LiDARs on autonomous vehicles. While most of the\nexisting works focus on developing new deep learning algorithms or model\narchitectures, we study the problem from the physical design perspective, i.e.,\nhow different placements of multiple LiDARs influence the learning-based\nperception. To this end, we introduce an easy-to-compute information-theoretic\nsurrogate metric to quantitatively and fast evaluate LiDAR placement for 3D\ndetection of different types of objects. We also present a new data collection,\ndetection model training and evaluation framework in the realistic CARLA\nsimulator to evaluate disparate multi-LiDAR configurations. Using several\nprevalent placements inspired by the designs of self-driving companies, we show\nthe correlation between our surrogate metric and object detection performance\nof different representative algorithms on KITTI through extensive experiments,\nvalidating the effectiveness of our LiDAR placement evaluation approach. Our\nresults show that sensor placement is non-negligible in 3D point cloud-based\nobject detection, which will contribute up to 10% performance discrepancy in\nterms of average precision in challenging 3D object detection settings. We\nbelieve that this is one of the first studies to quantitatively investigate the\ninfluence of LiDAR placement on perception performance. The code is available\non https://github.com/HanjiangHu/Multi-LiDAR-Placement-for-3D-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1\">Sharad Chitlangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnihotri_A/0/1/0/all/0/1\">Akhil Agnihotri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling 3D Object Detection with a Low-Resolution LiDAR. (arXiv:2105.01765v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01765","description":"<p>Light Detection And Ranging (LiDAR) has been widely used in autonomous\nvehicles for perception and localization. However, the cost of a\nhigh-resolution LiDAR is still prohibitively expensive, while its\nlow-resolution counterpart is much more affordable. Therefore, using\nlow-resolution LiDAR for autonomous driving is an economically viable solution,\nbut the point cloud sparsity makes it extremely challenging. In this paper, we\npropose a two-stage neural network framework that enables 3D object detection\nusing a low-resolution LiDAR. Taking input from a low-resolution LiDAR point\ncloud and a monocular camera image, a depth completion network is employed to\nproduce dense point cloud that is subsequently processed by a voxel-based\nnetwork for 3D object detection. Evaluated with KITTI dataset for 3D object\ndetection in Bird-Eye View (BEV), the experimental result shows that the\nproposed approach performs significantly better than directly applying the\n16-line LiDAR point cloud for object detection. For both easy and moderate\ncases, our 3D vehicle detection results are close to those using 64-line\nhigh-resolution LiDARs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey and Taxonomy on Image Dehazing Based on Deep Learning. (arXiv:2106.03323v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03323","description":"<p>With the development of convolutional neural networks, hundreds of deep\nlearning based dehazing methods have been proposed. In this paper, we provide a\ncomprehensive survey on supervised, semi-supervised, and unsupervised dehazing.\nWe first discuss the physical model, datasets, network modules, loss functions,\nand evaluation metrics that are commonly used. Then, the main contributions of\nvarious dehazing algorithms are categorized and summarized. Further,\nquantitative and qualitative experiments of various baseline methods are\ncarried out. Finally, the unsolved issues and challenges that can inspire the\nfuture research are pointed out. A collection of useful dehazing materials is\navailable at https://github.com/Xiaofeng-life/AwesomeDehazing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_J/0/1/0/all/0/1\">Jie Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xiaofeng Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiuxin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Deep Neural Network Calibration by Regularization and its Impact on Refinement. (arXiv:2106.09385v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09385","description":"<p>Deep neural networks have been shown to be highly miscalibrated. often they\ntend to be overconfident in their predictions. It poses a significant challenge\nfor safety-critical systems to utilise deep neural networks (DNNs), reliably.\nMany recently proposed approaches to mitigate this have demonstrated\nsubstantial progress in improving DNN calibration. However, they hardly touch\nupon refinement, which historically has been an essential aspect of\ncalibration. Refinement indicates separability of a network's correct and\nincorrect predictions. This paper presents a theoretically and empirically\nsupported exposition reviewing refinement of a calibrated model. Firstly, we\nshow the breakdown of expected calibration error (ECE), into predicted\nconfidence and refinement under the assumption of over-confident predictions.\nSecondly, linking with this result, we highlight that regularization based\ncalibration only focuses on naively reducing a model's confidence. This\nlogically has a severe downside to a model's refinement as correct and\nincorrect predictions become tightly coupled. Lastly, connecting refinement\nwith ECE also provides support to existing refinement based approaches which\nimprove calibration but do not explain the reasoning behind it. We support our\nclaims through rigorous empirical evaluations of many state of the art\ncalibration approaches on widely used datasets and neural networks. We find\nthat many calibration approaches with the likes of label smoothing, mixup etc.\nlower the usefulness of a DNN by degrading its refinement. Even under natural\ndata shift, this calibration-refinement trade-off holds for the majority of\ncalibration methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bay_A/0/1/0/all/0/1\">Alessandro Bay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1\">Biswa Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirabile_A/0/1/0/all/0/1\">Andrea Mirabile</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-domain Few-shot Learning with Task-specific Adapters. (arXiv:2107.00358v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00358","description":"<p>In this paper, we look at the problem of cross-domain few-shot classification\nthat aims to learn a classifier from previously unseen classes and domains with\nfew labeled samples. Recent approaches broadly solve this problem by\nparameterizing their few-shot classifiers with task-agnostic and task-specific\nweights where the former is typically learned on a large training set and the\nlatter is dynamically predicted through an auxiliary network conditioned on a\nsmall support set. In this work, we focus on the estimation of the latter, and\npropose to learn task-specific weights from scratch directly on a small support\nset, in contrast to dynamically estimating them. In particular, through\nsystematic analysis, we show that task-specific weights through parametric\nadapters in matrix form with residual connections to multiple intermediate\nlayers of a backbone network significantly improves the performance of the\nstate-of-the-art models in the Meta-Dataset benchmark with minor additional\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei-Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Slap Fingerprint Segmentation for Juveniles and Adults. (arXiv:2110.04067v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04067","description":"<p>Many fingerprint recognition systems capture four fingerprints in one image.\nIn such systems, the fingerprint processing pipeline must first segment each\nfour-fingerprint slap into individual fingerprints. Note that most of the\ncurrent fingerprint segmentation algorithms have been designed and evaluated\nusing only adult fingerprint datasets. In this work, we have developed a\nhuman-annotated in-house dataset of 15790 slaps of which 9084 are adult samples\nand 6706 are samples drawn from children from ages 4 to 12. Subsequently, the\ndataset is used to evaluate the matching performance of the NFSEG, a slap\nfingerprint segmentation system developed by NIST, on slaps from adults and\njuvenile subjects. Our results reveal the lower performance of NFSEG on slaps\nfrom juvenile subjects. Finally, we utilized our novel dataset to develop the\nMask-RCNN based Clarkson Fingerprint Segmentation (CFSEG). Our matching results\nusing the Verifinger fingerprint matcher indicate that CFSEG outperforms NFSEG\nfor both adults and juvenile slaps. The CFSEG model is publicly available at\n\\url{https://github.com/keivanB/Clarkson_Finger_Segment}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1\">M. G. Sarwar Murshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_R/0/1/0/all/0/1\">Robert Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_K/0/1/0/all/0/1\">Keivan Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1\">Faraz Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuckers_S/0/1/0/all/0/1\">Stephanie Schuckers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Authentication Attacks on Projection-based Cancelable Biometric Schemes (long version). (arXiv:2110.15163v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.15163","description":"<p>Cancelable biometric schemes aim at generating secure biometric templates by\ncombining user specific tokens, such as password, stored secret or salt, along\nwith biometric data. This type of transformation is constructed as a\ncomposition of a biometric transformation with a feature extraction algorithm.\nThe security requirements of cancelable biometric schemes concern the\nirreversibility, unlinkability and revocability of templates, without losing in\naccuracy of comparison. While several schemes were recently attacked regarding\nthese requirements, full reversibility of such a composition in order to\nproduce colliding biometric characteristics, and specifically presentation\nattacks, were never demonstrated to the best of our knowledge. In this paper,\nwe formalize these attacks for a traditional cancelable scheme with the help of\ninteger linear programming (ILP) and quadratically constrained quadratic\nprogramming (QCQP). Solving these optimization problems allows an adversary to\nslightly alter its fingerprint image in order to impersonate any individual.\nMoreover, in an even more severe scenario, it is possible to simultaneously\nimpersonate several individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1\">Axel Durbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafourcade_P/0/1/0/all/0/1\">Pascal Lafourcade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migdal_D/0/1/0/all/0/1\">Denis Migdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1\">Kevin Thiry-Atighehchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1\">Paul-Marie Grollemund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Palette: Image-to-Image Diffusion Models. (arXiv:2111.05826v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05826","description":"<p>This paper develops a unified framework for image-to-image translation based\non conditional diffusion models and evaluates this framework on four\nchallenging image-to-image translation tasks, namely colorization, inpainting,\nuncropping, and JPEG restoration. Our simple implementation of image-to-image\ndiffusion models outperforms strong GAN and regression baselines on all tasks,\nwithout task-specific hyper-parameter tuning, architecture customization, or\nany auxiliary loss or sophisticated new techniques needed. We uncover the\nimpact of an L2 vs. L1 loss in the denoising diffusion objective on sample\ndiversity, and demonstrate the importance of self-attention in the neural\narchitecture through empirical studies. Importantly, we advocate a unified\nevaluation protocol based on ImageNet, with human evaluation and sample quality\nscores (FID, Inception Score, Classification Accuracy of a pre-trained\nResNet-50, and Perceptual Distance against original images). We expect this\nstandardized evaluation protocol to play a role in advancing image-to-image\ntranslation research. Finally, we show that a generalist, multi-task diffusion\nmodel performs as well or better than task-specific specialist counterparts.\nCheck out https://diffusion-palette.github.io for an overview of the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chris A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-shape Transformer for Underwater Image Enhancement. (arXiv:2111.11843v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11843","description":"<p>The light absorption and scattering of underwater impurities lead to poor\nunderwater imaging quality. The existing data-driven based underwater image\nenhancement (UIE) techniques suffer from the lack of a large-scale dataset\ncontaining various underwater scenes and high-fidelity reference images.\nBesides, the inconsistent attenuation in different color channels and space\nareas is not fully considered for boosted enhancement. In this work, we\nconstructed a large-scale underwater image (LSUI) dataset including 5004 image\npairs, and reported an U-shape Transformer network where the transformer model\nis for the first time introduced to the UIE task. The U-shape Transformer is\nintegrated with a channel-wise multi-scale feature fusion transformer (CMSFFT)\nmodule and a spatial-wise global feature modeling transformer (SGFMT) module,\nwhich reinforce the network's attention to the color channels and space areas\nwith more serious attenuation. Meanwhile, in order to further improve the\ncontrast and saturation, a novel loss function combining RGB, LAB and LCH color\nspaces is designed following the human vision principle. The extensive\nexperiments on available datasets validate the state-of-the-art performance of\nthe reported technique with more than 2dB superiority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lintao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chunli Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1\">Liheng Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multiple Dense Prediction Tasks from Partially Annotated Data. (arXiv:2111.14893v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14893","description":"<p>Despite the recent advances in multi-task learning of dense prediction\nproblems, most methods rely on expensive labelled datasets. In this paper, we\npresent a label efficient approach and look at jointly learning of multiple\ndense prediction tasks on partially annotated data (i.e. not all the task\nlabels are available for each image), which we call multi-task\npartially-supervised learning. We propose a multi-task training procedure that\nsuccessfully leverages task relations to supervise its multi-task learning when\ndata is partially annotated. In particular, we learn to map each task pair to a\njoint pairwise task-space which enables sharing information between them in a\ncomputationally efficient way through another network conditioned on task\npairs, and avoids learning trivial cross-task relations by retaining high-level\ninformation about the input image. We rigorously demonstrate that our proposed\nmethod effectively exploits the images with unlabelled tasks and outperforms\nexisting semi-supervised learning approaches and related methods on three\nstandard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei-Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Text-Guided Object Generation with Dream Fields. (arXiv:2112.01455v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01455","description":"<p>We combine neural rendering with multi-modal image and text representations\nto synthesize diverse 3D objects solely from natural language descriptions. Our\nmethod, Dream Fields, can generate the geometry and color of a wide range of\nobjects without 3D supervision. Due to the scarcity of diverse, captioned 3D\ndata, prior methods only generate objects from a handful of categories, such as\nShapeNet. Instead, we guide generation with image-text models pre-trained on\nlarge datasets of captioned images from the web. Our method optimizes a Neural\nRadiance Field from many camera views so that rendered images score highly with\na target caption according to a pre-trained CLIP model. To improve fidelity and\nvisual quality, we introduce simple geometric priors, including\nsparsity-inducing transmittance regularization, scene bounds, and new MLP\narchitectures. In experiments, Dream Fields produce realistic, multi-view\nconsistent object geometry and color from a variety of natural language\ncaptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ajay Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1\">Ben Poole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Music-to-Dance Generation with Optimal Transport. (arXiv:2112.01806v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.01806","description":"<p>Dance choreography for a piece of music is a challenging task, having to be\ncreative in presenting distinctive stylistic dance elements while taking into\naccount the musical theme and rhythm. It has been tackled by different\napproaches such as similarity retrieval, sequence-to-sequence modeling and\ngenerative adversarial networks, but their generated dance sequences are often\nshort of motion realism, diversity and music consistency. In this paper, we\npropose a Music-to-Dance with Optimal Transport Network (MDOT-Net) for learning\nto generate 3D dance choreographies from music. We introduce an optimal\ntransport distance for evaluating the authenticity of the generated dance\ndistribution and a Gromov-Wasserstein distance to measure the correspondence\nbetween the dance distribution and the input music. This gives a well defined\nand non-divergent training objective that mitigates the limitation of standard\nGAN training which is frequently plagued with instability and divergent\ngenerator loss issues. Extensive experiments demonstrate that our MDOT-Net can\nsynthesize realistic and diverse dances which achieve an organic unity with the\ninput music, reflecting the shared intentionality and matching the rhythmic\narticulation. Sample results are found at\nhttps://www.youtube.com/watch?v=dErfBkrlUO8.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Animation of Fluid Elements in Still Images. (arXiv:2112.03051v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03051","description":"<p>We propose a method to interactively control the animation of fluid elements\nin still images to generate cinemagraphs. Specifically, we focus on the\nanimation of fluid elements like water, smoke, fire, which have the properties\nof repeating textures and continuous fluid motion. Taking inspiration from\nprior works, we represent the motion of such fluid elements in the image in the\nform of a constant 2D optical flow map. To this end, we allow the user to\nprovide any number of arrow directions and their associated speeds along with a\nmask of the regions the user wants to animate. The user-provided input arrow\ndirections, their corresponding speed values, and the mask are then converted\ninto a dense flow map representing a constant optical flow map (FD). We observe\nthat FD, obtained using simple exponential operations can closely approximate\nthe plausible motion of elements in the image. We further refine computed dense\noptical flow map FD using a generative-adversarial network (GAN) to obtain a\nmore realistic flow map. We devise a novel UNet based architecture to\nautoregressively generate future frames using the refined optical flow map by\nforward-warping the input image features at different resolutions. We conduct\nextensive experiments on a publicly available dataset and show that our method\nis superior to the baselines in terms of qualitative and quantitative metrics.\nIn addition, we show the qualitative animations of the objects in directions\nthat did not exist in the training set and provide a way to synthesize videos\nthat otherwise would not exist in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1\">Aniruddha Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1\">Kuldeep Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensembling Off-the-shelf Models for GAN Training. (arXiv:2112.09130v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09130","description":"<p>The advent of large-scale training has produced a cornucopia of powerful\nvisual recognition models. However, generative models, such as GANs, have\ntraditionally been trained from scratch in an unsupervised manner. Can the\ncollective \"knowledge\" from a large bank of pretrained vision models be\nleveraged to improve GAN training? If so, with so many models to choose from,\nwhich one(s) should be selected, and in what manner are they most effective? We\nfind that pretrained computer vision models can significantly improve\nperformance when used in an ensemble of discriminators. Notably, the particular\nsubset of selected models greatly affects performance. We propose an effective\nselection mechanism, by probing the linear separability between real and fake\nsamples in pretrained model embeddings, choosing the most accurate model, and\nprogressively adding it to the discriminator ensemble. Interestingly, our\nmethod can improve GAN training in both limited data and large-scale settings.\nGiven only 10k training samples, our FID on LSUN Cat matches the StyleGAN2\ntrained on 1.6M images. On the full dataset, our method improves FID by 1.5x to\n2x on cat, church, and horse categories of LSUN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumari_N/0/1/0/all/0/1\">Nupur Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Streaming Volumetric Image Generation Framework for Development and Evaluation of Out-of-Core Methods. (arXiv:2112.09809v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09809","description":"<p>Advances in 3D imaging technology in recent years have allowed for\nincreasingly high resolution volumetric images of large specimen. The resulting\ndatasets of hundreds of Gigabytes in size call for new scalable and memory\nefficient approaches in the field of image processing, where some progress has\nbeen made already. At the same time, quantitative evaluation of these new\nmethods is difficult both in terms of the availability of specific data sizes\nand in the generation of associated ground truth data. In this paper we present\nan algorithmic framework that can be used to efficiently generate test (and\nground truth) volume data, optionally even in a streaming fashion. As the\nproposed nested sweeps algorithm is fast, it can be used to generate test data\non demand. We analyze the asymptotic run time of the presented algorithm and\ncompare it experimentally to alternative approaches as well as a hypothetical\nbest-case baseline method. In a case study, the framework is applied to the\npopular VascuSynth software for vascular image generation, making it capable of\nefficiently producing larger-than-main memory volumes which is demonstrated by\ngenerating a trillion voxel (1TB) image. Implementations of the presented\nframework are available online in the form of the modified version of\nVascusynth and the code used for the experimental evaluation. In addition, the\ntest data generation procedure has been integrated into the popular volume\nrendering and processing framework Voreen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drees_D/0/1/0/all/0/1\">Dominik Drees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding. (arXiv:2112.10728v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.10728","description":"<p>Recently, there has been an increasing interest in building question\nanswering (QA) models that reason across multiple modalities, such as text and\nimages. However, QA using images is often limited to just picking the answer\nfrom a pre-defined set of options. In addition, images in the real world,\nespecially in news, have objects that are co-referential to the text, with\ncomplementary information from both modalities. In this paper, we present a new\nQA evaluation benchmark with 1,384 questions over news articles that require\ncross-media grounding of objects in images onto text. Specifically, the task\ninvolves multi-hop questions that require reasoning over image-caption pairs to\nidentify the grounded visual object being referred to and then predicting a\nspan from the news body text to answer the question. In addition, we introduce\na novel multimedia data augmentation framework, based on cross-media knowledge\nextraction and synthetic question-answer generation, to automatically augment\ndata that can provide weak supervision for this task. We evaluate both\npipeline-based and end-to-end pretraining-based multimedia QA models on our\nbenchmark, and show that they achieve promising performance, while considerably\nlagging behind human performance hence leaving large room for future work on\nthis challenging new task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1\">Xilin Rui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Haoyang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for brain metastasis detection and segmentation in longitudinal MRI data. (arXiv:2112.11833v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.11833","description":"<p>Brain metastases occur frequently in patients with metastatic cancer. Early\nand accurate detection of brain metastases is very essential for treatment\nplanning and prognosis in radiation therapy. To improve brain metastasis\ndetection performance with deep learning, a custom detection loss called\nvolume-level sensitivity-specificity (VSS) is proposed, which rates individual\nmetastasis detection sensitivity and specificity in (sub-)volume levels. As\nsensitivity and precision are always a trade-off in a metastasis level, either\na high sensitivity or a high precision can be achieved by adjusting the weights\nin the VSS loss without decline in dice score coefficient for segmented\nmetastases. To reduce metastasis-like structures being detected as false\npositive metastases, a temporal prior volume is proposed as an additional input\nof DeepMedic. The modified network is called DeepMedic+ for distinction. Our\nproposed VSS loss improves the sensitivity of brain metastasis detection for\nDeepMedic, increasing the sensitivity from 85.3% to 97.5%. Alternatively, it\nimproves the precision from 69.1% to 98.7%. Comparing DeepMedic+ with DeepMedic\nwith the same VSS loss, 44.4% of the false positive metastases are reduced in\nthe high sensitivity model and the precision reaches 99.6% for the high\nspecificity model. The mean dice coefficient for all metastases is about 0.81.\nWith the ensemble of the high sensitivity and high specificity models, on\naverage only 1.5 false positive metastases per patient needs further check,\nwhile the majority of true positive metastases are confirmed. The ensemble\nlearning is able to distinguish high confidence true positive metastases from\nmetastases candidates that require special expert review or further follow-up,\nbeing particularly well-fit to the requirements of expert support in real\nclinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bert_C/0/1/0/all/0/1\">Christoph Bert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sommer_P/0/1/0/all/0/1\">Philipp Sommer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frey_B/0/1/0/all/0/1\">Benjamin Frey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaipl_U/0/1/0/all/0/1\">Udo Gaipl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Distel_L/0/1/0/all/0/1\">Luitpold V. Distel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weissmann_T/0/1/0/all/0/1\">Thomas Weissmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uder_M/0/1/0/all/0/1\">Michael Uder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_M/0/1/0/all/0/1\">Manuel A. Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorfler_A/0/1/0/all/0/1\">Arnd D&#xf6;rfler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fietkau_R/0/1/0/all/0/1\">Rainer Fietkau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putz_F/0/1/0/all/0/1\">Florian Putz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. (arXiv:2201.05989v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05989","description":"<p>Neural graphics primitives, parameterized by fully connected neural networks,\ncan be costly to train and evaluate. We reduce this cost with a versatile new\ninput encoding that permits the use of a smaller network without sacrificing\nquality, thus significantly reducing the number of floating point and memory\naccess operations: a small neural network is augmented by a multiresolution\nhash table of trainable feature vectors whose values are optimized through\nstochastic gradient descent. The multiresolution structure allows the network\nto disambiguate hash collisions, making for a simple architecture that is\ntrivial to parallelize on modern GPUs. We leverage this parallelism by\nimplementing the whole system using fully-fused CUDA kernels with a focus on\nminimizing wasted bandwidth and compute operations. We achieve a combined\nspeedup of several orders of magnitude, enabling training of high-quality\nneural graphics primitives in a matter of seconds, and rendering in tens of\nmilliseconds at a resolution of ${1920\\!\\times\\!1080}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_A/0/1/0/all/0/1\">Alex Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schied_C/0/1/0/all/0/1\">Christoph Schied</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1\">Alexander Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Backdoor Attacks on Visual Object Tracking. (arXiv:2201.13178v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13178","description":"<p>Visual object tracking (VOT) has been widely adopted in mission-critical\napplications, such as autonomous driving and intelligent surveillance systems.\nIn current practice, third-party resources such as datasets, backbone networks,\nand training platforms are frequently used to train high-performance VOT\nmodels. Whilst these resources bring certain convenience, they also introduce\nnew security threats into VOT models. In this paper, we reveal such a threat\nwhere an adversary can easily implant hidden backdoors into VOT models by\ntempering with the training process. Specifically, we propose a simple yet\neffective few-shot backdoor attack (FSBA) that optimizes two losses\nalternately: 1) a \\emph{feature loss} defined in the hidden feature space, and\n2) the standard \\emph{tracking loss}. We show that, once the backdoor is\nembedded into the target model by our FSBA, it can trick the model to lose\ntrack of specific objects even when the \\emph{trigger} only appears in one or a\nfew frames. We examine our attack in both digital and physical-world settings\nand show that it can significantly degrade the performance of state-of-the-art\nVOT trackers. We also show that our attack is resistant to potential defenses,\nhighlighting the vulnerability of VOT models to potential backdoor attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Haoxiang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04203","description":"<p>A long-standing goal of intelligent assistants such as AR glasses/robots has\nbeen to assist users in affordance-centric real-world scenarios, such as \"how\ncan I run the microwave for 1 minute?\". However, there is still no clear task\ndefinition and suitable benchmarks. In this paper, we define a new task called\nAffordance-centric Question-driven Task Completion, where the AI assistant\nshould learn from instructional videos and scripts to guide the user\nstep-by-step. To support the task, we constructed AssistQ, a new dataset\ncomprising 531 question-answer samples derived from 100 newly filmed\nfirst-person videos. Each question should be completed with multi-step\nguidances by inferring from visual details (e.g., buttons' position) and\ntextural details (e.g., actions like press/turn). To address this unique task,\nwe developed a Question-to-Actions (Q2A) model that significantly outperforms\nseveral baseline methods while still having large room for improvement. We\nexpect our task and dataset to advance Egocentric AI Assistant's development.\nOur project page is available at: https://showlab.github.io/assistq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_B/0/1/0/all/0/1\">Benita Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Joya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1\">Dongxing Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDsrv -- visual sharing and analysis of molecular dynamics simulations. (arXiv:2203.13658v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13658","description":"<p>Molecular dynamics simulation is a proven technique for computing and\nvisualizing the time-resolved motion of macromolecules at atomic resolution.\nThe MDsrv is a tool that streams MD trajectories and displays them\ninteractively in web browsers without requiring advanced skills, facilitating\ninteractive exploration and collaborative visual analysis. We have now enhanced\nthe MDsrv to further simplify the upload and sharing of MD trajectories and\nimprove their online viewing and analysis. With the new instance, the MDsrv\nsimplifies the creation of sessions, which allows the exchange of MD\ntrajectories with preset representations and perspectives. An important\ninnovation is that the MDsrv can now access and visualize trajectories from\nremote datasets, which greatly expands its applicability and use, as the data\nno longer needs to be accessible on a local server. In addition, initial\nanalyses such as sequence or structure alignments, distance measurements, or\nRMSD calculations have been implemented, which optionally support visual\nanalysis. Finally, the MDsrv now offers a faster and more efficient\nvisualization of even large trajectories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kampfrath_M/0/1/0/all/0/1\">Michelle Kampfrath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staritzbichler_R/0/1/0/all/0/1\">Ren&#xe9; Staritzbichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_G/0/1/0/all/0/1\">Guillermo P&#xe9;rez Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_A/0/1/0/all/0/1\">Alexander S. Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiemann_J/0/1/0/all/0/1\">Johanna K.S. Tiemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheuermann_G/0/1/0/all/0/1\">Gerik Scheuermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_D/0/1/0/all/0/1\">Daniel Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildebrand_P/0/1/0/all/0/1\">Peter W. Hildebrand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSDF: Real-Time Neural Signed Distance Fields for Robot Perception. (arXiv:2204.02296v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.02296","description":"<p>We present iSDF, a continual learning system for real-time signed distance\nfield (SDF) reconstruction. Given a stream of posed depth images from a moving\ncamera, it trains a randomly initialised neural network to map input 3D\ncoordinate to approximate signed distance. The model is self-supervised by\nminimising a loss that bounds the predicted signed distance using the distance\nto the closest sampled point in a batch of query points that are actively\nsampled. In contrast to prior work based on voxel grids, our neural method is\nable to provide adaptive levels of detail with plausible filling in of\npartially observed regions and denoising of observations, all while having a\nmore compact representation. In evaluations against alternative methods on real\nand synthetic datasets of indoor environments, we find that iSDF produces more\naccurate reconstructions, and better approximations of collision costs and\ngradients useful for downstream planners in domains from navigation to\nmanipulation. Code and video results can be found at our project page:\nhttps://joeaortiz.github.io/iSDF/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1\">Alexander Clegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucar_E/0/1/0/all/0/1\">Edgar Sucar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastMapSVM: Classifying Complex Objects Using the FastMap Algorithm and Support-Vector Machines. (arXiv:2204.05112v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05112","description":"<p>Neural Networks and related Deep Learning methods are currently at the\nleading edge of technologies used for classifying objects. However, they\ngenerally demand large amounts of time and data for model training; and their\nlearned models can sometimes be difficult to interpret. In this paper, we\nre-introduce FastMapSVM, an interpretable Machine Learning framework for\nclassifying complex objects. FastMapSVM combines the strengths of FastMap and\nSupport-Vector Machines. FastMap is an efficient linear-time algorithm that\nmaps complex objects to points in a Euclidean space, while preserving pairwise\nnon-Euclidean distances between them. We demonstrate the efficiency and\neffectiveness of FastMapSVM in the context of classifying seismograms. We show\nthat its performance, in terms of precision, recall, and accuracy, is\ncomparable to that of other state-of-the-art methods. However, compared to\nother methods, FastMapSVM uses significantly smaller amounts of time and data\nfor model training. It also provides a perspicuous visualization of the objects\nand the classification boundaries between them. We expect FastMapSVM to be\nviable for classification tasks in many other real-world domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Malcolm C. A. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Kushal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_T/0/1/0/all/0/1\">T. K. Satish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakata_N/0/1/0/all/0/1\">Nori Nakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency. (arXiv:2204.10310v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10310","description":"<p>Approaches to single-view reconstruction typically rely on viewpoint\nannotations, silhouettes, the absence of background, multiple views of the same\ninstance, a template shape, or symmetry. We avoid all of these supervisions and\nhypotheses by leveraging explicitly the consistency between images of different\nobject instances. As a result, our method can learn from large collections of\nunlabelled images depicting the same object category. Our main contributions\nare two approaches to leverage cross-instance consistency: (i) progressive\nconditioning, a training strategy to gradually specialize the model from\ncategory to instances in a curriculum learning fashion; (ii) swap\nreconstruction, a loss enforcing consistency between instances having similar\nshape or texture. Critical to the success of our method are also: our\nstructured autoencoding architecture decomposing an image into explicit shape,\ntexture, pose, and background; an adapted formulation of differential\nrendering, and; a new optimization scheme alternating between 3D and pose\nlearning. We compare our approach, UNICORN, both on the diverse synthetic\nShapeNet dataset - the classical benchmark for methods requiring multiple views\nas supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB-200)\nfor which most methods require known templates and silhouette annotations. We\nalso showcase applicability to more challenging real-world collections\n(CompCars, LSUN), where silhouettes are not available and images are not\ncropped around the object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monnier_T/0/1/0/all/0/1\">Tom Monnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamic View Synthesis With Few RGBD Cameras. (arXiv:2204.10477v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10477","description":"<p>There have been significant advancements in dynamic novel view synthesis in\nrecent years. However, current deep learning models often require (1) prior\nmodels (e.g., SMPL human models), (2) heavy pre-processing, or (3) per-scene\noptimization. We propose to utilize RGBD cameras to remove these limitations\nand synthesize free-viewpoint videos of dynamic indoor scenes. We generate\nfeature point clouds from RGBD frames and then render them into free-viewpoint\nvideos via a neural renderer. However, the inaccurate, unstable, and incomplete\ndepth measurements induce severe distortions, flickering, and ghosting\nartifacts. We enforce spatial-temporal consistency via the proposed Cycle\nReconstruction Consistency and Temporal Stabilization module to reduce these\nartifacts. We introduce a simple Regional Depth-Inpainting module that\nadaptively inpaints missing depth values to render complete novel views.\nAdditionally, we present a Human-Things Interactions dataset to validate our\napproach and facilitate future research. The dataset consists of 43 multi-view\nRGBD video sequences of everyday activities, capturing complex interactions\nbetween human subjects and their surroundings. Experiments on the HTI dataset\nshow that our method outperforms the baseline per-frame image fidelity and\nspatial-temporal consistency. We will release our code, and the dataset on the\nwebsite soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">YoungJoong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+State_A/0/1/0/all/0/1\">Andrei State</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_H/0/1/0/all/0/1\">Henry Fuchs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Image Captioning. (arXiv:2204.13324v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13324","description":"<p>State-of-the-art image captioners can generate accurate sentences to describe\nimages in a sequence to sequence manner without considering the controllability\nand interpretability. This, however, is far from making image captioning widely\nused as an image can be interpreted in infinite ways depending on the target\nand the context at hand. Achieving controllability is important especially when\nthe image captioner is used by different people with different way of\ninterpreting the images. In this paper, we introduce a novel framework for\nimage captioning which can generate diverse descriptions by capturing the\nco-dependence between Part-Of-Speech tags and semantics. Our model decouples\ndirect dependence between successive variables. In this way, it allows the\ndecoder to exhaustively search through the latent Part-Of-Speech choices, while\nkeeping decoding speed proportional to the size of the POS vocabulary. Given a\ncontrol signal in the form of a sequence of Part-Of-Speech tags, we propose a\nmethod to generate captions through a Transformer network, which predicts words\nbased on the input Part-Of-Speech tag sequences. Experiments on publicly\navailable datasets show that our model significantly outperforms\nstate-of-the-art methods on generating diverse image captions with high\nqualities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maxwell_L/0/1/0/all/0/1\">Luka Maxwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual networks based 3D Multi-Person Pose Estimation from Monocular Video. (arXiv:2205.00748v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00748","description":"<p>Monocular 3D human pose estimation has made progress in recent years. Most of\nthe methods focus on single persons, which estimate the poses in the\nperson-centric coordinates, i.e., the coordinates based on the center of the\ntarget person. Hence, these methods are inapplicable for multi-person 3D pose\nestimation, where the absolute coordinates (e.g., the camera coordinates) are\nrequired. Moreover, multi-person pose estimation is more challenging than\nsingle pose estimation, due to inter-person occlusion and close human\ninteractions. Existing top-down multi-person methods rely on human detection\n(i.e., top-down approach), and thus suffer from the detection errors and cannot\nproduce reliable pose estimation in multi-person scenes. Meanwhile, existing\nbottom-up methods that do not use human detection are not affected by detection\nerrors, but since they process all persons in a scene at once, they are prone\nto errors, particularly for persons in small scales. To address all these\nchallenges, we propose the integration of top-down and bottom-up approaches to\nexploit their strengths. Our top-down network estimates human joints from all\npersons instead of one in an image patch, making it robust to possible\nerroneous bounding boxes. Our bottom-up network incorporates human-detection\nbased normalized heatmaps, allowing the network to be more robust in handling\nscale variations. Finally, the estimated 3D poses from the top-down and\nbottom-up networks are fed into our integration network for final 3D poses. To\naddress the common gaps between training and testing data, we do optimization\nduring the test time, by refining the estimated 3D human poses using high-order\ntemporal constraint, re-projection loss, and bone length regularizations. Our\nevaluations demonstrate the effectiveness of the proposed method. Code and\nmodels are available: https://github.com/3dpose/3D-Multi-Person-Pose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Robby T. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding CNNs from excitations. (arXiv:2205.00932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00932","description":"<p>For instance-level explanation, in order to reveal the relations between\nhigh-level semantics and detailed spatial information, this paper proposes a\nnovel cognitive approach to neural networks, which named PANE. Under the\nguidance of PANE, a novel saliency map representation method, named IOM, is\nproposed for CNN-like models. We make the comparison with eight\nstate-of-the-art saliency map representation methods. The experimental results\nshow that IOM far outperforms baselines. The work of this paper may bring a new\nperspective to understand deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zijian Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qianmu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhichao Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HL-Net: Heterophily Learning Network for Scene Graph Generation. (arXiv:2205.01316v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01316","description":"<p>Scene graph generation (SGG) aims to detect objects and predict their\npairwise relationships within an image. Current SGG methods typically utilize\ngraph neural networks (GNNs) to acquire context information between\nobjects/relationships. Despite their effectiveness, however, current SGG\nmethods only assume scene graph homophily while ignoring heterophily.\nAccordingly, in this paper, we propose a novel Heterophily Learning Network\n(HL-Net) to comprehensively explore the homophily and heterophily between\nobjects/relationships in scene graphs. More specifically, HL-Net comprises the\nfollowing 1) an adaptive reweighting transformer module, which adaptively\nintegrates the information from different layers to exploit both the\nheterophily and homophily in objects; 2) a relationship feature propagation\nmodule that efficiently explores the connections between relationships by\nconsidering heterophily in order to refine the relationship representation; 3)\na heterophily-aware message-passing scheme to further distinguish the\nheterophily and homophily between objects/relationships, thereby facilitating\nimproved message passing in graphs. We conducted extensive experiments on two\npublic datasets: Visual Genome (VG) and Open Images (OI). The experimental\nresults demonstrate the superiority of our proposed HL-Net over existing\nstate-of-the-art approaches. In more detail, HL-Net outperforms the second-best\ncompetitors by 2.1$\\%$ on the VG dataset for scene graph classification and\n1.2$\\%$ on the IO dataset for the final score. Code is available at\nhttps://github.com/siml3/HL-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network. (arXiv:2205.01550v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01550","description":"<p>Point clouds have the characteristics of disorder, unstructured and\nsparseness.Aiming at the problem of the non-structural nature of point clouds,\nthanks to the excellent performance of convolutional neural networks in image\nprocessing, one of the solutions is to extract features from point clouds based\non two-dimensional convolutional neural networks. The three-dimensional\ninformation carried in the point cloud can be converted to two-dimensional, and\nthen processed by a two-dimensional convolutional neural network, and finally\nback-projected to three-dimensional.In the process of projecting 3D information\nto 2D and back-projection, certain information loss will inevitably be caused\nto the point cloud and category inconsistency will be introduced in the\nback-projection stage;Another solution is the voxel-based point cloud\nsegmentation method, which divides the point cloud into small grids one by\none.However, the point cloud is sparse, and the direct use of 3D convolutional\nneural network inevitably wastes computing resources. In this paper, we propose\na feature extraction module based on multi-scale ultra-sparse convolution and a\nfeature selection module based on channel attention, and build a point cloud\nsegmentation network framework based on this.By introducing multi-scale sparse\nconvolution, network could capture richer feature information based on\nconvolution kernels of different sizes, improving the segmentation result of\npoint cloud segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yunzheng Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}