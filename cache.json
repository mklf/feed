{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-08-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Eye Gaze Estimation Model Analysis. (arXiv:2207.14373v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14373","description":"<p>We explore techniques for eye gaze estimation using machine learning. Eye\ngaze estimation is a common problem for various behavior analysis and\nhuman-computer interfaces. The purpose of this work is to discuss various model\ntypes for eye gaze estimation and present the results from predicting gaze\ndirection using eye landmarks in unconstrained settings. In unconstrained\nreal-world settings, feature-based and model-based methods are outperformed by\nrecent appearance-based methods due to factors like illumination changes and\nother visual artifacts. We discuss a learning-based method for eye region\nlandmark localization trained exclusively on synthetic data. We discuss how to\nuse detected landmarks as input to iterative model-fitting and lightweight\nlearning-based gaze estimation methods and how to use the model for\nperson-independent and personalized gaze estimations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kottwani_A/0/1/0/all/0/1\">Aveena Kottwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14382","description":"<p>Large Language Models (LLMs) have been transformative. They are pre-trained\nfoundational models that can be adapted with fine tuning to many different\nnatural language tasks, each of which previously would have required a separate\nnetwork model. This is one step closer to the extraordinary versatility of\nhuman language. GPT-3 and more recently LaMDA can carry on dialogs with humans\non many topics after minimal priming with a few examples. However, there has\nbeen a wide range of reactions on whether these LLMs understand what they are\nsaying or exhibit signs of intelligence. This high variance is exhibited in\nthree interviews with LLMs reaching wildly different conclusions. A new\npossibility was uncovered that could explain this divergence. What appears to\nbe intelligence in LLMs may in fact be a mirror that reflects the intelligence\nof the interviewer, a remarkable twist that could be considered a Reverse\nTuring Test. If so, then by studying interviews we may be learning more about\nthe intelligence and beliefs of the interviewer than the intelligence of the\nLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sejnowski_T/0/1/0/all/0/1\">Terrence Sejnowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Model Finetuning for Text Classification via Data Filtering. (arXiv:2207.14386v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14386","description":"<p>As model finetuning is central to the modern NLP, we set to maximize its\nefficiency. Motivated by training examples are often redundant, we design an\nalgorithm that filters the examples in a streaming fashion. Our key techniques\nare two: (1) automatically determine a training loss threshold for skipping the\nbackward propagation; and (2) maintain a meta predictor for further skipping\nthe forward propagation. Incarnated as a three-stage process, on a diverse set\nof benchmarks our algorithm reduces the required training examples by up to\n5$\\times$ while only seeing minor degradation on average. Our method is\neffective even for as few as one training epoch, where each training example is\nencountered once. It is simple to implement and is compatible with the existing\nmodel finetuning optimizations such as layer freezing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xu Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_S/0/1/0/all/0/1\">Shahina Mohd Azam Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Felix Xiaozhu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAD: Language Models as Data for Zero-Shot Dialog. (arXiv:2207.14393v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14393","description":"<p>To facilitate zero-shot generalization in taskoriented dialog, this paper\nproposes Language Models as Data (LAD). LAD is a paradigm for creating diverse\nand accurate synthetic data which conveys the necessary structural constraints\nand can be used to train a downstream neural dialog model. LAD leverages GPT-3\nto induce linguistic diversity. LAD achieves significant performance gains in\nzero-shot settings on intent prediction (+15%), slot filling (+31.4 F-1) and\nnext action prediction (+11 F1). Furthermore, an interactive human evaluation\nshows that training with LAD is competitive with training on human dialogs. LAD\nis open-sourced, with the code and data available at\nhttps://github.com/Shikib/lad.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altun_Y/0/1/0/all/0/1\">Yasemin Altun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Evaluation of Dialog Track at DSTC9. (arXiv:2207.14403v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14403","description":"<p>The ultimate goal of dialog research is to develop systems that can be\neffectively used in interactive settings by real users. To this end, we\nintroduced the Interactive Evaluation of Dialog Track at the 9th Dialog System\nTechnology Challenge. This track consisted of two sub-tasks. The first sub-task\ninvolved building knowledge-grounded response generation models. The second\nsub-task aimed to extend dialog models beyond static datasets by assessing them\nin an interactive setting with real users. Our track challenges participants to\ndevelop strong response generation models and explore strategies that extend\nthem to back-and-forth interactions with real users. The progression from\nstatic corpora to interactive evaluation introduces unique challenges and\nfacilitates a more thorough assessment of open-domain dialog systems. This\npaper provides an overview of the track, including the methodology and results.\nFurthermore, it provides insights into how to best evaluate open-domain dialog\nmodels\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yulan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_C/0/1/0/all/0/1\">Carla Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alavi_S/0/1/0/all/0/1\">Seyed Hossein Alavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traum_D/0/1/0/all/0/1\">David Traum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Specific Wav2vec 2.0 Fine-tuning For The SE&R 2022 Challenge. (arXiv:2207.14418v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14418","description":"<p>This paper presents our efforts to build a robust ASR model for the shared\ntask Automatic Speech Recognition for spontaneous and prepared speech &amp; Speech\nEmotion Recognition in Portuguese (SE&amp;R 2022). The goal of the challenge is to\nadvance the ASR research for the Portuguese language, considering prepared and\nspontaneous speech in different dialects. Our method consist on fine-tuning an\nASR model in a domain-specific approach, applying gain normalization and\nselective noise insertion. The proposed method improved over the strong\nbaseline provided on the test set in 3 of the 4 tracks available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_A/0/1/0/all/0/1\">Alef Iury Siqueira Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_G/0/1/0/all/0/1\">Gustavo dos Reis Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Comment Inconsistency Detection with BERT and Longformer. (arXiv:2207.14444v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14444","description":"<p>Comments, or natural language descriptions of source code, are standard\npractice among software developers. By communicating important aspects of the\ncode such as functionality and usage, comments help with software project\nmaintenance. However, when the code is modified without an accompanying\ncorrection to the comment, an inconsistency between the comment and code can\narise, which opens up the possibility for developer confusion and bugs. In this\npaper, we propose two models based on BERT (Devlin et al., 2019) and Longformer\n(Beltagy et al., 2020) to detect such inconsistencies in a natural language\ninference (NLI) context. Through an evaluation on a previously established\ncorpus of comment-method pairs both during and after code changes, we\ndemonstrate that our models outperform multiple baselines and yield comparable\nresults to the state-of-the-art models that exclude linguistic and lexical\nfeatures. We further discuss ideas for future research in using pretrained\nlanguage models for both inconsistency detection and automatic comment\nupdating.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steiner_T/0/1/0/all/0/1\">Theo Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GTrans: Grouping and Fusing Transformer Layers for Neural Machine Translation. (arXiv:2207.14467v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14467","description":"<p>Transformer structure, stacked by a sequence of encoder and decoder network\nlayers, achieves significant development in neural machine translation.\nHowever, vanilla Transformer mainly exploits the top-layer representation,\nassuming the lower layers provide trivial or redundant information and thus\nignoring the bottom-layer feature that is potentially valuable. In this work,\nwe propose the Group-Transformer model (GTrans) that flexibly divides\nmulti-layer representations of both encoder and decoder into different groups\nand then fuses these group features to generate target words. To corroborate\nthe effectiveness of the proposed method, extensive experiments and analytic\nexperiments are conducted on three bilingual translation benchmarks and two\nmultilingual translation tasks, including the IWLST-14, IWLST-17, LDC, WMT-14\nand OPUS-100 benchmark. Experimental and analytical results demonstrate that\nour model outperforms its Transformer counterparts by a consistent gain.\nFurthermore, it can be successfully scaled up to 60 encoder layers and 36\ndecoder layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Azerbaijani Neural Machine Translation. (arXiv:2207.14473v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14473","description":"<p>Little research has been done on Neural Machine Translation (NMT) for\nAzerbaijani. In this paper, we benchmark the performance of Azerbaijani-English\nNMT systems on a range of techniques and datasets. We evaluate which\nsegmentation techniques work best on Azerbaijani translation and benchmark the\nperformance of Azerbaijani NMT models across several domains of text. Our\nresults show that while Unigram segmentation improves NMT performance and\nAzerbaijani translation models scale better with dataset quality than quantity,\ncross-domain generalization remains a challenge\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chih-Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum Learning for Data-Efficient Vision-Language Alignment. (arXiv:2207.14525v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14525","description":"<p>Aligning image and text encoders from scratch using contrastive learning\nrequires large amounts of paired image-text data. We alleviate this need by\naligning individually pre-trained language and vision representation models\nusing a much smaller amount of paired data, augmented with a curriculum\nlearning algorithm to learn fine-grained vision-language alignments. TOnICS\n(Training with Ontology-Informed Contrastive Sampling) initially samples\nminibatches whose image-text pairs contain a wide variety of objects to learn\nobject-level alignment, and progressively samples minibatches where all\nimage-text pairs contain the same object to learn finer-grained contextual\nalignment. Aligning pre-trained BERT and VinVL models to each other using\nTOnICS outperforms CLIP on downstream zero-shot image retrieval while using\nless than 1% as much training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SERCNN: Stacked Embedding Recurrent Convolutional Neural Network in Detecting Depression on Twitter. (arXiv:2207.14535v1 [cs.AI])","link":"http://arxiv.org/abs/2207.14535","description":"<p>Conventional approaches to identify depression are not scalable, and the\npublic has limited awareness of mental health, especially in developing\ncountries. As evident by recent studies, social media has the potential to\ncomplement mental health screening on a greater scale. The vast amount of\nfirst-person narrative posts in chronological order can provide insights into\none's thoughts, feelings, behavior, or mood for some time, enabling a better\nunderstanding of depression symptoms reflected in the online space. In this\npaper, we propose SERCNN, which improves the user representation by (1)\nstacking two pretrained embeddings from different domains and (2) reintroducing\nthe embedding context to the MLP classifier. Our SERCNN shows great performance\nover state-of-the-art and other baselines, achieving 93.7% accuracy in a 5-fold\ncross-validation setting. Since not all users share the same level of online\nactivity, we introduced the concept of a fixed observation window that\nquantifies the observation period in a predefined number of posts. With as\nminimal as 10 posts per user, SERCNN performed exceptionally well with an 87%\naccuracy, which is on par with the BERT model, while having 98% less in the\nnumber of parameters. Our findings open up a promising direction for detecting\ndepression on social media with a smaller number of posts for inference,\ntowards creating solutions for a cost-effective and timely intervention. We\nhope that our work can bring this research area closer to real-world adoption\nin existing clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_H/0/1/0/all/0/1\">Heng Ee Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1\">Mei Kuan Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_C/0/1/0/all/0/1\">Chun Yong Chong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Phone Recognition from Unpaired Audio and Phone Sequences Based on Generative Adversarial Network. (arXiv:2207.14568v1 [cs.SD])","link":"http://arxiv.org/abs/2207.14568","description":"<p>ASR has been shown to achieve great performance recently. However, most of\nthem rely on massive paired data, which is not feasible for low-resource\nlanguages worldwide. This paper investigates how to learn directly from\nunpaired phone sequences and speech utterances. We design a two-stage iterative\nframework. GAN training is adopted in the first stage to find the mapping\nrelationship between unpaired speech and phone sequence. In the second stage,\nanother HMM model is introduced to train from the generator's output, which\nboosts the performance and provides a better segmentation for the next\niteration. In the experiment, we first investigate different choices of model\ndesigns. Then we compare the framework to different types of baselines: (i)\nsupervised methods (ii) acoustic unit discovery based methods (iii) methods\nlearning from unpaired data. Our framework performs consistently better than\nall acoustic unit discovery methods and previous methods learning from unpaired\ndata based on the TIMIT dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Da-rong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_P/0/1/0/all/0/1\">Po-chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sung-feng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Da-yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pronunciation-aware unique character encoding for RNN Transducer-based Mandarin speech recognition. (arXiv:2207.14578v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14578","description":"<p>For Mandarin end-to-end (E2E) automatic speech recognition (ASR) tasks,\ncompared to character-based modeling units, pronunciation-based modeling units\ncould improve the sharing of modeling units in model training but meet\nhomophone problems. In this study, we propose to use a novel\npronunciation-aware unique character encoding for building E2E RNN-T-based\nMandarin ASR systems. The proposed encoding is a combination of\npronunciation-base syllable and character index (CI). By introducing the CI,\nthe RNN-T model can overcome the homophone problem while utilizing the\npronunciation information for extracting modeling units. With the proposed\nencoding, the model outputs can be converted into the final recognition result\nthrough a one-to-one mapping. We conducted experiments on Aishell and MagicData\ndatasets, and the experimental results showed the effectiveness of the proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xugang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawai_H/0/1/0/all/0/1\">Hisashi Kawai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-NSF: Knowledge Graph Completion with a Negative-Sample-Free Approach. (arXiv:2207.14617v1 [cs.LG])","link":"http://arxiv.org/abs/2207.14617","description":"<p>Knowledge Graph (KG) completion is an important task that greatly benefits\nknowledge discovery in many fields (e.g. biomedical research). In recent years,\nlearning KG embeddings to perform this task has received considerable\nattention. Despite the success of KG embedding methods, they predominantly use\nnegative sampling, resulting in increased computational complexity as well as\nbiased predictions due to the closed world assumption. To overcome these\nlimitations, we propose \\textbf{KG-NSF}, a negative sampling-free framework for\nlearning KG embeddings based on the cross-correlation matrices of embedding\nvectors. It is shown that the proposed method achieves comparable link\nprediction performance to negative sampling-based methods while converging much\nfaster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahaj_A/0/1/0/all/0/1\">Adil Bahaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhazmir_S/0/1/0/all/0/1\">Safae Lhazmir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghogho_M/0/1/0/all/0/1\">Mounir Ghogho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking. (arXiv:2207.14627v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14627","description":"<p>While communicating with a user, a task-oriented dialogue system has to track\nthe user's needs at each turn according to the conversation history. This\nprocess called dialogue state tracking (DST) is crucial because it directly\ninforms the downstream dialogue policy. DST has received a lot of interest in\nrecent years with the text-to-text paradigm emerging as the favored approach.\nIn this review paper, we first present the task and its associated datasets.\nThen, considering a large number of recent publications, we identify highlights\nand advances of research in 2021-2022. Although neural approaches have enabled\nsignificant progress, we argue that some critical aspects of dialogue systems\nsuch as generalizability are still underexplored. To motivate future studies,\nwe propose several research avenues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacqmin_L/0/1/0/all/0/1\">L&#xe9;o Jacqmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina M. Rojas-Barahona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favre_B/0/1/0/all/0/1\">Benoit Favre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Spam Reviews on Vietnamese E-commerce Websites. (arXiv:2207.14636v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14636","description":"<p>The reviews of customers play an essential role in online shopping. People\noften refer to reviews or comments of previous customers to decide whether to\nbuy a new product. Catching up with this behavior, some people create untruths\nand illegitimate reviews to hoax customers about the fake quality of products.\nThese reviews are called spam reviews, which confuse consumers on online\nshopping platforms and negatively affect online shopping behaviors. We propose\nthe dataset called ViSpamReviews, which has a strict annotation procedure for\ndetecting spam reviews on e-commerce platforms. Our dataset consists of two\ntasks: the binary classification task for detecting whether a review is a spam\nor not and the multi-class classification task for identifying the type of\nspam. The PhoBERT obtained the highest results on both tasks, 88.93% and\n72.17%, respectively, by macro average F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_C/0/1/0/all/0/1\">Co Van Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Relation of User and News Representations in Content-Based Neural News Recommendation. (arXiv:2207.14704v1 [cs.IR])","link":"http://arxiv.org/abs/2207.14704","description":"<p>A number of models for neural content-based news recommendation have been\nproposed. However, there is limited understanding of the relative importances\nof the three main components of such systems (news encoder, user encoder, and\nscoring function) and the trade-offs involved. In this paper, we assess the\nhypothesis that the most widely used means of matching user and candidate news\nrepresentations is not expressive enough. We allow our system to model more\ncomplex relations between the two by assessing more expressive scoring\nfunctions. Across a wide range of baseline and established systems this results\nin consistent improvements of around 6 points in AUC. Our results also indicate\na trade-off between the complexity of news encoder and scoring function: A\nfairly simple baseline model scores well above 68% AUC on the MIND dataset and\ncomes within 2 points of the published state-of-the-art, while requiring a\nfraction of the computational costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moller_L/0/1/0/all/0/1\">Lucas M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple-hypothesis RNN-T Loss for Unsupervised Fine-tuning and Self-training of Neural Transducer. (arXiv:2207.14736v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14736","description":"<p>This paper proposes a new approach to perform unsupervised fine-tuning and\nself-training using unlabeled speech data for recurrent neural network\n(RNN)-Transducer (RNN-T) end-to-end (E2E) automatic speech recognition (ASR)\nsystems. Conventional systems perform fine-tuning/self-training using ASR\nhypothesis as the targets when using unlabeled audio data and are susceptible\nto the ASR performance of the base model. Here in order to alleviate the\ninfluence of ASR errors while using unlabeled data, we propose a\nmultiple-hypothesis RNN-T loss that incorporates multiple ASR 1-best hypotheses\ninto the loss function. For the fine-tuning task, ASR experiments on\nLibrispeech show that the multiple-hypothesis approach achieves a relative\nreduction of 14.2% word error rate (WER) when compared to the single-hypothesis\napproach, on the test_other set. For the self-training task, ASR models are\ntrained using supervised data from Wall Street Journal (WSJ), Aurora-4 along\nwith CHiME-4 real noisy data as unlabeled data. The multiple-hypothesis\napproach yields a relative reduction of 3.3% WER on the CHiME-4's\nsingle-channel real noisy evaluation set when compared with the\nsingle-hypothesis approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_C/0/1/0/all/0/1\">Cong-Thanh Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rating the Crisis of Online Public Opinion Using a Multi-Level Index System. (arXiv:2207.14740v1 [cs.SI])","link":"http://arxiv.org/abs/2207.14740","description":"<p>Online public opinion usually spreads rapidly and widely, thus a small\nincident probably evolves into a large social crisis in a very short time, and\nresults in a heavy loss in credit or economic aspects. We propose a method to\nrate the crisis of online public opinion based on a multi-level index system to\nevaluate the impact of events objectively. Firstly, the dissemination mechanism\nof online public opinion is explained from the perspective of information\necology. According to the mechanism, some evaluation indexes are selected\nthrough correlation analysis and principal component analysis. Then, a\nclassification model of text emotion is created via the training by deep\nlearning to achieve the accurate quantification of the emotional indexes in the\nindex system. Finally, based on the multi-level evaluation index system and\ngrey correlation analysis, we propose a method to rate the crisis of online\npublic opinion. The experiment with the real-time incident show that this\nmethod can objectively evaluate the emotional tendency of Internet users and\nrate the crisis in different dissemination stages of online public opinion. It\nis helpful to realizing the crisis warning of online public opinion and timely\nblocking the further spread of the crisis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanqi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xixi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval. (arXiv:2207.14757v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14757","description":"<p>Image-text matching is gaining a leading role among tasks involving the joint\nunderstanding of vision and language. In literature, this task is often used as\na pre-training objective to forge architectures able to jointly deal with\nimages and texts. Nonetheless, it has a direct downstream application:\ncross-modal retrieval, which consists in finding images related to a given\nquery text or vice-versa. Solving this task is of critical importance in\ncross-modal search engines. Many recent methods proposed effective solutions to\nthe image-text matching problem, mostly using recent large vision-language (VL)\nTransformer networks. However, these models are often computationally\nexpensive, especially at inference time. This prevents their adoption in\nlarge-scale cross-modal retrieval scenarios, where results should be provided\nto the user almost instantaneously. In this paper, we propose to fill in the\ngap between effectiveness and efficiency by proposing an ALign And DIstill\nNetwork (ALADIN). ALADIN first produces high-effective scores by aligning at\nfine-grained level images and texts. Then, it learns a shared embedding space -\nwhere an efficient kNN search can be performed - by distilling the relevance\nscores obtained from the fine-grained alignments. We obtained remarkable\nresults on MS-COCO, showing that our method can compete with state-of-the-art\nVL Transformers while being almost 90 times faster. The code for reproducing\nour results is available at https://github.com/mesnico/ALADIN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messina_N/0/1/0/all/0/1\">Nicola Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1\">Giuseppe Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilingual Terminology Extraction from Comparable E-Commerce Corpora. (arXiv:2104.07398v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07398","description":"<p>Bilingual terminologies are important machine translation resources in the\nfield of e-commerce, which are usually either manually translated or\nautomatically extracted from parallel data. The human translation is costly and\ne-commerce parallel corpora is very scarce. However, the comparable data in\ndifferent languages in the same commodity field is abundant. In this paper, we\npropose a novel framework of extracting e-commercial bilingual terminologies\nfrom comparable data. Benefiting from the cross-lingual pre-training in\ne-commerce, our framework can make full use of the deep semantic relationship\nbetween source-side terminology and target-side sentence to extract\ncorresponding target terminology. Experimental results on various language\npairs show that our approaches achieve significantly better performance than\nvarious strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Hao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuqin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xiangyu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dive into Deep Learning. (arXiv:2106.11342v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11342","description":"<p>This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech. (arXiv:2111.04040v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2111.04040","description":"<p>Personalizing a speech synthesis system is a highly desired application,\nwhere the system can generate speech with the user's voice with rare enrolled\nrecordings. There are two main approaches to build such a system in recent\nworks: speaker adaptation and speaker encoding. On the one hand, speaker\nadaptation methods fine-tune a trained multi-speaker text-to-speech (TTS) model\nwith few enrolled samples. However, they require at least thousands of\nfine-tuning steps for high-quality adaptation, making it hard to apply on\ndevices. On the other hand, speaker encoding methods encode enrollment\nutterances into a speaker embedding. The trained TTS model can synthesize the\nuser's speech conditioned on the corresponding speaker embedding. Nevertheless,\nthe speaker encoder suffers from the generalization gap between the seen and\nunseen speakers.\n</p>\n<p>In this paper, we propose applying a meta-learning algorithm to the speaker\nadaptation method. More specifically, we use Model Agnostic Meta-Learning\n(MAML) as the training algorithm of a multi-speaker TTS model, which aims to\nfind a great meta-initialization to adapt the model to any few-shot speaker\nadaptation tasks quickly. Therefore, we can also adapt the meta-trained TTS\nmodel to unseen speakers efficiently. Our experiments compare the proposed\nmethod (Meta-TTS) with two baselines: a speaker adaptation method baseline and\na speaker encoding method baseline. The evaluation results show that Meta-TTS\ncan synthesize high speaker-similarity speech from few enrollment samples with\nfewer adaptation steps than the speaker adaptation baseline and outperforms the\nspeaker encoding baseline under the same training scheme. When the speaker\nencoder of the baseline is pre-trained with extra 8371 speakers of data,\nMeta-TTS can still outperform the baseline on LibriTTS dataset and achieve\ncomparable results on VCTK dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sung-Feng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chyi-Jiunn Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Da-Rong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech. (arXiv:2111.10367v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10367","description":"<p>Progress in speech processing has been facilitated by shared datasets and\nbenchmarks. Historically these have focused on automatic speech recognition\n(ASR), speaker identification, or other lower-level tasks. Interest has been\ngrowing in higher-level spoken language understanding tasks, including using\nend-to-end models, but there are fewer annotated datasets for such tasks. At\nthe same time, recent work shows the possibility of pre-training generic\nrepresentations and then fine-tuning for several tasks using relatively little\nlabeled data. We propose to create a suite of benchmark tasks for Spoken\nLanguage Understanding Evaluation (SLUE) consisting of limited-size labeled\ntraining sets and corresponding evaluation sets. This resource would allow the\nresearch community to track progress, evaluate pre-trained representations for\nhigher-level tasks, and study open questions such as the utility of pipeline\nversus end-to-end approaches. We present the first phase of the SLUE benchmark\nsuite, consisting of named entity recognition, sentiment analysis, and ASR on\nthe corresponding datasets. We focus on naturally produced (not read or\nsynthesized) speech, and freely available datasets. We provide new\ntranscriptions and annotations on subsets of the VoxCeleb and VoxPopuli\ndatasets, evaluation metrics and results for baseline models, and an\nopen-source toolkit to reproduce the baselines and evaluate new models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1\">Suwon Shon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brusco_P/0/1/0/all/0/1\">Pablo Brusco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyu J. Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Monolingual End-to-End Automatic Speech Recognition. (arXiv:2112.09427v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2112.09427","description":"<p>Adapting Automatic Speech Recognition (ASR) models to new domains results in\na deterioration of performance on the original domain(s), a phenomenon called\nCatastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to\nnew accents, dialects, topics, etc. without suffering from CF, making them\nunable to be continually enhanced without storing all past data. Fortunately,\nContinual Learning (CL) methods, which aim to enable continual adaptation while\novercoming CF, can be used. In this paper, we implement an extensive number of\nCL methods for End-to-End ASR and test and compare their ability to extend a\nmonolingual Hybrid CTC-Transformer model across four new tasks. We find that\nthe best performing CL method closes the gap between the fine-tuned model\n(lower bound) and the model trained jointly on all tasks (upper bound) by more\nthan 40%, while requiring access to only 0.6% of the original data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eeckt_S/0/1/0/all/0/1\">Steven Vander Eeckt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2203.13366","description":"<p>For a long time, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language can describe almost anything and language grounding\nis a powerful medium to represent various problems or tasks, we present a\nflexible and unified text-to-text paradigm called \"Pretrain, Personalized\nPrompt, and Predict Paradigm\" (P5) for recommendation, which unifies various\nrecommendation tasks in a shared framework. In P5, all data such as user-item\ninteractions, user descriptions, item metadata, and user reviews are converted\nto a common format -- natural language sequences. The rich information from\nnatural language assists P5 to capture deeper semantics for personalization and\nrecommendation. Specifically, P5 learns different tasks with the same language\nmodeling objective during pretraining. Thus, it serves as the foundation model\nfor various downstream recommendation tasks, allows easy integration with other\nmodalities, and enables instruction-based recommendation based on prompts. P5\nadvances recommender systems from shallow model to deep model to big model, and\nwill revolutionize the technical form of recommender systems towards universal\nrecommendation engine. With adaptive personalized prompt for different users,\nP5 is able to make predictions in a zero-shot or few-shot manner and largely\nreduces the necessity for extensive fine-tuning. On several recommendation\nbenchmarks, we conduct experiments to show the effectiveness of P5. We release\nthe source code at \\url{https://github.com/jeykigung/P5}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transducer-based language embedding for spoken language identification. (arXiv:2204.03888v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03888","description":"<p>The acoustic and linguistic features are important cues for the spoken\nlanguage identification (LID) task. Recent advanced LID systems mainly use\nacoustic features that lack the usage of explicit linguistic feature encoding.\nIn this paper, we propose a novel transducer-based language embedding approach\nfor LID tasks by integrating an RNN transducer model into a language embedding\nframework. Benefiting from the advantages of the RNN transducer's linguistic\nrepresentation capability, the proposed method can exploit both\nphonetically-aware acoustic features and explicit linguistic features for LID\ntasks. Experiments were carried out on the large-scale multilingual LibriSpeech\nand VoxLingua107 datasets. Experimental results showed the proposed method\nsignificantly improves the performance on LID tasks with 12% to 59% and 16% to\n24% relative improvement on in-domain and cross-domain datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xugang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawai_H/0/1/0/all/0/1\">Hisashi Kawai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation. (arXiv:2204.03958v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03958","description":"<p>This paper introduces a model for incomplete utterance restoration (IUR)\ncalled JET (\\textbf{J}oint learning token \\textbf{E}xtraction and \\textbf{T}ext\ngeneration). Different from prior studies that only work on extraction or\nabstraction datasets, we design a simple but effective model, working for both\nscenarios of IUR. Our design simulates the nature of IUR, where omitted tokens\nfrom the context contribute to restoration. From this, we construct a Picker\nthat identifies the omitted tokens. To support the picker, we design two label\ncreation methods (soft and hard labels), which can work in cases of no\nannotation data for the omitted tokens. The restoration is done by using a\nGenerator with the help of the Picker on joint learning. Promising results on\nfour benchmark datasets in extraction and abstraction scenarios show that our\nmodel is better than the pretrained T5 and non-generative language model\nmethods in both rich and limited training data settings.\\footnote{The code is\navailable at \\url{https://github.com/shumpei19/JET}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shumpei Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tsungwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_N/0/1/0/all/0/1\">Nguyen Hong Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"FIJO\": a French Insurance Soft Skill Detection Dataset. (arXiv:2204.05208v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05208","description":"<p>Understanding the evolution of job requirements is becoming more important\nfor workers, companies and public organizations to follow the fast\ntransformation of the employment market. Fortunately, recent natural language\nprocessing (NLP) approaches allow for the development of methods to\nautomatically extract information from job ads and recognize skills more\nprecisely. However, these efficient approaches need a large amount of annotated\ndata from the studied domain which is difficult to access, mainly due to\nintellectual property. This article proposes a new public dataset, FIJO,\ncontaining insurance job offers, including many soft skill annotations. To\nunderstand the potential of this dataset, we detail some characteristics and\nsome limitations. Then, we present the results of skill detection algorithms\nusing a named entity recognition approach and show that transformers-based\nmodels have good token-wise performances on this dataset. Lastly, we analyze\nsome errors made by our best model to emphasize the difficulties that may arise\nwhen applying NLP approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beauchemin_D/0/1/0/all/0/1\">David Beauchemin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laumonier_J/0/1/0/all/0/1\">Julien Laumonier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ster_Y/0/1/0/all/0/1\">Yvan Le Ster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yassine_M/0/1/0/all/0/1\">Marouane Yassine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Email Spam Detection Using Hierarchical Attention Hybrid Deep Learning Method. (arXiv:2204.07390v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07390","description":"<p>Email is one of the most widely used ways to communicate, with millions of\npeople and businesses relying on it to communicate and share knowledge and\ninformation on a daily basis. Nevertheless, the rise in email users has\noccurred a dramatic increase in spam emails in recent years. Processing and\nmanaging emails properly for individuals and companies are getting increasingly\ndifficult. This article proposes a novel technique for email spam detection\nthat is based on a combination of convolutional neural networks, gated\nrecurrent units, and attention mechanisms. During system training, the network\nis selectively focused on necessary parts of the email text. The usage of\nconvolution layers to extract more meaningful, abstract, and generalizable\nfeatures by hierarchical representation is the major contribution of this\nstudy. Additionally, this contribution incorporates cross-dataset evaluation,\nwhich enables the generation of more independent performance results from the\nmodel's training dataset. According to cross-dataset evaluation results, the\nproposed technique advances the results of the present attention-based\ntechniques by utilizing temporal convolutions, which give us more flexible\nreceptive field sizes are utilized. The suggested technique's findings are\ncompared to those of state-of-the-art models and show that our approach\noutperforms them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zavrak_S/0/1/0/all/0/1\">Sultan Zavrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_S/0/1/0/all/0/1\">Seyhmus Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter Efficient Diff Pruning for Bias Mitigation. (arXiv:2205.15171v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.15171","description":"<p>In recent years language models have achieved state of the art performance on\na wide variety of natural language processing tasks. As these models are\ncontinuously growing in size it becomes increasingly important to explore\nmethods to make them more storage efficient. At the same time their increase\ncognitive abilities increase the danger that societal bias existing in datasets\nare implicitly encoded in the model weights. We propose an architecture which\ndeals with these two challenges at the same time using two techniques:\nDiffPruning and adversarial Training. The result is a modular architecture\nwhich extends the original DiffPruning setup with and additional sparse\nsubnetwork applied as a mask to diminish the effects of a predefined protected\nattribute at inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauzenberger_L/0/1/0/all/0/1\">Lukas Hauzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Pass Low Latency End-to-End Spoken Language Understanding. (arXiv:2207.06670v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.06670","description":"<p>End-to-end (E2E) models are becoming increasingly popular for spoken language\nunderstanding (SLU) systems and are beginning to achieve competitive\nperformance to pipeline-based approaches. However, recent work has shown that\nthese models struggle to generalize to new phrasings for the same intent\nindicating that models cannot understand the semantic content of the given\nutterance. In this work, we incorporated language models pre-trained on\nunlabeled text data inside E2E-SLU frameworks to build strong semantic\nrepresentations. Incorporating both semantic and acoustic information can\nincrease the inference time, leading to high latency when deployed for\napplications like voice assistants. We developed a 2-pass SLU system that makes\nlow latency prediction using acoustic information from the few seconds of the\naudio in the first pass and makes higher quality prediction in the second pass\nby combining semantic and acoustic representations. We take inspiration from\nprior work on 2-pass end-to-end speech recognition systems that attends on both\naudio and first-pass hypothesis using a deliberation network. The proposed\n2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech\nCommands Challenge Set and SLURP dataset and reduces latency, thus improving\nuser experience. Our code and models are publicly available as part of the\nESPnet-SLU toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowing Where and What: Unified Word Block Pretraining for Document Understanding. (arXiv:2207.13979v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.13979","description":"<p>Due to the complex layouts of documents, it is challenging to extract\ninformation for documents. Most previous studies develop multimodal pre-trained\nmodels in a self-supervised way. In this paper, we focus on the embedding\nlearning of word blocks containing text and layout information, and propose\nUTel, a language model with Unified TExt and Layout pre-training. Specifically,\nwe propose two pre-training tasks: Surrounding Word Prediction (SWP) for the\nlayout learning, and Contrastive learning of Word Embeddings (CWE) for\nidentifying different word blocks. Moreover, we replace the commonly used 1D\nposition embedding with a 1D clipped relative position embedding. In this way,\nthe joint training of Masked Layout-Language Modeling (MLLM) and two newly\nproposed tasks enables the interaction between semantic and spatial features in\na unified way. Additionally, the proposed UTel can process arbitrary-length\nsequences by removing the 1D position embedding, while maintaining competitive\nperformance. Extensive experimental results show UTel learns better joint\nrepresentations and achieves superior performance than previous methods on\nvarious downstream tasks, though requiring no image modality. Code is available\nat \\url{https://github.com/taosong2019/UTel}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Song Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Tiantian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Can Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Type Prediction Leveraging Graph Walks and Entity Descriptions. (arXiv:2207.14094v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.14094","description":"<p>The entity type information in Knowledge Graphs (KGs) such as DBpedia,\nFreebase, etc. is often incomplete due to automated generation or human\ncuration. Entity typing is the task of assigning or inferring the semantic type\nof an entity in a KG. This paper presents \\textit{GRAND}, a novel approach for\nentity typing leveraging different graph walk strategies in RDF2vec together\nwith textual entity descriptions. RDF2vec first generates graph walks and then\nuses a language model to obtain embeddings for each node in the graph. This\nstudy shows that the walk generation strategy and the embedding model have a\nsignificant effect on the performance of the entity typing task. The proposed\napproach outperforms the baseline approaches on the benchmark datasets DBpedia\nand FIGER for entity typing in KGs for both fine-grained and coarse-grained\nclasses. The results show that the combination of order-aware RDF2vec variants\ntogether with the contextual embeddings of the textual entity descriptions\nachieve the best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_R/0/1/0/all/0/1\">Russa Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1\">Jan Portisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1\">Harald Sack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Inverse Reinforcement Learning from Diverse Third-Person Videos via Graph Abstraction. (arXiv:2207.14299v1 [cs.LG])","link":"http://arxiv.org/abs/2207.14299","description":"<p>Research on Inverse Reinforcement Learning (IRL) from third-person videos has\nshown encouraging results on removing the need for manual reward design for\nrobotic tasks. However, most prior works are still limited by training from a\nrelatively restricted domain of videos. In this paper, we argue that the true\npotential of third-person IRL lies in increasing the diversity of videos for\nbetter scaling. To learn a reward function from diverse videos, we propose to\nperform graph abstraction on the videos followed by temporal matching in the\ngraph space to measure the task progress. Our insight is that a task can be\ndescribed by entity interactions that form a graph, and this graph abstraction\ncan help remove irrelevant information such as textures, resulting in more\nrobust reward functions. We evaluate our approach, GraphIRL, on\ncross-embodiment learning in X-MAGICAL and learning from human demonstrations\nfor real-robot manipulation. We show significant improvements in robustness to\ndiverse video demonstrations over previous approaches, and even achieve better\nresults than manual reward design on a real robot pushing task. Videos are\navailable at https://sateeshkumar21.github.io/GraphIRL .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sateesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamora_J/0/1/0/all/0/1\">Jonathan Zamora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jangir_R/0/1/0/all/0/1\">Rishabh Jangir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation. (arXiv:2207.14315v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14315","description":"<p>Visual anomaly detection is commonly used in industrial quality inspection.\nIn this paper, we present a new dataset as well as a new self-supervised\nlearning method for ImageNet pre-training to improve anomaly detection and\nsegmentation in 1-class and 2-class 5/10/high-shot training setups. We release\nthe Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color\nimages (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3\ndomains, making it the largest industrial anomaly detection dataset to date.\nBoth image and pixel-level labels are provided. We also propose a new\nself-supervised framework - SPot-the-difference (SPD) - which can regularize\ncontrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to\nbe more suitable for anomaly detection tasks. Our experiments on VisA and\nMVTec-AD dataset show that SPD consistently improves these contrastive\npre-training baselines and even the supervised pre-training. For example, SPD\nimproves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation\nby 5.9% and 6.8% over SimSiam and supervised pre-training respectively in the\n2-class high-shot regime. We open-source the project at\n<a href=\"http://github.com/amazon-research/spot-diff\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jongheon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pemula_L/0/1/0/all/0/1\">Latha Pemula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabeer_O/0/1/0/all/0/1\">Onkar Dabeer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aztec curve: proposal for a new space-filling curve. (arXiv:2207.14345v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14345","description":"<p>Different space-filling curves (SFCs) are briefly reviewed in this paper, and\na new one is proposed. A century has passed between the inception of this kind\nof curves, since then they have been found useful in computer science,\nparticularly in data storage and indexing due to their clustering properties,\nbeing Hilbert curve the most well-known member of the family of fractals. The\nproposed Aztec curve, with similar characteristics to the Hilbert's curve, is\nintroduced in this paper, accompanied by a grammatical description for its\nconstruction. It yields the possibility of creating bi-dimensional clusters,\nnot available for Hilbert nor Peano curves. Additional to this, a case of\napplication on the scope of Compressed Sensing is implemented, in which the use\nof Hilbert curve is contrasted with Aztec curve, having a similar performance,\nand positioning the Aztec curve as viable and a new alternative for future\nexploitation on applications that make use of SFC's.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayala_D/0/1/0/all/0/1\">Diego Ayala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durini_D/0/1/0/all/0/1\">Daniel Durini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangel_Magdaleno_J/0/1/0/all/0/1\">Jose Rangel-Magdaleno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training a universal instance segmentation network for live cell images of various cell types and imaging modalities. (arXiv:2207.14347v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14347","description":"<p>We share our recent findings in an attempt to train a universal segmentation\nnetwork for various cell types and imaging modalities. Our method was built on\nthe generalized U-Net architecture, which allows the evaluation of each\ncomponent individually. We modified the traditional binary training targets to\ninclude three classes for direct instance segmentation. Detailed experiments\nwere performed regarding training schemes, training settings, network\nbackbones, and individual modules on the segmentation performance. Our proposed\ntraining scheme draws minibatches in turn from each dataset, and the gradients\nare accumulated before an optimization step. We found that the key to training\na universal network is all-time supervision on all datasets, and it is\nnecessary to sample each dataset in an unbiased way. Our experiments also\nsuggest that there might exist common features to define cell boundaries across\ncell types and imaging modalities, which could allow application of trained\nmodels to totally unseen datasets. A few training tricks can further boost the\nsegmentation performance, including uneven class weights in the cross-entropy\nloss function, well-designed learning rate scheduler, larger image crops for\ncontextual information, and additional loss terms for unbalanced classes. We\nalso found that segmentation performance can benefit from group normalization\nlayer and Atrous Spatial Pyramid Pooling module, thanks to their more reliable\nstatistics estimation and improved semantic understanding, respectively. We\nparticipated in the 6th Cell Tracking Challenge (CTC) held at IEEE\nInternational Symposium on Biomedical Imaging (ISBI) 2021 using one of the\ndeveloped variants. Our method was evaluated as the best runner up during the\ninitial submission for the primary track, and also secured the 3rd place in an\nadditional round of competition in preparation for the summary publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tianqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_L/0/1/0/all/0/1\">Luis Solorio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allebach_J/0/1/0/all/0/1\">Jan P. Allebach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eye Gaze Estimation Model Analysis. (arXiv:2207.14373v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14373","description":"<p>We explore techniques for eye gaze estimation using machine learning. Eye\ngaze estimation is a common problem for various behavior analysis and\nhuman-computer interfaces. The purpose of this work is to discuss various model\ntypes for eye gaze estimation and present the results from predicting gaze\ndirection using eye landmarks in unconstrained settings. In unconstrained\nreal-world settings, feature-based and model-based methods are outperformed by\nrecent appearance-based methods due to factors like illumination changes and\nother visual artifacts. We discuss a learning-based method for eye region\nlandmark localization trained exclusively on synthetic data. We discuss how to\nuse detected landmarks as input to iterative model-fitting and lightweight\nlearning-based gaze estimation methods and how to use the model for\nperson-independent and personalized gaze estimations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kottwani_A/0/1/0/all/0/1\">Aveena Kottwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pro-tuning: Unified Prompt Tuning for Vision Tasks. (arXiv:2207.14381v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14381","description":"<p>In computer vision, fine-tuning is the de-facto approach to leverage\npre-trained vision models to perform downstream tasks. However, deploying it in\npractice is quite challenging, due to adopting parameter inefficient global\nupdate and heavily relying on high-quality downstream data. Recently,\nprompt-based learning, which adds a task-relevant prompt to adapt the\ndownstream tasks to pre-trained models, has drastically boosted the performance\nof many natural language downstream tasks. In this work, we extend this notable\ntransfer ability benefited from prompt into vision models as an alternative to\nfine-tuning. To this end, we propose parameter-efficient Prompt tuning\n(Pro-tuning) to adapt frozen vision models to various downstream vision tasks.\nThe key to Pro-tuning is prompt-based tuning, i.e., learning task-specific\nvision prompts for downstream input images with the pre-trained model frozen.\nBy only training a few additional parameters, it can work on diverse CNN-based\nand Transformer-based architectures. Extensive experiments evidence that\nPro-tuning outperforms fine-tuning in a broad range of vision tasks and\nscenarios, including image classification (generic objects, class imbalance,\nimage corruption, adversarial robustness, and out-of-distribution\ngeneralization), and dense prediction tasks such as object detection and\nsemantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xing Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bolin Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianlong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaomeng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_C/0/1/0/all/0/1\">Chunlei Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Shiming Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunhong Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Generative Approach to Oversampling in Ptychography. (arXiv:2207.14392v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14392","description":"<p>Ptychography is a well-studied phase imaging method that makes non-invasive\nimaging possible at a nanometer scale. It has developed into a mainstream\ntechnique with various applications across a range of areas such as material\nscience or the defense industry. One major drawback of ptychography is the long\ndata acquisition time due to the high overlap requirement between adjacent\nillumination areas to achieve a reasonable reconstruction. Traditional\napproaches with reduced overlap between scanning areas result in\nreconstructions with artifacts. In this paper, we propose complementing\nsparsely acquired or undersampled data with data sampled from a deep generative\nnetwork to satisfy the oversampling requirement in ptychography. Because the\ndeep generative network is pre-trained and its output can be computed as we\ncollect data, the experimental data and the time to acquire the data can be\nreduced. We validate the method by presenting the reconstruction quality\ncompared to the previously proposed and traditional approaches and comment on\nthe strengths and drawbacks of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Barutcu_S/0/1/0/all/0/1\">Semih Barutcu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gursoy_D/0/1/0/all/0/1\">Do&#x11f;a G&#xfc;rsoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low Cost Embedded Vision System For Location And Tracking Of A Color Object. (arXiv:2207.14396v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14396","description":"<p>This paper describes the development of an embedded vision system for\ndetection, location, and tracking of a color object; it makes use of a single\n32-bit microprocessor to acquire image data, process, and perform actions\naccording to the interpreted data. The system is intended for applications that\nneed to make use of artificial vision for detection, location and tracking of a\ncolor object and its objective is to have achieve at reduced terms of size,\npower consumption, and cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayala_D/0/1/0/all/0/1\">Diego Ayala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavez_D/0/1/0/all/0/1\">Danilo Chavez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robles_L/0/1/0/all/0/1\">Leopoldo Altamirano Robles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for understanding multilabel imbalanced Chest X-ray datasets. (arXiv:2207.14408v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14408","description":"<p>Over the last few years, convolutional neural networks (CNNs) have dominated\nthe field of computer vision thanks to their ability to extract features and\ntheir outstanding performance in classification problems, for example in the\nautomatic analysis of X-rays. Unfortunately, these neural networks are\nconsidered black-box algorithms, i.e. it is impossible to understand how the\nalgorithm has achieved the final result. To apply these algorithms in different\nfields and test how the methodology works, we need to use eXplainable AI\ntechniques. Most of the work in the medical field focuses on binary or\nmulticlass classification problems. However, in many real-life situations, such\nas chest X-rays, radiological signs of different diseases can appear at the\nsame time. This gives rise to what is known as \"multilabel classification\nproblems\". A disadvantage of these tasks is class imbalance, i.e. different\nlabels do not have the same number of samples. The main contribution of this\npaper is a Deep Learning methodology for imbalanced, multilabel chest X-ray\ndatasets. It establishes a baseline for the currently underutilised PadChest\ndataset and a new eXplainable AI technique based on heatmaps. This technique\nalso includes probabilities and inter-model matching. The results of our system\nare promising, especially considering the number of labels used. Furthermore,\nthe heatmaps match the expected areas, i.e. they mark the areas that an expert\nwould use to make the decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liz_H/0/1/0/all/0/1\">Helena Liz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huertas_Tato_J/0/1/0/all/0/1\">Javier Huertas-Tato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_Montanes_M/0/1/0/all/0/1\">Manuel S&#xe1;nchez-Monta&#xf1;&#xe9;s</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Camacho_D/0/1/0/all/0/1\">David Camacho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image. (arXiv:2207.14425v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14425","description":"<p>In this paper, we investigate an open research task of generating 3D cartoon\nface shapes from single 2D GAN generated human faces and without 3D\nsupervision, where we can also manipulate the facial expressions of the 3D\nshapes. To this end, we discover the semantic meanings of StyleGAN latent\nspace, such that we are able to produce face images of various expressions,\nposes, and lighting by controlling the latent codes. Specifically, we first\nfinetune the pretrained StyleGAN face model on the cartoon datasets. By feeding\nthe same latent codes to face and cartoon generation models, we aim to realize\nthe translation from 2D human face images to cartoon styled avatars. We then\ndiscover semantic directions of the GAN latent space, in an attempt to change\nthe facial expressions while preserving the original identity. As we do not\nhave any 3D annotations for cartoon faces, we manipulate the latent codes to\ngenerate images with different poses and lighting, such that we can reconstruct\nthe 3D cartoon face shapes. We validate the efficacy of our method on three\ncartoon datasets qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval. (arXiv:2207.14428v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14428","description":"<p>This paper investigates an open research problem of generating text-image\npairs to improve the training of fine-grained image-to-text cross-modal\nretrieval task, and proposes a novel framework for paired data augmentation by\nuncovering the hidden semantic information of StyleGAN2 model. Specifically, we\nfirst train a StyleGAN2 model on the given dataset. We then project the real\nimages back to the latent space of StyleGAN2 to obtain the latent codes. To\nmake the generated images manipulatable, we further introduce a latent space\nalignment module to learn the alignment between StyleGAN2 latent codes and the\ncorresponding textual caption features. When we do online paired data\naugmentation, we first generate augmented text through random token\nreplacement, then pass the augmented text into the latent space alignment\nmodule to output the latent codes, which are finally fed to StyleGAN2 to\ngenerate the augmented images. We evaluate the efficacy of our augmented data\napproach on two public cross-modal retrieval datasets, in which the promising\nexperimental results demonstrate the augmented text-image pair data can be\ntrained together with the original data to boost the image-to-text cross-modal\nretrieval performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Based Small Bowel Path Tracking with Cylindrical Constraints. (arXiv:2207.14436v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14436","description":"<p>We present a new graph-based method for small bowel path tracking based on\ncylindrical constraints. A distinctive characteristic of the small bowel\ncompared to other organs is the contact between parts of itself along its\ncourse, which makes the path tracking difficult together with the indistinct\nappearance of the wall. It causes the tracked path to easily cross over the\nwalls when relying on low-level features like the wall detection. To circumvent\nthis, a series of cylinders that are fitted along the course of the small bowel\nare used to guide the tracking to more reliable directions. It is implemented\nas soft constraints using a new cost function. The proposed method is evaluated\nagainst ground-truth paths that are all connected from start to end of the\nsmall bowel for 10 abdominal CT scans. The proposed method showed clear\nimprovements compared to the baseline method in tracking the path without\nmaking an error. Improvements of 6.6% and 17.0%, in terms of the tracked\nlength, were observed for two different settings related to the small bowel\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seung Yeon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1\">Ronald M. Summers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset and Evaluation algorithm design for GOALS Challenge. (arXiv:2207.14447v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14447","description":"<p>Glaucoma causes irreversible vision loss due to damage to the optic nerve,\nand there is no cure for glaucoma.OCT imaging modality is an essential\ntechnique for assessing glaucomatous damage since it aids in quantifying fundus\nstructures. To promote the research of AI technology in the field of\nOCT-assisted diagnosis of glaucoma, we held a Glaucoma OCT Analysis and Layer\nSegmentation (GOALS) Challenge in conjunction with the International Conference\non Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022 to\nprovide data and corresponding annotations for researchers studying layer\nsegmentation from OCT images and the classification of glaucoma. This paper\ndescribes the released 300 circumpapillary OCT images, the baselines of the two\nsub-tasks, and the evaluation methodology. The GOALS Challenge is accessible at\nhttps://aistudio.baidu.com/aistudio/competition/detail/230.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiulan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PC-GANs: Progressive Compensation Generative Adversarial Networks for Pan-sharpening. (arXiv:2207.14451v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14451","description":"<p>The fusion of multispectral and panchromatic images is always dubbed\npansharpening. Most of the available deep learning-based pan-sharpening methods\nsharpen the multispectral images through a one-step scheme, which strongly\ndepends on the reconstruction ability of the network. However, remote sensing\nimages always have large variations, as a result, these one-step methods are\nvulnerable to the error accumulation and thus incapable of preserving spatial\ndetails as well as the spectral information. In this paper, we propose a novel\ntwo-step model for pan-sharpening that sharpens the MS image through the\nprogressive compensation of the spatial and spectral information. Firstly, a\ndeep multiscale guided generative adversarial network is used to preliminarily\nenhance the spatial resolution of the MS image. Starting from the pre-sharpened\nMS image in the coarse domain, our approach then progressively refines the\nspatial and spectral residuals over a couple of generative adversarial networks\n(GANs) that have reverse architectures. The whole model is composed of triple\nGANs, and based on the specific architecture, a joint compensation loss\nfunction is designed to enable the triple GANs to be trained simultaneously.\nMoreover, the spatial-spectral residual compensation structure proposed in this\npaper can be extended to other pan-sharpening methods to further enhance their\nfusion results. Extensive experiments are performed on different datasets and\nthe results demonstrate the effectiveness and efficiency of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xing_Y/0/1/0/all/0/1\">Yinghui Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuyuan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Occluded Person Re-identification: A Survey. (arXiv:2207.14452v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14452","description":"<p>Occluded person re-identification (Re-ID) aims at addressing the occlusion\nproblem when retrieving the person of interest across multiple cameras. With\nthe promotion of deep learning technology and the increasing demand for\nintelligent video surveillance, the frequent occlusion in real-world\napplications has made occluded person Re-ID draw considerable interest from\nresearchers. A large number of occluded person Re-ID methods have been proposed\nwhile there are few surveys that focus on occlusion. To fill this gap and help\nboost future research, this paper provides a systematic survey of occluded\nperson Re-ID. Through an in-depth analysis of the occlusion in person Re-ID,\nmost existing methods are found to only consider part of the problems brought\nby occlusion. Therefore, we review occlusion-related person Re-ID methods from\nthe perspective of issues and solutions. We summarize four issues caused by\nocclusion in person Re-ID, i.e., position misalignment, scale misalignment,\nnoisy information, and missing information. The occlusion-related methods\naddressing different issues are then categorized and introduced accordingly.\nAfter that, we summarize and compare the performance of recent occluded person\nRe-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS,\nOccluded-ReID, and Occluded-DukeMTMC. Finally, we provide insights on promising\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yunjie Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Saihui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chunshui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Density-Distance Fields. (arXiv:2207.14455v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14455","description":"<p>The success of neural fields for 3D vision tasks is now indisputable.\nFollowing this trend, several methods aiming for visual localization (e.g.,\nSLAM) have been proposed to estimate distance or density fields using neural\nfields. However, it is difficult to achieve high localization performance by\nonly density fields-based methods such as Neural Radiance Field (NeRF) since\nthey do not provide density gradient in most empty regions. On the other hand,\ndistance field-based methods such as Neural Implicit Surface (NeuS) have\nlimitations in objects' surface shapes. This paper proposes Neural\nDensity-Distance Field (NeDDF), a novel 3D representation that reciprocally\nconstrains the distance and density fields. We extend distance field\nformulation to shapes with no explicit boundary surface, such as fur or smoke,\nwhich enable explicit conversion from distance field to density field.\nConsistent distance and density fields realized by explicit conversion enable\nboth robustness to initial values and high-quality registration. Furthermore,\nthe consistency between fields allows fast convergence from sparse point\nclouds. Experiments show that NeDDF can achieve high localization performance\nwhile providing comparable results to NeRF on novel view synthesis. The code is\navailable at https://github.com/ueda0319/neddf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ueda_I/0/1/0/all/0/1\">Itsuki Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuhara_Y/0/1/0/all/0/1\">Yoshihiro Fukuhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kataoka_H/0/1/0/all/0/1\">Hirokatsu Kataoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_H/0/1/0/all/0/1\">Hiroaki Aizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shishido_H/0/1/0/all/0/1\">Hidehiko Shishido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitahara_I/0/1/0/all/0/1\">Itaru Kitahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Complexity Loeffler DCT Approximations for Image and Video Coding. (arXiv:2207.14463v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14463","description":"<p>This paper introduced a matrix parametrization method based on the Loeffler\ndiscrete cosine transform (DCT) algorithm. As a result, a new class of\neight-point DCT approximations was proposed, capable of unifying the\nmathematical formalism of several eight-point DCT approximations archived in\nthe literature. Pareto-efficient DCT approximations are obtained through\nmulticriteria optimization, where computational complexity, proximity, and\ncoding performance are considered. Efficient approximations and their scaled\n16- and 32-point versions are embedded into image and video encoders, including\na JPEG-like codec and H.264/AVC and H.265/HEVC standards. Results are compared\nto the unmodified standard codecs. Efficient approximations are mapped and\nimplemented on a Xilinx VLX240T FPGA and evaluated for area, speed, and power\nconsumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Coelho_D/0/1/0/all/0/1\">D. F. G. Coelho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cintra_R/0/1/0/all/0/1\">R. J. Cintra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayer_F/0/1/0/all/0/1\">F. M. Bayer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kulasekera_S/0/1/0/all/0/1\">S. Kulasekera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madanayake_A/0/1/0/all/0/1\">A. Madanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martinez_P/0/1/0/all/0/1\">P. A. C. Martinez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silveira_T/0/1/0/all/0/1\">T. L. T. Silveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_R/0/1/0/all/0/1\">R. S. Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dimitrov_V/0/1/0/all/0/1\">V. S. Dimitrov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Retrieval Prompt Tuning. (arXiv:2207.14465v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14465","description":"<p>Fine-grained object retrieval aims to learn discriminative representation to\nretrieve visually similar objects. However, existing top-performing works\nusually impose pairwise similarities on the semantic embedding spaces to\ncontinually fine-tune the entire model in limited-data regimes, thus resulting\nin easily converging to suboptimal solutions. In this paper, we develop\nFine-grained Retrieval Prompt Tuning (FRPT), which steers a frozen pre-trained\nmodel to perform the fine-grained retrieval task from the perspectives of\nsample prompt and feature adaptation. Specifically, FRPT only needs to learn\nfewer parameters in the prompt and adaptation instead of fine-tuning the entire\nmodel, thus solving the convergence to suboptimal solutions caused by\nfine-tuning the entire model. Technically, as sample prompts, a structure\nperturbation prompt (SPP) is introduced to zoom and even exaggerate some pixels\ncontributing to category prediction via a content-aware inhomogeneous sampling\noperation. In this way, SPP can make the fine-grained retrieval task aided by\nthe perturbation prompts close to the solved task during the original\npre-training. Besides, a category-specific awareness head is proposed and\nregarded as feature adaptation, which removes the species discrepancies in the\nfeatures extracted by the pre-trained model using instance normalization, and\nthus makes the optimized features only include the discrepancies among\nsubcategories. Extensive experiments demonstrate that our FRPT with fewer\nlearnable parameters achieves the state-of-the-art performance on three\nwidely-used fine-grained datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianlong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Domain-agnostic Depth Completion. (arXiv:2207.14466v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14466","description":"<p>Existing depth completion methods are often targeted at a specific sparse\ndepth type, and generalize poorly across task domains. We present a method to\ncomplete sparse/semi-dense, noisy, and potentially low-resolution depth maps\nobtained by various range sensors, including those in modern mobile phones, or\nby multi-view reconstruction algorithms. Our method leverages a data driven\nprior in the form of a single image depth prediction network trained on\nlarge-scale datasets, the output of which is used as an input to our model. We\npropose an effective training scheme where we simulate various sparsity\npatterns in typical task domains. In addition, we design two new benchmarks to\nevaluate the generalizability and the robustness of depth completion methods.\nOur simple method shows superior cross-domain generalization ability against\nstate-of-the-art depth completion methods, introducing a practical solution to\nhigh quality depth capture on a mobile device. Code is available at:\nhttps://github.com/YvanYin/FillDepth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1\">Oliver Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_S/0/1/0/all/0/1\">Simon Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simon Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation. (arXiv:2207.14472v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14472","description":"<p>Automatic tumor or lesion segmentation is a crucial step in medical image\nanalysis for computer-aided diagnosis. Although the existing methods based on\nConvolutional Neural Networks (CNNs) have achieved the state-of-the-art\nperformance, many challenges still remain in medical tumor segmentation. This\nis because, although the human visual system can detect symmetries in 2D images\neffectively, regular CNNs can only exploit translation invariance, overlooking\nfurther inherent symmetries existing in medical images such as rotations and\nreflections. To solve this problem, we propose a novel group equivariant\nsegmentation framework by encoding those inherent symmetries for learning more\nprecise representations. First, kernel-based equivariant operations are devised\non each orientation, which allows it to effectively address the gaps of\nlearning symmetries in existing approaches. Then, to keep segmentation networks\nglobally equivariant, we design distinctive group layers with layer-wise\nsymmetry constraints. Finally, based on our novel framework, extensive\nexperiments conducted on real-world clinical data demonstrate that a Group\nEquivariant Res-UNet (named GER-UNet) outperforms its regular CNN-based\ncounterpart and the state-of-the-art segmentation methods in the tasks of\nhepatic tumor segmentation, COVID-19 lung infection segmentation and retinal\nvessel detection. More importantly, the newly built GER-UNet also shows\npotential in reducing the sample complexity and the redundancy of filters,\nupgrading current segmentation CNNs and delineating organs on other medical\nimaging modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pang_S/0/1/0/all/0/1\">Shuchao Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_A/0/1/0/all/0/1\">Anan Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orgun_M/0/1/0/all/0/1\">Mehmet A. Orgun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_Q/0/1/0/all/0/1\">Quan Z. Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shoujin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoshui Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zhenmei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Centrality and Consistency: Two-Stage Clean Samples Identification for Learning with Instance-Dependent Noisy Labels. (arXiv:2207.14476v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14476","description":"<p>Deep models trained with noisy labels are prone to over-fitting and struggle\nin generalization. Most existing solutions are based on an ideal assumption\nthat the label noise is class-conditional, i.e., instances of the same class\nshare the same noise model, and are independent of features. While in practice,\nthe real-world noise patterns are usually more fine-grained as\ninstance-dependent ones, which poses a big challenge, especially in the\npresence of inter-class imbalance. In this paper, we propose a two-stage clean\nsamples identification method to address the aforementioned challenge. First,\nwe employ a class-level feature clustering procedure for the early\nidentification of clean samples that are near the class-wise prediction\ncenters. Notably, we address the class imbalance problem by aggregating rare\nclasses according to their prediction entropy. Second, for the remaining clean\nsamples that are close to the ground truth class boundary (usually mixed with\nthe samples with instance-dependent noises), we propose a novel\nconsistency-based classification method that identifies them using the\nconsistency of two classifier heads: the higher the consistency, the larger the\nprobability that a sample is clean. Extensive experiments on several\nchallenging benchmarks demonstrate the superior performance of our method\nagainst the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Ganlong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCSN: Global Context Aware Segmentation by Learning the Fourier Coefficients of Objects in Medical Images. (arXiv:2207.14477v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14477","description":"<p>The encoder-decoder model is a commonly used Deep Neural Network (DNN) model\nfor medical image segmentation. Conventional encoder-decoder models make\npixel-wise predictions focusing heavily on local patterns around the pixel.\nThis makes it challenging to give segmentation that preserves the object's\nshape and topology, which often requires an understanding of the global context\nof the object. In this work, we propose a Fourier Coefficient Segmentation\nNetwork~(FCSN) -- a novel DNN-based model that segments an object by learning\nthe complex Fourier coefficients of the object's masks. The Fourier\ncoefficients are calculated by integrating over the whole contour. Therefore,\nfor our model to make a precise estimation of the coefficients, the model is\nmotivated to incorporate the global context of the object, leading to a more\naccurate segmentation of the object's shape. This global context awareness also\nmakes our model robust to unseen local perturbations during inference, such as\nadditive noise or motion blur that are prevalent in medical images. When FCSN\nis compared with other state-of-the-art models (UNet+, DeepLabV3+, UNETR) on 3\nmedical image segmentation tasks (ISIC\\_2018, RIM\\_CUP, RIM\\_DISC), FCSN\nattains significantly lower Hausdorff scores of 19.14 (6\\%), 17.42 (6\\%), and\n9.16 (14\\%) on the 3 tasks, respectively. Moreover, FCSN is lightweight by\ndiscarding the decoder module, which incurs significant computational overhead.\nFCSN only requires 22.2M parameters, 82M and 10M fewer parameters than UNETR\nand DeepLabV3+. FCSN attains inference and training speeds of 1.6ms/img and\n6.3ms/img, that is 8$\\times$ and 3$\\times$ faster than UNet and UNETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeon_Y/0/1/0/all/0/1\">Young Seok Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hongfei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1\">Mengling Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleAM: Perception-Oriented Unsupervised Domain Adaption for Non-reference Image Quality Assessment. (arXiv:2207.14489v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14489","description":"<p>Deep neural networks (DNNs) have shown great potential in non-reference image\nquality assessment (NR-IQA). However, the annotation of NR-IQA is\nlabor-intensive and time-consuming, which severely limits their application\nespecially for authentic images. To relieve the dependence on quality\nannotation, some works have applied unsupervised domain adaptation (UDA) to\nNR-IQA. However, the above methods ignore that the alignment space used in\nclassification is sub-optimal, since the space is not elaborately designed for\nperception. To solve this challenge, we propose an effective\nperception-oriented unsupervised domain adaptation method StyleAM for NR-IQA,\nwhich transfers sufficient knowledge from label-rich source domain data to\nlabel-free target domain images via Style Alignment and Mixup. Specifically, we\nfind a more compact and reliable space i.e., feature style space for\nperception-oriented UDA based on an interesting/amazing observation, that the\nfeature style (i.e., the mean and variance) of the deep layer in DNNs is\nexactly associated with the quality score in NR-IQA. Therefore, we propose to\nalign the source and target domains in a more perceptual-oriented space i.e.,\nthe feature style space, to reduce the intervention from other\nquality-irrelevant feature factors. Furthermore, to increase the consistency\nbetween quality score and its feature style, we also propose a novel feature\naugmentation strategy Style Mixup, which mixes the feature styles (i.e., the\nmean and variance) before the last layer of DNNs together with mixing their\nlabels. Extensive experimental results on two typical cross-domain settings\n(i.e., synthetic to authentic, and multiple distortions to one distortion) have\ndemonstrated the effectiveness of our proposed StyleAM on NR-IQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yiting Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conservative Generator, Progressive Discriminator: Coordination of Adversaries in Few-shot Incremental Image Synthesis. (arXiv:2207.14491v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14491","description":"<p>The capacity to learn incrementally from an online stream of data is an\nenvied trait of human learners, as deep neural networks typically suffer from\ncatastrophic forgetting and stability-plasticity dilemma. Several works have\npreviously explored incremental few-shot learning, a task with greater\nchallenges due to data constraint, mostly in classification setting with mild\nsuccess. In this work, we study the underrepresented task of generative\nincremental few-shot learning. To effectively handle the inherent challenges of\nincremental learning and few-shot learning, we propose a novel framework named\nConPro that leverages the two-player nature of GANs. Specifically, we design a\nconservative generator that preserves past knowledge in parameter and compute\nefficient manner, and a progressive discriminator that learns to reason\nsemantic distances between past and present task samples, minimizing\noverfitting with few data points and pursuing good forward transfer. We present\nexperiments to validate the effectiveness of ConPro.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chaerin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reference-Guided Texture and Structure Inference for Image Inpainting. (arXiv:2207.14498v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14498","description":"<p>Existing learning-based image inpainting methods are still in challenge when\nfacing complex semantic environments and diverse hole patterns. The prior\ninformation learned from the large scale training data is still insufficient\nfor these situations. Reference images captured covering the same scenes share\nsimilar texture and structure priors with the corrupted images, which offers\nnew prospects for the image inpainting tasks. Inspired by this, we first build\na benchmark dataset containing 10K pairs of input and reference images for\nreference-guided inpainting. Then we adopt an encoder-decoder structure to\nseparately infer the texture and structure features of the input image\nconsidering their pattern discrepancy of texture and structure during\ninpainting. A feature alignment module is further designed to refine these\nfeatures of the input image with the guidance of a reference image. Both\nquantitative and qualitative evaluations demonstrate the superiority of our\nmethod over the state-of-the-art methods in terms of completing complex holes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Taorong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Liang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_S/0/1/0/all/0/1\">Shin&#x27;ichi Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Difficulty Based Methods for Long-Tailed Visual Recognition. (arXiv:2207.14499v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14499","description":"<p>Long-tailed datasets are very frequently encountered in real-world use cases\nwhere few classes or categories (known as majority or head classes) have higher\nnumber of data samples compared to the other classes (known as minority or tail\nclasses). Training deep neural networks on such datasets gives results biased\ntowards the head classes. So far, researchers have come up with multiple\nweighted loss and data re-sampling techniques in efforts to reduce the bias.\nHowever, most of such techniques assume that the tail classes are always the\nmost difficult classes to learn and therefore need more weightage or attention.\nHere, we argue that the assumption might not always hold true. Therefore, we\npropose a novel approach to dynamically measure the instantaneous difficulty of\neach class during the training phase of the model. Further, we use the\ndifficulty measures of each class to design a novel weighted loss technique\ncalled `class-wise difficulty based weighted (CDB-W) loss' and a novel data\nsampling technique called `class-wise difficulty based sampling (CDB-S)'. To\nverify the wide-scale usability of our CDB methods, we conducted extensive\nexperiments on multiple tasks such as image classification, object detection,\ninstance segmentation and video-action classification. Results verified that\nCDB-W loss and CDB-S could achieve state-of-the-art results on many\nclass-imbalanced datasets such as ImageNet-LT, LVIS and EGTEA, that resemble\nreal-world use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Saptarshi Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohashi_H/0/1/0/all/0/1\">Hiroki Ohashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1\">Katsuyuki Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transfer Learning-Based Approach to Marine Vessel Re-Identification. (arXiv:2207.14500v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14500","description":"<p>Marine vessel re-identification technology is an important component of\nintelligent shipping systems and an important part of the visual perception\ntasks required for marine surveillance. However, unlike the situation on land,\nthe maritime environment is complex and variable with fewer samples, and it is\nmore difficult to perform vessel re-identification at sea. Therefore, this\npaper proposes a transfer dynamic alignment algorithm and simulates the swaying\nsituation of vessels at sea, using a well-camouflaged and similar warship as\nthe test target to improve the recognition difficulty and thus cope with the\nimpact caused by complex sea conditions, and discusses the effect of different\ntypes of vessels as transfer objects. The experimental results show that the\nimproved algorithm improves the mean average accuracy (mAP) by 10.2% and the\nfirst hit rate (Rank1) by 4.9% on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guangmiao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wanneng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Anhui Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPU-accelerated SIFT-aided source identification of stabilized videos. (arXiv:2207.14507v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14507","description":"<p>Video stabilization is an in-camera processing commonly applied by modern\nacquisition devices. While significantly improving the visual quality of the\nresulting videos, it has been shown that such operation typically hinders the\nforensic analysis of video signals. In fact, the correct identification of the\nacquisition source usually based on Photo Response non-Uniformity (PRNU) is\nsubject to the estimation of the transformation applied to each frame in the\nstabilization phase. A number of techniques have been proposed for dealing with\nthis problem, which however typically suffer from a high computational burden\ndue to the grid search in the space of inversion parameters. Our work attempts\nto alleviate these shortcomings by exploiting the parallelization capabilities\nof Graphics Processing Units (GPUs), typically used for deep learning\napplications, in the framework of stabilised frames inversion. Moreover, we\npropose to exploit SIFT features {to estimate the camera momentum and} %to\nidentify less stabilized temporal segments, thus enabling a more accurate\nidentification analysis, and to efficiently initialize the frame-wise parameter\nsearch of consecutive frames. Experiments on a consolidated benchmark dataset\nconfirm the effectiveness of the proposed approach in reducing the required\ncomputational time and improving the source identification accuracy. {The code\nis available at \\url{https://github.com/AMontiB/GPU-PRNU-SIFT}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montibeller_A/0/1/0/all/0/1\">Andrea Montibeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquini_C/0/1/0/all/0/1\">Cecilia Pasquini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boato_G/0/1/0/all/0/1\">Giulia Boato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DellAnna_S/0/1/0/all/0/1\">Stefano Dell&#x27;Anna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Gonzalez_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Gonz&#xe1;lez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder. (arXiv:2207.14508v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14508","description":"<p>It is common practice to reuse models initially trained on different data to\nincrease downstream task performance. Especially in the computer vision domain,\nImageNet-pretrained weights have been successfully used for various tasks. In\nthis work, we investigate the impact of transfer learning for segmentation\nproblems, being pixel-wise classification problems that can be tackled with\nencoder-decoder architectures. We find that transfer learning the decoder does\nnot help downstream segmentation tasks, while transfer learning the encoder is\ntruly beneficial. We demonstrate that pretrained weights for a decoder may\nyield faster convergence, but they do not improve the overall model performance\nas one can obtain equivalent results with randomly initialized decoders.\nHowever, we show that it is more effective to reuse encoder weights trained on\na segmentation or reconstruction task than reusing encoder weights trained on\nclassification tasks. This finding implicates that using ImageNet-pretrained\nencoders for downstream segmentation problems is suboptimal. We also propose a\ncontrastive self-supervised approach with multiple self-reconstruction tasks,\nwhich provides encoders that are suitable for transfer learning in segmentation\nproblems in the absence of segmentation labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dippel_J/0/1/0/all/0/1\">Jonas Dippel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1\">Matthias Lenga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goerttler_T/0/1/0/all/0/1\">Thomas Goerttler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obermayer_K/0/1/0/all/0/1\">Klaus Obermayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_J/0/1/0/all/0/1\">Johannes H&#xf6;hne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Driven Action Quality Assessment. (arXiv:2207.14513v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14513","description":"<p>Automatic action quality assessment (AQA) has attracted more interests due to\nits wide applications. However, existing AQA methods usually employ the\nmulti-branch models to generate multiple scores, which is not flexible for\ndealing with a variable number of judges. In this paper, we propose a novel\nUncertainty-Driven AQA (UD-AQA) model to generate multiple predictions only\nusing one single branch. Specifically, we design a CVAE (Conditional\nVariational Auto-Encoder) based module to encode the uncertainty, where\nmultiple scores can be produced by sampling from the learned latent space\nmultiple times. Moreover, we output the estimation of uncertainty and utilize\nthe predicted uncertainty to re-weight AQA regression loss, which can reduce\nthe contributions of uncertain samples for training. We further design an\nuncertainty-guided training strategy to dynamically adjust the learning order\nof the samples from low uncertainty to high uncertainty. The experiments show\nthat our proposed method achieves new state-of-the-art results on the Olympic\nevents MTL-AQA and surgical skill JIGSAWS datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Caixia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaping Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Practicality of Learned Image Compression. (arXiv:2207.14524v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14524","description":"<p>Learned image compression has achieved extraordinary rate-distortion\nperformance in PSNR and MS-SSIM compared to traditional methods. However, it\nsuffers from intensive computation, which is intolerable for real-world\napplications and leads to its limited industrial application for now. In this\npaper, we introduce neural architecture search (NAS) to designing more\nefficient networks with lower latency, and leverage quantization to accelerate\nthe inference process. Meanwhile, efforts in engineering like multi-threading\nand SIMD have been made to improve efficiency. Optimized using a hybrid loss of\nPSNR and MS-SSIM for better visual quality, we obtain much higher MS-SSIM than\nJPEG, JPEG XL and AVIF over all bit rates, and PSNR between that of JPEG XL and\nAVIF. Our software implementation of LIC achieves comparable or even faster\ninference speed compared to jpeg-turbo while being multiple times faster than\nJPEG XL and AVIF. Besides, our implementation of LIC reaches stunning\nthroughput of 145 fps for encoding and 208 fps for decoding on a Tesla T4 GPU\nfor 1080p images. On CPU, the latency of our implementation is comparable with\nJPEG XL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hongjiu Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Q/0/1/0/all/0/1\">Qiancheng Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Jin Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_X/0/1/0/all/0/1\">Xingyuan Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jixiang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Dailan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yilong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Pengbo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanyuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Y/0/1/0/all/0/1\">Yaxu Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum Learning for Data-Efficient Vision-Language Alignment. (arXiv:2207.14525v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14525","description":"<p>Aligning image and text encoders from scratch using contrastive learning\nrequires large amounts of paired image-text data. We alleviate this need by\naligning individually pre-trained language and vision representation models\nusing a much smaller amount of paired data, augmented with a curriculum\nlearning algorithm to learn fine-grained vision-language alignments. TOnICS\n(Training with Ontology-Informed Contrastive Sampling) initially samples\nminibatches whose image-text pairs contain a wide variety of objects to learn\nobject-level alignment, and progressively samples minibatches where all\nimage-text pairs contain the same object to learn finer-grained contextual\nalignment. Aligning pre-trained BERT and VinVL models to each other using\nTOnICS outperforms CLIP on downstream zero-shot image retrieval while using\nless than 1% as much training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings. (arXiv:2207.14539v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14539","description":"<p>Pre-training trajectory embeddings is a fundamental and critical procedure in\nspatial-temporal trajectory mining, and is beneficial for a wide range of\ndownstream tasks. The key for generating effective trajectory embeddings is to\nextract high-level travel semantics from trajectories, including movement\npatterns and travel purposes, with consideration of the trajectories' long-term\nspatial-temporal correlations. Despite the existing efforts, there are still\nmajor challenges in pre-training trajectory embeddings. First, commonly used\ngenerative pretext tasks are not suitable for extracting high-level semantics\nfrom trajectories. Second, existing data augmentation methods fit badly on\ntrajectory datasets. Third, current encoder designs fail to fully incorporate\nlong-term spatial-temporal correlations hidden in trajectories. To tackle these\nchallenges, we propose a novel Contrastive Spatial-Temporal Trajectory\nEmbedding (CSTTE) model for learning comprehensive trajectory embeddings. CSTTE\nadopts the contrastive learning framework so that its pretext task is robust to\nnoise. A specially designed data augmentation method for trajectories is\ncoupled with the contrastive pretext task to preserve the high-level travel\nsemantics. We also build an efficient spatial-temporal trajectory encoder to\nefficiently and comprehensively model the long-term spatial-temporal\ncorrelations in trajectories. Extensive experiments on two downstream tasks and\nthree real-world datasets prove the superiority of our model compared with the\nexisting trajectory embedding methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Huaiyu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shengnan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A One-Shot Reparameterization Method for Reducing the Loss of Tile Pruning on DNNs. (arXiv:2207.14545v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14545","description":"<p>Recently, tile pruning has been widely studied to accelerate the inference of\ndeep neural networks (DNNs). However, we found that the loss due to tile\npruning, which can eliminate important elements together with unimportant\nelements, is large on trained DNNs. In this study, we propose a one-shot\nreparameterization method, called TileTrans, to reduce the loss of tile\npruning. Specifically, we repermute the rows or columns of the weight matrix\nsuch that the model architecture can be kept unchanged after\nreparameterization. This repermutation realizes the reparameterization of the\nDNN model without any retraining. The proposed reparameterization method\ncombines important elements into the same tile; thus, preserving the important\nelements after the tile pruning. Furthermore, TileTrans can be seamlessly\nintegrated into existing tile pruning methods because it is a pre-processing\nmethod executed before pruning, which is orthogonal to most existing methods.\nThe experimental results demonstrate that our method is essential in reducing\nthe loss of tile pruning on DNNs. Specifically, the accuracy is improved by up\nto 17% for AlexNet while 5% for ResNet-34, where both models are pre-trained on\nImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingzhong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ino_F/0/1/0/all/0/1\">Fumihiko Ino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise Perspective for Medical Image Segmentation. (arXiv:2207.14552v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14552","description":"<p>Recently, a variety of vision transformers have been developed as their\ncapability of modeling long-range dependency. In current transformer-based\nbackbones for medical image segmentation, convolutional layers were replaced\nwith pure transformers, or transformers were added to the deepest encoder to\nlearn global context. However, there are mainly two challenges in a scale-wise\nperspective: (1) intra-scale problem: the existing methods lacked in extracting\nlocal-global cues in each scale, which may impact the signal propagation of\nsmall objects; (2) inter-scale problem: the existing methods failed to explore\ndistinctive information from multiple scales, which may hinder the\nrepresentation learning from objects with widely variable size, shape and\nlocation. To address these limitations, we propose a novel backbone, namely\nScaleFormer, with two appealing designs: (1) A scale-wise intra-scale\ntransformer is designed to couple the CNN-based local features with the\ntransformer-based global cues in each scale, where the row-wise and column-wise\nglobal dependencies can be extracted by a lightweight Dual-Axis MSA. (2) A\nsimple and effective spatial-aware inter-scale transformer is designed to\ninteract among consensual regions in multiple scales, which can highlight the\ncross-scale dependency and resolve the complex scale variations. Experimental\nresults on different benchmarks demonstrate that our Scale-Former outperforms\nthe current state-of-the-art methods. The code is publicly available at:\nhttps://github.com/ZJUGiveLab/ScaleFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huimin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie1_S/0/1/0/all/0/1\">Shiao Xie1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_Y/0/1/0/all/0/1\">Yutaro Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Ruofeng Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting for Multi-Modal Tracking. (arXiv:2207.14571v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14571","description":"<p>Multi-modal tracking gains attention due to its ability to be more accurate\nand robust in complex scenarios compared to traditional RGB-based tracking. Its\nkey lies in how to fuse multi-modal data and reduce the gap between modalities.\nHowever, multi-modal tracking still severely suffers from data deficiency, thus\nresulting in the insufficient learning of fusion modules. Instead of building\nsuch a fusion module, in this paper, we provide a new perspective on\nmulti-modal tracking by attaching importance to the multi-modal visual prompts.\nWe design a novel multi-modal prompt tracker (ProTrack), which can transfer the\nmulti-modal inputs to a single modality by the prompt paradigm. By best\nemploying the tracking ability of pre-trained RGB trackers learning at scale,\nour ProTrack can achieve high-performance multi-modal tracking by only altering\nthe inputs, even without any extra training on multi-modal data. Extensive\nexperiments on 5 benchmark datasets demonstrate the effectiveness of the\nproposed ProTrack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Augmentation for Satellite Images. (arXiv:2207.14580v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14580","description":"<p>This study proposes the use of generative models (GANs) for augmenting the\nEuroSAT dataset for the Land Use and Land Cover (LULC) Classification task. We\nused DCGAN and WGAN-GP to generate images for each class in the dataset. We\nthen explored the effect of augmenting the original dataset by about 10% in\neach case on model performance. The choice of GAN architecture seems to have no\napparent effect on the model performance. However, a combination of geometric\naugmentation and GAN-generated images improved baseline results. Our study\nshows that GANs augmentation can improve the generalizability of deep\nclassification models on satellite images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adedeji_O/0/1/0/all/0/1\">Oluwadara Adedeji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owoade_P/0/1/0/all/0/1\">Peter Owoade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajayi_O/0/1/0/all/0/1\">Opeyemi Ajayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arowolo_O/0/1/0/all/0/1\">Olayiwola Arowolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Prototype via Placeholder for Zero-shot Recognition. (arXiv:2207.14581v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14581","description":"<p>Zero-shot learning (ZSL) aims to recognize unseen classes by exploiting\nsemantic descriptions shared between seen classes and unseen classes. Current\nmethods show that it is effective to learn visual-semantic alignment by\nprojecting semantic embeddings into the visual space as class prototypes.\nHowever, such a projection function is only concerned with seen classes. When\napplied to unseen classes, the prototypes often perform suboptimally due to\ndomain shift. In this paper, we propose to learn prototypes via placeholders,\ntermed LPL, to eliminate the domain shift between seen and unseen classes.\nSpecifically, we combine seen classes to hallucinate new classes which play as\nplaceholders of the unseen classes in the visual and semantic space. Placed\nbetween seen classes, the placeholders encourage prototypes of seen classes to\nbe highly dispersed. And more space is spared for the insertion of\nwell-separated unseen ones. Empirically, well-separated prototypes help\ncounteract visual-semantic misalignment caused by domain shift. Furthermore, we\nexploit a novel semantic-oriented fine-tuning to guarantee the semantic\nreliability of placeholders. Extensive experiments on five benchmark datasets\ndemonstrate the significant performance gain of LPL over the state-of-the-art\nmethods. Code is available at https://github.com/zaiquanyang/LPL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zaiquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Chao Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Deformable 3D Caricatures with Learned Shape Control. (arXiv:2207.14593v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14593","description":"<p>A 3D caricature is an exaggerated 3D depiction of a human face. The goal of\nthis paper is to model the variations of 3D caricatures in a compact parameter\nspace so that we can provide a useful data-driven toolkit for handling 3D\ncaricature deformations. To achieve the goal, we propose an MLP-based framework\nfor building a deformable surface model, which takes a latent code and produces\na 3D surface. In the framework, a SIREN MLP models a function that takes a 3D\nposition on a fixed template surface and returns a 3D displacement vector for\nthe input position. We create variations of 3D surfaces by learning a\nhypernetwork that takes a latent code and produces the parameters of the MLP.\nOnce learned, our deformable model provides a nice editing space for 3D\ncaricatures, supporting label-based semantic editing and point-handle-based\ndeformation, both of which produce highly exaggerated and natural 3D caricature\nshapes. We also demonstrate other applications of our deformable model, such as\nautomatic 3D caricature creation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yucheol Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Wonjong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soongjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WISE: Whitebox Image Stylization by Example-based Learning. (arXiv:2207.14606v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14606","description":"<p>Image-based artistic rendering can synthesize a variety of expressive styles\nusing algorithmic image filtering. In contrast to deep learning-based methods,\nthese heuristics-based filtering techniques can operate on high-resolution\nimages, are interpretable, and can be parameterized according to various design\naspects. However, adapting or extending these techniques to produce new styles\nis often a tedious and error-prone task that requires expert knowledge. We\npropose a new paradigm to alleviate this problem: implementing algorithmic\nimage filtering techniques as differentiable operations that can learn\nparametrizations aligned to certain reference styles. To this end, we present\nWISE, an example-based image-processing system that can handle a multitude of\nstylization techniques, such as watercolor, oil or cartoon stylization, within\na common framework. By training parameter prediction networks for global and\nlocal filter parameterizations, we can simultaneously adapt effects to\nreference styles and image content, e.g., to enhance facial features. Our\nmethod can be optimized in a style-transfer framework or learned in a\ngenerative-adversarial setting for image-to-image translation. We demonstrate\nthat jointly training an XDoG filter and a CNN for postprocessing can achieve\ncomparable results to a state-of-the-art GAN-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lotzsch_W/0/1/0/all/0/1\">Winfried L&#xf6;tzsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimann_M/0/1/0/all/0/1\">Max Reimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bussemeyer_M/0/1/0/all/0/1\">Martin B&#xfc;ssemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semmo_A/0/1/0/all/0/1\">Amir Semmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollner_J/0/1/0/all/0/1\">J&#xfc;rgen D&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Matthias Trapp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational complexity reduction of deep neural networks. (arXiv:2207.14620v1 [cs.LG])","link":"http://arxiv.org/abs/2207.14620","description":"<p>Deep neural networks (DNN) have been widely used and play a major role in the\nfield of computer vision and autonomous navigation. However, these DNNs are\ncomputationally complex and their deployment over resource-constrained\nplatforms is difficult without additional optimizations and customization.\n</p>\n<p>In this manuscript, we describe an overview of DNN architecture and propose\nmethods to reduce computational complexity in order to accelerate training and\ninference speeds to fit them on edge computing platforms with low computational\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Im_M/0/1/0/all/0/1\">Mee Seong Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasari_V/0/1/0/all/0/1\">Venkat R. Dasari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph Theoretic Exploration of Coronary Vascular Trees. (arXiv:2207.14624v1 [cs.DM])","link":"http://arxiv.org/abs/2207.14624","description":"<p>The aim of this study was to automate the generation of small coronary\nvascular networks from large point clouds that represent the coronary arterial\nnetwork. Smaller networks that can be generated in a predictable manner can be\nused to assess the impact of network morphometry on, for example, blood flow in\nhemodynamic simulations. We develop a set of algorithms for generating coronary\nvascular networks from large point clouds. These algorithms sort the point\ncloud, simplify its network structure without information loss, and produce\nsubgraphs based on given, physiologically meaningful parameters. The data were\noriginally collected from optical fluorescence cryomicrotome images and\nprocessed before their use here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mackenzie_J/0/1/0/all/0/1\">Jay Aodh Mackenzie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-Aware Differential Privacy with Conditional Invertible Neural Networks. (arXiv:2207.14625v1 [cs.CR])","link":"http://arxiv.org/abs/2207.14625","description":"<p>Differential privacy (DP) has arisen as the gold standard in protecting an\nindividual's privacy in datasets by adding calibrated noise to each data\nsample. While the application to categorical data is straightforward, its\nusability in the context of images has been limited. Contrary to categorical\ndata the meaning of an image is inherent in the spatial correlation of\nneighboring pixels making the simple application of noise infeasible.\nInvertible Neural Networks (INN) have shown excellent generative performance\nwhile still providing the ability to quantify the exact likelihood. Their\nprinciple is based on transforming a complicated distribution into a simple one\ne.g. an image into a spherical Gaussian. We hypothesize that adding noise to\nthe latent space of an INN can enable differentially private image\nmodification. Manipulation of the latent space leads to a modified image while\npreserving important details. Further, by conditioning the INN on meta-data\nprovided with the dataset we aim at leaving dimensions important for downstream\ntasks like classification untouched while altering other parts that potentially\ncontain identifying information. We term our method content-aware differential\nprivacy (CADP). We conduct experiments on publicly available benchmarking\ndatasets as well as dedicated medical ones. In addition, we show the\ngeneralizability of our method to categorical data. The source code is publicly\navailable at https://github.com/Cardio-AI/CADP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolle_M/0/1/0/all/0/1\">Malte T&#xf6;lle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1\">Ullrich K&#xf6;the</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_F/0/1/0/all/0/1\">Florian Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meder_B/0/1/0/all/0/1\">Benjamin Meder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models. (arXiv:2207.14626v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14626","description":"<p>Image restoration under adverse weather conditions has been of significant\ninterest for various computer vision applications. Recent successful methods\nrely on the current progress in deep neural network architectural designs\n(e.g., with vision transformers). Motivated by the recent progress achieved\nwith state-of-the-art conditional generative models, we present a novel\npatch-based image restoration algorithm based on denoising diffusion\nprobabilistic models. Our patch-based diffusion modeling approach enables\nsize-agnostic image restoration by using a guided denoising process with\nsmoothed noise estimates across overlapping patches during inference. We\nempirically evaluate our model on benchmark datasets for image desnowing,\ncombined deraining and dehazing, and raindrop removal. We demonstrate our\napproach to achieve state-of-the-art performances on both weather-specific and\nmulti-weather image restoration, and qualitatively show strong generalization\nto real-world test images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozdenizci_O/0/1/0/all/0/1\">Ozan &#xd6;zdenizci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legenstein_R/0/1/0/all/0/1\">Robert Legenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14650","description":"<p>Artificial intelligence (AI), machine learning, and deep learning (DL)\nmethods are becoming increasingly important in the field of biomedical image\nanalysis. However, to exploit the full potential of such methods, a\nrepresentative number of experimentally acquired images containing a\nsignificant number of manually annotated objects is needed as training data.\nHere we introduce SYNTA (synthetic data) as a novel approach for the generation\nof synthetic, photo-realistic, and highly complex biomedical images as training\ndata for DL systems. We show the versatility of our approach in the context of\nmuscle fiber and connective tissue analysis in histological sections. We\ndemonstrate that it is possible to perform robust and expert-level segmentation\ntasks on previously unseen real-world data, without the need for manual\nannotations using synthetic training data alone. Being a fully parametric\ntechnique, our approach poses an interpretable and controllable alternative to\nGenerative Adversarial Networks (GANs) and has the potential to significantly\naccelerate quantitative image analysis in a variety of biomedical applications\nin microscopy and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mill_L/0/1/0/all/0/1\">Leonid Mill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aust_O/0/1/0/all/0/1\">Oliver Aust</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ackermann_J/0/1/0/all/0/1\">Jochen A. Ackermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burger_P/0/1/0/all/0/1\">Philipp Burger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pascual_M/0/1/0/all/0/1\">Monica Pascual</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palumbo_Zerr_K/0/1/0/all/0/1\">Katrin Palumbo-Zerr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kronke_G/0/1/0/all/0/1\">Gerhard Kr&#xf6;nke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uderhardt_S/0/1/0/all/0/1\">Stefan Uderhardt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schett_G/0/1/0/all/0/1\">Georg Schett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clemen_C/0/1/0/all/0/1\">Christoph S. Clemen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schroder_R/0/1/0/all/0/1\">Rolf Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holtzhausen_C/0/1/0/all/0/1\">Christian Holtzhausen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1\">Samir Jabari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gruneboom_A/0/1/0/all/0/1\">Anika Gr&#xfc;neboom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal SuperCon: Classifier for Drivers of Deforestation in Indonesia. (arXiv:2207.14656v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14656","description":"<p>Deforestation is one of the contributing factors to climate change. Climate\nchange has a serious impact on human life, and it occurs due to emission of\ngreenhouse gases, such as carbon dioxide, to the atmosphere. It is important to\nknow the causes of deforestation for mitigation efforts, but there is a lack of\ndata-driven research studies to predict these deforestation drivers. In this\nwork, we propose a contrastive learning architecture, called Multimodal\nSuperCon, for classifying drivers of deforestation in Indonesia using satellite\nimages obtained from Landsat 8. Multimodal SuperCon is an architecture which\ncombines contrastive learning and multimodal fusion to handle the available\ndeforestation dataset. Our proposed model outperforms previous work on driver\nclassification, giving a 7% improvement in accuracy in comparison to a\nstate-of-the-art rotation equivariant model for the same task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartanti_B/0/1/0/all/0/1\">Bella Septina Ika Hartanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vito_V/0/1/0/all/0/1\">Valentino Vito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arymurthy_A/0/1/0/all/0/1\">Aniati Murni Arymurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Setiyoko_A/0/1/0/all/0/1\">Andie Setiyoko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching with AffNet based rectifications. (arXiv:2207.14660v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14660","description":"<p>We consider the problem of two-view matching under significant viewpoint\nchanges with view synthesis. We propose two novel methods, minimizing the view\nsynthesis overhead. The first one, named DenseAffNet, uses dense affine shapes\nestimates from AffNet, which allows it to partition the image, rectifying each\npartition with just a single affine map. The second one, named DepthAffNet,\ncombines information from depth maps and affine shapes estimates to produce\ndifferent sets of rectifying affine maps for different image partitions.\nDenseAffNet is faster than the state-of-the-art and more accurate on generic\nscenes. DepthAffNet is on par with the state of the art on scenes containing\nlarge planes. The evaluation is performed on 3 public datasets - EVD Dataset,\nStrong ViewPoint Changes Dataset and IMC Phototourism Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vavra_V/0/1/0/all/0/1\">V&#xe1;clav V&#xe1;vra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1\">Dmytro Mishkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Matas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling. (arXiv:2207.14663v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14663","description":"<p>Personalised 3D vascular models are valuable for diagnosis, prognosis and\ntreatment planning in patients with cardiovascular disease. Traditionally, such\nmodels have been constructed with explicit representations such as meshes and\nvoxel masks, or implicit representations such as radial basis functions or\natomic (tubular) shapes. Here, we propose to represent surfaces by the zero\nlevel set of their signed distance function (SDF) in a differentiable implicit\nneural representation (INR). This allows us to model complex vascular\nstructures with a representation that is implicit, continuous, light-weight,\nand easy to integrate with deep learning algorithms. We here demonstrate the\npotential of this approach with three practical examples. First, we obtain an\naccurate and watertight surface for an abdominal aortic aneurysm (AAA) from CT\nimages and show robust fitting from as little as 200 points on the surface.\nSecond, we simultaneously fit nested vessel walls in a single INR without\nintersections. Third, we show how 3D models of individual arteries can be\nsmoothly blended into a single watertight surface. Our results show that INRs\nare a flexible representation with potential for minimally interactive\nannotation and manipulation of complex vascular structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alblas_D/0/1/0/all/0/1\">Dieuwertje Alblas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brune_C/0/1/0/all/0/1\">Christoph Brune</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_K/0/1/0/all/0/1\">Kak Khee Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolterink_J/0/1/0/all/0/1\">Jelmer M. Wolterink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Dynamic Range and Super-Resolution from Raw Image Bursts. (arXiv:2207.14671v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14671","description":"<p>Photographs captured by smartphones and mid-range cameras have limited\nspatial resolution and dynamic range, with noisy response in underexposed\nregions and color artefacts in saturated areas. This paper introduces the first\napproach (to the best of our knowledge) to the reconstruction of\nhigh-resolution, high-dynamic range color images from raw photographic bursts\ncaptured by a handheld camera with exposure bracketing. This method uses a\nphysically-accurate model of image formation to combine an iterative\noptimization algorithm for solving the corresponding inverse problem with a\nlearned image representation for robust alignment and a learned natural image\nprior. The proposed algorithm is fast, with low memory requirements compared to\nstate-of-the-art learning-based approaches to image restoration, and features\nthat are learned end to end from synthetic yet realistic data. Extensive\nexperiments demonstrate its excellent performance with super-resolution factors\nof up to $\\times 4$ on real photographs taken in the wild with hand-held\ncameras, and high robustness to low-light conditions, noise, camera shake, and\nmoderate object motion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lecouat_B/0/1/0/all/0/1\">Bruno Lecouat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eboli_T/0/1/0/all/0/1\">Thomas Eboli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Laser-Scan Matching with Online Error Estimation for Highway and Tunnel Driving. (arXiv:2207.14674v1 [cs.RO])","link":"http://arxiv.org/abs/2207.14674","description":"<p>Lidar data can be used to generate point clouds for the navigation of\nautonomous vehicles or mobile robotics platforms. Scan matching, the process of\nestimating the rigid transformation that best aligns two point clouds, is the\nbasis for lidar odometry, a form of dead reckoning. Lidar odometry is\nparticularly useful when absolute sensors, like GPS, are not available. Here we\npropose the Iterative Closest Ellipsoidal Transform (ICET), a scan matching\nalgorithm which provides two novel improvements over the current\nstate-of-the-art Normal Distributions Transform (NDT). Like NDT, ICET\ndecomposes lidar data into voxels and fits a Gaussian distribution to the\npoints within each voxel. The first innovation of ICET reduces geometric\nambiguity along large flat surfaces by suppressing the solution along those\ndirections. The second innovation of ICET is to infer the output error\ncovariance associated with the position and orientation transformation between\nsuccessive point clouds; the error covariance is particularly useful when ICET\nis incorporated into a state-estimation routine such as an extended Kalman\nfilter. We constructed a simulation to compare the performance of ICET and NDT\nin 2D space both with and without geometric ambiguity and found that ICET\nproduces superior estimates while accurately predicting solution accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McDermott_M/0/1/0/all/0/1\">Matthew McDermott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rife_J/0/1/0/all/0/1\">Jason Rife</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-Local Self-Distillation for Visual Representation Learning. (arXiv:2207.14676v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14676","description":"<p>The downstream accuracy of self-supervised methods is tightly linked to the\nproxy task solved during training and the quality of the gradients extracted\nfrom it. Richer and more meaningful gradients updates are key to allow\nself-supervised methods to learn better and in a more efficient manner. In a\ntypical self-distillation framework, the representation of two augmented images\nare enforced to be coherent at the global level. Nonetheless, incorporating\nlocal cues in the proxy task can be beneficial and improve the model accuracy\non downstream tasks. This leads to a dual objective in which, on the one hand,\ncoherence between global-representations is enforced and on the other,\ncoherence between local-representations is enforced. Unfortunately, an exact\ncorrespondence mapping between two sets of local-representations does not exist\nmaking the task of matching local-representations from one augmentation to\nanother non-trivial. We propose to leverage the spatial information in the\ninput images to obtain geometric matchings and compare this geometric approach\nagainst previous methods based on similarity matchings. Our study shows that\nnot only 1) geometric matchings perform better than similarity based matchings\nin low-data regimes but also 2) that similarity based matchings are highly\nhurtful in low-data regimes compared to the vanilla baseline without local\nself-distillation. The code will be released upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lebailly_T/0/1/0/all/0/1\">Tim Lebailly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlphaVC: High-Performance and Efficient Learned Video Compression. (arXiv:2207.14678v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14678","description":"<p>Recently, learned video compression has drawn lots of attention and show a\nrapid development trend with promising results. However, the previous works\nstill suffer from some criticial issues and have a performance gap with\ntraditional compression standards in terms of widely used PSNR metric. In this\npaper, we propose several techniques to effectively improve the performance.\nFirst, to address the problem of accumulative error, we introduce a\nconditional-I-frame as the first frame in the GoP, which stabilizes the\nreconstructed quality and saves the bit-rate. Second, to efficiently improve\nthe accuracy of inter prediction without increasing the complexity of decoder,\nwe propose a pixel-to-feature motion prediction method at encoder side that\nhelps us to obtain high-quality motion information. Third, we propose a\nprobability-based entropy skipping method, which not only brings performance\ngain, but also greatly reduces the runtime of entropy coding. With these\npowerful techniques, this paper proposes AlphaVC, a high-performance and\nefficient learned video compression scheme. To the best of our knowledge,\nAlphaVC is the first E2E AI codec that exceeds the latest compression standard\nVVC on all common test datasets for both PSNR (-28.2% BD-rate saving) and\nMSSSIM (-52.2% BD-rate saving), and has very fast encoding (0.001x VVC) and\ndecoding (1.69x VVC) speeds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yibo Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yunying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jue Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks. (arXiv:2207.14682v1 [cs.SD])","link":"http://arxiv.org/abs/2207.14682","description":"<p>Freely available and easy-to-use audio editing tools make it straightforward\nto perform audio splicing. Convincing forgeries can be created by combining\nvarious speech samples from the same person. Detection of such splices is\nimportant both in the public sector when considering misinformation, and in a\nlegal context to verify the integrity of evidence. Unfortunately, most existing\ndetection algorithms for audio splicing use handcrafted features and make\nspecific assumptions. However, criminal investigators are often faced with\naudio samples from unconstrained sources with unknown characteristics, which\nraises the need for more generally applicable methods.\n</p>\n<p>With this work, we aim to take a first step towards unconstrained audio\nsplicing detection to address this need. We simulate various attack scenarios\nin the form of post-processing operations that may disguise splicing. We\npropose a Transformer sequence-to-sequence (seq2seq) network for splicing\ndetection and localization. Our extensive evaluation shows that the proposed\nmethod outperforms existing dedicated approaches for splicing detection [3, 10]\nas well as the general-purpose networks EfficientNet [28] and RegNet [25].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moussa_D/0/1/0/all/0/1\">Denise Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_G/0/1/0/all/0/1\">Germans Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1\">Christian Riess</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Replacing the Framingham-based equation for prediction of cardiovascular disease risk and adverse outcome by using artificial intelligence and retinal imaging. (arXiv:2207.14685v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14685","description":"<p>Purpose: To create and evaluate the accuracy of an artificial intelligence\nDeep learning platform (ORAiCLE) capable of using only retinal fundus images to\npredict both an individuals overall 5 year cardiovascular risk (CVD) and the\nrelative contribution of the component risk factors that comprise this risk.\nMethods: We used 165,907 retinal images from a database of 47,236 patient\nvisits. Initially, each image was paired with biometric data age, ethnicity,\nsex, presence and duration of diabetes a HDL/LDL ratios as well as any CVD\nevent wtihin 5 years of the retinal image acquisition. A risk score based on\nFramingham equations was calculated. The real CVD event rate was also\ndetermined for the individuals and overall population. Finally, ORAiCLE was\ntrained using only age, ethnicity, sex plus retinal images. Results: Compared\nto Framingham-based score, ORAiCLE was up to 12% more accurate in prediciting\ncardiovascular event in he next 5-years, especially for the highest risk group\nof people. The reliability and accuracy of each of the restrictive models was\nsuboptimal to ORAiCLE performance ,indicating that it was using data from both\nsets of data to derive its final results. Conclusion: Retinal photography is\ninexpensive and only minimal training is required to acquire them as fully\nautomated, inexpensive camera systems are now widely available. As such,\nAI-based CVD risk algorithms such as ORAiCLE promise to make CV health\nscreening more accurate, more afforadable and more accessible for all.\nFurthermore, ORAiCLE unique ability to assess the relative contribution of the\ncomponents that comprise an individuals overall risk would inform treatment\ndecisions based on the specific needs of an individual, thereby increasing the\nlikelihood of positive health outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vaghefi_E/0/1/0/all/0/1\">Ehsan Vaghefi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Squirrell_D/0/1/0/all/0/1\">David Squirrell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_S/0/1/0/all/0/1\">Songyang An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Song Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marshall_J/0/1/0/all/0/1\">John Marshall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forensic License Plate Recognition with Compression-Informed Transformers. (arXiv:2207.14686v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14686","description":"<p>Forensic license plate recognition (FLPR) remains an open challenge in legal\ncontexts such as criminal investigations, where unreadable license plates (LPs)\nneed to be deciphered from highly compressed and/or low resolution footage,\ne.g., from surveillance cameras. In this work, we propose a side-informed\nTransformer architecture that embeds knowledge on the input compression level\nto improve recognition under strong compression. We show the effectiveness of\nTransformers for license plate recognition (LPR) on a low-quality real-world\ndataset. We also provide a synthetic dataset that includes strongly degraded,\nillegible LP images and analyze the impact of knowledge embedding on it. The\nnetwork outperforms existing FLPR methods and standard state-of-the art image\nrecognition models while requiring less parameters. For the severest degraded\nimages, we can improve recognition by up to 8.9 percent points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moussa_D/0/1/0/all/0/1\">Denise Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Anatol Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spruck_A/0/1/0/all/0/1\">Andreas Spruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seiler_J/0/1/0/all/0/1\">J&#xfc;rgen Seiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1\">Christian Riess</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding. (arXiv:2207.14698v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14698","description":"<p>Temporal grounding aims to locate a target video moment that semantically\ncorresponds to the given sentence query in an untrimmed video. However, recent\nworks find that existing methods suffer a severe temporal bias problem. These\nmethods do not reason the target moment locations based on the visual-textual\nsemantic alignment but over-rely on the temporal biases of queries in training\nsets. To this end, this paper proposes a novel training framework for grounding\nmodels to use shuffled videos to address temporal bias problem without losing\ngrounding accuracy. Our framework introduces two auxiliary tasks, cross-modal\nmatching and temporal order discrimination, to promote the grounding model\ntraining. The cross-modal matching task leverages the content consistency\nbetween shuffled and original videos to force the grounding model to mine\nvisual contents to semantically match queries. The temporal order\ndiscrimination task leverages the difference in temporal order to strengthen\nthe understanding of long-term temporal contexts. Extensive experiments on\nCharades-STA and ActivityNet Captions demonstrate the effectiveness of our\nmethod for mitigating the reliance on temporal biases and strengthening the\nmodel's generalization ability against the different temporal distributions.\nCode is available at https://github.com/haojc/ShufflingVideosForTSG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jiachang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haifeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengfei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jianxin Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Small Lesion Segmentation in CT Scans using Intensity Distribution Supervision: Application to Small Bowel Carcinoid Tumor. (arXiv:2207.14700v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14700","description":"<p>Finding small lesions is very challenging due to lack of noticeable features,\nsevere class imbalance, as well as the size itself. One approach to improve\nsmall lesion segmentation is to reduce the region of interest and inspect it at\na higher sensitivity rather than performing it for the entire region. It is\nusually implemented as sequential or joint segmentation of organ and lesion,\nwhich requires additional supervision on organ segmentation. Instead, we\npropose to utilize an intensity distribution of a target lesion at no\nadditional labeling cost to effectively separate regions where the lesions are\npossibly located from the background. It is incorporated into network training\nas an auxiliary task. We applied the proposed method to segmentation of small\nbowel carcinoid tumors in CT scans. We observed improvements for all metrics\n(33.5% $\\rightarrow$ 38.2%, 41.3% $\\rightarrow$ 47.8%, 30.0% $\\rightarrow$\n35.9% for the global, per case, and per tumor Dice scores, respectively.)\ncompared to the baseline method, which proves the validity of our idea. Our\nmethod can be one option for explicitly incorporating intensity distribution\ninformation of a target in network training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shin_S/0/1/0/all/0/1\">Seung Yeon Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_T/0/1/0/all/0/1\">Thomas C. Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wank_S/0/1/0/all/0/1\">Stephen A. Wank</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Summers_R/0/1/0/all/0/1\">Ronald M. Summers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Quantitative Susceptibility Mapping via Approximate Message Passing. (arXiv:2207.14709v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14709","description":"<p>Purpose: It has been challenging to recover QSM in the presence of phase\nerrors, which could be caused by the noise or strong local susceptibility\nshifts in cases of brain hemorrhage and calcification. We propose a Bayesian\nformulation for QSM where a two-component Gaussian-mixture distribution is used\nto model the long-tailed noise (error) distribution, and design an approximate\nmessage passing (AMP) algorithm with automatic and adaptive parameter\nestimation.\n</p>\n<p>Theory: Wavelet coefficients of the susceptibility map follow the Laplace\ndistribution. The measurement noise follows a two-component Gaussian-mixture\ndistribution where the second Gaussian component models the noise outliers. The\ndistribution parameters are treated as unknown variables and jointly recovered\nwith the susceptibility using AMP.\n</p>\n<p>Methods: The proposed AMP with parameter estimation (AMP-PE) is compared with\nthe state-of-the-art nonlinear L1-QSM and MEDI approaches that adopt the\nL1-norm and L2-norm data-fidelity terms respectively. The three approaches are\ntested on the Sim2Snr1 data from QSM challenge 2.0, the in vivo data from both\nhealthy and hemorrhage scans.\n</p>\n<p>Results: On the simulated Sim2Snr1 dataset, AMP-PE achieved the lowest NRMSE\nand SSIM, MEDI achieved the lowest HFEN, and each approach also has its own\nstrong suit when it comes to various local evaluation metrics. On the in vivo\ndataset, AMP-PE is better at preserving structural details and removing\nstreaking artifacts than L1-QSM and MEDI.\n</p>\n<p>Conclusion: By leveraging a customized Gaussian-mixture noise prior, AMP-PE\nachieves better performance on the challenging QSM cases involving hemorrhage\nand calcification. It is equipped with built-in parameter estimation, which\navoids subjective bias from the usual visual fine-tuning step of in vivo\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1\">Shuai Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lah_J/0/1/0/all/0/1\">James J. Lah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Allen_J/0/1/0/all/0/1\">Jason W. Allen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_D/0/1/0/all/0/1\">Deqiang Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end View Synthesis via NeRF Attention. (arXiv:2207.14741v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14741","description":"<p>In this paper, we present a simple seq2seq formulation for view synthesis\nwhere we take a set of ray points as input and output colors corresponding to\nthe rays. Directly applying a standard transformer on this seq2seq formulation\nhas two limitations. First, the standard attention cannot successfully fit the\nvolumetric rendering procedure, and therefore high-frequency components are\nmissing in the synthesized views. Second, applying global attention to all rays\nand pixels is extremely inefficient. Inspired by the neural radiance field\n(NeRF), we propose the NeRF attention (NeRFA) to address the above problems. On\nthe one hand, NeRFA considers the volumetric rendering equation as a soft\nfeature modulation procedure. In this way, the feature modulation enhances the\ntransformers with the NeRF-like inductive bias. On the other hand, NeRFA\nperforms multi-stage attention to reduce the computational overhead.\nFurthermore, the NeRFA model adopts the ray and pixel transformers to learn the\ninteractions between rays and pixels. NeRFA demonstrates superior performance\nover NeRF and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D.\nBesides, NeRFA establishes a new state-of-the-art under two settings: the\nsingle-scene view synthesis and the category-centric novel view synthesis. The\ncode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zelin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval. (arXiv:2207.14757v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14757","description":"<p>Image-text matching is gaining a leading role among tasks involving the joint\nunderstanding of vision and language. In literature, this task is often used as\na pre-training objective to forge architectures able to jointly deal with\nimages and texts. Nonetheless, it has a direct downstream application:\ncross-modal retrieval, which consists in finding images related to a given\nquery text or vice-versa. Solving this task is of critical importance in\ncross-modal search engines. Many recent methods proposed effective solutions to\nthe image-text matching problem, mostly using recent large vision-language (VL)\nTransformer networks. However, these models are often computationally\nexpensive, especially at inference time. This prevents their adoption in\nlarge-scale cross-modal retrieval scenarios, where results should be provided\nto the user almost instantaneously. In this paper, we propose to fill in the\ngap between effectiveness and efficiency by proposing an ALign And DIstill\nNetwork (ALADIN). ALADIN first produces high-effective scores by aligning at\nfine-grained level images and texts. Then, it learns a shared embedding space -\nwhere an efficient kNN search can be performed - by distilling the relevance\nscores obtained from the fine-grained alignments. We obtained remarkable\nresults on MS-COCO, showing that our method can compete with state-of-the-art\nVL Transformers while being almost 90 times faster. The code for reproducing\nour results is available at https://github.com/mesnico/ALADIN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messina_N/0/1/0/all/0/1\">Nicola Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1\">Giuseppe Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Quality Assessment: Integrating Model-Centric and Data-Centric Approaches. (arXiv:2207.14769v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14769","description":"<p>Learning-based image quality assessment (IQA) has made remarkable progress in\nthe past decade, but nearly all consider the two key components - model and\ndata - in relative isolation. Specifically, model-centric IQA focuses on\ndeveloping \"better\" objective quality methods on fixed and extensively reused\ndatasets, with a great danger of overfitting. Data-centric IQA involves\nconducting psychophysical experiments to construct \"better\" human-annotated\ndatasets, which unfortunately ignores current IQA models during dataset\ncreation. In this paper, we first design a series of experiments to probe\ncomputationally that such isolation of model and data impedes further progress\nof IQA. We then describe a computational framework that integrates\nmodel-centric and data-centric IQA. As a specific example, we design\ncomputational modules to quantify the sampling-worthiness of candidate images\nbased on blind IQA (BIQA) model predictions and deep content-aware features.\nExperimental results show that the proposed sampling-worthiness module\nsuccessfully spots diverse failures of the examined BIQA models, which are\nindeed worthy samples to be included in next-generation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Peibei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-radiomics: A Research Protocol to Make Radiomics-based Machine Learning Pipelines Reproducible. (arXiv:2207.14776v1 [q-bio.QM])","link":"http://arxiv.org/abs/2207.14776","description":"<p>The application of artificial intelligence (AI) techniques to medical imaging\ndata has yielded promising results. As an important branch of AI pipelines in\nmedical imaging, radiomics faces two major challenges namely reproducibility\nand accessibility. In this work, we introduce open-radiomics, a set of\nradiomics datasets, and a comprehensive radiomics pipeline that investigates\nthe effects of radiomics feature extraction settings such as binWidth and image\nnormalization on the reproducibility of the radiomics results performance. To\nmake radiomics research more accessible and reproducible, we provide guidelines\nfor building machine learning (ML) models on radiomics data, introduce\nOpen-radiomics, an evolving collection of open-source radiomics datasets, and\npublish baseline models for the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ernest/0/1/0/all/0/1\">Ernest</a> (Khashayar) <a href=\"http://arxiv.org/find/q-bio/1/au:+Namdar/0/1/0/all/0/1\">Namdar</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wagner_M/0/1/0/all/0/1\">Matthias W. Wagner</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ertl_Wagner_B/0/1/0/all/0/1\">Birgit B. Ertl-Wagner</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khalvati_F/0/1/0/all/0/1\">Farzad Khalvati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Multi-modal Data for Improving Generalizability and Explainability of Disease Classification in Radiology. (arXiv:2207.14781v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14781","description":"<p>Traditional datasets for the radiological diagnosis tend to only provide the\nradiology image alongside the radiology report. However, radiology reading as\nperformed by radiologists is a complex process, and information such as the\nradiologist's eye-fixations over the course of the reading has the potential to\nbe an invaluable data source to learn from. Nonetheless, the collection of such\ndata is expensive and time-consuming. This leads to the question of whether\nsuch data is worth the investment to collect. This paper utilizes the recently\npublished Eye-Gaze dataset to perform an exhaustive study on the impact on\nperformance and explainability of deep learning (DL) classification in the face\nof varying levels of input features, namely: radiology images, radiology report\ntext, and radiologist eye-gaze data. We find that the best classification\nperformance of X-ray images is achieved with a combination of radiology report\nfree-text and radiology image, with the eye-gaze data providing no performance\nboost. Nonetheless, eye-gaze data serving as secondary ground truth alongside\nthe class label results in highly explainable models that generate better\nattention maps compared to models trained to do classification and attention\nmap generation without eye-gaze data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agnihotri_P/0/1/0/all/0/1\">Pranav Agnihotri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ketabi_S/0/1/0/all/0/1\">Sara Ketabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashayar/0/1/0/all/0/1\">Khashayar</a> (Ernest) <a href=\"http://arxiv.org/find/cs/1/au:+Namdar/0/1/0/all/0/1\">Namdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalvati_F/0/1/0/all/0/1\">Farzad Khalvati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion. (arXiv:2207.14782v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14782","description":"<p>Explicit neural surface representations allow for exact and efficient\nextraction of the encoded surface at arbitrary precision, as well as analytic\nderivation of differential geometric properties such as surface normal and\ncurvature. Such desirable properties, which are absent in its implicit\ncounterpart, makes it ideal for various applications in computer vision,\ngraphics and robotics. However, SOTA works are limited in terms of the topology\nit can effectively describe, distortion it introduces to reconstruct complex\nsurfaces and model efficiency. In this work, we present Minimal Neural Atlas, a\nnovel atlas-based explicit neural surface representation. At its core is a\nfully learnable parametric domain, given by an implicit probabilistic occupancy\nfield defined on an open square of the parametric space. In contrast, prior\nworks generally predefine the parametric domain. The added flexibility enables\ncharts to admit arbitrary topology and boundary. Thus, our representation can\nlearn a minimal atlas of 3 charts with distortion-minimal parameterization for\nsurfaces of arbitrary topology, including closed and open surfaces with\narbitrary connected components. Our experiments support the hypotheses and show\nthat our reconstructions are more accurate in terms of the overall geometry,\ndue to the separation of concerns on topology and geometry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Low_W/0/1/0/all/0/1\">Weng Fei Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition of Handwritten Chinese Text by Segmentation: A Segment-annotation-free Approach. (arXiv:2207.14801v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14801","description":"<p>Online and offline handwritten Chinese text recognition (HTCR) has been\nstudied for decades. Early methods adopted oversegmentation-based strategies\nbut suffered from low speed, insufficient accuracy, and high cost of character\nsegmentation annotations. Recently, segmentation-free methods based on\nconnectionist temporal classification (CTC) and attention mechanism, have\ndominated the field of HCTR. However, people actually read text character by\ncharacter, especially for ideograms such as Chinese. This raises the question:\nare segmentation-free strategies really the best solution to HCTR? To explore\nthis issue, we propose a new segmentation-based method for recognizing\nhandwritten Chinese text that is implemented using a simple yet efficient fully\nconvolutional network. A novel weakly supervised learning method is proposed to\nenable the network to be trained using only transcript annotations; thus, the\nexpensive character segmentation annotations required by previous\nsegmentation-based methods can be avoided. Owing to the lack of context\nmodeling in fully convolutional networks, we propose a contextual\nregularization method to integrate contextual information into the network\nduring the training stage, which can further improve the recognition\nperformance. Extensive experiments conducted on four widely used benchmarks,\nnamely CASIA-HWDB, CASIA-OLHWDB, ICDAR2013, and SCUT-HCCDoc, show that our\nmethod significantly surpasses existing methods on both online and offline\nHCTR, and exhibits a considerably higher inference speed than\nCTC/attention-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weihong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Canyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hesuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artifact Identification in X-ray Diffraction Data using Machine Learning Methods. (arXiv:2207.14804v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14804","description":"<p>The in situ synchrotron high-energy X-ray powder diffraction (XRD) technique\nis highly utilized by researchers to analyze the crystallographic structures of\nmaterials in functional devices (e.g., battery materials) or in complex sample\nenvironments (e.g., diamond anvil cells or syntheses reactors). An atomic\nstructure of a material can be identified by its diffraction pattern, along\nwith detailed analysis such as Rietveld refinement which indicates how the\nmeasured structure deviates from the ideal structure (e.g., internal stresses\nor defects). For in situ experiments, a series of XRD images is usually\ncollected on the same sample at different conditions (e.g., adiabatic\nconditions), yielding different states of matter, or simply collected\ncontinuously as a function of time to track the change of a sample over a\nchemical or physical process. In situ experiments are usually performed with\narea detectors, collecting 2D images composed of diffraction rings for ideal\npowders. Depending on the material's form, one may observe different\ncharacteristics other than the typical Debye Scherrer rings for a realistic\nsample and its environments, such as textures or preferred orientations and\nsingle crystal diffraction spots in the 2D XRD image. In this work, we present\nan investigation of machine learning methods for fast and reliable\nidentification and separation of the single crystal diffraction spots in XRD\nimages. The exclusion of artifacts during an XRD image integration process\nallows a precise analysis of the powder diffraction rings of interest. We\nobserve that the gradient boosting method can consistently produce high\naccuracy results when it is trained with small subsets of highly diverse\ndatasets. The method dramatically decreases the amount of time spent on\nidentifying and separating single crystal spots in comparison to the\nconventional method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yanxon_H/0/1/0/all/0/1\">Howard Yanxon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weng_J/0/1/0/all/0/1\">James Weng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parraga_H/0/1/0/all/0/1\">Hannah Parraga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1\">Wenqian Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruett_U/0/1/0/all/0/1\">Uta Ruett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwarz_N/0/1/0/all/0/1\">Nicholas Schwarz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition. (arXiv:2207.14807v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14807","description":"<p>Handwritten Chinese text recognition (HCTR) has been an active research topic\nfor decades. However, most previous studies solely focus on the recognition of\ncropped text line images, ignoring the error caused by text line detection in\nreal-world applications. Although some approaches aimed at page-level text\nrecognition have been proposed in recent years, they either are limited to\nsimple layouts or require very detailed annotations including expensive\nline-level and even character-level bounding boxes. To this end, we propose\nPageNet for end-to-end weakly supervised page-level HCTR. PageNet detects and\nrecognizes characters and predicts the reading order between them, which is\nmore robust and flexible when dealing with complex layouts including\nmulti-directional and curved text lines. Utilizing the proposed weakly\nsupervised learning framework, PageNet requires only transcripts to be\nannotated for real data; however, it can still output detection and recognition\nresults at both the character and line levels, avoiding the labor and cost of\nlabeling bounding boxes of characters and text lines. Extensive experiments\nconducted on five datasets demonstrate the superiority of PageNet over existing\nweakly supervised and fully supervised page-level methods. These experimental\nresults may spark further research beyond the realms of existing methods based\non connectionist temporal classification or attention. The source code is\navailable at https://github.com/shannanyinxiang/PageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songxuan Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleLight: HDR Panorama Generation for Lighting Estimation and Editing. (arXiv:2207.14811v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14811","description":"<p>We present a new lighting estimation and editing framework to generate\nhigh-dynamic-range (HDR) indoor panorama lighting from a single limited\nfield-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras.\nExisting lighting estimation methods either directly regress lighting\nrepresentation parameters or decompose this problem into LFOV-to-panorama and\nLDR-to-HDR lighting generation sub-tasks. However, due to the partial\nobservation, the high-dynamic-range lighting, and the intrinsic ambiguity of a\nscene, lighting estimation remains a challenging task. To tackle this problem,\nwe propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that\nintegrates LDR and HDR panorama synthesis into a unified framework. The LDR and\nHDR panorama synthesis share a similar generator but have separate\ndiscriminators. During inference, given an LDR LFOV image, we propose a\nfocal-masked GAN inversion method to find its latent code by the LDR panorama\nsynthesis branch and then synthesize the HDR panorama by the HDR panorama\nsynthesis branch. StyleLight takes LFOV-to-panorama and LDR-to-HDR lighting\ngeneration into a unified framework and thus greatly improves lighting\nestimation. Extensive experiments demonstrate that our framework achieves\nsuperior performance over state-of-the-art methods on indoor lighting\nestimation. Notably, StyleLight also enables intuitive lighting editing on\nindoor HDR panoramas, which is suitable for real-world applications. Code is\navailable at https://style-light.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond. (arXiv:2207.14812v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14812","description":"<p>We show that pre-trained Generative Adversarial Networks (GANs) such as\nStyleGAN and BigGAN can be used as a latent bank to improve the performance of\nimage super-resolution. While most existing perceptual-oriented approaches\nattempt to generate realistic outputs through learning with adversarial loss,\nour method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by\ndirectly leveraging rich and diverse priors encapsulated in a pre-trained GAN.\nBut unlike prevalent GAN inversion methods that require expensive\nimage-specific optimization at runtime, our approach only needs a single\nforward pass for restoration. GLEAN can be easily incorporated in a simple\nencoder-bank-decoder architecture with multi-resolution skip connections.\nEmploying priors from different generative models allows GLEAN to be applied to\ndiverse categories (\\eg~human faces, cats, buildings, and cars). We further\npresent a lightweight version of GLEAN, named LightGLEAN, which retains only\nthe critical components in GLEAN. Notably, LightGLEAN consists of only 21% of\nparameters and 35% of FLOPs while achieving comparable image quality. We extend\nour method to different tasks including image colorization and blind image\nrestoration, and extensive experiments show that our proposed models perform\nfavorably in comparison to existing methods. Codes and models are available at\nhttps://github.com/open-mmlab/mmediting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kelvin C.K. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Data Augmentation for LiDAR based 3D Object Detection. (arXiv:2004.01643v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.01643","description":"<p>In this work, we shed light on different data augmentation techniques\ncommonly used in Light Detection and Ranging (LiDAR) based 3D Object Detection.\nFor the bulk of our experiments, we utilize the well known PointPillars\npipeline and the well established KITTI dataset. We investigate a variety of\nglobal and local augmentation techniques, where global augmentation techniques\nare applied to the entire point cloud of a scene and local augmentation\ntechniques are only applied to points belonging to individual objects in the\nscene. Our findings show that both types of data augmentation can lead to\nperformance increases, but it also turns out, that some augmentation\ntechniques, such as individual object translation, for example, can be\ncounterproductive and can hurt the overall performance. We show that these\nfindings transfer and generalize well to other state of the art 3D Object\nDetection methods and the challenging STF dataset. On the KITTI dataset we can\ngain up to 1.5% and on the STF dataset up to 1.7% in 3D mAP on the moderate car\nclass.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahner_M/0/1/0/all/0/1\">Martin Hahner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effects of Image Size on Deep Learning. (arXiv:2101.11508v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11508","description":"<p>This paper presents the effects of late gadolinium enhancement (LGE) magnetic\nresonance imaging (MRI) image size on deep learning based fully automated\nquantification of myocardial infarction (MI). The main objective is to\ndetermine the best size for LGE MRI images in the training dataset to achieve\noptimal deep learning training outcomes. To determine the new size of LGE MRI\nimages of the reference training dataset, non-extra pixel and extra pixel\ninterpolation algorithms are used. A novel strategy based on thresholding,\nmedian filtering, and subtraction operations is introduced and applied to\nremove extra class labels in interpolated ground truth (GT) segmentation masks.\nFully automated quantification is achieved using the expectation maximization,\nweighted intensity, a priori information (EWA) algorithm, and the outcome of\nautomatic semantic segmentation of LGE-MRI images with the convolutional neural\nnetwork (CNN). In the experiments, common class metrics are used to evaluate\nthe quality of semantic segmentation with a CNN architecture of interest\n(U-net) against the GT segmentation. Arbitrary threshold, comparison of the\nsums, and sums of differences are used to estimate the relationship between\nsemi-automatic and fully automated quantification of MI results. A close\nrelationship between semi-automatic and fully automated quantification of MI\nresults was more identified in the case involving the dataset of bigger LGE MRI\nimages than in that of the dataset of smaller LGE MRI images, where\nquantification results based on the dataset of bigger LGE MRI images were 55.5%\ncloser the manual or semi-automatic results while quantification results based\non the dataset of smaller LGE MRI images were 22.2% closer the manual results\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization: A Survey. (arXiv:2103.02503v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.02503","description":"<p>Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Over the last ten years, research in DG has made great progress,\nleading to a broad spectrum of methodologies, e.g., those based on domain\nalignment, meta-learning, data augmentation, or ensemble learning, to name a\nfew; DG has also been studied in various application areas including computer\nvision, speech recognition, natural language processing, medical imaging, and\nreinforcement learning. In this paper, for the first time a comprehensive\nliterature review in DG is provided to summarize the developments over the past\ndecade. Specifically, we first cover the background by formally defining DG and\nrelating it to other relevant fields like domain adaptation and transfer\nlearning. Then, we conduct a thorough review into existing methods and\ntheories. Finally, we conclude this survey with insights and discussions on\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations. (arXiv:2103.06384v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.06384","description":"<p>Machine learning and computer vision techniques have grown rapidly in recent\nyears due to their automation, suitability, and ability to generate astounding\nresults. Hence, in this paper, we survey the key studies that are published\nbetween 2014 and 2022, showcasing the different machine learning algorithms\nresearchers have used to segment the liver, hepatic tumors, and\nhepatic-vasculature structures. We divide the surveyed studies based on the\ntissue of interest (hepatic-parenchyma, hepatic-tumors, or hepatic-vessels),\nhighlighting the studies that tackle more than one task simultaneously.\nAdditionally, the machine learning algorithms are classified as either\nsupervised or unsupervised, and they are further partitioned if the amount of\nwork that falls under a certain scheme is significant. Moreover, different\ndatasets and challenges found in literature and websites containing masks of\nthe aforementioned tissues are thoroughly discussed, highlighting the\norganizers' original contributions and those of other researchers. Also, the\nmetrics used excessively in literature are mentioned in our review, stressing\ntheir relevance to the task at hand. Finally, critical challenges and future\ndirections are emphasized for innovative researchers to tackle, exposing gaps\nthat need addressing, such as the scarcity of many studies on the vessels'\nsegmentation challenge and why their absence needs to be dealt with sooner than\nlater.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Al_Kababji_A/0/1/0/all/0/1\">Ayman Al-Kababji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bensaali_F/0/1/0/all/0/1\">Faycal Bensaali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dakua_S/0/1/0/all/0/1\">Sarada Prasad Dakua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Himeur_Y/0/1/0/all/0/1\">Yassine Himeur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Subject Domain Adaptation for Classifying Working Memory Load with Multi-Frame EEG Images. (arXiv:2106.06769v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.06769","description":"<p>Working memory (WM), denoting the information temporally stored in the mind,\nis a fundamental research topic in the field of human cognition.\nElectroencephalograph (EEG), which can monitor the electrical activity of the\nbrain, has been widely used in measuring the level of WM. However, one of the\ncritical challenges is that individual differences may cause ineffective\nresults, especially when the established model meets an unfamiliar subject. In\nthis work, we propose a cross-subject deep adaptation model with spatial\nattention (CS-DASA) to generalize the workload classifications across subjects.\nFirst, we transform EEG time series into multi-frame EEG images incorporating\nspatial, spectral, and temporal information. First, the Subject-Shared module\nin CS-DASA receives multi-frame EEG image data from both source and target\nsubjects and learns the common feature representations. Then, in the\nsubject-specific module, the maximum mean discrepancy is implemented to measure\nthe domain distribution divergence in a reproducing kernel Hilbert space, which\ncan add an effective penalty loss for domain adaptation. Additionally, the\nsubject-to-subject spatial attention mechanism is employed to focus on the\ndiscriminative spatial features from the target image data. Experiments\nconducted on a public WM EEG dataset containing 13 subjects show that the\nproposed model is capable of achieving better performance than existing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dive into Deep Learning. (arXiv:2106.11342v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11342","description":"<p>This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Motion Prior for Weakly-Supervised Temporal Action Localization. (arXiv:2108.05607v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05607","description":"<p>Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize\nactions in untrimmed videos with only video-level labels. Currently, most\nstate-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline:\nproducing snippet-level predictions first and then aggregating to the\nvideo-level prediction. However, we argue that existing methods have overlooked\ntwo important drawbacks: 1) inadequate use of motion information and 2) the\nincompatibility of prevailing cross-entropy training loss. In this paper, we\nanalyze that the motion cues behind the optical flow features are complementary\ninformative. Inspired by this, we propose to build a context-dependent motion\nprior, termed as motionness. Specifically, a motion graph is introduced to\nmodel motionness based on the local motion carrier (e.g., optical flow). In\naddition, to highlight more informative video snippets, a motion-guided loss is\nproposed to modulate the network training conditioned on motionness scores.\nExtensive ablation studies confirm that motionness efficaciously models\naction-of-interest, and the motion-guided loss leads to more accurate results.\nBesides, our motion-guided loss is a plug-and-play loss function and is\napplicable with existing WSTAL methods. Without loss of generality, based on\nthe standard MIL pipeline, our method achieves new state-of-the-art performance\non three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and\nv1.3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducible radiomics through automated machine learning validated on twelve clinical applications. (arXiv:2108.08618v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.08618","description":"<p>Radiomics uses quantitative medical imaging features to predict clinical\noutcomes. Currently, in a new clinical application, finding the optimal\nradiomics method out of the wide range of available options has to be done\nmanually through a heuristic trial-and-error process. In this study we propose\na framework for automatically optimizing the construction of radiomics\nworkflows per application. To this end, we formulate radiomics as a modular\nworkflow and include a large collection of common algorithms for each\ncomponent. To optimize the workflow per application, we employ automated\nmachine learning using a random search and ensembling. We evaluate our method\nin twelve different clinical applications, resulting in the following area\nunder the curves: 1) liposarcoma (0.83); 2) desmoid-type fibromatosis (0.82);\n3) primary liver tumors (0.80); 4) gastrointestinal stromal tumors (0.77); 5)\ncolorectal liver metastases (0.61); 6) melanoma metastases (0.45); 7)\nhepatocellular carcinoma (0.75); 8) mesenteric fibrosis (0.80); 9) prostate\ncancer (0.72); 10) glioma (0.71); 11) Alzheimer's disease (0.87); and 12) head\nand neck cancer (0.84). We show that our framework has a competitive\nperformance compared human experts, outperforms a radiomics baseline, and\nperforms similar or superior to Bayesian optimization and more advanced\nensemble approaches. Concluding, our method fully automatically optimizes the\nconstruction of radiomics workflows, thereby streamlining the search for\nradiomics biomarkers in new applications. To facilitate reproducibility and\nfuture research, we publicly release six datasets, the software implementation\nof our framework, and the code to reproduce this study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Starmans_M/0/1/0/all/0/1\">Martijn P. A. Starmans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Voort_S/0/1/0/all/0/1\">Sebastian R. van der Voort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phil_T/0/1/0/all/0/1\">Thomas Phil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timbergen_M/0/1/0/all/0/1\">Milea J. M. Timbergen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vos_M/0/1/0/all/0/1\">Melissa Vos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padmos_G/0/1/0/all/0/1\">Guillaume A. Padmos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kessels_W/0/1/0/all/0/1\">Wouter Kessels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hanff_D/0/1/0/all/0/1\">David Hanff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grunhagen_D/0/1/0/all/0/1\">Dirk J. Grunhagen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verhoef_C/0/1/0/all/0/1\">Cornelis Verhoef</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sleijfer_S/0/1/0/all/0/1\">Stefan Sleijfer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bent_M/0/1/0/all/0/1\">Martin J. van den Bent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smits_M/0/1/0/all/0/1\">Marion Smits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dwarkasing_R/0/1/0/all/0/1\">Roy S. Dwarkasing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Els_C/0/1/0/all/0/1\">Christopher J. Els</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fiduzi_F/0/1/0/all/0/1\">Federico Fiduzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leenders_G/0/1/0/all/0/1\">Geert J. L. H. van Leenders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blazevic_A/0/1/0/all/0/1\">Anela Blazevic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hofland_J/0/1/0/all/0/1\">Johannes Hofland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brabander_T/0/1/0/all/0/1\">Tessa Brabander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gils_R/0/1/0/all/0/1\">Renza A. H. van Gils</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Franssen_G/0/1/0/all/0/1\">Gaston J. H. Franssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feelders_R/0/1/0/all/0/1\">Richard A. Feelders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herder_W/0/1/0/all/0/1\">Wouter W. de Herder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buisman_F/0/1/0/all/0/1\">Florian E. Buisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willemssen_F/0/1/0/all/0/1\">Francois E. J. A. Willemssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koerkamp_B/0/1/0/all/0/1\">Bas Groot Koerkamp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angus_L/0/1/0/all/0/1\">Lindsay Angus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veldt_A/0/1/0/all/0/1\">Astrid A. M. van der Veldt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajicic_A/0/1/0/all/0/1\">Ana Rajicic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Odink_A/0/1/0/all/0/1\">Arlette E. Odink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deen_M/0/1/0/all/0/1\">Mitchell Deen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+T%2E_J/0/1/0/all/0/1\">Jose M. Castillo T.</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veenland_J/0/1/0/all/0/1\">Jifke Veenland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schoots_I/0/1/0/all/0/1\">Ivo Schoots</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Renckens_M/0/1/0/all/0/1\">Michel Renckens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doukas_M/0/1/0/all/0/1\">Michail Doukas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Man_R/0/1/0/all/0/1\">Rob A. de Man</a>, <a href=\"http://arxiv.org/find/eess/1/au:+IJzermans_J/0/1/0/all/0/1\">Jan N. M. IJzermans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miclea_R/0/1/0/all/0/1\">Razvan L. Miclea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vermeulen_P/0/1/0/all/0/1\">Peter B. Vermeulen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bron_E/0/1/0/all/0/1\">Esther E. Bron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomeer_M/0/1/0/all/0/1\">Maarten G. Thomeer</a>, et al. (3 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12043","description":"<p>Disentangled representation learning has been proposed as an approach to\nlearning general representations even in the absence of, or with limited,\nsupervision. A good general representation can be fine-tuned for new target\ntasks using modest amounts of data, or used directly in unseen domains\nachieving remarkable performance in the corresponding task. This alleviation of\nthe data and annotation requirements offers tantalising prospects for\napplications in computer vision and healthcare. In this tutorial paper, we\nmotivate the need for disentangled representations, revisit key concepts, and\ndescribe practical building blocks and criteria for learning such\nrepresentations. We survey applications in medical imaging emphasising choices\nmade in exemplar key works, and then discuss links to computer vision\napplications. We conclude by presenting limitations, challenges, and\nopportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pedro Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Framework for COVID-19 Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.09241","description":"<p>The objective of this study is to develop a robust deep learning-based\nframework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and\nNormal cases based on chest CT scans acquired in different imaging centers\nusing various protocols, and radiation doses. We showed that while our proposed\nmodel is trained on a relatively small dataset acquired from only one imaging\ncenter using a specific scanning protocol, the model performs well on\nheterogeneous test sets obtained by multiple scanners using different technical\nparameters. We also showed that the model can be updated via an unsupervised\napproach to cope with the data shift between the train and test sets and\nenhance the robustness of the model upon receiving a new external dataset from\na different center. We adopted an ensemble architecture to aggregate the\npredictions from multiple versions of the model. For initial training and\ndevelopment purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76\nNormal cases was used, which contained volumetric CT scans acquired from one\nimaging center using a constant standard radiation dose scanning protocol. To\nevaluate the model, we collected four different test sets retrospectively to\ninvestigate the effects of the shifts in the data characteristics on the\nmodel's performance. Among the test cases, there were CT scans with similar\ncharacteristics as the train set as well as noisy low-dose and ultra-low dose\nCT scans. In addition, some test CT scans were obtained from patients with a\nhistory of cardiovascular diseases or surgeries. The entire test dataset used\nin this study contained 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental\nresults indicate that our proposed framework performs well on all test sets\nachieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity\nof 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI:\n[76.50-99.19]).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khademi_S/0/1/0/all/0/1\">Sadaf Khademi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heidarian_S/0/1/0/all/0/1\">Shahin Heidarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afshar_P/0/1/0/all/0/1\">Parnian Afshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Enshaei_N/0/1/0/all/0/1\">Nastaran Enshaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naderkhani_F/0/1/0/all/0/1\">Farnoosh Naderkhani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rafiee_M/0/1/0/all/0/1\">Moezedin Javad Rafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oikonomou_A/0/1/0/all/0/1\">Anastasia Oikonomou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shafiee_A/0/1/0/all/0/1\">Akbar Shafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fard_F/0/1/0/all/0/1\">Faranak Babaki Fard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Visual-Auditory Fixation Prediction with Multigranularity Perception. (arXiv:2112.13697v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13697","description":"<p>Thanks to the rapid advances in deep learning techniques and the wide\navailability of large-scale training sets, the performance of video saliency\ndetection models has been improving steadily and significantly. However, deep\nlearning-based visualaudio fixation prediction is still in its infancy. At\npresent, only a few visual-audio sequences have been furnished, with real\nfixations being recorded in real visual-audio environments. Hence, it would be\nneither efficient nor necessary to recollect real fixations under the same\nvisual-audio circumstances. To address this problem, this paper promotes a\nnovel approach in a weakly supervised manner to alleviate the demand of\nlarge-scale training sets for visual-audio model training. By using only the\nvideo category tags, we propose the selective class activation mapping (SCAM)\nand its upgrade (SCAM+). In the spatial-temporal-audio circumstance, the former\nfollows a coarse-to-fine strategy to select the most discriminative regions,\nand these regions are usually capable of exhibiting high consistency with the\nreal human-eye fixations. The latter equips the SCAM with an additional\nmulti-granularity perception mechanism, making the whole process more\nconsistent with that of the real human visual system. Moreover, we distill\nknowledge from these regions to obtain complete new spatial-temporal-audio\n(STA) fixation prediction (FP) networks, enabling broad applications in cases\nwhere video tags are not available. Without resorting to any real human-eye\nfixation, the performances of these STA FP networks are comparable to those of\nfully supervised networks. The code and results are publicly available at\nhttps://github.com/guotaowang/STANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guotao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chenglizhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_A/0/1/0/all/0/1\">Aimin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hong Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Twenty-thousand Classes using Image-level Supervision. (arXiv:2201.02605v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02605","description":"<p>Current object detectors are limited in vocabulary size due to the small\nscale of detection datasets. Image classifiers, on the other hand, reason about\nmuch larger vocabularies, as their datasets are larger and easier to collect.\nWe propose Detic, which simply trains the classifiers of a detector on image\nclassification data and thus expands the vocabulary of detectors to tens of\nthousands of concepts. Unlike prior work, Detic does not need complex\nassignment schemes to assign image labels to boxes based on model predictions,\nmaking it much easier to implement and compatible with a range of detection\narchitectures and backbones. Our results show that Detic yields excellent\ndetectors even for classes without box annotations. It outperforms prior work\non both open-vocabulary and long-tail detection benchmarks. Detic provides a\ngain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the\nopen-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains\n41.7 mAP when evaluated on all classes, or only rare classes, hence closing the\ngap in performance for object categories with few samples. For the first time,\nwe train a detector with all the twenty-one-thousand classes of the ImageNet\ndataset and show that it generalizes to new datasets without finetuning. Code\nis available at \\url{https://github.com/facebookresearch/Detic}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02832","description":"<p>Convolutional Neural Networks have demonstrated human-level performance in\nthe classification of melanoma and other skin lesions, but evident performance\ndisparities between differing skin tones should be addressed before widespread\ndeployment. In this work, we propose an efficient yet effective algorithm for\nautomatically labelling the skin tone of lesion images, and use this to\nannotate the benchmark ISIC dataset. We subsequently use these automated labels\nas the target for two leading bias unlearning techniques towards mitigating\nskin tone bias. Our experimental results provide evidence that our skin tone\ndetection algorithm outperforms existing solutions and that unlearning skin\ntone may improve generalisation and can reduce the performance disparity\nbetween melanoma detection in lighter and darker skin tones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChimeraMix: Image Classification on Small Datasets via Masked Feature Mixing. (arXiv:2202.11616v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11616","description":"<p>Deep convolutional neural networks require large amounts of labeled data\nsamples. For many real-world applications, this is a major limitation which is\ncommonly treated by augmentation methods. In this work, we address the problem\nof learning deep neural networks on small datasets. Our proposed architecture\ncalled ChimeraMix learns a data augmentation by generating compositions of\ninstances. The generative model encodes images in pairs, combines the features\nguided by a mask, and creates new samples. For evaluation, all methods are\ntrained from scratch without any additional data. Several experiments on\nbenchmark datasets, e.g. ciFAIR-10, STL-10, and ciFAIR-100, demonstrate the\nsuperior performance of ChimeraMix compared to current state-of-the-art methods\nfor classification on small datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reinders_C/0/1/0/all/0/1\">Christoph Reinders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1\">Frederik Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose Estimation. (arXiv:2203.07628v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07628","description":"<p>This paper introduces a novel Pre-trained Spatial Temporal Many-to-One\n(P-STMO) model for 2D-to-3D human pose estimation task. To reduce the\ndifficulty of capturing spatial and temporal information, we divide this task\ninto two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I,\na self-supervised pre-training sub-task, termed masked pose modeling, is\nproposed. The human joints in the input sequence are randomly masked in both\nspatial and temporal domains. A general form of denoising auto-encoder is\nexploited to recover the original 2D poses and the encoder is capable of\ncapturing spatial and temporal dependencies in this way. In Stage II, the\npre-trained encoder is loaded to STMO model and fine-tuned. The encoder is\nfollowed by a many-to-one frame aggregator to predict the 3D pose in the\ncurrent frame. Especially, an MLP block is utilized as the spatial feature\nextractor in STMO, which yields better performance than other methods. In\naddition, a temporal downsampling strategy is proposed to diminish data\nredundancy. Extensive experiments on two benchmarks show that our method\noutperforms state-of-the-art methods with fewer parameters and less\ncomputational overhead. For example, our P-STMO model achieves 42.1mm MPJPE on\nHuman3.6M dataset when using 2D poses from CPN as inputs. Meanwhile, it brings\na 1.5-7.1 times speedup to state-of-the-art methods. Code is available at\nhttps://github.com/paTRICK-swk/P-STMO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shan_W/0/1/0/all/0/1\">Wenkang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images. (arXiv:2203.08138v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08138","description":"<p>Cryo-electron microscopy (cryo-EM) has become a tool of fundamental\nimportance in structural biology, helping us understand the basic building\nblocks of life. The algorithmic challenge of cryo-EM is to jointly estimate the\nunknown 3D poses and the 3D electron scattering potential of a biomolecule from\nmillions of extremely noisy 2D images. Existing reconstruction algorithms,\nhowever, cannot easily keep pace with the rapidly growing size of cryo-EM\ndatasets due to their high computational and memory cost. We introduce cryoAI,\nan ab initio reconstruction algorithm for homogeneous conformations that uses\ndirect gradient-based optimization of particle poses and the electron\nscattering potential from single-particle cryo-EM data. CryoAI combines a\nlearned encoder that predicts the poses of each particle image with a\nphysics-based decoder to aggregate each particle image into an implicit\nrepresentation of the scattering potential volume. This volume is stored in the\nFourier domain for computational efficiency and leverages a modern coordinate\nnetwork architecture for memory efficiency. Combined with a symmetrized loss\nfunction, this framework achieves results of a quality on par with\nstate-of-the-art cryo-EM solvers for both simulated and experimental data, one\norder of magnitude faster for large datasets and with significantly lower\nmemory requirements than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1\">Axel Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poitevin_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Poitevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1\">Julien Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nashed_Y/0/1/0/all/0/1\">Youssef Nashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peck_A/0/1/0/all/0/1\">Ariana Peck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_D/0/1/0/all/0/1\">Daniel Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunne_M/0/1/0/all/0/1\">Mike Dunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting a kNN-based Image Classification System with High-capacity Storage. (arXiv:2204.01186v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01186","description":"<p>In existing image classification systems that use deep neural networks, the\nknowledge needed for image classification is implicitly stored in model\nparameters. If users want to update this knowledge, then they need to fine-tune\nthe model parameters. Moreover, users cannot verify the validity of inference\nresults or evaluate the contribution of knowledge to the results. In this\npaper, we investigate a system that stores knowledge for image classification,\nsuch as image feature maps, labels, and original images, not in model\nparameters but in external high-capacity storage. Our system refers to the\nstorage like a database when classifying input images. To increase knowledge,\nour system updates the database instead of fine-tuning model parameters, which\navoids catastrophic forgetting in incremental learning scenarios. We revisit a\nkNN (k-Nearest Neighbor) classifier and employ it in our system. By analyzing\nthe neighborhood samples referred by the kNN algorithm, we can interpret how\nknowledge learned in the past is used for inference results. Our system\nachieves 79.8% top-1 accuracy on the ImageNet dataset without fine-tuning model\nparameters after pretraining, and 90.8% accuracy on the Split CIFAR-100 dataset\nin the task incremental learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1\">Kengo Nakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Youyang Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyashita_D/0/1/0/all/0/1\">Daisuke Miyashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maki_A/0/1/0/all/0/1\">Asuka Maki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Chieh Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deguchi_J/0/1/0/all/0/1\">Jun Deguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Event Camera-based Odometry for Planetary Robots. (arXiv:2204.05880v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05880","description":"<p>Due to their resilience to motion blur and high robustness in low-light and\nhigh dynamic range conditions, event cameras are poised to become enabling\nsensors for vision-based exploration on future Mars helicopter missions.\nHowever, existing event-based visual-inertial odometry (VIO) algorithms either\nsuffer from high tracking errors or are brittle, since they cannot cope with\nsignificant depth uncertainties caused by an unforeseen loss of tracking or\nother effects. In this work, we introduce EKLT-VIO, which addresses both\nlimitations by combining a state-of-the-art event-based frontend with a\nfilter-based backend. This makes it both accurate and robust to uncertainties,\noutperforming event- and frame-based VIO algorithms on challenging benchmarks\nby 32%. In addition, we demonstrate accurate performance in hover-like\nconditions (outperforming existing event-based methods) as well as high\nrobustness in newly collected Mars-like and high-dynamic-range sequences, where\nexisting frame-based methods fail. In doing so, we show that event-based VIO is\nthe way forward for vision-based exploration on Mars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahlknecht_F/0/1/0/all/0/1\">Florian Mahlknecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_J/0/1/0/all/0/1\">Jeremy Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockenbauer_F/0/1/0/all/0/1\">Friedrich M. Rockenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrell_B/0/1/0/all/0/1\">Benjamin Morrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaune_J/0/1/0/all/0/1\">Jeff Delaune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Grounded Indoor 3D Semantic Segmentation in the Wild. (arXiv:2204.07761v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07761","description":"<p>Recent advances in 3D semantic segmentation with deep neural networks have\nshown remarkable success, with rapid performance increase on available\ndatasets. However, current 3D semantic segmentation benchmarks contain only a\nsmall number of categories -- less than 30 for ScanNet and SemanticKITTI, for\ninstance, which are not enough to reflect the diversity of real environments\n(e.g., semantic image understanding covers hundreds to thousands of classes).\nThus, we propose to study a larger vocabulary for 3D semantic segmentation with\na new extended benchmark on ScanNet data with 200 class categories, an order of\nmagnitude more than previously studied. This large number of class categories\nalso induces a large natural class imbalance, both of which are challenging for\nexisting 3D semantic segmentation methods. To learn more robust 3D features in\nthis context, we propose a language-driven pre-training method to encourage\nlearned 3D features that might have limited training examples to lie close to\ntheir pre-trained text embeddings. Extensive experiments show that our approach\nconsistently outperforms state-of-the-art 3D pre-training for 3D semantic\nsegmentation on our proposed benchmark (+9% relative mIoU), including\nlimited-data scenarios with +25% relative mIoU using only 5% annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozenberszki_D/0/1/0/all/0/1\">David Rozenberszki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing. (arXiv:2204.11573v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11573","description":"<p>This paper focuses on the weakly-supervised audio-visual video parsing task,\nwhich aims to recognize all events belonging to each modality and localize\ntheir temporal boundaries. This task is challenging because only overall labels\nindicating the video events are provided for training. However, an event might\nbe labeled but not appear in one of the modalities, which results in a\nmodality-specific noisy label problem. In this work, we propose a training\nstrategy to identify and remove modality-specific noisy labels dynamically. It\nis motivated by two key observations: 1) networks tend to learn clean samples\nfirst; and 2) a labeled event would appear in at least one modality.\nSpecifically, we sort the losses of all instances within a mini-batch\nindividually in each modality, and then select noisy samples according to the\nrelationships between intra-modal and inter-modal losses. Besides, we also\npropose a simple but valid noise ratio estimation method by calculating the\nproportion of instances whose confidence is below a preset threshold. Our\nmethod makes large improvements over the previous state of the arts (\\eg, from\n60.0\\% to 63.8\\% in segment-level visual metric), which demonstrates the\neffectiveness of our approach. Code and trained models are publicly available\nat \\url{https://github.com/MCG-NJU/JoMoLD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Haoyue Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning on rail profiles matching. (arXiv:2205.08687v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08687","description":"<p>Matching the rail cross-section profiles measured on site with the designed\nprofile is a must to evaluate the wear of the rail, which is very important for\ntrack maintenance and rail safety. So far, the measured rail profiles to be\nmatched usually have four features, that is, large amount of data, diverse\nsection shapes, hardware made errors, and human experience needs to be\nintroduced to solve the complex situation on site during matching process.\nHowever, traditional matching methods based on feature points or feature lines\ncould no longer meet the requirements. To this end, we first establish the rail\nprofiles matching dataset composed of 46386 pairs of professional manual\nmatched data, then propose a general high-precision method for rail profiles\nmatching using pre-trained convolutional neural network (CNN). This new method\nbased on deep learning is promising to be the dominant approach for this issue.\nSource code is at\nhttps://github.com/Kunqi1994/Deep-learning-on-rail-profile-matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kunqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_D/0/1/0/all/0/1\">Daolin Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1\">Jing Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_P/0/1/0/all/0/1\">Peiyuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuguo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09479","description":"<p>Generative Adversarial Network (GAN) is one of the state-of-the-art\ngenerative models for realistic image synthesis. While training and evaluating\nGAN becomes increasingly important, the current GAN research ecosystem does not\nprovide reliable benchmarks for which the evaluation is conducted consistently\nand fairly. Furthermore, because there are few validated GAN implementations,\nresearchers devote considerable time to reproducing baselines. We study the\ntaxonomy of GAN approaches and present a new open-source library named\nStudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4\nadversarial losses, 13 regularization modules, 3 differentiable augmentations,\n7 evaluation metrics, and 5 evaluation backbones. With our training and\nevaluation protocol, we present a large-scale benchmark using various datasets\n(CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3\ndifferent evaluation backbones (InceptionV3, SwAV, and Swin Transformer).\nUnlike other benchmarks used in the GAN community, we train representative\nGANs, including BigGAN, StyleGAN2, and StyleGAN3, in a unified training\npipeline and quantify generation performance with 7 evaluation metrics. The\nbenchmark evaluates other cutting-edge generative models(e.g., StyleGAN-XL,\nADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations,\ntraining, and evaluation scripts with the pre-trained weights. StudioGAN is\navailable at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minguk Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joonghyuk Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Graph Matching Algorithms in Computer Vision. (arXiv:2207.00291v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00291","description":"<p>The graph matching optimization problem is an essential component for many\ntasks in computer vision, such as bringing two deformable objects in\ncorrespondence. Naturally, a wide range of applicable algorithms have been\nproposed in the last decades. Since a common standard benchmark has not been\ndeveloped, their performance claims are often hard to verify as evaluation on\ndiffering problem instances and criteria make the results incomparable. To\naddress these shortcomings, we present a comparative study of graph matching\nalgorithms. We create a uniform benchmark where we collect and categorize a\nlarge set of existing and publicly available computer vision graph matching\nproblems in a common format. At the same time we collect and categorize the\nmost popular open-source implementations of graph matching algorithms. Their\nperformance is evaluated in a way that is in line with the best practices for\ncomparing optimization algorithms. The study is designed to be reproducible and\nextensible to serve as a valuable resource in the future.\n</p>\n<p>Our study provides three notable insights:\n</p>\n<p>1.) popular problem instances are exactly solvable in substantially less than\n1 second and, therefore, are insufficient for future empirical evaluations;\n</p>\n<p>2.) the most popular baseline methods are highly inferior to the best\navailable methods;\n</p>\n<p>3.) despite the NP-hardness of the problem, instances coming from vision\napplications are often solvable in a few seconds even for graphs with more than\n500 vertices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haller_S/0/1/0/all/0/1\">Stefan Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feineis_L/0/1/0/all/0/1\">Lorenz Feineis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutschenreiter_L/0/1/0/all/0/1\">Lisa Hutschenreiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainmuller_D/0/1/0/all/0/1\">Dagmar Kainm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savchynskyy_B/0/1/0/all/0/1\">Bogdan Savchynskyy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Detection with Cardboard Human Modeling. (arXiv:2207.02013v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02013","description":"<p>Multiview detection uses multiple calibrated cameras with overlapping fields\nof views to locate occluded pedestrians. In this field, existing methods\ntypically adopt a \"human modeling - aggregation\" strategy. To find robust\npedestrian representations, some intuitively use locations of detected 2D\nbounding boxes, while others use entire frame features projected to the ground\nplane. However, the former does not consider human appearance and leads to many\nambiguities, and the latter suffers from projection errors due to the lack of\naccurate height of the human torso and head. In this paper, we propose a new\npedestrian representation scheme based on human point clouds modeling.\nSpecifically, using ray tracing for holistic human depth estimation, we model\npedestrians as upright, thin cardboard point clouds on the ground. Then, we\naggregate the point clouds of the pedestrian cardboard across multiple views\nfor a final decision. Compared with existing representations, the proposed\nmethod explicitly leverages human appearance and reduces projection errors\nsignificantly by relatively accurate height estimation. On two standard\nevaluation benchmarks, the proposed method achieves very competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiahao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zicheng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yunzhong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02335","description":"<p>Early detection of retinal diseases is one of the most important means of\npreventing partial or permanent blindness in patients. In this research, a\nnovel multi-label classification system is proposed for the detection of\nmultiple retinal diseases, using fundus images collected from a variety of\nsources. First, a new multi-label retinal disease dataset, the MuReD dataset,\nis constructed, using a number of publicly available datasets for fundus\ndisease classification. Next, a sequence of post-processing steps is applied to\nensure the quality of the image data and the range of diseases, present in the\ndataset. For the first time in fundus multi-label disease classification, a\ntransformer-based model optimized through extensive experimentation is used for\nimage analysis and decision making. Numerous experiments are performed to\noptimize the configuration of the proposed system. It is shown that the\napproach performs better than state-of-the-art works on the same task by 7.9%\nand 8.1% in terms of AUC score for disease detection and disease\nclassification, respectively. The obtained results further support the\npotential applications of transformer-based architectures in the medical\nimaging field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1\">M. A. Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlMarzouqi_H/0/1/0/all/0/1\">H. AlMarzouqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liatsis_P/0/1/0/all/0/1\">P. Liatsis</a> (Department of Electrical Engineering and Computer Science, Khalifa University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-based Monocular 3D Reconstruction of Birds: A Contemporary Survey. (arXiv:2207.04512v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04512","description":"<p>In nature, the collective behavior of animals, such as flying birds is\ndominated by the interactions between individuals of the same species. However,\nthe study of such behavior among the bird species is a complex process that\nhumans cannot perform using conventional visual observational techniques such\nas focal sampling in nature. For social animals such as birds, the mechanism of\ngroup formation can help ecologists understand the relationship between social\ncues and their visual characteristics over time (e.g., pose and shape). But,\nrecovering the varying pose and shapes of flying birds is a highly challenging\nproblem. A widely-adopted solution to tackle this bottleneck is to extract the\npose and shape information from 2D image to 3D correspondence. Recent advances\nin 3D vision have led to a number of impressive works on the 3D shape and pose\nestimation, each with different pros and cons. To the best of our knowledge,\nthis work is the first attempt to provide an overview of recent advances in 3D\nbird reconstruction based on monocular vision, give both computer vision and\nbiology researchers an overview of existing approaches, and compare their\ncharacteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahromi_M/0/1/0/all/0/1\">Mohammad N.S. Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaghani_J/0/1/0/all/0/1\">Javad Khaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodsman_D/0/1/0/all/0/1\">Devin Goodsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1\">Nadir Erbilgin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Background Distraction in Video Object Segmentation. (arXiv:2207.06953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06953","description":"<p>Semi-supervised video object segmentation (VOS) aims to densely track certain\ndesignated objects in videos. One of the main challenges in this task is the\nexistence of background distractors that appear similar to the target objects.\nWe propose three novel strategies to suppress such distractors: 1) a\nspatio-temporally diversified template construction scheme to obtain\ngeneralized properties of the target objects; 2) a learnable distance-scoring\nfunction to exclude spatially-distant distractors by exploiting the temporal\nconsistency between two consecutive frames; 3) swap-and-attach augmentation to\nforce each object to have unique features by providing training samples\ncontaining entangled objects. On all public benchmark datasets, our model\nachieves a comparable performance to contemporary state-of-the-art approaches,\neven with real-time performance. Qualitative results also demonstrate the\nsuperiority of our approach over existing methods. We believe our approach will\nbe widely used for future VOS research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Suhwan Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heansung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Sungjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minjung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype-Guided Continual Adaptation for Class-Incremental Unsupervised Domain Adaptation. (arXiv:2207.10856v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10856","description":"<p>This paper studies a new, practical but challenging problem, called\nClass-Incremental Unsupervised Domain Adaptation (CI-UDA), where the labeled\nsource domain contains all classes, but the classes in the unlabeled target\ndomain increase sequentially. This problem is challenging due to two\ndifficulties. First, source and target label sets are inconsistent at each time\nstep, which makes it difficult to conduct accurate domain alignment. Second,\nprevious target classes are unavailable in the current step, resulting in the\nforgetting of previous knowledge. To address this problem, we propose a novel\nPrototype-guided Continual Adaptation (ProCA) method, consisting of two\nsolution strategies. 1) Label prototype identification: we identify target\nlabel prototypes by detecting shared classes with cumulative prediction\nprobabilities of target samples. 2) Prototype-based alignment and replay: based\non the identified label prototypes, we align both domains and enforce the model\nto retain previous knowledge. With these two strategies, ProCA is able to adapt\nthe source model to a class-incremental unlabeled target domain effectively.\nExtensive experiments demonstrate the effectiveness and superiority of ProCA in\nresolving CI-UDA. The source code is available at\nhttps://github.com/Hongbin98/ProCA.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongbin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanxia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Steganography Network. (arXiv:2207.13867v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.13867","description":"<p>Steganography usually modifies cover media to embed secret data. A new\nsteganographic approach called generative steganography (GS) has emerged\nrecently, in which stego images (images containing secret data) are generated\nfrom secret data directly without cover media. However, existing GS schemes are\noften criticized for their poor performances. In this paper, we propose an\nadvanced generative steganography network (GSN) that can generate realistic\nstego images without using cover images, in which mutual information is firstly\nintroduced in stego image generation. Our model contains four sub-networks,\ni.e., an image generator ($G$), a discriminator ($D$), a steganalyzer ($S$),\nand a data extractor ($E$). $D$ and $S$ act as two adversarial discriminators\nto ensure the visual and statistical imperceptibility of generated stego\nimages. $E$ is to extract the hidden secret from generated stego images. The\ngenerator $G$ is flexibly constructed to synthesize either cover or stego\nimages with different inputs. It facilitates covert communication by hiding the\nfunction of generating stego images in a normal image generator. A module named\nsecret block is designed delicately to conceal secret data in the feature maps\nduring image generation, with which high hiding capacity and image fidelity are\nachieved. In addition, a novel hierarchical gradient decay skill is developed\nto resist steganalysis detection. Experiments demonstrate the superiority of\nour work over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Ping Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Ge Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qing Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.14024","description":"<p>Large-scale deployment of autonomous vehicles has been continually delayed\ndue to safety concerns. On the one hand, comprehensive scene understanding is\nindispensable, a lack of which would result in vulnerability to rare but\ncomplex traffic situations, such as the sudden emergence of unknown objects.\nHowever, reasoning from a global context requires access to sensors of multiple\ntypes and adequate fusion of multi-modal sensor signals, which is difficult to\nachieve. On the other hand, the lack of interpretability in learning models\nalso hampers the safety with unverifiable failure causes. In this paper, we\npropose a safety-enhanced autonomous driving framework, named Interpretable\nSensor Fusion Transformer(InterFuser), to fully process and fuse information\nfrom multi-modal multi-view sensors for achieving comprehensive scene\nunderstanding and adversarial event detection. Besides, intermediate\ninterpretable features are generated from our framework, which provide more\nsemantics and are exploited to better constrain actions to be within the safe\nsets. We conducted extensive experiments on CARLA benchmarks, where our model\noutperforms prior methods, ranking the first on the public CARLA Leaderboard.\nOur code will be made available at https://github.com/opendilab/InterFuser\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hao Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Letian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">RuoBing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images. (arXiv:2207.14067v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2207.14067","description":"<p>We present Neural Strands, a novel learning framework for modeling accurate\nhair geometry and appearance from multi-view image inputs. The learned hair\nmodel can be rendered in real-time from any viewpoint with high-fidelity\nview-dependent effects. Our model achieves intuitive shape and style control\nunlike volumetric counterparts. To enable these properties, we propose a novel\nhair representation based on a neural scalp texture that encodes the geometry\nand appearance of individual strands at each texel location. Furthermore, we\nintroduce a novel neural rendering framework based on rasterization of the\nlearned hair strands. Our neural rendering is strand-accurate and anti-aliased,\nmaking the rendering view-consistent and photorealistic. Combining appearance\nwith a multi-view geometric prior, we enable, for the first time, the joint\nlearning of appearance and explicit hair geometry from a multi-view setup. We\ndemonstrate the efficacy of our approach in terms of fidelity and efficiency\nfor various hairstyles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1\">Giljoo Nam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}