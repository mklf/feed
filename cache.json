{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas. (arXiv:2203.13838v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13838","description":"<p>Vision and language navigation (VLN) is a challenging visually-grounded\nlanguage understanding task. Given a natural language navigation instruction, a\nvisual agent interacts with a graph-based environment equipped with panorama\nimages and tries to follow the described route. Most prior work has been\nconducted in indoor scenarios where best results were obtained for navigation\non routes that are similar to the training routes, with sharp drops in\nperformance when testing on unseen environments. We focus on VLN in outdoor\nscenarios and find that in contrast to indoor VLN, most of the gain in outdoor\nVLN on unseen data is due to features like junction type embedding or heading\ndelta that are specific to the respective environment graph, while image\ninformation plays a very minor role in generalizing VLN to unseen outdoor\nareas. These findings show a bias to specifics of graph representations of\nurban environments, demanding that VLN tasks grow in scale and diversity of\ngeographical environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1\">Raphael Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Selection Curriculum for Neural Machine Translation. (arXiv:2203.13867v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13867","description":"<p>Neural Machine Translation (NMT) models are typically trained on\nheterogeneous data that are concatenated and randomly shuffled. However, not\nall of the training data are equally useful to the model. Curriculum training\naims to present the data to the NMT models in a meaningful order. In this work,\nwe introduce a two-stage curriculum training framework for NMT where we\nfine-tune a base NMT model on subsets of data, selected by both deterministic\nscoring using pre-trained methods and online scoring that considers prediction\nscores of the emerging NMT model. Through comprehensive experiments on six\nlanguage pairs comprising low- and high-resource languages from WMT'21, we have\nshown that our curriculum strategies consistently demonstrate better quality\n(up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer\nupdates).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1\">Tasnim Mohiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUTOLEX: An Automatic Framework for Linguistic Exploration. (arXiv:2203.13901v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13901","description":"<p>Each language has its own complex systems of word, phrase, and sentence\nconstruction, the guiding principles of which are often summarized in grammar\ndescriptions for the consumption of linguists or language learners. However,\nmanual creation of such descriptions is a fraught process, as creating\ndescriptions which describe the language in \"its own terms\" without bias or\nerror requires both a deep understanding of the language at hand and\nlinguistics as a whole. We propose an automatic framework AutoLEX that aims to\nease linguists' discovery and extraction of concise descriptions of linguistic\nphenomena. Specifically, we apply this framework to extract descriptions for\nthree phenomena: morphological agreement, case marking, and word order, across\nseveral languages. We evaluate the descriptions with the help of language\nexperts and propose a method for automated evaluation when human evaluation is\ninfeasible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Z/0/1/0/all/0/1\">Zaid Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Canary Extraction in Natural Language Understanding Models. (arXiv:2203.13920v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13920","description":"<p>Natural Language Understanding (NLU) models can be trained on sensitive\ninformation such as phone numbers, zip-codes etc. Recent literature has focused\non Model Inversion Attacks (ModIvA) that can extract training data from model\nparameters. In this work, we present a version of such an attack by extracting\ncanaries inserted in NLU training data. In the attack, an adversary with\nopen-box access to the model reconstructs the canaries contained in the model's\ntraining set. We evaluate our approach by performing text completion on\ncanaries and demonstrate that by using the prefix (non-sensitive) tokens of the\ncanary, we can generate the full canary. As an example, our attack is able to\nreconstruct a four digit code in the training dataset of the NLU model with a\nprobability of 0.5 in its best configuration. As countermeasures, we identify\nseveral defense mechanisms that, when combined, effectively eliminate the risk\nof ModIvA in our experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_R/0/1/0/all/0/1\">Rahil Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues. (arXiv:2203.13926v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13926","description":"<p>This paper addresses the problem of dialogue reasoning with contextualized\ncommonsense inference. We curate CICERO, a dataset of dyadic conversations with\nfive types of utterance-level reasoning-based inferences: cause, subsequent\nevent, prerequisite, motivation, and emotional reaction. The dataset contains\n53,105 of such inferences from 5,672 dialogues. We use this dataset to solve\nrelevant generative and discriminative tasks: generation of cause and\nsubsequent event; generation of prerequisite, motivation, and listener's\nemotional reaction; and selection of plausible alternatives. Our results\nascertain the value of such dialogue-centric commonsense knowledge datasets. It\nis our hope that CICERO will open new research avenues into commonsense-based\ndialogue reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation. (arXiv:2203.13927v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13927","description":"<p>Accurate automatic evaluation metrics for open-domain dialogs are in high\ndemand. Existing model-based metrics for system response evaluation are trained\non human annotated data, which is cumbersome to collect. In this work, we\npropose to use information that can be automatically extracted from the next\nuser utterance, such as its sentiment or whether the user explicitly ends the\nconversation, as a proxy to measure the quality of the previous system\nresponse. This allows us to train on a massive set of dialogs with weak\nsupervision, without requiring manual system turn quality annotations.\nExperiments show that our model is comparable to models trained on human\nannotated data. Furthermore, our model generalizes across both spoken and\nwritten open-domain dialog corpora collected from real and paid users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations. (arXiv:2203.13928v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13928","description":"<p>Multiple metrics have been introduced to measure fairness in various natural\nlanguage processing tasks. These metrics can be roughly categorized into two\ncategories: 1) \\emph{extrinsic metrics} for evaluating fairness in downstream\napplications and 2) \\emph{intrinsic metrics} for estimating fairness in\nupstream contextualized language representation models. In this paper, we\nconduct an extensive correlation study between intrinsic and extrinsic metrics\nacross bias notions using 19 contextualized language models. We find that\nintrinsic and extrinsic metrics do not necessarily correlate in their original\nsetting, even when correcting for metric misalignments, noise in evaluation\ndatasets, and confounding factors such as experiment configuration for\nextrinsic metrics. %al\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Trista Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1\">Yada Pruksachatkun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic Dataset for Narrative Comprehension. (arXiv:2203.13947v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13947","description":"<p>Question answering (QA) is a fundamental means to facilitate assessment and\ntraining of narrative comprehension skills for both machines and young\nchildren, yet there is scarcity of high-quality QA datasets carefully designed\nto serve this purpose. In particular, existing datasets rarely distinguish\nfine-grained reading skills, such as the understanding of varying narrative\nelements. Drawing on the reading education research, we introduce FairytaleQA,\na dataset focusing on narrative comprehension of kindergarten to eighth-grade\nstudents. Generated by educational experts based on an evidence-based\ntheoretical framework, FairytaleQA consists of 10,580 explicit and implicit\nquestions derived from 278 children-friendly stories, covering seven types of\nnarrative elements or relations. Our dataset is valuable in two folds: First,\nwe ran existing QA models on our dataset and confirmed that this annotation\nhelps assess models' fine-grained learning skills. Second, the dataset supports\nquestion generation (QG) task in the education domain. Through benchmarking\nwith QG models, we show that the QG model trained on FairytaleQA is capable of\nasking high-quality and more diverse questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradford_N/0/1/0/all/0/1\">Nora Bradford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Branda Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Tran Bao Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1\">Yisi Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warschauer_M/0/1/0/all/0/1\">Mark Warschauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Densely Connected Criss-Cross Attention Network for Document-level Relation Extraction. (arXiv:2203.13953v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13953","description":"<p>Document-level relation extraction (RE) aims to identify relations between\ntwo entities in a given document. Compared with its sentence-level counterpart,\ndocument-level RE requires complex reasoning. Previous research normally\ncompleted reasoning through information propagation on the mention-level or\nentity-level document-graph, but rarely considered reasoning at the\nentity-pair-level.In this paper, we propose a novel model, called Densely\nConnected Criss-Cross Attention Network (Dense-CCNet), for document-level RE,\nwhich can complete logical reasoning at the entity-pair-level. Specifically,\nthe Dense-CCNet performs entity-pair-level logical reasoning through the\nCriss-Cross Attention (CCA), which can collect contextual information in\nhorizontal and vertical directions on the entity-pair matrix to enhance the\ncorresponding entity-pair representation. In addition, we densely connect\nmultiple layers of the CCA to simultaneously capture the features of single-hop\nand multi-hop logical reasoning.We evaluate our Dense-CCNet model on three\npublic document-level RE datasets, DocRED, CDR, and GDA. Experimental results\ndemonstrate that our model achieves state-of-the-art performance on these three\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yidong Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Linguistic Steganography Based on BERT and Consistency Coding. (arXiv:2203.13972v1 [cs.CR])","link":"http://arxiv.org/abs/2203.13972","description":"<p>Linguistic steganography (LS) conceals the presence of communication by\nembedding secret information into a text. How to generate a high-quality text\ncarrying secret information is a key problem. With the widespread application\nof deep learning in natural language processing, recent algorithms use a\nlanguage model (LM) to generate the steganographic text, which provides a\nhigher payload compared with many previous arts. However, the security still\nneeds to be enhanced. To tackle with this problem, we propose a novel\nautoregressive LS algorithm based on BERT and consistency coding, which\nachieves a better trade-off between embedding payload and system security. In\nthe proposed work, based on the introduction of the masked LM, given a text, we\nuse consistency coding to make up for the shortcomings of block coding used in\nthe previous work so that we can encode arbitrary-size candidate token set and\ntake advantages of the probability distribution for information hiding. The\nmasked positions to be embedded are filled with tokens determined by an\nautoregressive manner to enhance the connection between contexts and therefore\nmaintain the quality of the text. Experimental results have shown that,\ncompared with related works, the proposed work improves the fluency of the\nsteganographic text while guaranteeing security, and also increases the\nembedding payload to a certain extent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoyan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Transformer/RNN Architecture for Gesture Typing in Indic Languages. (arXiv:2203.14049v1 [cs.LG])","link":"http://arxiv.org/abs/2203.14049","description":"<p>Gesture typing is a method of typing words on a touch-based keyboard by\ncreating a continuous trace passing through the relevant keys. This work is\naimed at developing a keyboard that supports gesture typing in Indic languages.\nWe begin by noting that when dealing with Indic languages, one needs to cater\nto two different sets of users: (i) users who prefer to type in the native\nIndic script (Devanagari, Bengali, etc.) and (ii) users who prefer to type in\nthe English script but want the output transliterated into the native script.\nIn both cases, we need a model that takes a trace as input and maps it to the\nintended word. To enable the development of these models, we create and release\ntwo datasets. First, we create a dataset containing keyboard traces for 193,658\nwords from 7 Indic languages. Second, we curate 104,412 English-Indic\ntransliteration pairs from Wikidata across these languages. Using these\ndatasets we build a model that performs path decoding, transliteration, and\ntransliteration correction. Unlike prior approaches, our proposed model does\nnot make co-character independence assumptions during decoding. The overall\naccuracy of our model across the 7 languages varies from 70-95%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biju_E/0/1/0/all/0/1\">Emil Biju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Anirudh Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MQDD -- Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain. (arXiv:2203.14093v1 [cs.CL])","link":"http://arxiv.org/abs/2203.14093","description":"<p>This work proposes a new pipeline for leveraging data collected on the Stack\nOverflow website for pre-training a multimodal model for searching duplicates\non question answering websites. Our multimodal model is trained on question\ndescriptions and source codes in multiple programming languages. We design two\nnew learning objectives to improve duplicate detection capabilities. The result\nof this work is a mature, fine-tuned Multimodal Question Duplicity Detection\n(MQDD) model, ready to be integrated into a Stack Overflow search system, where\nit can help users find answers for already answered questions. Alongside the\nMQDD model, we release two datasets related to the software engineering domain.\nThe first Stack Overflow Dataset (SOD) represents a massive corpus of paired\nquestions and answers. The second Stack Overflow Duplicity Dataset (SODD)\ncontains data for training duplicate detection models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasek_J/0/1/0/all/0/1\">Jan Pa&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Roadmap for Big Model. (arXiv:2203.14101v1 [cs.LG])","link":"http://arxiv.org/abs/2203.14101","description":"<p>With the rapid development of deep learning, training Big Models (BMs) for\nmultiple downstream tasks becomes a popular paradigm. Researchers have achieved\nvarious outcomes in the construction of BMs and the BM application in many\nfields. At present, there is a lack of research work that sorts out the overall\nprogress of BMs and guides the follow-up research. In this paper, we cover not\nonly the BM technologies themselves but also the prerequisites for BM training\nand applications with BMs, dividing the BM review into four parts: Resource,\nModels, Key Technologies and Application. We introduce 16 specific BM-related\ntopics in those four parts, they are Data, Knowledge, Computing System,\nParallel Training System, Language Model, Vision Model, Multi-modal Model,\nTheory&amp;Interpretability, Commonsense Reasoning, Reliability&amp;Security,\nGovernance, Evaluation, Machine Translation, Text Generation, Dialogue and\nProtein Research. In each topic, we summarize clearly the current studies and\npropose some future research directions. At the end of this paper, we conclude\nthe further development of BMs in a more general view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yangxiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiaao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chence Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zuobai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaoyu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Weicheng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zheni Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenzhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, et al. (34 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lite Unified Modeling for Discriminative Reading Comprehension. (arXiv:2203.14103v1 [cs.CL])","link":"http://arxiv.org/abs/2203.14103","description":"<p>As a broad and major category in machine reading comprehension (MRC), the\ngeneralized goal of discriminative MRC is answer prediction from the given\nmaterials. However, the focuses of various discriminative MRC tasks may be\ndiverse enough: multi-choice MRC requires model to highlight and integrate all\npotential critical evidence globally; while extractive MRC focuses on higher\nlocal boundary preciseness for answer extraction. Among previous works, there\nlacks a unified design with pertinence for the overall discriminative MRC\ntasks. To fill in above gap, we propose a lightweight POS-Enhanced Iterative\nCo-Attention Network (POI-Net) as the first attempt of unified modeling with\npertinence, to handle diverse discriminative MRC tasks synchronously. Nearly\nwithout introducing more parameters, our lite unified design brings model\nsignificant improvement with both encoder and decoder components. The\nevaluation results on four discriminative MRC benchmarks consistently indicate\nthe general effectiveness and applicability of our model, and the code is\navailable at https://github.com/Yilin1111/poi-net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Libin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yinggong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparative analysis of Graph Neural Networks and commonly used machine learning algorithms on fake news detection. (arXiv:2203.14132v1 [cs.LG])","link":"http://arxiv.org/abs/2203.14132","description":"<p>Fake news on social media is increasingly regarded as one of the most\nconcerning issues. Low cost, simple accessibility via social platforms, and a\nplethora of low-budget online news sources are some of the factors that\ncontribute to the spread of false news. Most of the existing fake news\ndetection algorithms are solely focused on the news content only but engaged\nusers prior posts or social activities provide a wealth of information about\ntheir views on news and have significant ability to improve fake news\nidentification. Graph Neural Networks are a form of deep learning approach that\nconducts prediction on graph-described data. Social media platforms are\nfollowed graph structure in their representation, Graph Neural Network are\nspecial types of neural networks that could be usually applied to graphs,\nmaking it much easier to execute edge, node, and graph-level prediction.\nTherefore, in this paper, we present a comparative analysis among some commonly\nused machine learning algorithms and Graph Neural Networks for detecting the\nspread of false news on social media platforms. In this study, we take the UPFD\ndataset and implement several existing machine learning algorithms on text data\nonly. Besides this, we create different GNN layers for fusing graph-structured\nnews propagation data and the text data as the node feature in our GNN models.\nGNNs provide the best solutions to the dilemma of identifying false news in our\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_F/0/1/0/all/0/1\">Fahim Belal Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayhan_M/0/1/0/all/0/1\">Mahi Md. Sadek Rayhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuvo_M/0/1/0/all/0/1\">Mahdi Hasan Shuvo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadia_I/0/1/0/all/0/1\">Islam Sadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morol_M/0/1/0/all/0/1\">Md.Kishor Morol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages. (arXiv:2203.14139v1 [cs.CL])","link":"http://arxiv.org/abs/2203.14139","description":"<p>Human languages are full of metaphorical expressions. Metaphors help people\nunderstand the world by connecting new concepts and domains to more familiar\nones. Large pre-trained language models (PLMs) are therefore assumed to encode\nmetaphorical knowledge useful for NLP systems. In this paper, we investigate\nthis hypothesis for PLMs, by probing metaphoricity information in their\nencodings, and by measuring the cross-lingual and cross-dataset generalization\nof this information. We present studies in multiple metaphor detection datasets\nand in four languages (i.e., English, Spanish, Russian, and Farsi). Our\nextensive experiments suggest that contextual representations in PLMs do encode\nmetaphorical knowledge, and mostly in their middle layers. The knowledge is\ntransferable between languages and datasets, especially when the annotation is\nconsistent across training and testing sets. Our findings give helpful insights\nfor both cognitive and NLP scientists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aghazadeh_E/0/1/0/all/0/1\">Ehsan Aghazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demonstrating CAT: Synthesizing Data-Aware Conversational Agents for Transactional Databases. (arXiv:2203.14144v1 [cs.DB])","link":"http://arxiv.org/abs/2203.14144","description":"<p>Databases for OLTP are often the backbone for applications such as hotel room\nor cinema ticket booking applications. However, developing a conversational\nagent (i.e., a chatbot-like interface) to allow end-users to interact with an\napplication using natural language requires both immense amounts of training\ndata and NLP expertise. This motivates CAT, which can be used to easily create\nconversational agents for transactional databases. The main idea is that, for a\ngiven OLTP database, CAT uses weak supervision to synthesize the required\ntraining data to train a state-of-the-art conversational agent, allowing users\nto interact with the OLTP database. Furthermore, CAT provides an out-of-the-box\nintegration of the resulting agent with the database. As a major difference to\nexisting conversational agents, agents synthesized by CAT are data-aware. This\nmeans that the agent decides which information should be requested from the\nuser based on the current data distributions in the database, which typically\nresults in markedly more efficient dialogues compared with non-data-aware\nagents. We publish the code for CAT as open source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gassen_M/0/1/0/all/0/1\">Marius Gassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hattasch_B/0/1/0/all/0/1\">Benjamin H&#xe4;ttasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilprecht_B/0/1/0/all/0/1\">Benjamin Hilprecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geisler_N/0/1/0/all/0/1\">Nadja Geisler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binnig_C/0/1/0/all/0/1\">Carsten Binnig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Word Information with Multi-Level Word Adapter for Chinese Spoken Language Understanding. (arXiv:2010.03903v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.03903","description":"<p>In this paper, we improve Chinese spoken language understanding (SLU) by\ninjecting word information. Previous studies on Chinese SLU do not consider the\nword information, failing to detect word boundaries that are beneficial for\nintent detection and slot filling. To address this issue, we propose a\nmulti-level word adapter to inject word information for Chinese SLU, which\nconsists of (1) sentence-level word adapter, which directly fuses the sentence\nrepresentations of the word information and character information to perform\nintent detection and (2) character-level word adapter, which is applied at each\ncharacter for selectively controlling weights on word information as well as\ncharacter information. Experimental results on two Chinese SLU datasets show\nthat our model can capture useful word information and achieve state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teng_D/0/1/0/all/0/1\">Dechuan Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaM-Gen:Causally-aware Metric-guided Text Generation. (arXiv:2010.12795v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12795","description":"<p>Content is created for a well-defined purpose, often described by a metric or\nsignal represented in the form of structured information. The relationship\nbetween the goal (metrics) of target content and the content itself is\nnon-trivial. While large-scale language models show promising text generation\ncapabilities, guiding the generated text with external metrics is challenging.\nThese metrics and content tend to have inherent relationships and not all of\nthem may be of consequence. We introduce CaM-Gen: Causally aware Generative\nNetworks guided by user-defined target metrics incorporating the causal\nrelationships between the metric and content features. We leverage causal\ninference techniques to identify causally significant aspects of a text that\nlead to the target metric and then explicitly guide generative models towards\nthese by a feedback mechanism. We propose this mechanism for variational\nautoencoder and Transformer-based generative models. The proposed models beat\nbaselines in terms of the target metric control while maintaining fluency and\nlanguage quality of the generated text. To the best of our knowledge, this is\none of the early attempts at controlled generation incorporating a metric guide\nusing causal inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Navita Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paneri_R/0/1/0/all/0/1\">Roodram Paneri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ayush Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalani_U/0/1/0/all/0/1\">Udit Kalani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sancheti_A/0/1/0/all/0/1\">Abhilasha Sancheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1\">Niyati Chhaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reference Knowledgeable Network for Machine Reading Comprehension. (arXiv:2012.03709v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.03709","description":"<p>Multi-choice Machine Reading Comprehension (MRC) as a challenge requires\nmodels to select the most appropriate answer from a set of candidates with a\ngiven passage and question. Most of the existing researches focus on the\nmodeling of specific tasks or complex networks, without explicitly referring to\nrelevant and credible external knowledge sources, which are supposed to greatly\nmake up for the deficiency of the given passage. Thus we propose a novel\nreference-based knowledge enhancement model called Reference Knowledgeable\nNetwork (RekNet), which simulates human reading strategies to refine critical\ninformation from the passage and quote explicit knowledge in necessity. In\ndetail, RekNet refines finegrained critical information and defines it as\nReference Span, then quotes explicit knowledge quadruples by the co-occurrence\ninformation of Reference Span and candidates. The proposed RekNet is evaluated\non three multi-choice MRC benchmarks: RACE, DREAM and Cosmos QA, obtaining\nconsistent and remarkable performance improvement with observable statistical\nsignificance level over strong baselines. Our code is available at\nhttps://github.com/Yilin1111/RekNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04631","description":"<p>Machine translation between many languages at once is highly challenging,\nsince training with ground truth requires supervision between all language\npairs, which is difficult to obtain. Our key insight is that, while languages\nmay vary drastically, the underlying visual appearance of the world remains\nconsistent. We introduce a method that uses visual observations to bridge the\ngap between languages, rather than relying on parallel corpora or topological\nproperties of the representations. We train a model that aligns segments of\ntext from different languages if and only if the images associated with them\nare similar and each image in turn is well-aligned with its textual\ndescription. We train our model from scratch on a new dataset of text in over\nfifty languages with accompanying images. Experiments show that our method\noutperforms previous work on unsupervised word and sentence translation using\nretrieval. Code, models and data are available on globetrotter.cs.columbia.edu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1\">D&#xed;dac Sur&#xed;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06669","description":"<p>Many implicit inferences exist in text depending on how it is structured that\ncan critically impact the text's interpretation and meaning. One such\nstructural aspect present in text with chronology is the order of its\npresentation. For narratives or stories, this is known as the narrative order.\nReordering a narrative can impact the temporal, causal, event-based, and other\ninferences readers draw from it, which in turn can have strong effects both on\nits interpretation and interestingness. In this paper, we propose and\ninvestigate the task of Narrative Reordering (NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot. We\npresent a dataset, NAREORC, with human rewritings of stories within ROCStories\nin non-linear orders, and conduct a detailed analysis of it. Further, we\npropose novel task-specific training methods with suitable evaluation metrics.\nWe perform experiments on NAREORC using state-of-the-art models such as BART\nand T5 and conduct extensive automatic and human evaluations. We demonstrate\nthat although our models can perform decently, NAREOR is a challenging task\nwith potential for further exploration. We also investigate two applications of\nNAREOR: generation of more interesting variations of stories and serving as\nadversarial sets for temporal/event-related tasks, besides discussing other\nprospective ones, such as for pedagogical setups related to language skills\nlike essay writing and applications to medicine involving clinical narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Feature and Instance Attribution to Detect Artifacts. (arXiv:2107.00323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00323","description":"<p>Training the deep neural networks that dominate NLP requires large datasets.\nThese are often collected automatically or via crowdsourcing, and may exhibit\nsystematic biases or annotation artifacts. By the latter we mean spurious\ncorrelations between inputs and outputs that do not represent a generally held\ncausal relationship between features and classes; models that exploit such\ncorrelations may appear to perform a given task well, but fail on out of sample\ndata. In this paper we evaluate use of different attribution methods for aiding\nidentification of training data artifacts. We propose new hybrid approaches\nthat combine saliency maps (which highlight important input features) with\ninstance attribution methods (which retrieve training samples influential to a\ngiven prediction). We show that this proposed training-feature attribution can\nbe used to efficiently uncover artifacts in training data when a challenging\nvalidation set is available. We also carry out a small user study to evaluate\nwhether these methods are useful to NLP researchers in practice, with promising\nresults. We make code for all methods and experiments in this paper available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06712","description":"<p>Tables are often created with hierarchies, but existing works on table\nreasoning mainly focus on flat tables and neglect hierarchical tables.\nHierarchical tables challenge existing methods by hierarchical indexing, as\nwell as implicit relationships of calculation and semantics. This work presents\nHiTab, a free and open dataset to study question answering (QA) and natural\nlanguage generation (NLG) over hierarchical tables. HiTab is a cross-domain\ndataset constructed from a wealth of statistical reports (analyses) and\nWikipedia pages, and has unique characteristics: (1) nearly all tables are\nhierarchical, and (2) both target sentences for NLG and questions for QA are\nrevised from original, meaningful, and diverse descriptive sentences authored\nby analysts and professions of reports. (3) to reveal complex numerical\nreasoning in statistical analyses, we provide fine-grained annotations of\nentity and quantity alignment. HiTab provides 10,686 QA pairs and descriptive\nsentences with well-annotated quantity and entity alignment on 3,597 tables\nwith broad coverage of table hierarchies and numerical reasoning types.\n</p>\n<p>Targeting hierarchical structure, we devise a novel hierarchy-aware logical\nform for symbolic reasoning over tables, which shows high effectiveness.\nTargeting complex numerical reasoning, we propose partially supervised training\ngiven annotations of entity and quantity alignment, which helps models to\nlargely reduce spurious predictions in the QA task. In the NLG task, we find\nthat entity and quantity alignment also helps NLG models to generate better\nresults in a conditional generation setting. Experiment results of\nstate-of-the-art baselines suggest that this dataset presents a strong\nchallenge and a valuable benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain. (arXiv:2108.06955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06955","description":"<p>We present MobIE, a German-language dataset, which is human-annotated with 20\ncoarse- and fine-grained entity types and entity linking information for\ngeographically linkable entities. The dataset consists of 3,232 social media\ntexts and traffic reports with 91K tokens, and contains 20.5K annotated\nentities, 13.1K of which are linked to a knowledge base. A subset of the\ndataset is human-annotated with seven mobility-related, n-ary relation types,\nwhile the remaining documents are annotated using a weakly-supervised labeling\napproach implemented with the Snorkel framework. To the best of our knowledge,\nthis is the first German-language dataset that combines annotations for NER, EL\nand RE, and thus can be used for joint and multi-task learning of these\nfundamental information extraction tasks. We make MobIE public at\nhttps://github.com/dfki-nlp/mobie.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Phuc Tran Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabryszak_A/0/1/0/all/0/1\">Aleksandra Gabryszak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08614","description":"<p>Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents the first QA system\nthat can seamlessly operate over RDF datasets and text corpora, or both\ntogether, in a unified framework. Our method, called UNIQORN, builds a context\ngraph on-the-fly, by retrieving question-relevant evidences from the RDF data\nand/or a text corpus, using fine-tuned BERT models. The resulting graph is\ntypically rich but highly noisy. UNIQORN copes with this input by a graph\nalgorithm for Group Steiner Trees, that identifies the best answer candidates\nin the context graph. Experimental results on several benchmarks of complex\nquestions with multiple entities and relations, show that UNIQORN significantly\noutperforms state-of-the-art methods for QA over heterogeneous sources. The\ngraph-based methodology provides user-interpretable evidence for the complete\nanswering process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Multi-Aspect Explainability Analyses on Machine Reading Comprehension Models. (arXiv:2108.11574v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11574","description":"<p>Achieving human-level performance on some of the Machine Reading\nComprehension (MRC) datasets is no longer challenging with the help of powerful\nPre-trained Language Models (PLMs). However, the internal mechanism of these\nartifacts remains unclear, placing an obstacle for further understanding these\nmodels. This paper focuses on conducting a series of analytical experiments to\nexamine the relations between the multi-head self-attention and the final MRC\nsystem performance, revealing the potential explainability in PLM-based MRC\nmodels. To ensure the robustness of the analyses, we perform our experiments in\na multilingual way on top of various PLMs. We discover that passage-to-question\nand passage understanding attentions are the most important ones in the\nquestion answering process, showing strong correlations to the final\nperformance than other parts. Through comprehensive visualizations and case\nstudies, we also observe several general findings on the attention maps, which\ncan be helpful to understand how these models solve the questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Nan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Scaling Visual Question Answering (VQA) to the open-domain and multi-hop\nnature of web searches, requires fundamental advances in visual representation\nlearning, knowledge aggregation, and language generation. In this work, we\nintroduce WebQA, a challenging new benchmark that proves difficult for\nlarge-scale state-of-the-art models which lack language groundable visual\nrepresentations for novel objects and the ability to reason, yet trivial for\nhumans. WebQA mirrors the way humans use the web: 1) Ask a question, 2) Choose\nsources to aggregate, and 3) Produce a fluent language response. This is the\nbehavior we should be expecting from IoT devices and digital assistants.\nExisting work prefers to assume that a model can either reason about knowledge\nin images or in text. WebQA includes a secondary text-only QA task to ensure\nimproved visual performance does not come at the cost of language\nunderstanding. Our challenge for the community is to create unified multimodal\nreasoning models that answer questions regardless of the source modality,\nmoving us closer to digital assistants that not only query language knowledge,\nbut also the richer visual online world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It is AI's Turn to Ask Humans a Question: Question-Answer Pair Generation for Children's Story Books. (arXiv:2109.03423v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03423","description":"<p>Existing question answering (QA) techniques are created mainly to answer\nquestions asked by humans. But in educational applications, teachers often need\nto decide what questions they should ask, in order to help students to improve\ntheir narrative understanding capabilities. We design an automated\nquestion-answer generation (QAG) system for this education scenario: given a\nstory book at the kindergarten to eighth-grade level as input, our system can\nautomatically generate QA pairs that are capable of testing a variety of\ndimensions of a student's comprehension skills. Our proposed QAG model\narchitecture is demonstrated using a new expert-annotated FairytaleQA dataset,\nwhich has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and\nhuman evaluations show that our model outperforms state-of-the-art QAG baseline\nsystems. On top of our QAG system, we also start to build an interactive\nstory-telling application for the future real-world deployment in this\neducational scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning. (arXiv:2109.07589v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07589","description":"<p>Named Entity Recognition (NER) in Few-Shot setting is imperative for entity\ntagging in low resource domains. Existing approaches only learn class-specific\nsemantic features and intermediate representations from source domains. This\naffects generalizability to unseen target domains, resulting in suboptimal\nperformances. To this end, we present CONTaiNER, a novel contrastive learning\ntechnique that optimizes the inter-token distribution distance for Few-Shot\nNER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a\ngeneralized objective of differentiating between token categories based on\ntheir Gaussian-distributed embeddings. This effectively alleviates overfitting\nissues originating from training domains. Our experiments in several\ntraditional test domains (OntoNotes, CoNLL'03, WNUT '17, GUM) and a new large\nscale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER\noutperforms previous methods by 3%-13% absolute F1 points while showing\nconsistent performance trends, even in challenging scenarios where previous\napproaches could not achieve appreciable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sarkar Snigdha Sarathi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katiyar_A/0/1/0/all/0/1\">Arzoo Katiyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10852","description":"<p>We present Pix2Seq, a simple and generic framework for object detection.\nUnlike existing approaches that explicitly integrate prior knowledge about the\ntask, we cast object detection as a language modeling task conditioned on the\nobserved pixel inputs. Object descriptions (e.g., bounding boxes and class\nlabels) are expressed as sequences of discrete tokens, and we train a neural\nnetwork to perceive the image and generate the desired sequence. Our approach\nis based mainly on the intuition that if a neural network knows about where and\nwhat the objects are, we just need to teach it how to read them out. Beyond the\nuse of task-specific data augmentations, our approach makes minimal assumptions\nabout the task, yet it achieves competitive results on the challenging COCO\ndataset, compared to highly specialized and well optimized detection\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Saurabh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Non-local Features for Neural Constituency Parsing. (arXiv:2109.12814v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12814","description":"<p>Thanks to the strong representation power of neural encoders, neural\nchart-based parsers have achieved highly competitive performance by using local\nfeatures. Recently, it has been shown that non-local features in CRF structures\nlead to improvements. In this paper, we investigate injecting non-local\nfeatures into the training process of a local span-based parser, by predicting\nconstituent n-gram non-local patterns and ensuring consistency between\nnon-local patterns and local constituents. Results show that our simple method\ngives better results than the self-attentive parser on both PTB and CTB.\nBesides, our method achieves state-of-the-art BERT-based performance on PTB\n(95.92 F1) and strong performance on CTB (92.31 F1). Our parser also achieves\nbetter or competitive performance in multilingual and zero-shot cross-domain\nsettings compared with the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Channel End-to-End Neural Diarization with Distributed Microphones. (arXiv:2110.04694v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.04694","description":"<p>Recent progress on end-to-end neural diarization (EEND) has enabled\noverlap-aware speaker diarization with a single neural network. This paper\nproposes to enhance EEND by using multi-channel signals from distributed\nmicrophones. We replace Transformer encoders in EEND with two types of encoders\nthat process a multi-channel input: spatio-temporal and co-attention encoders.\nBoth are independent of the number and geometry of microphones and suitable for\ndistributed microphone settings. We also propose a model adaptation method\nusing only single-channel recordings. With simulated and real-recorded\ndatasets, we demonstrated that the proposed method outperformed conventional\nEEND when a multi-channel input was given while maintaining comparable\nperformance with a single-channel input. We also showed that the proposed\nmethod performed well even when spatial information is inoperative given\nmulti-channel inputs, such as in hybrid meetings in which the utterances of\nmultiple remote participants are played back from the same loudspeaker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_Y/0/1/0/all/0/1\">Yuki Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1\">Yohei Kawaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end translation of human neural activity to speech with a dual-dual generative adversarial network. (arXiv:2110.06634v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.06634","description":"<p>In a recent study of auditory evoked potential (AEP) based brain-computer\ninterface (BCI), it was shown that, with an encoder-decoder framework, it is\npossible to translate human neural activity to speech (T-CAS). However, current\nencoder-decoder-based methods achieve T-CAS often with a two-step method where\nthe information is passed between the encoder and decoder with a shared\ndimension reduction vector, which may result in a loss of information. A\npotential approach to this problem is to design an end-to-end method by using a\ndual generative adversarial network (DualGAN) without dimension reduction of\npassing information, but it cannot realize one-to-one signal-to-signal\ntranslation (see Fig.1 (a) and (b)). In this paper, we propose an end-to-end\nmodel to translate human neural activity to speech directly, create a new\nelectroencephalogram (EEG) datasets for participants with good attention by\ndesign a device to detect participants' attention, and introduce a dual-dual\ngenerative adversarial network (Dual-DualGAN) (see Fig. 1 (c) and (d)) to\naddress an end-to-end translation of human neural activity to speech (ET-CAS)\nproblem by group labelling EEG signals and speech signals, inserting a\ntransition domain to realize cross-domain mapping. In the transition domain,\nthe transition signals are cascaded by the corresponding EEG and speech signals\nin a certain proportion, which can build bridges for EEG and speech signals\nwithout corresponding features, and realize one-to-one cross-domain\nEEG-to-speech translation. The proposed method can translate word-length and\nsentence-length sequences of neural activity to speech. Experimental evaluation\nhas been conducted to show that the proposed method significantly outperforms\nstate-of-the-art methods on both words and sentences of auditory stimulus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yina Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhenying Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Anhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MReD: A Meta-Review Dataset for Controllable Text Generation. (arXiv:2110.07474v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07474","description":"<p>When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited. A typical example is\nwhen using CNN/Daily Mail dataset for controllable text summarization, there is\nno guided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and the control signal to guide\nthe generation, which can only be built with a deep understanding of the domain\nknowledge. Motivated by this vision, our paper introduces a new text generation\ndataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its\n45k meta-review sentences are manually annotated with one of the 9 carefully\ndefined categories, including abstract, strength, decision, etc. We present\nexperimental results on start-of-the-art summarization models, and propose\nmethods for structure-controlled generation with both extractive and\nabstractive models using our annotated data. By exploring various settings and\nanalyzing the model behavior with respect to the control signal, we demonstrate\nthe challenges of our proposed task and the values of our dataset MReD.\nMeanwhile, MReD also allows us to have a better understanding of the\nmeta-review domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction. (arXiv:2110.08345v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08345","description":"<p>Existing studies on semantic parsing focus primarily on mapping a\nnatural-language utterance to a corresponding logical form in one turn.\nHowever, because natural language can contain a great deal of ambiguity and\nvariability, this is a difficult challenge. In this work, we investigate an\ninteractive semantic parsing framework that explains the predicted logical form\nstep by step in natural language and enables the user to make corrections\nthrough natural-language feedback for individual steps. We focus on question\nanswering over knowledge bases (KBQA) as an instantiation of our framework,\naiming to increase the transparency of the parsing process and help the user\nappropriately trust the final answer. To do so, we construct INSPIRED, a\ncrowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our\nexperiments show that the interactive framework with human feedback has the\npotential to greatly improve overall parse accuracy. Furthermore, we develop a\npipeline for dialogue simulation to evaluate our framework w.r.t. a variety of\nstate-of-the-art KBQA models without involving further crowdsourcing effort.\nThe results demonstrate that our interactive semantic parsing framework\npromises to be effective across such models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1\">Lingbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_A/0/1/0/all/0/1\">Ashley Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Michael White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Up Vision-Language Pre-training for Image Captioning. (arXiv:2111.12233v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12233","description":"<p>In recent years, we have witnessed significant performance boost in the image\ncaptioning task based on vision-language pre-training (VLP). Scale is believed\nto be an important factor for this advance. However, most existing work only\nfocuses on pre-training transformers with moderate sizes (e.g., 12 or 24\nlayers) on roughly 4 million images. In this paper, we present LEMON, a\nLargE-scale iMage captiONer, and provide the first empirical study on the\nscaling behavior of VLP for image captioning. We use the state-of-the-art VinVL\nmodel as our reference model, which consists of an image feature extractor and\na transformer model, and scale the transformer both up and down, with model\nsizes ranging from 13 to 675 million parameters. In terms of data, we conduct\nexperiments with up to 200 million image-text pairs which are automatically\ncollected from web based on the alt attribute of the image (dubbed as ALT200M).\nExtensive analysis helps to characterize the performance trend as the model\nsize and the pre-training data size increase. We also compare different\ntraining recipes, especially for training on large-scale noisy data. As a\nresult, LEMON achieves new state of the arts on several major image captioning\nbenchmarks, including COCO Caption, nocaps, and Conceptual Captions. We also\nshow LEMON can generate captions with long-tail visual concepts when used in a\nzero-shot manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yumao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIREX: A Unified Learning Framework for Language Model Rationale Extraction. (arXiv:2112.08802v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08802","description":"<p>An extractive rationale explains a language model's (LM's) prediction on a\ngiven task instance by highlighting the text inputs that most influenced the\nprediction. Ideally, rationale extraction should be faithful (reflective of\nLM's actual behavior) and plausible (convincing to humans), without\ncompromising the LM's (i.e., task model's) task performance. Although\nattribution algorithms and select-predict pipelines are commonly used in\nrationale extraction, they both rely on certain heuristics that hinder them\nfrom satisfying all three desiderata. In light of this, we propose UNIREX, a\nflexible learning framework which generalizes rationale extractor optimization\nas follows: (1) specify architecture for a learned rationale extractor; (2)\nselect explainability objectives (i.e., faithfulness and plausibility\ncriteria); and (3) jointly the train task model and rationale extractor on the\ntask using selected objectives. UNIREX enables replacing prior works' heuristic\ndesign choices with a generic learned rationale extractor in (1) and optimizing\nit for all three desiderata in (2)-(3). To facilitate comparison between\nmethods w.r.t. multiple desiderata, we introduce the Normalized Relative Gain\n(NRG) metric. Across five text classification datasets, our best UNIREX\nconfiguration outperforms baselines by an average of 32.9% NRG. Plus, we find\nthat UNIREX-trained rationale extractors can even generalize to unseen datasets\nand tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaochang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logically at Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09253","description":"<p>This paper describes our participant system for the multi-modal fact\nverification (Factify) challenge at AAAI 2022. Despite the recent advance in\ntext based verification techniques and large pre-trained multimodal models\ncross vision and language, very limited work has been done in applying\nmultimodal techniques to automate fact checking process, particularly\nconsidering the increasing prevalence of claims and fake news about images and\nvideos on social media. In our work, the challenge is treated as multimodal\nentailment task and framed as multi-class classification. Two baseline\napproaches are proposed and explored including an ensemble model (combining two\nuni-modal models) and a multi-modal attention network (modeling the interaction\nbetween image and text pair from claim and evidence document). We conduct\nseveral experiments investigating and benchmarking different SoTA pre-trained\ntransformers and vision models in this work. Our best model is ranked first in\nleaderboard which obtains a weighted average F-measure of 0.77 on both\nvalidation and test set. Exploratory analysis of dataset is also carried out on\nthe Factify data set and uncovers salient patterns and issues (e.g., word\noverlapping, visual entailment correlation, source bias) that motivates our\nhypothesis. Finally, we highlight challenges of the task and multimodal dataset\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_H/0/1/0/all/0/1\">Hella-Franziska Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_S/0/1/0/all/0/1\">Stylianos Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiskovski_D/0/1/0/all/0/1\">David Kiskovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandhakavi_A/0/1/0/all/0/1\">Anil Bandhakavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-ABSA: Automatic Detection of Aspects in Aspect-Based Sentiment Analysis. (arXiv:2202.00484v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00484","description":"<p>After transformer is proposed, lots of pre-trained language models have been\ncome up with and sentiment analysis (SA) task has been improved. In this paper,\nwe proposed a method that uses an auxiliary sentence about aspects that the\nsentence contains to help sentiment prediction. The first is aspect detection,\nwhich uses a multi-aspects detection model to predict all aspects that the\nsentence has. Combining the predicted aspects and the original sentence as\nSentiment Analysis (SA) model's input. The second is to do out-of-domain\naspect-based sentiment analysis(ABSA), train sentiment classification model\nwith one kind of dataset and validate it with another kind of dataset. Finally,\nwe created two baselines, they use no aspect and all aspects as sentiment\nclassification model's input, respectively. Compare two baselines performance\nto our method, found that our method really makes sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bolun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yijie Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.07028","description":"<p>We study the problem of developing autonomous agents that can follow human\ninstructions to infer and perform a sequence of actions to complete the\nunderlying task. Significant progress has been made in recent years, especially\nfor tasks with short horizons. However, when it comes to long-horizon tasks\nwith extended sequences of actions, an agent can easily ignore some\ninstructions or get stuck in the middle of the long instructions and eventually\nfail the task. To address this challenge, we propose a model-agnostic\nmilestone-based task tracker (M-TRACK) to guide the agent and monitor its\nprogress. Specifically, we propose a milestone builder that tags the\ninstructions with navigation and interaction milestones which the agent needs\nto complete step by step, and a milestone checker that systemically checks the\nagent's progress in its current milestone and determines when to proceed to the\nnext. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and\n52% relative improvement in unseen success rate over two competitive base\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1\">Jihyung Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.11705","description":"<p>Many applications of text generation require incorporating different\nconstraints to control the semantics or style of generated text. These\nconstraints can be hard (e.g., ensuring certain keywords are included in the\noutput) and soft (e.g., contextualizing the output with the left- or right-hand\ncontext). In this paper, we present Energy-based Constrained Decoding with\nLangevin Dynamics (COLD), a decoding framework which unifies constrained\ngeneration as specifying constraints through an energy function, then\nperforming efficient differentiable reasoning over the constraints through\ngradient-based sampling. COLD decoding is a flexible framework that can be\napplied directly to off-the-shelf left-to-right language models without the\nneed for any task-specific fine-tuning, as demonstrated through three\nchallenging text generation applications: lexically-constrained generation,\nabductive reasoning, and counterfactual reasoning. Our experiments on these\nconstrained generation tasks point to the effectiveness of our approach, both\nin terms of automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction. (arXiv:2202.12109v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12109","description":"<p>In this paper, we propose an effective yet efficient model PAIE for both\nsentence-level and document-level Event Argument Extraction (EAE), which also\ngeneralizes well when there is a lack of training data. On the one hand, PAIE\nutilizes prompt tuning for extractive objectives to take the best advantages of\nPre-trained Language Models (PLMs). It introduces two span selectors based on\nthe prompt to select start/end tokens among input texts for each role. On the\nother hand, it captures argument interactions via multi-role prompts and\nconducts joint optimization with optimal span assignments via a bipartite\nmatching loss. Also, with a flexible prompt design, PAIE can extract multiple\narguments with the same role instead of conventional heuristic threshold\ntuning. We have conducted extensive experiments on three benchmarks, including\nboth sentence- and document-level EAE. The results present promising\nimprovements from PAIE (3.5\\% and 2.3\\% F1 gains in average on three\nbenchmarks, for PAIE-base and PAIE-large respectively). Further analysis\ndemonstrates the efficiency, generalization to few-shot settings, and\neffectiveness of different extractive prompt tuning strategies. Our code is\navailable at https://github.com/mayubo2333/PAIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yubo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Korean Tokenization for Beam Search Rescoring in Speech Recognition. (arXiv:2203.03583v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03583","description":"<p>The performance of automatic speech recognition (ASR) models can be greatly\nimproved by proper beam-search decoding with external language model (LM).\nThere has been an increasing interest in Korean speech recognition, but not\nmany studies have been focused on the decoding procedure. In this paper, we\npropose a Korean tokenization method for neural network-based LM used for\nKorean ASR. Although the common approach is to use the same tokenization method\nfor external LM as the ASR model, we show that it may not be the best choice\nfor Korean. We propose a new tokenization method that inserts a special token,\nSkipTC, when there is no trailing consonant in a Korean syllable. By utilizing\nthe proposed SkipTC token, the input sequence for LM becomes very regularly\npatterned so that the LM can better learn the linguistic characteristics. Our\nexperiments show that the proposed approach achieves a lower word error rate\ncompared to the same LM model without SkipTC. In addition, we are the first to\nreport the ASR performance for the recently introduced large-scale 7,600h\nKorean speech dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyuhong Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1\">Hyewon Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering. (arXiv:2203.04911v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04911","description":"<p>Spoken Question Answering (SQA) is to find the answer from a spoken document\ngiven a question, which is crucial for personal assistants when replying to the\nqueries from the users. Existing SQA methods all rely on Automatic Speech\nRecognition (ASR) transcripts. Not only does ASR need to be trained with\nmassive annotated data that are time and cost-prohibitive to collect for\nlow-resourced languages, but more importantly, very often the answers to the\nquestions include name entities or out-of-vocabulary words that cannot be\nrecognized correctly. Also, ASR aims to minimize recognition errors equally\nover all words, including many function words irrelevant to the SQA task.\nTherefore, SQA without ASR transcripts (textless) is always highly desired,\nalthough known to be very difficult.\n</p>\n<p>This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging\nunlabeled data for pre-training and fine-tuned by the SQA downstream task. The\ntime intervals of spoken answers can be directly predicted from spoken\ndocuments. We also release a new SQA benchmark corpus, NMSQA, for data with\nmore realistic scenarios. We empirically showed that DUAL yields results\ncomparable to those obtained by cascading ASR and text QA model and robust to\nreal-world data. Our code and model will be open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Ho-Lam Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lin-shan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderived from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps of the semantic features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor perform poorly in the TAGV task. To bridge these gaps, we\npropose a visual-prompt text span localizing (VPTSL) method, which introduces\nthe timestamped subtitles as a passage to perform the text span localization\nfor the input text question, and prompts the visual highlight features into the\npre-trained language model (PLM) for enhancing the joint semantic\nrepresentations. Specifically, the context query attention is utilized to\nperform cross-modal interaction between the extracted textual and visual\nfeatures. Then, the highlight features are obtained through the video-text\nhighlighting for the visual prompt. To alleviate semantic differences between\ntextual and visual features, we design the text span predictor by encoding the\nquestion, the subtitles, and the prompted visual highlight features with the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms the\nstate-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,\nwhich demonstrates the effectiveness of the proposed visual prompt and the text\nspan predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.07996","description":"<p>Training Transformer-based models demands a large amount of data, while\nobtaining aligned and labelled data in multimodality is rather cost-demanding,\nespecially for audio-visual speech recognition (AVSR). Thus it makes a lot of\nsense to make use of unlabelled unimodal data. On the other side, although the\neffectiveness of large-scale self-supervised learning is well established in\nboth audio and visual modalities, how to integrate those pre-trained models\ninto a multimodal scenario remains underexplored. In this work, we successfully\nleverage unimodal self-supervised learning to promote the multimodal AVSR. In\nparticular, audio and visual front-ends are trained on large-scale unimodal\ndatasets, then we integrate components of both front-ends into a larger\nmultimodal framework which learns to recognize parallel audio-visual data into\ncharacters through a combination of CTC and seq2seq decoding. We show that both\ncomponents inherited from unimodal self-supervised learning cooperate well,\nresulting in that the multimodal framework yields competitive results through\nfine-tuning. Our model is experimentally validated on both word-level and\nsentence-level tasks. Especially, even without an external language model, our\nproposed model raises the state-of-the-art performances on the widely accepted\nLip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative\nimprovement of 30%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xichen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yichen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Helong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Word Translation via Two-Stage Contrastive Learning. (arXiv:2203.08307v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08307","description":"<p>Word translation or bilingual lexicon induction (BLI) is a key cross-lingual\ntask, aiming to bridge the lexical gap between different languages. In this\nwork, we propose a robust and effective two-stage contrastive learning\nframework for the BLI task. At Stage C1, we propose to refine standard\ncross-lingual linear maps between static word embeddings (WEs) via a\ncontrastive learning objective; we also show how to integrate it into the\nself-learning procedure for even more refined cross-lingual maps. In Stage C2,\nwe conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word\ntranslation capability. We also show that static WEs induced from the\n`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments\non standard BLI datasets for diverse languages and different experimental\nsetups demonstrate substantial gains achieved by our framework. While the BLI\nmethod from Stage C1 already yields substantial gains over all state-of-the-art\nBLI methods in our comparison, even stronger improvements are met with the full\ntwo-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28\nlanguage pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaoyiran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake News Detection Using Majority Voting Technique. (arXiv:2203.09936v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09936","description":"<p>Due to the evolution of the Web and social network platforms it becomes very\neasy to disseminate the information. Peoples are creating and sharing more\ninformation than ever before, which may be misleading, misinformation or fake\ninformation. Fake news detection is a crucial and challenging task due to the\nunstructured nature of the available information. In the recent years,\nresearchers have provided significant solutions to tackle with the problem of\nfake news detection, but due to its nature there are still many open issues. In\nthis paper, we have proposed majority voting approach to detect fake news\narticles. We have used different textual properties of fake and real news. We\nhave used publicly available fake news dataset, comprising of 20,800 news\narticles among which 10,387 are real and 10,413 are fake news labeled as binary\n0 and 1. For the evaluation of our approach, we have used commonly used machine\nlearning classifiers like, Decision Tree, Logistic Regression, XGBoost, Random\nForest, Extra Trees, AdaBoost, SVM, SGD and Naive Bayes. Using the\naforementioned classifiers, we built a multi-model fake news detection system\nusing Majority Voting technique to achieve the more accurate results. The\nexperimental results show that, our proposed approach achieved accuracy of\n96.38%, precision of 96%, recall of 96% and F1-measure of 96%. The evaluation\nconfirms that, Majority Voting technique achieved more acceptable results as\ncompare to individual learning technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_D/0/1/0/all/0/1\">Dharmaraj R. Patil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaMEL: Case Marker Extraction without Labels. (arXiv:2203.10010v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10010","description":"<p>We introduce CaMEL (Case Marker Extraction without Labels), a novel and\nchallenging task in computational morphology that is especially relevant for\nlow-resource languages. We propose a first model for CaMEL that uses a\nmassively multilingual corpus to extract case markers in 83 languages based\nonly on a noun phrase chunker and an alignment system. To evaluate CaMEL, we\nautomatically construct a silver standard from UniMorph. The case markers\nextracted by our model can be used to detect and visualise similarities and\ndifferences between the case systems of different languages as well as to\nannotate fine-grained deep cases in languages in which they are not overtly\nmarked.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weissweiler_L/0/1/0/all/0/1\">Leonie Weissweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1\">Valentin Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity and Content-based Phonetic Self Attention for Speech Recognition. (arXiv:2203.10252v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10252","description":"<p>Transformer-based speech recognition models have achieved great success due\nto the self-attention (SA) mechanism that utilizes every frame in the feature\nextraction process. Especially, SA heads in lower layers capture various\nphonetic characteristics by the query-key dot product, which is designed to\ncompute the pairwise relationship between frames. In this paper, we propose a\nvariant of SA to extract more representative phonetic features. The proposed\nphonetic self-attention (phSA) is composed of two different types of phonetic\nattention; one is similarity-based and the other is content-based. In short,\nsimilarity-based attention utilizes the correlation between frames while\ncontent-based attention only considers each frame without being affected by\nothers. We identify which parts of the original dot product are related to two\ndifferent attention patterns and improve each part by simple modifications. Our\nexperiments on phoneme classification and speech recognition show that\nreplacing SA with phSA for lower layers improves the recognition performance\nwithout increasing the latency and the parameter size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyuhong Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.10750","description":"<p>In this paper, we develop a new multi-singer Chinese neural singing voice\nsynthesis (SVS) system named WeSinger. To improve the accuracy and naturalness\nof synthesized singing voice, we design several specifical modules and\ntechniques: 1) A deep bi-directional LSTM based duration model with multi-scale\nrhythm loss and post-processing step; 2) A Transformer-alike acoustic model\nwith progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet\nneural vocoder to produce high-quality singing waveforms; 4) A novel data\naugmentation method with multi-singer pre-training for stronger robustness and\nnaturalness. To our knowledge, WeSinger is the first SVS system to adopt 24 kHz\nLPCNet and multi-singer pre-training simultaneously. Both quantitative and\nqualitative evaluation results demonstrate the effectiveness of WeSinger in\nterms of accuracy and naturalness, and WeSinger achieves state-of-the-art\nperformance on the recently public Chinese singing corpus Opencpop. Some\nsynthesized singing samples are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zewang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VoicePrivacy 2022 Challenge Evaluation Plan. (arXiv:2203.12468v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.12468","description":"<p>For new participants - Executive summary: (1) The task is to develop a voice\nanonymization system for speech data which conceals the speaker's voice\nidentity while protecting linguistic content, paralinguistic attributes,\nintelligibility and naturalness. (2) Training, development and evaluation\ndatasets are provided in addition to 3 different baseline anonymization\nsystems, evaluation scripts, and metrics. Participants apply their developed\nanonymization systems, run evaluation scripts and submit objective evaluation\nresults and anonymized speech data to the organizers. (3) Results will be\npresented at a workshop held in conjunction with INTERSPEECH 2022 to which all\nparticipants are invited to present their challenge systems and to submit\nadditional workshop papers.\n</p>\n<p>For readers familiar with the VoicePrivacy Challenge - Changes w.r.t. 2020:\n(1) A stronger, semi-informed attack model in the form of an automatic speaker\nverification (ASV) system trained on anonymized (per-utterance) speech data.\n(2) Complementary metrics comprising the equal error rate (EER) as a privacy\nmetric, the word error rate (WER) as a primary utility metric, and the pitch\ncorrelation and gain of voice distinctiveness as secondary utility metrics. (3)\nA new ranking policy based upon a set of minimum target privacy requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_X/0/1/0/all/0/1\">Xiaoxiao Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nourtel_H/0/1/0/all/0/1\">Hubert Nourtel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Champion_P/0/1/0/all/0/1\">Pierre Champion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonastre_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bonastre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linearizing Transformer with Key-Value Memory Bank. (arXiv:2203.12644v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12644","description":"<p>Transformer has brought great success to a wide range of natural language\nprocessing tasks. Nevertheless, the computational overhead of the vanilla\ntransformer scales quadratically with sequence length. Many efforts have been\nmade to develop more efficient transformer variants. A line of work (e.g.,\nLinformer) projects the input sequence into a low-rank space, achieving linear\ntime complexity. However, Linformer does not suit well for text generation\ntasks as the sequence length must be pre-specified. We propose MemSizer, an\napproach also projects the source sequence into lower dimension representation\nbut can take input with dynamic length, with a different perspective of the\nattention mechanism. MemSizer not only achieves the same linear time complexity\nbut also enjoys efficient recurrent-style autoregressive generation, which\nyields constant memory complexity and reduced computation at inference. We\ndemonstrate that MemSizer provides an improved tradeoff between efficiency and\naccuracy over the vanilla transformer and other linear variants in language\nmodeling and machine translation tasks, revealing a viable direction towards\nfurther inference efficiency improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13474","description":"<p>Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification. We propose a conversational program synthesis\napproach via large language models, which addresses the challenges of searching\nover a vast program space and user intent specification faced in prior\napproaches. Our new approach casts the process of writing a specification and\nprogram as a multi-turn conversation between a user and a system. It treats\nprogram synthesis as a sequence prediction problem, in which the specification\nis expressed in natural language and the desired program is conditionally\nsampled. We train a family of large language models, called CodeGen, on natural\nlanguage and programming language data. With weak supervision in the data and\nthe scaling up of data size and model size, conversational capacities emerge\nfrom the simple autoregressive language modeling. To study the model behavior\non conversational program synthesis, we develop a multi-turn programming\nbenchmark (MTPB), where solving each problem requires multi-step synthesis via\nmulti-turn conversation between the user and the model. Our findings show the\nemergence of conversational capabilities and the effectiveness of the proposed\nconversational program synthesis paradigm. In addition, our model CodeGen (with\nup to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the\nHumanEval benchmark. We plan to make the training library JaxFormer including\ncheckpoints available as open source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1\">Erik Nijkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations. (arXiv:2203.13602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13602","description":"<p>The current workflow for Information Extraction (IE) analysts involves the\ndefinition of the entities/relations of interest and a training corpus with\nannotated examples. In this demonstration we introduce a new workflow where the\nanalyst directly verbalizes the entities/relations, which are then used by a\nTextual Entailment model to perform zero-shot IE. We present the design and\nimplementation of a toolkit with a user interface, as well as experiments on\nfour IE tasks that show that the system achieves very good performance at\nzero-shot learning using only 5--15 minutes per type of a user's effort. Our\ndemonstration system is open-sourced at https://github.com/BBN-E/ZS4IE . A\ndemonstration video is available at https://vimeo.<a href=\"/abs/com/6761383\">com/6761383</a>40 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haoling Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1\">Bonan Min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13687","description":"<p>In our previous work, we proposed a discriminative autoencoder (DcAE) for\nspeech recognition. DcAE combines two training schemes into one. First, since\nDcAE aims to learn encoder-decoder mappings, the squared error between the\nreconstructed speech and the input speech is minimized. Second, in the code\nlayer, frame-based phonetic embeddings are obtained by minimizing the\ncategorical cross-entropy between ground truth labels and predicted\ntriphone-state scores. DcAE is developed based on the Kaldi toolkit by treating\nvarious TDNN models as encoders. In this paper, we further propose three new\nversions of DcAE. First, a new objective function that considers both\ncategorical cross-entropy and mutual information between ground truth and\npredicted triphone-state sequences is used. The resulting DcAE is called a\nchain-based DcAE (c-DcAE). For application to robust speech recognition, we\nfurther extend c-DcAE to hierarchical and parallel structures, resulting in\nhc-DcAE and pc-DcAE. In these two models, both the error between the\nreconstructed noisy speech and the input noisy speech and the error between the\nenhanced speech and the reference clean speech are taken into the objective\nfunction. Experimental results on the WSJ and Aurora-4 corpora show that our\nDcAE models outperform baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pin-Tuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UKP-SQUARE: An Online Platform for Question Answering Research. (arXiv:2203.13693v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13693","description":"<p>Recent advances in NLP and information retrieval have given rise to a diverse\nset of question answering tasks that are of different formats (e.g.,\nextractive, abstractive), require different model architectures (e.g.,\ngenerative, discriminative), and setups (e.g., with or without retrieval).\nDespite having a large number of powerful, specialized QA pipelines (which we\nrefer to as Skills) that consider a single domain, model or setup, there exists\nno framework where users can easily explore and compare such pipelines and can\nextend them according to their needs. To address this issue, we present\nUKP-SQUARE, an extensible online QA platform for researchers which allows users\nto query and analyze a large collection of modern Skills via a user-friendly\nweb interface and integrated behavioural tests. In addition, QA researchers can\ndevelop, manage, and share their custom Skills using our microservices that\nsupport a wide range of models (Transformers, Adapters, ONNX), datastores and\nretrieval techniques (e.g., sparse and dense). UKP-SQUARE is available on\nhttps://square.ukp-lab.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_T/0/1/0/all/0/1\">Tim Baumg&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Rachneet Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichler_M/0/1/0/all/0/1\">Max Eichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poth_C/0/1/0/all/0/1\">Clifton Poth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterz_H/0/1/0/all/0/1\">Hannah Sterz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration. (arXiv:2203.13834v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13834","description":"<p>Deep Neural Networks ( DNN s) are known to make overconfident mistakes, which\nmakes their use problematic in safety-critical applications. State-of-the-art (\nSOTA ) calibration techniques improve on the confidence of predicted labels\nalone and leave the confidence of non-max classes (e.g. top-2, top-5)\nuncalibrated. Such calibration is not suitable for label refinement using\npost-processing. Further, most SOTA techniques learn a few hyper-parameters\npost-hoc, leaving out the scope for image, or pixel specific calibration. This\nmakes them unsuitable for calibration under domain shift, or for dense\nprediction tasks like semantic segmentation. In this paper, we argue for\nintervening at the train time itself, so as to directly produce calibrated DNN\nmodels. We propose a novel auxiliary loss function: Multi-class Difference in\nConfidence and Accuracy ( MDCA ), to achieve the same MDCA can be used in\nconjunction with other application/task-specific loss functions. We show that\ntraining with MDCA leads to better-calibrated models in terms of Expected\nCalibration Error ( ECE ), and Static Calibration Error ( SCE ) on image\nclassification, and segmentation tasks. We report ECE ( SCE ) score of 0.72\n(1.60) on the CIFAR 100 dataset, in comparison to 1.90 (1.71) by the SOTA.\nUnder domain shift, a ResNet-18 model trained on PACS dataset using MDCA gives\nan average ECE ( SCE ) score of 19.7 (9.7) across all domains, compared to 24.2\n(11.8) by the SOTA. For the segmentation task, we report a 2X reduction in\ncalibration error on PASCAL - VOC dataset in comparison to Focal Loss. Finally,\nMDCA training improves calibration even on imbalanced data, and for natural\nlanguage classification tasks. We have released the code here: code is\navailable at https://github.com/mdca-loss\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hebbalaguppe_R/0/1/0/all/0/1\">Ramya Hebbalaguppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_J/0/1/0/all/0/1\">Jatin Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1\">Neelabh Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas. (arXiv:2203.13838v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13838","description":"<p>Vision and language navigation (VLN) is a challenging visually-grounded\nlanguage understanding task. Given a natural language navigation instruction, a\nvisual agent interacts with a graph-based environment equipped with panorama\nimages and tries to follow the described route. Most prior work has been\nconducted in indoor scenarios where best results were obtained for navigation\non routes that are similar to the training routes, with sharp drops in\nperformance when testing on unseen environments. We focus on VLN in outdoor\nscenarios and find that in contrast to indoor VLN, most of the gain in outdoor\nVLN on unseen data is due to features like junction type embedding or heading\ndelta that are specific to the respective environment graph, while image\ninformation plays a very minor role in generalizing VLN to unseen outdoor\nareas. These findings show a bias to specifics of graph representations of\nurban environments, demanding that VLN tasks grow in scale and diversity of\ngeographical environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1\">Raphael Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets. (arXiv:2203.13856v1 [eess.IV])","link":"http://arxiv.org/abs/2203.13856","description":"<p>Deep learning has been proposed for the assessment and classification of\nmedical images. However, many medical image databases with appropriately\nlabeled and annotated images are small and imbalanced, and thus unsuitable to\ntrain and validate such models. The option is to generate synthetic images and\none successful technique has been patented which limits its use for others. We\nhave developed a free-access, alternate method for generating synthetic\nhigh-resolution images using Generative Adversarial Networks (GAN) for data\naugmentation and showed their effectiveness using eye-fundus images for\nAge-Related Macular Degeneration (AMD) identification. Ten different GAN\narchitectures were compared to generate synthetic eye-fundus images with and\nwithout AMD. Data from three public databases were evaluated using the\nFr\\'echet Inception Distance (FID), two clinical experts and deep-learning\nclassification. The results show that StyleGAN2 reached the lowest FID\n(166.17), and clinicians could not accurately differentiate between real and\nsynthetic images. ResNet-18 architecture obtained the best performance with 85%\naccuracy and outperformed the two experts in detecting AMD fundus images, whose\naverage accuracy was 77.5%. These results are similar to a recently patented\nmethod, and will provide an alternative to generating high-quality synthetic\nmedical images. Free access has been provided to the entire method to\nfacilitate the further development of this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_G/0/1/0/all/0/1\">Guilherme C. Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo H. Rosa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedronette_D/0/1/0/all/0/1\">Daniel C. G. Pedronette</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o P. Papa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_H/0/1/0/all/0/1\">Himeesh Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Passos_L/0/1/0/all/0/1\">Leandro A. Passos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_D/0/1/0/all/0/1\">Dinesh Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeReplayer: Unlocking the Potential of Event Cameras for Video Interpolation. (arXiv:2203.13859v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13859","description":"<p>Recording fast motion in a high FPS (frame-per-second) requires expensive\nhigh-speed cameras. As an alternative, interpolating low-FPS videos from\ncommodity cameras has attracted significant attention. If only low-FPS videos\nare available, motion assumptions (linear or quadratic) are necessary to infer\nintermediate frames, which fail to model complex motions. Event camera, a new\ncamera with pixels producing events of brightness change at the temporal\nresolution of $\\mu s$ $(10^{-6}$ second $)$, is a game-changing device to\nenable video interpolation at the presence of arbitrarily complex motion. Since\nevent camera is a novel sensor, its potential has not been fulfilled due to the\nlack of processing algorithms. The pioneering work Time Lens introduced event\ncameras to video interpolation by designing optical devices to collect a large\namount of paired training data of high-speed frames and events, which is too\ncostly to scale. To fully unlock the potential of event cameras, this paper\nproposes a novel TimeReplayer algorithm to interpolate videos captured by\ncommodity cameras with events. It is trained in an unsupervised\ncycle-consistent style, canceling the necessity of high-speed training data and\nbringing the additional ability of video extrapolation. Its state-of-the-art\nresults and demo videos in supplementary reveal the promising future of\nevent-based vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Weihua He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Kaichao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhendong Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jianxing Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FD-SLAM: 3-D Reconstruction Using Features and Dense Matching. (arXiv:2203.13861v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13861","description":"<p>It is well known that visual SLAM systems based on dense matching are locally\naccurate but are also susceptible to long-term drift and map corruption. In\ncontrast, feature matching methods can achieve greater long-term consistency\nbut can suffer from inaccurate local pose estimation when feature information\nis sparse. Based on these observations, we propose an RGB-D SLAM system that\nleverages the advantages of both approaches: using dense frame-to-model\nodometry to build accurate sub-maps and on-the-fly feature-based matching\nacross sub-maps for global map optimisation. In addition, we incorporate a\nlearning-based loop closure component based on 3-D features which further\nstabilises map building. We have evaluated the approach on indoor sequences\nfrom public datasets, and the results show that it performs on par or better\nthan state-of-the-art systems in terms of map reconstruction quality and pose\nestimation. The approach can also scale to large scenes where other systems\noften fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yuhang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1\">Andrew Calway</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent Masking: Deep Q-Learning for Context Encoding in Medical Image Analysis. (arXiv:2203.13865v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13865","description":"<p>The need for a large amount of labeled data in the supervised setting has led\nrecent studies to utilize self-supervised learning to pre-train deep neural\nnetworks using unlabeled data. Many self-supervised training strategies have\nbeen investigated especially for medical datasets to leverage the information\navailable in the much fewer unlabeled data. One of the fundamental strategies\nin image-based self-supervision is context prediction. In this approach, a\nmodel is trained to reconstruct the contents of an arbitrary missing region of\nan image based on its surroundings. However, the existing methods adopt a\nrandom and blind masking approach by focusing uniformly on all regions of the\nimages. This approach results in a lot of unnecessary network updates that\ncause the model to forget the rich extracted features. In this work, we develop\na novel self-supervised approach that occludes targeted regions to improve the\npre-training procedure. To this end, we propose a reinforcement learning-based\nagent which learns to intelligently mask input images through deep Q-learning.\nWe show that training the agent against the prediction model can significantly\nimprove the semantic features extracted for downstream classification tasks. We\nperform our experiments on two public datasets for diagnosing breast cancer in\nthe ultrasound images and detecting lower-grade glioma with MR images. In our\nexperiments, we show that our novel masking strategy advances the learned\nfeatures according to the performance on the classification task in terms of\naccuracy, macro F1, and AUROC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahrami_M/0/1/0/all/0/1\">Mojtaba Bahrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_M/0/1/0/all/0/1\">Mahsa Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Semantic Segmentation Grounded in Visual Concepts. (arXiv:2203.13868v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13868","description":"<p>Unsupervised semantic segmentation requires assigning a label to every pixel\nwithout any human annotations. Despite recent advances in self-supervised\nrepresentation learning for individual images, unsupervised semantic\nsegmentation with pixel-level representations is still a challenging task and\nremains underexplored. In this work, we propose a self-supervised pixel\nrepresentation learning method for semantic segmentation by using visual\nconcepts (i.e., groups of pixels with semantic meanings, such as parts,\nobjects, and scenes) extracted from images. To guide self-supervised learning,\nwe leverage three types of relationships between pixels and concepts, including\nthe relationships between pixels and local concepts, local and global concepts,\nas well as the co-occurrence of concepts. We evaluate the learned pixel\nembeddings and visual concepts on three datasets, including PASCAL VOC 2012,\nCOCO 2017, and DAVIS 2017. Our results show that the proposed method gains\nconsistent and substantial improvements over recent unsupervised semantic\nsegmentation approaches, and also demonstrate that visual concepts can reveal\ninsights into image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surmeier_W/0/1/0/all/0/1\">William Surmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekar_A/0/1/0/all/0/1\">Arvind Kumar Shekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1\">Liang Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised machine learning model for analysis of nanowire morphologies from transmission electron microscopy images. (arXiv:2203.13875v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2203.13875","description":"<p>In the field of soft materials, microscopy is the first and often only\naccessible method for structural characterization. There is a growing interest\nin the development of machine learning methods that can automate the analysis\nand interpretation of microscopy images. Typically training of machine learning\nmodels require large numbers of images with associated structural labels,\nhowever, manual labeling of images requires domain knowledge and is prone to\nhuman error and subjectivity. To overcome these limitations, we present a\nself-supervised transfer learning approach that uses a small number of labeled\nmicroscopy images for training and performs as effectively as methods trained\non significantly larger data sets. Specifically, we train an image encoder with\nunlabeled images and use that encoder for transfer learning of different\ndownstream image tasks (classification and segmentation) with a minimal number\nof labeled images for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Lu_S/0/1/0/all/0/1\">Shizhao Lu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Montz_B/0/1/0/all/0/1\">Brian Montz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Emrick_T/0/1/0/all/0/1\">Todd Emrick</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jayaraman_A/0/1/0/all/0/1\">Arthi Jayaraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning with Action-Free Pre-Training from Videos. (arXiv:2203.13880v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13880","description":"<p>Recent unsupervised pre-training methods have shown to be effective on\nlanguage and vision domains by learning useful representations for multiple\ndownstream tasks. In this paper, we investigate if such unsupervised\npre-training methods can also be effective for vision-based reinforcement\nlearning (RL). To this end, we introduce a framework that learns\nrepresentations useful for understanding the dynamics via generative\npre-training on videos. Our framework consists of two phases: we pre-train an\naction-free latent video prediction model, and then utilize the pre-trained\nrepresentations for efficiently learning action-conditional world models on\nunseen environments. To incorporate additional action inputs during\nfine-tuning, we introduce a new architecture that stacks an action-conditional\nlatent prediction model on top of the pre-trained action-free prediction model.\nMoreover, for better exploration, we propose a video-based intrinsic bonus that\nleverages pre-trained representations. We demonstrate that our framework\nsignificantly improves both final performances and sample-efficiency of\nvision-based RL in a variety of manipulation and locomotion tasks. Code is\navailable at https://github.com/younggyoseo/apv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Younggyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13883","description":"<p>As social media platforms are evolving from text-based forums into\nmulti-modal environments, the nature of misinformation in social media is also\nchanging accordingly. Taking advantage of the fact that visual modalities such\nas images and videos are more favorable and attractive to the users, and\ntextual contents are sometimes skimmed carelessly, misinformation spreaders\nhave recently targeted contextual correlations between modalities e.g., text\nand image. Thus, many research efforts have been put into development of\nautomatic techniques for detecting possible cross-modal discordances in\nweb-based media. In this work, we aim to analyze, categorize and identify\nexisting approaches in addition to challenges and shortcomings they face in\norder to unearth new opportunities in furthering the research in the field of\nmulti-modal misinformation detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1\">Sara Abdali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sylph: A Hypernetwork Framework for Incremental Few-shot Object Detection. (arXiv:2203.13903v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13903","description":"<p>We study the challenging incremental few-shot object detection (iFSD)\nsetting. Recently, hypernetwork-based approaches have been studied in the\ncontext of continuous and finetune-free iFSD with limited success. We take a\ncloser look at important design choices of such methods, leading to several key\nimprovements and resulting in a more accurate and flexible framework, which we\ncall Sylph. In particular, we demonstrate the effectiveness of decoupling\nobject classification from localization by leveraging a base detector that is\npretrained for class-agnostic localization on large-scale dataset. Contrary to\nwhat previous results have suggested, we show that with a carefully designed\nclass-conditional hypernetwork, finetune-free iFSD can be highly effective,\nespecially when a large number of base categories with abundant data are\navailable for meta-training, almost approaching alternatives that undergo\ntest-time-training. This result is even more significant considering its many\npractical advantages: (1) incrementally learning new classes in sequence\nwithout additional training, (2) detecting both novel and seen classes in a\nsingle pass, and (3) no forgetting of previously seen classes. We benchmark our\nmodel on both COCO and LVIS, reporting as high as $17\\%$ AP on the long-tail\nrare classes on LVIS, indicating the promise of hypernetwork-based iFSD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Li Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1\">Juan M Perez-Rua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kevin J Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Embedding Analysis: A Review. (arXiv:2203.13909v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13909","description":"<p>Deep neural networks (DNNs) have found their way into many applications with\npotential impact on the safety, security, and fairness of\nhuman-machine-systems. Such require basic understanding and sufficient trust by\nthe users. This motivated the research field of explainable artificial\nintelligence (XAI), i.e. finding methods for opening the \"black-boxes\" DNNs\nrepresent. For the computer vision domain in specific, practical assessment of\nDNNs requires a globally valid association of human interpretable concepts with\ninternals of the model. The research field of concept (embedding) analysis (CA)\ntackles this problem: CA aims to find global, assessable associations of\nhumanly interpretable semantic concepts (e.g., eye, bearded) with internal\nrepresentations of a DNN. This work establishes a general definition of CA and\na taxonomy for CA methods, uniting several ideas from literature. That allows\nto easily position and compare CA approaches. Guided by the defined notions,\nthe current state-of-the-art research regarding CA methods and interesting\napplications are reviewed. More than thirty relevant methods are discussed,\ncompared, and categorized. Finally, for practitioners, a survey of fifteen\ndatasets is provided that have been used for supervised concept analysis. Open\nchallenges and research directions are pointed out at the end.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwalbe_G/0/1/0/all/0/1\">Gesina Schwalbe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cross-Domain Approach for Continuous Impression Recognition from Dyadic Audio-Visual-Physio Signals. (arXiv:2203.13932v1 [cs.MM])","link":"http://arxiv.org/abs/2203.13932","description":"<p>The impression we make on others depends not only on what we say, but also,\nto a large extent, on how we say it. As a sub-branch of affective computing and\nsocial signal processing, impression recognition has proven critical in both\nhuman-human conversations and spoken dialogue systems. However, most research\nhas studied impressions only from the signals expressed by the emitter,\nignoring the response from the receiver. In this paper, we perform impression\nrecognition using a proposed cross-domain architecture on the dyadic IMPRESSION\ndataset. This improved architecture makes use of cross-domain attention and\nregularization. The cross-domain attention consists of intra- and\ninter-attention mechanisms, which capture intra- and inter-domain relatedness,\nrespectively. The cross-domain regularization includes knowledge distillation\nand similarity enhancement losses, which strengthen the feature connections\nbetween the emitter and receiver. The experimental evaluation verified the\neffectiveness of our approach. Our approach achieved a concordance correlation\ncoefficient of 0.770 in competence dimension and 0.748 in warmth dimension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SolidGen: An Autoregressive Model for Direct B-rep Synthesis. (arXiv:2203.13944v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13944","description":"<p>The Boundary representation (B-rep) format is the de-facto shape\nrepresentation in computer-aided design (CAD) to model watertight solid\nobjects. Recent approaches to generating CAD models have focused on learning\nsketch-and-extrude modeling sequences that are executed by a solid modeling\nkernel in postprocess to recover a B-rep. In this paper we present a new\napproach that enables learning from and synthesizing B-reps without the need\nfor supervision through CAD modeling sequence data. Our method SolidGen, is an\nautoregressive neural network that models the B-rep directly by predicting the\nvertices, edges and faces using Transformer-based and pointer neural networks.\nKey to achieving this is our Indexed Boundary Representation that references\nB-rep vertices, edges and faces in a well-defined hierarchy to capture the\ngeometric and topological relations suitable for use with machine learning.\nSolidGen can be easily conditioned on contexts e.g., class labels thanks to its\nprobabilistic modeling of the B-rep distribution. We demonstrate qualitatively,\nquantitatively and through perceptual evaluation by human subjects that\nSolidGen can produce high quality, realistic looking CAD models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1\">Pradeep Kumar Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph G. Lambourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_N/0/1/0/all/0/1\">Nishkrit Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1\">Karl D.D. Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_N/0/1/0/all/0/1\">Nigel J.W. Morris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-augmented histopathologic review using image analysis to optimize DNA yield and tumor purity from FFPE slides. (arXiv:2203.13948v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13948","description":"<p>To achieve minimum DNA input and tumor purity requirements for\nnext-generation sequencing (NGS), pathologists visually estimate\nmacrodissection and slide count decisions. Misestimation may cause tissue waste\nand increased laboratory costs. We developed an AI-augmented smart pathology\nreview system (SmartPath) to empower pathologists with quantitative metrics for\ndetermining tissue extraction parameters. Using digitized H&amp;E-stained FFPE\nslides as inputs, SmartPath segments tumors, extracts cell-based features, and\nsuggests a macrodissection areas. To predict DNA yield per slide, the extracted\nfeatures are correlated with known DNA yields. Then, a pathologist-defined\ntarget yield divided by the predicted DNA yield/slide gives the number of\nslides to scrape. Following model development, an internal validation trial was\nconducted within the Tempus Labs molecular sequencing laboratory. We evaluated\nour system on 501 clinical colorectal cancer slides, where half received\nSmartPath-augmented review and half traditional pathologist review. The\nSmartPath cohort had 25% more DNA yields within a desired target range of\n100-2000ng. The SmartPath system recommended fewer slides to scrape for large\ntissue sections, saving tissue in these cases. Conversely, SmartPath\nrecommended more slides to scrape for samples with scant tissue sections,\nhelping prevent costly re-extraction due to insufficient extraction yield. A\nstatistical analysis was performed to measure the impact of covariates on the\nresults, offering insights on how to improve future applications of SmartPath.\nOverall, the study demonstrated that AI-augmented histopathologic review using\nSmartPath could decrease tissue waste, sequencing time, and laboratory costs by\noptimizing DNA yields and tumor purity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Boles&#x142;aw L. Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+BenTaieb_A/0/1/0/all/0/1\">A&#xef;cha BenTaieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_I/0/1/0/all/0/1\">Irvin Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Ryan D. Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rohan P. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westley_A/0/1/0/all/0/1\">Andrew Westley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_M/0/1/0/all/0/1\">Michael Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1\">Caleb Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schleicher_L/0/1/0/all/0/1\">Luke Schleicher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahon_B/0/1/0/all/0/1\">Brett M. Mahon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection. (arXiv:2203.13954v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13954","description":"<p>The task of Human-Object Interaction~(HOI) detection could be divided into\ntwo core problems, i.e., human-object association and interaction\nunderstanding. In this paper, we reveal and address the disadvantages of the\nconventional query-driven HOI detectors from the two aspects. For the\nassociation, previous two-branch methods suffer from complex and costly\npost-matching, while single-branch methods ignore the features distinction in\ndifferent tasks. We propose Guided-Embedding Network~(GEN) to attain a\ntwo-branch pipeline without post-matching. In GEN, we design an instance\ndecoder to detect humans and objects with two independent query sets and a\nposition Guided Embedding~(p-GE) to mark the human and object in the same\nposition as a pair. Besides, we design an interaction decoder to classify\ninteractions, where the interaction queries are made of instance Guided\nEmbeddings (i-GE) generated from the outputs of each instance decoder layer.\nFor the interaction understanding, previous methods suffer from long-tailed\ndistribution and zero-shot discovery. This paper proposes a Visual-Linguistic\nKnowledge Transfer (VLKT) training strategy to enhance interaction\nunderstanding by transferring knowledge from a visual-linguistic pre-trained\nmodel CLIP. In specific, we extract text embeddings for all labels with CLIP to\ninitialize the classifier and adopt a mimic loss to minimize the visual feature\ndistance between GEN and CLIP. As a result, GEN-VLKT outperforms the state of\nthe art by large margins on multiple datasets, e.g., +5.05 mAP on HICO-Det. The\nsource codes are available at https://github.com/YueLiao/gen-vlkt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aixi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Miao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution. (arXiv:2203.13963v1 [eess.IV])","link":"http://arxiv.org/abs/2203.13963","description":"<p>Magnetic resonance imaging (MRI) can present multi-contrast images of the\nsame anatomical structures, enabling multi-contrast super-resolution (SR)\ntechniques. Compared with SR reconstruction using a single-contrast,\nmulti-contrast SR reconstruction is promising to yield SR images with higher\nquality by leveraging diverse yet complementary information embedded in\ndifferent imaging modalities. However, existing methods still have two\nshortcomings: (1) they neglect that the multi-contrast features at different\nscales contain different anatomical details and hence lack effective mechanisms\nto match and fuse these features for better reconstruction; and (2) they are\nstill deficient in capturing long-range dependencies, which are essential for\nthe regions with complicated anatomical structures. We propose a novel network\nto comprehensively address these problems by developing a set of innovative\nTransformer-empowered multi-scale contextual matching and aggregation\ntechniques; we call it McMRSR. Firstly, we tame transformers to model\nlong-range dependencies in both reference and target images. Then, a new\nmulti-scale contextual matching method is proposed to capture corresponding\ncontexts from reference features at different scales. Furthermore, we introduce\na multi-scale aggregation mechanism to gradually and interactively aggregate\nmulti-scale matched features for reconstructing the target SR MR image.\nExtensive experiments demonstrate that our network outperforms state-of-the-art\napproaches and has great potential to be applied in clinical practice. Codes\nare available at https://github.com/XAIMI-Lab/McMRSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guangyuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengyan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Global and Local Features for Generalized AI-Synthesized Image Detection. (arXiv:2203.13964v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13964","description":"<p>With the development of the Generative Adversarial Networks (GANs) and\nDeepFakes, AI-synthesized images are now of such high quality that humans can\nhardly distinguish them from real images. It is imperative for media forensics\nto develop detectors to expose them accurately. Existing detection methods have\nshown high performance in generated images detection, but they tend to\ngeneralize poorly in the real-world scenarios, where the synthetic images are\nusually generated with unseen models using unknown source data. In this work,\nwe emphasize the importance of combining information from the whole image and\ninformative patches in improving the generalization ability of AI-synthesized\nimage detection. Specifically, we design a two-branch model to combine global\nspatial information from the whole image and local informative features from\nmultiple patches selected by a novel patch selection module. Multi-head\nattention mechanism is further utilized to fuse the global and local features.\nWe collect a highly diverse dataset synthesized by 19 models with various\nobjects and resolutions to evaluate our model. Experimental results demonstrate\nthe high accuracy and good generalization ability of our method in detecting\ngenerated images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Shan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lipeng Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongfei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1\">Koki Nagano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Self-Attention for Visual Intersection Classification. (arXiv:2203.13977v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13977","description":"<p>In robot vision, self-attention has recently emerged as a technique for\ncapturing non-local contexts. In this study, we introduced a self-attention\nmechanism into the intersection recognition system as a method to capture the\nnon-local contexts behind the scenes. An intersection classification system\ncomprises two distinctive modules: (a) a first-person vision (FPV) module,\nwhich uses a short egocentric view sequence as the intersection is passed, and\n(b) a third-person vision (TPV) module, which uses a single view immediately\nbefore entering the intersection. The self-attention mechanism is effective in\nthe TPV module because most parts of the local pattern (e.g., road edges,\nbuildings, and sky) are similar to each other, and thus the use of a non-local\ncontext (e.g., the angle between two diagonal corners around an intersection)\nwould be effective. This study makes three major contributions. First, we\nproposed a self-attention-based approach for intersection classification using\nTPVs. Second, we presented a practical system in which a self-attention-based\nTPV module is combined with an FPV module to improve the overall recognition\nperformance. Finally, experiments using the public KITTI dataset show that the\nabove self-attention-based system outperforms conventional recognition based on\nlocal patterns and recognition based on convolution operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakata_H/0/1/0/all/0/1\">Haruki Nakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1\">Kanji Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koji Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Current Source Localization Using Deep Prior with Depth Weighting. (arXiv:2203.13981v1 [eess.SP])","link":"http://arxiv.org/abs/2203.13981","description":"<p>This paper proposes a novel neuronal current source localization method based\non Deep Prior that represents a more complicated prior distribution of current\nsource using convolutional networks. Deep Prior has been suggested as a means\nof an unsupervised learning approach that does not require learning using\ntraining data, and randomly-initialized neural networks are used to update a\nsource location using a single observation. In our previous work, a\nDeep-Prior-based current source localization method in the brain has been\nproposed but the performance was not almost the same as those of conventional\napproaches, such as sLORETA. In order to improve the Deep-Prior-based approach,\nin this paper, a depth weight of the current source is introduced for Deep\nPrior, where depth weighting amounts to assigning more penalty to the\nsuperficial currents. Its effectiveness is confirmed by experiments of current\nsource estimation on simulated MEG data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yamana_R/0/1/0/all/0/1\">Rio Yamana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yano_H/0/1/0/all/0/1\">Hajime Yano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_R/0/1/0/all/0/1\">Ryoichi Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takiguchi_T/0/1/0/all/0/1\">Tetsuya Takiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nakagawa_S/0/1/0/all/0/1\">Seiji Nakagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Monocular Depth Estimation Provide Better Pre-training than Classification for Semantic Segmentation?. (arXiv:2203.13987v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13987","description":"<p>Training a deep neural network for semantic segmentation is labor-intensive,\nso it is common to pre-train it for a different task, and then fine-tune it\nwith a small annotated dataset. State-of-the-art methods use image\nclassification for pre-training, which introduces uncontrolled biases. We test\nthe hypothesis that depth estimation from unlabeled videos may provide better\npre-training. Despite the absence of any semantic information, we argue that\nestimating scene geometry is closer to the task of semantic segmentation than\nclassifying whole images into semantic classes. Since analytical validation is\nintractable, we test the hypothesis empirically by introducing a pre-training\nscheme that yields an improvement of 5.7% mIoU and 4.1% pixel accuracy over\nclassification-based pre-training. While annotation is not needed for\npre-training, it is needed for testing the hypothesis. We use the KITTI\n(outdoor) and NYU-V2 (indoor) benchmarks to that end, and provide an extensive\ndiscussion of the benefits and limitations of the proposed scheme in relation\nto existing unsupervised, self-supervised, and semi-supervised pre-training\nprotocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lao_D/0/1/0/all/0/1\">Dong Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RSCFed: Random Sampling Consensus Federated Semi-supervised Learning. (arXiv:2203.13993v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13993","description":"<p>Federated semi-supervised learning (FSSL) aims to derive a global model by\ntraining fully-labeled and fully-unlabeled clients or training partially\nlabeled clients. The existing approaches work well when local clients have\nindependent and identically distributed (IID) data but fail to generalize to a\nmore practical FSSL setting, i.e., Non-IID setting. In this paper, we present a\nRandom Sampling Consensus Federated learning, namely RSCFed, by considering the\nuneven reliability among models from fully-labeled clients, fully-unlabeled\nclients or partially labeled clients. Our key motivation is that given models\nwith large deviations from either labeled clients or unlabeled clients, the\nconsensus could be reached by performing random sub-sampling over clients. To\nachieve it, instead of directly aggregating local models, we first distill\nseveral sub-consensus models by random sub-sampling over clients and then\naggregating the sub-consensus models to the global model. To enhance the\nrobustness of sub-consensus models, we also develop a novel distance-reweighted\nmodel aggregation method. Experimental results show that our method outperforms\nstate-of-the-art methods on three benchmarked datasets, including both natural\nand medical images. The code is available at\nhttps://github.com/XMed-Lab/RSCFed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaoxiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Predict RNA Sequence Expressions from Whole Slide Images with Applications for Search and Classification. (arXiv:2203.13997v1 [eess.IV])","link":"http://arxiv.org/abs/2203.13997","description":"<p>Deep learning methods are widely applied in digital pathology to address\nclinical challenges such as prognosis and diagnosis. As one of the most recent\napplications, deep models have also been used to extract molecular features\nfrom whole slide images. Although molecular tests carry rich information, they\nare often expensive, time-consuming, and require additional tissue to sample.\nIn this paper, we propose tRNAsfomer, an attention-based topology that can\nlearn both to predict the bulk RNA-seq from an image and represent the whole\nslide image of a glass slide simultaneously. The tRNAsfomer uses multiple\ninstance learning to solve a weakly supervised problem while the pixel-level\nannotation is not available for an image. We conducted several experiments and\nachieved better performance and faster convergence in comparison to the\nstate-of-the-art algorithms. The proposed tRNAsfomer can assist as a\ncomputational pathology tool to facilitate a new generation of search and\nclassification methods by combining the tissue morphology and the molecular\nfingerprint of the biopsy samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Safarpoor_A/0/1/0/all/0/1\">Amir Safarpoor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hipp_J/0/1/0/all/0/1\">Jason D. Hipp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation with the Reused Teacher Classifier. (arXiv:2203.14001v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14001","description":"<p>Knowledge distillation aims to compress a powerful yet cumbersome teacher\nmodel into a lightweight student model without much sacrifice of performance.\nFor this purpose, various approaches have been proposed over the past few\nyears, generally with elaborately designed knowledge representations, which in\nturn increase the difficulty of model development and interpretation. In\ncontrast, we empirically show that a simple knowledge distillation technique is\nenough to significantly narrow down the teacher-student performance gap. We\ndirectly reuse the discriminative classifier from the pre-trained teacher model\nfor student inference and train a student encoder through feature alignment\nwith a single $\\ell_2$ loss. In this way, the student model is able to achieve\nexactly the same performance as the teacher model provided that their extracted\nfeatures are perfectly aligned. An additional projector is developed to help\nthe student encoder match with the teacher classifier, which renders our\ntechnique applicable to various teacher and student architectures. Extensive\nexperiments demonstrate that our technique achieves state-of-the-art results at\nthe modest cost of compression ratio due to the added projector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jian-Ping Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hailin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Adapt for Monocular Depth Estimation. (arXiv:2203.14005v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14005","description":"<p>Monocular depth estimation is one of the fundamental tasks in environmental\nperception and has achieved tremendous progress in virtue of deep learning.\nHowever, the performance of trained models tends to degrade or deteriorate when\nemployed on other new datasets due to the gap between different datasets.\nThough some methods utilize domain adaptation technologies to jointly train\ndifferent domains and narrow the gap between them, the trained models cannot\ngeneralize to new domains that are not involved in training. To boost the\ntransferability of depth estimation models, we propose an adversarial depth\nestimation task and train the model in the pipeline of meta-learning. Our\nproposed adversarial task mitigates the issue of meta-overfitting, since the\nnetwork is trained in an adversarial manner and aims to extract domain\ninvariant representations. In addition, we propose a constraint to impose upon\ncross-task depth consistency to compel the depth estimation to be identical in\ndifferent adversarial tasks, which improves the performance of our method and\nsmoothens the training process. Experiments demonstrate that our method adapts\nwell to new datasets after few training steps during the test procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_G/0/1/0/all/0/1\">Gary G. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoqiang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EYNet: Extended YOLO for Airport Detection in Remote Sensing Images. (arXiv:2203.14007v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14007","description":"<p>Nowadays, airport detection in remote sensing images has attracted\nconsiderable attention due to its strategic role in civilian and military\nscopes. In particular, uncrewed and operated aerial vehicles must immediately\ndetect safe areas to land in emergencies. The previous schemes suffered from\nvarious aspects, including complicated backgrounds, scales, and shapes of the\nairport. Meanwhile, the rapid action and accuracy of the method are confronted\nwith significant concerns. Hence, this study proposes an effective scheme by\nextending YOLOV3 and ShearLet transform. In this way, MobileNet and ResNet18,\nwith fewer layers and parameters retrained on a similar dataset, are parallelly\ntrained as base networks. According to airport geometrical characteristics, the\nShearLet filters with different scales and directions are considered in the\nfirst convolution layers of ResNet18 as a visual attention mechanism. Besides,\nthe major extended in YOLOV3 concerns the detection Sub-Networks with novel\nstructures which boost object expression ability and training efficiency. In\naddition, novel augmentation and negative mining strategies are presented to\nsignificantly increase the localization phase's performance. The experimental\nresults on the DIOR dataset reveal that the framework reliably detects\ndifferent types of airports in a varied area and acquires robust results in\ncomplex scenes compared to traditional YOLOV3 and state-of-the-art schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirhajianmoghadam_H/0/1/0/all/0/1\">Hengameh Mirhajianmoghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghighi_B/0/1/0/all/0/1\">Behrouz Bolourian Haghighi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGDR: Semantic-guided Disentangled Representation for Unsupervised Cross-modality Medical Image Segmentation. (arXiv:2203.14025v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14025","description":"<p>Disentangled representation is a powerful technique to tackle domain shift\nproblem in medical image analysis in unsupervised domain adaptation\nsetting.However, previous methods only focus on exacting domain-invariant\nfeature and ignore whether exacted feature is meaningful for downstream\ntasks.We propose a novel framework, called semantic-guided disentangled\nrepresentation (SGDR), an effective method to exact semantically meaningful\nfeature for segmentation task to improve performance of cross modality medical\nimage segmentation in unsupervised domain adaptation setting.To exact the\nmeaningful domain-invariant features of different modality, we introduce a\ncontent discriminator to force the content representation to be embedded to the\nsame space and a feature discriminator to exact the meaningful\nrepresentation.We also use pixel-level annotations to guide the encoder to\nlearn features that are meaningful for segmentation task.We validated our\nmethod on two public datasets and experiment results show that our approach\noutperforms the state of the art methods on two evaluation metrics by a\nsignificant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_L/0/1/0/all/0/1\">Li Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medicinal Boxes Recognition on a Deep Transfer Learning Augmented Reality Mobile Application. (arXiv:2203.14031v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14031","description":"<p>Taking medicines is a fundamental aspect to cure illnesses. However, studies\nhave shown that it can be hard for patients to remember the correct posology.\nMore aggravating, a wrong dosage generally causes the disease to worsen.\nAlthough, all relevant instructions for a medicine are summarized in the\ncorresponding patient information leaflet, the latter is generally difficult to\nnavigate and understand. To address this problem and help patients with their\nmedication, in this paper we introduce an augmented reality mobile application\nthat can present to the user important details on the framed medicine. In\nparticular, the app implements an inference engine based on a deep neural\nnetwork, i.e., a densenet, fine-tuned to recognize a medicinal from its\npackage. Subsequently, relevant information, such as posology or a simplified\nleaflet, is overlaid on the camera feed to help a patient when taking a\nmedicine. Extensive experiments to select the best hyperparameters were\nperformed on a dataset specifically collected to address this task; ultimately\nobtaining up to 91.30\\% accuracy as well as real-time capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avola_D/0/1/0/all/0/1\">Danilo Avola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagioli_A/0/1/0/all/0/1\">Alessio Fagioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foresti_G/0/1/0/all/0/1\">Gian Luca Foresti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marini_M/0/1/0/all/0/1\">Marco Raoul Marini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mecca_A/0/1/0/all/0/1\">Alessio Mecca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannone_D/0/1/0/all/0/1\">Daniele Pannone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Abductive Reasoning. (arXiv:2203.14040v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14040","description":"<p>Abductive reasoning seeks the likeliest possible explanation for partial\nobservations. Although abduction is frequently employed in human daily\nreasoning, it is rarely explored in computer vision literature. In this paper,\nwe propose a new task and dataset, Visual Abductive Reasoning (VAR), for\nexamining abductive reasoning ability of machine intelligence in everyday\nvisual situations. Given an incomplete set of visual events, AI systems are\nrequired to not only describe what is observed, but also infer the hypothesis\nthat can best explain the visual premise. Based on our large-scale VAR dataset,\nwe devise a strong baseline model, Reasoner (causal-and-cascaded reasoning\nTransformer). First, to capture the causal structure of the observations, a\ncontextualized directional position embedding strategy is adopted in the\nencoder, that yields discriminative representations for the premise and\nhypothesis. Then, multiple decoders are cascaded to generate and progressively\nrefine the premise and hypothesis sentences. The prediction scores of the\nsentences are used to guide cross-sentence information flow in the cascaded\nreasoning procedure. Our VAR benchmarking results show that Reasoner surpasses\nmany famous video-language models, while still being far behind human\nperformance. This work is expected to foster future efforts in the\nreasoning-beyond-observation paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation by Early Region Proxy. (arXiv:2203.14043v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14043","description":"<p>Typical vision backbones manipulate structured features. As a compromise,\nsemantic segmentation has long been modeled as per-point prediction on dense\nregular grids. In this work, we present a novel and efficient modeling that\nstarts from interpreting the image as a tessellation of learnable regions, each\nof which has flexible geometrics and carries homogeneous semantics. To model\nregion-wise context, we exploit Transformer to encode regions in a\nsequence-to-sequence manner by applying multi-layer self-attention on the\nregion embeddings, which serve as proxies of specific regions. Semantic\nsegmentation is now carried out as per-region prediction on top of the encoded\nregion embeddings using a single linear classifier, where a decoder is no\nlonger needed. The proposed RegProxy model discards the common Cartesian\nfeature layout and operates purely at region level. Hence, it exhibits the most\ncompetitive performance-efficiency trade-off compared with the conventional\ndense prediction methods. For example, on ADE20K, the small-sized RegProxy-S/16\noutperforms the best CNN model using 25% parameters and 4% computation, while\nthe largest RegProxy-L/16 achieves 52.9mIoU which outperforms the\nstate-of-the-art by 2.1% with fewer resources. Codes and models are available\nat https://github.com/YiF-Zhang/RegionProxy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptively Lighting up Facial Expression Crucial Regions via Local Non-Local Joint Network. (arXiv:2203.14045v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14045","description":"<p>Facial expression recognition (FER) is still one challenging research due to\nthe small inter-class discrepancy in the facial expression data. In view of the\nsignificance of facial crucial regions for FER, many existing researches\nutilize the prior information from some annotated crucial points to improve the\nperformance of FER. However, it is complicated and time-consuming to manually\nannotate facial crucial points, especially for vast wild expression images.\nBased on this, a local non-local joint network is proposed to adaptively light\nup the facial crucial regions in feature learning of FER in this paper. In the\nproposed method, two parts are constructed based on facial local and non-local\ninformation respectively, where an ensemble of multiple local networks are\nproposed to extract local features corresponding to multiple facial local\nregions and a non-local attention network is addressed to explore the\nsignificance of each local region. Especially, the attention weights obtained\nby the non-local network is fed into the local part to achieve the interactive\nfeedback between the facial global and local information. Interestingly, the\nnon-local weights corresponding to local regions are gradually updated and\nhigher weights are given to more crucial regions. Moreover, U-Net is employed\nto extract the integrated features of deep semantic information and low\nhierarchical detail information of expression images. Finally, experimental\nresults illustrate that the proposed method achieves more competitive\nperformance compared with several state-of-the art methods on five benchmark\ndatasets. Noticeably, the analyses of the non-local weights corresponding to\nlocal regions demonstrate that the proposed method can automatically enhance\nsome crucial regions in the process of feature learning without any facial\nlandmark information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shasha Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guanghui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_S/0/1/0/all/0/1\">Shuiping Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dandan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Lin Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies. (arXiv:2203.14046v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14046","description":"<p>In the last a few decades, deep neural networks have achieved remarkable\nsuccess in machine learning, computer vision, and pattern recognition. Recent\nstudies however show that neural networks (both shallow and deep) may be easily\nfooled by certain imperceptibly perturbed input samples called adversarial\nexamples. Such security vulnerability has resulted in a large body of research\nin recent years because real-world threats could be introduced due to vast\napplications of neural networks. To address the robustness issue to adversarial\nexamples particularly in pattern recognition, robust adversarial training has\nbecome one mainstream. Various ideas, methods, and applications have boomed in\nthe field. Yet, a deep understanding of adversarial training including\ncharacteristics, interpretations, theories, and connections among different\nmodels has still remained elusive. In this paper, we present a comprehensive\nsurvey trying to offer a systematic and structured investigation on robust\nadversarial training in pattern recognition. We start with fundamentals\nincluding definition, notations, and properties of adversarial examples. We\nthen introduce a unified theoretical framework for defending against\nadversarial samples - robust adversarial training with visualizations and\ninterpretations on why adversarial training can lead to model robustness.\nConnections will be also established between adversarial training and other\ntraditional learning theories. After that, we summarize, review, and discuss\nvarious methodologies with adversarial attack and defense/training algorithms\nin a structured way. Finally, we present analysis, outlook, and remarks of\nadversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhuang Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiu-Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu-Yao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds. (arXiv:2203.14048v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14048","description":"<p>Deep Implicit Function (DIF) has gained popularity as an efficient 3D shape\nrepresentation. To capture geometry details, current methods usually learn DIF\nusing local latent codes, which discretize the space into a regular 3D grid (or\noctree) and store local codes in grid points (or octree nodes). Given a query\npoint, the local feature is computed by interpolating its neighboring local\ncodes with their positions. However, the local codes are constrained at\ndiscrete and regular positions like grid points, which makes the code positions\ndifficult to be optimized and limits their representation ability. To solve\nthis problem, we propose to learn DIF with Dynamic Code Cloud, named DCC-DIF.\nOur method explicitly associates local codes with learnable position vectors,\nand the position vectors are continuous and can be dynamically optimized, which\nimproves the representation ability. In addition, we propose a novel code\nposition loss to optimize the code positions, which heuristically guides more\nlocal codes to be distributed around complex geometric details. In contrast to\nprevious methods, our DCC-DIF represents 3D shapes more efficiently with a\nsmall amount of local codes, and improves the reconstruction quality.\nExperiments demonstrate that DCC-DIF achieves better performance over previous\nmethods. Code and data are available at https://github.com/lity20/DCCDIF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hua Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceVerse: a Fine-grained and Detail-changeable 3D Neural Face Model from a Hybrid Dataset. (arXiv:2203.14057v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14057","description":"<p>We present FaceVerse, a fine-grained 3D Neural Face Model, which is built\nfrom hybrid East Asian face datasets containing 60K fused RGB-D images and 2K\nhigh-fidelity 3D head scan models. A novel coarse-to-fine structure is proposed\nto take better advantage of our hybrid dataset. In the coarse module, we\ngenerate a base parametric model from large-scale RGB-D images, which is able\nto predict accurate rough 3D face models in different genders, ages, etc. Then\nin the fine module, a conditional StyleGAN architecture trained with\nhigh-fidelity scan models is introduced to enrich elaborate facial geometric\nand texture details. Note that different from previous methods, our base and\ndetailed modules are both changeable, which enables an innovative application\nof adjusting both the basic attributes and the facial details of 3D face\nmodels. Furthermore, we propose a single-image fitting framework based on\ndifferentiable rendering. Rich experiments show that our method outperforms the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lizhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture. (arXiv:2203.14065v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14065","description":"<p>Due to the visual ambiguity, purely kinematic formulations on monocular human\nmotion capture are often physically incorrect, biomechanically implausible, and\ncan not reconstruct accurate interactions. In this work, we focus on exploiting\nthe high-precision and non-differentiable physics simulator to incorporate\ndynamical constraints in motion capture. Our key-idea is to use real physical\nsupervisions to train a target pose distribution prior for sampling-based\nmotion control to capture physically plausible human motion. To obtain accurate\nreference motion with terrain interactions for the sampling, we first introduce\nan interaction constraint based on SDF (Signed Distance Field) to enforce\nappropriate ground contact modeling. We then design a novel two-branch decoder\nto avoid stochastic error from pseudo ground-truth and train a distribution\nprior with the non-differentiable physics simulator. Finally, we regress the\nsampling distribution from the current state of the physical character with the\ntrained prior and sample satisfied target poses to track the estimated\nreference motion. Qualitative and quantitative results show that we can obtain\nphysically plausible human motion with complex terrain interactions, human\nshape variations, and diverse behaviors. More information can be found\nat~\\url{https://www.yangangwang.com/papers/HBZ-NM-2022-03.html}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Buzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jingyi Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Answer Questions in Dynamic Audio-Visual Scenarios. (arXiv:2203.14072v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14072","description":"<p>In this paper, we focus on the Audio-Visual Question Answering (AVQA) task,\nwhich aims to answer questions regarding different visual objects, sounds, and\ntheir associations in videos. The problem requires comprehensive multimodal\nunderstanding and spatio-temporal reasoning over audio-visual scenes. To\nbenchmark this task and facilitate our study, we introduce a large-scale\nMUSIC-AVQA dataset, which contains more than 45K question-answer pairs covering\n33 different question templates spanning over different modalities and question\ntypes. We develop several baselines and introduce a spatio-temporal grounded\naudio-visual network for the AVQA problem. Our results demonstrate that AVQA\nbenefits from multisensory perception and our model outperforms recent A-, V-,\nand AVQA approaches. We believe that our built dataset has the potential to\nserve as testbed for evaluating and promoting progress in audio-visual scene\nunderstanding and spatio-temporal reasoning. Code and dataset:\n<a href=\"http://gewu-lab.github.io/MUSIC-AVQA/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yake Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V3GAN: Decomposing Background, Foreground and Motion for Video Generation. (arXiv:2203.14074v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14074","description":"<p>Video generation is a challenging task that requires modeling plausible\nspatial and temporal dynamics in a video. Inspired by how humans perceive a\nvideo by grouping a scene into moving and stationary components, we propose a\nmethod that decomposes the task of video generation into the synthesis of\nforeground, background and motion. Foreground and background together describe\nthe appearance, whereas motion specifies how the foreground moves in a video\nover time. We propose V3GAN, a novel three-branch generative adversarial\nnetwork where two branches model foreground and background information, while\nthe third branch models the temporal information without any supervision. The\nforeground branch is augmented with our novel feature-level masking layer that\naids in learning an accurate mask for foreground and background separation. To\nencourage motion consistency, we further propose a shuffling loss for the video\ndiscriminator. Extensive quantitative and qualitative analysis on synthetic as\nwell as real-world benchmark datasets demonstrates that V3GAN outperforms the\nstate-of-the-art methods by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keshari_A/0/1/0/all/0/1\">Arti Keshari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sukhendu Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Point Cloud Representation Learning with Occlusion Auto-Encoder. (arXiv:2203.14084v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14084","description":"<p>Learning representations for point clouds is an important task in 3D computer\nvision, especially without manually annotated supervision. Previous methods\nusually take the common aid from auto-encoders to establish the\nself-supervision by reconstructing the input itself. However, the existing\nself-reconstruction based auto-encoders merely focus on the global shapes, and\nignore the hierarchical context between the local and global geometries, which\nis a crucial supervision for 3D representation learning. To resolve this issue,\nwe present a novel self-supervised point cloud representation learning\nframework, named 3D Occlusion Auto-Encoder (3D-OAE). Our key idea is to\nrandomly occlude some local patches of the input point cloud and establish the\nsupervision via recovering the occluded patches using the remaining visible\nones. Specifically, we design an encoder for learning the features of visible\nlocal patches, and a decoder for leveraging these features to predict the\noccluded patches. In contrast with previous methods, our 3D-OAE can remove a\nlarge proportion of patches and predict them only with a small number of\nvisible patches, which enable us to significantly accelerate training and yield\na nontrivial self-supervisory performance. The trained encoder can be further\ntransferred to various downstream tasks. We demonstrate our superior\nperformances over the state-of-the-art methods in different discriminant and\ngenerative applications under widely used benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junsheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Near-Infrared Depth-Independent Image Dehazing using Haar Wavelets. (arXiv:2203.14085v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14085","description":"<p>We propose a fusion algorithm for haze removal that combines color\ninformation from an RGB image and edge information extracted from its\ncorresponding NIR image using Haar wavelets. The proposed algorithm is based on\nthe key observation that NIR edge features are more prominent in the hazy\nregions of the image than the RGB edge features in those same regions. To\ncombine the color and edge information, we introduce a haze-weight map which\nproportionately distributes the color and edge information during the fusion\nprocess. Because NIR images are, intrinsically, nearly haze-free, our work\nmakes no assumptions like existing works that rely on a scattering model and\nessentially designing a depth-independent method. This helps in minimizing\nartifacts and gives a more realistic sense to the restored haze-free image.\nExtensive experiments show that the proposed algorithm is both qualitatively\nand quantitatively better on several key metrics when compared to existing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laha_S/0/1/0/all/0/1\">Sumit Laha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ankit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengnan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1\">Hassan Foroosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition. (arXiv:2203.14092v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14092","description":"<p>The physical and textural attributes of objects have been widely studied for\nrecognition, detection and segmentation tasks in computer vision. A number of\ndatasets, such as large scale ImageNet, have been proposed for feature learning\nusing data hungry deep neural networks and for hand-crafted feature extraction.\nTo intelligently interact with objects, robots and intelligent machines need\nthe ability to infer beyond the traditional physical/textural attributes, and\nunderstand/learn visual cues, called visual affordances, for affordance\nrecognition, detection and segmentation. To date there is no publicly available\nlarge dataset for visual affordance understanding and learning. In this paper,\nwe introduce a large scale multi-view RGBD visual affordance learning dataset,\na benchmark of 47210 RGBD images from 37 object categories, annotated with 15\nvisual affordance categories and 35 cluttered/complex scenes with different\nobjects and multiple affordances. To the best of our knowledge, this is the\nfirst ever and the largest multi-view RGBD visual affordance learning dataset.\nWe benchmark the proposed dataset for affordance recognition and segmentation.\nTo achieve this we propose an Affordance Recognition Network a.k.a ARNet. In\naddition, four state-of-the-art deep learning networks are evaluated for\naffordance segmentation task. Our experimental results showcase the challenging\nnature of the dataset and present definite prospects for new and robust\naffordance learning algorithms. The dataset is available at:\nhttps://sites.google.com/view/afaqshah/dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_Z/0/1/0/all/0/1\">Zeyad Osama Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Syed Afaq Ali Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation. (arXiv:2203.14098v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14098","description":"<p>A fundamental and challenging problem in deep learning is catastrophic\nforgetting, i.e. the tendency of neural networks to fail to preserve the\nknowledge acquired from old tasks when learning new tasks. This problem has\nbeen widely investigated in the research community and several Incremental\nLearning (IL) approaches have been proposed in the past years. While earlier\nworks in computer vision have mostly focused on image classification and object\ndetection, more recently some IL approaches for semantic segmentation have been\nintroduced. These previous works showed that, despite its simplicity, knowledge\ndistillation can be effectively employed to alleviate catastrophic forgetting.\nIn this paper, we follow this research direction and, inspired by recent\nliterature on contrastive learning, we propose a novel distillation framework,\nUncertainty-aware Contrastive Distillation (\\method). In a nutshell, \\method~is\noperated by introducing a novel distillation loss that takes into account all\nthe images in a mini-batch, enforcing similarity between features associated to\nall the pixels from the same classes, and pulling apart those corresponding to\npixels from different classes. In order to mitigate catastrophic forgetting, we\ncontrast features of the new model with features extracted by a frozen model\nlearned at the previous incremental step. Our experimental results demonstrate\nthe advantage of the proposed distillation technique, which can be used in\nsynergy with previous IL approaches, and leads to state-of-art performance on\nthree commonly adopted benchmarks for incremental semantic segmentation. The\ncode is available at \\url{https://github.com/ygjwd12345/UCD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanglei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1\">Paolo Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingli Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos. (arXiv:2203.14104v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14104","description":"<p>Action recognition models have shown a promising capability to classify human\nactions in short video clips. In a real scenario, multiple correlated human\nactions commonly occur in particular orders, forming semantically meaningful\nhuman activities. Conventional action recognition approaches focus on analyzing\nsingle actions. However, they fail to fully reason about the contextual\nrelations between adjacent actions, which provide potential temporal logic for\nunderstanding long videos. In this paper, we propose a prompt-based framework,\nBridge-Prompt (Br-Prompt), to model the semantics across adjacent actions, so\nthat it simultaneously exploits both out-of-context and contextual information\nfrom a series of ordinal actions in instructional videos. More specifically, we\nreformulate the individual action labels as integrated text prompts for\nsupervision, which bridge the gap between individual action semantics. The\ngenerated text prompts are paired with corresponding video clips, and together\nco-train the text encoder and the video encoder via a contrastive approach. The\nlearned vision encoder has a stronger capability for ordinal-action-related\ndownstream tasks, e.g. action segmentation and human activity recognition. We\nevaluate the performances of our approach on several video datasets: Georgia\nTech Egocentric Activities (GTEA), 50Salads, and the Breakfast dataset.\nBr-Prompt achieves state-of-the-art on multiple benchmarks. Code is available\nat https://github.com/ttlmh/Bridge-Prompt\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhilan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianjiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Registration for Gaussian Process 3D shape modelling in the presence of extensive missing data. (arXiv:2203.14113v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14113","description":"<p>Gaussian Processes are a powerful tool for shape modelling. While the\nexisting methods on this area prove to work well for the general case of the\nhuman head, when looking at more detailed and deformed data, with a high\nprevalence of missing data, such as the ears, the results are not satisfactory.\nIn order to overcome this, we formulate the shape fitting problem as a\nmulti-annotator Gaussian Process Regression and establish a parallel with the\nstandard probabilistic registration. The achieved method GPReg shows better\nperformance when dealing with extensive areas of missing data when compared to\na state-of-the-art registration method and the current approach for\nregistration with GP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valdeira_F/0/1/0/all/0/1\">Filipa Valdeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_R/0/1/0/all/0/1\">Ricardo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheletti_A/0/1/0/all/0/1\">Alessandra Micheletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1\">Cl&#xe1;udia Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Selective Transformer for Semantic Image Segmentation. (arXiv:2203.14124v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14124","description":"<p>Recently, it has attracted more and more attentions to fuse multi-scale\nfeatures for semantic image segmentation. Various works were proposed to employ\nprogressive local or global fusion, but the feature fusions are not rich enough\nfor modeling multi-scale context features. In this work, we focus on fusing\nmulti-scale features from Transformer-based backbones for semantic\nsegmentation, and propose a Feature Selective Transformer (FeSeFormer), which\naggregates features from all scales (or levels) for each query feature.\nSpecifically, we first propose a Scale-level Feature Selection (SFS) module,\nwhich can choose an informative subset from the whole multi-scale feature set\nfor each scale, where those features that are important for the current scale\n(or level) are selected and the redundant are discarded. Furthermore, we\npropose a Full-scale Feature Fusion (FFF) module, which can adaptively fuse\nfeatures of all scales for queries. Based on the proposed SFS and FFF modules,\nwe develop a Feature Selective Transformer (FeSeFormer), and evaluate our\nFeSeFormer on four challenging semantic segmentation benchmarks, including\nPASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sitong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14128","description":"<p>In the last two years, millions of lives have been lost due to COVID-19.\nDespite the vaccination programmes for a year, hospitalization rates and deaths\nare still high due to the new variants of COVID-19. Stringent guidelines and\nCOVID-19 screening measures such as temperature check and mask check at all\npublic places are helping reduce the spread of COVID-19. Visual inspections to\nensure these screening measures can be taxing and erroneous. Automated\ninspection ensures an effective and accurate screening. Traditional approaches\ninvolve identification of faces and masks from visual camera images followed by\nextraction of temperature values from thermal imaging cameras. Use of visual\nimaging as a primary modality limits these applications only for good-lighting\nconditions. The use of thermal imaging alone for these screening measures makes\nthe system invariant to illumination. However, lack of open source datasets is\nan issue to develop such systems. In this paper, we discuss our work on using\nmachine learning over thermal video streams for face and mask detection and\nsubsequent temperature screening in a passive non-invasive way that enables an\neffective automated COVID-19 screening method in public places. We open source\nour NTIC dataset that was used for training our models and was collected at 8\ndifferent locations. Our results show that the use of thermal imaging is as\neffective as visual imaging in the presence of high illumination. This\nperformance stays the same for thermal images even under low-lighting\nconditions, whereas the performance with visual trained classifiers show more\nthan 50% degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katte_P/0/1/0/all/0/1\">Pratik Katte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakileti_S/0/1/0/all/0/1\">Siva Teja Kakileti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhu_H/0/1/0/all/0/1\">Himanshu J. Madhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_G/0/1/0/all/0/1\">Geetha Manjunath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGBD Object Tracking: An In-depth Review. (arXiv:2203.14134v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14134","description":"<p>RGBD object tracking is gaining momentum in computer vision research thanks\nto the development of depth sensors. Although numerous RGBD trackers have been\nproposed with promising performance, an in-depth review for comprehensive\nunderstanding of this area is lacking. In this paper, we firstly review RGBD\nobject trackers from different perspectives, including RGBD fusion, depth\nusage, and tracking framework. Then, we summarize the existing datasets and the\nevaluation metrics. We benchmark a representative set of RGBD trackers, and\ngive detailed analyses based on their performances. Particularly, we are the\nfirst to provide depth quality evaluation and analysis of tracking results in\ndepth-friendly scenarios in RGBD tracking. For long-term settings in most RGBD\ntracking videos, we give an analysis of trackers' performance on handling\ntarget disappearance. To enable better understanding of RGBD trackers, we\npropose robustness evaluation against input perturbations. Finally, we\nsummarize the challenges and provide open directions for this community. All\nresources are publicly available at\nhttps://github.com/memoryunreal/RGBD-tracking-review.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Song Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamarainen_J/0/1/0/all/0/1\">Joni-Kristian K&#xe4;m&#xe4;r&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reverse Engineering of Imperceptible Adversarial Image Perturbations. (arXiv:2203.14145v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14145","description":"<p>It has been well recognized that neural network based image classifiers are\neasily fooled by images with tiny perturbations crafted by an adversary. There\nhas been a vast volume of research to generate and defend such adversarial\nattacks. However, the following problem is left unexplored: How to\nreverse-engineer adversarial perturbations from an adversarial image? This\nleads to a new adversarial learning paradigm--Reverse Engineering of Deceptions\n(RED). If successful, RED allows us to estimate adversarial perturbations and\nrecover the original images. However, carefully crafted, tiny adversarial\nperturbations are difficult to recover by optimizing a unilateral RED\nobjective. For example, the pure image denoising method may overfit to\nminimizing the reconstruction error but hardly preserve the classification\nproperties of the true adversarial perturbations. To tackle this challenge, we\nformalize the RED problem and identify a set of principles crucial to the RED\napproach design. Particularly, we find that prediction alignment and proper\ndata augmentation (in terms of spatial transformations) are two criteria to\nachieve a generalizable RED approach. By integrating these RED principles with\nimage denoising, we propose a new Class-Discriminative Denoising based RED\nframework, termed CDD-RED. Extensive experiments demonstrate the effectiveness\nof CDD-RED under different evaluation metrics (ranging from the pixel-level,\nprediction-level to the attribution-level alignment) and a variety of attack\ngeneration methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuguang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yimeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image Matching. (arXiv:2203.14148v1 [cs.CV])","link":"http://arxiv.org/abs/2203.14148","description":"<p>We address the problem of ground-to-satellite image geo-localization, that\nis, estimating the camera latitude, longitude and orientation (azimuth angle)\nby matching a query image captured at the ground level against a large-scale\ndatabase with geotagged satellite images. Our prior arts treat the above task\nas pure image retrieval by selecting the most similar satellite reference image\nmatching the ground-level query image. However, such an approach often produces\ncoarse location estimates because the geotag of the retrieved satellite image\nonly corresponds to the image center while the ground camera can be located at\nany point within the image. To further consolidate our prior research findings,\nwe present a novel geometry-aware geo-localization method. Our new method is\nable to achieve the fine-grained location of a query image, up to pixel size\nprecision of the satellite image, once its coarse location and orientation have\nbeen determined. Moreover, we propose a new geometry-aware image retrieval\npipeline to improve the coarse localization accuracy. Apart from a polar\ntransform in our conference work, this new pipeline also maps satellite image\npixels to the ground-level plane in the ground-view via a geometry-constrained\nprojective transform to emphasize informative regions, such as road structures,\nfor cross-view geo-localization. Extensive quantitative and qualitative\nexperiments demonstrate the effectiveness of our newly proposed framework. We\nalso significantly improve the performance of coarse localization results\ncompared to the state-of-the-art in terms of location recalls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujiao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Optimization Method for Label Noisy Datasets Based on Adaptive Threshold: Adaptive-k. (arXiv:2203.14165v1 [cs.LG])","link":"http://arxiv.org/abs/2203.14165","description":"<p>SGD does not produce robust results on datasets with label noise. Because the\ngradients calculated according to the losses of the noisy samples cause the\noptimization process to go in the wrong direction. In this paper, as an\nalternative to SGD, we recommend using samples with loss less than a threshold\nvalue determined during the optimization process, instead of using all samples\nin the mini-batch. Our proposed method, Adaptive-k, aims to exclude label noise\nsamples from the optimization process and make the process robust. On noisy\ndatasets, we found that using a threshold-based approach, such as Adaptive-k,\nproduces better results than using all samples or a fixed number of low-loss\nsamples in the mini-batch. Based on our theoretical analysis and experimental\nresults, we show that the Adaptive-k method is closest to the performance of\nthe oracle, in which noisy samples are entirely removed from the dataset.\nAdaptive-k is a simple but effective method. It does not require prior\nknowledge of the noise ratio of the dataset, does not require additional model\ntraining, and does not increase training time significantly. The code for\nAdaptive-k is available at https://github.com/enesdedeoglu-TR/Adaptive-k\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dedeoglu_E/0/1/0/all/0/1\">Enes Dedeoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1\">Himmet Toprak Kesgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1\">Mehmet Fatih Amasyali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThunderNet: Towards Real-time Generic Object Detection. (arXiv:1903.11752v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1903.11752","description":"<p>Real-time generic object detection on mobile platforms is a crucial but\nchallenging computer vision task. However, previous CNN-based detectors suffer\nfrom enormous computational cost, which hinders them from real-time inference\nin computation-constrained scenarios. In this paper, we investigate the\neffectiveness of two-stage detectors in real-time generic detection and propose\na lightweight two-stage detector named ThunderNet. In the backbone part, we\nanalyze the drawbacks in previous lightweight backbones and present a\nlightweight backbone designed for object detection. In the detection part, we\nexploit an extremely efficient RPN and detection head design. To generate more\ndiscriminative feature representation, we design two efficient architecture\nblocks, Context Enhancement Module and Spatial Attention Module. At last, we\ninvestigate the balance between the input resolution, the backbone, and the\ndetection head. Compared with lightweight one-stage detectors, ThunderNet\nachieves superior performance with only 40% of the computational cost on PASCAL\nVOC and COCO benchmarks. Without bells and whistles, our model runs at 24.1 fps\non an ARM-based device. To the best of our knowledge, this is the first\nreal-time detector reported on ARM platforms. Our code and models are available\nat \\url{https://github.com/qinzheng93/ThunderNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zheng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yiping Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuxing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Low-light Image Enhancement with Decoupled Networks. (arXiv:2005.02818v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2005.02818","description":"<p>In this paper, we tackle the problem of enhancing real-world low-light images\nwith significant noise in an unsupervised fashion. Conventional unsupervised\nlearning-based approaches usually tackle the low-light image enhancement\nproblem using an image-to-image translation model. They focus primarily on\nillumination or contrast enhancement but fail to suppress the noise that\nubiquitously exists in images taken under real-world low-light conditions. To\naddress this issue, we explicitly decouple this task into two sub-tasks:\nillumination enhancement and noise suppression. We propose to learn a two-stage\nGAN-based framework to enhance the real-world low-light images in a fully\nunsupervised fashion. To facilitate the unsupervised training of our model, we\nconstruct samples with pseudo labels. Furthermore, we propose an adaptive\ncontent loss to suppress real image noise in different regions based on\nillumination intensity. In addition to conventional benchmark datasets, a new\nunpaired low-light image enhancement dataset is built and used to thoroughly\nevaluate the performance of our model. Extensive experiments show that our\nproposed method outperforms the state-of-the-art unsupervised image enhancement\nmethods in terms of both illumination enhancement and noise reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xiong_W/0/1/0/all/0/1\">Wei Xiong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Ding Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_X/0/1/0/all/0/1\">Xiaohui Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_C/0/1/0/all/0/1\">Chen Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection. (arXiv:2008.06254v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.06254","description":"<p>We consider the problem of Human-Object Interaction (HOI) Detection, which\naims to locate and recognize HOI instances in the form of &lt;human, action,\nobject&gt; in images. Most existing works treat HOIs as individual interaction\ncategories, thus can not handle the problem of long-tail distribution and\npolysemy of action labels. We argue that multi-level consistencies among\nobjects, actions and interactions are strong cues for generating semantic\nrepresentations of rare or previously unseen HOIs. Leveraging the compositional\nand relational peculiarities of HOI labels, we propose ConsNet, a\nknowledge-aware framework that explicitly encodes the relations among objects,\nactions and interactions into an undirected graph called consistency graph, and\nexploits Graph Attention Networks (GATs) to propagate knowledge among HOI\ncategories as well as their constituents. Our model takes visual features of\ncandidate human-object pairs and word embeddings of HOI labels as inputs, maps\nthem into visual-semantic joint embedding space and obtains detection results\nby measuring their similarities. We extensively evaluate our model on the\nchallenging V-COCO and HICO-DET datasets, and results validate that our\napproach outperforms state-of-the-arts under both fully-supervised and\nzero-shot settings. Code is available at https://github.com/yeliudev/ConsNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Compress Videos without Computing Motion. (arXiv:2009.14110v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2009.14110","description":"<p>With the development of higher resolution contents and displays, its\nsignificant volume poses significant challenges to the goals of acquiring,\ntransmitting, compressing, and displaying high-quality video content. In this\npaper, we propose a new deep learning video compression architecture that does\nnot require motion estimation, which is the most expensive element of modern\nhybrid video compression codecs like H.264 and HEVC. Our framework exploits the\nregularities inherent to video motion, which we capture by using displaced\nframe differences as video representations to train the neural network. In\naddition, we propose a new space-time reconstruction network based on both an\nLSTM model and a UNet model, which we call LSTM-UNet. The new video compression\nframework has three components: a Displacement Calculation Unit (DCU), a\nDisplacement Compression Network (DCN), and a Frame Reconstruction Network\n(FRN). The DCU removes the need for motion estimation found in hybrid codecs\nand is less expensive. In the DCN, an RNN-based network is utilized to compress\ndisplaced frame differences as well as retain temporal information between\nframes. The LSTM-UNet is used in the FRN to learn space-time differential\nrepresentations of videos. Our experimental results show that our compression\nmodel, which we call the MOtionless VIdeo Codec (MOVI-Codec), learns how to\nefficiently compress videos without computing motion. Our experiments show that\nMOVI-Codec outperforms the Low-Delay P veryfast setting of the video coding\nstandard H.264 and exceeds the performance of the modern global standard HEVC\ncodec, using the same setting, as measured by MS-SSIM, especially on higher\nresolution videos. In addition, our network outperforms the latest H.266 (VVC)\ncodec at higher bitrates, when assessed using MS-SSIM, on high-resolution\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Meixu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goodall_T/0/1/0/all/0/1\">Todd Goodall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patney_A/0/1/0/all/0/1\">Anjul Patney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Representations of Positive Functions via First and Second-Order Pseudo-Mirror Descent. (arXiv:2011.07142v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2011.07142","description":"<p>We consider expected risk minimization problems when the range of the\nestimator is required to be nonnegative, motivated by the settings of maximum\nlikelihood estimation (MLE) and trajectory optimization. To facilitate\nnonlinear interpolation, we hypothesize that the search space is a Reproducing\nKernel Hilbert Space (RKHS). We develop first and second-order variants of\nstochastic mirror descent employing (i) \\emph{pseudo-gradients} and (ii)\ncomplexity-reducing projections. Compressive projection in the first-order\nscheme is executed via kernel orthogonal matching pursuit (KOMP), which\novercomes the fact that the vanilla RKHS parameterization grows unbounded with\nthe iteration index in the stochastic setting. Moreover, pseudo-gradients are\nneeded when gradient estimates for cost are only computable up to some\nnumerical error, which arise in, e.g., integral approximations. Under constant\nstep-size and compression budget, we establish tradeoffs between the radius of\nconvergence of the expected sub-optimality and the projection budget parameter,\nas well as non-asymptotic bounds on the model complexity. To refine the\nsolution's precision, we develop a second-order extension which employs\nrecursively averaged pseudo-gradient outer-products to approximate the Hessian\ninverse, whose convergence in mean is established under an additional\neigenvalue decay condition on the Hessian of the optimal RKHS element, which is\nunique to this work. Experiments demonstrate favorable performance on\ninhomogeneous Poisson Process intensity estimation in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Chakraborty_A/0/1/0/all/0/1\">Abhishek Chakraborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Segmentation for Terracotta Warrior Point Cloud (SRG-Net). (arXiv:2012.00433v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.00433","description":"<p>The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum\nSite Museum is handcrafted by experts, and the increasing amounts of unearthed\npieces of terracotta warriors make the archaeologists too challenging to\nconduct the restoration of terracotta warriors efficiently. We hope to segment\nthe 3D point cloud data of the terracotta warriors automatically and store the\nfragment data in the database to assist the archaeologists in matching the\nactual fragments with the ones in the database, which could result in higher\nrepairing efficiency of terracotta warriors. Moreover, the existing 3D neural\nnetwork research is mainly focusing on supervised classification, clustering,\nunsupervised representation, and reconstruction. There are few pieces of\nresearches concentrating on unsupervised point cloud part segmentation. In this\npaper, we present SRG-Net for 3D point clouds of terracotta warriors to address\nthese problems. Firstly, we adopt a customized seed-region-growing algorithm to\nsegment the point cloud coarsely. Then we present a supervised segmentation and\nunsupervised reconstruction networks to learn the characteristics of 3D point\nclouds. Finally, we combine the SRG algorithm with our improved CNN(convolution\nneural network) using a refinement method. This pipeline is called SRG-Net,\nwhich aims at conducting segmentation tasks on the terracotta warriors. Our\nproposed SRG-Net is evaluated on the terracotta warrior data and ShapeNet\ndataset by measuring the accuracy and the latency. The experimental results\nshow that our SRG-Net outperforms the state-of-the-art methods. Our code is\navailable at https://github.com/hyoau/SRG-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_G/0/1/0/all/0/1\">Guohua Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01699","description":"<p>Preprocessing defenses such as pixel discretization are appealing to remove\nadversarial attacks due to their simplicity. However, they have been shown to\nbe ineffective except on simple datasets such as MNIST. We hypothesize that\nexisting discretization approaches failed because using a fixed codebook for\nthe entire dataset limits their ability to balance image representation and\ncodeword separability. We propose a per-image adaptive preprocessing defense\ncalled Essential Features, which first applies adaptive blurring to push\nperturbed pixel values back to their original value and then discretizes the\nimage to an image-adaptive codebook to reduce the color space. Essential\nFeatures thus constrains the attack space by forcing the adversary to perturb\nlarge regions both locally and color-wise for its effects to survive the\npreprocessing. Against adaptive attacks, we find that our approach increases\nthe $L_2$ and $L_\\infty$ robustness on higher resolution datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wu-chi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are DNNs fooled by extremely unrecognizable images?. (arXiv:2012.03843v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03843","description":"<p>Fooling images are a potential threat to deep neural networks (DNNs). These\nimages are not recognizable to humans as natural objects, such as dogs and\ncats, but are misclassified by DNNs as natural-object classes with high\nconfidence scores. Despite their original design concept, existing fooling\nimages retain some features that are characteristic of the target objects if\nlooked into closely. Hence, DNNs can react to these features. In this paper, we\naddress the question of whether there can be fooling images with no\ncharacteristic pattern of natural objects locally or globally. As a minimal\ncase, we introduce single-color images with a few pixels altered, called sparse\nfooling images (SFIs). We first prove that SFIs always exist under mild\nconditions for linear and nonlinear models and reveal that complex models are\nmore likely to be vulnerable to SFI attacks. With two SFI generation methods,\nwe demonstrate that in deeper layers, SFIs end up with similar features to\nthose of natural images, and consequently, fool DNNs successfully. Among other\nlayers, we discovered that the max pooling layer causes the vulnerability\nagainst SFIs. The defense against SFIs and transferability are also discussed.\nThis study highlights the new vulnerability of DNNs by introducing a novel\nclass of images that distributes extremely far from natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumano_S/0/1/0/all/0/1\">Soichiro Kumano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kera_H/0/1/0/all/0/1\">Hiroshi Kera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamasaki_T/0/1/0/all/0/1\">Toshihiko Yamasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04631","description":"<p>Machine translation between many languages at once is highly challenging,\nsince training with ground truth requires supervision between all language\npairs, which is difficult to obtain. Our key insight is that, while languages\nmay vary drastically, the underlying visual appearance of the world remains\nconsistent. We introduce a method that uses visual observations to bridge the\ngap between languages, rather than relying on parallel corpora or topological\nproperties of the representations. We train a model that aligns segments of\ntext from different languages if and only if the images associated with them\nare similar and each image in turn is well-aligned with its textual\ndescription. We train our model from scratch on a new dataset of text in over\nfifty languages with accompanying images. Experiments show that our method\noutperforms previous work on unsupervised word and sentence translation using\nretrieval. Code, models and data are available on globetrotter.cs.columbia.edu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1\">D&#xed;dac Sur&#xed;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Handheld to Unconstrained Object Detection: a Weakly-supervised On-line Learning Approach. (arXiv:2012.14345v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14345","description":"<p>Deep Learning (DL) based methods for object detection achieve remarkable\nperformance at the cost of computationally expensive training and extensive\ndata labeling. Robots embodiment can be exploited to mitigate this burden by\nacquiring automatically annotated training data via a natural interaction with\na human showing the object of interest, handheld. However, learning solely from\nthis data may introduce biases (the so-called domain shift), and prevents\nadaptation to novel tasks. While Weakly-supervised Learning (WSL) offers a\nwell-established set of techniques to cope with these problems in\ngeneral-purpose Computer Vision, its adoption in challenging robotic domains is\nstill at a preliminary stage. In this work, we target the scenario of a robot\ntrained in a teacher-learner setting to detect handheld objects. The aim is to\nimprove detection performance in different settings by letting the robot\nexplore the environment with a limited human labeling budget. We compare\nseveral techniques for WSL in detection pipelines to reduce model re-training\ncosts without compromising accuracy, proposing solutions which target the\nconsidered robotic scenario. We show that the robot can improve adaptation to\nnovel domains, either by interacting with a human teacher (Active Learning) or\nwith an autonomous supervision (Semi-supervised Learning). We integrate our\nstrategies into an on-line detection method, achieving efficient model update\ncapabilities with few labels. We experimentally benchmark our method on\nchallenging robotic object detection tasks under domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiettini_E/0/1/0/all/0/1\">Elisa Maiettini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maracani_A/0/1/0/all/0/1\">Andrea Maracani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camoriano_R/0/1/0/all/0/1\">Raffaello Camoriano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquale_G/0/1/0/all/0/1\">Giulia Pasquale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhanoff_V/0/1/0/all/0/1\">Vadim Tikhanoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosasco_L/0/1/0/all/0/1\">Lorenzo Rosasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1\">Lorenzo Natale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05260","description":"<p>In cases of serious crime, including sexual abuse, often the only available\ninformation with demonstrated potential for identification is images of the\nhands. Since this evidence is captured in uncontrolled situations, it is\ndifficult to analyse. As global approaches to feature comparison are limited in\nthis case, it is important to extend to consider local information. In this\nwork, we propose hand-based person identification by learning both global and\nlocal deep feature representations. Our proposed method, Global and Part-Aware\nNetwork (GPA-Net), creates global and local branches on the conv-layer for\nlearning robust discriminative global and part-level features. For learning the\nlocal (part-level) features, we perform uniform partitioning on the conv-layer\nin both horizontal and vertical directions. We retrieve the parts by conducting\na soft partition without explicitly partitioning the images or requiring\nexternal cues such as pose estimation. We make extensive evaluations on two\nlarge multi-ethnic and publicly available hand datasets, demonstrating that our\nproposed method significantly outperforms competing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery. (arXiv:2103.01623v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01623","description":"<p>This paper presents a new approach for synthesizing a novel street-view\npanorama given an overhead satellite image. Taking a small satellite image\npatch as input, our method generates a Google's omnidirectional street-view\ntype panorama, as if it is captured from the same geographical location as the\ncenter of the satellite patch. Existing works tackle this task as an image\ngeneration problem which adopts generative adversarial networks to implicitly\nlearn the cross-view transformations, while ignoring the domain relevance. In\nthis paper, we propose to explicitly establish the geometric correspondences\nbetween the two-view images so as to facilitate the cross-view transformation\nlearning. Specifically, we observe that when a 3D point in the real world is\nvisible in both views, there is a deterministic mapping between the projected\npoints in the two-view images given the height information of this 3D point.\nMotivated by this, we develop a novel Satellite to Street-view image Projection\n(S2SP) module which explicitly establishes such geometric correspondences and\nprojects the satellite images to the street viewpoint. With these projected\nsatellite images as network input, we next employ a generator to synthesize\nrealistic street-view panoramas that are geometrically consistent with the\nsatellite images. Our S2SP module is differentiable and the whole framework is\ntrained in an end-to-end manner. Extensive experimental results on two\ncross-view benchmark datasets demonstrate that our method generates images that\nbetter respect the scene geometry than existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujiao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. (arXiv:2103.04922v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.04922","description":"<p>Deep generative models are a class of techniques that train deep neural\nnetworks to model the distribution of training samples. Research has fragmented\ninto various interconnected approaches, each of which make trade-offs including\nrun-time, diversity, and architectural restrictions. In particular, this\ncompendium covers energy-based models, variational autoencoders, generative\nadversarial networks, autoregressive models, normalizing flows, in addition to\nnumerous hybrid approaches. These techniques are compared and contrasted,\nexplaining the premises behind each and how they are interrelated, while\nreviewing current state-of-the-art advances and implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bond_Taylor_S/0/1/0/all/0/1\">Sam Bond-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_A/0/1/0/all/0/1\">Adam Leach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stepwise Goal-Driven Networks for Trajectory Prediction. (arXiv:2103.14107v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14107","description":"<p>We propose to predict the future trajectories of observed agents (e.g.,\npedestrians or vehicles) by estimating and using their goals at multiple time\nscales. We argue that the goal of a moving agent may change over time, and\nmodeling goals continuously provides more accurate and detailed information for\nfuture trajectory estimation. To this end, we present a recurrent network for\ntrajectory prediction, called Stepwise Goal-Driven Network (SGNet). Unlike\nprior work that models only a single, long-term goal, SGNet estimates and uses\ngoals at multiple temporal scales. In particular, it incorporates an encoder\nthat captures historical information, a stepwise goal estimator that predicts\nsuccessive goals into the future, and a decoder that predicts future\ntrajectory. We evaluate our model on three first-person traffic datasets\n(HEV-I, JAAD, and PIE) as well as on three bird's eye view datasets (NuScenes,\nETH, and UCY), and show that our model achieves state-of-the-art results on all\ndatasets. Code has been made available at:\nhttps://github.com/ChuhuaW/SGNet.pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David J. Crandall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignMixup: Improving Representations By Interpolating Aligned Features. (arXiv:2103.15375v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15375","description":"<p>Mixup is a powerful data augmentation method that interpolates between two or\nmore examples in the input or feature space and between the corresponding\ntarget labels. Many recent mixup methods focus on cutting and pasting two or\nmore objects into one image, which is more about efficient processing than\ninterpolation. However, how to best interpolate images is not well defined. In\nthis sense, mixup has been connected to autoencoders, because often\nautoencoders \"interpolate well\", for instance generating an image that\ncontinuously deforms into another.\n</p>\n<p>In this work, we revisit mixup from the interpolation perspective and\nintroduce AlignMix, where we geometrically align two images in the feature\nspace. The correspondences allow us to interpolate between two sets of\nfeatures, while keeping the locations of one set. Interestingly, this gives\nrise to a situation where mixup retains mostly the geometry or pose of one\nimage and the texture of the other, connecting it to style transfer. More than\nthat, we show that an autoencoder can still improve representation learning\nunder mixup, without the classifier ever seeing decoded images. AlignMix\noutperforms state-of-the-art mixup methods on five different benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_S/0/1/0/all/0/1\">Shashanka Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1\">Ewa Kijak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1\">Laurent Amsaleg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation. (arXiv:2103.17235v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17235","description":"<p>The increase of available large clinical and experimental datasets has\ncontributed to a substantial amount of important contributions in the area of\nbiomedical image analysis. Image segmentation, which is crucial for any\nquantitative analysis, has especially attracted attention. Recent hardware\nadvancement has led to the success of deep learning approaches. However,\nalthough deep learning models are being trained on large datasets, existing\nmethods do not use the information from different learning epochs effectively.\nIn this work, we leverage the information of each training epoch to prune the\nprediction maps of the subsequent epochs. We propose a novel architecture\ncalled feedback attention network (FANet) that unifies the previous epoch mask\nwith the feature map of the current training epoch. The previous epoch mask is\nthen used to provide a hard attention to the learned feature maps at different\nconvolutional layers. The network also allows to rectify the predictions in an\niterative fashion during the test time. We show that our proposed\n\\textit{feedback attention} model provides a substantial improvement on most\nsegmentation metrics tested on seven publicly available biomedical imaging\ndatasets demonstrating the effectiveness of FANet. The source code is available\nat \\url{https://github.com/nikhilroxtomar/FANet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomar_N/0/1/0/all/0/1\">Nikhil Kumar Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_H/0/1/0/all/0/1\">H&#xe5;vard D. Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_D/0/1/0/all/0/1\">Dag Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes. (arXiv:2104.07300v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07300","description":"<p>We consider the problem of recovering a single person's 3D human mesh from\nin-the-wild crowded scenes. While much progress has been in 3D human mesh\nestimation, existing methods struggle when test input has crowded scenes. The\nfirst reason for the failure is a domain gap between training and testing data.\nA motion capture dataset, which provides accurate 3D labels for training, lacks\ncrowd data and impedes a network from learning crowded scene-robust image\nfeatures of a target person. The second reason is a feature processing that\nspatially averages the feature map of a localized bounding box containing\nmultiple people. Averaging the whole feature map makes a target person's\nfeature indistinguishable from others. We present 3DCrowdNet that firstly\nexplicitly targets in-the-wild crowded scenes and estimates a robust 3D human\nmesh by addressing the above issues. First, we leverage 2D human pose\nestimation that does not require a motion capture dataset with 3D labels for\ntraining and does not suffer from the domain gap. Second, we propose a\njoint-based regressor that distinguishes a target person's feature from others.\nOur joint-based regressor preserves the spatial activation of a target by\nsampling features from the target's joint locations and regresses human model\nparameters. As a result, 3DCrowdNet learns target-focused features and\neffectively excludes the irrelevant features of nearby persons. We conduct\nexperiments on various benchmarks and prove the robustness of 3DCrowdNet to the\nin-the-wild crowded scenes both quantitatively and qualitatively. Codes are\navailable in https://github.com/hongsukchoi/3DCrowdNet_RELEASE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hongsuk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_G/0/1/0/all/0/1\">Gyeongsik Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">JoonKyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion. (arXiv:2104.07661v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07661","description":"<p>This paper studies the problem of StyleGAN inversion, which plays an\nessential role in enabling the pretrained StyleGAN to be used for real image\nediting tasks. The goal of StyleGAN inversion is to find the exact latent code\nof the given image in the latent space of StyleGAN. This problem has a high\ndemand for quality and efficiency. Existing optimization-based methods can\nproduce high-quality results, but the optimization often takes a long time. On\nthe contrary, forward-based methods are usually faster but the quality of their\nresults is inferior. In this paper, we present a new feed-forward network\n\"E2Style\" for StyleGAN inversion, with significant improvement in terms of\nefficiency and effectiveness. In our inversion network, we introduce: 1) a\nshallower backbone with multiple efficient heads across scales; 2) multi-layer\nidentity loss and multi-layer face parsing loss to the loss function; and 3)\nmulti-stage refinement. Combining these designs together forms an effective and\nefficient method that exploits all benefits of optimization-based and\nforward-based methods. Quantitative and qualitative results show that our\nE2Style performs better than existing forward-based methods and comparably to\nstate-of-the-art optimization-based methods while maintaining the high\nefficiency as well as forward-based methods. Moreover, a number of real image\nediting applications demonstrate the efficacy of our E2Style. Our code is\navailable at \\url{https://github.com/wty-ustc/e2style}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianyi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09008","description":"<p>In the paper of ExquisiteNetV1, the ability of classification of\nExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and\nbetter model ExquisiteNetV2. We conduct many experiments to evaluate its\nperformance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known\nmodels on 15 credible datasets under the same condition. According to the\nexperimental results, ExquisiteNetV2 gets the highest classification accuracy\nover half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts\nof parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing\nspeed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shi-Yao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chung-Yen Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smaller Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning. (arXiv:2105.14106v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14106","description":"<p>The design of machines and algorithms capable of learning in a dynamically\nchanging environment has become an increasingly topical problem with the\nincrease of the size and heterogeneity of data available to learning systems.\nAs a consequence, the key issue of Continual Learning has become that of\naddressing the stability-plasticity dilemma of connectionist systems, as they\nneed to adapt their model without forgetting previously acquired knowledge.\nWithin this context, rehearsal-based methods i.e., solutions in where the\nlearner exploits memory to revisit past data, has proven to be very effective,\nleading to performance at the state-of-the-art. In our study, we propose an\nanalysis of the memory quantity/quality trade-off adopting various data\nreduction approaches to increase the number of instances storable in memory. In\nparticular, we investigate complex instance compression techniques such as deep\nencoders, but also trivial approaches such as image resizing and linear\ndimensionality reduction. Our findings suggest that the optimal trade-off is\nseverely skewed toward instance quantity, where rehearsal approaches with\nseveral heavily compressed instances easily outperform state-of-the-art\napproaches with the same amount of memory at their disposal. Further, in high\nmemory configurations, deep approaches extracting spatial structure combined\nwith extreme resizing (of the order of $8\\times8$ images) yield the best\nresults, while in memory-constrained configurations where deep approaches\ncannot be used due to their memory requirement in training, Extreme Learning\nMachines (ELM) offer a clear advantage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1\">Francesco Pelosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1\">Andrea Torsello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Multi-View Object Recognition and Grasping in Open-Ended Domains. (arXiv:2106.01866v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2106.01866","description":"<p>A robot working in human-centric environments needs to know which kind of\nobjects exist in the scene, where they are, and how to grasp and manipulate\nvarious objects in different situations to help humans in everyday tasks.\nTherefore, object recognition and grasping are two key functionalities for such\nrobots. Most state-of-the-art tackles object recognition and grasping as two\nseparate problems while both use visual input. Furthermore, the knowledge of\nthe robot is fixed after the training phase. In such cases, if the robot faces\nnew object categories, it must retrain from scratch to incorporate new\ninformation without catastrophic interference. To address this problem, we\npropose a deep learning architecture with augmented memory capacities to handle\nopen-ended object recognition and grasping simultaneously. In particular, our\napproach takes multi-views of an object as input and jointly estimates\npixel-wise grasp configuration as well as a deep scale- and rotation-invariant\nrepresentation as outputs. The obtained representation is then used for\nopen-ended object recognition through a meta-active learning technique. We\ndemonstrate the ability of our approach to grasp never-seen-before objects and\nto rapidly learn new object categories using very few examples on-site in both\nsimulation and real-world settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1\">Hamidreza Kasaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Sha Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasso_R/0/1/0/all/0/1\">Remo Sasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_M/0/1/0/all/0/1\">Mohammadreza Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering. (arXiv:2106.03798v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03798","description":"<p>We introduce DoubleField, a novel framework combining the merits of both\nsurface field and radiance field for high-fidelity human reconstruction and\nrendering. Within DoubleField, the surface field and radiance field are\nassociated together by a shared feature embedding and a surface-guided sampling\nstrategy. Moreover, a view-to-view transformer is introduced to fuse multi-view\nfeatures and learn view-dependent features directly from high-resolution\ninputs. With the modeling power of DoubleField and the view-to-view\ntransformer, our method significantly improves the reconstruction quality of\nboth geometry and appearance, while supporting direct inference, scene-specific\nhigh-resolution finetuning, and fast rendering. The efficacy of DoubleField is\nvalidated by the quantitative evaluations on several datasets and the\nqualitative results in a real-world sparse multi-view system, showing its\nsuperior capability for high-quality human model reconstruction and\nphoto-realistic free-viewpoint human rendering. Data and source code will be\nmade public for the research purpose. Please refer to our project page:\n<a href=\"http://www.liuyebin.com/dbfield/dbfield.html.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruizhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingjia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanpei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Sparse R-CNN for Direct Scene Graph Generation. (arXiv:2106.10815v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10815","description":"<p>Scene graph generation (SGG) is to detect object pairs with their relations\nin an image. Existing SGG approaches often use multi-stage pipelines to\ndecompose this task into object detection, relation graph construction, and\ndense or dense-to-sparse relation prediction. Instead, from a perspective on\nSGG as a direct set prediction, this paper presents a simple, sparse, and\nunified framework, termed as Structured Sparse R-CNN. The key to our method is\na set of learnable triplet queries and a structured triplet detector which\ncould be jointly optimized from the training set in an end-to-end manner.\nSpecifically, the triplet queries encode the general prior for object pairs\nwith their relations, and provide an initial guess of scene graphs for\nsubsequent refinement. The triplet detector presents a cascaded architecture to\nprogressively refine the detected scene graphs with the customized dynamic\nheads. In addition, to relieve the training difficulty of our method, we\npropose a relaxed and enhanced training strategy based on knowledge\ndistillation from a Siamese Sparse R-CNN. We perform experiments on several\ndatasets: Visual Genome and Open Images V4/V6, and the results demonstrate that\nour method achieves the state-of-the-art performance. In addition, we also\nperform in-depth ablation studies to provide insights on our structured\nmodeling in triplet detector design and training strategies. The code and\nmodels are made available at https://github.com/MCG-NJU/Structured-Sparse-RCNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yao Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12226","description":"<p>Cloud removal is a relevant topic in Remote Sensing as it fosters the\nusability of high-resolution optical images for Earth monitoring and study.\nRelated techniques have been analyzed for years with a progressively clearer\nview of the appropriate methods to adopt, from multi-spectral to inpainting\nmethods. Recent applications of deep generative models and\nsequence-to-sequence-based models have proved their capability to advance the\nfield significantly. Nevertheless, there are still some gaps, mostly related to\nthe amount of cloud coverage, the density and thickness of clouds, and the\noccurred temporal landscape changes. In this work, we fill some of these gaps\nby introducing a novel multi-modal method that uses different sources of\ninformation, both spatial and temporal, to restore the whole optical scene of\ninterest. The proposed method introduces an innovative deep model, using the\noutcomes of both temporal-sequence blending and direct translation from\nSynthetic Aperture Radar (SAR) to optical images to obtain a pixel-wise\nrestoration of the whole scene. The advantage of our approach is demonstrated\nacross a variety of atmospheric conditions tested on a dataset we have\ngenerated and made available. Quantitative and qualitative results prove that\nthe proposed method obtains cloud-free images, preserving scene details without\nresorting to a huge portion of a clean image and coping with landscape changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Artur Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1\">Erika Puglisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1\">Jamila Mifdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1\">Fiora Pirri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation. (arXiv:2107.02718v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02718","description":"<p>Hand segmentation is a crucial task in first-person vision. Since\nfirst-person images exhibit strong bias in appearance among different\nenvironments, adapting a pre-trained segmentation model to a new domain is\nrequired in hand segmentation. Here, we focus on appearance gaps for hand\nregions and backgrounds separately. We propose (i) foreground-aware image\nstylization and (ii) consensus pseudo-labeling for domain adaptation of hand\nsegmentation. We stylize source images independently for the foreground and\nbackground using target images as style. To resolve the domain shift that the\nstylization has not addressed, we apply careful pseudo-labeling by taking a\nconsensus between the models trained on the source and stylized source images.\nWe validated our method on domain adaptation of hand segmentation from real and\nsimulation images. Our method achieved state-of-the-art performance in both\nsettings. We also demonstrated promising results in challenging multi-target\ndomain adaptation and domain generalization settings. Code is available at\nhttps://github.com/ut-vision/FgSty-CPL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yagi_T/0/1/0/all/0/1\">Takuma Yagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1\">Atsushi Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1\">Yoshitaka Ushiku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers. (arXiv:2107.11472v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.11472","description":"<p>Hyperbolic space can embed hierarchical structures continuously. Hyperbolic\nNeural Networks (HNNs) exploit such representational power by lifting Euclidean\nfeatures into hyperbolic space for classification, outperforming Euclidean\nneural networks (ENNs) on datasets with known hierarchical structures. However,\nHNNs underperform ENNs on standard benchmarks with unclear hierarchies, greatly\nrestricting HNNs' practical applicability.\n</p>\n<p>Our key insight is that HNNs' poorer general classification performance\nresults from vanishing gradients during backpropagation, caused by their hybrid\narchitecture connecting Euclidean features to a hyperbolic classifier. We\npropose an effective solution by simply clipping the Euclidean feature\nmagnitude while training HNNs.\n</p>\n<p>Our experimental results demonstrate that clipped HNNs become\nsuper-hyperbolic classifiers: They are not only consistently better than HNNs\nwhich already outperform ENNs on hierarchical data, but also on-par with ENNs\non MNIST, CIFAR10, CIFAR100 and ImageNet benchmarks, with better adversarial\nrobustness and out-of-distribution detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Rays for Occlusion-aware Image-based Rendering. (arXiv:2107.13421v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13421","description":"<p>We present a new neural representation, called Neural Ray (NeuRay), for the\nnovel view synthesis task. Recent works construct radiance fields from image\nfeatures of input views to render novel view images, which enables the\ngeneralization to new scenes. However, due to occlusions, a 3D point may be\ninvisible to some input views. On such a 3D point, these generalization methods\nwill include inconsistent image features from invisible views, which interfere\nwith the radiance field construction. To solve this problem, we predict the\nvisibility of 3D points to input views within our NeuRay representation. This\nvisibility enables the radiance field construction to focus on visible image\nfeatures, which significantly improves its rendering quality. Meanwhile, a\nnovel consistency loss is proposed to refine the visibility in NeuRay when\nfinetuning on a specific scene. Experiments demonstrate that our approach\nachieves state-of-the-art performance on the novel view synthesis task when\ngeneralizing to unseen scenes and outperforms per-scene optimization methods\nafter finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DarkLighter: Light Up the Darkness for UAV Tracking. (arXiv:2107.14389v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14389","description":"<p>Recent years have witnessed the fast evolution and promising performance of\nthe convolutional neural network (CNN)-based trackers, which aim at imitating\nbiological visual systems. However, current CNN-based trackers can hardly\ngeneralize well to low-light scenes that are commonly lacked in the existing\ntraining set. In indistinguishable night scenarios frequently encountered in\nunmanned aerial vehicle (UAV) tracking-based applications, the robustness of\nthe state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial\ntracking in the dark through a general fashion, this work proposes a low-light\nimage enhancer namely DarkLighter, which dedicates to alleviate the impact of\npoor illumination and noise iteratively. A lightweight map estimation network,\ni.e., ME-Net, is trained to efficiently estimate illumination maps and noise\nmaps jointly. Experiments are conducted with several SOTA trackers on numerous\nUAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability\nand universality of DarkLighter, with high efficiency. Moreover, DarkLighter\nhas further been implemented on a typical UAV system. Real-world tests at night\nscenes have verified its practicability and dependability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangze Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Dependency Guided Contrastive Learning for Multiple Labeling in Prenatal Ultrasound. (arXiv:2108.05055v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05055","description":"<p>Standard plane recognition plays an important role in prenatal ultrasound\n(US) screening. Automatically recognizing the standard plane along with the\ncorresponding anatomical structures in US image can not only facilitate US\nimage interpretation but also improve diagnostic efficiency. In this study, we\nbuild a novel multi-label learning (MLL) scheme to identify multiple standard\nplanes and corresponding anatomical structures of fetus simultaneously. Our\ncontribution is three-fold. First, we represent the class correlation by word\nembeddings to capture the fine-grained semantic and latent statistical\nconcurrency. Second, we equip the MLL with a graph convolutional network to\nexplore the inner and outer relationship among categories. Third, we propose a\nnovel cluster relabel-based contrastive learning algorithm to encourage the\ndivergence among ambiguous classes. Extensive validation was performed on our\nlarge in-house dataset. Our approach reports the highest accuracy as 90.25% for\nstandard planes labeling, 85.59% for planes and structures labeling and mAP as\n94.63%. The proposed MLL scheme provides a novel perspective for standard plane\nrecognition and can be easily extended to other medical image classification\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuangchi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_X/0/1/0/all/0/1\">Xue Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Ziwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiduo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruobing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11250","description":"<p>A panoptic driving perception system is an essential part of autonomous\ndriving. A high-precision and real-time perception system can assist the\nvehicle in making the reasonable decision while driving. We present a panoptic\ndriving perception network (YOLOP) to perform traffic object detection,\ndrivable area segmentation and lane detection simultaneously. It is composed of\none encoder for feature extraction and three decoders to handle the specific\ntasks. Our model performs extremely well on the challenging BDD100K dataset,\nachieving state-of-the-art on all three tasks in terms of accuracy and speed.\nBesides, we verify the effectiveness of our multi-task learning model for joint\ntraining via ablative studies. To our best knowledge, this is the first work\nthat can process these three visual perception tasks simultaneously in\nreal-time on an embedded device Jetson TX2(23 FPS) and maintain excellent\naccuracy. To facilitate further research, the source codes and pre-trained\nmodels are released at https://github.com/hustvl/YOLOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Manwen Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wenqing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Filter Adaptive Network for Single Image Defocus Deblurring. (arXiv:2108.13610v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13610","description":"<p>We propose a novel end-to-end learning-based approach for single image\ndefocus deblurring. The proposed approach is equipped with a novel Iterative\nFilter Adaptive Network (IFAN) that is specifically designed to handle\nspatially-varying and large defocus blur. For adaptively handling\nspatially-varying blur, IFAN predicts pixel-wise deblurring filters, which are\napplied to defocused features of an input image to generate deblurred features.\nFor effectively managing large blur, IFAN models deblurring filters as stacks\nof small-sized separable filters. Predicted separable deblurring filters are\napplied to defocused features using a novel Iterative Adaptive Convolution\n(IAC) layer. We also propose a training scheme based on defocus disparity\nestimation and reblurring, which significantly boosts the deblurring quality.\nWe demonstrate that our method achieves state-of-the-art performance both\nquantitatively and qualitatively on real-world images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Hyeongseok Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rim_J/0/1/0/all/0/1\">Jaesung Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Scaling Visual Question Answering (VQA) to the open-domain and multi-hop\nnature of web searches, requires fundamental advances in visual representation\nlearning, knowledge aggregation, and language generation. In this work, we\nintroduce WebQA, a challenging new benchmark that proves difficult for\nlarge-scale state-of-the-art models which lack language groundable visual\nrepresentations for novel objects and the ability to reason, yet trivial for\nhumans. WebQA mirrors the way humans use the web: 1) Ask a question, 2) Choose\nsources to aggregate, and 3) Produce a fluent language response. This is the\nbehavior we should be expecting from IoT devices and digital assistants.\nExisting work prefers to assume that a model can either reason about knowledge\nin images or in text. WebQA includes a secondary text-only QA task to ensure\nimproved visual performance does not come at the cost of language\nunderstanding. Our challenge for the community is to create unified multimodal\nreasoning models that answer questions regardless of the source modality,\nmoving us closer to digital assistants that not only query language knowledge,\nbut also the richer visual online world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10852","description":"<p>We present Pix2Seq, a simple and generic framework for object detection.\nUnlike existing approaches that explicitly integrate prior knowledge about the\ntask, we cast object detection as a language modeling task conditioned on the\nobserved pixel inputs. Object descriptions (e.g., bounding boxes and class\nlabels) are expressed as sequences of discrete tokens, and we train a neural\nnetwork to perceive the image and generate the desired sequence. Our approach\nis based mainly on the intuition that if a neural network knows about where and\nwhat the objects are, we just need to teach it how to read them out. Beyond the\nuse of task-specific data augmentations, our approach makes minimal assumptions\nabout the task, yet it achieves competitive results on the challenging COCO\ndataset, compared to highly specialized and well optimized detection\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Saurabh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Attribute and Structure Subspace Clustering Network. (arXiv:2109.13742v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13742","description":"<p>Deep self-expressiveness-based subspace clustering methods have demonstrated\neffectiveness. However, existing works only consider the attribute information\nto conduct the self-expressiveness, which may limit the clustering performance.\nIn this paper, we propose a novel adaptive attribute and structure subspace\nclustering network (AASSC-Net) to simultaneously consider the attribute and\nstructure information in an adaptive graph fusion manner. Specifically, we\nfirst exploit an auto-encoder to represent input data samples with latent\nfeatures for the construction of an attribute matrix. We also construct a mixed\nsigned and symmetric structure matrix to capture the local geometric structure\nunderlying data samples. Then, we perform self-expressiveness on the\nconstructed attribute and structure matrices to learn their affinity graphs\nseparately. Finally, we design a novel attention-based fusion module to\nadaptively leverage these two affinity graphs to construct a more\ndiscriminative affinity graph. Extensive experimental results on commonly used\nbenchmark datasets demonstrate that our AASSC-Net significantly outperforms\nstate-of-the-art methods. In addition, we conduct comprehensive ablation\nstudies to discuss the effectiveness of the designed modules. The code will be\npublicly available at https://github.com/ZhihaoPENG-CityU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhihao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02711","description":"<p>Recently, GAN inversion methods combined with Contrastive Language-Image\nPretraining (CLIP) enables zero-shot image manipulation guided by text prompts.\nHowever, their applications to diverse real images are still difficult due to\nthe limited GAN inversion capability. Specifically, these approaches often have\ndifficulties in reconstructing images with novel poses, views, and highly\nvariable contents compared to the training data, altering object identity, or\nproducing unwanted image artifacts. To mitigate these problems and enable\nfaithful manipulation of real images, we propose a novel method, dubbed\nDiffusionCLIP, that performs text-driven image manipulation using diffusion\nmodels. Based on full inversion capability and high-quality image generation\npower of recent diffusion models, our method performs zero-shot image\nmanipulation successfully even between unseen domains and takes another step\ntowards general application by manipulating images from a widely varying\nImageNet dataset. Furthermore, we propose a novel noise combination method that\nallows straightforward multi-attribute manipulation. Extensive experiments and\nhuman evaluation confirmed robust and superior manipulation performance of our\nmethods compared to the existing baselines. Code is available at\nhttps://github.com/gwang-kim/DiffusionCLIP.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gwanghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1\">Taesung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Higher-Order Dynamics in Video-Based Cardiac Measurement. (arXiv:2110.03690v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.03690","description":"<p>Computer vision methods typically optimize for first-order dynamics (e.g.,\noptical flow). However, in many cases the properties of interest are subtle\nvariations in higher-order changes, such as acceleration. This is true in the\ncardiac pulse, where the second derivative can be used as an indicator of blood\npressure and arterial disease. Recent developments in camera-based vital sign\nmeasurement have shown that cardiac measurements can be recovered with\nimpressive accuracy from videos; however, most of the research has focused on\nextracting summary statistics such as heart rate. Less emphasis has been put on\nthe accuracy of waveform morphology that is necessary for many clinically\nmeaningful assessments. In this work, we provide evidence that higher-order\ndynamics are better estimated by neural models when explicitly optimized for in\nthe loss function. Furthermore, adding second-derivative inputs also improves\nperformance when estimating second-order dynamics. We illustrate this, by\nshowing that incorporating the second derivative of both the input frames and\nthe target vital sign signals into the training procedure, models are better\nable to estimate left ventricle ejection time (LVET) intervals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hill_B/0/1/0/all/0/1\">Brian L. Hill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient ImageNet: How to discover spurious features in Deep Learning?. (arXiv:2110.04301v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.04301","description":"<p>Deep neural networks can be unreliable in the real world especially when they\nheavily use {\\it spurious} features for their predictions. Focusing on image\nclassifications, we define {\\it core features} as the set of visual features\nthat are always a part of the object definition while {\\it spurious features}\nare the ones that are likely to {\\it co-occur} with the object but not a part\nof it (e.g., attribute \"fingers\" for class \"band aid\"). Traditional methods for\ndiscovering spurious features either require extensive human annotations (thus,\nnot scalable), or are useful on specific models. In this work, we introduce a\n{\\it general} framework to discover a subset of spurious and core visual\nfeatures used in inferences of a general model and localize them on a large\nnumber of images with minimal human supervision. Our methodology is based on\nthis key idea: to identify spurious or core \\textit{visual features} used in\nmodel predictions, we identify spurious or core \\textit{neural features}\n(penultimate layer neurons of a robust model) via limited human supervision\n(e.g., using top 5 activating images per feature). We then show that these\nneural feature annotations {\\it generalize} extremely well to many more images\n{\\it without} any human supervision. We use the activation maps for these\nneural features as the soft masks to highlight spurious or core visual\nfeatures. Using this methodology, we introduce the {\\it Salient Imagenet}\ndataset containing core and spurious masks for a large set of samples from\nImagenet. Using this dataset, we show that several popular Imagenet models rely\nheavily on various spurious features in their predictions, indicating the\nstandard accuracy alone is not sufficient to fully assess model performance.\nCode and dataset for reproducing all experiments in the paper is available at\n\\url{https://github.com/singlasahil14/salient_imagenet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05319","description":"<p>Data-driven fault detection has been regarded as a 3D image segmentation\ntask. The models trained from synthetic data are difficult to generalize in\nsome surveys. Recently, training 3D fault segmentation using sparse manual 2D\nslices is thought to yield promising results, but manual labeling has many\nfalse negative labels (abnormal annotations), which is detrimental to training\nand consequently to detection performance. Motivated to train 3D fault\nsegmentation networks under sparse 2D labels while suppressing false negative\nlabels, we analyze the training process gradient and propose the Mask Dice (MD)\nloss. Moreover, the fault is an edge feature, and current encoderdecoder\narchitectures widely used for fault detection (e.g., Ushape network) are not\nconducive to edge representation and have redundant parameters. Consequently,\nFault-Net is proposed, which is designed for the characteristics of faults,\nemploys high-resolution propagation features, and embeds Multi-Scale\nCompression Fusion module to fuse multi-scale information, which allows the\nedge information to be fully preserved during propagation and fusion, thus\nenabling advanced performance via few computational resources. Experimental\ndemonstrates that MD loss supports the inclusion of human experience in\ntraining and suppresses false negative labels therein, allowing the baseline\nmodel to generalize to more surveys. The Fault-Net parameter is only 0.42MB,\nsupport up to 5283 (FP32) and 6403 (FP16) size cuboid inference on 16GB RAM,\nits inference speed is significantly faster than other models. Our approach\nemploys fewer computational resources while providing more reliable and clearer\ninterpretations of seismic faults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yimin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Timing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaoquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zongchao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDACNN: Target-domain-free Domain Adaptation Convolutional Neural Network for Drift Compensation in Gas Sensors. (arXiv:2110.07509v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2110.07509","description":"<p>Sensor drift is a long-existing unpredictable problem that deteriorates the\nperformance of gaseous substance recognition, calling for an antidrift domain\nadaptation algorithm. However, the prerequisite for traditional methods to\nachieve fine results is to have data from both nondrift distributions (source\ndomain) and drift distributions (target domain) for domain alignment, which is\nusually unrealistic and unachievable in real-life scenarios. To compensate for\nthis, in this paper, deep learning based on a target-domain-free domain\nadaptation convolutional neural network (TDACNN) is proposed. The main concept\nis that CNNs extract not only the domain-specific features of samples but also\nthe domain-invariant features underlying both the source and target domains.\nMaking full use of these various levels of embedding features can lead to\ncomprehensive utilization of different levels of characteristics, thus\nachieving drift compensation by the extracted intermediate features between two\ndomains. In the TDACNN, a flexible multibranch backbone with a multiclassifier\nstructure is proposed under the guidance of bionics, which utilizes multiple\nembedding features comprehensively without involving target domain data during\ntraining. A classifier ensemble method based on maximum mean discrepancy (MMD)\nis proposed to evaluate all the classifiers jointly based on the credibility of\nthe pseudolabel. To optimize network training, an additive angular margin\nsoftmax loss with parameter dynamic adjustment is utilized. Experiments on two\ndrift datasets under different settings demonstrate the superiority of TDACNN\ncompared with several state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuelin Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xiang_S/0/1/0/all/0/1\">Sihao Xiang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_Z/0/1/0/all/0/1\">Zehuan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Peng_X/0/1/0/all/0/1\">Xiaoyan Peng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tian_Y/0/1/0/all/0/1\">Yutong Tian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Duan_S/0/1/0/all/0/1\">Shukai Duan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yan_J/0/1/0/all/0/1\">Jia Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A unifying framework for $n$-dimensional quasi-conformal mappings. (arXiv:2110.10437v2 [cs.CG] UPDATED)","link":"http://arxiv.org/abs/2110.10437","description":"<p>With the advancement of computer technology, there is a surge of interest in\neffective mapping methods for objects in higher-dimensional spaces. To\nestablish a one-to-one correspondence between objects, higher-dimensional\nquasi-conformal theory can be utilized for ensuring the bijectivity of the\nmappings. In addition, it is often desirable for the mappings to satisfy\ncertain prescribed geometric constraints and possess low distortion in\nconformality or volume. In this work, we develop a unifying framework for\ncomputing $n$-dimensional quasi-conformal mappings. More specifically, we\npropose a variational model that integrates quasi-conformal distortion,\nvolumetric distortion, landmark correspondence, intensity mismatch and volume\nprior information to handle a large variety of deformation problems. We further\nprove the existence of a minimizer for the proposed model and devise efficient\nnumerical methods to solve the optimization problem. We demonstrate the\neffectiveness of the proposed framework using various experiments in two- and\nthree-dimensions, with applications to medical image registration, adaptive\nremeshing and shape modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daoping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1\">Gary P. T. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11524","description":"<p>A key component of understanding hand-object interactions is the ability to\nidentify the active object -- the object that is being manipulated by the human\nhand. In order to accurately localize the active object, any method must reason\nusing information encoded by each image pixel, such as whether it belongs to\nthe hand, the object, or the background. To leverage each pixel as evidence to\ndetermine the bounding box of the active object, we propose a pixel-wise voting\nfunction. Our pixel-wise voting function takes an initial bounding box as input\nand produces an improved bounding box of the active object as output. The\nvoting function is designed so that each pixel inside of the input bounding box\nvotes for an improved bounding box, and the box with the majority vote is\nselected as the output. We call the collection of bounding boxes generated\ninside of the voting function, the Relational Box Field, as it characterizes a\nfield of bounding boxes defined in relationship to the current bounding box.\nWhile our voting function is able to improve the bounding box of the active\nobject, one round of voting is typically not enough to accurately localize the\nactive object. Therefore, we repeatedly apply the voting function to\nsequentially improve the location of the bounding box. However, since it is\nknown that repeatedly applying a one-step predictor (i.e., auto-regressive\nprocessing with our voting function) can cause a data distribution shift, we\nmitigate this issue using reinforcement learning (RL). We adopt standard RL to\nlearn the voting function parameters and show that it provides a meaningful\nimprovement over a standard supervised learning approach. We perform\nexperiments on two large-scale datasets: 100DOH and MECCANO, improving AP50\nperformance by 8% and 30%, respectively, over the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Co-segmentation by Segment Swapping for Retrieval and Discovery. (arXiv:2110.15904v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.15904","description":"<p>The goal of this work is to efficiently identify visually similar patterns in\nimages, e.g. identifying an artwork detail copied between an engraving and an\noil painting, or recognizing parts of a night-time photograph visible in its\ndaytime counterpart. Lack of training data is a key challenge for this\nco-segmentation task. We present a simple yet surprisingly effective approach\nto overcome this difficulty: we generate synthetic training pairs by selecting\nsegments in an image and copy-pasting them into another image. We then learn to\npredict the repeated region masks. We find that it is crucial to predict the\ncorrespondences as an auxiliary task and to use Poisson blending and style\ntransfer on the training pairs to generalize on real data. We analyse results\nwith two deep architectures relevant to our joint image analysis task: a\ntransformer-based architecture and Sparse Nc-Net, a recent network designed to\npredict coarse correspondences using 4D convolutions. We show our approach\nprovides clear improvements for artwork details retrieval on the Brueghel\ndataset and achieves competitive performance on two place recognition\nbenchmarks, Tokyo247 and Pitts30K. We also demonstrate the potential of our\napproach for unsupervised image collection analysis by introducing a spectral\ngraph clustering approach to object discovery and demonstrating it on the\nobject discovery dataset of \\cite{rubinstein2013unsupervised} and the Brueghel\ndataset. Our code and data are available at\n<a href=\"http://imagine.enpc.fr/~shenx/SegSwap/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images. (arXiv:2111.11057v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11057","description":"<p>The task of instance segmentation in remote sensing images, aiming at\nperforming per-pixel labeling of objects at instance level, is of great\nimportance for various civil applications. Despite previous successes, most\nexisting instance segmentation methods designed for natural images encounter\nsharp performance degradations when they are directly applied to top-view\nremote sensing images. Through careful analysis, we observe that the challenges\nmainly come from the lack of discriminative object features due to severe scale\nvariations, low contrasts, and clustered distributions. In order to address\nthese problems, a novel context aggregation network (CATNet) is proposed to\nimprove the feature extraction process. The proposed model exploits three\nlightweight plug-and-play modules, namely dense feature pyramid network\n(DenseFPN), spatial context pyramid (SCP), and hierarchical region of interest\nextractor (HRoIE), to aggregate global visual context at feature, spatial, and\ninstance domains, respectively. DenseFPN is a multi-scale feature propagation\nmodule that establishes more flexible information flows by adopting inter-level\nresidual connections, cross-level dense connections, and feature re-weighting\nstrategy. Leveraging the attention mechanism, SCP further augments the features\nby aggregating global spatial context into local regions. For each instance,\nHRoIE adaptively generates RoI features for different downstream tasks. We\ncarry out extensive evaluations of the proposed scheme on the challenging\niSAID, DIOR, NWPU VHR-10, and HRSID datasets. The evaluation results\ndemonstrate that the proposed approach outperforms state-of-the-arts under\nsimilar computational costs. Source code and pre-trained models are available\nat https://github.com/yeliudev/CATNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huifang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shuang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion. (arXiv:2111.11326v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11326","description":"<p>Deep network architectures struggle to continually learn new tasks without\nforgetting the previous tasks. A recent trend indicates that dynamic\narchitectures based on an expansion of the parameters can reduce catastrophic\nforgetting efficiently in continual learning. However, existing approaches\noften require a task identifier at test-time, need complex tuning to balance\nthe growing number of parameters, and barely share any information across\ntasks. As a result, they struggle to scale to a large number of tasks without\nsignificant overhead. In this paper, we propose a transformer architecture\nbased on a dedicated encoder/decoder framework. Critically, the encoder and\ndecoder are shared among all tasks. Through a dynamic expansion of special\ntokens, we specialize each forward of our decoder network on a task\ndistribution. Our strategy scales to a large number of tasks while having\nnegligible memory and time overheads due to strict control of the parameters\nexpansion. Moreover, this efficient strategy doesn't need any hyperparameter\ntuning to control the network's expansion. Our model reaches excellent results\non CIFAR100 and state-of-the-art performances on the large-scale ImageNet100\nand ImageNet1000 while having less parameters than concurrent dynamic\nframeworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1\">Arthur Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1\">Alexandre Ram&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1\">Guillaume Couairon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. (arXiv:2111.12077v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12077","description":"<p>Though neural radiance fields (NeRF) have demonstrated impressive view\nsynthesis results on objects and small bounded regions of space, they struggle\non \"unbounded\" scenes, where the camera may point in any direction and content\nmay exist at any distance. In this setting, existing NeRF-like models often\nproduce blurry or low-resolution renderings (due to the unbalanced detail and\nscale of nearby and distant objects), are slow to train, and may exhibit\nartifacts due to the inherent ambiguity of the task of reconstructing a large\nscene from a small set of images. We present an extension of mip-NeRF (a NeRF\nvariant that addresses sampling and aliasing) that uses a non-linear scene\nparameterization, online distillation, and a novel distortion-based regularizer\nto overcome the challenges presented by unbounded scenes. Our model, which we\ndub \"mip-NeRF 360\" as we target scenes in which the camera rotates 360 degrees\naround a point, reduces mean-squared error by 57% compared to mip-NeRF, and is\nable to produce realistic synthesized views and detailed depth maps for highly\nintricate, unbounded real-world scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbin_D/0/1/0/all/0/1\">Dor Verbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Peter Hedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Up Vision-Language Pre-training for Image Captioning. (arXiv:2111.12233v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12233","description":"<p>In recent years, we have witnessed significant performance boost in the image\ncaptioning task based on vision-language pre-training (VLP). Scale is believed\nto be an important factor for this advance. However, most existing work only\nfocuses on pre-training transformers with moderate sizes (e.g., 12 or 24\nlayers) on roughly 4 million images. In this paper, we present LEMON, a\nLargE-scale iMage captiONer, and provide the first empirical study on the\nscaling behavior of VLP for image captioning. We use the state-of-the-art VinVL\nmodel as our reference model, which consists of an image feature extractor and\na transformer model, and scale the transformer both up and down, with model\nsizes ranging from 13 to 675 million parameters. In terms of data, we conduct\nexperiments with up to 200 million image-text pairs which are automatically\ncollected from web based on the alt attribute of the image (dubbed as ALT200M).\nExtensive analysis helps to characterize the performance trend as the model\nsize and the pre-training data size increase. We also compare different\ntraining recipes, especially for training on large-scale noisy data. As a\nresult, LEMON achieves new state of the arts on several major image captioning\nbenchmarks, including COCO Caption, nocaps, and Conceptual Captions. We also\nshow LEMON can generate captions with long-tail visual concepts when used in a\nzero-shot manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yumao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. (arXiv:2111.12707v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12707","description":"<p>Estimating 3D human poses from monocular videos is a challenging task due to\ndepth ambiguity and self-occlusion. Most existing works attempt to solve both\nissues by exploiting spatial and temporal relationships. However, those works\nignore the fact that it is an inverse problem where multiple feasible solutions\n(i.e., hypotheses) exist. To relieve this limitation, we propose a\nMulti-Hypothesis Transformer (MHFormer) that learns spatio-temporal\nrepresentations of multiple plausible pose hypotheses. In order to effectively\nmodel multi-hypothesis dependencies and build strong relationships across\nhypothesis features, the task is decomposed into three stages: (i) Generate\nmultiple initial hypothesis representations; (ii) Model self-hypothesis\ncommunication, merge multiple hypotheses into a single converged representation\nand then partition it into several diverged hypotheses; (iii) Learn\ncross-hypothesis communication and aggregate the multi-hypothesis features to\nsynthesize the final 3D pose. Through the above processes, the final\nrepresentation is enhanced and the synthesized pose is much more accurate.\nExtensive experiments show that MHFormer achieves state-of-the-art results on\ntwo challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and\nwhistles, its performance surpasses the previous best result by a large margin\nof 3% on Human3.6M. Code and models are available at\n\\url{https://github.com/Vegetebird/MHFormer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation. (arXiv:2111.12903v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12903","description":"<p>Consistency learning using input image, feature, or network perturbations has\nshown remarkable results in semi-supervised semantic segmentation, but this\napproach can be seriously affected by inaccurate predictions of unlabelled\ntraining images. There are two consequences of these inaccurate predictions: 1)\nthe training based on the \"strict\" cross-entropy (CE) loss can easily overfit\nprediction mistakes, leading to confirmation bias; and 2) the perturbations\napplied to these inaccurate predictions will use potentially erroneous\npredictions as training signals, degrading consistency learning. In this paper,\nwe address the prediction accuracy problem of consistency learning methods with\nnovel extensions of the mean-teacher (MT) model, which include a new auxiliary\nteacher, and the replacement of MT's mean square error (MSE) by a stricter\nconfidence-weighted cross-entropy (Conf-CE) loss. The accurate prediction by\nthis model allows us to use a challenging combination of network, input data\nand feature perturbations to improve the consistency learning generalisation,\nwhere the feature perturbations consist of a new adversarial perturbation.\nResults on public benchmarks show that our approach achieves remarkable\nimprovements over the previous SOTA methods in the field. Our code is available\nat https://github.com/yyliu01/PS-MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation. (arXiv:2111.12940v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12940","description":"<p>Self-training has greatly facilitated domain adaptive semantic segmentation,\nwhich iteratively generates pseudo labels on unlabeled target data and retrains\nthe network. However, realistic segmentation datasets are highly imbalanced,\npseudo labels are typically biased to the majority classes and basically noisy,\nleading to an error-prone and suboptimal model. In this paper, we propose a\nsimple region-based active learning approach for semantic segmentation under a\ndomain shift, aiming to automatically query a small partition of image regions\nto be labeled while maximizing segmentation performance. Our algorithm, Region\nImpurity and Prediction Uncertainty (RIPU), introduces a new acquisition\nstrategy characterizing the spatial adjacency of image regions along with the\nprediction confidence. We show that the proposed region-based selection\nstrategy makes more efficient use of a limited budget than image-based or\npoint-based counterparts. Further, we enforce local prediction consistency\nbetween a pixel and its nearest neighbors on a source image. Alongside, we\ndevelop a negative learning loss to make the features more discriminative.\nExtensive experiments demonstrate that our method only requires very few\nannotations to almost reach the supervised performance and substantially\noutperforms state-of-the-art methods. The code is available at\nhttps://github.com/BIT-DA/RIPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Binhui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Longhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xinjing Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13196","description":"<p>The canonical approach to video captioning dictates a caption generation\nmodel to learn from offline-extracted dense video features. These feature\nextractors usually operate on video frames sampled at a fixed frame rate and\nare often trained on image/video understanding tasks, without adaption to video\ncaptioning data. In this work, we present SwinBERT, an end-to-end\ntransformer-based model for video captioning, which takes video frame patches\ndirectly as inputs, and outputs a natural language description. Instead of\nleveraging multiple 2D/3D feature extractors, our method adopts a video\ntransformer to encode spatial-temporal representations that can adapt to\nvariable lengths of video input without dedicated design for different frame\nrates. Based on this model architecture, we show that video captioning can\nbenefit significantly from more densely sampled video frames as opposed to\nprevious successes with sparsely sampled video frames for video-and-language\nunderstanding tasks (e.g., video question answering). Moreover, to avoid the\ninherent redundancy in consecutive video frames, we propose adaptively learning\na sparse attention mask and optimizing it for task-specific performance\nimprovement through better long-range video sequence modeling. Through\nextensive experiments on 5 video captioning datasets, we show that SwinBERT\nachieves across-the-board performance improvements over previous methods, often\nby a large margin. The learned sparse attention masks in addition push the\nlimit to new state of the arts, and can be transferred between different video\nlengths and between different datasets. Code is available at\nhttps://github.com/microsoft/SwinBERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faisal Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yumao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers. (arXiv:2111.13587v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13587","description":"<p>Vision transformers have delivered tremendous success in representation\nlearning. This is primarily due to effective token mixing through self\nattention. However, this scales quadratically with the number of pixels, which\nbecomes infeasible for high-resolution inputs. To cope with this challenge, we\npropose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer\nthat learns to mix in the Fourier domain. AFNO is based on a principled\nfoundation of operator learning which allows us to frame token mixing as a\ncontinuous global convolution without any dependence on the input resolution.\nThis principle was previously used to design FNO, which solves global\nconvolution efficiently in the Fourier domain and has shown promise in learning\nchallenging PDEs. To handle challenges in visual representation learning such\nas discontinuities in images and high resolution inputs, we propose principled\narchitectural modifications to FNO which results in memory and computational\nefficiency. This includes imposing a block-diagonal structure on the channel\nmixing weights, adaptively sharing weights across tokens, and sparsifying the\nfrequency modes via soft-thresholding and shrinkage. The resulting model is\nhighly parallel with a quasi-linear complexity and has linear memory in the\nsequence size. AFNO outperforms self-attention mechanisms for few-shot\nsegmentation in terms of both efficiency and accuracy. For Cityscapes\nsegmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of\n65k and outperforms other efficient self-attention mechanisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guibas_J/0/1/0/all/0/1\">John Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1\">Morteza Mardani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1\">Andrew Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13680","description":"<p>Learning-based optical flow estimation has been dominated with the pipeline\nof cost volume with convolutions for flow regression, which is inherently\nlimited to local correlations and thus is hard to address the long-standing\nchallenge of large displacements. To alleviate this, the state-of-the-art\nframework RAFT gradually improves its prediction quality by using a large\nnumber of iterative refinements, achieving remarkable performance but\nintroducing linearly increasing inference time. To enable both high accuracy\nand efficiency, we completely revamp the dominant flow regression pipeline by\nreformulating optical flow as a global matching problem, which identifies the\ncorrespondences by directly comparing feature similarities. Specifically, we\npropose a GMFlow framework, which consists of three main components: a\ncustomized Transformer for feature enhancement, a correlation and softmax layer\nfor global feature matching, and a self-attention layer for flow propagation.\nWe further introduce a refinement step that reuses GMFlow at higher feature\nresolution for residual flow prediction. Our new framework outperforms\n31-refinements RAFT on the challenging Sintel benchmark, while using only one\nrefinement and running faster, suggesting a new paradigm for accurate and\nefficient optical flow estimation. Code is available at\nhttps://github.com/haofeixu/gmflow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haofei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Frame Interpolation Transformer. (arXiv:2111.13817v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13817","description":"<p>Existing methods for video interpolation heavily rely on deep convolution\nneural networks, and thus suffer from their intrinsic limitations, such as\ncontent-agnostic kernel weights and restricted receptive field. To address\nthese issues, we propose a Transformer-based video interpolation framework that\nallows content-aware aggregation weights and considers long-range dependencies\nwith the self-attention operations. To avoid the high computational cost of\nglobal self-attention, we introduce the concept of local attention into video\ninterpolation and extend it to the spatial-temporal domain. Furthermore, we\npropose a space-time separation strategy to save memory usage, which also\nimproves performance. In addition, we develop a multi-scale frame synthesis\nscheme to fully realize the potential of Transformers. Extensive experiments\ndemonstrate the proposed model performs favorably against the state-of-the-art\nmethods both quantitatively and qualitatively on a variety of benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhihao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deblur-NeRF: Neural Radiance Fields from Blurry Images. (arXiv:2111.14292v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14292","description":"<p>Neural Radiance Field (NeRF) has gained considerable attention recently for\n3D scene reconstruction and novel view synthesis due to its remarkable\nsynthesis quality. However, image blurriness caused by defocus or motion, which\noften occurs when capturing scenes in the wild, significantly degrades its\nreconstruction quality. To address this problem, We propose Deblur-NeRF, the\nfirst method that can recover a sharp NeRF from blurry input. We adopt an\nanalysis-by-synthesis approach that reconstructs blurry views by simulating the\nblurring process, thus making NeRF robust to blurry inputs. The core of this\nsimulation is a novel Deformable Sparse Kernel (DSK) module that models\nspatially-varying blur kernels by deforming a canonical sparse kernel at each\nspatial location. The ray origin of each kernel point is jointly optimized,\ninspired by the physical blurring process. This module is parameterized as an\nMLP that has the ability to be generalized to various blur types. Jointly\noptimizing the NeRF and the DSK module allows us to restore a sharp NeRF. We\ndemonstrate that our method can be used on both camera motion blur and defocus\nblur: the two most common types of blur in real scenes. Evaluation results on\nboth synthetic and real-world data show that our method outperforms several\nbaselines. The synthetic and real datasets along with the source code is\npublicly available at https://limacv.github.io/deblurnerf/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sander_P/0/1/0/all/0/1\">Pedro V. Sander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-instance Point Cloud Registration by Efficient Correspondence Clustering. (arXiv:2111.14582v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14582","description":"<p>We address the problem of estimating the poses of multiple instances of the\nsource point cloud within a target point cloud. Existing solutions require\nsampling a lot of hypotheses to detect possible instances and reject the\noutliers, whose robustness and efficiency degrade notably when the number of\ninstances and outliers increase. We propose to directly group the set of noisy\ncorrespondences into different clusters based on a distance invariance matrix.\nThe instances and outliers are automatically identified through clustering. Our\nmethod is robust and fast. We evaluated our method on both synthetic and\nreal-world datasets. The results show that our approach can correctly register\nup to 20 instances with an F1 score of 90.46% in the presence of 70% outliers,\nwhich performs significantly better and at least 10x faster than existing\nmethods\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Weixuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Danping Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis. (arXiv:2111.14791v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14791","description":"<p>Vision Transformers (ViT)s have shown great performance in self-supervised\nlearning of global and local representations that can be transferred to\ndownstream applications. Inspired by these results, we introduce a novel\nself-supervised learning framework with tailored proxy tasks for medical image\nanalysis. Specifically, we propose: (i) a new 3D transformer-based model,\ndubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for\nself-supervised pre-training; (ii) tailored proxy tasks for learning the\nunderlying pattern of human anatomy. We demonstrate successful pre-training of\nthe proposed model on 5,050 publicly available computed tomography (CT) images\nfrom various body organs. The effectiveness of our approach is validated by\nfine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV)\nSegmentation Challenge with 13 abdominal organs and segmentation tasks from the\nMedical Segmentation Decathlon (MSD) dataset. Our model is currently the\nstate-of-the-art (i.e. ranked 1st) on the public test leaderboards of both MSD\nand BTCV datasets. Code: https://monai.io/research/swin-unetr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_V/0/1/0/all/0/1\">Vishwesh Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blended Diffusion for Text-driven Editing of Natural Images. (arXiv:2111.14818v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14818","description":"<p>Natural language offers a highly intuitive interface for image editing. In\nthis paper, we introduce the first solution for performing local (region-based)\nedits in generic natural images, based on a natural language description along\nwith an ROI mask. We achieve our goal by leveraging and combining a pretrained\nlanguage-image model (CLIP), to steer the edit towards a user-provided text\nprompt, with a denoising diffusion probabilistic model (DDPM) to generate\nnatural-looking results. To seamlessly fuse the edited region with the\nunchanged parts of the image, we spatially blend noised versions of the input\nimage with the local text-guided diffusion latent at a progression of noise\nlevels. In addition, we show that adding augmentations to the diffusion process\nmitigates adversarial results. We compare against several baselines and related\nmethods, both qualitatively and quantitatively, and show that our method\noutperforms these solutions in terms of overall realism, ability to preserve\nthe background and matching the text. Finally, we show several text-driven\nediting applications, including adding a new object to an image,\nremoving/replacing/altering existing objects, background replacement, and image\nextrapolation. Code is available at:\nhttps://omriavrahami.com/blended-diffusion-page/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1\">Omri Avrahami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-domain Integrative Swin Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.14831","description":"<p>Decreasing projection views to lower X-ray radiation dose usually leads to\nsevere streak artifacts. To improve image quality from sparse-view data, a\nMulti-domain Integrative Swin Transformer network (MIST-net) was developed in\nthis article. First, MIST-net incorporated lavish domain features from data,\nresidual-data, image, and residual-image using flexible network architectures,\nwhere residual-data and residual-image sub-network was considered as data\nconsistency module to eliminate interpolation and reconstruction errors.\nSecond, a trainable edge enhancement filter was incorporated to detect and\nprotect image edges. Third, a high-quality reconstruction Swin transformer\n(i.e., Recformer) was designed to capture image global features. The experiment\nresults on numerical and real cardiac clinical datasets with 48-views\ndemonstrated that our proposed MIST-net provided better image quality with more\nsmall features and sharp edges than other competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jiayi Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Heye Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZZ-Net: A Universal Rotation Equivariant Architecture for 2D Point Clouds. (arXiv:2111.15341v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15341","description":"<p>In this paper, we are concerned with rotation equivariance on 2D point cloud\ndata. We describe a particular set of functions able to approximate any\ncontinuous rotation equivariant and permutation invariant function. Based on\nthis result, we propose a novel neural network architecture for processing 2D\npoint clouds and we prove its universality for approximating functions\nexhibiting these symmetries.\n</p>\n<p>We also show how to extend the architecture to accept a set of 2D-2D\ncorrespondences as indata, while maintaining similar equivariance properties.\nExperiments are presented on the estimation of essential matrices in stereo\nvision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokman_G/0/1/0/all/0/1\">Georg B&#xf6;kman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1\">Fredrik Kahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flinth_A/0/1/0/all/0/1\">Axel Flinth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"360MonoDepth: High-Resolution 360{\\deg} Monocular Depth Estimation. (arXiv:2111.15669v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15669","description":"<p>360{\\deg} cameras can capture complete environments in a single shot, which\nmakes 360{\\deg} imagery alluring in many computer vision tasks. However,\nmonocular depth estimation remains a challenge for 360{\\deg} data, particularly\nfor high resolutions like 2K (2048x1024) and beyond that are important for\nnovel-view synthesis and virtual reality applications. Current CNN-based\nmethods do not support such high resolutions due to limited GPU memory. In this\nwork, we propose a flexible framework for monocular depth estimation from\nhigh-resolution 360{\\deg} images using tangent images. We project the 360{\\deg}\ninput image onto a set of tangent planes that produce perspective views, which\nare suitable for the latest, most accurate state-of-the-art perspective\nmonocular depth estimators. To achieve globally consistent disparity estimates,\nwe recombine the individual depth estimates using deformable multi-scale\nalignment followed by gradient-domain blending. The result is a dense,\nhigh-resolution 360{\\deg} depth map with a high level of detail, also for\noutdoor scenes which are not supported by existing methods. Our source code and\ndata are available at https://manurare.github.io/360monodepth/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rey_Area_M/0/1/0/all/0/1\">Manuel Rey-Area</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingze Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1\">Christian Richardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification. (arXiv:2112.00412v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00412","description":"<p>The problem of class imbalanced data is that the generalization performance\nof the classifier deteriorates due to the lack of data from minority classes.\nIn this paper, we propose a novel minority over-sampling method to augment\ndiversified minority samples by leveraging the rich context of the majority\nclasses as background images. To diversify the minority samples, our key idea\nis to paste an image from a minority class onto rich-context images from a\nmajority class, using them as background images. Our method is simple and can\nbe easily combined with the existing long-tailed recognition methods. We\nempirically prove the effectiveness of the proposed oversampling method through\nextensive experiments and ablation studies. Without any architectural changes\nor complex algorithms, our method achieves state-of-the-art performance on\nvarious long-tailed classification benchmarks. Our code is made available at\nhttps://github.com/naver-ai/cmo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seulki Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Youngkyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jin Young Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions. (arXiv:2112.00431v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00431","description":"<p>The recent and increasing interest in video-language research has driven the\ndevelopment of large-scale datasets that enable data-intensive machine learning\ntechniques. In comparison, limited effort has been made at assessing the\nfitness of these datasets for the video-language grounding task. Recent works\nhave begun to discover significant limitations in these datasets, suggesting\nthat state-of-the-art techniques commonly overfit to hidden dataset biases. In\nthis work, we present MAD (Movie Audio Descriptions), a novel benchmark that\ndeparts from the paradigm of augmenting existing video datasets with text\nannotations and focuses on crawling and aligning available audio descriptions\nof mainstream movies. MAD contains over 384,000 natural language sentences\ngrounded in over 1,200 hours of videos and exhibits a significant reduction in\nthe currently diagnosed biases for video-language grounding datasets. MAD's\ncollection strategy enables a novel and more challenging version of\nvideo-language grounding, where short temporal moments (typically seconds long)\nmust be accurately grounded in diverse long-form videos that can last up to\nthree hours. We have released MAD's data and baselines code at\nhttps://github.com/Soldelli/MAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Transferability of Supervised Pretraining: an MLP Perspective. (arXiv:2112.00496v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00496","description":"<p>The pretrain-finetune paradigm is a classical pipeline in visual learning.\nRecent progress on unsupervised pretraining methods shows superior transfer\nperformance to their supervised counterparts. This paper revisits this\nphenomenon and sheds new light on understanding the transferability gap between\nunsupervised and supervised pretraining from a multilayer perceptron (MLP)\nperspective. While previous works focus on the effectiveness of MLP on\nunsupervised image classification where pretraining and evaluation are\nconducted on the same dataset, we reveal that the MLP projector is also the key\nfactor to better transferability of unsupervised pretraining methods than\nsupervised pretraining methods. Based on this observation, we attempt to close\nthe transferability gap between supervised and unsupervised pretraining by\nadding an MLP projector before the classifier in supervised pretraining. Our\nanalysis indicates that the MLP projector can help retain intra-class variation\nof visual features, decrease the feature distribution distance between\npretraining and evaluation datasets, and reduce feature redundancy. Extensive\nexperiments on public benchmarks demonstrate that the added MLP projector\nsignificantly boosts the transferability of supervised pretraining, e.g. +7.2%\ntop-1 accuracy on the concept generalization task, +5.8% top-1 accuracy for\nlinear evaluation on 12-domain classification tasks, and +0.8% AP on COCO\nobject detection task, making supervised pretraining comparable or even better\nthan unsupervised pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Donglian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-ImageNet: Towards Robust, Fine-Grained Object Recognition with Event Cameras. (arXiv:2112.01041v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01041","description":"<p>We introduce N-ImageNet, a large-scale dataset targeted for robust,\nfine-grained object recognition with event cameras. The dataset is collected\nusing programmable hardware in which an event camera consistently moves around\na monitor displaying images from ImageNet. N-ImageNet serves as a challenging\nbenchmark for event-based object recognition, due to its large number of\nclasses and samples. We empirically show that pretraining on N-ImageNet\nimproves the performance of event-based classifiers and helps them learn with\nfew labeled data. In addition, we present several variants of N-ImageNet to\ntest the robustness of event-based classifiers under diverse camera\ntrajectories and severe lighting conditions, and propose a novel event\nrepresentation to alleviate the performance degradation. To the best of our\nknowledge, we are the first to quantitatively investigate the consequences\ncaused by various environmental conditions on event-based object recognition\nalgorithms. N-ImageNet and its variants are expected to guide practical\nimplementations for deploying event-based object recognition algorithms in the\nreal world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jaehyeok Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gangin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongsu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Head Avatars from Monocular RGB Videos. (arXiv:2112.01554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01554","description":"<p>We present Neural Head Avatars, a novel neural representation that explicitly\nmodels the surface geometry and appearance of an animatable human avatar that\ncan be used for teleconferencing in AR/VR or other applications in the movie or\ngames industry that rely on a digital human. Our representation can be learned\nfrom a monocular RGB portrait video that features a range of different\nexpressions and views. Specifically, we propose a hybrid representation\nconsisting of a morphable model for the coarse shape and expressions of the\nface, and two feed-forward networks, predicting vertex offsets of the\nunderlying mesh as well as a view- and expression-dependent texture. We\ndemonstrate that this representation is able to accurately extrapolate to\nunseen poses and view points, and generates natural expressions while providing\nsharp texture details. Compared to previous works on head avatars, our method\nprovides a disentangled shape and appearance model of the complete human head\n(including hair) that is compatible with the standard graphics pipeline.\nMoreover, it quantitatively and qualitatively outperforms current state of the\nart in terms of reconstruction quality and novel-view synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grassal_P/0/1/0/all/0/1\">Philip-William Grassal</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Prinzler_M/0/1/0/all/0/1\">Malte Prinzler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Leistner_T/0/1/0/all/0/1\">Titus Leistner</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a> (3) ((1) Heidelberg University, (2) Technical University of Munich, (3) Max Planck Institute for Intelligent Systems)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Two-Stage Detection of Human-Object Interactions with a Novel Unary-Pairwise Transformer. (arXiv:2112.01838v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01838","description":"<p>Recent developments in transformer models for visual data have led to\nsignificant improvements in recognition and detection tasks. In particular,\nusing learnable queries in place of region proposals has given rise to a new\nclass of one-stage detection models, spearheaded by the Detection Transformer\n(DETR). Variations on this one-stage approach have since dominated human-object\ninteraction (HOI) detection. However, the success of such one-stage HOI\ndetectors can largely be attributed to the representation power of\ntransformers. We discovered that when equipped with the same transformer, their\ntwo-stage counterparts can be more performant and memory-efficient, while\ntaking a fraction of the time to train. In this work, we propose the\nUnary-Pairwise Transformer, a two-stage detector that exploits unary and\npairwise representations for HOIs. We observe that the unary and pairwise parts\nof our transformer network specialise, with the former preferentially\nincreasing the scores of positive examples and the latter decreasing the scores\nof negative examples. We evaluate our method on the HICO-DET and V-COCO\ndatasets, and significantly outperform state-of-the-art approaches. At\ninference time, our model with ResNet50 approaches real-time performance on a\nsingle GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frederic Z. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training. (arXiv:2112.03552v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03552","description":"<p>Recently, vision Transformers (ViTs) are developing rapidly and starting to\nchallenge the domination of convolutional neural networks (CNNs) in the realm\nof computer vision (CV). With the general-purpose Transformer architecture\nreplacing the hard-coded inductive biases of convolution, ViTs have surpassed\nCNNs, especially in data-sufficient circumstances. However, ViTs are prone to\nover-fit on small datasets and thus rely on large-scale pre-training, which\nexpends enormous time. In this paper, we strive to liberate ViTs from\npre-training by introducing CNNs' inductive biases back to ViTs while\npreserving their network architectures for higher upper bound and setting up\nmore suitable optimization objectives. To begin with, an agent CNN is designed\nbased on the given ViT with inductive biases. Then a bootstrapping training\nalgorithm is proposed to jointly optimize the agent and ViT with weight\nsharing, during which the ViT learns inductive biases from the intermediate\nfeatures of the agent. Extensive experiments on CIFAR-10/100 and ImageNet-1k\nwith limited training data have shown encouraging results that the inductive\nbiases help ViTs converge significantly faster and outperform conventional CNNs\nwith even fewer parameters. Our code is publicly available at\nhttps://github.com/zhfeing/Bootstrapping-ViTs-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiarui Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mengqi Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BACON: Band-limited Coordinate Networks for Multiscale Scene Representation. (arXiv:2112.04645v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04645","description":"<p>Coordinate-based networks have emerged as a powerful tool for 3D\nrepresentation and scene reconstruction. These networks are trained to map\ncontinuous input coordinates to the value of a signal at each point. Still,\ncurrent architectures are black boxes: their spectral characteristics cannot be\neasily analyzed, and their behavior at unsupervised points is difficult to\npredict. Moreover, these networks are typically trained to represent a signal\nat a single scale, so naive downsampling or upsampling results in artifacts. We\nintroduce band-limited coordinate networks (BACON), a network architecture with\nan analytical Fourier spectrum. BACON has constrained behavior at unsupervised\npoints, can be designed based on the spectral characteristics of the\nrepresented signal, and can represent signals at multiple scales without\nper-scale supervision. We demonstrate BACON for multiscale neural\nrepresentation of images, radiance fields, and 3D scenes using signed distance\nfunctions and show that it outperforms conventional single-scale coordinate\nnetworks in terms of interpretability and quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1\">David B. Lindell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veen_D/0/1/0/all/0/1\">Dave Van Veen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeong Joon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic Prior. (arXiv:2112.05077v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05077","description":"<p>Evaluating and improving planning for autonomous vehicles requires scalable\ngeneration of long-tail traffic scenarios. To be useful, these scenarios must\nbe realistic and challenging, but not impossible to drive through safely. In\nthis work, we introduce STRIVE, a method to automatically generate challenging\nscenarios that cause a given planner to produce undesirable behavior, like\ncollisions. To maintain scenario plausibility, the key idea is to leverage a\nlearned model of traffic motion in the form of a graph-based conditional VAE.\nScenario generation is formulated as an optimization in the latent space of\nthis traffic model, perturbing an initial real-world scene to produce\ntrajectories that collide with a given planner. A subsequent optimization is\nused to find a \"solution\" to the scenario, ensuring it is useful to improve the\ngiven planner. Further analysis clusters generated scenarios based on collision\ntype. We attack two planners and show that STRIVE successfully generates\nrealistic, challenging scenarios in both cases. We additionally \"close the\nloop\" and use these scenarios to optimize hyperparameters of a rule-based\nplanner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1\">Davis Rempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philion_J/0/1/0/all/0/1\">Jonah Philion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06116","description":"<p>We study the effect of adversarial perturbations of images on deep stereo\nmatching networks for the disparity estimation task. We present a method to\ncraft a single set of perturbations that, when added to any stereo image pair\nin a dataset, can fool a stereo network to significantly alter the perceived\nscene geometry. Our perturbation images are \"universal\" in that they not only\ncorrupt estimates of the network on the dataset they are optimized for, but\nalso generalize to different architectures trained on different datasets. We\nevaluate our approach on multiple benchmark datasets where our perturbations\ncan increase the D1-error (akin to fooling rate) of state-of-the-art stereo\nnetworks from 1% to as much as 87%. We investigate the effect of perturbations\non the estimated scene geometry and identify object classes that are most\nvulnerable. Our analysis on the activations of registered points between left\nand right images led us to find architectural components that can increase\nrobustness against adversaries. By simply designing networks with such\ncomponents, one can reduce the effect of adversaries by up to 60.5%, which\nrivals the robustness of networks fine-tuned with costly adversarial data\naugmentation. Our design principle also improves their robustness against\ncommon image corruptions by an average of 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_Z/0/1/0/all/0/1\">Zachary Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Parth Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active learning with MaskAL reduces annotation effort for training Mask R-CNN. (arXiv:2112.06586v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06586","description":"<p>The generalisation performance of a convolutional neural network (CNN) is\ninfluenced by the quantity, quality, and variety of the training images.\nTraining images must be annotated, and this is time consuming and expensive.\nThe goal of our work was to reduce the number of annotated images needed to\ntrain a CNN while maintaining its performance. We hypothesised that the\nperformance of a CNN can be improved faster by ensuring that the set of\ntraining images contains a large fraction of hard-to-classify images. The\nobjective of our study was to test this hypothesis with an active learning\nmethod that can automatically select the hard-to-classify images. We developed\nan active learning method for Mask Region-based CNN (Mask R-CNN) and named this\nmethod MaskAL. MaskAL involved the iterative training of Mask R-CNN, after\nwhich the trained model was used to select a set of unlabelled images about\nwhich the model was most uncertain. The selected images were then annotated and\nused to retrain Mask R-CNN, and this was repeated for a number of sampling\niterations. In our study, MaskAL was compared to a random sampling method on a\nbroccoli dataset with five visually similar classes. MaskAL performed\nsignificantly better than the random sampling. In addition, MaskAL had the same\nperformance after sampling 900 images as the random sampling had after 2300\nimages. Compared to a Mask R-CNN model that was trained on the entire training\nset (14,000 images), MaskAL achieved 93.9% of that model's performance with\n17.9% of its training data. The random sampling achieved 81.9% of that model's\nperformance with 16.4% of its training data. We conclude that by using MaskAL,\nthe annotation effort can be reduced for training Mask R-CNN on a broccoli\ndataset with visually similar classes. Our software is available on\nhttps://github.com/pieterblok/maskal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blok_P/0/1/0/all/0/1\">Pieter M. Blok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kootstra_G/0/1/0/all/0/1\">Gert Kootstra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elghor_H/0/1/0/all/0/1\">Hakim Elchaoui Elghor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diallo_B/0/1/0/all/0/1\">Boubacar Diallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evert_F/0/1/0/all/0/1\">Frits K. van Evert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henten_E/0/1/0/all/0/1\">Eldert J. van Henten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logically at Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09253","description":"<p>This paper describes our participant system for the multi-modal fact\nverification (Factify) challenge at AAAI 2022. Despite the recent advance in\ntext based verification techniques and large pre-trained multimodal models\ncross vision and language, very limited work has been done in applying\nmultimodal techniques to automate fact checking process, particularly\nconsidering the increasing prevalence of claims and fake news about images and\nvideos on social media. In our work, the challenge is treated as multimodal\nentailment task and framed as multi-class classification. Two baseline\napproaches are proposed and explored including an ensemble model (combining two\nuni-modal models) and a multi-modal attention network (modeling the interaction\nbetween image and text pair from claim and evidence document). We conduct\nseveral experiments investigating and benchmarking different SoTA pre-trained\ntransformers and vision models in this work. Our best model is ranked first in\nleaderboard which obtains a weighted average F-measure of 0.77 on both\nvalidation and test set. Exploratory analysis of dataset is also carried out on\nthe Factify data set and uncovers salient patterns and issues (e.g., word\noverlapping, visual entailment correlation, source bias) that motivates our\nhypothesis. Finally, we highlight challenges of the task and multimodal dataset\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_H/0/1/0/all/0/1\">Hella-Franziska Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_S/0/1/0/all/0/1\">Stylianos Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiskovski_D/0/1/0/all/0/1\">David Kiskovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandhakavi_A/0/1/0/all/0/1\">Anil Bandhakavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10325","description":"<p>Due to the constraints of the imaging device and high cost in operation time,\ncomputer tomography (CT) scans are usually acquired with low intra-slice\nresolution. Improving the intra-slice resolution is beneficial to the disease\ndiagnosis for both human experts and computer-aided systems. To this end, this\npaper builds a novel medical slice synthesis to increase the between-slice\nresolution. Considering that the ground-truth intermediate medical slices are\nalways absent in clinical practice, we introduce the incremental cross-view\nmutual distillation strategy to accomplish this task in the self-supervised\nlearning manner. Specifically, we model this problem from three different\nviews: slice-wise interpolation from axial view and pixel-wise interpolation\nfrom coronal and sagittal views. Under this circumstance, the models learned\nfrom different views can distill valuable knowledge to guide the learning\nprocesses of each other. We can repeat this process to make the models\nsynthesize intermediate slice data with increasing inter-slice resolution. To\ndemonstrate the effectiveness of the proposed approach, we conduct\ncomprehensive experiments on a large-scale CT dataset. Quantitative and\nqualitative comparison results show that our method outperforms\nstate-of-the-art algorithms by clear margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization. (arXiv:2112.11177v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11177","description":"<p>For medical image segmentation, imagine if a model was only trained using MR\nimages in source domain, how about its performance to directly segment CT\nimages in target domain? This setting, namely generalizable cross-modality\nsegmentation, owning its clinical potential, is much more challenging than\nother related settings, e.g., domain adaptation. To achieve this goal, we in\nthis paper propose a novel dual-normalization model by leveraging the augmented\nsource-similar and source-dissimilar images during our generalizable\nsegmentation. To be specific, given a single source domain, aiming to simulate\nthe possible appearance change in unseen target domains, we first utilize a\nnonlinear transformation to augment source-similar and source-dissimilar\nimages. Then, to sufficiently exploit these two types of augmentations, our\nproposed dual-normalization based model employs a shared backbone yet\nindependent batch normalization layer for separate normalization. Afterward, we\nput forward a style-based selection scheme to automatically choose the\nappropriate path in the test stage. Extensive experiments on three publicly\navailable datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal\nMulti-Organ datasets, have demonstrated that our method outperforms other\nstate-of-the-art domain generalization methods. Code is available at\nhttps://github.com/zzzqzhou/Dual-Normalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recur, Attend or Convolve? Frame Dependency Modeling Matters for Cross-Domain Robustness in Action Recognition. (arXiv:2112.12175v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12175","description":"<p>Most action recognition models today are highly parameterized, and evaluated\non datasets with predominantly spatially distinct classes. Previous results for\nsingle images have shown that 2D Convolutional Neural Networks (CNNs) tend to\nbe biased toward texture rather than shape for various computer vision tasks\n(Geirhos et al., 2019), reducing generalization. Taken together, this raises\nsuspicion that large video models learn spurious correlations rather than to\ntrack relevant shapes over time and infer generalizable semantics from their\nmovement. A natural way to avoid parameter explosion when learning visual\npatterns over time is to make use of recurrence across the time-axis. In this\narticle, we empirically study the cross-domain robustness of models with\ndifferent frame dependency modeling (recurrent, attention-based or 3D\nconvolutional). In order to enable a light-weight and systematic assessment of\nthe ability to capture temporal structure, not revealed from single frames, we\nprovide the Temporal Shape dataset. We find that when controlling for\nperformance and layer structure, convolutional-recurrent models show better\nout-of-domain generalization ability on the Temporal Shape dataset than 3D\nconvolution- and attention-based models. Moreover, our experiments indicate\nthat convolution- and attention-based models exhibit more texture bias on\nDiving48 than convolutional-recurrent models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokropek_E/0/1/0/all/0/1\">Ernest Pokropek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2201.01763","description":"<p>Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Makes Weakly Supervised Local Feature Better. (arXiv:2201.02861v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02861","description":"<p>Weakly supervised learning can help local feature methods to overcome the\nobstacle of acquiring a large-scale dataset with densely labeled\ncorrespondences. However, since weak supervision cannot distinguish the losses\ncaused by the detection and description steps, directly conducting weakly\nsupervised learning within a joint describe-then-detect pipeline suffers\nlimited performance. In this paper, we propose a decoupled describe-then-detect\npipeline tailored for weakly supervised local feature learning. Within our\npipeline, the detection step is decoupled from the description step and\npostponed until discriminative and robust descriptors are learned. In addition,\nwe introduce a line-to-window search strategy to explicitly use the camera pose\ninformation for better descriptor learning. Extensive experiments show that our\nmethod, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous\nfully and weakly supervised methods and achieves state-of-the-art performance\non a wide range of downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1\">Qing Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A statistical shape model for radiation-free assessment and classification of craniosynostosis. (arXiv:2201.03288v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.03288","description":"<p>The assessment of craniofacial deformities requires patient data which is\nsparsely available. Statistical shape models provide realistic and synthetic\ndata enabling comparisons of existing methods on a common dataset.\n</p>\n<p>We build the first publicly available statistical 3D head model of\ncraniosynostosis patients and the first model focusing on infants younger than\n1.5 years. We further present a shape-model-based classification pipeline to\ndistinguish between three different classes of craniosynostosis and a control\ngroup on photogrammetric surface scans. To the best of our knowledge, our study\nuses the largest dataset of craniosynostosis patients in a classification study\nfor craniosynostosis and statistical shape modeling to date.\n</p>\n<p>We demonstrate that our shape model performs similar to other statistical\nshape models of the human head. Craniosynostosis-specific pathologies are\nrepresented in the first eigenmodes of the model. Regarding the automatic\nclassification of craniosynostis, our classification approach yields an\naccuracy of 97.8%, comparable to other state-of-the-art methods using both\ncomputed tomography scans and stereophotogrammetry.\n</p>\n<p>Our publicly available, craniosynostosis-specific statistical shape model\nenables the assessment of craniosynostosis on realistic and synthetic data. We\nfurther present a state-of-the-art shape-model-based classification approach\nfor a radiation-free diagnosis of craniosynostosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schaufelberger_M/0/1/0/all/0/1\">Matthias Schaufelberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuhle_R/0/1/0/all/0/1\">Reinald Peter K&#xfc;hle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wachter_A/0/1/0/all/0/1\">Andreas Wachter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weichel_F/0/1/0/all/0/1\">Frederic Weichel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hagen_N/0/1/0/all/0/1\">Niclas Hagen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ringwald_F/0/1/0/all/0/1\">Friedemann Ringwald</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eisenmann_U/0/1/0/all/0/1\">Urs Eisenmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_J/0/1/0/all/0/1\">J&#xfc;rgen Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Engel_M/0/1/0/all/0/1\">Michael Engel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Freudlsperger_C/0/1/0/all/0/1\">Christian Freudlsperger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahm_W/0/1/0/all/0/1\">Werner Nahm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Constrained Arbitrary Image Style Transfer using SIFT and DCNN. (arXiv:2201.05346v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05346","description":"<p>This paper develops a new image synthesis approach to transfer an example\nimage (style image) to other images (content images) by using Deep\nConvolutional Neural Networks (DCNN) model. When common neural style transfer\nmethods are used, the textures and colors in the style image are usually\ntransferred imperfectly to the content image, or some visible errors are\ngenerated. This paper proposes a novel saliency constrained method to reduce or\navoid such effects. It first evaluates some existing saliency detection methods\nto select the most suitable one for use in our method. The selected saliency\ndetection method is used to detect the object in the style image, corresponding\nto the object of the content image with the same saliency. In addition, aim to\nsolve the problem that the size or resolution is different in the style image\nand content, the scale-invariant feature transform is used to generate a series\nof style images and content images which can be used to generate more feature\nmaps for patches matching. It then proposes a new loss function combining the\nsaliency loss, style loss and content loss, adding gradient of saliency\nconstraint into style transfer in iterations. Finally the source images and\nsaliency detection results are utilized as multichannel input to an improved\ndeep CNN framework for style transfer. The experiments show that the saliency\nmaps of source images can help find the correct matching and avoid artifacts.\nExperimental results on different kind of images demonstrate that our method\noutperforms nine representative methods from recent publications and has good\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">HuiHuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STURE: Spatial-Temporal Mutual Representation Learning for Robust Data Association in Online Multi-Object Tracking. (arXiv:2201.06824v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06824","description":"<p>Online multi-object tracking (MOT) is a longstanding task for computer vision\nand intelligent vehicle platform. At present, the main paradigm is\ntracking-by-detection, and the main difficulty of this paradigm is how to\nassociate current candidate detections with historical tracklets. However, in\nthe MOT scenarios, each historical tracklet is composed of an object sequence,\nwhile each candidate detection is just a flat image, which lacks temporal\nfeatures of the object sequence. The feature difference between current\ncandidate detections and historical tracklets makes the object association much\nharder. Therefore, we propose a Spatial-Temporal Mutual Representation Learning\n(STURE) approach which learns spatial-temporal representations between current\ncandidate detections and historical sequences in a mutual representation space.\nFor historical trackelets, the detection learning network is forced to match\nthe representations of sequence learning network in a mutual representation\nspace. The proposed approach is capable of extracting more distinguishing\ndetection and sequence representations by using various designed losses in\nobject association. As a result, spatial-temporal feature is learned mutually\nto reinforce the current detection features, and the feature difference can be\nrelieved. To prove the robustness of the STURE, it is applied to the public MOT\nchallenge benchmarks and performs well compared with various state-of-the-art\nonline MOT trackers based on identity-preserving metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiyong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nai_K/0/1/0/all/0/1\">Ke Nai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1\">Ming Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CP-Net: Contour-Perturbed Reconstruction Network for Self-Supervised Point Cloud Learning. (arXiv:2201.08215v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08215","description":"<p>Self-supervised learning has not been fully explored for point cloud\nanalysis. Current frameworks are mainly based on point cloud reconstruction.\nGiven only 3D coordinates, such approaches tend to learn local geometric\nstructures and contours, while failing in understanding high level semantic\ncontent. Consequently, they achieve unsatisfactory performance in downstream\ntasks such as classification, segmentation, etc. To fill this gap, we propose a\ngeneric Contour-Perturbed Reconstruction Network (CP-Net), which can\neffectively guide self-supervised reconstruction to learn semantic content in\nthe point cloud, and thus promote discriminative power of point cloud\nrepresentation. First, we introduce a concise contour-perturbed augmentation\nmodule for point cloud reconstruction. With guidance of geometry disentangling,\nwe divide point cloud into contour and content components. Subsequently, we\nperturb the contour components and preserve the content components on the point\ncloud. As a result, self supervisor can effectively focus on semantic content,\nby reconstructing the original point cloud from such perturbed one. Second, we\nuse this perturbed reconstruction as an assistant branch, to guide the learning\nof basic reconstruction branch via a distinct dual-branch consistency loss. In\nthis case, our CP-Net not only captures structural contour but also learn\nsemantic content for discriminative downstream tasks. Finally, we perform\nextensive experiments on a number of point cloud benchmarks. Part segmentation\nresults demonstrate that our CP-Net (81.5% of mIoU) outperforms the previous\nself-supervised models, and narrows the gap with the fully-supervised methods.\nFor classification, we get a competitive result with the fully-supervised\nmethods on ModelNet40 (92.5% accuracy) and ScanObjectNN (87.9% accuracy). The\ncodes and models will be released afterwards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingye Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Global Diversity and Local Context for Video Summarization. (arXiv:2201.11345v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11345","description":"<p>Video summarization aims to automatically generate a diverse and concise\nsummary which is useful in large-scale video processing. Most of the methods\ntend to adopt self-attention mechanism across video frames, which fails to\nmodel the diversity of video frames. To alleviate this problem, we revisit the\npairwise similarity measurement in self-attention mechanism and find that the\nexisting inner-product affinity leads to discriminative features rather than\ndiversified features. In light of this phenomenon, we propose global diverse\nattention which uses the squared Euclidean distance instead to compute the\naffinities. Moreover, we model the local contextual information by novel local\ncontextual attention to remove the redundancy in the video. By combining these\ntwo attention mechanisms, a video SUMmarization model with Diversified\nContextual Attention scheme is developed, namely SUM-DCA. Extensive experiments\nare conducted on benchmark data sets to verify the effectiveness and the\nsuperiority of SUM-DCA in terms of F-score and rank-based evaluation without\nany bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingchao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_O/0/1/0/all/0/1\">Ouhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongjin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures. (arXiv:2201.12763v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12763","description":"<p>We introduce RIM-Net, a neural network which learns recursive implicit fields\nfor unsupervised inference of hierarchical shape structures. Our network\nrecursively decomposes an input 3D shape into two parts, resulting in a binary\ntree hierarchy. Each level of the tree corresponds to an assembly of shape\nparts, represented as implicit functions, to reconstruct the input shape. At\neach node of the tree, simultaneous feature decoding and shape decomposition\nare carried out by their respective feature and part decoders, with weight\nsharing across the same hierarchy level. As an implicit field decoder, the part\ndecoder is designed to decompose a sub-shape, via a two-way branched\nreconstruction, where each branch predicts a set of parameters defining a\nGaussian to serve as a local point distribution for shape reconstruction. With\nreconstruction losses accounted for at each hierarchy level and a decomposition\nloss at each node, our network training does not require any ground-truth\nsegmentations, let alone hierarchies. Through extensive experiments and\ncomparisons to state-of-the-art alternatives, we demonstrate the quality,\nconsistency, and interpretability of hierarchical structural inference by\nRIM-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chengjie Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion. (arXiv:2202.02000v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02000","description":"<p>Multi-atlas segmentation (MAS) is a promising framework for medical image\nsegmentation. Generally, MAS methods register multiple atlases, i.e., medical\nimages with corresponding labels, to a target image; and the transformed atlas\nlabels can be combined to generate target segmentation via label fusion\nschemes. Many conventional MAS methods employed the atlases from the same\nmodality as the target image. However, the number of atlases with the same\nmodality may be limited or even missing in many clinical applications. Besides,\nconventional MAS methods suffer from the computational burden of registration\nor label fusion procedures. In this work, we design a novel cross-modality MAS\nframework, which uses available atlases from a certain modality to segment a\ntarget image from another modality. To boost the computational efficiency of\nthe framework, both the image registration and label fusion are achieved by\nwell-designed deep neural networks. For the atlas-to-target image registration,\nwe propose a bi-directional registration network (BiRegNet), which can\nefficiently align images from different modalities. For the label fusion, we\ndesign a similarity estimation network (SimNet), which estimates the fusion\nweight of each atlas by measuring its similarity to the target image. SimNet\ncan learn multi-scale information for similarity estimation to improve the\nperformance of label fusion. The proposed framework was evaluated by the left\nventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets,\nrespectively. Results have shown that the framework is effective for\ncross-modality MAS in both registration and label fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wangbin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liqin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Message Passing Neural PDE Solvers. (arXiv:2202.03376v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.03376","description":"<p>The numerical solution of partial differential equations (PDEs) is difficult,\nhaving led to a century of research so far. Recently, there have been pushes to\nbuild neural--numerical hybrid solvers, which piggy-backs the modern trend\ntowards fully end-to-end learned systems. Most works so far can only generalize\nover a subset of properties to which a generic solver would be faced,\nincluding: resolution, topology, geometry, boundary conditions, domain\ndiscretization regularity, dimensionality, etc. In this work, we build a\nsolver, satisfying these properties, where all the components are based on\nneural message passing, replacing all heuristically designed components in the\ncomputation graph with backprop-optimized neural function approximators. We\nshow that neural message passing solvers representationally contain some\nclassical methods, such as finite differences, finite volumes, and WENO\nschemes. In order to encourage stability in training autoregressive models, we\nput forward a method that is based on the principle of zero-stability, posing\nstability as a domain adaptation problem. We validate our method on various\nfluid-like flow problems, demonstrating fast, stable, and accurate performance\nacross different domain topologies, discretization, etc. in 1D and 2D. Our\nmodel outperforms state-of-the-art numerical solvers in the low resolution\nregime in terms of speed and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1\">Johannes Brandstetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worrall_D/0/1/0/all/0/1\">Daniel Worrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.07028","description":"<p>We study the problem of developing autonomous agents that can follow human\ninstructions to infer and perform a sequence of actions to complete the\nunderlying task. Significant progress has been made in recent years, especially\nfor tasks with short horizons. However, when it comes to long-horizon tasks\nwith extended sequences of actions, an agent can easily ignore some\ninstructions or get stuck in the middle of the long instructions and eventually\nfail the task. To address this challenge, we propose a model-agnostic\nmilestone-based task tracker (M-TRACK) to guide the agent and monitor its\nprogress. Specifically, we propose a milestone builder that tags the\ninstructions with navigation and interaction milestones which the agent needs\nto complete step by step, and a milestone checker that systemically checks the\nagent's progress in its current milestone and determines when to proceed to the\nnext. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and\n52% relative improvement in unseen success rate over two competitive base\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1\">Jihyung Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering. (arXiv:2202.09277v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09277","description":"<p>Spatio-temporal scene-graph approaches to video-based reasoning tasks, such\nas video question-answering (QA), typically construct such graphs for every\nvideo frame. These approaches often ignore the fact that videos are essentially\nsequences of 2D \"views\" of events happening in a 3D space, and that the\nsemantics of the 3D scene can thus be carried over from frame to frame.\nLeveraging this insight, we propose a (2.5+1)D scene graph representation to\nbetter capture the spatio-temporal information flows inside the videos.\nSpecifically, we first create a 2.5D (pseudo-3D) scene graph by transforming\nevery 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D\ntransformation module, following which we register the video frames into a\nshared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it.\nSuch a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic\nsub-graph, corresponding to whether the objects within them usually move in the\nworld. The nodes in the dynamic graph are enriched with motion features\ncapturing their interactions with other graph nodes. Next, for the video QA\ntask, we present a novel transformer-based reasoning pipeline that embeds the\n(2.5+1)D graph into a spatio-temporal hierarchical latent space, where the\nsub-graphs and their interactions are captured at varied granularity. To\ndemonstrate the effectiveness of our approach, we present experiments on the\nNExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D\nrepresentation leads to faster training and inference, while our hierarchical\nmodel showcases superior performance on the video QA task versus the state of\nthe art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1\">Chiori Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_T/0/1/0/all/0/1\">Tim K. Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSSNet: Multi-Scale-Stage Network for Single Image Deblurring. (arXiv:2202.09652v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09652","description":"<p>Most of traditional single image deblurring methods before deep learning\nadopt a coarse-to-fine scheme that estimates a sharp image at a coarse scale\nand progressively refines it at finer scales. While this scheme has also been\nadopted to several deep learning-based approaches, recently a number of\nsingle-scale approaches have been introduced showing superior performance to\nprevious coarse-to-fine approaches both in quality and computation time. In\nthis paper, we revisit the coarse-to-fine scheme, and analyze defects of\nprevious coarse-to-fine approaches that degrade their performance. Based on the\nanalysis, we propose Multi-Scale-Stage Network (MSSNet), a novel deep\nlearning-based approach to single image deblurring that adopts our remedies to\nthe defects. Specifically, MSSNet adopts three novel technical components:\nstage configuration reflecting blur scales, an inter-scale information\npropagation scheme, and a pixel-shuffle-based multi-scale scheme. Our\nexperiments show that MSSNet achieves the state-of-the-art performance in terms\nof quality, network size, and computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-Supervised Descriptor for Image Copy Detection. (arXiv:2202.10261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10261","description":"<p>Image copy detection is an important task for content moderation. We\nintroduce SSCD, a model that builds on a recent self-supervised contrastive\ntraining objective. We adapt this method to the copy detection task by changing\nthe architecture and training objective, including a pooling operator from the\ninstance matching literature, and adapting contrastive learning to\naugmentations that combine images.\n</p>\n<p>Our approach relies on an entropy regularization term, promoting consistent\nseparation between descriptor vectors, and we demonstrate that this\nsignificantly improves copy detection accuracy. Our method produces a compact\ndescriptor vector, suitable for real-world web scale applications. Statistical\ninformation from a background image distribution can be incorporated into the\ndescriptor.\n</p>\n<p>On the recent DISC2021 benchmark, SSCD is shown to outperform both baseline\ncopy detection models and self-supervised architectures designed for image\nclassification by huge margins, in all settings. For example, SSCD out-performs\nSimCLR descriptors by 48% absolute. Code is available at\nhttps://github.com/facebookresearch/sscd-copy-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1\">Ed Pizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sreya Dutta Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindra_S/0/1/0/all/0/1\">Sugosh Nagavara Ravindra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Priya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Bulk Motion Artifact Removal in Optical Coherence Tomography Angiography. (arXiv:2202.10360v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10360","description":"<p>Optical coherence tomography angiography (OCTA) is an important imaging\nmodality in many bioengineering tasks. The image quality of OCTA, however, is\noften degraded by Bulk Motion Artifacts (BMA), which are due to micromotion of\nsubjects and typically appear as bright stripes surrounded by blurred areas.\nState-of-the-art methods usually treat BMA removal as a learning-based image\ninpainting problem, but require numerous training samples with nontrivial\nannotation. In addition, these methods discard the rich structural and\nappearance information carried in the BMA stripe region. To address these\nissues, in this paper we propose a self-supervised content-aware BMA removal\nmodel. First, the gradient-based structural information and appearance feature\nare extracted from the BMA area and injected into the model to capture more\nconnectivity. Second, with easily collected defective masks, the model is\ntrained in a self-supervised manner, in which only the clear areas are used for\ntraining while the BMA areas for inference. With the structural information and\nappearance feature from noisy image as references, our model can remove larger\nBMA and produce better visualizing result. In addition, only 2D images with\ndefective masks are involved, hence improving the efficiency of our method.\nExperiments on OCTA of mouse cortex demonstrate that our model can remove most\nBMA with extremely large sizes and inconsistent intensities while previous\nmethods fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiaxiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kicheon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingtian Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training with Triple Contrastive Learning. (arXiv:2202.10401v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10401","description":"<p>Vision-language representation learning largely benefits from image-text\nalignment through contrastive losses (e.g., InfoNCE loss). The success of this\nalignment strategy is attributed to its capability in maximizing the mutual\ninformation (MI) between an image and its matched text. However, simply\nperforming cross-modal alignment (CMA) ignores data potential within each\nmodality, which may result in degraded representations. For instance, although\nCMA-based models are able to map image-text pairs close together in the\nembedding space, they fail to ensure that similar inputs from the same modality\nstay close by. This problem can get even worse when the pre-training data is\nnoisy. In this paper, we propose triple contrastive learning (TCL) for\nvision-language pre-training by leveraging both cross-modal and intra-modal\nself-supervision. Besides CMA, TCL introduces an intra-modal contrastive\nobjective to provide complementary benefits in representation learning. To take\nadvantage of localized and structural information from image and text input,\nTCL further maximizes the average MI between local regions of image/text and\ntheir global summary. To the best of our knowledge, ours is the first work that\ntakes into account local structure information for multi-modality\nrepresentation learning. Experimental evaluations show that our approach is\ncompetitive and achieves the new state of the art on various common down-stream\nvision-language tasks such as image-text retrieval and visual question\nanswering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sampath Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction. (arXiv:2202.11884v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.11884","description":"<p>Predicting future motions of road participants is an important task for\ndriving autonomously in urban scenes. Existing models excel at predicting\nmarginal trajectories for single agents, yet it remains an open question to\njointly predict scene compliant trajectories over multiple agents. The\nchallenge is due to exponentially increasing prediction space as a function of\nthe number of agents. In this work, we exploit the underlying relations between\ninteracting agents and decouple the joint prediction problem into marginal\nprediction problems. Our proposed approach M2I first classifies interacting\nagents as pairs of influencers and reactors, and then leverages a marginal\nprediction model and a conditional prediction model to predict trajectories for\nthe influencers and reactors, respectively. The predictions from interacting\nagents are combined and selected according to their joint likelihoods.\nExperiments show that our simple but effective approach achieves\nstate-of-the-art performance on the Waymo Open Motion Dataset interactive\nprediction benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Junru Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian C. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralHOFusion: Neural Volumetric Rendering under Human-object Interactions. (arXiv:2202.12825v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12825","description":"<p>4D modeling of human-object interactions is critical for numerous\napplications. However, efficient volumetric capture and rendering of complex\ninteraction scenarios, especially from sparse inputs, remain challenging. In\nthis paper, we propose NeuralHOFusion, a neural approach for volumetric\nhuman-object capture and rendering using sparse consumer RGBD sensors. It\nmarries traditional non-rigid fusion with recent neural implicit modeling and\nblending advances, where the captured humans and objects are layerwise\ndisentangled. For geometry modeling, we propose a neural implicit inference\nscheme with non-rigid key-volume fusion, as well as a template-aid robust\nobject tracking pipeline. Our scheme enables detailed and complete geometry\ngeneration under complex interactions and occlusions. Moreover, we introduce a\nlayer-wise human-object texture rendering scheme, which combines volumetric and\nimage-based rendering in both spatial and temporal domains to obtain\nphoto-realistic results. Extensive experiments demonstrate the effectiveness\nand efficiency of our approach in synthesizing photo-realistic free-view\nresults under complex human-object interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Suyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guoxing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhuo Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kaiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Deep Multi-View Photometric Stereo. (arXiv:2202.13071v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13071","description":"<p>This paper presents a simple and effective solution to the longstanding\nclassical multi-view photometric stereo (MVPS) problem. It is well-known that\nphotometric stereo (PS) is excellent at recovering high-frequency surface\ndetails, whereas multi-view stereo (MVS) can help remove the low-frequency\ndistortion due to PS and retain the global geometry of the shape. This paper\nproposes an approach that can effectively utilize such complementary strengths\nof PS and MVS. Our key idea is to combine them suitably while considering the\nper-pixel uncertainty of their estimates. To this end, we estimate per-pixel\nsurface normals and depth using an uncertainty-aware deep-PS network and\ndeep-MVS network, respectively. Uncertainty modeling helps select reliable\nsurface normal and depth estimates at each pixel which then act as a true\nrepresentative of the dense surface geometry. At each pixel, our approach\neither selects or discards deep-PS and deep-MVS network prediction depending on\nthe prediction uncertainty measure. For dense, detailed, and precise inference\nof the object's surface profile, we propose to learn the implicit neural shape\nrepresentation via a multilayer perceptron (MLP). Our approach encourages the\nMLP to converge to a natural zero-level set surface using the confident\nprediction from deep-PS and deep-MVS networks, providing superior dense surface\nreconstruction. Extensive experiments on the DiLiGenT-MV benchmark dataset show\nthat our method provides high-quality shape recovery with a much lower memory\nfootprint while outperforming almost all of the existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaya_B/0/1/0/all/0/1\">Berk Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_C/0/1/0/all/0/1\">Carlos Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Alignment using Representation Codebook. (arXiv:2203.00048v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00048","description":"<p>Aligning signals from different modalities is an important step in\nvision-language representation learning as it affects the performance of later\nstages such as cross-modality fusion. Since image and text typically reside in\ndifferent regions of the feature space, directly aligning them at instance\nlevel is challenging especially when features are still evolving during\ntraining. In this paper, we propose to align at a higher and more stable level\nusing cluster representation. Specifically, we treat image and text as two\n\"views\" of the same entity, and encode them into a joint vision-language coding\nspace spanned by a dictionary of cluster centers (codebook). We contrast\npositive and negative samples via their cluster assignments while\nsimultaneously optimizing the cluster centers. To further smooth out the\nlearning process, we adopt a teacher-student distillation paradigm, where the\nmomentum teacher of one view guides the student learning of the other. We\nevaluated our approach on common vision language benchmarks and obtain new SoTA\non zero-shot cross modality retrieval while being competitive on various other\ntransfer tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Person Re-Identification via Self-Supervised Batch Norm Test-Time Adaption. (arXiv:2203.00672v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00672","description":"<p>In this paper, we investigate the generalization problem of person\nre-identification (re-id), whose major challenge is the distribution shift on\nan unseen domain. As an important tool of regularizing the distribution, batch\nnormalization (BN) has been widely used in existing methods. However, they\nneglect that BN is severely biased to the training domain and inevitably\nsuffers the performance drop if directly generalized without being updated. To\ntackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel\nre-id framework that applies the self-supervised strategy to update BN\nparameters adaptively. Specifically, BNTA quickly explores the domain-aware\ninformation within unlabeled target data before inference, and accordingly\nmodulates the feature distribution normalized by BN to adapt to the target\ndomain. This is accomplished by two designed self-supervised auxiliary tasks,\nnamely part positioning and part nearest neighbor matching, which help the\nmodel mine the domain-aware information with respect to the structure and\nidentity of body parts, respectively. To demonstrate the effectiveness of our\nmethod, we conduct extensive experiments on three re-id datasets and confirm\nthe superior performance to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Ke Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenyang Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There is a Time and Place for Reasoning Beyond the Image. (arXiv:2203.00758v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00758","description":"<p>Images are often more significant than only the pixels to human eyes, as we\ncan infer, associate, and reason with contextual information from other sources\nto establish a more complete picture. For example, in Figure 1, we can find a\nway to identify the news articles related to the picture through segment-wise\nunderstandings of the signs, the buildings, the crowds, and more. This\nreasoning could provide the time and place the image was taken, which will help\nus in subsequent tasks, such as automatic storyline construction, correction of\nimage source in intended effect photographs, and upper-stream processing such\nas image clustering for certain location or time.\n</p>\n<p>In this work, we formulate this problem and introduce TARA: a dataset with\n16k images with their associated news, time, and location, automatically\nextracted from New York Times, and an additional 61k examples as distant\nsupervision from WIT. On top of the extractions, we present a crowdsourced\nsubset in which we believe it is possible to find the images' spatio-temporal\ninformation for evaluation purpose. We show that there exists a $70\\%$ gap\nbetween a state-of-the-art joint model and human performance, which is slightly\nfilled by our proposed model that uses segment-wise reasoning, motivating\nhigher-level vision-language joint models that can conduct open-ended reasoning\nwith world knowledge. The data and code are publicly available at\nhttps://github.com/zeyofu/TARA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xingyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Ben Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandratreya_I/0/1/0/all/0/1\">Ishaan Preetam Chandratreya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00859","description":"<p>Recent transformer-based solutions have been introduced to estimate 3D human\npose from 2D keypoint sequence by considering body joints among all frames\nglobally to learn spatio-temporal correlation. We observe that the motions of\ndifferent joints differ significantly. However, the previous methods cannot\nefficiently model the solid inter-frame correspondence of each joint, leading\nto insufficient learning of spatial-temporal correlation. We propose MixSTE\n(Mixed Spatio-Temporal Encoder), which has a temporal transformer block to\nseparately model the temporal motion of each joint and a spatial transformer\nblock to learn inter-joint spatial correlation. These two blocks are utilized\nalternately to obtain better spatio-temporal feature encoding. In addition, the\nnetwork output is extended from the central frame to entire frames of the input\nvideo, thereby improving the coherence between the input and output sequences.\nExtensive experiments are conducted on three benchmarks (Human3.6M,\nMPI-INF-3DHP, and HumanEva). The results show that our model outperforms the\nstate-of-the-art approach by 10.9% P-MPJPE and 7.6% MPJPE. The code is\navailable at https://github.com/JinluZhang1126/MixSTE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAFE: Learning to Condense Dataset by Aligning Features. (arXiv:2203.01531v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01531","description":"<p>Dataset condensation aims at reducing the network training effort through\ncondensing a cumbersome training set into a compact synthetic one.\nState-of-the-art approaches largely rely on learning the synthetic data by\nmatching the gradients between the real and synthetic data batches. Despite the\nintuitive motivation and promising results, such gradient-based methods, by\nnature, easily overfit to a biased set of samples that produce dominant\ngradients, and thus lack global supervision of data distribution. In this\npaper, we propose a novel scheme to Condense dataset by Aligning FEatures\n(CAFE), which explicitly attempts to preserve the real-feature distribution as\nwell as the discriminant power of the resulting synthetic set, lending itself\nto strong generalization capability to various architectures. At the heart of\nour approach is an effective strategy to align features from the real and\nsynthetic data across various scales, while accounting for the classification\nof real samples. Our scheme is further backed up by a novel dynamic bi-level\noptimization, which adaptively adjusts parameter updates to prevent\nover-/under-fitting. We validate the proposed CAFE across various datasets, and\ndemonstrate that it generally outperforms the state of the art: on the SVHN\ndataset, for example, the performance gain is up to 11%. Extensive experiments\nand analyses verify the effectiveness and necessity of proposed designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2203.01601v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01601","description":"<p>Handwritten mathematical expression recognition (HMER) is a challenging task\nthat has many potential applications. Recent methods for HMER have achieved\noutstanding performance with an encoder-decoder architecture. However, these\nmethods adhere to the paradigm that the prediction is made \"from one character\nto another\", which inevitably yields prediction errors due to the complicated\nstructures of mathematical expressions or crabbed handwritings. In this paper,\nwe propose a simple and efficient method for HMER, which is the first to\nincorporate syntax information into an encoder-decoder network. Specifically,\nwe present a set of grammar rules for converting the LaTeX markup sequence of\neach expression into a parsing tree; then, we model the markup sequence\nprediction as a tree traverse process with a deep neural network. In this way,\nthe proposed method can effectively describe the syntax context of expressions,\nalleviating the structure prediction errors of HMER. Experiments on three\nbenchmark datasets demonstrate that our method achieves better recognition\nperformance than prior arts. To further validate the effectiveness of our\nmethod, we create a large-scale dataset consisting of 100k handwritten\nmathematical expression images acquired from ten thousand writers. The source\ncode, new dataset, and pre-trained models of this work will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikubab_W/0/1/0/all/0/1\">Wondimu Dikubab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCTrack: Temporal Contexts for Aerial Tracking. (arXiv:2203.01885v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01885","description":"<p>Temporal contexts among consecutive frames are far from being fully utilized\nin existing visual trackers. In this work, we present TCTrack, a comprehensive\nframework to fully exploit temporal contexts for aerial tracking. The temporal\ncontexts are incorporated at \\textbf{two levels}: the extraction of\n\\textbf{features} and the refinement of \\textbf{similarity maps}. Specifically,\nfor feature extraction, an online temporally adaptive convolution is proposed\nto enhance the spatial features using temporal information, which is achieved\nby dynamically calibrating the convolution weights according to the previous\nframes. For similarity map refinement, we propose an adaptive temporal\ntransformer, which first effectively encodes temporal knowledge in a\nmemory-efficient way, before the temporal knowledge is decoded for accurate\nadjustment of the similarity map. TCTrack is effective and efficient:\nevaluation on four aerial tracking benchmarks shows its impressive performance;\nreal-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX\nXavier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Image Synthesis with Panoptic Layout Generation. (arXiv:2203.02104v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02104","description":"<p>Interactive image synthesis from user-guided input is a challenging task when\nusers wish to control the scene structure of a generated image with\nease.Although remarkable progress has been made on layout-based image synthesis\napproaches, in order to get realistic fake image in interactive scene, existing\nmethods require high-precision inputs, which probably need adjustment several\ntimes and are unfriendly to novice users. When placement of bounding boxes is\nsubject to perturbation, layout-based models suffer from \"missing regions\" in\nthe constructed semantic layouts and hence undesirable artifacts in the\ngenerated images. In this work, we propose Panoptic Layout Generative\nAdversarial Networks (PLGAN) to address this challenge. The PLGAN employs\npanoptic theory which distinguishes object categories between \"stuff\" with\namorphous boundaries and \"things\" with well-defined shapes, such that stuff and\ninstance layouts are constructed through separate branches and later fused into\npanoptic layouts. In particular, the stuff layouts can take amorphous shapes\nand fill up the missing regions left out by the instance layouts. We\nexperimentally compare our PLGAN with state-of-the-art layout-based models on\nthe COCO-Stuff, Visual Genome, and Landscape datasets. The advantages of PLGAN\nare not only visually demonstrated but quantitatively verified in terms of\ninception score, Fr\\'echet inception distance, classification accuracy score,\nand coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Peng Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02503","description":"<p>Pansharpening aims to fuse a registered high-resolution panchromatic image\n(PAN) with a low-resolution hyperspectral image (LR-HSI) to generate an\nenhanced HSI with high spectral and spatial resolution. Existing pansharpening\napproaches neglect using an attention mechanism to transfer HR texture features\nfrom PAN to LR-HSI features, resulting in spatial and spectral distortions. In\nthis paper, we present a novel attention mechanism for pansharpening called\nHyperTransformer, in which features of LR-HSI and PAN are formulated as queries\nand keys in a transformer, respectively. HyperTransformer consists of three\nmain modules, namely two separate feature extractors for PAN and HSI, a\nmulti-head feature soft attention module, and a spatial-spectral feature fusion\nmodule. Such a network improves both spatial and spectral quality measures of\nthe pansharpened HSI by learning cross-feature space dependencies and\nlong-range details of PAN and LR-HSI. Furthermore, HyperTransformer can be\nutilized across multiple spatial scales at the backbone for obtaining improved\nperformance. Extensive experiments conducted on three widely used datasets\ndemonstrate that HyperTransformer achieves significant improvement over the\nstate-of-the-art methods on both spatial and spectral quality measures.\nImplementation code and pre-trained weights can be accessed at\nhttps://github.com/wgcban/HyperTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Applications in Lung Cancer Diagnosis, Treatment and Prognosis. (arXiv:2203.02794v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.02794","description":"<p>The recent development of imaging and sequencing technologies enables\nsystematic advances in the clinical study of lung cancer. Meanwhile, the human\nmind is limited in effectively handling and fully utilizing the accumulation of\nsuch enormous amounts of data. Machine learning-based approaches play a\ncritical role in integrating and analyzing these large and complex datasets,\nwhich have extensively characterized lung cancer through the use of different\nperspectives from these accrued data. In this article, we provide an overview\nof machine learning-based approaches that strengthen the varying aspects of\nlung cancer diagnosis and therapy, including early detection, auxiliary\ndiagnosis, prognosis prediction and immunotherapy practice. Moreover, we\nhighlight the challenges and opportunities for future applications of machine\nlearning in lung cancer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos. (arXiv:2203.03014v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03014","description":"<p>With the assumption that a video dataset is multimodality annotated in which\nauditory and visual modalities both are labeled or class-relevant, current\nmultimodal methods apply modality fusion or cross-modality attention. However,\neffectively leveraging the audio modality in vision-specific annotated videos\nfor action recognition is of particular challenge. To tackle this challenge, we\npropose a novel audio-visual framework that effectively leverages the audio\nmodality in any solely vision-specific annotated dataset. We adopt the language\nmodels (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD)\nthat maps each video label to its most K-relevant audio labels in which SAVLD\nserves as a bridge between audio and video datasets. Then, SAVLD along with a\npretrained audio multi-label model are used to estimate the audio-visual\nmodality relevance during the training phase. Accordingly, a novel learnable\nirrelevant modality dropout (IMD) is proposed to completely drop out the\nirrelevant audio modality and fuse only the relevant modalities. Moreover, we\npresent a new two-stream video Transformer for efficiently modeling the visual\nmodalities. Results on several vision-specific annotated datasets including\nKinetics400 and UCF-101 validated our framework as it outperforms most relevant\naction recognition methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfasly_S/0/1/0/all/0/1\">Saghir Alfasly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuru Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild. (arXiv:2203.03089v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03089","description":"<p>In this paper, we tackle the problem of category-level 9D pose estimation in\nthe wild, given a single RGB-D frame. Using supervised data of real-world 9D\nposes is tedious and erroneous, and also fails to generalize to unseen\nscenarios. Besides, category-level pose estimation requires a method to be able\nto generalize to unseen objects at test time, which is also challenging.\nDrawing inspirations from traditional point pair features (PPFs), in this\npaper, we design a novel Category-level PPF (CPPF) voting method to achieve\naccurate, robust and generalizable 9D pose estimation in the wild. To obtain\nrobust pose estimation, we sample numerous point pairs on an object, and for\neach pair our model predicts necessary SE(3)-invariant voting statistics on\nobject centers, orientations and scales. A novel coarse-to-fine voting\nalgorithm is proposed to eliminate noisy point pair samples and generate final\npredictions from the population. To get rid of false positives in the\norientation voting process, an auxiliary binary disambiguating classification\ntask is introduced for each sampled point pair. In order to detect objects in\nthe wild, we carefully design our sim-to-real pipeline by training on synthetic\npoint clouds only, unless objects have ambiguous poses in geometry. Under this\ncircumstance, color information is leveraged to disambiguate these poses.\nResults on standard benchmarks show that our method is on par with current\nstate of the arts with real-world training data. Extensive experiments further\nshow that our method is robust to noise and gives promising results under\nextremely challenging scenarios. Our code is available on\nhttps://github.com/qq456cvb/CPPF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Ruoxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer. (arXiv:2203.03121v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03121","description":"<p>While deep face recognition (FR) systems have shown amazing performance in\nidentification and verification, they also arouse privacy concerns for their\nexcessive surveillance on users, especially for public face images widely\nspread on social networks. Recently, some studies adopt adversarial examples to\nprotect photos from being identified by unauthorized face recognition systems.\nHowever, existing methods of generating adversarial face images suffer from\nmany limitations, such as awkward visual, white-box setting, weak\ntransferability, making them difficult to be applied to protect face privacy in\nreality. In this paper, we propose adversarial makeup transfer GAN (AMT-GAN), a\nnovel face protection method aiming at constructing adversarial face images\nthat preserve stronger black-box transferability and better visual quality\nsimultaneously. AMT-GAN leverages generative adversarial networks (GAN) to\nsynthesize adversarial face images with makeup transferred from reference\nimages. In particular, we introduce a new regularization module along with a\njoint training strategy to reconcile the conflicts between the adversarial\nnoises and the cycle consistence loss in makeup transfer, achieving a desirable\nbalance between the attack strength and visual changes. Extensive experiments\nverify that compared with state of the arts, AMT-GAN can not only preserve a\ncomfortable visual quality, but also achieve a higher attack success rate over\ncommercial FR APIs, including Face++, Aliyun, and Microsoft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengshan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaogeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yechao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leo Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Libing Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Semi-Supervised Learning for Video Action Detection. (arXiv:2203.04251v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04251","description":"<p>In this work, we focus on semi-supervised learning for video action detection\nwhich utilizes both labeled as well as unlabeled data. We propose a simple\nend-to-end consistency based approach which effectively utilizes the unlabeled\ndata. Video action detection requires both, action class prediction as well as\na spatio-temporal localization of actions. Therefore, we investigate two types\nof constraints, classification consistency, and spatio-temporal consistency.\nThe presence of predominant background and static regions in a video makes it\nchallenging to utilize spatio-temporal consistency for action detection. To\naddress this, we propose two novel regularization constraints for\nspatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness.\nBoth these aspects exploit the temporal continuity of action in videos and are\nfound to be effective for utilizing unlabeled videos for action detection. We\ndemonstrate the effectiveness of the proposed approach on two different action\ndetection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show\nthe effectiveness of the proposed approach for video object segmentation on the\nYoutube-VOS which demonstrates its generalization capability The proposed\napproach achieves competitive performance by using merely 20% of annotations on\nUCF101-24 when compared with recent fully supervised methods. On UCF101-24, it\nimproves the score by +8.9% and +11% at 0.5 f-mAP and v-mAP respectively,\ncompared to supervised approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Akash Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh Singh Rawat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Trajectory Prediction via Transferable GNN. (arXiv:2203.05046v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05046","description":"<p>Pedestrian trajectory prediction is an essential component in a wide range of\nAI applications such as autonomous driving and robotics. Existing methods\nusually assume the training and testing motions follow the same pattern while\nignoring the potential distribution differences (e.g., shopping mall and\nstreet). This issue results in inevitable performance decrease. To address this\nissue, we propose a novel Transferable Graph Neural Network (T-GNN) framework,\nwhich jointly conducts trajectory prediction as well as domain alignment in a\nunified framework. Specifically, a domain-invariant GNN is proposed to explore\nthe structural motion knowledge where the domain-specific knowledge is reduced.\nMoreover, an attention-based adaptive knowledge learning module is further\nproposed to explore fine-grained individual-level feature representations for\nknowledge transfer. By this way, disparities across different trajectory\ndomains will be better alleviated. More challenging while practical trajectory\nprediction experiments are designed, and the experimental results verify the\nsuperior performance of our proposed model. To the best of our knowledge, our\nwork is the pioneer which fills the gap in benchmarks and techniques for\npractical pedestrian trajectory prediction across different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05154","description":"<p>Defense models against adversarial attacks have grown significantly, but the\nlack of practical evaluation methods has hindered progress. Evaluation can be\ndefined as looking for defense models' lower bound of robustness given a budget\nnumber of iterations and a test dataset. A practical evaluation method should\nbe convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and\nreliable (i.e., approaching the lower bound of robustness). Towards this\ntarget, we propose a parameter-free Adaptive Auto Attack (A$^3$) evaluation\nmethod which addresses the efficiency and reliability in a test-time-training\nfashion. Specifically, by observing that adversarial examples to a specific\ndefense model follow some regularities in their starting points, we design an\nAdaptive Direction Initialization strategy to speed up the evaluation.\nFurthermore, to approach the lower bound of robustness under the budget number\nof iterations, we propose an online statistics-based discarding strategy that\nautomatically identifies and abandons hard-to-attack images. Extensive\nexperiments demonstrate the effectiveness of our A$^3$. Particularly, we apply\nA$^3$ to nearly 50 widely-used defense models. By consuming much fewer\niterations than existing methods, i.e., $1/10$ on average (10$\\times$ speed\nup), we achieve lower robust accuracy in all cases. Notably, we won\n$\\textbf{first place}$ out of 1681 teams in CVPR 2021 White-box Adversarial\nAttacks on Defense Models competitions with this method. Code is available at:\n$\\href{https://github.com/liuye6666/adaptive_auto_attack}{https://github.com/liuye6666/adaptive\\_auto\\_attack}$\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability. (arXiv:2203.05180v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05180","description":"<p>Large-scale pre-training has been proven to be crucial for various computer\nvision tasks. However, with the increase of pre-training data amount, model\narchitecture amount, and the private/inaccessible data, it is not very\nefficient or possible to pre-train all the model architectures on large-scale\ndatasets. In this work, we investigate an alternative strategy for\npre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP),\naiming to efficiently transfer the learned feature representation from existing\npre-trained models to new student models for future downstream tasks. We\nobserve that existing Knowledge Distillation (KD) methods are unsuitable\ntowards pre-training since they normally distill the logits that are going to\nbe discarded when transferred to downstream tasks. To resolve this problem, we\npropose a feature-based KD method with non-parametric feature dimension\naligning. Notably, our method performs comparably with supervised pre-training\ncounterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less\ndata and 5x less pre-training time. Code is available at\nhttps://github.com/CVMI-Lab/KDEP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruifei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement. (arXiv:2203.05238v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05238","description":"<p>In this paper, we propose a weakly-supervised approach for 3D object\ndetection, which makes it possible to train a strong 3D detector with\nposition-level annotations (i.e. annotations of object centers). In order to\nremedy the information loss from box annotations to centers, our method, namely\nBack to Reality (BR), makes use of synthetic 3D shapes to convert the weak\nlabels into fully-annotated virtual scenes as stronger supervision, and in turn\nutilizes the perfect virtual labels to complement and refine the real labels.\nSpecifically, we first assemble 3D shapes into physically reasonable virtual\nscenes according to the coarse scene layout extracted from position-level\nannotations. Then we go back to reality by applying a virtual-to-real domain\nadaptation method, which refine the weak labels and additionally supervise the\ntraining of detector with the virtual scenes. Furthermore, we propose a more\nchallenging benckmark for indoor 3D object detection with more diversity in\nobject sizes to better show the potential of BR. With less than 5% of the\nlabeling labor, we achieve comparable detection performance with some popular\nfully-supervised approaches on the widely used ScanNet dataset. Code is\navailable at: https://github.com/wyf-ACCEPT/BackToReality\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiuwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Font Shape-to-Impression Translation. (arXiv:2203.05808v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05808","description":"<p>Different fonts have different impressions, such as elegant, scary, and cool.\nThis paper tackles part-based shape-impression analysis based on the\nTransformer architecture, which is able to handle the correlation among local\nparts by its self-attention mechanism. This ability will reveal how\ncombinations of local parts realize a specific impression of a font. The\nversatility of Transformer allows us to realize two very different approaches\nfor the analysis, i.e., multi-label classification and translation. A\nquantitative evaluation shows that our Transformer-based approaches estimate\nthe font impressions from a set of local parts more accurately than other\napproaches. A qualitative evaluation then indicates the important local parts\nfor a specific impression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masaya Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection. (arXiv:2203.06398v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06398","description":"<p>Domain Adaptive Object Detection (DAOD) leverages a labeled domain to learn\nan object detector generalizing to a novel domain free of annotations. Recent\nadvances align class-conditional distributions by narrowing down cross-domain\nprototypes (class centers). Though great success,they ignore the significant\nwithin-class variance and the domain-mismatched semantics within the training\nbatch, leading to a sub-optimal adaptation. To overcome these challenges, we\npropose a novel SemantIc-complete Graph MAtching (SIGMA) framework for DAOD,\nwhich completes mismatched semantics and reformulates the adaptation with graph\nmatching. Specifically, we design a Graph-embedded Semantic Completion module\n(GSC) that completes mismatched semantics through generating hallucination\ngraph nodes in missing categories. Then, we establish cross-image graphs to\nmodel class-conditional distributions and learn a graph-guided memory bank for\nbetter semantic completion in turn. After representing the source and target\ndata as graphs, we reformulate the adaptation as a graph matching problem,\ni.e., finding well-matched node pairs across graphs to reduce the domain gap,\nwhich is solved with a novel Bipartite Graph Matching adaptor (BGM). In a\nnutshell, we utilize graph nodes to establish semantic-aware node affinity and\nleverage graph edges as quadratic constraints in a structure-aware matching\nloss, achieving fine-grained adaptation with a node-to-node graph matching.\nExtensive experiments verify that SIGMA outperforms existing works\nsignificantly. Our code is available at\nhttps://github.com/CityU-AIM-Group/SIGMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning. (arXiv:2203.06541v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06541","description":"<p>Heatmap regression methods have dominated face alignment area in recent years\nwhile they ignore the inherent relation between different landmarks. In this\npaper, we propose a Sparse Local Patch Transformer (SLPT) for learning the\ninherent relation. The SLPT generates the representation of each single\nlandmark from a local patch and aggregates them by an adaptive inherent\nrelation based on the attention mechanism. The subpixel coordinate of each\nlandmark is predicted independently based on the aggregated feature. Moreover,\na coarse-to-fine framework is further introduced to incorporate with the SLPT,\nwhich enables the initial landmarks to gradually converge to the target facial\nlandmarks using fine-grained features from dynamically resized local patches.\nExtensive experiments carried out on three popular benchmarks, including WFLW,\n300W and COFW, demonstrate that the proposed method works at the\nstate-of-the-art level with much less computational complexity by learning the\ninherent relation between facial landmarks. The code is available at the\nproject website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jiahao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+qu_W/0/1/0/all/0/1\">Weiwei qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenjian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Autoencoders for Point Cloud Self-supervised Learning. (arXiv:2203.06604v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06604","description":"<p>As a promising scheme of self-supervised learning, masked autoencoding has\nsignificantly advanced natural language processing and computer vision.\nInspired by this, we propose a neat scheme of masked autoencoders for point\ncloud self-supervised learning, addressing the challenges posed by point\ncloud's properties, including leakage of location information and uneven\ninformation density. Concretely, we divide the input point cloud into irregular\npoint patches and randomly mask them at a high ratio. Then, a standard\nTransformer based autoencoder, with an asymmetric design and a shifting mask\ntokens operation, learns high-level latent features from unmasked point\npatches, aiming to reconstruct the masked point patches. Extensive experiments\nshow that our approach is efficient during pre-training and generalizes well on\nvarious downstream tasks. Specifically, our pre-trained models achieve 85.18%\naccuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all\nthe other self-supervised learning methods. We show with our scheme, a simple\narchitecture entirely based on standard Transformers can surpass dedicated\nTransformer models from supervised learning. Our approach also advances\nstate-of-the-art accuracies by 1.5%-2.3% in the few-shot object classification.\nFurthermore, our work inspires the feasibility of applying unified\narchitectures from languages and images to the point cloud.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yatian Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_F/0/1/0/all/0/1\">Francis E.H. Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderived from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps of the semantic features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor perform poorly in the TAGV task. To bridge these gaps, we\npropose a visual-prompt text span localizing (VPTSL) method, which introduces\nthe timestamped subtitles as a passage to perform the text span localization\nfor the input text question, and prompts the visual highlight features into the\npre-trained language model (PLM) for enhancing the joint semantic\nrepresentations. Specifically, the context query attention is utilized to\nperform cross-modal interaction between the extracted textual and visual\nfeatures. Then, the highlight features are obtained through the video-text\nhighlighting for the visual prompt. To alleviate semantic differences between\ntextual and visual features, we design the text span predictor by encoding the\nquestion, the subtitles, and the prompted visual highlight features with the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms the\nstate-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,\nwhich demonstrates the effectiveness of the proposed visual prompt and the text\nspan predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.07996","description":"<p>Training Transformer-based models demands a large amount of data, while\nobtaining aligned and labelled data in multimodality is rather cost-demanding,\nespecially for audio-visual speech recognition (AVSR). Thus it makes a lot of\nsense to make use of unlabelled unimodal data. On the other side, although the\neffectiveness of large-scale self-supervised learning is well established in\nboth audio and visual modalities, how to integrate those pre-trained models\ninto a multimodal scenario remains underexplored. In this work, we successfully\nleverage unimodal self-supervised learning to promote the multimodal AVSR. In\nparticular, audio and visual front-ends are trained on large-scale unimodal\ndatasets, then we integrate components of both front-ends into a larger\nmultimodal framework which learns to recognize parallel audio-visual data into\ncharacters through a combination of CTC and seq2seq decoding. We show that both\ncomponents inherited from unimodal self-supervised learning cooperate well,\nresulting in that the multimodal framework yields competitive results through\nfine-tuning. Our model is experimentally validated on both word-level and\nsentence-level tasks. Especially, even without an external language model, our\nproposed model raises the state-of-the-art performances on the widely accepted\nLip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative\nimprovement of 30%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xichen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yichen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Helong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnseenNet: Fast Training Detector for Any Unseen Concept. (arXiv:2203.08759v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08759","description":"<p>Training of object detection models using less data is currently the focus of\nexisting N-shot learning models in computer vision. Such methods use\nobject-level labels and takes hours to train on unseen classes. There are many\ncases where we have large amount of image-level labels available for training\nbut cannot be utilized by few shot object detection models for training. There\nis a need for a machine learning framework that can be used for training any\nunseen class and can become useful in real-time situations. In this paper, we\nproposed an \"Unseen Class Detector\" that can be trained within a very short\ntime for any possible unseen class without bounding boxes with competitive\naccuracy. We build our approach on \"Strong\" and \"Weak\" baseline detectors,\nwhich we trained on existing object detection and image classification\ndatasets, respectively. Unseen concepts are fine-tuned on the strong baseline\ndetector using only image-level labels and further adapted by transferring the\nclassifier-detector knowledge between baselines. We use semantic as well as\nvisual similarities to identify the source class (i.e. Sheep) for the\nfine-tuning and adaptation of unseen class (i.e. Goat). Our model (UnseenNet)\nis trained on the ImageNet classification dataset for unseen classes and tested\non an object detection dataset (OpenImages). UnseenNet improves the mean\naverage precision (mAP) by 10% to 30% over existing baselines (semi-supervised\nand few-shot) of object detection on different unseen class splits. Moreover,\ntraining time of our model is &lt;10 min for each unseen class. Qualitative\nresults demonstrate that UnseenNet is suitable not only for few classes of\nPascal VOC but for unseen classes of any dataset or web. Code is available at\nhttps://github.com/Asra-Aslam/UnseenNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aslam_A/0/1/0/all/0/1\">Asra Aslam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curry_E/0/1/0/all/0/1\">Edward Curry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing Global Explanations of Point Cloud DNNs. (arXiv:2203.09505v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09505","description":"<p>In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose a point cloud-applicable explainability approach\nbased on a local surrogate model-based method to show which components\ncontribute to the classification. Moreover, we propose quantitative fidelity\nvalidations for generated explanations that enhance the persuasive power of\nexplainability and compare the plausibility of different existing point\ncloud-applicable explainability methods. Our new explainability approach\nprovides a fairly accurate, more semantically coherent and widely applicable\nexplanation for point cloud classification tasks. Our code is available at\nhttps://github.com/Explain3D/LIME-3D\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance. (arXiv:2203.09887v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09887","description":"<p>Transformers have gained much attention by outperforming convolutional neural\nnetworks in many 2D vision tasks. However, they are known to have\ngeneralization problems and rely on massive-scale pre-training and\nsophisticated training techniques. When applying to 3D tasks, the irregular\ndata structure and limited data scale add to the difficulty of transformer's\napplication. We propose CodedVTR (Codebook-based Voxel TRansformer), which\nimproves data efficiency and generalization ability for 3D sparse voxel\ntransformers. On the one hand, we propose the codebook-based attention that\nprojects an attention space into its subspace represented by the combination of\n\"prototypes\" in a learnable codebook. It regularizes attention learning and\nimproves generalization. On the other hand, we propose geometry-aware\nself-attention that utilizes geometric information (geometric pattern, density)\nto guide attention learning. CodedVTR could be embedded into existing sparse\nconvolution-based methods, and bring consistent performance improvements for\nindoor and outdoor 3D semantic segmentation tasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Niansong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking. (arXiv:2203.09948v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09948","description":"<p>Situation-aware technologies enabled by multiobject tracking (MOT) methods\nwill create new services and applications in fields such as autonomous\nnavigation and applied ocean sciences. Belief propagation (BP) is a\nstate-of-the-art method for Bayesian MOT but fully relies on a statistical\nmodel and preprocessed sensor measurements. In this paper, we establish a\nhybrid method for model-based and data-driven MOT. The proposed neural enhanced\nbelief propagation (NEBP) approach complements BP by information learned from\nraw sensor data with the goal to improve data association and to reject false\nalarm measurements. We evaluate the performance of our NEBP approach for MOT on\nthe nuScenes autonomous driving dataset and demonstrate that it can outperform\nstate-of-the-art reference methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingchao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_F/0/1/0/all/0/1\">Florian Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion and Volume Maximization-Based Clustering of Highly Mixed Hyperspectral Images. (arXiv:2203.09992v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09992","description":"<p>Hyperspectral images of a scene or object are a rich data source, often\nencoding a hundred or more spectral bands of reflectance at each pixel. Despite\nbeing very high-dimensional, these images typically encode latent\nlow-dimensional structure that can be exploited for material discrimination.\nHowever, due to an inherent trade-off between spectral and spatial resolution,\nmany hyperspectral images are generated at a coarse spatial scale, and single\npixels may correspond to spatial regions containing multiple materials. This\narticle introduces the Diffusion and Volume maximization-based Image Clustering\n(D-VIC) algorithm for unsupervised material discrimination. D-VIC locates\ncluster modes - high-density, high-purity pixels in the hyperspectral image\nthat are far in diffusion distance (a data-dependent distance metric) from\nother high-density, high-purity pixels - and assigns these pixels unique\nlabels, as these points are meant to exemplify underlying material structure.\nNon-modal pixels are labeled according to their diffusion distance nearest\nneighbor of higher density and purity that is already labeled. By directly\nincorporating pixel purity into its modal and non-modal labeling, D-VIC\nupweights pixels that correspond to a spatial region containing just a single\nmaterial, yielding more interpretable clusterings. D-VIC is shown to outperform\nbaseline and comparable state-of-the-art methods in extensive numerical\nexperiments on a range of hyperspectral images, implying that it is\nwell-equipped for material discrimination and clustering of these data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1\">Sam L. Polk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kangning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plemmons_R/0/1/0/all/0/1\">Robert J. Plemmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1\">James M. Murphy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation. (arXiv:2203.10026v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10026","description":"<p>Semi-supervised semantic segmentation learns from small amounts of labelled\nimages and large amounts of unlabelled images, which has witnessed impressive\nprogress with the recent advance of deep neural networks. However, it often\nsuffers from severe class-bias problem while exploring the unlabelled images,\nlargely due to the clear pixel-wise class imbalance in the labelled images.\nThis paper presents an unbiased subclass regularization network (USRN) that\nalleviates the class imbalance issue by learning class-unbiased segmentation\nfrom balanced subclass distributions. We build the balanced subclass\ndistributions by clustering pixels of each original class into multiple\nsubclasses of similar sizes, which provide class-balanced pseudo supervision to\nregularize the class-biased segmentation. In addition, we design an\nentropy-based gate mechanism to coordinate learning between the original\nclasses and the clustered subclasses which facilitates subclass regularization\neffectively by suppressing unconfident subclass predictions. Extensive\nexperiments over multiple public benchmarks show that USRN achieves superior\nperformance as compared with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TO-FLOW: Efficient Continuous Normalizing Flows with Temporal Optimization adjoint with Moving Speed. (arXiv:2203.10335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10335","description":"<p>Continuous normalizing flows (CNFs) construct invertible mappings between an\narbitrary complex distribution and an isotropic Gaussian distribution using\nNeural Ordinary Differential Equations (neural ODEs). It has not been tractable\non large datasets due to the incremental complexity of the neural ODE training.\nOptimal Transport theory has been applied to regularize the dynamics of the ODE\nto speed up training in recent works. In this paper, a temporal optimization is\nproposed by optimizing the evolutionary time for forward propagation of the\nneural ODE training. In this appoach, we optimize the network weights of the\nCNF alternately with evolutionary time by coordinate descent. Further with\ntemporal regularization, stability of the evolution is ensured. This approach\ncan be used in conjunction with the original regularization approach. We have\nexperimentally demonstrated that the proposed approach can significantly\naccelerate training without sacrifying performance over baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shian Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yihong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Delu Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.10417","description":"<p>Deep learning (DL) methods where interpretability is intrinsically considered\nas part of the model are required to better understand the relationship of\nclinical and imaging-based attributes with DL outcomes, thus facilitating their\nuse in reasoning medical decisions. Latent space representations built with\nvariational autoencoders (VAE) do not ensure individual control of data\nattributes. Attribute-based methods enforcing attribute disentanglement have\nbeen proposed in the literature for classical computer vision tasks in\nbenchmark data. In this paper, we propose a VAE approach, the Attri-VAE, that\nincludes an attribute regularization term to associate clinical and medical\nimaging attributes with different regularized dimensions in the generated\nlatent space, enabling a better disentangled interpretation of the attributes.\nFurthermore, the generated attention maps explained the attribute encoding in\nthe regularized latent space dimensions. The Attri-VAE approach analyzed\nhealthy and myocardial infarction patients with clinical, cardiac morphology,\nand radiomics attributes. The proposed model provided an excellent trade-off\nbetween reconstruction fidelity, disentanglement, and interpretability,\noutperforming state-of-the-art VAE approaches according to several quantitative\nmetrics. The resulting latent space allowed the generation of realistic\nsynthetic data in the trajectory between two distinct input samples or along a\nspecific attribute dimension to better interpret changes between different\ncardiac conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cetin_I/0/1/0/all/0/1\">Irem Cetin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Camara_O/0/1/0/all/0/1\">Oscar Camara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballester_M/0/1/0/all/0/1\">Miguel Angel Gonzalez Ballester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breast Cancer Induced Bone Osteolysis Prediction Using Temporal Variational Auto-Encoders. (arXiv:2203.10645v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.10645","description":"<p>Objective and Impact Statement. We adopt a deep learning model for bone\nosteolysis prediction on computed tomography (CT) images of murine breast\ncancer bone metastases. Given the bone CT scans at previous time steps, the\nmodel incorporates the bone-cancer interactions learned from the sequential\nimages and generates future CT images. Its ability of predicting the\ndevelopment of bone lesions in cancer-invading bones can assist in assessing\nthe risk of impending fractures and choosing proper treatments in breast cancer\nbone metastasis. Introduction. Breast cancer often metastasizes to bone, causes\nosteolytic lesions, and results in skeletal related events (SREs) including\nsevere pain and even fatal fractures. Although current imaging techniques can\ndetect macroscopic bone lesions, predicting the occurrence and progression of\nbone lesions remains a challenge. Methods. We adopt a temporal variational\nauto-encoder (T-VAE) model that utilizes a combination of variational\nauto-encoders and long short-term memory networks to predict bone lesion\nemergence on our micro-CT dataset containing sequential images of murine\ntibiae. Given the CT scans of murine tibiae at early weeks, our model can learn\nthe distribution of their future states from data. Results. We test our model\nagainst other deep learning-based prediction models on the bone lesion\nprogression prediction task. Our model produces much more accurate predictions\nthan existing models under various evaluation metrics. Conclusion. We develop a\ndeep learning framework that can accurately predict and visualize the\nprogression of osteolytic bone lesions. It will assist in planning and\nevaluating treatment strategies to prevent SREs in breast cancer patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xiong_W/0/1/0/all/0/1\">Wei Xiong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_N/0/1/0/all/0/1\">Neil Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shubo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liyun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boost Test-Time Performance with Closed-Loop Inference. (arXiv:2203.10853v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10853","description":"<p>Conventional deep models predict a test sample with a single forward\npropagation, which, however, may not be sufficient for predicting\nhard-classified samples. On the contrary, we human beings may need to carefully\ncheck the sample many times before making a final decision. During the recheck\nprocess, one may refine/adjust the prediction by referring to related samples.\nMotivated by this, we propose to predict those hard-classified test samples in\na looped manner to boost the model performance. However, this idea may pose a\ncritical challenge: how to construct looped inference, so that the original\nerroneous predictions on these hard test samples can be corrected with little\nadditional effort. To address this, we propose a general Closed-Loop Inference\n(CLI) method. Specifically, we first devise a filtering criterion to identify\nthose hard-classified test samples that need additional inference loops. For\neach hard sample, we construct an additional auxiliary learning task based on\nits original top-$K$ predictions to calibrate the model, and then use the\ncalibrated model to obtain the final prediction. Promising results on ImageNet\n(in-distribution test samples) and ImageNet-C (out-of-distribution test\nsamples) demonstrate the effectiveness of CLI in improving the performance of\nany pre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guanghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haokun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer. (arXiv:2203.10981v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10981","description":"<p>Monocular 3D object detection is an important yet challenging task in\nautonomous driving. Some existing methods leverage depth information from an\noff-the-shelf depth estimator to assist 3D detection, but suffer from the\nadditional computational burden and achieve limited performance caused by\ninaccurate depth priors. To alleviate this, we propose MonoDTR, a novel\nend-to-end depth-aware transformer network for monocular 3D object detection.\nIt mainly consists of two components: (1) the Depth-Aware Feature Enhancement\n(DFE) module that implicitly learns depth-aware features with auxiliary\nsupervision without requiring extra computation, and (2) the Depth-Aware\nTransformer (DTR) module that globally integrates context- and depth-aware\nfeatures. Moreover, different from conventional pixel-wise positional\nencodings, we introduce a novel depth positional encoding (DPE) to inject depth\npositional hints into transformers. Our proposed depth-aware modules can be\neasily plugged into existing image-only monocular 3D object detectors to\nimprove the performance. Extensive experiments on the KITTI dataset demonstrate\nthat our approach outperforms previous state-of-the-art monocular-based methods\nand achieves real-time detection. Code is available at\nhttps://github.com/kuanchihhuang/MonoDTR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Chih Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Neural Vernier Caliper. (arXiv:2203.11018v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11018","description":"<p>We propose a new object-centric framework for learning-based stereo 3D object\ndetection. Previous studies build scene-centric representations that do not\nconsider the significant variation among outdoor instances and thus lack the\nflexibility and functionalities that an instance-level model can offer. We\nbuild such an instance-level model by formulating and tackling a local update\nproblem, i.e., how to predict a refined update given an initial 3D cuboid\nguess. We demonstrate how solving this problem can complement scene-centric\napproaches in (i) building a coarse-to-fine multi-resolution system, (ii)\nperforming model-agnostic object location refinement, and (iii) conducting\nstereo 3D tracking-by-detection. Extensive experiments demonstrate the\neffectiveness of our approach, which achieves state-of-the-art performance on\nthe KITTI benchmark. Code and pre-trained models are available at\nhttps://github.com/Nicholasli1995/SNVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11442","description":"<p>This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computation resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\njointly and collaboratively. In detail, AOT employs an identification mechanism\nto associate multiple targets into the same high-dimensional embedding space.\nThus, we can simultaneously process multiple objects' matching and segmentation\ndecoding as efficiently as processing a single object. To sufficiently model\nmulti-object association, a Long Short-Term Transformer (LSTT) is devised to\nconstruct hierarchical matching and propagation. Based on AOT, we further\npropose a more flexible and robust framework, Associating Objects with Scalable\nTransformers (AOST), in which a scalable version of LSTT is designed to enable\nrun-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces\na better layer-wise manner to couple identification and vision embeddings. We\nconduct extensive experiments on multi-object and single-object benchmarks to\nexamine AOT series frameworks. Compared to the state-of-the-art competitors,\nour methods can maintain times of run-time efficiency with superior\nperformance. Notably, we achieve new state-of-the-art performance on three\npopular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test\n(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:\nhttps://github.com/z-x-yang/AOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jiaxu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11819","description":"<p>Deep models must learn robust and transferable representations in order to\nperform well on new domains. While domain transfer methods (e.g., domain\nadaptation, domain generalization) have been proposed to learn transferable\nrepresentations across domains, they are typically applied to ResNet backbones\npre-trained on ImageNet. Thus, existing works pay little attention to the\neffects of pre-training on domain transfer tasks. In this paper, we provide a\nbroad study and in-depth analysis of pre-training for domain adaptation and\ngeneralization, namely: network architectures, size, pre-training loss, and\ndatasets. We observe that simply using a state-of-the-art backbone outperforms\nexisting state-of-the-art domain adaptation baselines and set new baselines on\nOffice-Home and DomainNet improving by 10.7\\% and 5.5\\%. We hope that this work\ncan provide more insights for future domain transfer research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11894","description":"<p>In this work we demonstrate the vulnerability of vision transformers (ViTs)\nto gradient-based inversion attacks. During this attack, the original data\nbatch is reconstructed given model weights and the corresponding gradients. We\nintroduce a method, named GradViT, that optimizes random noise into naturally\nlooking images via an iterative process. The optimization objective consists of\n(i) a loss on matching the gradients, (ii) image prior in the form of distance\nto batch-normalization statistics of a pretrained CNN model, and (iii) a total\nvariation regularization on patches to guide correct recovery locations. We\npropose a unique loss scheduling function to overcome local minima during\noptimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and\nobserve unprecedentedly high fidelity and closeness to the original (hidden)\ndata. During the analysis we find that vision transformers are significantly\nmore vulnerable than previously studied CNNs due to the presence of the\nattention mechanism. Our method demonstrates new state-of-the-art results for\ngradient inversion in both qualitative and quantitative metrics. Project page\nat https://gradvit.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-Equivalent Distillation for Semi-Supervised Object Detection. (arXiv:2203.12244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12244","description":"<p>Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on\nself-training, i.e., generating hard pseudo-labels by a teacher model on\nunlabeled data as supervisory signals. Although they achieved certain success,\nthe limited labeled data in semi-supervised learning scales up the challenges\nof object detection. We analyze the challenges these methods meet with the\nempirical experiment results. We find that the massive False Negative samples\nand inferior localization precision lack consideration. Besides, the large\nvariance of object sizes and class imbalance (i.e., the extreme ratio between\nbackground and object) hinder the performance of prior arts. Further, we\novercome these challenges by introducing a novel approach, Scale-Equivalent\nDistillation (SED), which is a simple yet effective end-to-end knowledge\ndistillation framework robust to large object size variance and class\nimbalance. SED has several appealing benefits compared to the previous works.\n(1) SED imposes a consistency regularization to handle the large scale variance\nproblem. (2) SED alleviates the noise problem from the False Negative samples\nand inferior localization precision. (3) A re-weighting strategy can implicitly\nscreen the potential foreground regions of the unlabeled data to reduce the\neffect of class imbalance. Extensive experiments show that SED consistently\noutperforms the recent state-of-the-art methods on different datasets with\nsignificant margins. For example, it surpasses the supervised counterpart by\nmore than 10 mAP when using 5% and 10% labeled data on MS-COCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qiushan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yao Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ev-TTA: Test-Time Adaptation for Event-Based Object Recognition. (arXiv:2203.12247v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12247","description":"<p>We introduce Ev-TTA, a simple, effective test-time adaptation algorithm for\nevent-based object recognition. While event cameras are proposed to provide\nmeasurements of scenes with fast motions or drastic illumination changes, many\nexisting event-based recognition algorithms suffer from performance\ndeterioration under extreme conditions due to significant domain shifts. Ev-TTA\nmitigates the severe domain gaps by fine-tuning the pre-trained classifiers\nduring the test phase using loss functions inspired by the spatio-temporal\ncharacteristics of events. Since the event data is a temporal stream of\nmeasurements, our loss function enforces similar predictions for adjacent\nevents to quickly adapt to the changed environment online. Also, we utilize the\nspatial correlations between two polarities of events to handle noise under\nextreme illumination, where different polarities of events exhibit distinctive\nnoise distributions. Ev-TTA demonstrates a large amount of performance gain on\na wide range of event-based object recognition tasks without extensive\nadditional training. Our formulation can be successfully applied regardless of\ninput representations and further extended into regression tasks. We expect\nEv-TTA to provide the key technique to deploy event-based vision algorithms in\nchallenging real-world applications where significant domain shift is\ninevitable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inwoo Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DR.VIC: Decomposition and Reasoning for Video Individual Counting. (arXiv:2203.12335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12335","description":"<p>Pedestrian counting is a fundamental tool for understanding pedestrian\npatterns and crowd flow analysis. Existing works (e.g., image-level pedestrian\ncounting, crossline crowd counting et al.) either only focus on the image-level\ncounting or are constrained to the manual annotation of lines. In this work, we\npropose to conduct the pedestrian counting from a new perspective - Video\nIndividual Counting (VIC), which counts the total number of individual\npedestrians in the given video (a person is only counted once). Instead of\nrelying on the Multiple Object Tracking (MOT) techniques, we propose to solve\nthe problem by decomposing all pedestrians into the initial pedestrians who\nexisted in the first frame and the new pedestrians with separate identities in\neach following frame. Then, an end-to-end Decomposition and Reasoning Network\n(DRNet) is designed to predict the initial pedestrian count with the density\nestimation method and reason the new pedestrian's count of each frame with the\ndifferentiable optimal transport. Extensive experiments are conducted on two\ndatasets with congested pedestrians and diverse scenes, demonstrating the\neffectiveness of our method over baselines with great superiority in counting\nthe individual pedestrians. Code: https://github.com/taohan10200/DRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label Transformer for Action Unit Detection. (arXiv:2203.12531v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12531","description":"<p>Action Unit (AU) Detection is the branch of affective computing that aims at\nrecognizing unitary facial muscular movements. It is key to unlock unbiased\ncomputational face representations and has therefore aroused great interest in\nthe past few years. One of the main obstacles toward building efficient deep\nlearning based AU detection system is the lack of wide facial image databases\nannotated by AU experts. In that extent the ABAW challenge paves the way toward\nbetter AU detection as it involves a 2M frames AU annotated dataset. In this\npaper, we present our submission to the ABAW3 challenge. In a nutshell, we\napplied a multi-label detection transformer that leverage multi-head attention\nto learn which part of the face image is the most relevant to predict each AU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tallec_G/0/1/0/all/0/1\">Gauthier Tallec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvinec_E/0/1/0/all/0/1\">Edouard Yvinec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1\">Kevin Bailly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StructToken : Rethinking Semantic Segmentation with Structural Prior. (arXiv:2203.12612v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12612","description":"<p>In this paper, we present structure token (StructToken), a new paradigm for\nsemantic segmentation. From a perspective on semantic segmentation as per-pixel\nclassification, the previous deep learning-based methods learn the per-pixel\nrepresentation first through an encoder and a decoder head and then classify\neach pixel representation to a specific category to obtain the semantic masks.\nDifferently, we propose a structure-aware algorithm that takes structural\ninformation as prior to predict semantic masks directly without per-pixel\nclassification. Specifically, given an input image, the learnable structure\ntoken interacts with the image representations to reason the final semantic\nmasks. Three interaction approaches are explored and the results not only\noutperform the state-of-the-art methods but also contain more structural\ninformation. Experiments are conducted on three widely used datasets including\nADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could\nserve as an alternative for semantic segmentation and inspire future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhanhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Miao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to generate line drawings that convey geometry and semantics. (arXiv:2203.12691v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12691","description":"<p>This paper presents an unpaired method for creating line drawings from\nphotographs. Current methods often rely on high quality paired datasets to\ngenerate line drawings. However, these datasets often have limitations due to\nthe subjects of the drawings belonging to a specific domain, or in the amount\nof data collected. Although recent work in unsupervised image-to-image\ntranslation has shown much progress, the latest methods still struggle to\ngenerate compelling line drawings. We observe that line drawings are encodings\nof scene information and seek to convey 3D shape and semantic meaning. We build\nthese observations into a set of objectives and train an image translation to\nmap photographs into line drawings. We introduce a geometry loss which predicts\ndepth information from the image features of a line drawing, and a semantic\nloss which matches the CLIP features of a line drawing with its corresponding\nphotograph. Our approach outperforms state-of-the-art unpaired image\ntranslation and line drawing generation methods on creating line drawings from\narbitrary photographs. For code and demo visit our webpage\ncarolineec.github.io/informative_drawings\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Caroline Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fredo Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Challenges of Continuous Self-Supervised Learning. (arXiv:2203.12710v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12710","description":"<p>Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks\nin representation learning - the need for human annotations. As a result, SSL\nholds the promise to learn representations from data in-the-wild, i.e., without\nthe need for finite and static datasets. Instead, true SSL algorithms should be\nable to exploit the continuous stream of data being generated on the internet\nor by agents exploring their environments. But do traditional self-supervised\nlearning approaches work in this setup? In this work, we investigate this\nquestion by conducting experiments on the continuous self-supervised learning\nproblem. While learning in the wild, we expect to see a continuous (infinite)\nnon-IID data stream that follows a non-stationary distribution of visual\nconcepts. The goal is to learn a representation that can be robust, adaptive\nyet not forgetful of concepts seen in the past. We show that a direct\napplication of current methods to such continuous setup is 1) inefficient both\ncomputationally and in the amount of data required, 2) leads to inferior\nrepresentations due to temporal correlations (non-IID data) in some sources of\nstreaming data and 3) exhibits signs of catastrophic forgetting when trained on\nsources with non-stationary data distributions. We propose the use of replay\nbuffers as an approach to alleviate the issues of inefficiency and temporal\ncorrelations. We further propose a novel method to enhance the replay buffer by\nmaintaining the least redundant samples. Minimum redundancy (MinRed) buffers\nallow us to learn effective representations even in the most challenging\nstreaming scenarios composed of sequential visual data obtained from a single\nembodied agent, and alleviates the problem of catastrophic forgetting when\nlearning from data with non-stationary semantic distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1\">Senthil Purushwalkam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1\">Pedro Morgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection. (arXiv:2203.12745v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12745","description":"<p>Finding relevant moments and highlights in videos according to natural\nlanguage queries is a natural and highly valuable common need in the current\nvideo content explosion era. Nevertheless, jointly conducting moment retrieval\nand highlight detection is an emerging research topic, even though its\ncomponent problems and some related tasks have already been studied for a\nwhile. In this paper, we present the first unified framework, named Unified\nMulti-modal Transformers (UMT), capable of realizing such joint optimization\nwhile can also be easily degenerated for solving individual problems. As far as\nwe are aware, this is the first scheme to integrate multi-modal (visual-audio)\nlearning for either joint optimization or the individual moment retrieval task,\nand tackles moment retrieval as a keypoint detection problem using a novel\nquery generator and query decoder. Extensive comparisons with existing methods\nand ablation studies on QVHighlights, Charades-STA, YouTube Highlights, and\nTVSum datasets demonstrate the effectiveness, superiority, and flexibility of\nthe proposed method under various settings. Source code and pre-trained models\nare available at https://github.com/TencentARC/UMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12861","description":"<p>Convolutional neural networks (CNN) have demonstrated outstanding Compressed\nSensing (CS) performance compared to traditional, hand-crafted methods.\nHowever, they are broadly limited in terms of generalisability, inductive bias\nand difficulty to model long distance relationships. Transformer neural\nnetworks (TNN) overcome such issues by implementing an attention mechanism\ndesigned to capture dependencies between inputs. However, high-resolution tasks\ntypically require vision Transformers (ViT) to decompose an image into\npatch-based tokens, limiting inputs to inherently local contexts. We propose a\nnovel image decomposition that naturally embeds images into low-resolution\ninputs. These Kaleidoscope tokens (KD) provide a mechanism for global\nattention, at the same computational cost as a patch-based approach. To\nshowcase this development, we replace CNN components in a well-known CS-MRI\nneural network with TNN blocks and demonstrate the improvements afforded by KD.\nWe also propose an ensemble of image tokens, which enhance overall image\nquality and reduces model size. Supplementary material is available:\nhttps://github.com/uqmarlonbran/TCS.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenzana_M/0/1/0/all/0/1\">Marlon Bran Lorenzana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engstrom_C/0/1/0/all/0/1\">Craig Engstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1\">Shekhar S. Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks. (arXiv:2203.12915v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.12915","description":"<p>Deep learning has recently been widely applied to many applications across\ndifferent domains, e.g., image classification and audio recognition. However,\nthe quality of Deep Neural Networks (DNNs) still raises concerns in the\npractical operational environment, which calls for systematic testing,\nespecially in safety-critical scenarios. Inspired by software testing, a number\nof structural coverage criteria are designed and proposed to measure the test\nadequacy of DNNs. However, due to the blackbox nature of DNN, the existing\nstructural coverage criteria are difficult to interpret, making it hard to\nunderstand the underlying principles of these criteria. The relationship\nbetween the structural coverage and the decision logic of DNNs is unknown.\nMoreover, recent studies have further revealed the non-existence of correlation\nbetween the structural coverage and DNN defect detection, which further posts\nconcerns on what a suitable DNN testing criterion should be.\n</p>\n<p>In this paper, we propose the interpretable coverage criteria through\nconstructing the decision structure of a DNN. Mirroring the control flow graph\nof the traditional program, we first extract a decision graph from a DNN based\non its interpretation, where a path of the decision graph represents a decision\nlogic of the DNN. Based on the control flow and data flow of the decision\ngraph, we propose two variants of path coverage to measure the adequacy of the\ntest cases in exercising the decision logic. The higher the path coverage, the\nmore diverse decision logic the DNN is expected to be explored. Our large-scale\nevaluation results demonstrate that: the path in the decision graph is\neffective in characterizing the decision of the DNN, and the proposed coverage\ncriteria are also sensitive with errors including natural errors and\nadversarial examples, and strongly correlated with the output impartiality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. (arXiv:2203.13009v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13009","description":"<p>Recently, significant progress has been made on image denoising with strong\nsupervision from large-scale datasets. However, obtaining well-aligned\nnoisy-clean training image pairs for each specific scenario is complicated and\ncostly in practice. Consequently, applying a conventional supervised denoising\nnetwork on in-the-wild noisy inputs is not straightforward. Although several\nstudies have challenged this problem without strong supervision, they rely on\nless practical assumptions and cannot be applied to practical situations\ndirectly. To address the aforementioned challenges, we propose a novel and\npowerful self-supervised denoising method called CVF-SID based on a Cyclic\nmulti-Variate Function (CVF) module and a self-supervised image disentangling\n(SID) framework. The CVF module can output multiple decomposed variables of the\ninput and take a combination of the outputs back as an input in a cyclic\nmanner. Our CVF-SID can disentangle a clean image and noise maps from the input\nby leveraging various self-supervised loss terms. Unlike several methods that\nonly consider the signal-independent noise models, we also deal with\nsignal-dependent noise components for real-world applications. Furthermore, we\ndo not rely on any prior assumptions about the underlying noise distribution,\nmaking CVF-SID more generalizable toward realistic noise. Extensive experiments\non real-world datasets show that CVF-SID achieves state-of-the-art\nself-supervised image denoising performance and is comparable to other existing\napproaches. The code is publicly available from\nhttps://github.com/Reyhanehne/CVF-SID_PyTorch .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neshatavar_R/0/1/0/all/0/1\">Reyhaneh Neshatavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavartanoo_M/0/1/0/all/0/1\">Mohsen Yavartanoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13032","description":"<p>In this paper, we briefly introduce our submission to the Valence-Arousal\nEstimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)\ncompetition. Our method utilizes the multi-modal information, i.e., the visual\nand audio information, and employs a temporal encoder to model the temporal\ncontext in the videos. Besides, a smooth processor is applied to get more\nreasonable predictions, and a model ensemble strategy is used to improve the\nperformance of our proposed method. The experiment results show that our method\nachieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation\nset of the Aff-Wild2 dataset, which prove the effectiveness of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Liyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaopei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tenggan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning. (arXiv:2203.13049v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13049","description":"<p>Temporal grounding in videos aims to localize one target video segment that\nsemantically corresponds to a given query sentence. Thanks to the semantic\ndiversity of natural language descriptions, temporal grounding allows activity\ngrounding beyond pre-defined classes and has received increasing attention in\nrecent years. The semantic diversity is rooted in the principle of\ncompositionality in linguistics, where novel semantics can be systematically\ndescribed by combining known words in novel ways (compositional\ngeneralization). However, current temporal grounding datasets do not\nspecifically test for the compositional generalizability. To systematically\nmeasure the compositional generalizability of temporal grounding models, we\nintroduce a new Compositional Temporal Grounding task and construct two new\ndataset splits, i.e., Charades-CG and ActivityNet-CG. Evaluating the\nstate-of-the-art methods on our new dataset splits, we empirically find that\nthey fail to generalize to queries with novel combinations of seen words. To\ntackle this challenge, we propose a variational cross-graph reasoning framework\nthat explicitly decomposes video and language into multiple structured\nhierarchies and learns fine-grained semantic correspondence among them.\nExperiments illustrate the superior compositional generalizability of our\napproach. The repository of this work is at https://github.com/YYJMJC/\nCompositional-Temporal-Grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Junlin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Long Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13052","description":"<p>Facial expression recognition plays an important role in human-computer\ninteraction. In this paper, we propose the Coarse-to-Fine Cascaded network with\nSmooth Predicting (CFC-SP) to improve the performance of facial expression\nrecognition. CFC-SP contains two core components, namely Coarse-to-Fine\nCascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups\nseveral similar emotions to form a rough category, and then employs a network\nto conduct a coarse but accurate classification. Later, an additional network\nfor these grouped emotions is further used to obtain fine-grained predictions.\nFor SP, it improves the recognition capability of the model by capturing both\nuniversal and unique expression features. To be specific, the universal\nfeatures denote the general characteristic of facial emotions within a period\nand the unique features denote the specific characteristic at this moment.\nExperiments on Aff-Wild2 show the effectiveness of the proposed CFSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fanglei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zichang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhongsong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13167","description":"<p>In this paper, we investigate the continual learning of Vision Transformers\n(ViT) for the challenging exemplar-free scenario, with special focus on how to\nefficiently distill the knowledge of its crucial self-attention mechanism\n(SAM). Our work takes an initial step towards a surgical investigation of SAM\nfor designing coherent continual learning methods in ViTs. We first carry out\nan evaluation of established continual learning regularization techniques. We\nthen examine the effect of regularization when applied to two key enablers of\nSAM: (a) the contextualized embedding layers, for their ability to capture\nwell-scaled representations with respect to the values, and (b) the prescaled\nattention maps, for carrying value-independent global contextual information.\nWe depict the perks of each distilling strategy on two image recognition\nbenchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall\naccuracy, (b) helps enhance the rigidity by maintaining competitive\nperformances. Furthermore, we identify the limitation imposed by the symmetric\nnature of regularization losses. To alleviate this, we propose an asymmetric\nvariant and apply it to the pooled output distillation (POD) loss adapted for\nViTs. Our experiments confirm that introducing asymmetry to POD boosts its\nplasticity while retaining stability across (a) and (b). Moreover, we\nacknowledge low forgetting measures for all the compared methods, indicating\nthat ViTs might be naturally inclined continual learner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1\">Francesco Pelosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Saurav Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1\">Andrea Torsello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Multi-label Facial Action Unit Detection with Transformer. (arXiv:2203.13301v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13301","description":"<p>Facial Action Coding System is an important approach of facial expression\nanalysis.This paper describes our submission to the third Affective Behavior\nAnalysis (ABAW) 2022 competition. We proposed a transfomer based model to\ndetect facial action unit (FAU) in video. To be specific, we firstly trained a\nmulti-modal model to extract both audio and visual feature. After that, we\nproposed a action units correlation module to learn relationships between each\naction unit labels and refine action unit detection result. Experimental\nresults on validation dataset shows that our method achieves better performance\nthan baseline model, which verifies that the effectiveness of proposed network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13310","description":"<p>Monocular 3D object detection has long been a challenging task in autonomous\ndriving, which requires to decode 3D predictions solely from a single 2D image.\nMost existing methods follow conventional 2D object detectors to first localize\nobjects by their centers, and then predict 3D attributes using\ncenter-neighboring local features. However, such center-based pipeline views 3D\nprediction as a subordinate task and lacks inter-object depth interactions with\nglobal spatial clues. In this paper, we introduce a simple framework for\nMonocular DEtection with depth-aware TRansformer, named MonoDETR. We enable the\nvanilla transformer to be depth-aware and enforce the whole detection process\nguided by depth. Specifically, we represent 3D object candidates as a set of\nqueries and produce non-local depth embeddings of the input image by a\nlightweight depth predictor and an attention-based depth encoder. Then, we\npropose a depth-aware decoder to conduct both inter-query and query-scene depth\nfeature communication. In this way, each object estimates its 3D attributes\nadaptively from the depth-informative regions on the image, not limited by\ncenter-around features. With minimal handcrafted designs, MonoDETR is an\nend-to-end framework without additional data, anchors or NMS and achieves\ncompetitive performance on KITTI benchmark among state-of-the-art center-based\nnetworks. Extensive ablation studies demonstrate the effectiveness of our\napproach and its potential to serve as a transformer baseline for future\nmonocular research. Code is available at\nhttps://github.com/ZrrSkywalker/MonoDETR.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuanzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration Framework. (arXiv:2003.06513v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2003.06513","description":"<p>Weight pruning of deep neural networks (DNNs) has been proposed to satisfy\nthe limited storage and computing capability of mobile edge devices. However,\nprevious pruning methods mainly focus on reducing the model size and/or\nimproving performance without considering the privacy of user data. To mitigate\nthis concern, we propose a privacy-preserving-oriented pruning and mobile\nacceleration framework that does not require the private training dataset. At\nthe algorithm level of the proposed framework, a systematic weight pruning\ntechnique based on the alternating direction method of multipliers (ADMM) is\ndesigned to iteratively solve the pattern-based pruning problem for each layer\nwith randomly generated synthetic data. In addition, corresponding\noptimizations at the compiler level are leveraged for inference accelerations\non devices. With the proposed framework, users could avoid the time-consuming\npruning process for non-experts and directly benefit from compressed models.\nExperimental results show that the proposed framework outperforms three\nstate-of-art end-to-end DNN frameworks, i.e., TensorFlow-Lite, TVM, and MNN,\nwith speedup up to 4.2X, 2.5X, and 2.0X, respectively, with almost no accuracy\nloss, while preserving data privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaolin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.08910","description":"<p>Though recent years have witnessed remarkable progress in single image\nsuper-resolution (SISR) tasks with the prosperous development of deep neural\nnetworks (DNNs), the deep learning methods are confronted with the computation\nand memory consumption issues in practice, especially for resource-limited\nplatforms such as mobile devices. To overcome the challenge and facilitate the\nreal-time deployment of SISR tasks on mobile, we combine neural architecture\nsearch with pruning search and propose an automatic search framework that\nderives sparse super-resolution (SR) models with high image quality while\nsatisfying the real-time inference requirement. To decrease the search cost, we\nleverage the weight sharing strategy by introducing a supernet and decouple the\nsearch problem into three stages, including supernet construction,\ncompiler-aware architecture and pruning search, and compiler-aware pruning\nratio search. With the proposed framework, we are the first to achieve\nreal-time SR inference (with only tens of milliseconds per frame) for\nimplementing 720p resolution with competitive image quality (in terms of PSNR\nand SSIM) on mobile platforms (Samsung Galaxy S20).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yushu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jayaweera_M/0/1/0/all/0/1\">Malith Jayaweera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaeli_D/0/1/0/all/0/1\">David Kaeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration. (arXiv:2111.11581v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2111.11581","description":"<p>Weight pruning is an effective model compression technique to tackle the\nchallenges of achieving real-time deep neural network (DNN) inference on mobile\ndevices. However, prior pruning schemes have limited application scenarios due\nto accuracy degradation, difficulty in leveraging hardware acceleration, and/or\nrestriction on certain types of DNN layers. In this paper, we propose a\ngeneral, fine-grained structured pruning scheme and corresponding compiler\noptimizations that are applicable to any type of DNN layer while achieving high\naccuracy and hardware inference performance. With the flexibility of applying\ndifferent pruning schemes to different layers enabled by our compiler\noptimizations, we further probe into the new problem of determining the\nbest-suited pruning scheme considering the different acceleration and accuracy\nperformance of various pruning schemes. Two pruning scheme mapping methods, one\nis search-based and the other is rule-based, are proposed to automatically\nderive the best-suited pruning regularity and block size for each layer of any\ngiven DNN. Experimental results demonstrate that our pruning scheme mapping\nmethods, together with the general fine-grained structured pruning scheme,\noutperform the state-of-the-art DNN optimization framework with up to\n2.48$\\times$ and 1.73$\\times$ DNN inference acceleration on CIFAR-10 and\nImageNet dataset without accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuxuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xulong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}