{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Does entity abstraction help generative Transformers reason?. (arXiv:2201.01787v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01787","description":"<p>Pre-trained language models (LMs) often struggle to reason logically or\ngeneralize in a compositional fashion. Recent work suggests that incorporating\nexternal entity knowledge can improve LMs' abilities to reason and generalize.\nHowever, the effect of explicitly providing entity abstraction remains unclear,\nespecially with recent studies suggesting that pre-trained LMs already encode\nsome of that knowledge in their parameters. We study the utility of\nincorporating entity type abstractions into pre-trained Transformers and test\nthese methods on four NLP tasks requiring different forms of logical reasoning:\n(1) compositional language understanding with text-based relational reasoning\n(CLUTRR), (2) abductive reasoning (ProofWriter), (3) multi-hop question\nanswering (HotpotQA), and (4) conversational question answering (CoQA). We\npropose and empirically explore three ways to add such abstraction: (i) as\nadditional input embeddings, (ii) as a separate sequence to encode, and (iii)\nas an auxiliary prediction task for the model. Overall, our analysis\ndemonstrates that models with abstract entity knowledge performs better than\nwithout it. However, our experiments also show that the benefits strongly\ndepend on the technique used and the task at hand. The best abstraction aware\nmodels achieved an overall accuracy of 88.8% and 91.8% compared to the baseline\nmodel achieving 62.3% and 89.8% on CLUTRR and ProofWriter respectively. In\naddition, abstraction-aware models showed improved compositional generalization\nin both interpolation and extrapolation settings. However, for HotpotQA and\nCoQA, we find that F1 scores improve by only 0.5% on average. Our results\nsuggest that the benefit of explicit abstraction is significant in formally\ndefined logical reasoning settings requiring many reasoning hops, but point to\nthe notion that it is less beneficial for NLP tasks having less formal logical\nstructure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1\">Nicolas Gontier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KUDO Interpreter Assist: Automated Real-time Support for Remote Interpretation. (arXiv:2201.01800v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01800","description":"<p>High-quality human interpretation requires linguistic and factual preparation\nas well as the ability to retrieve information in real-time. This situation\nbecomes particularly relevant in the context of remote simultaneous\ninterpreting (RSI) where time-to-event may be short, posing new challenges to\nprofessional interpreters and their commitment to delivering high-quality\nservices. In order to mitigate these challenges, we present Interpreter Assist,\na computer-assisted interpreting tool specifically designed for the integration\nin RSI scenarios. Interpreter Assist comprises two main feature sets: an\nautomatic glossary creation tool and a real-time suggestion system. In this\npaper, we describe the overall design of our tool, its integration into the\ntypical RSI workflow, and the results achieved on benchmark tests both in terms\nof quality and relevance of glossary creation as well as in precision and\nrecall of the real-time suggestion feature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fantinuoli_C/0/1/0/all/0/1\">Claudio Fantinuoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchesini_G/0/1/0/all/0/1\">Giulia Marchesini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landan_D/0/1/0/all/0/1\">David Landan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horak_L/0/1/0/all/0/1\">Lukas Horak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frame Shift Prediction. (arXiv:2201.01837v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01837","description":"<p>Frame shift is a cross-linguistic phenomenon in translation which results in\ncorresponding pairs of linguistic material evoking different frames. The\nability to predict frame shifts enables automatic creation of multilingual\nFrameNets through annotation projection. Here, we propose the Frame Shift\nPrediction task and demonstrate that graph attention networks, combined with\nauxiliary training, can learn cross-linguistic frame-to-frame correspondence\nand predict frame shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_P/0/1/0/all/0/1\">Patrick D. Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrent_T/0/1/0/all/0/1\">Tiago Timponi Torrent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czulo_O/0/1/0/all/0/1\">Oliver Czulo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_C/0/1/0/all/0/1\">Collin F. Baker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation. (arXiv:2201.01845v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01845","description":"<p>Common designs of model evaluation typically focus on monolingual settings,\nwhere different models are compared according to their performance on a single\ndata set that is assumed to be representative of all possible data for the task\nat hand. While this may be reasonable for a large data set, this assumption is\ndifficult to maintain in low-resource scenarios, where artifacts of the data\ncollection can yield data sets that are outliers, potentially making\nconclusions about model performance coincidental. To address these concerns, we\ninvestigate model generalizability in crosslinguistic low-resource scenarios.\nUsing morphological segmentation as the test case, we compare three broad\nclasses of models with different parameterizations, taking data from 11\nlanguages across 6 language families. In each experimental setting, we evaluate\nall models on a first data set, then examine their performance consistency when\nintroducing new randomly sampled data sets with the same size and when applying\nthe trained models to unseen test sets of varying sizes. The results\ndemonstrate that the extent of model generalization depends on the\ncharacteristics of the data set, and does not necessarily rely heavily on the\ndata set size. Among the characteristics that we studied, the ratio of morpheme\noverlap and that of the average number of morphemes per word between the\ntraining and test sets are the two most prominent factors. Our findings suggest\nthat future work should adopt random sampling to construct data sets with\ndifferent sizes in order to make more responsible claims about model\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zoey Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudhommeaux_E/0/1/0/all/0/1\">Emily Prud&#x27;hommeaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Related Work Generation: A Meta Study. (arXiv:2201.01880v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01880","description":"<p>Academic research is an exploration activity to solve problems that have\nnever been resolved before. By this nature, each academic research work is\nrequired to perform a literature review to distinguish its novelties that have\nnot been addressed by prior works. In natural language processing, this\nliterature review is usually conducted under the \"Related Work\" section. The\ntask of automatic related work generation aims to automatically generate the\n\"Related Work\" section given the rest of the research paper and a list of cited\npapers. Although this task was proposed over 10 years ago, it received little\nattention until very recently, when it was cast as a variant of the scientific\nmulti-document summarization problem. However, even today, the problems of\nautomatic related work and citation text generation are not yet standardized.\nIn this survey, we conduct a meta-study to compare the existing literature on\nrelated work generation from the perspectives of problem formulation, dataset\ncollection, methodological approach, performance evaluation, and future\nprospects to provide the reader insight into the progress of the\nstate-of-the-art studies, as well as and how future studies can be conducted.\nWe also survey relevant fields of study that we suggest future work to consider\nintegrating.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jessica Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Object Grounding Using Scene Graphs. (arXiv:2201.01901v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01901","description":"<p>Object grounding tasks aim to locate the target object in an image through\nverbal communications. Understanding human command is an important process\nneeded for effective human-robot communication. However, this is challenging\nbecause human commands can be ambiguous and erroneous. This paper aims to\ndisambiguate the human's referring expressions by allowing the agent to ask\nrelevant questions based on semantic data obtained from scene graphs. We test\nif our agent can use relations between objects from a scene graph to ask\nsemantically relevant questions that can disambiguate the original user\ncommand. In this paper, we present Incremental Grounding using Scene Graphs\n(IGSG), a disambiguation model that uses semantic data from an image scene\ngraph and linguistic structures from a language scene graph to ground objects\nbased on human command. Compared to the baseline, IGSG shows promising results\nin complex real-world scenes where there are multiple identical target objects.\nIGSG can effectively disambiguate ambiguous or wrong referring expressions by\nasking disambiguating questions back to the user.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">John Seon Keun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1\">Sonia Chernova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuSpaCy: an industrial-strength Hungarian natural language processing toolkit. (arXiv:2201.01956v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01956","description":"<p>Although there are a couple of open-source language processing pipelines\navailable for Hungarian, none of them satisfies the requirements of today's NLP\napplications. A language processing pipeline should consist of close to\nstate-of-the-art lemmatization, morphosyntactic analysis, entity recognition\nand word embeddings. Industrial text processing applications have to satisfy\nnon-functional software quality requirements, what is more, frameworks\nsupporting multiple languages are more and more favored. This paper introduces\nHuSpaCy, an industryready Hungarian language processing pipeline. The presented\ntool provides components for the most important basic linguistic analysis\ntasks. It is open-source and is available under a permissive license. Our\nsystem is built upon spaCy's NLP components which means that it is fast, has a\nrich ecosystem of NLP applications and extensions, comes with extensive\ndocumentation and a well-known API. Besides the overview of the underlying\nmodels, we also present rigorous evaluation on common benchmark datasets. Our\nexperiments confirm that HuSpaCy has high accuracy in all subtasks while\nmaintaining resource-efficient prediction capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orosz_G/0/1/0/all/0/1\">Gy&#xf6;rgy Orosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szanto_Z/0/1/0/all/0/1\">Zsolt Sz&#xe1;nt&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkecz_P/0/1/0/all/0/1\">P&#xe9;ter Berkecz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1\">Gerg&#x151; Szab&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkas_R/0/1/0/all/0/1\">Rich&#xe1;rd Farkas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compact Bidirectional Transformer for Image Captioning. (arXiv:2201.01984v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01984","description":"<p>Most current image captioning models typically generate captions from left to\nright. This unidirectional property makes them can only leverage past context\nbut not future context. Though recent refinement-based models can exploit both\npast and future context by generating a new caption in the second stage based\non pre-retrieved or pre-generated captions in the first stage, the decoder of\nthese models generally consists of two networks~(i.e. a retriever or captioner\nin the first stage and a refiner in the second stage), which can only be\nexecuted sequentially. In this paper, we introduce a Compact Bidirectional\nTransformer model for image captioning that can leverage bidirectional context\nimplicitly and explicitly while the decoder can be executed parallelly.\nSpecifically, it is implemented by tightly coupling left-to-right(L2R) and\nright-to-left(R2L) flows into a single compact model~(i.e. implicitly) and\noptionally allowing interaction of the two flows(i.e. explicitly), while the\nfinal caption is chosen from either L2R or R2L flow in a sentence-level\nensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark\nand find that the compact architecture, which serves as a regularization for\nimplicitly exploiting bidirectional context, and the sentence-level ensemble\nplay more important roles than the explicit interaction mechanism. By combining\nwith word-level ensemble seamlessly, the effect of the sentence-level ensemble\nis further enlarged. We further extend the conventional one-flow self-critical\ntraining to the two-flows version under this architecture and achieve new\nstate-of-the-art results in comparison with non-vision-language-pretraining\nmodels. Source code is available at\n{\\color{magenta}\\url{https://github.com/YuanEZhou/CBTrans}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenzhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_H/0/1/0/all/0/1\">Huixia Ben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mandarin End-to-End Speech Recognition with Word N-gram Language Model. (arXiv:2201.01995v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01995","description":"<p>Despite the rapid progress of end-to-end (E2E) automatic speech recognition\n(ASR), it has been shown that incorporating external language models (LMs) into\nthe decoding can further improve the recognition performance of E2E ASR\nsystems. To align with the modeling units adopted in E2E ASR systems,\nsubword-level (e.g., characters, BPE) LMs are usually used to cooperate with\ncurrent E2E ASR systems. However, the use of subword-level LMs will ignore the\nword-level information, which may limit the strength of the external LMs in E2E\nASR. Although several methods have been proposed to incorporate word-level\nexternal LMs in E2E ASR, these methods are mainly designed for languages with\nclear word boundaries such as English and cannot be directly applied to\nlanguages like Mandarin, in which each character sequence can have multiple\ncorresponding word sequences. To this end, we propose a novel decoding\nalgorithm where a word-level lattice is constructed on-the-fly to consider all\npossible word sequences for each partial hypothesis. Then, the LM score of the\nhypothesis is obtained by intersecting the generated lattice with an external\nword N-gram LM. The proposed method is examined on both Attention-based\nEncoder-Decoder (AED) and Neural Transducer (NT) frameworks. Experiments\nsuggest that our method consistently outperforms subword-level LMs, including\nN-gram LM and neural network LM. We achieve state-of-the-art results on both\nAishell-1 (CER 4.18%) and Aishell-2 (CER 5.06%) datasets and reduce CER by\n14.8% relatively on a 21K-hour Mandarin dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chao Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An exploratory experiment on Hindi, Bengali hate-speech detection and transfer learning using neural networks. (arXiv:2201.01997v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01997","description":"<p>This work presents our approach to train a neural network to detect\nhate-speech texts in Hindi and Bengali. We also explore how transfer learning\ncan be applied to learning these languages, given that they have the same\norigin and thus, are similar to some extend. Even though the whole experiment\nwas conducted with low computational power, the obtained result is comparable\nto the results of other, more expensive, models. Furthermore, since the\ntraining data in use is relatively small and the two languages are almost\nentirely unknown to us, this work can be generalized as an effort to demystify\nlost or alien languages that no human is capable of understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1\">Tung Minh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cloos_J/0/1/0/all/0/1\">Jan Cloos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-level Adversarial Example Generation for Neural Machine Translation. (arXiv:2201.02009v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02009","description":"<p>While end-to-end neural machine translation (NMT) has achieved impressive\nprogress, noisy input usually leads models to become fragile and unstable.\nGenerating adversarial examples as the augmented data is proved to be useful to\nalleviate this problem. Existing methods for adversarial example generation\n(AEG) are word-level or character-level. In this paper, we propose a\nphrase-level adversarial example generation (PAEG) method to enhance the\nrobustness of the model. Our method leverages a gradient-based strategy to\nsubstitute phrases of vulnerable positions in the source input. We verify our\nmethod on three benchmarks, including LDC Chinese-English, IWSLT14\nGerman-English, and WMT14 English-German tasks. Experimental results\ndemonstrate that our approach significantly improves performance compared to\nprevious methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Juncheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training Vision Language BERTs with a Unified Conditional Model. (arXiv:2201.02010v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02010","description":"<p>Natural language BERTs are trained with language corpus in a self-supervised\nmanner. Unlike natural language BERTs, vision language BERTs need paired data\nto train, which restricts the scale of VL-BERT pretraining. We propose a\nself-training approach that allows training VL-BERTs from unlabeled image data.\nThe proposed method starts with our unified conditional model -- a vision\nlanguage BERT model that can perform zero-shot conditional generation. Given\ndifferent conditions, the unified conditional model can generate captions,\ndense captions, and even questions. We use the labeled image data to train a\nteacher model and use the trained model to generate pseudo captions on\nunlabeled image data. We then combine the labeled data and pseudo labeled data\nto train a student model. The process is iterated by putting the student model\nas a new teacher. By using the proposed self-training approach and only 300k\nunlabeled extra data, we are able to get competitive or even better\nperformances compared to the models of similar model size trained with 3\nmillion extra image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaofeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1\">Fengmao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis. (arXiv:2201.02026v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02026","description":"<p>In recent years, pretrained language models have revolutionized the NLP\nworld, while achieving state of the art performance in various downstream\ntasks. However, in many cases, these models do not perform well when labeled\ndata is scarce and the model is expected to perform in the zero or few shot\nsetting. Recently, several works have shown that continual pretraining or\nperforming a second phase of pretraining (inter-training) which is better\naligned with the downstream task, can lead to improved results, especially in\nthe scarce data setting. Here, we propose to leverage sentiment-carrying\ndiscourse markers to generate large-scale weakly-labeled data, which in turn\ncan be used to adapt language models for sentiment analysis. Extensive\nexperimental results show the value of our approach on various benchmark\ndatasets, including the finance domain. Code, models and data are available at\nhttps://github.com/ibm/tslm-discourse-markers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ein_Dor_L/0/1/0/all/0/1\">Liat Ein-Dor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnayderman_I/0/1/0/all/0/1\">Ilya Shnayderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_A/0/1/0/all/0/1\">Artem Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankin_L/0/1/0/all/0/1\">Lena Dankin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Regression Approach for Building and Stacking Predictive Models in Time Series Analytics. (arXiv:2201.02034v1 [stat.AP])","link":"http://arxiv.org/abs/2201.02034","description":"<p>The paper describes the use of Bayesian regression for building time series\nmodels and stacking different predictive models for time series. Using Bayesian\nregression for time series modeling with nonlinear trend was analyzed. This\napproach makes it possible to estimate an uncertainty of time series prediction\nand calculate value at risk characteristics. A hierarchical model for time\nseries using Bayesian regression has been considered. In this approach, one set\nof parameters is the same for all data samples, other parameters can be\ndifferent for different groups of data samples. Such an approach allows using\nthis model in the case of short historical data for specified time series, e.g.\nin the case of new stores or new products in the sales prediction problem. In\nthe study of predictive models stacking, the models ARIMA, Neural Network,\nRandom Forest, Extra Tree were used for the prediction on the first level of\nmodel ensemble. On the second level, time series predictions of these models on\nthe validation set were used for stacking by Bayesian regression. This approach\ngives distributions for regression coefficients of these models. It makes it\npossible to estimate the uncertainty contributed by each model to stacking\nresult. The information about these distributions allows us to select an\noptimal set of stacking models, taking into account the domain knowledge. The\nprobabilistic approach for stacking predictive models allows us to make risk\nassessment for the predictions that are important in a decision-making process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forming Predictive Features of Tweets for Decision-Making Support. (arXiv:2201.02049v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02049","description":"<p>The article describes the approaches for forming different predictive\nfeatures of tweet data sets and using them in the predictive analysis for\ndecision-making support. The graph theory as well as frequent itemsets and\nassociation rules theory is used for forming and retrieving different features\nfrom these datasests. The use of these approaches makes it possible to reveal a\nsemantic structure in tweets related to a specified entity. It is shown that\nquantitative characteristics of semantic frequent itemsets can be used in\npredictive regression models with specified target variables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sales Time Series Analytics Using Deep Q-Learning. (arXiv:2201.02058v1 [cs.LG])","link":"http://arxiv.org/abs/2201.02058","description":"<p>The article describes the use of deep Q-learning models in the problems of\nsales time series analytics. In contrast to supervised machine learning which\nis a kind of passive learning using historical data, Q-learning is a kind of\nactive learning with goal to maximize a reward by optimal sequence of actions.\nModel free Q-learning approach for optimal pricing strategies and supply-demand\nproblems was considered in the work. The main idea of the study is to show that\nusing deep Q-learning approach in time series analytics, the sequence of\nactions can be optimized by maximizing the reward function when the environment\nfor learning agent interaction can be modeled using the parametric model and in\nthe case of using the model which is based on the historical data. In the\npricing optimizing case study environment was modeled using sales dependence on\nextras price and randomly simulated demand. In the pricing optimizing case\nstudy, the environment was modeled using sales dependence on extra price and\nrandomly simulated demand. In the supply-demand case study, it was proposed to\nuse historical demand time series for environment modeling, agent states were\nrepresented by promo actions, previous demand values and weekly seasonality\nfeatures. Obtained results show that using deep Q-learning, we can optimize the\ndecision making process for price optimization and supply-demand problems.\nEnvironment modeling using parametric models and historical data can be used\nfor the cold start of learning agent. On the next steps, after the cold start,\nthe trained agent can be used in real business environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign Language. (arXiv:2201.02065v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02065","description":"<p>Sign language is an essential resource enabling access to communication and\nproper socioemotional development for individuals suffering from disabling\nhearing loss. As this population is expected to reach 700 million by 2050, the\nimportance of the language becomes even more essential as it plays a critical\nrole to ensure the inclusion of such individuals in society. The Sign Language\nRecognition field aims to bridge the gap between users and non-users of sign\nlanguages. However, the scarcity in quantity and quality of datasets is one of\nthe main challenges limiting the exploration of novel approaches that could\nlead to significant advancements in this research area. Thus, this paper\ncontributes by introducing two new datasets for the American Sign Language: the\nfirst is composed of the three-dimensional representation of the signers and,\nthe second, by an unprecedented linguistics-based representation containing a\nset of phonological attributes of the signs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amorim_C/0/1/0/all/0/1\">Cleison Correia de Amorim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERN2: an advanced neural biomedical named entity recognition and normalization tool. (arXiv:2201.02080v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02080","description":"<p>In biomedical natural language processing, named entity recognition (NER) and\nnamed entity normalization (NEN) are key tasks that enable the automatic\nextraction of biomedical entities (e.g., diseases and chemicals) from the\never-growing biomedical literature. In this paper, we present BERN2 (Advanced\nBiomedical Entity Recognition and Normalization), a tool that improves the\nprevious neural network-based NER tool (Kim et al., 2019) by employing a\nmulti-task NER model and neural network-based NEN models to achieve much faster\nand more accurate inference. We hope that our tool can help annotate\nlarge-scale biomedical texts more accurately for various tasks such as\nbiomedical knowledge graph construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minbyul Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yonghwa Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConTrip: Consensus Sentiment review Analysis and Platform ratings in a single score. (arXiv:2201.02113v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02113","description":"<p>People unequivocally employ reviews to decide on purchasing an item or an\nexperience on the internet. In that regard, the growing significance and number\nof opinions have led to the development of methods to assess their sentiment\ncontent automatically. However, it is not straightforward for the models to\ncreate a consensus value that embodies the agreement of the different reviews\nand differentiates across equal ratings for an item. Based on the approach\nproposed by Nguyen et al. in 2020, we derive a novel consensus value named\nConTrip that merges their consensus score and the overall rating of a platform\nfor an item. ConTrip lies in the rating range values, which makes it more\ninterpretable while maintaining the ability to differentiate across equally\nrated experiences. ConTrip is implemented and freely available under MIT\nlicense at https://github.com/pepebonet/contripscore\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonet_J/0/1/0/all/0/1\">Jos&#xe9; Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonet_J/0/1/0/all/0/1\">Jos&#xe9; Bonet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Opinion Mining of Text in COVID-19 Issues along with Comparative Study in ML, BERT & RNN. (arXiv:2201.02119v1 [cs.NE])","link":"http://arxiv.org/abs/2201.02119","description":"<p>The global world is crossing a pandemic situation where this is a\ncatastrophic outbreak of Respiratory Syndrome recognized as COVID-19. This is a\nglobal threat all over the 212 countries that people every day meet with mighty\nsituations. On the contrary, thousands of infected people live rich in\nmountains. Mental health is also affected by this worldwide coronavirus\nsituation. Due to this situation online sources made a communicative place that\ncommon people shares their opinion in any agenda. Such as affected news related\npositive and negative, financial issues, country and family crisis, lack of\nimport and export earning system etc. different kinds of circumstances are\nrecent trendy news in anywhere. Thus, vast amounts of text are produced within\nmoments therefore, in subcontinent areas the same as situation in other\ncountries and peoples opinion of text and situation also same but the language\nis different. This article has proposed some specific inputs along with Bangla\ntext comments from individual sources which can assure the goal of illustration\nthat machine learning outcome capable of building an assistive system. Opinion\nmining assistive system can be impactful in all language preferences possible.\nTo the best of our knowledge, the article predicted the Bangla input text on\nCOVID-19 issues proposed ML algorithms and deep learning models analysis also\ncheck the future reachability with a comparative analysis. Comparative analysis\nstates a report on text prediction accuracy is 91% along with ML algorithms and\n79% along with Deep Learning Models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sany_M/0/1/0/all/0/1\">Md. Mahadi Hasan Sany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keya_M/0/1/0/all/0/1\">Mumenunnesa Keya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khushbu_S/0/1/0/all/0/1\">Sharun Akter Khushbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabby_A/0/1/0/all/0/1\">Akm Shahariar Azad Rabby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masum_A/0/1/0/all/0/1\">Abu Kaisar Mohammad Masum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis and Sarcasm Detection of Indian General Election Tweets. (arXiv:2201.02127v1 [cs.IR])","link":"http://arxiv.org/abs/2201.02127","description":"<p>Social Media usage has increased to an all-time high level in today's digital\nworld. The majority of the population uses social media tools (like Twitter,\nFacebook, YouTube, etc.) to share their thoughts and experiences with the\ncommunity. Analysing the sentiments and opinions of the common public is very\nimportant for both the government and the business people. This is the reason\nbehind the activeness of many media agencies during the election time for\nperforming various kinds of opinion polls. In this paper, we have worked\ntowards analysing the sentiments of the people of India during the Lok Sabha\nelection of 2019 using the Twitter data of that duration. We have built an\nautomatic tweet analyser using the Transfer Learning technique to handle the\nunsupervised nature of this problem. We have used the Linear Support Vector\nClassifiers method in our Machine Learning model, also, the Term Frequency\nInverse Document Frequency (TF-IDF) methodology for handling the textual data\nof tweets. Further, we have increased the capability of the model to address\nthe sarcastic tweets posted by some of the users, which has not been yet\nconsidered by the researchers in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khare_A/0/1/0/all/0/1\">Arpit Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_A/0/1/0/all/0/1\">Amisha Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sudhakar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Shiv Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization. (arXiv:2109.02492v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02492","description":"<p>Dialogue is an essential part of human communication and cooperation.\nExisting research mainly focuses on short dialogue scenarios in a one-on-one\nfashion. However, multi-person interactions in the real world, such as meetings\nor interviews, are frequently over a few thousand words. There is still a lack\nof corresponding research and powerful tools to understand and process such\nlong dialogues. Therefore, in this work, we present a pre-training framework\nfor long dialogue understanding and summarization. Considering the nature of\nlong conversations, we propose a window-based denoising approach for generative\npre-training. For a dialogue, it corrupts a window of text with\ndialogue-inspired noise, and guides the model to reconstruct this window based\non the content of the remaining conversation. Furthermore, to process longer\ninput, we augment the model with sparse attention which is combined with\nconventional attention in a hybrid manner. We conduct extensive experiments on\nfive datasets of long dialogues, covering tasks of dialogue summarization,\nabstractive question answering and topic segmentation. Experimentally, we show\nthat our pre-trained model DialogLM significantly surpasses the\nstate-of-the-art models across datasets and tasks. Source code and all the\npre-trained models are available on our GitHub repository\n(https://github.com/microsoft/DialogLM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FST Morphological Analyser and Generator for Mapud\\\"ungun. (arXiv:2109.09176v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09176","description":"<p>Following the Mapuche grammar by Smeets, this article describes the main\nmorphophonological aspects of Mapud\\\"ungun, explaining what triggers them and\nthe contexts where they arise. We present a computational approach producing a\nfinite state morphological analyser (and generator) capable of classifying and\nappropriately tagging all the components (roots and suffixes) that interact in\na Mapuche word form. The bulk of the article focuses on presenting details\nabout the morphology of Mapud\\\"ungun verb and its formalisation using FOMA. A\nsystem evaluation process and its results are also present in this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandia_A/0/1/0/all/0/1\">Andr&#xe9;s Chand&#xed;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HydraText: Multi-objective Optimization for Adversarial Textual Attack. (arXiv:2111.01528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01528","description":"<p>The field of adversarial textual attack has significantly grown over the last\nfew years, where the commonly considered objective is to craft adversarial\nexamples (AEs) that can successfully fool the target model. However, the\nimperceptibility of attacks, which is also an essential objective for practical\nattackers, is often left out by previous studies. In consequence, the crafted\nAEs tend to have obvious structural and semantic differences from the original\nhuman-written texts, making them easily perceptible. In this paper, we advocate\nsimultaneously considering both objectives of successful and imperceptible\nattacks. Specifically, we formulate the problem of crafting AEs as a\nmulti-objective set maximization problem, and propose a novel evolutionary\nalgorithm (dubbed HydraText) to solve it. To the best of our knowledge,\nHydraText is currently the only approach that can be effectively applied to\nboth score-based and decision-based attack settings. Exhaustive experiments\ninvolving 44237 instances demonstrate that HydraText consistently achieves\nhigher attack success rates and better attack imperceptibility than the\nstate-of-the-art textual attack approaches. A human evaluation study also shows\nthat the AEs crafted by HydraText are more indistinguishable from human-written\ntexts. Finally, these AEs exhibit good transferability and can bring notable\nrobustness improvement to the target models by adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenjing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection. (arXiv:2111.14592v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.14592","description":"<p>Pre-trained models have proved to be powerful in enhancing task-oriented\ndialog systems. However, current pre-training methods mainly focus on enhancing\ndialog understanding and generation tasks while neglecting the exploitation of\ndialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog\nmodel that explicitly learns dialog policy from limited labeled dialogs and\nlarge-scale unlabeled dialog corpora via semi-supervised learning.\nSpecifically, we introduce a dialog act prediction task for policy optimization\nduring pre-training and employ a consistency regularization term to refine the\nlearned representation with the help of unlabeled dialogs. We also implement a\ngating mechanism to weigh suitable unlabeled dialog samples. Empirical results\nshow that GALAXY substantially improves the performance of task-oriented dialog\nsystems, and achieves new state-of-the-art results on benchmark datasets:\nIn-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores\nby 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a\nstronger few-shot ability than existing models under various low-resource\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dermot Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JABER and SABER: Junior and Senior Arabic BERt. (arXiv:2112.04329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04329","description":"<p>Language-specific pre-trained models have proven to be more accurate than\nmultilingual ones in a monolingual evaluation setting, Arabic is no exception.\nHowever, we found that previously released Arabic BERT models were\nsignificantly under-trained. In this technical report, we present JABER and\nSABER, Junior and Senior Arabic BERt respectively, our pre-trained language\nmodel prototypes dedicated for Arabic. We conduct an empirical study to\nsystematically evaluate the performance of models across a diverse set of\nexisting Arabic NLU tasks. Experimental results show that JABER and SABER\nachieve state-of-the-art performances on ALUE, a new benchmark for Arabic\nLanguage Understanding Evaluation, as well as on a well-established NER\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yimeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_K/0/1/0/all/0/1\">Khalil Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinyu_D/0/1/0/all/0/1\">Duan Xinyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text is no more Enough! A Benchmark for Profile-based Spoken Language Understanding. (arXiv:2112.11953v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11953","description":"<p>Current researches on spoken language understanding (SLU) heavily are limited\nto a simple setting: the plain text-based SLU that takes the user utterance as\ninput and generates its corresponding semantic frames (e.g., intent and slots).\nUnfortunately, such a simple setting may fail to work in complex real-world\nscenarios when an utterance is semantically ambiguous, which cannot be achieved\nby the text-based SLU models. In this paper, we first introduce a new and\nimportant task, Profile-based Spoken Language Understanding (ProSLU), which\nrequires the model that not only relies on the plain text but also the\nsupporting profile information to predict the correct intents and slots. To\nthis end, we further introduce a large-scale human-annotated Chinese dataset\nwith over 5K utterances and their corresponding supporting profile information\n(Knowledge Graph (KG), User Profile (UP), Context Awareness (CA)). In addition,\nwe evaluate several state-of-the-art baseline models and explore a multi-level\nknowledge adapter to effectively incorporate profile information. Experimental\nresults reveal that all existing text-based SLU models fail to work when the\nutterances are semantically ambiguous and our proposed framework can\neffectively fuse the supporting information for sentence-level intent detection\nand token-level slot filling. Finally, we summarize key challenges and provide\nnew points for future directions, which hopes to facilitate the research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiji Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Learners' Phonetic Transfer of /i/ from Mandarin Chinese to General American English: Evidence from Perception and Production Experiments. (arXiv:2112.13571v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.13571","description":"<p>Ever since the development of Contrastive Analysis (CA) in the 1950s, which\nfocuses on comparing and contrasting two language systems, linguists have\nstarted to systematically explore the influence of the mother tongue on\nacquiring a second language. This phenomenon is later defined as \"language\ntransfer\". The current paper concerns language transfer at the phonetic level\nand concentrates on the transfer phenomenon existing in advanced-level Chinese\nlearners' acquisition of English vowels /i/ and its lax counterpart. By\ndetermining whether advanced-level Chinese English-language learners (ELLs) can\naccurately distinguish between /i/ and its lax counterpart, and pronounce them\nin English words precisely, this paper serves as a reference for further\nstudying Chinese ELLs' language transfer. Two objectives were to be met:\nfirstly, learners' perceptual ability to distinguish between vowels /i/ and its\nlax counterpart should be examined; and secondly, the effect of the phonetic\ntransfer should be determined. A perception test and a production test were\nused to attain these two objectives. Both tests were completed by six\nadvanced-level Chinese ELLs, three males and three females. Results indicate\nthat both male and female participants could consciously distinguish between\n/i/ and its lax counterpart. All participants have signs of experiencing\nnegative phonetic transfer in their pronunciation, except that the current data\ndo not decisively reflect an impact of the phonetic transfer on female ELLs'\nacquisition of the high front lax vowel in English words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lintao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Adverse Drug Reactions from Unstructured Mediums at Scale. (arXiv:2201.01405v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01405","description":"<p>Adverse drug reactions / events (ADR/ADE) have a major impact on patient\nhealth and health care costs. Detecting ADR's as early as possible and sharing\nthem with regulators, pharma companies, and healthcare providers can prevent\nmorbidity and save many lives. While most ADR's are not reported via formal\nchannels, they are often documented in a variety of unstructured conversations\nsuch as social media posts by patients, customer support call transcripts, or\nCRM notes of meetings between healthcare providers and pharma sales reps. In\nthis paper, we propose a natural language processing (NLP) solution that\ndetects ADR's in such unstructured free-text conversations, which improves on\nprevious work in three ways. First, a new Named Entity Recognition (NER) model\nobtains new state-of-the-art accuracy for ADR and Drug entity extraction on the\nADE, CADEC, and SMM4H benchmark datasets (91.75%, 78.76%, and 83.41% F1 scores\nrespectively). Second, two new Relation Extraction (RE) models are introduced -\none based on BioBERT while the other utilizing crafted features over a Fully\nConnected Neural Network (FCNN) - are shown to perform on par with existing\nstate-of-the-art models, and outperform them when trained with a supplementary\nclinician-annotated RE dataset. Third, a new text classification model, for\ndeciding if a conversation includes an ADR, obtains new state-of-the-art\naccuracy on the CADEC dataset (86.69% F1 score). The complete solution is\nimplemented as a unified NLP pipeline in a production-grade library built on\ntop of Apache Spark, making it natively scalable and able to process millions\nof batch or streaming records on commodity clusters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haq_H/0/1/0/all/0/1\">Hasham Ul Haq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocaman_V/0/1/0/all/0/1\">Veysel Kocaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talby_D/0/1/0/all/0/1\">David Talby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need In Sign Language Production. (arXiv:2201.01609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01609","description":"<p>Sign Language is the dominant form of communication language used in the deaf\nand hearing-impaired community. To make an easy and mutual communication\nbetween the hearing-impaired and the hearing communities, building a robust\nsystem capable of translating the spoken language into sign language and vice\nversa is fundamental. To this end, sign language recognition and production are\ntwo necessary parts for making such a two-way system. Sign language recognition\nand production need to cope with some critical challenges. In this survey, we\nreview recent advances in Sign Language Production (SLP) and related areas\nusing deep learning. To have more realistic perspectives to sign language, we\npresent an introduction to the Deaf culture, Deaf centers, psychological\nperspective of sign language, the main differences between spoken language and\nsign language. Furthermore, we present the fundamental components of a\nbi-directional sign language translation system, discussing the main challenges\nin this area. Also, the backbone architectures and methods in SLP are briefly\nintroduced and the proposed taxonomy on SLP is presented. Finally, a general\nframework for SLP and performance evaluation, and also a discussion on the\nrecent developments, advantages, and limitations in SLP, commenting on possible\nlines for future research are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Quantum Capsule Networks. (arXiv:2201.01778v1 [quant-ph])","link":"http://arxiv.org/abs/2201.01778","description":"<p>Capsule networks, which incorporate the paradigms of connectionism and\nsymbolism, have brought fresh insights into artificial intelligence. The\ncapsule, as the building block of capsule networks, is a group of neurons\nrepresented by a vector to encode different features of an entity. The\ninformation is extracted hierarchically through capsule layers via routing\nalgorithms. Here, we introduce a quantum capsule network (dubbed QCapsNet)\ntogether with a quantum dynamic routing algorithm. Our model enjoys an\nexponential speedup in the dynamic routing process and exhibits an enhanced\nrepresentation power. To benchmark the performance of the QCapsNet, we carry\nout extensive numerical simulations on the classification of handwritten digits\nand symmetry-protected topological phases, and show that the QCapsNet can\nachieve the state-of-the-art accuracy and outperforms conventional quantum\nclassifiers evidently. We further unpack the output capsule state and find that\na particular subspace may correspond to a human-understandable feature of the\ninput data, which indicates the potential explainability of such networks. Our\nwork reveals an intriguing prospect of quantum capsule networks in quantum\nmachine learning, which may provide a valuable guide towards explainable\nquantum artificial intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Liu_Z/0/1/0/all/0/1\">Zidu Liu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Shen_P/0/1/0/all/0/1\">Pei-Xin Shen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_W/0/1/0/all/0/1\">Weikang Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Duan_L/0/1/0/all/0/1\">L.-M. Duan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Deng_D/0/1/0/all/0/1\">Dong-Ling Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Scoring of Graphical Open-Ended Responses Using Artificial Neural Networks. (arXiv:2201.01783v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01783","description":"<p>Automated scoring of free drawings or images as responses has yet to be\nutilized in large-scale assessments of student achievement. In this study, we\npropose artificial neural networks to classify these types of graphical\nresponses from a computer based international mathematics and science\nassessment. We are comparing classification accuracy of convolutional and\nfeedforward approaches. Our results show that convolutional neural networks\n(CNNs) outperform feedforward neural networks in both loss and accuracy. The\nCNN models classified up to 97.71% of the image responses into the appropriate\nscoring category, which is comparable to, if not more accurate, than typical\nhuman raters. These findings were further strengthened by the observation that\nthe most accurate CNN models correctly classified some image responses that had\nbeen incorrectly scored by the human raters. As an additional innovation, we\noutline a method to select human rated responses for the training sample based\non an application of the expected response function derived from item response\ntheory. This paper argues that CNN-based automated scoring of image responses\nis a highly accurate procedure that could potentially replace the workload and\ncost of second human raters for large scale assessments, while improving the\nvalidity and comparability of scoring complex constructed-response items.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davier_M/0/1/0/all/0/1\">Matthias von Davier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyack_L/0/1/0/all/0/1\">Lillian Tyack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorramdel_L/0/1/0/all/0/1\">Lale Khorramdel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Deep Subspace Alignment for Unsupervised Domain Adaptation. (arXiv:2201.01806v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01806","description":"<p>Unsupervised domain adaptation (UDA) aims to transfer and adapt knowledge\nfrom a labeled source domain to an unlabeled target domain. Traditionally,\nsubspace-based methods form an important class of solutions to this problem.\nDespite their mathematical elegance and tractability, these methods are often\nfound to be ineffective at producing domain-invariant features with complex,\nreal-world datasets. Motivated by the recent advances in representation\nlearning with deep networks, this paper revisits the use of subspace alignment\nfor UDA and proposes a novel adaptation algorithm that consistently leads to\nimproved generalization. In contrast to existing adversarial training-based DA\nmethods, our approach isolates feature learning and distribution alignment\nsteps, and utilizes a primary-auxiliary optimization strategy to effectively\nbalance the objectives of domain invariance and model fidelity. While providing\na significant reduction in target data and computational requirements, our\nsubspace-based DA performs competitively and sometimes even outperforms\nstate-of-the-art approaches on several standard UDA benchmarks. Furthermore,\nsubspace alignment leads to intrinsically well-regularized models that\ndemonstrate strong generalization even in the challenging partial DA setting.\nFinally, the design of our UDA framework inherently supports progressive\nadaptation to new target domains at test-time, without requiring retraining of\nthe model from scratch. In summary, powered by powerful feature learners and an\neffective optimization strategy, we establish subspace-based DA as a highly\neffective approach for visual recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thopalli_K/0/1/0/all/0/1\">Kowshik Thopalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan K Turaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal Analysis of Art: Proxy Learning of Visual Concepts from Style Through Language Models. (arXiv:2201.01819v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01819","description":"<p>We present a machine learning system that can quantify fine art paintings\nwith a set of visual elements and principles of art. This formal analysis is\nfundamental for understanding art, but developing such a system is challenging.\nPaintings have high visual complexities, but it is also difficult to collect\nenough training data with direct labels. To resolve these practical\nlimitations, we introduce a novel mechanism, called proxy learning, which\nlearns visual concepts in paintings though their general relation to styles.\nThis framework does not require any visual annotation, but only uses style\nlabels and a general relationship between visual concepts and style. In this\npaper, we propose a novel proxy model and reformulate four pre-existing methods\nin the context of proxy learning. Through quantitative and qualitative\ncomparison, we evaluate these methods and compare their effectiveness in\nquantifying the artistic visual concepts, where the general relationship is\nestimated by language models; GloVe or BERT. The language modeling is a\npractical and scalable solution requiring no labeling, but it is inevitably\nimperfect. We demonstrate how the new proxy model is robust to the\nimperfection, while the other models are sensitively affected by it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Diana Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgammal_A/0/1/0/all/0/1\">Ahmed Elgammal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzone_M/0/1/0/all/0/1\">Marian Mazzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Semantic Ambiguities for Zero-Shot Learning. (arXiv:2201.01823v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01823","description":"<p>Zero-shot learning (ZSL) aims at recognizing classes for which no visual\nsample is available at training time. To address this issue, one can rely on a\nsemantic description of each class. A typical ZSL model learns a mapping\nbetween the visual samples of seen classes and the corresponding semantic\ndescriptions, in order to do the same on unseen classes at test time. State of\nthe art approaches rely on generative models that synthesize visual features\nfrom the prototype of a class, such that a classifier can then be learned in a\nsupervised manner. However, these approaches are usually biased towards seen\nclasses whose visual instances are the only one that can be matched to a given\nclass prototype. We propose a regularization method that can be applied to any\nconditional generative-based ZSL method, by leveraging only the semantic class\nprototypes. It learns to synthesize discriminative features for possible\nsemantic description that are not available at training time, that is the\nunseen ones. The approach is evaluated for ZSL and GZSL on four datasets\ncommonly used in the literature, either in inductive and transductive settings,\nwith results on-par or above state of the art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanouti_C/0/1/0/all/0/1\">Celina Hanouti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1\">Herv&#xe9; Le Borgne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POCO: Point Convolution for Surface Reconstruction. (arXiv:2201.01831v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01831","description":"<p>Implicit neural networks have been successfully used for surface\nreconstruction from point clouds. However, many of them face scalability issues\nas they encode the isosurface function of a whole object or scene into a single\nlatent vector. To overcome this limitation, a few approaches infer latent\nvectors on a coarse regular 3D grid or on 3D patches, and interpolate them to\nanswer occupancy queries. In doing so, they loose the direct connection with\nthe input points sampled on the surface of objects, and they attach information\nuniformly in space rather than where it matters the most, i.e., near the\nsurface. Besides, relying on fixed patch sizes may require discretization\ntuning. To address these issues, we propose to use point cloud convolutions and\ncompute latent vectors at each input point. We then perform a learning-based\ninterpolation on nearest neighbors using inferred weights. Experiments on both\nobject and scene datasets show that our approach significantly outperforms\nother methods on most classical metrics, producing finer details and better\nreconstructing thinner volumes. The code is available at\nhttps://github.com/valeoai/POCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Sclerosis Lesions Segmentation using Attention-Based CNNs in FLAIR Images. (arXiv:2201.01832v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01832","description":"<p>Objective: Multiple Sclerosis (MS) is an autoimmune, and demyelinating\ndisease that leads to lesions in the central nervous system. This disease can\nbe tracked and diagnosed using Magnetic Resonance Imaging (MRI). Up to now a\nmultitude of multimodality automatic biomedical approaches is used to segment\nlesions which are not beneficial for patients in terms of cost, time, and\nusability. The authors of the present paper propose a method employing just one\nmodality (FLAIR image) to segment MS lesions accurately. Methods: A patch-based\nConvolutional Neural Network (CNN) is designed, inspired by 3D-ResNet and\nspatial-channel attention module, to segment MS lesions. The proposed method\nconsists of three stages: (1) the contrast-limited adaptive histogram\nequalization (CLAHE) is applied to the original images and concatenated to the\nextracted edges in order to create 4D images; (2) the patches of size 80 * 80 *\n80 * 2 are randomly selected from the 4D images; and (3) the extracted patches\nare passed into an attention-based CNN which is used to segment the lesions.\nFinally, the proposed method was compared to previous studies of the same\ndataset. Results: The current study evaluates the model, with a test set of\nISIB challenge data. Experimental results illustrate that the proposed approach\nsignificantly surpasses existing methods in terms of Dice similarity and\nAbsolute Volume Difference while the proposed method use just one modality\n(FLAIR) to segment the lesions. Conclusions: The authors have introduced an\nautomated approach to segment the lesions which is based on, at most, two\nmodalities as an input. The proposed architecture is composed of convolution,\ndeconvolution, and an SCA-VoxRes module as an attention module. The results\nshow, the proposed method outperforms well compare to other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+SadeghiBakhi_M/0/1/0/all/0/1\">Mehdi SadeghiBakhi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pourreza_H/0/1/0/all/0/1\">Hamidreza Pourreza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahyar_H/0/1/0/all/0/1\">Hamidreza Mahyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lumbar Bone Mineral Density Estimation from Chest X-ray Images: Anatomy-aware Attentive Multi-ROI Modeling. (arXiv:2201.01838v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01838","description":"<p>Osteoporosis is a common chronic metabolic bone disease that is often\nunder-diagnosed and under-treated due to the limited access to bone mineral\ndensity (BMD) examinations, e.g. via Dual-energy X-ray Absorptiometry (DXA). In\nthis paper, we propose a method to predict BMD from Chest X-ray (CXR), one of\nthe most commonly accessible and low-cost medical imaging examinations. Our\nmethod first automatically detects Regions of Interest (ROIs) of local and\nglobal bone structures from the CXR. Then a multi-ROI deep model with\ntransformer encoder is developed to exploit both local and global information\nin the chest X-ray image for accurate BMD estimation. Our method is evaluated\non 13719 CXR patient cases with their ground truth BMD scores measured by\ngold-standard DXA. The model predicted BMD has a strong correlation with the\nground truth (Pearson correlation coefficient 0.889 on lumbar 1). When applied\nfor osteoporosis screening, it achieves a high classification performance (AUC\n0.963 on lumbar 1). As the first effort in the field using CXR scans to predict\nthe BMD, the proposed algorithm holds strong potential in early osteoporosis\nscreening and public health promotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fakai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_K/0/1/0/all/0/1\">Kang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chang-Fu Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_S/0/1/0/all/0/1\">Shun Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Real-World Adversarial Robustness of Real-Time Semantic Segmentation Models for Autonomous Driving. (arXiv:2201.01850v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01850","description":"<p>The existence of real-world adversarial examples (commonly in the form of\npatches) poses a serious threat for the use of deep learning models in\nsafety-critical computer vision tasks such as visual perception in autonomous\ndriving. This paper presents an extensive evaluation of the robustness of\nsemantic segmentation models when attacked with different types of adversarial\npatches, including digital, simulated, and physical ones. A novel loss function\nis proposed to improve the capabilities of attackers in inducing a\nmisclassification of pixels. Also, a novel attack strategy is presented to\nimprove the Expectation Over Transformation method for placing a patch in the\nscene. Finally, a state-of-the-art method for detecting adversarial patch is\nfirst extended to cope with semantic segmentation models, then improved to\nobtain real-time performance, and eventually evaluated in real-world scenarios.\nExperimental results reveal that, even though the adversarial effect is visible\nwith both digital and real-world attacks, its impact is often spatially\nconfined to areas of the image around the patch. This opens to further\nquestions about the spatial robustness of real-time semantic segmentation\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rossolini_G/0/1/0/all/0/1\">Giulio Rossolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesti_F/0/1/0/all/0/1\">Federico Nesti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_G/0/1/0/all/0/1\">Gianluca D&#x27;Amico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Saasha Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_A/0/1/0/all/0/1\">Alessandro Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttazzo_G/0/1/0/all/0/1\">Giorgio Buttazzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Grid Redundant Bounding Box Annotation for Accurate Object Detection. (arXiv:2201.01857v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01857","description":"<p>Modern leading object detectors are either two-stage or one-stage networks\nrepurposed from a deep CNN-based backbone classifier network. YOLOv3 is one\nsuch very-well known state-of-the-art one-shot detector that takes in an input\nimage and divides it into an equal-sized grid matrix. The grid cell having the\ncenter of an object is the one responsible for detecting the particular object.\nThis paper presents a new mathematical approach that assigns multiple grids per\nobject for accurately tight-fit bounding box prediction. We also propose an\neffective offline copy-paste data augmentation for object detection. Our\nproposed method significantly outperforms some current state-of-the-art object\ndetectors with a prospect for further better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tesema_S/0/1/0/all/0/1\">Solomon Negussie Tesema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourennane_E/0/1/0/all/0/1\">El-Bay Bourennane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards realistic symmetry-based completion of previously unseen point clouds. (arXiv:2201.01858v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01858","description":"<p>3D scanning is a complex multistage process that generates a point cloud of\nan object typically containing damaged parts due to occlusions, reflections,\nshadows, scanner motion, specific properties of the object surface, imperfect\nreconstruction algorithms, etc. Point cloud completion is specifically designed\nto fill in the missing parts of the object and obtain its high-quality 3D\nrepresentation. The existing completion approaches perform well on the academic\ndatasets with a predefined set of object classes and very specific types of\ndefects; however, their performance drops significantly in the real-world\nsettings and degrades even further on previously unseen object classes.\n</p>\n<p>We propose a novel framework that performs well on symmetric objects, which\nare ubiquitous in man-made environments. Unlike learning-based approaches, the\nproposed framework does not require training data and is capable of completing\nnon-critical damages occurring in customer 3D scanning process using e.g.\nKinect, time-of-flight, or structured light scanners. With thorough\nexperiments, we demonstrate that the proposed framework achieves\nstate-of-the-art efficiency in point cloud completion of real-world customer\nscans. We benchmark the framework performance on two types of datasets:\nproperly augmented existing academic dataset and the actual 3D scans of various\nobjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rumezhak_T/0/1/0/all/0/1\">Taras Rumezhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobosevych_O/0/1/0/all/0/1\">Oles Dobosevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hryniv_R/0/1/0/all/0/1\">Rostyslav Hryniv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selotkin_V/0/1/0/all/0/1\">Vladyslav Selotkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpiv_V/0/1/0/all/0/1\">Volodymyr Karpiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymenko_M/0/1/0/all/0/1\">Mykola Maksymenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMLS: Geometry-Aware Control Point Deformation. (arXiv:2201.01873v1 [cs.GR])","link":"http://arxiv.org/abs/2201.01873","description":"<p>We introduce DeepMLS, a space-based deformation technique, guided by a set of\ndisplaced control points. We leverage the power of neural networks to inject\nthe underlying shape geometry into the deformation parameters. The goal of our\ntechnique is to enable a realistic and intuitive shape deformation. Our method\nis built upon moving least-squares (MLS), since it minimizes a weighted sum of\nthe given control point displacements. Traditionally, the influence of each\ncontrol point on every point in space (i.e., the weighting function) is defined\nusing inverse distance heuristics. In this work, we opt to learn the weighting\nfunction, by training a neural network on the control points from a single\ninput shape, and exploit the innate smoothness of neural networks. Our\ngeometry-aware control point deformation is agnostic to the surface\nrepresentation and quality; it can be applied to point clouds or meshes,\nincluding non-manifold and disconnected surface soups. We show that our\ntechnique facilitates intuitive piecewise smooth deformations, which are well\nsuited for manufactured objects. We show the advantages of our approach\ncompared to existing surface and space-based deformation techniques, both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shechter_M/0/1/0/all/0/1\">Meitar Shechter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzer_G/0/1/0/all/0/1\">Gal Metzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-guided Image De-raining Using Time-Lapse Data. (arXiv:2201.01883v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01883","description":"<p>This paper addresses the problem of single image de-raining, that is, the\ntask of recovering clean and rain-free background scenes from a single image\nobscured by a rainy artifact. Although recent advances adopt real-world\ntime-lapse data to overcome the need for paired rain-clean images, they are\nlimited to fully exploit the time-lapse data. The main cause is that, in terms\nof network architectures, they could not capture long-term rain streak\ninformation in the time-lapse data during training owing to the lack of memory\ncomponents. To address this problem, we propose a novel network architecture\nbased on a memory network that explicitly helps to capture long-term rain\nstreak information in the time-lapse data. Our network comprises the\nencoder-decoder networks and a memory network. The features extracted from the\nencoder are read and updated in the memory network that contains several memory\nitems to store rain streak-aware feature representations. With the read/update\noperation, the memory network retrieves relevant memory items in terms of the\nqueries, enabling the memory items to represent the various rain streaks\nincluded in the time-lapse data. To boost the discriminative power of memory\nfeatures, we also present a novel background selective whitening (BSW) loss for\ncapturing only rain streak information in the memory network by erasing the\nbackground information. Experimental results on standard benchmarks demonstrate\nthe effectiveness and superiority of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaehoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow-Guided Sparse Transformer for Video Deblurring. (arXiv:2201.01893v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01893","description":"<p>Exploiting similar and sharper scene patches in spatio-temporal neighborhoods\nis critical for video deblurring. However, CNN-based methods show limitations\nin capturing long-range dependencies and modeling non-local self-similarity. In\nthis paper, we propose a novel framework, Flow-Guided Sparse Transformer\n(FGST), for video deblurring. In FGST, we customize a self-attention module,\nFlow-Guided Sparse Window-based Multi-head Self-Attention (FGSW-MSA). For each\n$query$ element on the blurry reference frame, FGSW-MSA enjoys the guidance of\nthe estimated optical flow to globally sample spatially sparse yet highly\nrelated $key$ elements corresponding to the same scene patch in neighboring\nframes. Besides, we present a Recurrent Embedding (RE) mechanism to transfer\ninformation from past frames and strengthen long-range temporal dependencies.\nComprehensive experiments demonstrate that our proposed FGST outperforms\nstate-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields\nmore visually pleasing results in real video deblurring. Code and models will\nbe released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_X/0/1/0/all/0/1\">Xueyi Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Object Grounding Using Scene Graphs. (arXiv:2201.01901v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01901","description":"<p>Object grounding tasks aim to locate the target object in an image through\nverbal communications. Understanding human command is an important process\nneeded for effective human-robot communication. However, this is challenging\nbecause human commands can be ambiguous and erroneous. This paper aims to\ndisambiguate the human's referring expressions by allowing the agent to ask\nrelevant questions based on semantic data obtained from scene graphs. We test\nif our agent can use relations between objects from a scene graph to ask\nsemantically relevant questions that can disambiguate the original user\ncommand. In this paper, we present Incremental Grounding using Scene Graphs\n(IGSG), a disambiguation model that uses semantic data from an image scene\ngraph and linguistic structures from a language scene graph to ground objects\nbased on human command. Compared to the baseline, IGSG shows promising results\nin complex real-world scenes where there are multiple identical target objects.\nIGSG can effectively disambiguate ambiguous or wrong referring expressions by\nasking disambiguating questions back to the user.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">John Seon Keun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1\">Sonia Chernova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Neighborhood Alignment. (arXiv:2201.01922v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01922","description":"<p>We present Contrastive Neighborhood Alignment (CNA), a manifold learning\napproach to maintain the topology of learned features whereby data points that\nare mapped to nearby representations by the source (teacher) model are also\nmapped to neighbors by the target (student) model. The target model aims to\nmimic the local structure of the source representation space using a\ncontrastive loss. CNA is an unsupervised learning algorithm that does not\nrequire ground-truth labels for the individual samples. CNA is illustrated in\nthree scenarios: manifold learning, where the model maintains the local\ntopology of the original data in a dimension-reduced space; model distillation,\nwhere a small student model is trained to mimic a larger teacher; and legacy\nmodel update, where an older model is replaced by a more powerful one.\nExperiments show that CNA is able to capture the manifold in a high-dimensional\nspace and improves performance compared to the competing methods in their\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengkai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_L/0/1/0/all/0/1\">Luis Goncalves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1\">Vijay Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization. (arXiv:2201.01928v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01928","description":"<p>Augmented reality devices have the potential to enhance human perception and\nenable other assistive functionalities in complex conversational environments.\nEffectively capturing the audio-visual context necessary for understanding\nthese social interactions first requires detecting and localizing the voice\nactivities of the device wearer and the surrounding people. These tasks are\nchallenging due to their egocentric nature: the wearer's head motion may cause\nmotion blur, surrounding people may appear in difficult viewing angles, and\nthere may be occlusions, visual clutter, audio noise, and bad lighting. Under\nthese conditions, previous state-of-the-art active speaker detection methods do\nnot give satisfactory results. Instead, we tackle the problem from a new\nsetting using both video and multi-channel microphone array audio. We propose a\nnovel end-to-end deep learning approach that is able to give robust voice\nactivity detection and localization results. In contrast to previous methods,\nour method localizes active speakers from all possible directions on the\nsphere, even outside the camera's field of view, while simultaneously detecting\nthe device wearer's own voice activity. Our experiments show that the proposed\nmethod gives superior results, can run in real time, and is robust against\nnoise and clutter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murdock_C/0/1/0/all/0/1\">Calvin Murdock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1\">Vamsi Krishna Ithapu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decompose to Adapt: Cross-domain Object Detection via Feature Disentanglement. (arXiv:2201.01929v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01929","description":"<p>Recent advances in unsupervised domain adaptation (UDA) techniques have\nwitnessed great success in cross-domain computer vision tasks, enhancing the\ngeneralization ability of data-driven deep learning architectures by bridging\nthe domain distribution gaps. For the UDA-based cross-domain object detection\nmethods, the majority of them alleviate the domain bias by inducing the\ndomain-invariant feature generation via adversarial learning strategy. However,\ntheir domain discriminators have limited classification ability due to the\nunstable adversarial training process. Therefore, the extracted features\ninduced by them cannot be perfectly domain-invariant and still contain\ndomain-private factors, bringing obstacles to further alleviate the\ncross-domain discrepancy. To tackle this issue, we design a Domain\nDisentanglement Faster-RCNN (DDF) to eliminate the source-specific information\nin the features for detection task learning. Our DDF method facilitates the\nfeature disentanglement at the global and local stages, with a Global Triplet\nDisentanglement (GTD) module and an Instance Similarity Disentanglement (ISD)\nmodule, respectively. By outperforming state-of-the-art methods on four\nbenchmark UDA object detection tasks, our DDF method is demonstrated to be\neffective with wide applicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnett_M/0/1/0/all/0/1\">Michael Barnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Scene Parsing: From Tile-level Scene Classification to Pixel-wise Semantic Labeling. (arXiv:2201.01953v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01953","description":"<p>Given an aerial image, aerial scene parsing (ASP) targets to interpret the\nsemantic structure of the image content, e.g., by assigning a semantic label to\nevery pixel of the image. With the popularization of data-driven methods, the\npast decades have witnessed promising progress on ASP by approaching the\nproblem with the schemes of tile-level scene classification or\nsegmentation-based image analysis, when using high-resolution aerial images.\nHowever, the former scheme often produces results with tile-wise boundaries,\nwhile the latter one needs to handle the complex modeling process from pixels\nto semantics, which often requires large-scale and well-annotated image samples\nwith pixel-wise semantic labels. In this paper, we address these issues in ASP,\nwith perspectives from tile-level scene classification to pixel-wise semantic\nlabeling. Specifically, we first revisit aerial image interpretation by a\nliterature review. We then present a large-scale scene classification dataset\nthat contains one million aerial images termed Million-AID. With the presented\ndataset, we also report benchmarking experiments using classical convolutional\nneural networks (CNNs). Finally, we perform ASP by unifying the tile-level\nscene classification and object-based image analysis to achieve pixel-wise\nsemantic labeling. Intensive experiments show that Million-AID is a challenging\nyet useful dataset, which can serve as a benchmark for evaluating newly\ndeveloped algorithms. When transferring knowledge from Million-AID, fine-tuning\nCNN models pretrained on Million-AID perform consistently better than those\npretrained ImageNet for aerial scene classification. Moreover, our designed\nhierarchical multi-task learning method achieves the state-of-the-art\npixel-wise classification on the challenging GID, bridging the tile-level scene\nclassification toward pixel-wise semantic labeling for aerial image\ninterpretation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deren Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Generalization and Specialization in Zero-shot Learning. (arXiv:2201.01961v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01961","description":"<p>Zero-Shot Learning (ZSL) aims to transfer classification capability from seen\nto unseen classes. Recent methods have proved that generalization and\nspecialization are two essential abilities to achieve good performance in ZSL.\nHowever, they all focus on only one of the abilities, resulting in models that\nare either too general with the degraded classifying ability or too specialized\nto generalize to unseen classes. In this paper, we propose an end-to-end\nnetwork with balanced generalization and specialization abilities, termed as\nBGSNet, to take advantage of both abilities, and balance them at instance- and\ndataset-level. Specifically, BGSNet consists of two branches: the\nGeneralization Network (GNet), which applies episodic meta-learning to learn\ngeneralized knowledge, and the Balanced Specialization Network (BSNet), which\nadopts multiple attentive extractors to extract discriminative features and\nfulfill the instance-level balance. A novel self-adjusting diversity loss is\ndesigned to optimize BSNet with less redundancy and more diversity. We further\npropose a differentiable dataset-level balance and update the weights in a\nlinear annealing schedule to simulate network pruning and thus obtain the\noptimal structure for BSNet at a low cost with dataset-level balance achieved.\nExperiments on four benchmark datasets demonstrate our model's effectiveness.\nSufficient component ablations prove the necessity of integrating\ngeneralization and specialization abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lina Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Classification on Remote-Sensing Images. (arXiv:2201.01971v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01971","description":"<p>Acquiring information on large areas on the earth's surface through satellite\ncameras allows us to see much more than we can see while standing on the\nground. This assists us in detecting and monitoring the physical\ncharacteristics of an area like land-use patterns, atmospheric conditions,\nforest cover, and many unlisted aspects. The obtained images not only keep\ntrack of continuous natural phenomena but are also crucial in tackling the\nglobal challenge of severe deforestation. Among which Amazon basin accounts for\nthe largest share every year. Proper data analysis would help limit detrimental\neffects on the ecosystem and biodiversity with a sustainable healthy\natmosphere. This report aims to label the satellite image chips of the Amazon\nrainforest with atmospheric and various classes of land cover or land use\nthrough different machine learning and superior deep learning models.\nEvaluation is done based on the F2 metric, while for loss function, we have\nboth sigmoid cross-entropy as well as softmax cross-entropy. Images are fed\nindirectly to the machine learning classifiers after only features are\nextracted using pre-trained ImageNet architectures. Whereas for deep learning\nmodels, ensembles of fine-tuned ImageNet pre-trained models are used via\ntransfer learning. Our best score was achieved so far with the F2 metric is\n0.927.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditya Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_B/0/1/0/all/0/1\">B. Uma Shankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SASA: Semantics-Augmented Set Abstraction for Point-based 3D Object Detection. (arXiv:2201.01976v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01976","description":"<p>Although point-based networks are demonstrated to be accurate for 3D point\ncloud modeling, they are still falling behind their voxel-based competitors in\n3D detection. We observe that the prevailing set abstraction design for\ndown-sampling points may maintain too much unimportant background information\nthat can affect feature learning for detecting objects. To tackle this issue,\nwe propose a novel set abstraction method named Semantics-Augmented Set\nAbstraction (SASA). Technically, we first add a binary segmentation module as\nthe side output to help identify foreground points. Based on the estimated\npoint-wise foreground scores, we then propose a semantics-guided point sampling\nalgorithm to help retain more important foreground points during down-sampling.\nIn practice, SASA shows to be effective in identifying valuable points related\nto foreground objects and improving feature learning for point-based 3D\ndetection. Additionally, it is an easy-to-plug-in module and able to boost\nvarious point-based detectors, including single-stage and two-stage ones.\nExtensive experiments on the popular KITTI and nuScenes datasets validate the\nsuperiority of SASA, lifting point-based detection models to reach comparable\nperformance to state-of-the-art voxel-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Abstraction-Refinement Approach to Verifying Convolutional Neural Networks. (arXiv:2201.01978v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01978","description":"<p>Convolutional neural networks have gained vast popularity due to their\nexcellent performance in the fields of computer vision, image processing, and\nothers. Unfortunately, it is now well known that convolutional networks often\nproduce erroneous results - for example, minor perturbations of the inputs of\nthese networks can result in severe classification errors. Numerous\nverification approaches have been proposed in recent years to prove the absence\nof such errors, but these are typically geared for fully connected networks and\nsuffer from exacerbated scalability issues when applied to convolutional\nnetworks. To address this gap, we present here the Cnn-Abs framework, which is\nparticularly aimed at the verification of convolutional networks. The core of\nCnn-Abs is an abstraction-refinement technique, which simplifies the\nverification problem through the removal of convolutional connections in a way\nthat soundly creates an over-approximation of the original problem; and which\nrestores these connections if the resulting problem becomes too abstract.\nCnn-Abs is designed to use existing verification engines as a backend, and our\nevaluation demonstrates that it can significantly boost the performance of a\nstate-of-the-art DNN verification engine, reducing runtime by 15.7% on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ostrovsky_M/0/1/0/all/0/1\">Matan Ostrovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1\">Clark Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Guy Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Domain Joint Training for Person Re-Identification. (arXiv:2201.01983v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01983","description":"<p>Deep learning-based person Re-IDentification (ReID) often requires a large\namount of training data to achieve good performance. Thus it appears that\ncollecting more training data from diverse environments tends to improve the\nReID performance. This paper re-examines this common belief and makes a somehow\nsurprising observation: using more samples, i.e., training with samples from\nmultiple datasets, does not necessarily lead to better performance by using the\npopular ReID models. In some cases, training with more samples may even hurt\nthe performance of the evaluation is carried out in one of those datasets. We\npostulate that this phenomenon is due to the incapability of the standard\nnetwork in adapting to diverse environments. To overcome this issue, we propose\nan approach called Domain-Camera-Sample Dynamic network (DCSD) whose parameters\ncan be adaptive to various factors. Specifically, we consider the internal\ndomain-related factor that can be identified from the input features, and\nexternal domain-related factors, such as domain information or camera\ninformation. Our discovery is that training with such an adaptive model can\nbetter benefit from more training samples. Experimental results show that our\nDCSD can greatly boost the performance (up to 12.3%) while joint training in\nmultiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compact Bidirectional Transformer for Image Captioning. (arXiv:2201.01984v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01984","description":"<p>Most current image captioning models typically generate captions from left to\nright. This unidirectional property makes them can only leverage past context\nbut not future context. Though recent refinement-based models can exploit both\npast and future context by generating a new caption in the second stage based\non pre-retrieved or pre-generated captions in the first stage, the decoder of\nthese models generally consists of two networks~(i.e. a retriever or captioner\nin the first stage and a refiner in the second stage), which can only be\nexecuted sequentially. In this paper, we introduce a Compact Bidirectional\nTransformer model for image captioning that can leverage bidirectional context\nimplicitly and explicitly while the decoder can be executed parallelly.\nSpecifically, it is implemented by tightly coupling left-to-right(L2R) and\nright-to-left(R2L) flows into a single compact model~(i.e. implicitly) and\noptionally allowing interaction of the two flows(i.e. explicitly), while the\nfinal caption is chosen from either L2R or R2L flow in a sentence-level\nensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark\nand find that the compact architecture, which serves as a regularization for\nimplicitly exploiting bidirectional context, and the sentence-level ensemble\nplay more important roles than the explicit interaction mechanism. By combining\nwith word-level ensemble seamlessly, the effect of the sentence-level ensemble\nis further enlarged. We further extend the conventional one-flow self-critical\ntraining to the two-flows version under this architecture and achieve new\nstate-of-the-art results in comparison with non-vision-language-pretraining\nmodels. Source code is available at\n{\\color{magenta}\\url{https://github.com/YuanEZhou/CBTrans}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenzhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_H/0/1/0/all/0/1\">Huixia Ben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVPR: Transformer-based place recognition with multi-level attention aggregation. (arXiv:2201.02001v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02001","description":"<p>Visual place recognition is a challenging task for applications such as\nautonomous driving navigation and mobile robot localization. Distracting\nelements presenting in complex scenes often lead to deviations in the\nperception of visual place. To address this problem, it is crucial to integrate\ninformation from only task-relevant regions into image representations. In this\npaper, we introduce a novel holistic place recognition model, TransVPR, based\non vision Transformers. It benefits from the desirable property of the\nself-attention operation in Transformers which can naturally aggregate\ntask-relevant features. Attentions from multiple levels of the Transformer,\nwhich focus on different regions of interest, are further combined to generate\na global image representation. In addition, the output tokens from Transformer\nlayers filtered by the fused attention mask are considered as key-patch\ndescriptors, which are used to perform spatial matching to re-rank the\ncandidates retrieved by the global image features. The whole model allows\nend-to-end training with a single objective and image-level supervision.\nTransVPR achieves state-of-the-art performance on several real-world benchmarks\nwhile maintaining low computational time and storage requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruotong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Weiliang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_N/0/1/0/all/0/1\">Nanning Zhen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training Vision Language BERTs with a Unified Conditional Model. (arXiv:2201.02010v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02010","description":"<p>Natural language BERTs are trained with language corpus in a self-supervised\nmanner. Unlike natural language BERTs, vision language BERTs need paired data\nto train, which restricts the scale of VL-BERT pretraining. We propose a\nself-training approach that allows training VL-BERTs from unlabeled image data.\nThe proposed method starts with our unified conditional model -- a vision\nlanguage BERT model that can perform zero-shot conditional generation. Given\ndifferent conditions, the unified conditional model can generate captions,\ndense captions, and even questions. We use the labeled image data to train a\nteacher model and use the trained model to generate pseudo captions on\nunlabeled image data. We then combine the labeled data and pseudo labeled data\nto train a student model. The process is iterated by putting the student model\nas a new teacher. By using the proposed self-training approach and only 300k\nunlabeled extra data, we are able to get competitive or even better\nperformances compared to the models of similar model size trained with 3\nmillion extra image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaofeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1\">Fengmao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An unambiguous cloudiness index for nonwovens. (arXiv:2201.02011v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02011","description":"<p>Cloudiness or formation is a concept routinely used in industry to address\ndeviations from homogeneity in nonwovens and papers. Measuring a cloudiness\nindex based on image data is a common task in industrial quality assurance. The\ntwo most popular ways of quantifying cloudiness are based on power spectrum or\ncorrelation function on the one hand or the Laplacian pyramid on the other\nhand. Here, we recall the mathematical basis of the first approach\ncomprehensively, derive a cloudiness index, and demonstrate its practical\nestimation. We prove that the Laplacian pyramid as well as other quantities\ncharacterizing cloudiness like the range of interaction and the intensity of\nsmall-angle scattering are very closely related to the power spectrum. Finally,\nwe show that the power spectrum is easy to be measured image analytically and\ncarries more information than the alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godehardt_M/0/1/0/all/0/1\">Michael Godehardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghiseh_A/0/1/0/all/0/1\">Ali Moghiseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oetjen_C/0/1/0/all/0/1\">Christine Oetjen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohser_J/0/1/0/all/0/1\">Joachim Ohser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schladitz_K/0/1/0/all/0/1\">Katja Schladitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Egocentric 3D Pose Estimation with Third Person Views. (arXiv:2201.02017v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02017","description":"<p>In this paper, we propose a novel approach to enhance the 3D body pose\nestimation of a person computed from videos captured from a single wearable\ncamera. The key idea is to leverage high-level features linking first- and\nthird-views in a joint embedding space. To learn such embedding space we\nintroduce First2Third-Pose, a new paired synchronized dataset of nearly 2,000\nvideos depicting human activities captured from both first- and third-view\nperspectives. We explicitly consider spatial- and motion-domain features,\ncombined using a semi-Siamese architecture trained in a self-supervised\nfashion. Experimental results demonstrate that the joint multi-view embedded\nspace learned with our dataset is useful to extract discriminatory features\nfrom arbitrary single-view egocentric videos, without needing domain adaptation\nor knowledge of camera parameters. We achieve significant improvement of\negocentric 3D body pose estimation performance on two unconstrained datasets,\nover three supervised state-of-the-art approaches. Our dataset and code will be\navailable for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhamanaskar_A/0/1/0/all/0/1\">Ameya Dhamanaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1\">Enric Corona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1\">Albert Pumarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Light in the Dark: Deep Learning Practices for Industrial Computer Vision. (arXiv:2201.02028v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02028","description":"<p>In recent years, large pre-trained deep neural networks (DNNs) have\nrevolutionized the field of computer vision (CV). Although these DNNs have been\nshown to be very well suited for general image recognition tasks, application\nin industry is often precluded for three reasons: 1) large pre-trained DNNs are\nbuilt on hundreds of millions of parameters, making deployment on many devices\nimpossible, 2) the underlying dataset for pre-training consists of general\nobjects, while industrial cases often consist of very specific objects, such as\nstructures on solar wafers, 3) potentially biased pre-trained DNNs raise legal\nissues for companies. As a remedy, we study neural networks for CV that we\ntrain from scratch. For this purpose, we use a real-world case from a solar\nwafer manufacturer. We find that our neural networks achieve similar\nperformances as pre-trained DNNs, even though they consist of far fewer\nparameters and do not rely on third-party datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harl_M/0/1/0/all/0/1\">Maximilian Harl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herchenbach_M/0/1/0/all/0/1\">Marvin Herchenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschel_S/0/1/0/all/0/1\">Sven Kruschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hambauer_N/0/1/0/all/0/1\">Nico Hambauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zschech_P/0/1/0/all/0/1\">Patrick Zschech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Framework for Attention-Based Few-Shot Object Detection. (arXiv:2201.02052v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02052","description":"<p>Few-Shot Object Detection (FSOD) is a rapidly growing field in computer\nvision. It consists in finding all occurrences of a given set of classes with\nonly a few annotated examples for each class. Numerous methods have been\nproposed to address this challenge and most of them are based on attention\nmechanisms. However, the great variety of classic object detection frameworks\nand training strategies makes performance comparison between methods difficult.\nIn particular, for attention-based FSOD methods, it is laborious to compare the\nimpact of the different attention mechanisms on performance. This paper aims at\nfilling this shortcoming. To do so, a flexible framework is proposed to allow\nthe implementation of most of the attention techniques available in the\nliterature. To properly introduce such a framework, a detailed review of the\nexisting FSOD methods is firstly provided. Some different attention mechanisms\nare then reimplemented within the framework and compared with all other\nparameters fixed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeune_P/0/1/0/all/0/1\">Pierre Le Jeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokraoui_A/0/1/0/all/0/1\">Anissa Mokraoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLAN: A Graph-based Linear Assignment Network. (arXiv:2201.02057v1 [cs.LG])","link":"http://arxiv.org/abs/2201.02057","description":"<p>Differentiable solvers for the linear assignment problem (LAP) have attracted\nmuch research attention in recent years, which are usually embedded into\nlearning frameworks as components. However, previous algorithms, with or\nwithout learning strategies, usually suffer from the degradation of the\noptimality with the increment of the problem size. In this paper, we propose a\nlearnable linear assignment solver based on deep graph networks. Specifically,\nwe first transform the cost matrix to a bipartite graph and convert the\nassignment task to the problem of selecting reliable edges from the constructed\ngraph. Subsequently, a deep graph network is developed to aggregate and update\nthe features of nodes and edges. Finally, the network predicts a label for each\nedge that indicates the assignment relationship. The experimental results on a\nsynthetic dataset reveal that our method outperforms state-of-the-art baselines\nand achieves consistently high accuracy with the increment of the problem size.\nFurthermore, we also embed the proposed solver, in comparison with\nstate-of-the-art baseline solvers, into a popular multi-object tracking (MOT)\nframework to train the tracker in an end-to-end manner. The experimental\nresults on MOT benchmarks illustrate that the proposed LAP solver improves the\ntracker by the largest margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">He Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Congyan Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Songhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign Language. (arXiv:2201.02065v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02065","description":"<p>Sign language is an essential resource enabling access to communication and\nproper socioemotional development for individuals suffering from disabling\nhearing loss. As this population is expected to reach 700 million by 2050, the\nimportance of the language becomes even more essential as it plays a critical\nrole to ensure the inclusion of such individuals in society. The Sign Language\nRecognition field aims to bridge the gap between users and non-users of sign\nlanguages. However, the scarcity in quantity and quality of datasets is one of\nthe main challenges limiting the exploration of novel approaches that could\nlead to significant advancements in this research area. Thus, this paper\ncontributes by introducing two new datasets for the American Sign Language: the\nfirst is composed of the three-dimensional representation of the signers and,\nthe second, by an unprecedented linguistics-based representation containing a\nset of phonological attributes of the signs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amorim_C/0/1/0/all/0/1\">Cleison Correia de Amorim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EM-driven unsupervised learning for efficient motion segmentation. (arXiv:2201.02074v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02074","description":"<p>This paper presents a CNN-based fully unsupervised method for motion\nsegmentation from optical flow. We assume that the input optical flow can be\nrepresented as a piecewise set of parametric motion models, typically, affine\nor quadratic motion models.The core idea of this work is to leverage the\nExpectation-Maximization (EM) framework. It enables us to design in a\nwell-founded manner the loss function and the training procedure of our motion\nsegmentation neural network. However, in contrast to the classical iterative\nEM, once the network is trained, we can provide a segmentation for any unseen\noptical flow field in a single inference step, with no dependence on the\ninitialization of the motion model parameters since they are not estimated in\nthe inference stage. Different loss functions have been investigated including\nrobust ones. We also propose a novel data augmentation technique on the optical\nflow field with a noticeable impact on the performance. We tested our motion\nsegmentation network on the DAVIS2016 dataset. Our method outperforms\ncomparable unsupervised methods and is very efficient. Indeed, it can run at\n125fps making it usable for real-time applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meunier_E/0/1/0/all/0/1\">Etienne Meunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badoual_A/0/1/0/all/0/1\">Ana&#xef;s Badoual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouthemy_P/0/1/0/all/0/1\">Patrick Bouthemy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Based Classification System For Recognizing Local Spinach. (arXiv:2201.02093v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02093","description":"<p>A deep learning model gives an incredible result for image processing by\nstudying from the trained dataset. Spinach is a leaf vegetable that contains\nvitamins and nutrients. In our research, a Deep learning method has been used\nthat can automatically identify spinach and this method has a dataset of a\ntotal of five species of spinach that contains 3785 images. Four Convolutional\nNeural Network (CNN) models were used to classify our spinach. These models\ngive more accurate results for image classification. Before applying these\nmodels there is some preprocessing of the image data. For the preprocessing of\ndata, some methods need to happen. Those are RGB conversion, filtering, resize\n&amp; rescaling, and categorization. After applying these methods image data are\npre-processed and ready to be used in the classifier algorithms. The accuracy\nof these classifiers is in between 98.68% - 99.79%. Among those models, VGG16\nachieved the highest accuracy of 99.79%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mirajul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ria_N/0/1/0/all/0/1\">Nushrat Jahan Ria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ani_J/0/1/0/all/0/1\">Jannatul Ferdous Ani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masum_A/0/1/0/all/0/1\">Abu Kaisar Mohammad Masum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abujar_S/0/1/0/all/0/1\">Sheikh Abujar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Syed Akhter Hossain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperionSolarNet: Solar Panel Detection from Aerial Images. (arXiv:2201.02107v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02107","description":"<p>With the effects of global climate change impacting the world, collective\nefforts are needed to reduce greenhouse gas emissions. The energy sector is the\nsingle largest contributor to climate change and many efforts are focused on\nreducing dependence on carbon-emitting power plants and moving to renewable\nenergy sources, such as solar power. A comprehensive database of the location\nof solar panels is important to assist analysts and policymakers in defining\nstrategies for further expansion of solar energy. In this paper we focus on\ncreating a world map of solar panels. We identify locations and total surface\narea of solar panels within a given geographic area. We use deep learning\nmethods for automated detection of solar panel locations and their surface area\nusing aerial imagery. The framework, which consists of a two-branch model using\nan image classifier in tandem with a semantic segmentation model, is trained on\nour created dataset of satellite images. Our work provides an efficient and\nscalable method for detecting solar panels, achieving an accuracy of 0.96 for\nclassification and an IoU score of 0.82 for segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parhar_P/0/1/0/all/0/1\">Poonam Parhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawasaki_R/0/1/0/all/0/1\">Ryan Sawasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todeschini_A/0/1/0/all/0/1\">Alberto Todeschini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Colorado Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahabi_H/0/1/0/all/0/1\">Hossein Vahabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nusaputra_N/0/1/0/all/0/1\">Nathan Nusaputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vergara_F/0/1/0/all/0/1\">Felipe Vergara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eye Know You Too: A DenseNet Architecture for End-to-end Biometric Authentication via Eye Movements. (arXiv:2201.02110v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02110","description":"<p>Plain convolutional neural networks (CNNs) have been used to achieve\nstate-of-the-art performance in various domains in the past years, including\nbiometric authentication via eye movements. There have been many relatively\nrecent improvements to plain CNNs, including residual networks (ResNets) and\ndensely connected convolutional networks (DenseNets). Although these networks\nprimarily target image processing domains, they can be easily modified to work\nwith time series data. We employ a DenseNet architecture for end-to-end\nbiometric authentication via eye movements. We compare our model against the\nmost relevant prior works including the current state-of-the-art. We find that\nour model achieves state-of-the-art performance for all considered training\nconditions and data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lohr_D/0/1/0/all/0/1\">Dillon Lohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komogortsev_O/0/1/0/all/0/1\">Oleg V Komogortsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bio-inspired Min-Nets Improve the Performance and Robustness of Deep Networks. (arXiv:2201.02149v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02149","description":"<p>Min-Nets are inspired by end-stopped cortical cells with units that output\nthe minimum of two learned filters. We insert such Min-units into\nstate-of-the-art deep networks, such as the popular ResNet and DenseNet, and\nshow that the resulting Min-Nets perform better on the Cifar-10 benchmark.\nMoreover, we show that Min-Nets are more robust against JPEG compression\nartifacts. We argue that the minimum operation is the simplest way of\nimplementing an AND operation on pairs of filters and that such AND operations\nintroduce a bias that is appropriate given the statistics of natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gruning_P/0/1/0/all/0/1\">Philipp Gr&#xfc;ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_E/0/1/0/all/0/1\">Erhardt Barth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction. (arXiv:2201.02184v1 [eess.AS])","link":"http://arxiv.org/abs/2201.02184","description":"<p>Video recordings of speech contain correlated audio and visual information,\nproviding a strong signal for speech representation learning from the speaker's\nlip movements and the produced sound. We introduce Audio-Visual Hidden Unit\nBERT (AV-HuBERT), a self-supervised representation learning framework for\naudio-visual speech, which masks multi-stream video input and predicts\nautomatically discovered and iteratively refined multimodal hidden units.\nAV-HuBERT learns powerful audio-visual speech representation benefiting both\nlip-reading and automatic speech recognition. On the largest public lip-reading\nbenchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of\nlabeled data, outperforming the former state-of-the-art approach (33.6%)\ntrained with a thousand times more transcribed video data (31K hours). The\nlip-reading WER is further reduced to 26.9% when using all 433 hours of labeled\ndata from LRS3 and combined with self-training. Using our audio-visual\nrepresentation on the same benchmark for audio-only speech recognition leads to\na 40% relative WER reduction over the state-of-the-art performance (1.3% vs\n2.3%). Our code and models are available at\nhttps://github.com/facebookresearch/av_hubert\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Full-Body Anonymization with Surface-Guided GANs. (arXiv:2201.02193v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02193","description":"<p>Recent work on image anonymization has shown that generative adversarial\nnetworks (GANs) can generate near-photorealistic faces to anonymize\nindividuals. However, scaling these networks to the entire human body has\nremained a challenging and yet unsolved task. We propose a new anonymization\nmethod that generates close-to-photorealistic humans for in-the-wild images.A\nkey part of our design is to guide adversarial nets by dense pixel-to-surface\ncorrespondences between an image and a canonical 3D surface.We introduce\nVariational Surface-Adaptive Modulation (V-SAM) that embeds surface information\nthroughout the generator.Combining this with our novel discriminator surface\nsupervision loss, the generator can synthesize high quality humans with diverse\nappearance in complex and varying scenes.We show that surface guidance\nsignificantly improves image quality and diversity of samples, yielding a\nhighly practical generator.Finally, we demonstrate that surface-guided\nanonymization preserves the usability of data for future computer vision\ndevelopment\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hukkelaas_H/0/1/0/all/0/1\">H&#xe5;kon Hukkel&#xe5;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smebye_M/0/1/0/all/0/1\">Morten Smebye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mester_R/0/1/0/all/0/1\">Rudolf Mester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindseth_F/0/1/0/all/0/1\">Frank Lindseth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Metric Structured Learning For Facial Expression Recognition. (arXiv:2001.06612v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.06612","description":"<p>We propose a deep metric learning model to create embedded sub-spaces with a\nwell defined structure. A new loss function that imposes Gaussian structures on\nthe output space is introduced to create these sub-spaces thus shaping the\ndistribution of the data. Having a mixture of Gaussians solution space is\nadvantageous given its simplified and well established structure. It allows\nfast discovering of classes within classes and the identification of mean\nrepresentatives at the centroids of individual classes. We also propose a new\nsemi-supervised method to create sub-classes. We illustrate our methods on the\nfacial expression recognition problem and validate results on the FER+,\nAffectNet, Extended Cohn-Kanade (CK+), BU-3DFE, and JAFFE datasets. We\nexperimentally demonstrate that the learned embedding can be successfully used\nfor various applications including expression retrieval and emotion\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1\">Pedro D. Marrero Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyh_T/0/1/0/all/0/1\">Tsang Ing Jyh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pena_F/0/1/0/all/0/1\">Fidel A. Guerrero Pe&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunha_A/0/1/0/all/0/1\">Alexandre Cunha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increased-confidence adversarial examples for deep learning counter-forensics. (arXiv:2005.06023v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.06023","description":"<p>Transferability of adversarial examples is a key issue to apply this kind of\nattacks against multimedia forensics (MMF) techniques based on Deep Learning\n(DL) in a real-life setting. Adversarial example transferability, in fact,\nwould open the way to the deployment of successful counter forensics attacks\nalso in cases where the attacker does not have a full knowledge of the\nto-be-attacked system. Some preliminary works have shown that adversarial\nexamples against CNN-based image forensics detectors are in general\nnon-transferrable, at least when the basic versions of the attacks implemented\nin the most popular libraries are adopted. In this paper, we introduce a\ngeneral strategy to increase the strength of the attacks and evaluate their\ntransferability when such a strength varies. We experimentally show that, in\nthis way, attack transferability can be largely increased, at the expense of a\nlarger distortion. Our research confirms the security threats posed by the\nexistence of adversarial examples even in multimedia forensics scenarios, thus\ncalling for new defense strategies to improve the security of DL-based MMF\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1\">Benedetta Tondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1\">Rongrong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1\">Mauro Barni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v6 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2008.08170","description":"<p>In the paper, we propose a class of accelerated zeroth-order and first-order\nmomentum methods for both nonconvex mini-optimization and minimax-optimization.\nSpecifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)\nmethod for black-box mini-optimization. Moreover, we prove that our Acc-ZOM\nmethod achieves a lower query complexity of $\\tilde{O}(d^{3/4}\\epsilon^{-3})$\nfor finding an $\\epsilon$-stationary point, which improves the best known\nresult by a factor of $O(d^{1/4})$ where $d$ denotes the variable dimension. In\nparticular, the Acc-ZOM does not require large batches required in the existing\nzeroth-order stochastic algorithms. Meanwhile, we propose an accelerated\n\\textbf{zeroth-order} momentum descent ascent (Acc-ZOMDA) method for\n\\textbf{black-box} minimax-optimization, which obtains a query complexity of\n$\\tilde{O}((d_1+d_2)^{3/4}\\kappa_y^{4.5}\\epsilon^{-3})$ without large batches\nfor finding an $\\epsilon$-stationary point, where $d_1$ and $d_2$ denote\nvariable dimensions and $\\kappa_y$ is condition number. Moreover, we propose an\naccelerated \\textbf{first-order} momentum descent ascent (Acc-MDA) method for\n\\textbf{white-box} minimax optimization, which has a gradient complexity of\n$\\tilde{O}(\\kappa_y^{4.5}\\epsilon^{-3})$ without large batches for finding an\n$\\epsilon$-stationary point. In particular, our Acc-MDA can obtain a lower\ngradient complexity of $\\tilde{O}(\\kappa_y^{2.5}\\epsilon^{-3})$ with a batch\nsize $O(\\kappa_y^4)$. Extensive experimental results on black-box adversarial\nattack to deep neural networks and poisoning attack to logistic regression\ndemonstrate efficiency of our algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gao_S/0/1/0/all/0/1\">Shangqian Gao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Example-based Color Transfer with Gaussian Mixture Modeling. (arXiv:2008.13626v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.13626","description":"<p>Color transfer, which plays a key role in image editing, has attracted\nnoticeable attention recently. It has remained a challenge to date due to\nvarious issues such as time-consuming manual adjustments and prior segmentation\nissues. In this paper, we propose to model color transfer under a probability\nframework and cast it as a parameter estimation problem. In particular, we\nrelate the transferred image with the example image under the Gaussian Mixture\nModel (GMM) and regard the transferred image color as the GMM centroids. We\nemploy the Expectation-Maximization (EM) algorithm (E-step and M-step) for\noptimization. To better preserve gradient information, we introduce a Laplacian\nbased regularization term to the objective function at the M-step which is\nsolved by deriving a gradient descent algorithm. Given the input of a source\nimage and an example image, our method is able to generate continuous color\ntransfer results with increasing EM iterations. Various experiments show that\nour approach generally outperforms other competitive color transfer methods,\nboth visually and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chunzhi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals. (arXiv:2009.08270v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.08270","description":"<p>Counterfactual examples for an input -- perturbations that change specific\nfeatures but not others -- have been shown to be useful for evaluating bias of\nmachine learning models, e.g., against specific demographic groups. However,\ngenerating counterfactual examples for images is non-trivial due to the\nunderlying causal structure on the various features of an image. To be\nmeaningful, generated perturbations need to satisfy constraints implied by the\ncausal model. We present a method for generating counterfactuals by\nincorporating a structural causal model (SCM) in an improved variant of\nAdversarially Learned Inference (ALI), that generates counterfactuals in\naccordance with the causal relationships between attributes of an image. Based\non the generated counterfactuals, we show how to explain a pre-trained machine\nlearning classifier, evaluate its bias, and mitigate the bias using a\ncounterfactual regularizer. On the Morpho-MNIST dataset, our method generates\ncounterfactuals comparable in quality to prior work on SCM-based\ncounterfactuals (DeepSCM), while on the more complex CelebA dataset our method\noutperforms DeepSCM in generating high-quality valid counterfactuals. Moreover,\ngenerated counterfactuals are indistinguishable from reconstructed images in a\nhuman evaluation experiment and we subsequently use them to evaluate the\nfairness of a standard classifier trained on CelebA data. We show that the\nclassifier is biased w.r.t. skin and hair color, and how counterfactual\nregularization can remove those biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Saloni Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection. (arXiv:2102.00463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.00463","description":"<p>3D object detection is receiving increasing attention from both industry and\nacademia thanks to its wide applications in various fields. In this paper, we\npropose the Point-Voxel Region-based Convolution Neural Networks (PV-RCNNs) for\n3D object detection from point clouds. First, we propose a novel 3D detector,\nPV-RCNN, which consists of two steps: the voxel-to-keypoint scene encoding and\nkeypoint-to-grid RoI feature abstraction. These two steps deeply integrate the\n3D voxel CNN with the PointNet-based set abstraction for extracting\ndiscriminative features. Second, we propose an advanced framework, PV-RCNN++,\nfor more efficient and accurate 3D object detection. It consists of two major\nimprovements: the sectorized proposal-centric strategy for efficiently\nproducing more representative keypoints, and the VectorPool aggregation for\nbetter aggregating local point features with much less resource consumption.\nWith these two strategies, our PV-RCNN++ is more than 2x faster than PV-RCNN,\nwhile also achieving better performance on the large-scale Waymo Open Dataset\nwith 150m * 150m detection range. Also, our proposed PV-RCNNs achieve\nstate-of-the-art 3D detection performance on both the Waymo Open Dataset and\nthe highly-competitive KITTI benchmark. The source code is available at\nhttps://github.com/open-mmlab/OpenPCDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaoshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chaoxu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCPM-Net: An Anchor-free 3D Lung Nodule Detection Network using Sphere Representation and Center Points Matching. (arXiv:2104.05215v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05215","description":"<p>Lung nodule detection from 3D Computed Tomography scans plays a vital role in\nefficient lung cancer screening. Despite the SOTA performance obtained by\nrecent anchor-based detectors using CNNs for this task, they require\npredetermined anchor parameters such as the size, number, and aspect ratio of\nanchors, and have limited robustness when dealing with lung nodules with a\nmassive variety of sizes. To overcome these problems, we propose a 3D sphere\nrepresentation-based center-points matching detection network that is\nanchor-free and automatically predicts the position, radius, and offset of\nnodules without the manual design of nodule/anchor parameters. The SCPM-Net\nconsists of two novel components: sphere representation and center points\nmatching. First, to match the nodule annotation in clinical practice, we\nreplace the commonly used bounding box with our proposed bounding sphere to\nrepresent nodules with the centroid, radius, and local offset in 3D space. A\ncompatible sphere-based intersection over-union loss function is introduced to\ntrain the lung nodule detection network stably and efficiently. Second, we\nempower the network anchor-free by designing a positive center-points selection\nand matching process, which naturally discards pre-determined anchor boxes. An\nonline hard example mining and re-focal loss subsequently enable the CPM\nprocess to be more robust, resulting in more accurate point assignment and\nmitigation of class imbalance. In addition, to better capture spatial\ninformation and 3D context for the detection, we propose to fuse multi-level\nspatial coordinate maps with the feature extractor and combine them with 3D\nsqueeze-and-excitation attention modules. Experimental results on the LUNA16\ndataset showed that our proposed framework achieves superior performance\ncompared with existing anchor-based and anchor-free methods for lung nodule\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Tao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jieneng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRNN: Generative Regression Neural Network -- A Data Leakage Attack for Federated Learning. (arXiv:2105.00529v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.00529","description":"<p>Data privacy has become an increasingly important issue in Machine Learning\n(ML), where many approaches have been developed to tackle this challenge, e.g.\ncryptography (Homomorphic Encryption (HE), Differential Privacy (DP), etc.) and\ncollaborative training (Secure Multi-Party Computation (MPC), Distributed\nLearning and Federated Learning (FL)). These techniques have a particular focus\non data encryption or secure local computation. They transfer the intermediate\ninformation to the third party to compute the final result. Gradient exchanging\nis commonly considered to be a secure way of training a robust model\ncollaboratively in Deep Learning (DL). However, recent researches have\ndemonstrated that sensitive information can be recovered from the shared\ngradient. Generative Adversarial Network (GAN), in particular, has shown to be\neffective in recovering such information. However, GAN based techniques require\nadditional information, such as class labels which are generally unavailable\nfor privacy-preserved learning. In this paper, we show that, in the FL system,\nimage-based privacy data can be easily recovered in full from the shared\ngradient only via our proposed Generative Regression Neural Network (GRNN). We\nformulate the attack to be a regression problem and optimize two branches of\nthe generative model by minimizing the distance between gradients. We evaluate\nour method on several image classification tasks. The results illustrate that\nour proposed GRNN outperforms state-of-the-art methods with better stability,\nstronger robustness, and higher accuracy. It also has no convergence\nrequirement to the global FL model. Moreover, we demonstrate information\nleakage using face re-identification. Some defense strategies are also\ndiscussed in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingjing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xianghua Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExSinGAN: Learning an Explainable Generative Model from a Single Image. (arXiv:2105.07350v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07350","description":"<p>Generating images from a single sample, as a newly developing branch of image\nsynthesis, has attracted extensive attention. In this paper, we formulate this\nproblem as sampling from the conditional distribution of a single image, and\npropose a hierarchical framework that simplifies the learning of the intricate\nconditional distributions through the successive learning of the distributions\nabout structure, semantics and texture, making the process of learning and\ngeneration comprehensible. On this basis, we design ExSinGAN composed of three\ncascaded GANs for learning an explainable generative model from a given image,\nwhere the cascaded GANs model the distributions about structure, semantics and\ntexture successively. ExSinGAN is learned not only from the internal patches of\nthe given image as the previous works did, but also from the external prior\nobtained by the GAN inversion technique. Benefiting from the appropriate\ncombination of internal and external information, ExSinGAN has a more powerful\ncapability of generation and competitive generalization ability for the image\nmanipulation tasks compared with prior works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">ZiCheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">CongYing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">TianDe Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUGCO: Augmentation Consistency-guided Self-training for Source-free Domain Adaptive Semantic Segmentation. (arXiv:2107.10140v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10140","description":"<p>Most modern approaches for domain adaptive semantic segmentation rely on\ncontinued access to source data during adaptation, which may be infeasible due\nto computational or privacy constraints. We focus on source-free domain\nadaptation for semantic segmentation, wherein a source model must adapt itself\nto a new target domain given only unlabeled target data. We propose\nAugmentation Consistency-guided Self-training (AUGCO), a source-free adaptation\nalgorithm that uses the model's pixel-level predictive consistency across\ndiverse, automatically generated views of each target image along with model\nconfidence to identify reliable pixel predictions, and selectively self-trains\non those. AUGCO achieves state-of-the-art results for source-free adaptation on\n3 standard benchmarks for semantic segmentation, all within a simple to\nimplement and fast to converge method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1\">Shivam Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartik_D/0/1/0/all/0/1\">Deeksha Kartik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07917","description":"<p>A formal autism diagnosis can be an inefficient and lengthy process. Families\nmay wait months or longer before receiving a diagnosis for their child despite\nevidence that earlier intervention leads to better treatment outcomes. Digital\ntechnologies which detect the presence of behaviors related to autism can scale\naccess to pediatric diagnoses. This work aims to demonstrate the feasibility of\ndeep learning technologies for detecting hand flapping from unstructured home\nvideos as a first step towards validating whether models and digital\ntechnologies can be leveraged to aid with autism diagnoses. We used the\nSelf-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand\nflapping, head banging, and spinning exhibited by children. From all the hand\nflapping videos, we extracted 100 positive and control videos of hand flapping,\neach between 2 to 5 seconds in duration. Utilizing both\nlandmark-driven-approaches and MobileNet V2's pretrained convolutional layers,\nour highest performing model achieved a testing F1 score of 84% (90% precision\nand 80% recall) when evaluating with 5-fold cross validation 100 times. This\nwork provides the first step towards developing precise deep learning methods\nfor activity detection of autism-related behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1\">Anish Lakkapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Onur Cezmi Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1\">Kelley Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1\">Brianna Chrisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1\">Nate Stockham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB-D Saliency Detection via Cascaded Mutual Information Minimization. (arXiv:2109.07246v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07246","description":"<p>Existing RGB-D saliency detection models do not explicitly encourage RGB and\ndepth to achieve effective multi-modal learning. In this paper, we introduce a\nnovel multi-stage cascaded learning framework via mutual information\nminimization to \"explicitly\" model the multi-modal information between RGB\nimage and depth data. Specifically, we first map the feature of each mode to a\nlower dimensional feature vector, and adopt mutual information minimization as\na regularizer to reduce the redundancy between appearance features from RGB and\ngeometric features from depth. We then perform multi-stage cascaded learning to\nimpose the mutual information minimization constraint at every stage of the\nnetwork. Extensive experiments on benchmark RGB-D saliency datasets illustrate\nthe effectiveness of our framework. Further, to prosper the development of this\nfield, we contribute the largest (7x larger than NJU2K) dataset, which contains\n15,625 image pairs with high quality\npolygon-/scribble-/object-/instance-/rank-level annotations. Based on these\nrich labels, we additionally construct four new benchmarks with strong\nbaselines and observe some interesting phenomena, which can motivate future\nmodel design. Source code and dataset are available at\n\"https://github.com/JingZhang617/cascaded_rgbd_sod\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Layout Generation Algorithm of Graphic Design Based on Transformer-CVAE. (arXiv:2110.06794v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2110.06794","description":"<p>Graphic design is ubiquitous in people's daily lives. For graphic design, the\nmost time-consuming task is laying out various components in the interface.\nRepetitive manual layout design will waste a lot of time for professional\ngraphic designers. Existing templates are usually rudimentary and not suitable\nfor most designs, reducing efficiency and limiting creativity. This paper\nimplemented the Transformer model and conditional variational autoencoder\n(CVAE) to the graphic design layout generation task. It proposed an end-to-end\ngraphic design layout generation model named LayoutT-CVAE. We also proposed\nelement disentanglement and feature-based disentanglement strategies and\nintroduce new graphic design principles and similarity metrics into the model,\nwhich significantly increased the controllability and interpretability of the\ndeep model. Compared with the existing state-of-art models, the layout\ngenerated by ours performs better on many metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mengxi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dangqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaodong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-Conditional Knowledge Distillation for Object Detection. (arXiv:2110.12724v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12724","description":"<p>Knowledge distillation has shown great success in classification, however, it\nis still challenging for detection. In a typical image for detection,\nrepresentations from different locations may have different contributions to\ndetection targets, making the distillation hard to balance. In this paper, we\npropose a conditional distillation framework to distill the desired knowledge,\nnamely knowledge that is beneficial in terms of both classification and\nlocalization for every instance. The framework introduces a learnable\nconditional decoding module, which retrieves information given each target\ninstance as query. Specifically, we encode the condition information as query\nand use the teacher's representations as key. The attention between query and\nkey is used to measure the contribution of different features, guided by a\nlocalization-recognition-sensitive auxiliary task. Extensive experiments\ndemonstrate the efficacy of our method: we observe impressive improvements\nunder various settings. Notably, we boost RetinaNet with ResNet-50 backbone\nfrom 37.4 to 40.7 mAP (+3.3) under 1x schedule, that even surpasses the teacher\n(40.4 mAP) with ResNet-101 backbone under 3x schedule. Code has been released\non https://github.com/megvii-research/ICD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zijian Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peizhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers. (arXiv:2111.12710v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12710","description":"<p>This paper explores a better codebook for BERT pre-training of vision\ntransformers. The recent work BEiT successfully transfers BERT pre-training\nfrom NLP to the vision field. It directly adopts one simple discrete VAE as the\nvisual tokenizer, but has not considered the semantic level of the resulting\nvisual tokens. By contrast, the discrete tokens in NLP field are naturally\nhighly semantic. This difference motivates us to learn a perceptual codebook.\nAnd we surprisingly find one simple yet effective idea: enforcing perceptual\nsimilarity during the dVAE training. We demonstrate that the visual tokens\ngenerated by the proposed perceptual codebook do exhibit better semantic\nmeanings, and subsequently help pre-training achieve superior transfer\nperformance in various downstream tasks. For example, we achieve 84.5% Top-1\naccuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive\nmethod BEiT by +1.3 with the same pre-training epochs. It can also improve the\nperformance of object detection and segmentation tasks on COCO val by +1.3 box\nAP and +1.0 mask AP, semantic segmentation on ADE20k by +1.0 mIoU. Equipped\nwith a larger backbone ViT-H, we achieve the state-of-the-art performance\n(88.3% Top-1 accuracy) among the methods using only ImageNet-1K data. The code\nand models will be available at https://github.com/microsoft/PeCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensions of Motion: Learning to Predict a Subspace of Optical Flow from a Single Image. (arXiv:2112.01502v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01502","description":"<p>We introduce the problem of predicting, from a single video frame, a\nlow-dimensional subspace of optical flow which includes the actual\ninstantaneous optical flow. We show how several natural scene assumptions allow\nus to identify an appropriate flow subspace via a set of basis flow fields\nparameterized by disparity and a representation of object instances. The flow\nsubspace, together with a novel loss function, can be used for the tasks of\npredicting monocular depth or predicting depth plus an object instance\nembedding. This provides a new approach to learning these tasks in an\nunsupervised fashion using monocular input video without requiring camera\nintrinsics or poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bowen_R/0/1/0/all/0/1\">Richard Strong Bowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1\">Richard Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabih_R/0/1/0/all/0/1\">Ramin Zabih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSPACE: Synthetic Parametric Humans Animated in Complex Environments. (arXiv:2112.12867v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12867","description":"<p>Advances in the state of the art for 3d human sensing are currently limited\nby the lack of visual datasets with 3d ground truth, including multiple people,\nin motion, operating in real-world environments, with complex illumination or\nocclusion, and potentially observed by a moving camera. Sophisticated scene\nunderstanding would require estimating human pose and shape as well as\ngestures, towards representations that ultimately combine useful metric and\nbehavioral signals with free-viewpoint photo-realistic visualisation\ncapabilities. To sustain progress, we build a large-scale photo-realistic\ndataset, Human-SPACE (HSPACE), of animated humans placed in complex synthetic\nindoor and outdoor environments. We combine a hundred diverse individuals of\nvarying ages, gender, proportions, and ethnicity, with hundreds of motions and\nscenes, as well as parametric variations in body shape (for a total of 1,600\ndifferent humans), in order to generate an initial dataset of over 1 million\nframes. Human animations are obtained by fitting an expressive human body\nmodel, GHUM, to single scans of people, followed by novel re-targeting and\npositioning procedures that support the realistic animation of dressed humans,\nstatistical variation of body proportions, and jointly consistent scene\nplacement of multiple moving people. Assets are generated automatically, at\nscale, and are compatible with existing real time rendering and game engines.\nThe dataset with evaluation server will be made available for research. Our\nlarge-scale analysis of the impact of synthetic data, in connection with real\ndata and weak supervision, underlines the considerable potential for continuing\nquality improvements and limiting the sim-to-real gap, in this practical\nsetting, in connection with increased model capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bazavan_E/0/1/0/all/0/1\">Eduard Gabriel Bazavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_A/0/1/0/all/0/1\">Andrei Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1\">Mihai Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukthankar_R/0/1/0/all/0/1\">Rahul Sukthankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fistful of Words: Learning Transferable Visual Models from Bag-of-Words Supervision. (arXiv:2112.13884v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13884","description":"<p>Using natural language as a supervision for training visual recognition\nmodels holds great promise. Recent works have shown that if such supervision is\nused in the form of alignment between images and captions in large training\ndatasets, then the resulting aligned models perform well on zero-shot\nclassification as downstream tasks2. In this paper, we focus on teasing out\nwhat parts of the language supervision are essential for training zero-shot\nimage classification models. Through extensive and careful experiments, we show\nthat: 1) A simple Bag-of-Words (BoW) caption could be used as a replacement for\nmost of the image captions in the dataset. Surprisingly, we observe that this\napproach improves the zero-shot classification performance when combined with\nword balancing. 2) Using a BoW pretrained model, we can obtain more training\ndata by generating pseudo-BoW captions on images that do not have a caption.\nModels trained on images with real and pseudo-BoW captions achieve stronger\nzero-shot performance. On ImageNet-1k zero-shot evaluation, our best model,\nthat uses only 3M image-caption pairs, performs on-par with a CLIP model\ntrained on 15M image-caption pairs (31.5% vs 31.3%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bichen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.15402","description":"<p>Continual learning requires models to learn new tasks while maintaining\npreviously learned knowledge. Various algorithms have been proposed to address\nthis real challenge. Till now, rehearsal-based methods, such as experience\nreplay, have achieved state-of-the-art performance. These approaches save a\nsmall part of the data of the past tasks as a memory buffer to prevent models\nfrom forgetting previously learned knowledge. However, most of them treat every\nnew task equally, i.e., fixed the hyperparameters of the framework while\nlearning different new tasks. Such a setting lacks the consideration of the\nrelationship/similarity between past and new tasks. For example, the previous\nknowledge/features learned from dogs are more beneficial for the identification\nof cats (new task), compared to those learned from buses. In this regard, we\npropose a meta learning algorithm based on bi-level optimization to adaptively\ntune the relationship between the knowledge extracted from the past and new\ntasks. Therefore, the model can find an appropriate direction of gradient\nduring continual learning and avoid the serious overfitting problem on memory\nbuffer. Extensive experiments are conducted on three publicly available\ndatasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet). The experimental\nresults demonstrate that the proposed method can consistently improve the\nperformance of all baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution. (arXiv:2201.01014v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.01014","description":"<p>Infrared small target super-resolution (SR) aims to recover reliable and\ndetailed high-resolution image with highcontrast targets from its\nlow-resolution counterparts. Since the infrared small target lacks color and\nfine structure information, it is significant to exploit the supplementary\ninformation among sequence images to enhance the target. In this paper, we\npropose the first infrared small target SR method named local motion and\ncontrast prior driven deep network (MoCoPnet) to integrate the domain knowledge\nof infrared small target into deep network, which can mitigate the intrinsic\nfeature scarcity of infrared small targets. Specifically, motivated by the\nlocal motion prior in the spatio-temporal dimension, we propose a local\nspatiotemporal attention module to perform implicit frame alignment and\nincorporate the local spatio-temporal information to enhance the local features\n(especially for small targets). Motivated by the local contrast prior in the\nspatial dimension, we propose a central difference residual group to\nincorporate the central difference convolution into the feature extraction\nbackbone, which can achieve center-oriented gradient-aware feature extraction\nto further improve the target contrast. Extensive experiments have demonstrated\nthat our method can recover accurate spatial dependency and improve the target\ncontrast. Comparative results show that MoCoPnet can outperform the\nstate-of-the-art video SR and single image SR methods in terms of both SR\nperformance and target enhancement. Based on the SR results, we further\ninvestigate the influence of SR on infrared small target detection and the\nexperimental results demonstrate that MoCoPnet promotes the detection\nperformance. The code is available at https://github.com/XinyiYing/MoCoPnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ying_X/0/1/0/all/0/1\">Xinyi Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_W/0/1/0/all/0/1\">Weidong Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1\">Zaiping Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need In Sign Language Production. (arXiv:2201.01609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01609","description":"<p>Sign Language is the dominant form of communication language used in the deaf\nand hearing-impaired community. To make an easy and mutual communication\nbetween the hearing-impaired and the hearing communities, building a robust\nsystem capable of translating the spoken language into sign language and vice\nversa is fundamental. To this end, sign language recognition and production are\ntwo necessary parts for making such a two-way system. Sign language recognition\nand production need to cope with some critical challenges. In this survey, we\nreview recent advances in Sign Language Production (SLP) and related areas\nusing deep learning. To have more realistic perspectives to sign language, we\npresent an introduction to the Deaf culture, Deaf centers, psychological\nperspective of sign language, the main differences between spoken language and\nsign language. Furthermore, we present the fundamental components of a\nbi-directional sign language translation system, discussing the main challenges\nin this area. Also, the backbone architectures and methods in SLP are briefly\nintroduced and the proposed taxonomy on SLP is presented. Finally, a general\nframework for SLP and performance evaluation, and also a discussion on the\nrecent developments, advantages, and limitations in SLP, commenting on possible\nlines for future research are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}