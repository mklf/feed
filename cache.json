{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Searching for fingerspelled content in American Sign Language. (arXiv:2203.13291v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13291","description":"<p>Natural language processing for sign language video - including tasks like\nrecognition, translation, and search - is crucial for making artificial\nintelligence technologies accessible to deaf individuals, and is gaining\nresearch interest in recent years. In this paper, we address the problem of\nsearching for fingerspelled key-words or key phrases in raw sign language\nvideos. This is an important task since significant content in sign language is\noften conveyed via fingerspelling, and to our knowledge the task has not been\nstudied before. We propose an end-to-end model for this task, FSS-Net, that\njointly detects fingerspelling and matches it to a text sequence. Our\nexperiments, done on a large public dataset of ASL fingerspelling in the wild,\nshow the importance of fingerspelling detection as a component of a search and\nretrieval model. Our model significantly outperforms baseline methods adapted\nfrom prior work on related tasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brentari_D/0/1/0/all/0/1\">Diane Brentari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mix and Match: Learning-free Controllable Text Generation using Energy Language Models. (arXiv:2203.13299v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13299","description":"<p>Recent work on controlled text generation has either required attribute-based\nfine-tuning of the base language model (LM), or has restricted the\nparameterization of the attribute discriminator to be compatible with the base\nautoregressive LM. In this work, we propose Mix and Match LM, a global\nscore-based alternative for controllable text generation that combines\narbitrary pre-trained black-box models for achieving the desired attributes in\nthe generated text without involving any fine-tuning or structural assumptions\nabout the black-box models. We interpret the task of controllable generation as\ndrawing samples from an energy-based model whose energy values are a linear\ncombination of scores from black-box models that are separately responsible for\nfluency, the control attribute, and faithfulness to any conditioning context.\nWe use a Metropolis-Hastings sampling scheme to sample from this energy-based\nmodel using bidirectional context and global attribute features. We validate\nthe effectiveness of our approach on various controlled generation and\nstyle-based text revision tasks by outperforming recently proposed methods that\ninvolve extra training, fine-tuning, or restrictive assumptions over the form\nof models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1\">Kartik Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation. (arXiv:2203.13339v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13339","description":"<p>End-to-end speech-to-speech translation (S2ST) without relying on\nintermediate text representations is a rapidly emerging frontier of research.\nRecent works have demonstrated that the performance of such direct S2ST systems\nis approaching that of conventional cascade S2ST when trained on comparable\ndatasets. However, in practice, the performance of direct S2ST is bounded by\nthe availability of paired S2ST training data. In this work, we explore\nmultiple approaches for leveraging much more widely available unsupervised and\nweakly-supervised speech and text data to improve the performance of direct\nS2ST based on Translatotron 2. With our most effective approaches, the average\ntranslation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is\nimproved by +13.6 BLEU (or +113% relatively), as compared to the previous\nstate-of-the-art trained without additional data. The improvements on\nlow-resource language are even more significant (+398% relatively on average).\nOur comparative studies suggest future research directions for S2ST and speech\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morioka_N/0/1/0/all/0/1\">Nobuyuki Morioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linking Emergent and Natural Languages via Corpus Transfer. (arXiv:2203.13344v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13344","description":"<p>The study of language emergence aims to understand how human languages are\nshaped by perceptual grounding and communicative intent. Computational\napproaches to emergent communication (EC) predominantly consider referential\ngames in limited domains and analyze the learned protocol within the game\nframework. As a result, it remains unclear how the emergent languages from\nthese settings connect to natural languages or provide benefits in real-world\nlanguage processing tasks, where statistical models trained on large text\ncorpora dominate. In this work, we propose a novel way to establish such a link\nby corpus transfer, i.e. pretraining on a corpus of emergent language for\ndownstream natural language tasks, which is in contrast to prior work that\ndirectly transfers speaker and listener parameters. Our approach showcases\nnon-trivial transfer benefits for two different tasks -- language modeling and\nimage captioning. For example, in a low-resource setup (modeling 2 million\nnatural language tokens), pre-training on an emergent language corpus with just\n2 million tokens reduces model perplexity by $24.6\\%$ on average across ten\nnatural languages. We also introduce a novel metric to predict the\ntransferability of an emergent language by translating emergent messages to\nnatural language captions grounded on the same images. We find that our\ntranslation-based metric highly correlates with the downstream performance on\nmodeling natural languages (for instance $\\rho=0.83$ on Hebrew), while\ntopographic similarity, a popular metric in previous work, shows surprisingly\nlow correlation ($\\rho=0.003$), hinting that simple properties like attribute\ndisentanglement from synthetic domains might not capture the full complexities\nof natural language. Our findings also indicate potential benefits of moving\nlanguage emergence forward with natural language resources and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik R Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does human speech follow Benford's Law?. (arXiv:2203.13352v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13352","description":"<p>Researchers have observed that the frequencies of leading digits in many\nman-made and naturally occurring datasets follow a logarithmic curve, with\ndigits that start with the number 1 accounting for $\\sim 30\\%$ of all numbers\nin the dataset and digits that start with the number 9 accounting for $\\sim\n5\\%$ of all numbers in the dataset. This phenomenon, known as Benford's Law, is\nhighly repeatable and appears in lists of numbers from electricity bills, stock\nprices, tax returns, house prices, death rates, lengths of rivers, and\nnaturally occurring images. In this paper we demonstrate that human speech\nspectra also follow Benford's Law. We use this observation to motivate a new\nset of features that can be efficiently extracted from speech and demonstrate\nthat these features can be used to classify between human speech and synthetic\nspeech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_L/0/1/0/all/0/1\">Leo Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berisha_V/0/1/0/all/0/1\">Visar Berisha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia. (arXiv:2203.13357v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13357","description":"<p>NLP research is impeded by a lack of resources and awareness of the\nchallenges presented by underrepresented languages and dialects. Focusing on\nthe languages spoken in Indonesia, the second most linguistically diverse and\nthe fourth most populous nation of the world, we provide an overview of the\ncurrent state of NLP research for Indonesia's 700+ languages. We highlight\nchallenges in Indonesian NLP and how these affect the performance of current\nNLP systems. Finally, we provide general recommendations to help develop NLP\ntechnology not only for languages of Indonesia but also other underrepresented\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romadhony_A/0/1/0/all/0/1\">Ade Romadhony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1\">Rahmad Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurniawan_K/0/1/0/all/0/1\">Kemal Kurniawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeljadi_D/0/1/0/all/0/1\">David Moeljadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1\">Radityo Eko Prasojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v1 [cs.IR])","link":"http://arxiv.org/abs/2203.13366","description":"<p>For a long period, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language grounding is a powerful medium to describe and\nrepresent various problems or tasks, we present a flexible and unified\ntext-to-text paradigm called \"Pretrain, Personalized Prompt, and Predict\nParadigm\" (P5) for recommendation, which unifies various recommendation tasks\nin a shared framework. In P5, all data such as user-item interactions, item\nmetadata, and user reviews are converted to a common format -- natural language\nsequences. The rich information from natural language assist P5 to capture\ndeeper semantics for recommendation. P5 learns different tasks with the same\nlanguage modeling objective during pretraining. Thus, it possesses the\npotential to serve as the foundation model for downstream recommendation tasks,\nallows easy integration with other modalities, and enables instruction-based\nrecommendation, which will revolutionize the technical form of recommender\nsystem towards unified recommendation engine. With adaptive personalized prompt\nfor different users, P5 is able to make predictions in a zero-shot or few-shot\nmanner and largely reduces the necessity for extensive fine-tuning. On several\nrecommendation benchmarks, we conduct experiments to show the effectiveness of\nour generative approach. We will release our prompts and pretrained P5 language\nmodel to help advance future research on Recommendation as Language Processing\n(RLP) and Personalized Foundation Models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender and Racial Stereotype Detection in Legal Opinion Word Embeddings. (arXiv:2203.13369v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13369","description":"<p>Studies have shown that some Natural Language Processing (NLP) systems encode\nand replicate harmful biases with potential adverse ethical effects in our\nsociety. In this article, we propose an approach for identifying gender and\nracial stereotypes in word embeddings trained on judicial opinions from U.S.\ncase law. Embeddings containing stereotype information may cause harm when used\nby downstream systems for classification, information extraction, question\nanswering, or other machine learning systems used to build legal research\ntools. We first explain how previously proposed methods for identifying these\nbiases are not well suited for use with word embeddings trained on legal\nopinion text. We then propose a domain adapted method for identifying gender\nand racial biases in the legal domain. Our analyses using these methods suggest\nthat racial and gender biases are encoded into word embeddings trained on legal\nopinions. These biases are not mitigated by exclusion of historical data, and\nappear across multiple large topical areas of the law. Implications for\ndownstream systems that use legal opinion word embeddings and suggestions for\npotential mitigation strategies based on our observations are also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matthews_S/0/1/0/all/0/1\">Sean Matthews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudzina_J/0/1/0/all/0/1\">John Hudzina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sepehr_D/0/1/0/all/0/1\">Dawn Sepehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models. (arXiv:2203.13397v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13397","description":"<p>Deep learning (DL) techniques involving fine-tuning large numbers of model\nparameters have delivered impressive performance on the task of discriminating\nbetween language produced by cognitively healthy individuals, and those with\nAlzheimer's disease (AD). However, questions remain about their ability to\ngeneralize beyond the small reference sets that are publicly available for\nresearch. As an alternative to fitting model parameters directly, we propose a\nnovel method by which a Transformer DL model (GPT-2) pre-trained on general\nEnglish text is paired with an artificially degraded version of itself (GPT-D),\nto compute the ratio between these two models' \\textit{perplexities} on\nlanguage from cognitively healthy and impaired individuals. This technique\napproaches state-of-the-art performance on text data from a widely used \"Cookie\nTheft\" picture description task, and unlike established alternatives also\ngeneralizes well to spontaneous conversations. Furthermore, GPT-D generates\ntext with characteristics known to be associated with AD, demonstrating the\ninduction of dementia-related linguistic anomalies. Our study is a step toward\nbetter understanding of the relationships between the inner workings of\ngenerative neural language models, the language that they produce, and the\ndeleterious effects of dementia on human speech and language characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knopman_D/0/1/0/all/0/1\">David Knopman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weizhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1\">Serguei Pakhomov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Song Translation for Tonal Languages. (arXiv:2203.13420v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13420","description":"<p>This paper develops automatic song translation (AST) for tonal languages and\naddresses the unique challenge of aligning words' tones with melody of a song\nin addition to conveying the original meaning. We propose three criteria for\neffective AST -- preserving meaning, singability and intelligibility -- and\ndesign metrics for these criteria. We develop a new benchmark for\nEnglish--Mandarin song translation and develop an unsupervised AST system,\nGuided AliGnment for Automatic Song Translation (GagaST), which combines\npre-training with three decoding constraints. Both automatic and human\nevaluations show GagaST successfully balances semantics and singability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fenfei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qixin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kejun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plagiarism Detection in the Bengali Language: A Text Similarity-Based Approach. (arXiv:2203.13430v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13430","description":"<p>Plagiarism means taking another person's work and not giving any credit to\nthem for it. Plagiarism is one of the most serious problems in academia and\namong researchers. Even though there are multiple tools available to detect\nplagiarism in a document but most of them are domain-specific and designed to\nwork in English texts, but plagiarism is not limited to a single language only.\nBengali is the most widely spoken language of Bangladesh and the second most\nspoken language in India with 300 million native speakers and 37 million\nsecond-language speakers. Plagiarism detection requires a large corpus for\ncomparison. Bengali Literature has a history of 1300 years. Hence most Bengali\nLiterature books are not yet digitalized properly. As there was no such corpus\npresent for our purpose so we have collected Bengali Literature books from the\nNational Digital Library of India and with a comprehensive methodology\nextracted texts from it and constructed our corpus. Our experimental results\nfind out average accuracy between 72.10 % - 79.89 % in text extraction using\nOCR. Levenshtein Distance algorithm is used for determining Plagiarism. We have\nbuilt a web application for end-user and successfully tested it for Plagiarism\ndetection in Bengali texts. In future, we aim to construct a corpus with more\nbooks for more accurate detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Satyajit Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Aniruddha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_B/0/1/0/all/0/1\">Bittaswer Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhishek Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13474","description":"<p>Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification. We propose a conversational program synthesis\napproach via large language models, which addresses the challenges of searching\nover a vast program space and user intent specification faced in prior\napproaches. Our new approach casts the process of writing a specification and\nprogram as a multi-turn conversation between a user and a system. It treats\nprogram synthesis as a sequence prediction problem, in which the specification\nis expressed in natural language and the desired program is conditionally\nsampled. We train a family of large language models, called CodeGen, on natural\nlanguage and programming language data. With weak supervision in the data and\nthe scaling up of data size and model size, conversational capacities emerge\nfrom the simple autoregressive language modeling. To study the model behavior\non conversational program synthesis, we develop a multi-turn programming\nbenchmark (MTPB), where solving each problem requires multi-step synthesis via\nmulti-turn conversation between the user and the model. Our findings show the\nemergence of conversational capabilities and the effectiveness of the proposed\nconversational program synthesis paradigm. In addition, our model CodeGen (with\nup to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the\nHumanEval benchmark. We plan to make the training library JaxFormer including\ncheckpoints available as open source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1\">Erik Nijkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Striking a Balance: Alleviating Inconsistency in Pre-trained Models for Symmetric Classification Tasks. (arXiv:2203.13491v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13491","description":"<p>While fine-tuning pre-trained models for downstream classification is the\nconventional paradigm in NLP, often task-specific nuances may not get captured\nin the resultant models. Specifically, for tasks that take two inputs and\nrequire the output to be invariant of the order of the inputs, inconsistency is\noften observed in the predicted labels or confidence scores. We highlight this\nmodel shortcoming and apply a consistency loss function to alleviate\ninconsistency in symmetric classification. Our results show an improved\nconsistency in predictions for three paraphrase detection datasets without a\nsignificant drop in the accuracy scores. We examine the classification\nperformance of six datasets (both symmetric and non-symmetric) to showcase the\nstrengths and limitations of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashutosh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Aditya Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition. (arXiv:2203.13504v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13504","description":"<p>Emotion recognition in conversation (ERC) aims to analyze the speaker's state\nand identify their emotion in the conversation. Recent works in ERC focus on\ncontext modeling but ignore the representation of contextual emotional\ntendency. In order to extract multi-modal information and the emotional\ntendency of the utterance effectively, we propose a new structure named\nEmoformer to extract multi-modal emotion vectors from different modalities and\nfuse them with sentence vector to be an emotion capsule. Furthermore, we design\nan end-to-end ERC model called EmoCaps, which extracts emotion vectors through\nthe Emoformer structure and obtain the emotion classification results from a\ncontext analysis model. Through the experiments with two benchmark datasets,\nour model shows better performance than the existing state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaijing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fengxiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yusen Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation. (arXiv:2203.13528v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13528","description":"<p>Subword regularizations use multiple subword segmentations during training to\nimprove the robustness of neural machine translation models. In previous\nsubword regularizations, we use multiple segmentations in the training process\nbut use only one segmentation in the inference. In this study, we propose an\ninference strategy to address this discrepancy. The proposed strategy\napproximates the marginalized likelihood by using multiple segmentations\nincluding the most plausible segmentation and several sampled segmentations.\nBecause the proposed strategy aggregates predictions from several\nsegmentations, we can regard it as a single model ensemble that does not\nrequire any additional cost for training. Experimental results show that the\nproposed strategy improves the performance of models trained with subword\nregularization in low-resource machine translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1\">Tatsuya Hiraoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Target-Side Morphology in Neural Machine Translation: A Comparison of Strategies. (arXiv:2203.13550v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13550","description":"<p>Morphologically rich languages pose difficulties to machine translation.\nMachine translation engines that rely on statistical learning from parallel\ntraining data, such as state-of-the-art neural systems, face challenges\nespecially with rich morphology on the output language side. Key challenges of\nrich target-side morphology in data-driven machine translation include: (1) A\nlarge amount of differently inflected word surface forms entails a larger\nvocabulary and thus data sparsity. (2) Some inflected forms of infrequent terms\ntypically do not appear in the training corpus, which makes closed-vocabulary\nsystems unable to generate these unobserved variants. (3) Linguistic agreement\nrequires the system to correctly match the grammatical categories between\ninflected word forms in the output sentence, both in terms of target-side\nmorpho-syntactic wellformedness and semantic adequacy with respect to the\ninput.\n</p>\n<p>In this paper, we re-investigate two target-side linguistic processing\ntechniques: a lemma-tag strategy and a linguistically informed word\nsegmentation strategy. Our experiments are conducted on a English-German\ntranslation task under three training corpus conditions of different\nmagnitudes. We find that a stronger Transformer baseline leaves less room for\nimprovement than a shallow-RNN encoder-decoder model when translating\nin-domain. However, we find that linguistic modeling of target-side morphology\ndoes benefit the Transformer model when the same system is applied to\nout-of-domain input text. We also successfully apply our approach to English to\nCzech translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marco_M/0/1/0/all/0/1\">Marion Weller-Di Marco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huck_M/0/1/0/all/0/1\">Matthias Huck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation. (arXiv:2203.13560v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13560","description":"<p>Applying existing methods to emotional support conversation -- which provides\nvaluable assistance to people who are in need -- has two major limitations: (a)\nthey generally employ a conversation-level emotion label, which is too\ncoarse-grained to capture user's instant mental state; (b) most of them focus\non expressing empathy in the response(s) rather than gradually reducing user's\ndistress. To address the problems, we propose a novel model \\textbf{MISC},\nwhich firstly infers the user's fine-grained emotional status, and then\nresponds skillfully using a mixture of strategy. Experimental results on the\nbenchmark dataset demonstrate the effectiveness of our method and reveal the\nbenefits of fine-grained emotion understanding as well as mixed-up strategy\nmodeling. Our code and data could be found in\n\\url{https://github.com/morecry/MISC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Q/0/1/0/all/0/1\">Quan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Dataset on Acoustic Models for Automatic Speech Recognition. (arXiv:2203.13590v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13590","description":"<p>In Automatic Speech Recognition, GMM-HMM had been widely used for acoustic\nmodelling. With the current advancement of deep learning, the Gaussian Mixture\nModel (GMM) from acoustic models has been replaced with Deep Neural Network,\nnamely DNN-HMM Acoustic Models. The GMM models are widely used to create the\nalignments of the training data for the hybrid deep neural network model, thus\nmaking it an important task to create accurate alignments. Many factors such as\ntraining dataset size, training data augmentation, model hyperparameters, etc.,\naffect the model learning. Traditionally in machine learning, larger datasets\ntend to have better performance, while smaller datasets tend to trigger\nover-fitting. The collection of speech data and their accurate transcriptions\nis a significant challenge that varies over different languages, and in most\ncases, it might be limited to big organizations. Moreover, in the case of\navailable large datasets, training a model using such data requires additional\ntime and computing resources, which may not be available. While the data about\nthe accuracy of state-of-the-art ASR models on open-source datasets are\npublished, the study about the impact of the size of a dataset on acoustic\nmodels is not readily available. This work aims to investigate the impact of\ndataset size variations on the performance of various GMM-HMM Acoustic Models\nand their respective computational costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddhesh Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations. (arXiv:2203.13602v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13602","description":"<p>The current workflow for Information Extraction (IE) analysts involves the\ndefinition of the entities/relations of interest and a training corpus with\nannotated examples. In this demonstration we introduce a new workflow where the\nanalyst directly verbalizes the entities/relations, which are then used by a\nTextual Entailment model to perform zero-shot IE. We present the design and\nimplementation of a toolkit with a user interface, as well as experiments on\nfour IE tasks that show that the system achieves very good performance at\nzero-shot learning using only 5--15 minutes per type of a user's effort. Our\ndemonstration system is open-sourced at https://github.com/BBN-E/ZS4IE . A\ndemonstration video is available at https://vimeo.<a href=\"/abs/com/6761383\">com/6761383</a>40 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haoling Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1\">Bonan Min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Formality Style Transfer with Consistency Training. (arXiv:2203.13620v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13620","description":"<p>Formality style transfer (FST) is a task that involves paraphrasing an\ninformal sentence into a formal one without altering its meaning. To address\nthe data-scarcity problem of existing parallel datasets, previous studies tend\nto adopt a cycle-reconstruction scheme to utilize additional unlabeled data,\nwhere the FST model mainly benefits from target-side unlabeled sentences. In\nthis work, we propose a simple yet effective semi-supervised framework to\nbetter utilize source-side unlabeled sentences based on consistency training.\nSpecifically, our approach augments pseudo-parallel data obtained from a\nsource-side informal sentence by enforcing the model to generate similar\noutputs for its perturbed version. Moreover, we empirically examined the\neffects of various data perturbation methods and propose effective data\nfiltering strategies to improve our framework. Experimental results on the\nGYAFC benchmark demonstrate that our approach can achieve state-of-the-art\nresults, even with less than 40% of the parallel data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">An Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning. (arXiv:2203.13628v1 [cs.SD])","link":"http://arxiv.org/abs/2203.13628","description":"<p>Inspired by the recent progress in self-supervised learning for computer\nvision, in this paper, through the DeLoRes learning framework, we introduce two\nnew general-purpose audio representation learning approaches, the DeLoRes-S and\nDeLoRes-M. Our main objective is to make our network learn representations in a\nresource-constrained setting (both data and compute), that can generalize well\nacross a diverse set of downstream tasks. Inspired from the Barlow Twins\nobjective function, we propose to learn embeddings that are invariant to\ndistortions of an input audio sample, while making sure that they contain\nnon-redundant information about the sample. To achieve this, we measure the\ncross-correlation matrix between the outputs of two identical networks fed with\ndistorted versions of an audio segment sampled from an audio file and make it\nas close to the identity matrix as possible. We call this the DeLoRes learning\nframework, which we employ in different fashions with the DeLoRes-S and\nDeLoRes-M. We use a combination of a small subset of the large-scale AudioSet\ndataset and FSD50K for self-supervised learning and are able to learn with less\nthan half the parameters compared to state-of-the-art algorithms. For\nevaluation, we transfer these learned representations to 11 downstream\nclassification tasks, including speech, music, and animal sounds, and achieve\nstate-of-the-art results on 7 out of 11 tasks on linear evaluation with\nDeLoRes-M and show competitive results with DeLoRes-S, even when pre-trained\nusing only a fraction of the total data when compared to prior art. Our\ntransfer learning evaluation setup also shows extremely competitive results for\nboth DeLoRes-S and DeLoRes-M, with DeLoRes-M achieving state-of-the-art in 4\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-text Retrieval in Context. (arXiv:2203.13645v1 [cs.SD])","link":"http://arxiv.org/abs/2203.13645","description":"<p>Audio-text retrieval based on natural language descriptions is a challenging\ntask. It involves learning cross-modality alignments between long sequences\nunder inadequate data conditions. In this work, we investigate several audio\nfeatures as well as sequence aggregation methods for better audio-text\nalignment. Moreover, through a qualitative analysis we observe that semantic\nmapping is more important than temporal relations in contextual retrieval.\nUsing pre-trained audio features and a descriptor-based aggregation method, we\nbuild our contextual audio-text retrieval system. Specifically, we utilize\nPANNs features pre-trained on a large sound event dataset and NetRVLAD pooling,\nwhich directly works with averaged descriptors. Experiments are conducted on\nthe AudioCaps and CLOTHO datasets, and results are compared with the previous\nstate-of-the-art system. With our proposed system, a significant improvement\nhas been achieved on bidirectional audio-text retrieval, on all metrics\nincluding recall, median and mean rank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1\">Siyu Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Mediate Disparities Towards Pragmatic Communication. (arXiv:2203.13685v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13685","description":"<p>Human communication is a collaborative process. Speakers, on top of conveying\ntheir own intent, adjust the content and language expressions by taking the\nlisteners into account, including their knowledge background, personalities,\nand physical capabilities. Towards building AI agents with similar abilities in\nlanguage communication, we propose Pragmatic Rational Speaker (PRS), a\nframework extending Rational Speech Act (RSA). The PRS attempts to learn the\nspeaker-listener disparity and adjust the speech accordingly, by adding a\nlight-weighted disparity adjustment layer into working memory on top of\nspeaker's long-term memory system. By fixing the long-term memory, the PRS only\nneeds to update its working memory to learn and adapt to different types of\nlisteners. To validate our framework, we create a dataset that simulates\ndifferent types of speaker-listener disparities in the context of referential\ngames. Our empirical results demonstrate that the PRS is able to shift its\noutput towards the language that listener are able to understand, significantly\nimprove the collaborative task outcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yuwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v1 [cs.SD])","link":"http://arxiv.org/abs/2203.13687","description":"<p>In our previous work, we proposed a discriminative autoencoder (DcAE) for\nspeech recognition. DcAE combines two training schemes into one. First, since\nDcAE aims to learn encoder-decoder mappings, the squared error between the\nreconstructed speech and the input speech is minimized. Second, in the code\nlayer, frame-based phonetic embeddings are obtained by minimizing the\ncategorical cross-entropy between ground truth labels and predicted\ntriphone-state scores. DcAE is developed based on the Kaldi toolkit by treating\nvarious TDNN models as encoders. In this paper, we further propose three new\nversions of DcAE. First, a new objective function that considers both\ncategorical cross-entropy and mutual information between ground truth and\npredicted triphone-state sequences is used. The resulting DcAE is called a\nchain-based DcAE (c-DcAE). For application to robust speech recognition, we\nfurther extend c-DcAE to hierarchical and parallel structures, resulting in\nhc-DcAE and pc-DcAE. In these two models, both the error between the\nreconstructed noisy speech and the input noisy speech and the error between the\nenhanced speech and the reference clean speech are taken into the objective\nfunction. Experimental results on the WSJ and Aurora-4 corpora show that our\nDcAE models outperform baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pin-Tuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UKP-SQUARE: An Online Platform for Question Answering Research. (arXiv:2203.13693v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13693","description":"<p>Recent advances in NLP and information retrieval have given rise to a diverse\nset of question answering tasks that are of different formats (e.g.,\nextractive, abstractive), require different model architectures (e.g.,\ngenerative, discriminative), and setups (e.g., with or without retrieval).\nDespite having a large number of powerful, specialized QA pipelines (which we\nrefer to as Skills) that consider a single domain, model or setup, there exists\nno framework where users can easily explore and compare such pipelines and can\nextend them according to their needs. To address this issue, we present\nUKP-SQUARE, an extensible online QA platform for researchers which allows users\nto query and analyze a large collection of modern Skills via a user-friendly\nweb interface and integrated behavioural tests. In addition, QA researchers can\ndevelop, manage, and share their custom Skills using our microservices that\nsupport a wide range of models (Transformers, Adapters, ONNX), datastores and\nretrieval techniques (e.g., sparse and dense). UKP-SQUARE is available on\nhttps://square.ukp-lab.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_T/0/1/0/all/0/1\">Tim Baumg&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Rachneet Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichler_M/0/1/0/all/0/1\">Max Eichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poth_C/0/1/0/all/0/1\">Clifton Poth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterz_H/0/1/0/all/0/1\">Hannah Sterz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v1 [cs.SD])","link":"http://arxiv.org/abs/2203.13696","description":"<p>Compensation for channel mismatch and noise interference is essential for\nrobust automatic speech recognition. Enhanced speech has been introduced into\nthe multi-condition training of acoustic models to improve their generalization\nability. In this paper, a noise-aware training framework based on two cascaded\nneural structures is proposed to jointly optimize speech enhancement and speech\nrecognition. The feature enhancement module is composed of a multi-task\nautoencoder, where noisy speech is decomposed into clean speech and noise. By\nconcatenating its enhanced, noise-aware, and noisy features for each frame, the\nacoustic-modeling module maps each feature-augmented frame into a triphone\nstate by optimizing the lattice-free maximum mutual information and cross\nentropy between the predicted and actual state sequences. On top of the\nfactorized time delay neural network (TDNN-F) and its convolutional variant\n(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error\nrate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared\nwith the best existing systems that use bigram and trigram language models for\ndecoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction\nof 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based\nsystem also outperforms the baseline CNN-TDNNF system on the AMI task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Pre-Trained Language Models for Cross-Cultural Differences in Values. (arXiv:2203.13722v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13722","description":"<p>Language embeds information about social, cultural, and political values\npeople hold. Prior work has explored social and potentially harmful biases\nencoded in Pre-Trained Language models (PTLMs). However, there has been no\nsystematic study investigating how values embedded in these models vary across\ncultures. In this paper, we introduce probes to study which values across\ncultures are embedded in these models, and whether they align with existing\ntheories and cross-cultural value surveys. We find that PTLMs capture\ndifferences in values across cultures, but those only weakly align with\nestablished value surveys. We discuss implications of using mis-aligned models\nin cross-cultural settings, as well as ways of aligning PTLMs with value\nsurveys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaffee_L/0/1/0/all/0/1\">Lucie-Aim&#xe9;e Kaffee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models. (arXiv:2203.13778v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13778","description":"<p>Social media platforms are used by a large number of people prominently to\nexpress their thoughts and opinions. However, these platforms have contributed\nto a substantial amount of hateful and abusive content as well. Therefore, it\nis important to curb the spread of hate speech on these platforms. In India,\nMarathi is one of the most popular languages used by a wide audience. In this\nwork, we present L3Cube-MahaHate, the first major Hate Speech Dataset in\nMarathi. The dataset is curated from Twitter, annotated manually. Our dataset\nconsists of over 25000 distinct tweets labeled into four major classes i.e\nhate, offensive, profane, and not. We present the approaches used for\ncollecting and annotating the data and the challenges faced during the process.\nFinally, we present baseline classification results using deep learning models\nbased on CNN, LSTM, and Transformers. We explore mono-lingual and multi-lingual\nvariants of BERT like MahaBERT, IndicBERT, mBERT, and xlm-RoBERTa and show that\nmono-lingual models perform better than their multi-lingual counterparts. The\nMahaBERT model provides the best results on L3Cube-MahaHate Corpus. The data\nand models are available at https://github.com/l3cube-pune/MarathiNLP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velankar_A/0/1/0/all/0/1\">Abhishek Velankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1\">Hrushikesh Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gore_A/0/1/0/all/0/1\">Amol Gore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salunke_S/0/1/0/all/0/1\">Shubham Salunke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallels of human language in the behavior of bottlenose dolphins. (arXiv:1605.01661v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/1605.01661","description":"<p>A short review of similarities between dolphins and humans with the help of\nquantitative linguistics and information theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">R. Ferrer-i-Cancho</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lusseau_D/0/1/0/all/0/1\">D. Lusseau</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+McCowan_B/0/1/0/all/0/1\">B. McCowan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Generation from Knowledge Graphs with Graph Transformers. (arXiv:1904.02342v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1904.02342","description":"<p>Generating texts which express complex ideas spanning multiple sentences\nrequires a structured representation of their content (document plan), but\nthese representations are prohibitively expensive to manually produce. In this\nwork, we address the problem of generating coherent multi-sentence texts from\nthe output of an information extraction system, and in particular a knowledge\ngraph. Graphical knowledge representations are ubiquitous in computing, but\npose a significant challenge for text generation techniques due to their\nnon-hierarchical nature, collapsing of long-distance dependencies, and\nstructural variety. We introduce a novel graph transforming encoder which can\nleverage the relational structure of such knowledge graphs without imposing\nlinearization or hierarchical constraints. Incorporated into an encoder-decoder\nsetup, we provide an end-to-end trainable system for graph-to-text generation\nthat we apply to the domain of scientific text. Automatic and human evaluations\nshow that our technique produces more informative texts which exhibit better\ndocument structure than competitive encoder-decoder methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekal_D/0/1/0/all/0/1\">Dhanush Bekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.06499","description":"<p>We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1\">Andrew Nystrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1\">Douglas Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\infty$-former: Infinite Memory Transformer. (arXiv:2109.00301v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00301","description":"<p>Transformers are unable to model long-term memories effectively, since the\namount of computation they need to perform grows with the context length. While\nvariations of efficient transformers have been proposed, they all have a finite\nmemory capacity and are forced to drop old information. In this paper, we\npropose the $\\infty$-former, which extends the vanilla transformer with an\nunbounded long-term memory. By making use of a continuous-space attention\nmechanism to attend over the long-term memory, the $\\infty$-former's attention\ncomplexity becomes independent of the context length, trading off memory length\nwith precision. In order to control where precision is more important,\n$\\infty$-former maintains \"sticky memories\" being able to model arbitrarily\nlong contexts while keeping the computation budget fixed. Experiments on a\nsynthetic sorting task, language modeling, and document grounded dialogue\ngeneration demonstrate the $\\infty$-former's ability to retain information from\nlong sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03892","description":"<p>We investigate the use of multimodal information contained in images as an\neffective method for enhancing the commonsense of Transformer models for text\ngeneration. We perform experiments using BART and T5 on concept-to-text\ngeneration, specifically the task of generative commonsense reasoning, or\nCommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text\nGeneration. VisCTG involves captioning images representing appropriate everyday\nscenarios, and using these captions to enrich and steer the generation process.\nComprehensive evaluation and analysis demonstrate that VisCTG noticeably\nimproves model performance while successfully addressing several issues of the\nbaseline generations, including poor commonsense, fluency, and specificity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhuofu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Machine Translation Evaluation. (arXiv:2109.06352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06352","description":"<p>Several neural-based metrics have been recently proposed to evaluate machine\ntranslation quality. However, all of them resort to point estimates, which\nprovide limited information at segment level. This is made worse as they are\ntrained on noisy, biased and scarce human judgements, often resulting in\nunreliable quality predictions. In this paper, we introduce uncertainty-aware\nMT evaluation and analyze the trustworthiness of the predicted quality. We\ncombine the COMET framework with two uncertainty estimation methods, Monte\nCarlo dropout and deep ensembles, to obtain quality scores along with\nconfidence intervals. We compare the performance of our uncertainty-aware MT\nevaluation methods across multiple language pairs from the QT21 dataset and the\nWMT20 metrics task, augmented with MQM annotations. We experiment with varying\nnumbers of references and further discuss the usefulness of uncertainty-aware\nquality estimation (without references) to flag possibly critical translation\nmistakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glushkova_T/0/1/0/all/0/1\">Taisiya Glushkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing as Quantifying Inductive Bias. (arXiv:2110.08388v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08388","description":"<p>Pre-trained contextual representations have led to dramatic performance\nimprovements on a range of downstream tasks. Such performance improvements have\nmotivated researchers to quantify and understand the linguistic information\nencoded in these representations. In general, researchers quantify the amount\nof linguistic information through probing, an endeavor which consists of\ntraining a supervised model to predict a linguistic property directly from the\ncontextual representations. Unfortunately, this definition of probing has been\nsubject to extensive criticism in the literature, and has been observed to lead\nto paradoxical and counter-intuitive results. In the theoretical portion of\nthis paper, we take the position that the goal of probing ought to be measuring\nthe amount of inductive bias that the representations encode on a specific\ntask. We further describe a Bayesian framework that operationalizes this goal\nand allows us to quantify the representations' inductive bias. In the empirical\nportion of the paper, we apply our framework to a variety of NLP tasks. Our\nresults suggest that our proposed framework alleviates many previous problems\nfound in probing. Moreover, we are able to offer concrete evidence that -- for\nsome tasks -- fastText can offer a better inductive bias than BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Immer_A/0/1/0/all/0/1\">Alexander Immer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum Description Length Recurrent Neural Networks. (arXiv:2111.00600v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00600","description":"<p>We train neural networks to optimize a Minimum Description Length score,\ni.e., to balance between the complexity of the network and its accuracy at a\ntask. We show that networks optimizing this objective function master tasks\ninvolving memory challenges and go beyond context-free languages. These\nlearners master languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^{2n}$,\n$a^nb^mc^{n+m}$, and they perform addition. Moreover, they often do so with\n100% accuracy. The networks are small, and their inner workings are\ntransparent. We thus provide formal proofs that their perfect accuracy holds\nnot only on a given test set, but for any input sequence. To our knowledge, no\nother connectionist model has been shown to capture the underlying grammars for\nthese languages in full generality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_N/0/1/0/all/0/1\">Nur Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyer_M/0/1/0/all/0/1\">Michal Geyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemla_E/0/1/0/all/0/1\">Emmanuel Chemla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzir_R/0/1/0/all/0/1\">Roni Katzir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02080","description":"<p>Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07991","description":"<p>This paper presents contrastive-tuning, a simple method employing contrastive\ntraining to align image and text models while still taking advantage of their\npre-training. In our empirical study we find that locked pre-trained image\nmodels with unlocked text models work best. We call this instance of\ncontrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model\nto read out good representations from a pre-trained image model for new tasks.\nA LiT model gains the capability of zero-shot transfer to new vision tasks,\nsuch as image classification or retrieval. The proposed LiT is widely\napplicable; it works reliably with multiple pre-training methods (supervised\nand unsupervised) and across diverse architectures (ResNet, Vision Transformers\nand MLP-Mixer) using three different image-text datasets. With the\ntransformer-based pre-trained ViT-g/14 model, the LiT model achieves 84.5%\nzero-shot transfer accuracy on the ImageNet test set, and 81.1% on the\nchallenging out-of-distribution ObjectNet test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation. (arXiv:2112.01589v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01589","description":"<p>Assessing the quality of natural language generation systems through human\nannotation is very expensive. Additionally, human annotation campaigns are\ntime-consuming and include non-reusable human labour. In practice, researchers\nrely on automatic metrics as a proxy of quality. In the last decade, many\nstring-based metrics (e.g., BLEU) have been introduced. However, such metrics\nusually rely on exact matches and thus, do not robustly handle synonyms. In\nthis paper, we introduce InfoLM a family of untrained metrics that can be\nviewed as a string-based metric that addresses the aforementioned flaws thanks\nto a pre-trained masked language model. This family of metrics also makes use\nof information measures allowing the adaptation of InfoLM to various evaluation\ncriteria. Using direct assessment, we demonstrate that InfoLM achieves\nstatistically significant improvement and over $10$ points of correlation gains\nin many configurations on both summarization and data2text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Geography: Mapping Language Data to Language Users. (arXiv:2112.03497v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03497","description":"<p>As language technologies become more ubiquitous, there are increasing efforts\ntowards expanding the language diversity and coverage of natural language\nprocessing (NLP) systems. Arguably, the most important factor influencing the\nquality of modern NLP systems is data availability. In this work, we study the\ngeographical representativeness of NLP datasets, aiming to quantify if and by\nhow much do NLP datasets match the expected needs of the language speakers. In\ndoing so, we use entity recognition and linking systems, also making important\nobservations about their cross-lingual consistency and giving suggestions for\nmore robust evaluation. Last, we explore some geographical and economic factors\nthat may explain the observed dataset distributions. Code and data are\navailable here: https://github.com/ffaisal93/dataset_geography. Additional\nvisualizations are available here: https://nlp.cs.gmu.edu/project/datasetmaps/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinkai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02113","description":"<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKGC/tree/main/GenKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. (arXiv:2202.05932v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.05932","description":"<p>Large-scale multi-label text classification (LMTC) aims to associate a\ndocument with its relevant labels from a large candidate set. Most existing\nLMTC approaches rely on massive human-annotated training data, which are often\ncostly to obtain and suffer from a long-tailed label distribution (i.e., many\nlabels occur only a few times in the training set). In this paper, we study\nLMTC under the zero-shot setting, which does not require any annotated\ndocuments with labels and only relies on label surface names and descriptions.\nTo train a classifier that calculates the similarity score between a document\nand a label, we propose a novel metadata-induced contrastive learning (MICoL)\nmethod. Different from previous text-based contrastive learning techniques,\nMICoL exploits document metadata (e.g., authors, venues, and references of\nresearch papers), which are widely available on the Web, to derive similar\ndocument-document pairs. Experimental results on two large-scale datasets show\nthat: (1) MICoL significantly outperforms strong zero-shot text classification\nand contrastive learning baselines; (2) MICoL is on par with the\nstate-of-the-art supervised metadata-aware LMTC method trained on 10K-200K\nlabeled documents; and (3) MICoL tends to predict more infrequent labels than\nsupervised methods, thus alleviates the deteriorated performance on long-tailed\nlabels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chieh-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Boya Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Junheng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye-Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuansan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02668","description":"<p>It has been widely known that CAM (Class Activation Map) usually only\nactivates discriminative object regions and falsely includes lots of\nobject-related backgrounds. As only a fixed set of image-level object labels\nare available to the WSSS (weakly supervised semantic segmentation) model, it\ncould be very difficult to suppress those diverse background regions consisting\nof open set objects. In this paper, we propose a novel Cross Language Image\nMatching (CLIMS) framework, based on the recently introduced Contrastive\nLanguage-Image Pre-training (CLIP) model, for WSSS. The core idea of our\nframework is to introduce natural language supervision to activate more\ncomplete object regions and suppress closely-related open background regions.\nIn particular, we design object, background region and text label matching\nlosses to guide the model to excite more reasonable object regions for CAM of\neach category. In addition, we design a co-occurring background suppression\nloss to prevent the model from activating closely-related background regions,\nwith a predefined set of class-related background text descriptions. These\ndesigns enable the proposed CLIMS to generate a more complete and compact\nactivation map for the target objects. Extensive experiments on PASCAL VOC2012\ndataset show that our CLIMS significantly outperforms the previous\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xianxu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Variational Hierarchical Model for Neural Cross-Lingual Summarization. (arXiv:2203.03820v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03820","description":"<p>The goal of the cross-lingual summarization (CLS) is to convert a document in\none language (e.g., English) to a summary in another one (e.g., Chinese).\nEssentially, the CLS task is the combination of machine translation (MT) and\nmonolingual summarization (MS), and thus there exists the hierarchical\nrelationship between MT\\&amp;MS and CLS. Existing studies on CLS mainly focus on\nutilizing pipeline methods or jointly training an end-to-end model through an\nauxiliary MT or MS objective. However, it is very challenging for the model to\ndirectly conduct CLS as it requires both the abilities to translate and\nsummarize. To address this issue, we propose a hierarchical model for the CLS\ntask, based on the conditional variational auto-encoder. The hierarchical model\ncontains two kinds of latent variables at the local and global levels,\nrespectively. At the local level, there are two latent variables, one for\ntranslation and the other for summarization. As for the global level, there is\nanother latent variable for cross-lingual summarization conditioned on the two\nlocal-level variables. Experiments on two language directions (English-Chinese)\nverify the effectiveness and superiority of the proposed approach. In addition,\nwe show that our model is able to generate better cross-lingual summaries than\ncomparison models in the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis. (arXiv:2203.13278v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13278","description":"<p>While recent years have witnessed a dramatic upsurge of exploiting deep\nneural networks toward solving image denoising, existing methods mostly rely on\nsimple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG\ncompression noise and camera sensor noise, and a general-purpose blind\ndenoising method for real images remains unsolved. In this paper, we attempt to\nsolve this problem from the perspective of network architecture design and\ntraining data synthesis. Specifically, for the network architecture design, we\npropose a swin-conv block to incorporate the local modeling ability of residual\nconvolutional layer and non-local modeling ability of swin transformer block,\nand then plug it as the main building block into the widely-used image-to-image\ntranslation UNet architecture. For the training data synthesis, we design a\npractical noise degradation model which takes into consideration different\nkinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and\nprocessed camera sensor noises) and resizing, and also involves a random\nshuffle strategy and a double degradation strategy. Extensive experiments on\nAGWN removal and real image denoising demonstrate that the new network\narchitecture design achieves state-of-the-art performance and the new\ndegradation model can help to significantly improve the practicability. We\nbelieve our work can provide useful insights into current denoising research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectively leveraging Multi-modal Features for Movie Genre Classification. (arXiv:2203.13281v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13281","description":"<p>Movie genre classification has been widely studied in recent years due to its\nvarious applications in video editing, summarization, and recommendation. Prior\nwork has typically addressed this task by predicting genres based solely on the\nvisual content. As a result, predictions from these methods often perform\npoorly for genres such as documentary or musical, since non-visual modalities\nlike audio or language play an important role in correctly classifying these\ngenres. In addition, the analysis of long videos at frame level is always\nassociated with high computational cost and makes the prediction less\nefficient. To address these two issues, we propose a Multi-Modal approach\nleveraging shot information, MMShot, to classify video genres in an efficient\nand effective way. We evaluate our method on MovieNet and Condensed Movies for\ngenre classification, achieving 17% ~ 21% improvement on mean Average Precision\n(mAP) over the state-of-the-art. Extensive experiments are conducted to\ndemonstrate the ability of MMShot for long video analysis and uncover the\ncorrelations between genres and multiple movie elements. We also demonstrate\nour approach's ability to generalize by evaluating the scene boundary detection\ntask, achieving 1.1% improvement on Average Precision (AP) over the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yiwen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xin Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition. (arXiv:2203.13285v1 [cs.SD])","link":"http://arxiv.org/abs/2203.13285","description":"<p>In this paper, we present our submission to 3rd Affective Behavior Analysis\nin-the-wild (ABAW) challenge. Learningcomplex interactions among multimodal\nsequences is critical to recognise dimensional affect from in-the-wild\naudiovisual data. Recurrence and attention are the two widely used sequence\nmodelling mechanisms in the literature. To clearly understand the performance\ndifferences between recurrent and attention models in audiovisual affect\nrecognition, we present a comprehensive evaluation of fusion models based on\nLSTM-RNNs, self-attention and cross-modal attention, trained for valence and\narousal estimation. Particularly, we study the impact of some key design\nchoices: the modelling complexity of CNN backbones that provide features to the\nthe temporal models, with and without end-to-end learning. We trained the\naudiovisual affect recognition models on in-the-wild ABAW corpus by\nsystematically tuning the hyper-parameters involved in the network architecture\ndesign and training optimisation. Our extensive evaluation of the audiovisual\nfusion models shows that LSTM-RNNs can outperform the attention models when\ncoupled with low-complex CNN backbones and trained in an end-to-end fashion,\nimplying that attention models may not necessarily be the optimal choice for\ncontinuous-time multimodal emotion recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karas_V/0/1/0/all/0/1\">Vincent Karas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellamekala_M/0/1/0/all/0/1\">Mani Kumar Tellamekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallol_Ragolta_A/0/1/0/all/0/1\">Adria Mallol-Ragolta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1\">Michel Valstar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for fingerspelled content in American Sign Language. (arXiv:2203.13291v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13291","description":"<p>Natural language processing for sign language video - including tasks like\nrecognition, translation, and search - is crucial for making artificial\nintelligence technologies accessible to deaf individuals, and is gaining\nresearch interest in recent years. In this paper, we address the problem of\nsearching for fingerspelled key-words or key phrases in raw sign language\nvideos. This is an important task since significant content in sign language is\noften conveyed via fingerspelling, and to our knowledge the task has not been\nstudied before. We propose an end-to-end model for this task, FSS-Net, that\njointly detects fingerspelling and matches it to a text sequence. Our\nexperiments, done on a large public dataset of ASL fingerspelling in the wild,\nshow the importance of fingerspelling detection as a component of a search and\nretrieval model. Our model significantly outperforms baseline methods adapted\nfrom prior work on related tasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brentari_D/0/1/0/all/0/1\">Diane Brentari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RayTran: 3D pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers. (arXiv:2203.13296v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13296","description":"<p>We propose a transformer-based neural network architecture for multi-object\n3D reconstruction from RGB videos. It relies on two alternative ways to\nrepresent its knowledge: as a global 3D grid of features and an array of\nview-specific 2D grids. We progressively exchange information between the two\nwith a dedicated bidirectional attention mechanism. We exploit knowledge about\nthe image formation process to significantly sparsify the attention weight\nmatrix, making our architecture feasible on current hardware, both in terms of\nmemory and computation. We attach a DETR-style head on top of the 3D feature\ngrid in order to detect the objects in the scene and to predict their 3D pose\nand 3D shape. Compared to previous methods, our architecture is single stage,\nend-to-end trainable, and it can reason holistically about a scene from\nmultiple video frames without needing a brittle tracking step. We evaluate our\nmethod on the challenging Scan2CAD dataset, where we outperform (1) recent\nstate-of-the-art methods for 3D object pose estimation from RGB videos; and (2)\na strong alternative method combining Multi-view Stereo with RGB-D CAD\nalignment. We plan to release our source code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyszkiewicz_M/0/1/0/all/0/1\">Micha&#x142; J. Tyszkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maninis_K/0/1/0/all/0/1\">Kevis-Kokitsi Maninis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popov_S/0/1/0/all/0/1\">Stefan Popov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Multi-label Facial Action Unit Detection with Transformer. (arXiv:2203.13301v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13301","description":"<p>Facial Action Coding System is an important approach of facial expression\nanalysis.This paper describes our submission to the third Affective Behavior\nAnalysis (ABAW) 2022 competition. We proposed a transfomer based model to\ndetect facial action unit (FAU) in video. To be specific, we firstly trained a\nmulti-modal model to extract both audio and visual feature. After that, we\nproposed a action units correlation module to learn relationships between each\naction unit labels and refine action unit detection result. Experimental\nresults on validation dataset shows that our method achieves better performance\nthan baseline model, which verifies that the effectiveness of proposed network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Online Action Segmentation in Multi-View Instructional Videos. (arXiv:2203.13309v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13309","description":"<p>This paper addresses a new problem of weakly-supervised online action\nsegmentation in instructional videos. We present a framework to segment\nstreaming videos online at test time using Dynamic Programming and show its\nadvantages over greedy sliding window approach. We improve our framework by\nintroducing the Online-Offline Discrepancy Loss (OODL) to encourage the\nsegmentation results to have a higher temporal consistency. Furthermore, only\nduring training, we exploit frame-wise correspondence between multiple views as\nsupervision for training weakly-labeled instructional videos. In particular, we\ninvestigate three different multi-view inference techniques to generate more\naccurate frame-wise pseudo ground-truth with no additional annotation cost. We\npresent results and ablation studies on two benchmark multi-view datasets,\nBreakfast and IKEA ASM. Experimental results show efficacy of the proposed\nmethods both qualitatively and quantitatively in two domains of cooking and\nassembly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghoddoosian_R/0/1/0/all/0/1\">Reza Ghoddoosian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_I/0/1/0/all/0/1\">Isht Dwivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1\">Nakul Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dariush_B/0/1/0/all/0/1\">Behzad Dariush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13310","description":"<p>Monocular 3D object detection has long been a challenging task in autonomous\ndriving, which requires to decode 3D predictions solely from a single 2D image.\nMost existing methods follow conventional 2D object detectors to first localize\nobjects by their centers, and then predict 3D attributes using\ncenter-neighboring local features. However, such center-based pipeline views 3D\nprediction as a subordinate task and lacks inter-object depth interactions with\nglobal spatial clues. In this paper, we introduce a simple framework for\nMonocular DEtection with depth-aware TRansformer, named MonoDETR. We enable the\nvanilla transformer to be depth-aware and enforce the whole detection process\nguided by depth. Specifically, we represent 3D object candidates as a set of\nqueries and produce non-local depth embeddings of the input image by a\nlightweight depth predictor and an attention-based depth encoder. Then, we\npropose a depth-aware decoder to conduct both inter-query and query-scene depth\nfeature communication. In this way, each object estimates its 3D attributes\nadaptively from the depth-informative regions on the image, not limited by\ncenter-around features. With minimal handcrafted designs, MonoDETR is an\nend-to-end framework without additional data, anchors or NMS and achieves\ncompetitive performance on KITTI benchmark among state-of-the-art center-based\nnetworks. Extensive ablation studies demonstrate the effectiveness of our\napproach and its potential to serve as a transformer baseline for future\nmonocular research. Code is available at\nhttps://github.com/ZrrSkywalker/MonoDETR.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuanzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SharpContour: A Contour-based Boundary Refinement Approach for Efficient and Accurate Instance Segmentation. (arXiv:2203.13312v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13312","description":"<p>Excellent performance has been achieved on instance segmentation but the\nquality on the boundary area remains unsatisfactory, which leads to a rising\nattention on boundary refinement. For practical use, an ideal post-processing\nrefinement scheme are required to be accurate, generic and efficient. However,\nmost of existing approaches propose pixel-wise refinement, which either\nintroduce a massive computation cost or design specifically for different\nbackbone models. Contour-based models are efficient and generic to be\nincorporated with any existing segmentation methods, but they often generate\nover-smoothed contour and tend to fail on corner areas. In this paper, we\npropose an efficient contour-based boundary refinement approach, named\nSharpContour, to tackle the segmentation of boundary area. We design a novel\ncontour evolution process together with an Instance-aware Point Classifier. Our\nmethod deforms the contour iteratively by updating offsets in a discrete\nmanner. Differing from existing contour evolution methods, SharpContour\nestimates each offset more independently so that it predicts much sharper and\naccurate contours. Notably, our method is generic to seamlessly work with\ndiverse existing models with a small computational cost. Experiments show that\nSharpContour achieves competitive gains whilst preserving high efficiency\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liangdong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for laboratory earthquake prediction and autoregressive forecasting of fault zone stress. (arXiv:2203.13313v1 [physics.geo-ph])","link":"http://arxiv.org/abs/2203.13313","description":"<p>Earthquake forecasting and prediction have long and in some cases sordid\nhistories but recent work has rekindled interest based on advances in early\nwarning, hazard assessment for induced seismicity and successful prediction of\nlaboratory earthquakes. In the lab, frictional stick-slip events provide an\nanalog for earthquakes and the seismic cycle. Labquakes are ideal targets for\nmachine learning (ML) because they can be produced in long sequences under\ncontrolled conditions. Recent works show that ML can predict several aspects of\nlabquakes using fault zone acoustic emissions. Here, we generalize these\nresults and explore deep learning (DL) methods for labquake prediction and\nautoregressive (AR) forecasting. DL improves existing ML methods of labquake\nprediction. AR methods allow forecasting at future horizons via iterative\npredictions. We demonstrate that DL models based on Long-Short Term Memory\n(LSTM) and Convolution Neural Networks predict labquakes under several\nconditions, and that fault zone stress can be predicted with fidelity,\nconfirming that acoustic energy is a fingerprint of fault zone stress. We\npredict also time to start of failure (TTsF) and time to the end of Failure\n(TTeF) for labquakes. Interestingly, TTeF is successfully predicted in all\nseismic cycles, while the TTsF prediction varies with the amount of preseismic\nfault creep. We report AR methods to forecast the evolution of fault stress\nusing three sequence modeling frameworks: LSTM, Temporal Convolution Network\nand Transformer Network. AR forecasting is distinct from existing predictive\nmodels, which predict only a target variable at a specific time. The results\nfor forecasting beyond a single seismic cycle are limited but encouraging. Our\nML/DL models outperform the state-of-the-art and our autoregressive model\nrepresents a novel framework that could enhance current methods of earthquake\nforecasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Laurenti_L/0/1/0/all/0/1\">Laura Laurenti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tinti_E/0/1/0/all/0/1\">Elisa Tinti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Galasso_F/0/1/0/all/0/1\">Fabio Galasso</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Franco_L/0/1/0/all/0/1\">Luca Franco</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Marone_C/0/1/0/all/0/1\">Chris Marone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Gait Recognition Using Bag of Words Feature Representation Method. (arXiv:2203.13317v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13317","description":"<p>In this paper, we propose a novel gait recognition method based on a\nbag-of-words feature representation method. The algorithm is trained, tested\nand evaluated on a unique human gait data consisting of 93 individuals who\nwalked with comfortable pace between two end points during two different\nsessions. To evaluate the effectiveness of the proposed model, the results are\ncompared with the outputs of the classification using extracted features. As it\nis presented, the proposed method results in significant improvement accuracy\ncompared to using common statistical features, in all the used classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayat_N/0/1/0/all/0/1\">Nasrin Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_E/0/1/0/all/0/1\">Elham Rastegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qifeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NPBG++: Accelerating Neural Point-Based Graphics. (arXiv:2203.13318v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13318","description":"<p>We present a new system (NPBG++) for the novel view synthesis (NVS) task that\nachieves high rendering realism with low scene fitting time. Our method\nefficiently leverages the multiview observations and the point cloud of a\nstatic scene to predict a neural descriptor for each point, improving upon the\npipeline of Neural Point-Based Graphics in several important ways. By\npredicting the descriptors with a single pass through the source images, we\nlift the requirement of per-scene optimization while also making the neural\ndescriptors view-dependent and more suitable for scenes with strong\nnon-Lambertian effects. In our comparisons, the proposed system outperforms\nprevious NVS approaches in terms of fitting and rendering runtimes while\nproducing images of similar quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakhimov_R/0/1/0/all/0/1\">Ruslan Rakhimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardelean_A/0/1/0/all/0/1\">Andrei-Timotei Ardelean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1\">Victor Lempitsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to Mesh Without 3D Supervision Using Limit Subdivision. (arXiv:2203.13333v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13333","description":"<p>We present a technique for zero-shot generation of a 3D model using only a\ntarget text prompt. Without a generative model or any 3D supervision our method\ndeforms a control shape of a limit subdivided surface along with a texture map\nand normal map to obtain a 3D model asset that matches the input text prompt\nand can be deployed into games or modeling applications. We rely only on a\npre-trained CLIP model that compares the input text prompt with differentiably\nrendered images of our 3D model. While previous works have focused on\nstylization or required training of generative models we perform optimization\non mesh parameters directly to generate shape and texture. To improve the\nquality of results we also introduce a set of techniques such as render\naugmentations, primitive selection, prompt augmentation that guide the mesh\ntowards a suitable result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_N/0/1/0/all/0/1\">Nasir Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianhao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popa_T/0/1/0/all/0/1\">Tiberiu Popa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occluded Human Mesh Recovery. (arXiv:2203.13349v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13349","description":"<p>Top-down methods for monocular human mesh recovery have two stages: (1)\ndetect human bounding boxes; (2) treat each bounding box as an independent\nsingle-human mesh recovery task. Unfortunately, the single-human assumption\ndoes not hold in images with multi-human occlusion and crowding. Consequently,\ntop-down methods have difficulties in recovering accurate 3D human meshes under\nsevere person-person occlusion. To address this, we present Occluded Human Mesh\nRecovery (OCHMR) - a novel top-down mesh recovery approach that incorporates\nimage spatial context to overcome the limitations of the single-human\nassumption. The approach is conceptually simple and can be applied to any\nexisting top-down architecture. Along with the input image, we condition the\ntop-down model on spatial context from the image in the form of body-center\nheatmaps. To reason from the predicted body centermaps, we introduce Contextual\nNormalization (CoNorm) blocks to adaptively modulate intermediate features of\nthe top-down model. The contextual conditioning helps our model disambiguate\nbetween two severely overlapping human bounding-boxes, making it robust to\nmulti-person occlusion. Compared with state-of-the-art methods, OCHMR achieves\nsuperior performance on challenging multi-person benchmarks like 3DPW,\nCrowdPose and OCHuman. Specifically, our proposed contextual reasoning\narchitecture applied to the SPIN model with ResNet-50 backbone results in 75.2\nPMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a\nsignificant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the\nbaseline. Code and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1\">Rawal Khirodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Shashank Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks. (arXiv:2203.13371v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13371","description":"<p>Large-scale pretrained image-text models have shown incredible zero-shot\nperformance in a handful of tasks, including video ones such as action\nrecognition and text-to-video retrieval. However, these models haven't been\nadapted to video, mainly because they don't account for the time dimension but\nalso because video frames are different from the typical images (e.g.,\ncontaining motion blur, less sharpness). In this paper, we present a\nfine-tuning strategy to refine these large-scale pretrained image-text models\nfor zero-shot video understanding tasks. We show that by carefully adapting\nthese models we obtain considerable improvements on two zero-shot Action\nRecognition tasks and three zero-shot Text-to-video Retrieval tasks. The code\nis available at https://github.com/bryant1410/fitclip\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Representation Forgetting in Supervised and Unsupervised Continual Learning. (arXiv:2203.13381v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13381","description":"<p>Continual Learning research typically focuses on tackling the phenomenon of\ncatastrophic forgetting in neural networks. Catastrophic forgetting is\nassociated with an abrupt loss of knowledge previously learned by a model when\nthe task, or more broadly the data distribution, being trained on changes. In\nsupervised learning problems this forgetting, resulting from a change in the\nmodel's representation, is typically measured or observed by evaluating the\ndecrease in old task performance. However, a model's representation can change\nwithout losing knowledge about prior tasks. In this work we consider the\nconcept of representation forgetting, observed by using the difference in\nperformance of an optimal linear classifier before and after a new task is\nintroduced. Using this tool we revisit a number of standard continual learning\nbenchmarks and observe that, through this lens, model representations trained\nwithout any explicit control for forgetting often experience small\nrepresentation forgetting and can sometimes be comparable to methods which\nexplicitly control for forgetting, especially in longer task sequences. We also\nshow that representation forgetting can lead to new insights on the effect of\nmodel capacity and loss function used in continual learning. Based on our\nresults, we show that a simple yet competitive approach is to learn\nrepresentations continually with standard supervised contrastive learning while\nconstructing prototypes of class samples when queried on old samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davari_M/0/1/0/all/0/1\">MohammadReza Davari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadi_N/0/1/0/all/0/1\">Nader Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudur_S/0/1/0/all/0/1\">Sudhir Mudur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1\">Rahaf Aljundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation. (arXiv:2203.13387v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13387","description":"<p>3D human pose estimation can be handled by encoding the geometric\ndependencies between the body parts and enforcing the kinematic constraints.\nRecently, Transformer has been adopted to encode the long-range dependencies\nbetween the joints in the spatial and temporal domains. While they had shown\nexcellence in long-range dependencies, studies have noted the need for\nimproving the locality of vision Transformers. In this direction, we propose a\nnovel pose estimation Transformer featuring rich representations of body joints\ncritical for capturing subtle changes across frames (i.e., inter-feature\nrepresentation). Specifically, through two novel interaction modules;\nCross-Joint Interaction and Cross-Frame Interaction, the model explicitly\nencodes the local and global dependencies between the body joints. The proposed\narchitecture achieved state-of-the-art performance on two popular 3D human pose\nestimation datasets, Human3.6 and MPI-INF-3DHP. In particular, our proposed\nCrossFormer method boosts performance by 0.9% and 0.3%, compared to the closest\ncounterpart, PoseFormer, using the detected 2D poses and ground-truth settings\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassanin_M/0/1/0/all/0/1\">Mohammed Hassanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamiss_A/0/1/0/all/0/1\">Abdelwahed Khamiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radwan_I/0/1/0/all/0/1\">Ibrahim Radwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point2Seq: Detecting 3D Objects as Sequences. (arXiv:2203.13394v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13394","description":"<p>We present a simple and effective framework, named Point2Seq, for 3D object\ndetection from point clouds. In contrast to previous methods that normally\n{predict attributes of 3D objects all at once}, we expressively model the\ninterdependencies between attributes of 3D objects, which in turn enables a\nbetter detection accuracy. Specifically, we view each 3D object as a sequence\nof words and reformulate the 3D object detection task as decoding words from 3D\nscenes in an auto-regressive manner. We further propose a lightweight\nscene-to-sequence decoder that can auto-regressively generate words conditioned\non features from a 3D scene as well as cues from the preceding words. The\npredicted words eventually constitute a set of sequences that completely\ndescribe the 3D objects in the scene, and all the predicted sequences are then\nautomatically assigned to the respective ground truths through similarity-based\nsequence matching. Our approach is conceptually intuitive and can be readily\nplugged upon most existing 3D-detection backbones without adding too much\ncomputational overhead; the sequential decoding paradigm we proposed, on the\nother hand, can better exploit information from complex 3D scenes with the aid\nof preceding predicted words. Without bells and whistles, our method\nsignificantly outperforms previous anchor- and center-based 3D object detection\nframeworks, yielding the new state of the art on the challenging ONCE dataset\nas well as the Waymo Open Dataset. Code is available at\n\\url{https://github.com/ocNflag/point2seq}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yujing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Minzhe Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1\">Michael Bi Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale and Cross-scale Contrastive Learning for Semantic Segmentation. (arXiv:2203.13409v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13409","description":"<p>This work considers supervised contrastive learning for semantic\nsegmentation. Our approach is model agnostic. We apply contrastive learning to\nenhance the discriminative power of the multi-scale features extracted by\nsemantic segmentation networks. Our key methodological insight is to leverage\nsamples from the feature spaces emanating from multiple stages of a model's\nencoder itself requiring neither data augmentation nor online memory banks to\nobtain a diverse set of samples. To allow for such an extension we introduce an\nefficient and effective sampling process, that enables applying contrastive\nlosses over the encoder's features at multiple scales. Furthermore, by first\nmapping the encoder's multi-scale representations to a common feature space, we\ninstantiate a novel form of supervised local-global constraint by introducing\ncross-scale contrastive learning linking high-resolution local features to\nlow-resolution global features. Combined, our multi-scale and cross-scale\ncontrastive losses boost performance of various models (DeepLabV3, HRNet,\nOCRNet, UPerNet) with both CNN and Transformer backbones, when evaluated on 4\ndiverse datasets from natural (Cityscapes, PascalContext, ADE20K) but also\nsurgical (CaDIS) domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pissas_T/0/1/0/all/0/1\">Theodoros Pissas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravasio_C/0/1/0/all/0/1\">Claudio S. Ravasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1\">Lyndon Da Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeles_C/0/1/0/all/0/1\">Christos Bergeles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes. (arXiv:2203.13412v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13412","description":"<p>Sound source localization in visual scenes aims to localize objects emitting\nthe sound in a given image. Recent works showing impressive localization\nperformance typically rely on the contrastive learning framework. However, the\nrandom sampling of negatives, as commonly adopted in these methods, can result\nin misalignment between audio and visual features and thus inducing ambiguity\nin localization. In this paper, instead of following previous literature, we\npropose Self-Supervised Predictive Learning (SSPL), a negative-free method for\nsound localization via explicit positive mining. Specifically, we first devise\na three-stream network to elegantly associate sound source with two augmented\nviews of one corresponding video frame, leading to semantically coherent\nsimilarities between audio and visual features. Second, we introduce a novel\npredictive coding module for audio-visual feature alignment. Such a module\nassists SSPL to focus on target objects in a progressive manner and effectively\nlowers the positive-pair learning difficulty. Experiments show surprising\nresults that SSPL outperforms the state-of-the-art approach on two standard\nsound localization benchmarks. In particular, SSPL achieves significant\nimprovements of 8.6% cIoU and 3.4% AUC on SoundNet-Flickr compared to the\nprevious best. Code is available at: https://github.com/zjsong/SSPL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zengjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Junsong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Boundaries: Lemon or Lemonade for Semi-supervised Instance Segmentation?. (arXiv:2203.13427v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13427","description":"<p>Current instance segmentation methods rely heavily on pixel-level annotated\nimages. The huge cost to obtain such fully-annotated images restricts the\ndataset scale and limits the performance. In this paper, we formally address\nsemi-supervised instance segmentation, where unlabeled images are employed to\nboost the performance. We construct a framework for semi-supervised instance\nsegmentation by assigning pixel-level pseudo labels. Under this framework, we\npoint out that noisy boundaries associated with pseudo labels are double-edged.\nWe propose to exploit and resist them in a unified manner simultaneously: 1) To\ncombat the negative effects of noisy boundaries, we propose a noise-tolerant\nmask head by leveraging low-resolution features. 2) To enhance the positive\nimpacts, we introduce a boundary-preserving map for learning detailed\ninformation within boundary-relevant regions. We evaluate our approach by\nextensive experiments. It behaves extraordinarily, outperforming the supervised\nbaseline by a large margin, more than 6% on Cityscapes, 7% on COCO and 4.5% on\nBDD100k. On Cityscapes, our method achieves comparable performance by utilizing\nonly 30% labeled images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices. (arXiv:2203.13436v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13436","description":"<p>In this paper, we consider the problem of real-time video-based facial\nemotion analytics, namely, facial expression recognition, prediction of valence\nand arousal and detection of action unit points. We propose the novel\nframe-level emotion recognition algorithm by extracting facial features with\nthe single EfficientNet model pre-trained on AffectNet. As a result, our\napproach may be implemented even for video analytics on mobile devices.\nExperimental results for the large scale Aff-Wild2 database from the third\nAffective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our\nsimple model is significantly better when compared to the VggFace baseline. In\nparticular, our method is characterized by 0.15-0.2 higher performance measures\nfor validation sets in uni-task Expression Classification, Valence-Arousal\nEstimation and Expression Classification. Due to simplicity, our approach may\nbe considered as a new baseline for all four sub-challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey V. Savchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BCOT: A Markerless High-Precision 3D Object Tracking Benchmark. (arXiv:2203.13437v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13437","description":"<p>Template-based 3D object tracking still lacks a high-precision benchmark of\nreal scenes due to the difficulty of annotating the accurate 3D poses of real\nmoving video objects without using markers. In this paper, we present a\nmulti-view approach to estimate the accurate 3D poses of real moving objects,\nand then use binocular data to construct a new benchmark for monocular\ntextureless 3D object tracking. The proposed method requires no markers, and\nthe cameras only need to be synchronous, relatively fixed as cross-view and\ncalibrated. Based on our object-centered model, we jointly optimize the object\npose by minimizing shape re-projection constraints in all views, which greatly\nimproves the accuracy compared with the single-view approach, and is even more\naccurate than the depth-based method. Our new benchmark dataset contains 20\ntextureless objects, 22 scenes, 404 video sequences and 126K images captured in\nreal scenes. The annotation error is guaranteed to be less than 2mm, according\nto both theoretical analysis and validation experiments. We re-evaluate the\nstate-of-the-art 3D object tracking methods with our dataset, reporting their\nperformance ranking in real scenes. Our BCOT benchmark and code can be found at\nhttps://ar3dv.github.io/BCOT-Benchmark/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shiqiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Te Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jason Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xueying Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Microstructure Surface Reconstruction from SEM Images: An Alternative to Digital Image Correlation (DIC). (arXiv:2203.13438v1 [cs.GR])","link":"http://arxiv.org/abs/2203.13438","description":"<p>We reconstruct a 3D model of the surface of a material undergoing fatigue\ntesting and experiencing cracking. Specifically we reconstruct the surface\ndepth (out of plane intrusions and extrusions) and lateral (in-plane) motion\nfrom multiple views of the sample at the end of the experiment, combined with a\nreverse optical flow propagation backwards in time that utilizes interim single\nview images. These measurements can be mapped to a material strain tensor which\nhelps to understand material life and predict failure. This approach offers an\nalternative to the commonly used Digital Image Correlation (DIC) technique\nwhich relies on tracking a speckle pattern applied to the material surface. DIC\nonly produces in-plane (2D) measurements whereas our approach is 3D and\nnon-invasive (requires no pattern being applied to the material).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+El_Awady_K/0/1/0/all/0/1\">Khalid El-Awady</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D GAN Inversion for Controllable Portrait Image Animation. (arXiv:2203.13441v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13441","description":"<p>Millions of images of human faces are captured every single day; but these\nphotographs portray the likeness of an individual with a fixed pose,\nexpression, and appearance. Portrait image animation enables the post-capture\nadjustment of these attributes from a single image while maintaining a\nphotorealistic reconstruction of the subject's likeness or identity. Still,\ncurrent methods for portrait image animation are typically based on 2D warping\noperations or manipulations of a 2D generative adversarial network (GAN) and\nlack explicit mechanisms to enforce multi-view consistency. Thus these methods\nmay significantly alter the identity of the subject, especially when the\nviewpoint relative to the camera is changed. In this work, we leverage newly\ndeveloped 3D GANs, which allow explicit control over the pose of the image\nsubject with multi-view consistency. We propose a supervision strategy to\nflexibly manipulate expressions with 3D morphable models, and we show that the\nproposed method also supports editing appearance attributes, such as age or\nhairstyle, by interpolating within the latent space of the GAN. The proposed\ntechnique for portrait image animation outperforms previous methods in terms of\nimage quality, identity preservation, and pose transfer while also supporting\nattribute editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Connor Z. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1\">David B. Lindell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1\">Eric R. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDAN: Multi-level Dependent Attention Network for Visual Emotion Analysis. (arXiv:2203.13443v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13443","description":"<p>Visual Emotion Analysis (VEA) is attracting increasing attention. One of the\nbiggest challenges of VEA is to bridge the affective gap between visual clues\nin a picture and the emotion expressed by the picture. As the granularity of\nemotions increases, the affective gap increases as well. Existing deep\napproaches try to bridge the gap by directly learning discrimination among\nemotions globally in one shot without considering the hierarchical relationship\namong emotions at different affective levels and the affective level of\nemotions to be classified. In this paper, we present the Multi-level Dependent\nAttention Network (MDAN) with two branches, to leverage the emotion hierarchy\nand the correlation between different affective levels and semantic levels. The\nbottom-up branch directly learns emotions at the highest affective level and\nstrictly follows the emotion hierarchy while predicting emotions at lower\naffective levels. In contrast, the top-down branch attempt to disentangle the\naffective gap by one-to-one mapping between semantic levels and affective\nlevels, namely, Affective Semantic Mapping. At each semantic level, a local\nclassifier learns discrimination among emotions at the corresponding affective\nlevel. Finally, We integrate global learning and local learning into a unified\ndeep framework and optimize the network simultaneously. Moreover, to properly\nextract and leverage channel dependencies and spatial attention while\ndisentangling the affective gap, we carefully designed two attention modules:\nthe Multi-head Cross Channel Attention module and the Level-dependent Class\nActivation Map module. Finally, the proposed deep framework obtains new\nstate-of-the-art performance on six VEA benchmarks, where it outperforms\nexisting state-of-the-art methods by a large margin, e.g., +3.85% on the WEBEmo\ndataset at 25 classes classification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_S/0/1/0/all/0/1\">Simon Lui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Compression with Structured Pruning and Low Rank Approximation. (arXiv:2203.13444v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13444","description":"<p>Transformer architecture has gained popularity due to its ability to scale\nwith large dataset. Consequently, there is a need to reduce the model size and\nlatency, especially for on-device deployment. We focus on vision transformer\nproposed for image recognition task (Dosovitskiy et al., 2021), and explore the\napplication of different compression techniques such as low rank approximation\nand pruning for this purpose. Specifically, we investigate a structured pruning\nmethod proposed recently in Zhu et al. (2021) and find that mostly feedforward\nblocks are pruned with this approach, that too, with severe degradation in\naccuracy. We propose a hybrid compression approach to mitigate this where we\ncompress the attention blocks using low rank approximation and use the\npreviously mentioned pruning with a lower rate for feedforward blocks in each\ntransformer layer. Our technique results in 50% compression with 14% relative\nincrease in classification error whereas we obtain 44% compression with 20%\nrelative increase in error when only pruning is applied. We propose further\nenhancements to bridge the accuracy gap but leave it as a future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankur Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models. (arXiv:2203.13452v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13452","description":"<p>Photorealistic style transfer entails transferring the style of a reference\nimage to another image so the result seems like a plausible photo. Our work is\ninspired by the observation that existing models are slow due to their large\nsizes. We introduce PCA-based knowledge distillation to distill lightweight\nmodels and show it is motivated by theory. To our knowledge, this is the first\nknowledge distillation method for photorealistic style transfer. Our\nexperiments demonstrate its versatility for use with different backbone\narchitectures, VGG and MobileNet, across six image resolutions. Compared to\nexisting models, our top-performing model runs at speeds 5-20x faster using at\nmost 1\\% of the parameters. Additionally, our distilled models achieve a better\nbalance between stylization strength and content preservation than existing\nmodels. To support reproducing our method and models, we share the code at\n\\textit{https://github.com/chiutaiyin/PCA-Knowledge-Distillation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_T/0/1/0/all/0/1\">Tai-Yin Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN LEGO: Disassembling and Assembling Convolutional Neural Network. (arXiv:2203.13453v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13453","description":"<p>Convolutional Neural Network (CNN), which mimics human visual perception\nmechanism, has been successfully used in many computer vision areas. Some\npsychophysical studies show that the visual perception mechanism synchronously\nprocesses the form, color, movement, depth, etc., in the initial stage [7,20]\nand then integrates all information for final recognition [38]. What's more,\nthe human visual system [20] contains different subdivisions or different\ntasks. Inspired by the above visual perception mechanism, we investigate a new\ntask, termed as Model Disassembling and Assembling (MDA-Task), which can\ndisassemble the deep models into independent parts and assemble those parts\ninto a new deep model without performance cost like playing LEGO toys. To this\nend, we propose a feature route attribution technique (FRAT) for disassembling\nCNN classifiers in this paper. In FRAT, the positive derivatives of predicted\nclass probability w.r.t. the feature maps are adopted to locate the critical\nfeatures in each layer. Then, relevance analysis between the critical features\nand preceding/subsequent parameter layers is adopted to bridge the route\nbetween two adjacent parameter layers. In the assembling phase, class-wise\ncomponents of each layer are assembled into a new deep model for a specific\ntask. Extensive experiments demonstrate that the assembled CNN classifier can\nachieve close accuracy with the original classifier without any fine-tune, and\nexcess original performance with one-epoch fine-tune. What's more, we also\nconduct massive experiments to verify the broad application of MDA-Task on\nmodel decision route visualization, model compression, knowledge distillation,\ntransfer learning, incremental learning, and so on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiacong Hu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a> (1) ((1) Zhejiang University, (2) Zhejiang Lab, (3) Zhejiang University Of Technology)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training. (arXiv:2203.13455v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13455","description":"<p>Adversarial Training (AT) is known as an effective approach to enhance the\nrobustness of deep neural networks. Recently researchers notice that robust\nmodels with AT have good generative ability and can synthesize realistic\nimages, while the reason behind it is yet under-explored. In this paper, we\ndemystify this phenomenon by developing a unified probabilistic framework,\ncalled Contrastive Energy-based Models (CEM). On the one hand, we provide the\nfirst probabilistic characterization of AT through a unified understanding of\nrobustness and generative ability. On the other hand, our unified framework can\nbe extended to the unsupervised scenario, which interprets unsupervised\ncontrastive learning as an important sampling of CEM. Based on these, we\npropose a principled method to develop adversarial learning and sampling\nmethods. Experiments show that the sampling methods derived from our framework\nimprove the sample quality in both supervised and unsupervised learning.\nNotably, our unsupervised adversarial sampling method achieves an Inception\nscore of 9.61 on CIFAR-10, which is superior to previous energy-based models\nand comparable to state-of-the-art generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiansheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. (arXiv:2203.13457v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13457","description":"<p>Recently, contrastive learning has risen to be a promising approach for\nlarge-scale self-supervised learning. However, theoretical understanding of how\nit works is still unclear. In this paper, we propose a new guarantee on the\ndownstream performance without resorting to the conditional independence\nassumption that is widely adopted in previous work but hardly holds in\npractice. Our new theory hinges on the insight that the support of different\nintra-class samples will become more overlapped under aggressive data\naugmentations, thus simply aligning the positive samples (augmented views of\nthe same sample) could make contrastive learning cluster intra-class samples\ntogether. Based on this augmentation overlap perspective, theoretically, we\nobtain asymptotically closed bounds for downstream performance under weaker\nassumptions, and empirically, we propose an unsupervised model selection metric\nARC that aligns well with downstream accuracy. Our theory suggests an\nalternative understanding of contrastive learning: the role of aligning\npositive samples is more like a surrogate task than an ultimate goal, and the\noverlapped augmented views (i.e., the chaos) create a ladder for contrastive\nlearning to gradually learn class-separated representations. The code for\ncomputing ARC is available at https://github.com/zhangq327/ARC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiansheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PANDORA: Polarization-Aided Neural Decomposition Of Radiance. (arXiv:2203.13458v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13458","description":"<p>Reconstructing an object's geometry and appearance from multiple images, also\nknown as inverse rendering, is a fundamental problem in computer graphics and\nvision. Inverse rendering is inherently ill-posed because the captured image is\nan intricate function of unknown lighting conditions, material properties and\nscene geometry. Recent progress in representing scene properties as\ncoordinate-based neural networks have facilitated neural inverse rendering\nresulting in impressive geometry reconstruction and novel-view synthesis. Our\nkey insight is that polarization is a useful cue for neural inverse rendering\nas polarization strongly depends on surface normals and is distinct for diffuse\nand specular reflectance. With the advent of commodity, on-chip, polarization\nsensors, capturing polarization has become practical. Thus, we propose PANDORA,\na polarimetric inverse rendering approach based on implicit neural\nrepresentations. From multi-view polarization images of an object, PANDORA\njointly extracts the object's 3D geometry, separates the outgoing radiance into\ndiffuse and specular and estimates the illumination incident on the object. We\nshow that PANDORA outperforms state-of-the-art radiance decomposition\ntechniques. PANDORA outputs clean surface reconstructions free from texture\nartefacts, models strong specularities accurately and estimates illumination\nunder practical unstructured scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1\">Akshat Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yongyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeraraghavan_A/0/1/0/all/0/1\">Ashok Veeraraghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised and Deep learning Frameworks for Video Classification and Key-frame Identification. (arXiv:2203.13459v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13459","description":"<p>Automating video-based data and machine learning pipelines poses several\nchallenges including metadata generation for efficient storage and retrieval\nand isolation of key-frames for scene understanding tasks. In this work, we\npresent two semi-supervised approaches that automate this process of manual\nframe sifting in video streams by automatically classifying scenes for content\nand filtering frames for fine-tuning scene understanding tasks. The first\nrule-based method starts from a pre-trained object detector and it assigns\nscene type, uncertainty and lighting categories to each frame based on\nprobability distributions of foreground objects. Next, frames with the highest\nuncertainty and structural dissimilarity are isolated as key-frames. The second\nmethod relies on the simCLR model for frame encoding followed by\nlabel-spreading from 20% of frame samples to label the remaining frames for\nscene and lighting categories. Also, clustering the video frames in the encoded\nfeature space further isolates key-frames at cluster boundaries. The proposed\nmethods achieve 64-93% accuracy for automated scene categorization for outdoor\nimage videos from public domain datasets of JAAD and KITTI. Also, less than 10%\nof all input frames can be filtered as key-frames that can then be sent for\nannotation and fine tuning of machine vision algorithms. Thus, the proposed\nframework can be scaled to additional video data streams for automated training\nof perception-driven systems with minimal training images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sohini Roychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretation of Chest x-rays affected by bullets using deep transfer learning. (arXiv:2203.13461v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13461","description":"<p>The potential of deep learning, especially in medical imaging, initiated\nastonishing results and improved the methodologies after every passing day.\nDeep learning in radiology provides the opportunity to classify, detect and\nsegment different diseases automatically. In the proposed study, we worked on a\nnon-trivial aspect of medical imaging where we classified and localized the\nX-Rays affected by bullets. We tested Images on different classification and\nlocalization models to get considerable accuracy. The replicated data set used\nin the study was replicated on different images of chest X-Rays. The proposed\nmodel worked not only on chest radiographs but other body organs X-rays like\nleg, abdomen, head, even the training dataset based on chest radiographs.\nCustom models have been used for classification and localization purposes after\ntuning parameters. Finally, the results of our findings manifested using\ndifferent frameworks. This might assist the research enlightening towards this\nfield. To the best of our knowledge, this is the first study on the detection\nand classification of radiographs affected by bullets using deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shaheer Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farooq_A/0/1/0/all/0/1\">Azib Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1\">Israr Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Gulraiz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzaq_A/0/1/0/all/0/1\">Abdul Razzaq</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAD: Co-Adapting Discriminative Features for Improved Few-Shot Classification. (arXiv:2203.13465v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13465","description":"<p>Few-shot classification is a challenging problem that aims to learn a model\nthat can adapt to unseen classes given a few labeled samples. Recent approaches\npre-train a feature extractor, and then fine-tune for episodic meta-learning.\nOther methods leverage spatial features to learn pixel-level correspondence\nwhile jointly training a classifier. However, results using such approaches\nshow marginal improvements. In this paper, inspired by the transformer style\nself-attention mechanism, we propose a strategy to cross-attend and re-weight\ndiscriminative features for few-shot classification. Given a base\nrepresentation of support and query images after global pooling, we introduce a\nsingle shared module that projects features and cross-attends in two aspects:\n(i) query to support, and (ii) support to query. The module computes attention\nscores between features to produce an attention pooled representation of\nfeatures in the same class that is later added to the original representation\nfollowed by a projection head. This effectively re-weights features in both\naspects (i &amp; ii) to produce features that better facilitate improved\nmetric-based meta-learning. Extensive experiments on public benchmarks show our\napproach outperforms state-of-the-art methods by 3%~5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chikontwe_P/0/1/0/all/0/1\">Philip Chikontwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soopil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sang Hyun Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RD-Optimized Trit-Plane Coding of Deep Compressed Image Latent Tensors. (arXiv:2203.13467v1 [eess.IV])","link":"http://arxiv.org/abs/2203.13467","description":"<p>DPICT is the first learning-based image codec supporting fine granular\nscalability. In this paper, we describe how to implement two key components of\nDPICT efficiently: trit-plane slicing and RD-prioritized transmission. In\nDPICT, we transform an image into a latent tensor, represent the tensor in\nternary digits (trits), and encode the trits in the decreasing order of\nsignificance. For entropy encoding, we should compute the probability of each\ntrit, which demands high time complexity in both the encoder and the decoder.\nTo reduce the complexity, we develop a parallel computing scheme for the\nprobabilities and describe it in detail with pseudo-codes. Moreover, in this\npaper, we compare the trit-plane slicing in DPICT with the alternative\nbit-plane slicing. Experimental results show that the time complexity is\nreduced significantly by the parallel computing and that the trit-plane slicing\nprovides better rate-distortion performances than the bit-plane slicing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeon_S/0/1/0/all/0/1\">Seungmin Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jae-Han Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Style Transfer: All is Your Palette. (arXiv:2203.13470v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13470","description":"<p>Neural style transfer (NST) can create impressive artworks by transferring\nreference style to content image. Current image-to-image NST methods are short\nof fine-grained controls, which are often demanded by artistic editing. To\nmitigate this limitation, we propose a drawing-like interactive style transfer\n(IST) method, by which users can interactively create a harmonious-style image.\nOur IST method can serve as a brush, dip style from anywhere, and then paint to\nany region of the target content image. To determine the action scope, we\nformulate a fluid simulation algorithm, which takes styles as pigments around\nthe position of brush interaction, and diffusion in style or content images\naccording to the similarity maps. Our IST method expands the creative dimension\nof NST. By dipping and painting, even employing one style image can produce\nthousands of eye-catching works. The demo video is available in supplementary\nfiles or in <a href=\"http://mmcheng.net/ist.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kang-Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Probability Sampling Network for Stochastic Human Trajectory Prediction. (arXiv:2203.13471v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13471","description":"<p>Capturing multimodal natures is essential for stochastic pedestrian\ntrajectory prediction, to infer a finite set of future trajectories. The\ninferred trajectories are based on observation paths and the latent vectors of\npotential decisions of pedestrians in the inference step. However, stochastic\napproaches provide varying results for the same data and parameter settings,\ndue to the random sampling of the latent vector. In this paper, we analyze the\nproblem by reconstructing and comparing probabilistic distributions from\nprediction samples and socially-acceptable paths, respectively. Through this\nanalysis, we observe that the inferences of all stochastic models are biased\ntoward the random sampling, and fail to generate a set of realistic paths from\nfinite samples. The problem cannot be resolved unless an infinite number of\nsamples is available, which is infeasible in practice. We introduce that the\nQuasi-Monte Carlo (QMC) method, ensuring uniform coverage on the sampling\nspace, as an alternative to the conventional random sampling. With the same\nfinite number of samples, the QMC improves all the multimodal prediction\nresults. We take an additional step ahead by incorporating a learnable sampling\nnetwork into the existing networks for trajectory prediction. For this purpose,\nwe propose the Non-Probability Sampling Network (NPSN), a very small network\n(~5K parameters) that generates purposive sample sequences using the past paths\nof pedestrians and their social interactions. Extensive experiments confirm\nthat NPSN can significantly improve both the prediction accuracy (up to 60%)\nand reliability of the public pedestrian trajectory prediction benchmark. Code\nis publicly available at https://github.com/inhwanbae/NPSN .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_I/0/1/0/all/0/1\">Inhwan Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jin-Hwi Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hae-Gon Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Expression Recognition with Swin Transformer. (arXiv:2203.13472v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13472","description":"<p>The task of recognizing human facial expressions plays a vital role in\nvarious human-related systems, including health care and medical fields. With\nthe recent success of deep learning and the accessibility of a large amount of\nannotated data, facial expression recognition research has been mature enough\nto be utilized in real-world scenarios with audio-visual datasets. In this\npaper, we introduce Swin transformer-based facial expression approach for an\nin-the-wild audio-visual dataset of the Aff-Wild2 Expression dataset.\nSpecifically, we employ a three-stream network (i.e., Visual stream, Temporal\nstream, and Audio stream) for the audio-visual videos to fuse the multi-modal\ninformation into facial expression recognition. Experimental results on the\nAff-Wild2 dataset show the effectiveness of our proposed multi-modal\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jun-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Namho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_C/0/1/0/all/0/1\">Chee Sun Won</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Adversarial Transferability with Spatial Momentum. (arXiv:2203.13479v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13479","description":"<p>Deep Neural Networks (DNN) are vulnerable to adversarial examples. Although\nmany adversarial attack methods achieve satisfactory attack success rates under\nthe white-box setting, they usually show poor transferability when attacking\nother DNN models. Momentum-based attack (MI-FGSM) is one effective method to\nimprove transferability. It integrates the momentum term into the iterative\nprocess, which can stabilize the update directions by adding the gradients'\ntemporal correlation for each pixel. We argue that only this temporal momentum\nis not enough, the gradients from the spatial domain within an image, i.e.\ngradients from the context pixels centered on the target pixel are also\nimportant to the stabilization. For that, in this paper, we propose a novel\nmethod named Spatial Momentum Iterative FGSM Attack (SMI-FGSM), which\nintroduces the mechanism of momentum accumulation from temporal domain to\nspatial domain by considering the context gradient information from different\nregions within the image. SMI-FGSM is then integrated with MI-FGSM to\nsimultaneously stabilize the gradients' update direction from both the temporal\nand spatial domain. The final method is called SM$^2$I-FGSM. Extensive\nexperiments are conducted on the ImageNet dataset and results show that\nSM$^2$I-FGSM indeed further enhances the transferability. It achieves the best\ntransferability success rate for multiple mainstream undefended and defended\nmodels, which outperforms the state-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huanqian Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarization Multiplexed Diffractive Computing: All-Optical Implementation of a Group of Linear Transformations Through a Polarization-Encoded Diffractive Network. (arXiv:2203.13482v1 [physics.optics])","link":"http://arxiv.org/abs/2203.13482","description":"<p>Research on optical computing has recently attracted significant attention\ndue to the transformative advances in machine learning. Among different\napproaches, diffractive optical networks composed of spatially-engineered\ntransmissive surfaces have been demonstrated for all-optical statistical\ninference and performing arbitrary linear transformations using passive,\nfree-space optical layers. Here, we introduce a polarization multiplexed\ndiffractive processor to all-optically perform multiple, arbitrarily-selected\nlinear transformations through a single diffractive network trained using deep\nlearning. In this framework, an array of pre-selected linear polarizers is\npositioned between trainable transmissive diffractive materials that are\nisotropic, and different target linear transformations (complex-valued) are\nuniquely assigned to different combinations of input/output polarization\nstates. The transmission layers of this polarization multiplexed diffractive\nnetwork are trained and optimized via deep learning and error-backpropagation\nby using thousands of examples of the input/output fields corresponding to each\none of the complex-valued linear transformations assigned to different\ninput/output polarization combinations. Our results and analysis reveal that a\nsingle diffractive network can successfully approximate and all-optically\nimplement a group of arbitrarily-selected target transformations with a\nnegligible error when the number of trainable diffractive features/neurons (N)\napproaches N_p x N_i x N_o, where N_i and N_o represent the number of pixels at\nthe input and output fields-of-view, respectively, and N_p refers to the number\nof unique linear transformations assigned to different input/output\npolarization combinations. This polarization-multiplexed all-optical\ndiffractive processor can find various applications in optical computing and\npolarization-based machine vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1\">Jingxi Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hung_Y/0/1/0/all/0/1\">Yi-Chun Hung</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kulce_O/0/1/0/all/0/1\">Onur Kulce</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mengu_D/0/1/0/all/0/1\">Deniz Mengu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compare learning: bi-attention network for few-shot learning. (arXiv:2203.13487v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13487","description":"<p>Learning with few labeled data is a key challenge for visual recognition, as\ndeep neural networks tend to overfit using a few samples only. One of the\nFew-shot learning methods called metric learning addresses this challenge by\nfirst learning a deep distance metric to determine whether a pair of images\nbelong to the same category, then applying the trained metric to instances from\nother test set with limited labels. This method makes the most of the few\nsamples and limits the overfitting effectively. However, extant metric networks\nusually employ Linear classifiers or Convolutional neural networks (CNN) that\nare not precise enough to globally capture the subtle differences between\nvectors. In this paper, we propose a novel approach named Bi-attention network\nto compare the instances, which can measure the similarity between embeddings\nof instances precisely, globally and efficiently. We verify the effectiveness\nof our model on two benchmarks. Experiments show that our approach achieved\nimproved accuracy and convergence speed over baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Li Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Meng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1\">Weigao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive learning of Class-agnostic Activation Map for Weakly Supervised Object Localization and Semantic Segmentation. (arXiv:2203.13505v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13505","description":"<p>While class activation map (CAM) generated by image classification network\nhas been widely used for weakly supervised object localization (WSOL) and\nsemantic segmentation (WSSS), such classifiers usually focus on discriminative\nobject regions. In this paper, we propose Contrastive learning for\nClass-agnostic Activation Map (C$^2$AM) generation only using unlabeled image\ndata, without the involvement of image-level supervision. The core idea comes\nfrom the observation that i) semantic information of foreground objects usually\ndiffers from their backgrounds; ii) foreground objects with similar appearance\nor background with similar color/texture have similar representations in the\nfeature space. We form the positive and negative pairs based on the above\nrelations and force the network to disentangle foreground and background with a\nclass-agnostic activation map using a novel contrastive loss. As the network is\nguided to discriminate cross-image foreground-background, the class-agnostic\nactivation maps learned by our approach generate more complete object regions.\nWe successfully extracted from C$^2$AM class-agnostic object bounding boxes for\nobject localization and background cues to refine CAM generated by\nclassification network for semantic segmentation. Extensive experiments on\nCUB-200-2011, ImageNet-1K, and PASCAL VOC2012 datasets show that both WSOL and\nWSSS can benefit from the proposed C$^2$AM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jianfeng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xianxu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaodong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of the Production Strategy of Mask Types in the COVID-19 Environment. (arXiv:2203.13506v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13506","description":"<p>Since the outbreak of the COVID-19 in December 2019, medical protective\nequipment such as disposable medical masks and KN95 masks have become essential\nresources for the public. Enterprises in all sectors of society have also\ntransformed the production of medical masks. After the outbreak, how to choose\nthe right time to produce medical protective masks, and what type of medical\nmasks to produce will play a positive role in preventing and controlling the\nepidemic in a short time. In this regard, the evolutionary game competition\nanalysis will be conducted through the relevant data of disposable medical\nmasks and KN95 masks to determine the appropriate nodes for the production of\ncorresponding mask types. After the research and analysis of the production\nstrategy of mask types, it has a positive effect on how to guide the resumption\nof work and production.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiangri Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hongbin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Pre-training Based on Graph Attention Network for Document Understanding. (arXiv:2203.13530v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13530","description":"<p>Document intelligence as a relatively new research topic supports many\nbusiness applications. Its main task is to automatically read, understand, and\nanalyze documents. However, due to the diversity of formats (invoices, reports,\nforms, etc.) and layouts in documents, it is difficult to make machines\nunderstand documents. In this paper, we present the GraphDoc, a multimodal\ngraph attention-based model for various document understanding tasks. GraphDoc\nis pre-trained in a multimodal framework by utilizing text, layout, and image\ninformation simultaneously. In a document, a text block relies heavily on its\nsurrounding contexts, so we inject the graph structure into the attention\nmechanism to form a graph attention layer so that each input node can only\nattend to its neighborhoods. The input nodes of each graph attention layer are\ncomposed of textual, visual, and positional features from semantically\nmeaningful regions in a document image. We do the multimodal feature fusion of\neach node by the gate fusion layer. The contextualization between each node is\nmodeled by the graph attention layer. GraphDoc learns a generic representation\nfrom only 320k unlabeled documents via the Masked Sentence Modeling task.\nExtensive experimental results on the publicly available datasets show that\nGraphDoc achieves state-of-the-art performance, which demonstrates the\neffectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiefeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Licheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianshu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Performance Transformer Tracking. (arXiv:2203.13533v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13533","description":"<p>Correlation has a critical role in the tracking field, especially in recent\npopular Siamese-based trackers. The correlation operation is a simple fusion\nmanner to consider the similarity between the template and the search region.\nHowever, the correlation operation is a local linear matching process, losing\nsemantic information and falling into local optimum easily, which may be the\nbottleneck of designing high-accuracy tracking algorithms. In this work, to\ndetermine whether a better feature fusion method exists than correlation, a\nnovel attention-based feature fusion network, inspired by Transformer, is\npresented. This network effectively combines the template and the search region\nfeatures using attention. Specifically, the proposed method includes an\nego-context augment module based on self-attention and a cross-feature augment\nmodule based on cross-attention. First, we present a Transformer tracking\n(named TransT) method based on the Siamese-like feature extraction backbone,\nthe designed attention-based fusion mechanism, and the classification and\nregression head. Based on the TransT baseline, we further design a segmentation\nbranch to generate an accurate mask. Finally, we propose a stronger version of\nTransT by extending TransT with a multi-template design and an IoU prediction\nhead, named TransT-M. Experiments show that our TransT and TransT-M methods\nachieve promising results on seven popular datasets. Code and models are\navailable at https://github.com/chenxin-dlut/TransT-M.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance. (arXiv:2203.13535v1 [cs.MM])","link":"http://arxiv.org/abs/2203.13535","description":"<p>Recent years have witnessed the success of deep learning on the visual sound\nseparation task. However, existing works follow similar settings where the\ntraining and testing datasets share the same musical instrument categories,\nwhich to some extent limits the versatility of this task. In this work, we\nfocus on a more general and challenging scenario, namely the separation of\nunknown musical instruments, where the categories in training and testing\nphases have no overlap with each other. To tackle this new setting, we propose\nthe Separation-with-Consistency (SeCo) framework, which can accomplish the\nseparation on unknown categories by exploiting the consistency constraints.\nFurthermore, to capture richer characteristics of the novel melodies, we devise\nan online matching strategy, which can bring stable enhancements with no cost\nof extra parameters. Experiments demonstrate that our SeCo framework exhibits\nstrong adaptation ability on the novel musical categories and outperforms the\nbaseline methods by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinchi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongzhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Visual Tracking via Hierarchical Cross-Attention Transformer. (arXiv:2203.13537v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13537","description":"<p>In recent years, target tracking has made great progress in accuracy. This\ndevelopment is mainly attributed to powerful networks (such as transformers)\nand additional modules (such as online update and refinement modules). However,\nless attention has been paid to tracking speed. Most state-of-the-art trackers\nare satisfied with the real-time speed on powerful GPUs. However, practical\napplications necessitate higher requirements for tracking speed, especially\nwhen edge platforms with limited resources are used. In this work, we present\nan efficient tracking method via a hierarchical cross-attention transformer\nnamed HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on\nthe edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our\nHCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,\nUAV123, and VOT2020. Code and models are available at\nhttps://github.com/chenxin-dlut/HCAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable Butterfly: A Highly Structured and Sparse Linear Transform. (arXiv:2203.13556v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13556","description":"<p>We introduce a new kind of linear transform named Deformable Butterfly\n(DeBut) that generalizes the conventional butterfly matrices and can be adapted\nto various input-output dimensions. It inherits the fine-to-coarse-grained\nlearnable hierarchy of traditional butterflies and when deployed to neural\nnetworks, the prominent structures and sparsity in a DeBut layer constitutes a\nnew way for network compression. We apply DeBut as a drop-in replacement of\nstandard fully connected and convolutional layers, and demonstrate its\nsuperiority in homogenizing a neural network and rendering it favorable\nproperties such as light weight and low inference complexity, without\ncompromising accuracy. The natural complexity-accuracy tradeoff arising from\nthe myriad deformations of a DeBut layer also opens up new rooms for analytical\nand practical research. The codes and Appendix are publicly available at:\nhttps://github.com/ruilin0212/DeBut.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_J/0/1/0/all/0/1\">Jie Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_K/0/1/0/all/0/1\">King Hung Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesi_G/0/1/0/all/0/1\">Graziano Chesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Networks with Divisive normalization for image segmentation with application in cityscapes dataset. (arXiv:2203.13558v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13558","description":"<p>One of the key problems in computer vision is adaptation: models are too\nrigid to follow the variability of the inputs. The canonical computation that\nexplains adaptation in sensory neuroscience is divisive normalization, and it\nhas appealing effects on image manifolds. In this work we show that including\ndivisive normalization in current deep networks makes them more invariant to\nnon-informative changes in the images. In particular, we focus on U-Net\narchitectures for image segmentation. Experiments show that the inclusion of\ndivisive normalization in the U-Net architecture leads to better segmentation\nresults with respect to conventional U-Net. The gain increases steadily when\ndealing with images acquired in bad weather conditions. In addition to the\nresults on the Cityscapes and Foggy Cityscapes datasets, we explain these\nadvantages through visualization of the responses: the equalization induced by\nthe divisive normalization leads to more invariant features to local changes in\ncontrast and illumination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Camara_P/0/1/0/all/0/1\">Pablo Hern&#xe1;ndez-C&#xe1;mara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1\">Valero Laparra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1\">Jes&#xfa;s Malo</a> (Image Processing Lab., Universitat de Val&#xe8;ncia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Visual Navigation Perspective for Category-Level Object Pose Estimation. (arXiv:2203.13572v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13572","description":"<p>This paper studies category-level object pose estimation based on a single\nmonocular image. Recent advances in pose-aware generative models have paved the\nway for addressing this challenging task using analysis-by-synthesis. The idea\nis to sequentially update a set of latent variables, e.g., pose, shape, and\nappearance, of the generative model until the generated image best agrees with\nthe observation. However, convergence and efficiency are two challenges of this\ninference procedure. In this paper, we take a deeper look at the inference of\nanalysis-by-synthesis from the perspective of visual navigation, and\ninvestigate what is a good navigation policy for this specific task. We\nevaluate three different strategies, including gradient descent, reinforcement\nlearning and imitation learning, via thorough comparisons in terms of\nconvergence, robustness and efficiency. Moreover, we show that a simple hybrid\napproach leads to an effective and efficient solution. We further compare these\nstrategies to state-of-the-art methods, and demonstrate superior performance on\nsynthetic and real-world datasets leveraging off-the-shelf pose-aware\ngenerative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangxun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Test-Time Domain Adaptation. (arXiv:2203.13591v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13591","description":"<p>Test-time domain adaptation aims to adapt a source pre-trained model to a\ntarget domain without using any source data. Existing works mainly consider the\ncase where the target domain is static. However, real-world machine perception\nsystems are running in non-stationary and continually changing environments\nwhere the target domain distribution can change over time. Existing methods,\nwhich are mostly based on self-training and entropy regularization, can suffer\nfrom these non-stationary environments. Due to the distribution shift over time\nin the target domain, pseudo-labels become unreliable. The noisy pseudo-labels\ncan further lead to error accumulation and catastrophic forgetting. To tackle\nthese issues, we propose a continual test-time adaptation approach~(CoTTA)\nwhich comprises two parts. Firstly, we propose to reduce the error accumulation\nby using weight-averaged and augmentation-averaged predictions which are often\nmore accurate. On the other hand, to avoid catastrophic forgetting, we propose\nto stochastically restore a small part of the neurons to the source pre-trained\nweights during each iteration to help preserve source knowledge in the\nlong-term. The proposed method enables the long-term adaptation for all\nparameters in the network. CoTTA is easy to implement and can be readily\nincorporated in off-the-shelf pre-trained models. We demonstrate the\neffectiveness of our approach on four classification tasks and a segmentation\ntask for continual test-time adaptation, on which we outperform existing\nmethods. Our code is available at \\url{https://qin.ee/cotta}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1\">Olga Fink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Hybrid Image Retargeting. (arXiv:2203.13595v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13595","description":"<p>Image retargeting changes the aspect ratio of images while aiming to preserve\ncontent and minimise noticeable distortion. Fast and high-quality methods are\nparticularly relevant at present, due to the large variety of image and display\naspect ratios. We propose a retargeting method that quantifies and limits\nwarping distortions with the use of content-aware cropping. The pipeline of the\nproposed approach consists of the following steps. First, an importance map of\na source image is generated using deep semantic segmentation and saliency\ndetection models. Then, a preliminary warping mesh is computed using axis\naligned deformations, enhanced with the use of a distortion measure to ensure\nlow warping deformations. Finally, the retargeted image is produced using a\ncontent-aware cropping algorithm. In order to evaluate our method, we perform a\nuser study based on the RetargetMe benchmark. Experimental analyses show that\nour method outperforms recent approaches, while running in a fraction of their\nexecution time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valdez_Balderas_D/0/1/0/all/0/1\">Daniel Valdez-Balderas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muraveynyk_O/0/1/0/all/0/1\">Oleg Muraveynyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_T/0/1/0/all/0/1\">Timothy Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints. (arXiv:2203.13601v1 [cs.DB])","link":"http://arxiv.org/abs/2203.13601","description":"<p>As research interest surges, vector similarity search is applied in multiple\nfields, including data mining, computer vision, and information retrieval.\n{Given a set of objects (e.g., a set of images) and a query object, we can\neasily transform each object into a feature vector and apply the vector\nsimilarity search to retrieve the most similar objects. However, the original\nvector similarity search cannot well support \\textit{hybrid queries}, where\nusers not only input unstructured query constraint (i.e., the feature vector of\nquery object) but also structured query constraint (i.e., the desired\nattributes of interest). Hybrid query processing aims at identifying these\nobjects with similar feature vectors to query object and satisfying the given\nattribute constraints. Recent efforts have attempted to answer a hybrid query\nby performing attribute filtering and vector similarity search separately and\nthen merging the results later, which limits efficiency and accuracy because\nthey are not purpose-built for hybrid queries.} In this paper, we propose a\nnative hybrid query (NHQ) framework based on proximity graph (PG), which\nprovides the specialized \\textit{composite index and joint pruning} modules for\nhybrid queries. We easily deploy existing various PGs on this framework to\nprocess hybrid queries efficiently. Moreover, we present two novel navigable\nPGs (NPGs) with optimized edge selection and routing strategies, which obtain\nbetter overall performance than existing PGs. After that, we deploy the\nproposed NPGs in NHQ to form two hybrid query methods, which significantly\noutperform the state-of-the-art competitors on all experimental datasets\n(10$\\times$ faster under the same \\textit{Recall}), including eight public and\none in-house real-world datasets. Our code and datasets have been released at\n\\url{https://github.com/AshenOn3/NHQ}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengzhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_L/0/1/0/all/0/1\">Lingwei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Q/0/1/0/all/0/1\">Qiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jiongkang Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rope3D: TheRoadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task. (arXiv:2203.13608v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13608","description":"<p>Concurrent perception datasets for autonomous driving are mainly limited to\nfrontal view with sensors mounted on the vehicle. None of them is designed for\nthe overlooked roadside perception tasks. On the other hand, the data captured\nfrom roadside cameras have strengths over frontal-view data, which is believed\nto facilitate a safer and more intelligent autonomous driving system. To\naccelerate the progress of roadside perception, we present the first\nhigh-diversity challenging Roadside Perception 3D dataset- Rope3D from a novel\nview. The dataset consists of 50k images and over 1.5M 3D objects in various\nscenes, which are captured under different settings including various cameras\nwith ambiguous mounting positions, camera specifications, viewpoints, and\ndifferent environmental conditions. We conduct strict 2D-3D joint annotation\nand comprehensive data analysis, as well as set up a new 3D roadside perception\nbenchmark with metrics and evaluation devkit. Furthermore, we tailor the\nexisting frontal-view monocular 3D object detection approaches and propose to\nleverage the geometry constraint to solve the inherent ambiguities caused by\nvarious sensors, viewpoints. Our dataset is available on\nhttps://thudair.baai.ac.cn/rope.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Mao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Pre-training for Temporal Action Localization Tasks. (arXiv:2203.13609v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13609","description":"<p>Unsupervised video representation learning has made remarkable achievements\nin recent years. However, most existing methods are designed and optimized for\nvideo classification. These pre-trained models can be sub-optimal for temporal\nlocalization tasks due to the inherent discrepancy between video-level\nclassification and clip-level localization. To bridge this gap, we make the\nfirst attempt to propose a self-supervised pretext task, coined as Pseudo\nAction Localization (PAL) to Unsupervisedly Pre-train feature encoders for\nTemporal Action Localization tasks (UP-TAL). Specifically, we first randomly\nselect temporal regions, each of which contains multiple clips, from one video\nas pseudo actions and then paste them onto different temporal positions of the\nother two videos. The pretext task is to align the features of pasted pseudo\naction regions from two synthetic videos and maximize the agreement between\nthem. Compared to the existing unsupervised video representation learning\napproaches, our PAL adapts better to downstream TAL tasks by introducing a\ntemporal equivariant contrastive learning paradigm in a temporally dense and\nscale-aware manner. Extensive experiments show that PAL can utilize large-scale\nunlabeled video data to significantly boost the performance of existing TAL\nmethods. Our codes and models will be made publicly available at\nhttps://github.com/zhang-can/UP-TAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Junwu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Adapt to Unseen Abnormal Activities under Weak Supervision. (arXiv:2203.13610v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13610","description":"<p>We present a meta-learning framework for weakly supervised anomaly detection\nin videos, where the detector learns to adapt to unseen types of abnormal\nactivities effectively when only video-level annotations of binary labels are\navailable. Our work is motivated by the fact that existing methods suffer from\npoor generalization to diverse unseen examples. We claim that an anomaly\ndetector equipped with a meta-learning scheme alleviates the limitation by\nleading the model to an initialization point for better optimization. We\nevaluate the performance of our framework on two challenging datasets,\nUCF-Crime and ShanghaiTech. The experimental results demonstrate that our\nalgorithm boosts the capability to localize unseen abnormal events in a weakly\nsupervised setting. Besides the technical contributions, we perform the\nannotation of missing labels in the UCF-Crime dataset and make our task\nevaluated effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaeyoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Incremental Learning for Action Recognition in Videos. (arXiv:2203.13611v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13611","description":"<p>We tackle catastrophic forgetting problem in the context of class-incremental\nlearning for video recognition, which has not been explored actively despite\nthe popularity of continual learning. Our framework addresses this challenging\ntask by introducing time-channel importance maps and exploiting the importance\nmaps for learning the representations of incoming examples via knowledge\ndistillation. We also incorporate a regularization scheme in our objective\nfunction, which encourages individual features obtained from different time\nsteps in a video to be uncorrelated and eventually improves accuracy by\nalleviating catastrophic forgetting. We evaluate the proposed approach on\nbrand-new splits of class-incremental action recognition benchmarks constructed\nupon the UCF101, HMDB51, and Something-Something V2 datasets, and demonstrate\nthe effectiveness of our algorithm in comparison to the existing continual\nlearning methods that are originally designed for image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaeyoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minsoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13612","description":"<p>Deep Neural Networks (DNNs) have been widely used in software making\ndecisions impacting people's lives. However, they have been found to exhibit\nsevere erroneous behaviors that may lead to unfortunate outcomes. Previous work\nshows that such misbehaviors often occur due to class property violations\nrather than errors on a single image. Although methods for detecting such\nerrors have been proposed, fixing them has not been studied so far. Here, we\npropose a generic method called Weighted Regularization (WR) consisting of five\nconcrete methods targeting the error-producing classes to fix the DNNs. In\nparticular, it can repair confusion error and bias error of DNN models for both\nsingle-label and multi-label image classifications. A confusion error happens\nwhen a given DNN model tends to confuse between two classes. Each method in WR\nassigns more weights at a stage of DNN retraining or inference to mitigate the\nconfusion between target pair. A bias error can be fixed similarly. We evaluate\nand compare the proposed methods along with baselines on six widely-used\ndatasets and architecture combinations. The results suggest that WR methods\nhave different trade-offs but under each setting at least one WR method can\ngreatly reduce confusion/bias errors at a very limited cost of the overall\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Ziyuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuchi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1\">Conor J.Sweeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_Roman_V/0/1/0/all/0/1\">Vicente Ordonez-Roman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Graph Convolutional Networks with Topologically Consistent Magnitude Pruning. (arXiv:2203.13616v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13616","description":"<p>Graph convolution networks (GCNs) are currently mainstream in learning with\nirregular data. These models rely on message passing and attention mechanisms\nthat capture context and node-to-node relationships. With multi-head attention,\nGCNs become highly accurate but oversized, and their deployment on cheap\ndevices requires their pruning. However, pruning at high regimes usually leads\nto topologically inconsistent networks with weak generalization. In this paper,\nwe devise a novel method for lightweight GCN design. Our proposed approach\nparses and selects subnetworks with the highest magnitudes while guaranteeing\ntheir topological consistency. The latter is obtained by selecting only\naccessible and co-accessible connections which actually contribute in the\nevaluation of the selected subnetworks. Experiments conducted on the\nchallenging FPHA dataset show the substantial gain of our topologically\nconsistent pruning method especially at very high pruning regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness. (arXiv:2203.13639v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13639","description":"<p>Neural architectures based on attention such as vision transformers are\nrevolutionizing image recognition. Their main benefit is that attention allows\nreasoning about all parts of a scene jointly. In this paper, we show how the\nglobal reasoning of (scaled) dot-product attention can be the source of a major\nvulnerability when confronted with adversarial patch attacks. We provide a\ntheoretical understanding of this vulnerability and relate it to an adversary's\nability to misdirect the attention of all queries to a single key token under\nthe control of the adversarial patch. We propose novel adversarial objectives\nfor crafting adversarial patches which target this vulnerability explicitly. We\nshow the effectiveness of the proposed patch attacks on popular image\nclassification (ViTs and DeiTs) and object detection models (DETR). We find\nthat adversarial patches occupying 0.5% of the input can lead to robust\naccuracies as low as 0% for ViT on ImageNet, and reduce the mAP of DETR on MS\nCOCO to less than 3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovisotto_G/0/1/0/all/0/1\">Giulio Lovisotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1\">Nicole Finnie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_M/0/1/0/all/0/1\">Mauricio Munoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1\">Chaithanya Kumar Mummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StretchBEV: Stretching Future Instance Prediction Spatially and Temporally. (arXiv:2203.13641v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13641","description":"<p>In self-driving, predicting future in terms of location and motion of all the\nagents around the vehicle is a crucial requirement for planning. Recently, a\nnew joint formulation of perception and prediction has emerged by fusing rich\nsensory information perceived from multiple cameras into a compact bird's-eye\nview representation to perform prediction. However, the quality of future\npredictions degrades over time while extending to longer time horizons due to\nmultiple plausible predictions. In this work, we address this inherent\nuncertainty in future predictions with a stochastic temporal model. Our model\nlearns temporal dynamics in a latent space through stochastic residual updates\nat each time step. By sampling from a learned distribution at each time step,\nwe obtain more diverse future predictions that are also more accurate compared\nto previous work, especially stretching both spatially further regions in the\nscene and temporally over longer time horizons. Despite separate processing of\neach time step, our model is still efficient through decoupling of the learning\nof dynamics and the generation of future predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1\">Fatma G&#xfc;ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDsrv -- visual sharing and analysis of molecular dynamics simulations. (arXiv:2203.13658v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13658","description":"<p>Molecular dynamics simulation is a proven technique for computing and\nvisualizing the time-resolved motion of macromolecules at atomic resolution.\nThe MDsrv is a tool that streams MD trajectories and displays them\ninteractively in web browsers without requiring advanced skills, facilitating\ninteractive exploration and collaborative visual analysis. We have now enhanced\nthe MDsrv to further simplify the upload and sharing of MD trajectories and\nimprove their online viewing and analysis. With the new instance, the MDsrv\nsimplifies the creation of sessions, which allows the exchange of MD\ntrajectories with preset representations and perspectives. An important\ninnovation is that the MDsrv can now access and visualize trajectories from\nremote datasets, which greatly expands its applicability and use, as the data\nno longer needs to be accessible on a local server. In addition, initial\nanalyses such as sequence or structure alignments, distance measurements, or\nRMSD calculations have been implemented, which optionally support visual\nanalysis. Finally, the MDsrv now offers a faster and more efficient\nvisualization of even large trajectories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kampfrath_M/0/1/0/all/0/1\">Michelle Kampfrath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staritzbichler_R/0/1/0/all/0/1\">Ren&#xe9; Staritzbichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_G/0/1/0/all/0/1\">Guillermo P&#xe9;rez Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_A/0/1/0/all/0/1\">Alexander S. Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiemann_J/0/1/0/all/0/1\">Johanna K.S. Tiemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheuermann_G/0/1/0/all/0/1\">Gerik Scheuermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_D/0/1/0/all/0/1\">Daniel Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildebrand_P/0/1/0/all/0/1\">Peter W. Hildebrand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adjacent Context Coordination Network for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2203.13664v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13664","description":"<p>Salient object detection (SOD) in optical remote sensing images (RSIs), or\nRSI-SOD, is an emerging topic in understanding optical RSIs. However, due to\nthe difference between optical RSIs and natural scene images (NSIs), directly\napplying NSI-SOD methods to optical RSIs fails to achieve satisfactory results.\nIn this paper, we propose a novel Adjacent Context Coordination Network\n(ACCoNet) to explore the coordination of adjacent features in an\nencoder-decoder architecture for RSI-SOD. Specifically, ACCoNet consists of\nthree parts: an encoder, Adjacent Context Coordination Modules (ACCoMs), and a\ndecoder. As the key component of ACCoNet, ACCoM activates the salient regions\nof output features of the encoder and transmits them to the decoder. ACCoM\ncontains a local branch and two adjacent branches to coordinate the multi-level\nfeatures simultaneously. The local branch highlights the salient regions in an\nadaptive way, while the adjacent branches introduce global information of\nadjacent levels to enhance salient regions. Additionally, to extend the\ncapabilities of the classic decoder block (i.e., several cascaded convolutional\nlayers), we extend it with two bifurcations and propose a\nBifurcation-Aggregation Block to capture the contextual information in the\ndecoder. Extensive experiments on two benchmark datasets demonstrate that the\nproposed ACCoNet outperforms 22 state-of-the-art methods under nine evaluation\nmetrics, and runs up to 81 fps on a single NVIDIA Titan X GPU. The code and\nresults of our method are available at https://github.com/MathLee/ACCoNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Continuous-Time Optical Flow from Events and Frames. (arXiv:2203.13674v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13674","description":"<p>We present a method for estimating dense continuous-time optical flow.\nTraditional dense optical flow methods compute the pixel displacement between\ntwo images. Due to missing information, these approaches cannot recover the\npixel trajectories in the blind time between two images. In this work, we show\nthat it is possible to compute per-pixel, continuous-time optical flow by\nadditionally using events from an event camera. Events provide temporally\nfine-grained information about movement in image space due to their\nasynchronous nature and microsecond response time. We leverage these benefits\nto predict pixel trajectories densely in continuous-time via parameterized\nB\\'ezier curves. To achieve this, we introduce multiple innovations to build a\nneural network with strong inductive biases for this task: First, we build\nmultiple sequential correlation volumes in time using event data. Second, we\nuse B\\'ezier curves to index these correlation volumes at multiple timestamps\nalong the trajectory. Third, we use the retrieved correlation to update the\nB\\'ezier curve representations iteratively. Our method can optionally include\nimage pairs to boost performance further. The proposed approach outperforms\nexisting image-based and event-based methods by 11.5 % lower EPE on DSEC-Flow.\nFinally, we introduce a novel synthetic dataset MultiFlow for pixel trajectory\nregression on which our method is currently the only successful approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1\">Mathias Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muglikar_M/0/1/0/all/0/1\">Manasi Muglikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the performance of preconditioned methods to solve \\(L^p\\)-norm phase unwrapping. (arXiv:2203.13675v1 [math.NA])","link":"http://arxiv.org/abs/2203.13675","description":"<p>In this paper, we analyze and evaluate suitable preconditioning techniques to\nimprove the performance of the $L^p$-norm phase unwrapping method. We consider\nfive preconditioning techniques commonly found in the literature, and analyze\ntheir performance with different sizes of wrapped-phase maps. Keywords.- Phase\nunwrapping, $L^p$-norm based method, Preconditioning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Legarda_Saenz_R/0/1/0/all/0/1\">Ricardo Legarda-Saenz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Brito_Loeza_C/0/1/0/all/0/1\">Carlos Brito-Loeza</a>, <a href=\"http://arxiv.org/find/math/1/au:+Espinosa_Romero_A/0/1/0/all/0/1\">Arturo Espinosa-Romero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-FL: Style Transfer Preprocessing in Federated Learning for COVID-19 Segmentation. (arXiv:2203.13680v1 [eess.IV])","link":"http://arxiv.org/abs/2203.13680","description":"<p>Chest Computational Tomography (CT) scans present low cost, speed and\nobjectivity for COVID-19 diagnosis and deep learning methods have shown great\npromise in assisting the analysis and interpretation of these images. Most\nhospitals or countries can train their own models using in-house data, however\nempirical evidence shows that those models perform poorly when tested on new\nunseen cases, surfacing the need for coordinated global collaboration. Due to\nprivacy regulations, medical data sharing between hospitals and nations is\nextremely difficult. We propose a GAN-augmented federated learning model,\ndubbed ST-FL (Style Transfer Federated Learning), for COVID-19 image\nsegmentation. Federated learning (FL) permits a centralised model to be learned\nin a secure manner from heterogeneous datasets located in disparate private\ndata silos. We demonstrate that the widely varying data quality on FL client\nnodes leads to a sub-optimal centralised FL model for COVID-19 chest CT image\nsegmentation. ST-FL is a novel FL framework that is robust in the face of\nhighly variable data quality at client nodes. The robustness is achieved by a\ndenoising CycleGAN model at each client of the federation that maps arbitrary\nquality images into the same target quality, counteracting the severe data\nvariability evident in real-world FL use-cases. Each client is provided with\nthe target style, which is the same for all clients, and trains their own\ndenoiser. Our qualitative and quantitative results suggest that this FL model\nperforms comparably to, and in some cases better than, a model that has\ncentralised access to all the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Georgiadis_A/0/1/0/all/0/1\">Antonios Georgiadis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Babbar_V/0/1/0/all/0/1\">Varun Babbar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silavong_F/0/1/0/all/0/1\">Fran Silavong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moran_S/0/1/0/all/0/1\">Sean Moran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otter_R/0/1/0/all/0/1\">Rob Otter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satellite Infrastructure/Mission Tradeoffs. (arXiv:2203.13686v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13686","description":"<p>If a unit cannot receive intelligence from a source due to external factors,\nwe consider them disadvantaged users. We categorize this as a preoccupied unit\nworking on a low connectivity device on the edge. This case requires that we\nuse a different approach to deliver intelligence, particularly satellite\nimagery information, than normally employed. To address this, we propose a\nsurvey of information reduction techniques to deliver the information from a\nsatellite image in a smaller package. We investigate four techniques to aid in\nthe reduction of delivered information: traditional image compression, neural\nnetwork image compression, object detection image cutout, and image to caption.\nEach of these mechanisms have their benefits and tradeoffs when considered for\na disadvantaged user.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1\">Matthew Ciolino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The TerraByte Client: providing access to terabytes of plant data. (arXiv:2203.13691v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13691","description":"<p>In this paper we demonstrate the TerraByte Client, a software to download\nuser-defined plant datasets from a data portal hosted at Compute Canada. To\nthat end the client offers two key functionalities: (1) It allows the user to\nget an overview on what data is available and a quick way to visually check\nsamples of that data. For this the client receives the results of queries to a\ndatabase and displays the number of images that fulfill the search criteria.\nFurthermore, a sample can be downloaded within seconds to confirm that the data\nsuits the user's needs. (2) The user can then download the specified data to\ntheir own drive. This data is prepared into chunks server-side and sent to the\nuser's end-system, where it is automatically extracted into individual files.\nThe first chunks of data are available for inspection after a brief waiting\nperiod of a minute or less depending on available bandwidth and type of data.\nThe TerraByte Client has a full graphical user interface for easy usage and\nuses end-to-end encryption. The user interface is built on top of a low-level\nclient. This architecture in combination of offering the client program\nopen-source makes it possible for the user to develop their own user interface\nor use the client's functionality directly. An example for direct usage could\nbe to download specific data on demand within a larger application, such as\ntraining machine learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beck_M/0/1/0/all/0/1\">Michael A. Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidinosti_C/0/1/0/all/0/1\">Christopher P. Bidinosti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_C/0/1/0/all/0/1\">Christopher J. Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajmani_M/0/1/0/all/0/1\">Manisha Ajmani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Neural Representations for Variable Length Human Motion Generation. (arXiv:2203.13694v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13694","description":"<p>We propose an action-conditional human motion generation method using\nvariational implicit neural representations (INR). The variational formalism\nenables action-conditional distributions of INRs, from which one can easily\nsample representations to generate novel human motion sequences. Our method\noffers variable-length sequence generation by construction because a part of\nINR is optimized for a whole sequence of arbitrary length with temporal\nembeddings. In contrast, previous works reported difficulties with modeling\nvariable-length sequences. We confirm that our method with a Transformer\ndecoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC\ndatasets in terms of realism and diversity of generated motions. Surprisingly,\neven our method with an MLP decoder consistently outperforms the\nstate-of-the-art Transformer-based auto-encoder. In particular, we show that\nvariable-length motions generated by our method are better than fixed-length\nmotions generated by the state-of-the-art method in terms of realism and\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cervantes_P/0/1/0/all/0/1\">Pablo Cervantes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekikawa_Y/0/1/0/all/0/1\">Yusuke Sekikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Ikuro Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Koichi Shinoda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Deraining: Optimization Model Driven Deep CNN. (arXiv:2203.13699v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13699","description":"<p>The deep convolutional neural network has achieved significant progress for\nsingle image rain streak removal. However, most of the data-driven learning\nmethods are full-supervised or semi-supervised, unexpectedly suffering from\nsignificant performance drops when dealing with real rain. These data-driven\nlearning methods are representative yet generalize poor for real rain. The\nopposite holds true for the model-driven unsupervised optimization methods. To\novercome these problems, we propose a unified unsupervised learning framework\nwhich inherits the generalization and representation merits for real rain\nremoval. Specifically, we first discover a simple yet important domain\nknowledge that directional rain streak is anisotropic while the natural clean\nimage is isotropic, and formulate the structural discrepancy into the energy\nfunction of the optimization model. Consequently, we design an optimization\nmodel-driven deep CNN in which the unsupervised loss function of the\noptimization model is enforced on the proposed network for better\ngeneralization. In addition, the architecture of the network mimics the main\nrole of the optimization models with better feature representation. On one\nhand, we take advantage of the deep network to improve the representation. On\nthe other hand, we utilize the unsupervised loss of the optimization model for\nbetter generalization. Overall, the unsupervised learning framework achieves\ngood generalization and representation: unsupervised training (loss) with only\na few real rainy images (input) and physical meaning network (architecture).\nExtensive experiments on synthetic and real-world rain datasets show the\nsuperiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changfeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xile Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Luxin Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering Aided Weakly Supervised Training to Detect Anomalous Events in Surveillance Videos. (arXiv:2203.13704v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13704","description":"<p>Formulating learning systems for the detection of real-world anomalous events\nusing only video-level labels is a challenging task mainly due to the presence\nof noisy labels as well as the rare occurrence of anomalous events in the\ntraining data. We propose a weakly supervised anomaly detection system which\nhas multiple contributions including a random batch selection mechanism to\nreduce inter-batch correlation and a normalcy suppression block which learns to\nminimize anomaly scores over normal regions of a video by utilizing the overall\ninformation available in a training batch. In addition, a clustering loss block\nis proposed to mitigate the label noise and to improve the representation\nlearning for the anomalous and normal regions. This block encourages the\nbackbone network to produce two distinct feature clusters representing normal\nand anomalous events. Extensive analysis of the proposed approach is provided\nusing three popular anomaly detection datasets including UCF-Crime,\nShanghaiTech, and UCSD Ped2. The experiments demonstrate a superior anomaly\ndetection capability of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">Arif Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1\">Marcella Astrid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Network Width with Bilaterally Coupled Network. (arXiv:2203.13714v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13714","description":"<p>Searching for a more compact network width recently serves as an effective\nway of channel pruning for the deployment of convolutional neural networks\n(CNNs) under hardware constraints. To fulfill the searching, a one-shot\nsupernet is usually leveraged to efficiently evaluate the performance\n\\wrt~different network widths. However, current methods mainly follow a\n\\textit{unilaterally augmented} (UA) principle for the evaluation of each\nwidth, which induces the training unfairness of channels in supernet. In this\npaper, we introduce a new supernet called Bilaterally Coupled Network (BCNet)\nto address this issue. In BCNet, each channel is fairly trained and responsible\nfor the same amount of network widths, thus each network width can be evaluated\nmore accurately. Besides, we propose to reduce the redundant search space and\npresent the BCNetV2 as the enhanced supernet to ensure rigorous training\nfairness over channels. Furthermore, we leverage a stochastic complementary\nstrategy for training the BCNet, and propose a prior initial population\nsampling method to boost the performance of the evolutionary search. We also\npropose the first open-source width benchmark on macro structures named\nChannel-Bench-Macro for the better comparison of width search algorithms.\nExtensive experiments on benchmark CIFAR-10 and ImageNet datasets indicate that\nour method can achieve state-of-the-art or competing performance over other\nbaseline methods. Moreover, our method turns out to further boost the\nperformance of NAS models by refining their network widths. For example, with\nthe same FLOPs budget, our obtained EfficientNet-B0 achieves 77.53\\% Top-1\naccuracy on ImageNet dataset, surpassing the performance of original setting by\n0.65\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stabilizing Adversarially Learned One-Class Novelty Detection Using Pseudo Anomalies. (arXiv:2203.13716v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13716","description":"<p>Recently, anomaly scores have been formulated using reconstruction loss of\nthe adversarially learned generators and/or classification loss of\ndiscriminators. Unavailability of anomaly examples in the training data makes\noptimization of such networks challenging. Attributed to the adversarial\ntraining, performance of such models fluctuates drastically with each training\nstep, making it difficult to halt the training at an optimal point. In the\ncurrent study, we propose a robust anomaly detection framework that overcomes\nsuch instability by transforming the fundamental role of the discriminator from\nidentifying real vs. fake data to distinguishing good vs. bad quality\nreconstructions. For this purpose, we propose a method that utilizes the\ncurrent state as well as an old state of the same generator to create good and\nbad quality reconstruction examples. The discriminator is trained on these\nexamples to detect the subtle distortions that are often present in the\nreconstructions of anomalous data. In addition, we propose an efficient generic\ncriterion to stop the training of our model, ensuring elevated performance.\nExtensive experiments performed on six datasets across multiple domains\nincluding image and video based anomaly detection, medical diagnosis, and\nnetwork security, have demonstrated excellent performance of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jin Ha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">Arif Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1\">Marcella Astrid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digital Fingerprinting of Microstructures. (arXiv:2203.13718v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13718","description":"<p>Finding efficient means of fingerprinting microstructural information is a\ncritical step towards harnessing data-centric machine learning approaches. A\nstatistical framework is systematically developed for compressed\ncharacterisation of a population of images, which includes some classical\ncomputer vision methods as special cases. The focus is on materials\nmicrostructure. The ultimate purpose is to rapidly fingerprint sample images in\nthe context of various high-throughput design/make/test scenarios. This\nincludes, but is not limited to, quantification of the disparity between\nmicrostructures for quality control, classifying microstructures, predicting\nmaterials properties from image data and identifying potential processing\nroutes to engineer new materials with specific properties. Here, we consider\nmicrostructure classification and utilise the resulting features over a range\nof related machine learning tasks, namely supervised, semi-supervised, and\nunsupervised learning.\n</p>\n<p>The approach is applied to two distinct datasets to illustrate various\naspects and some recommendations are made based on the findings. In particular,\nmethods that leverage transfer learning with convolutional neural networks\n(CNNs), pretrained on the ImageNet dataset, are generally shown to outperform\nother methods. Additionally, dimensionality reduction of these CNN-based\nfingerprints is shown to have negligible impact on classification accuracy for\nthe supervised learning approaches considered. In situations where there is a\nlarge dataset with only a handful of images labelled, graph-based label\npropagation to unlabelled data is shown to be favourable over discarding\nunlabelled data and performing supervised learning. In particular, label\npropagation by Poisson learning is shown to be highly effective at low label\nrates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Michael D. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarakanov_A/0/1/0/all/0/1\">Alexander Tarakanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Race_C/0/1/0/all/0/1\">Christopher P. Race</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Withers_P/0/1/0/all/0/1\">Philip J. Withers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_K/0/1/0/all/0/1\">Kody J.H. Law</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salt Detection Using Segmentation of Seismic Image. (arXiv:2203.13721v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13721","description":"<p>In this project, a state-of-the-art deep convolution neural network (DCNN) is\npresented to segment seismic images for salt detection below the earth's\nsurface. Detection of salt location is very important for starting mining.\nHence, a seismic image is used to detect the exact salt location under the\nearth's surface. However, precisely detecting the exact location of salt\ndeposits is difficult. Therefore, professional seismic imaging still requires\nexpert human interpretation of salt bodies. This leads to very subjective,\nhighly variable renderings. Hence, to create the most accurate seismic images\nand 3D renderings, we need a robust algorithm that automatically and accurately\nidentifies if a surface target is a salt or not. Since the performance of DCNN\nis well-known and well-established for object recognition in images, DCNN is a\nvery good choice for this particular problem and being successfully applied to\na dataset of seismic images in which each pixel is labeled as salt or not. The\nresult of this algorithm is promising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1\">Mrinmoy Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FReSCO: Flow Reconstruction and Segmentation for low latency Cardiac Output monitoring using deep artifact suppression and segmentation. (arXiv:2203.13729v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13729","description":"<p>Purpose: Real-time monitoring of cardiac output (CO) requires low latency\nreconstruction and segmentation of real-time phase contrast MR (PCMR), which\nhas previously been difficult to perform. Here we propose a deep learning\nframework for 'Flow Reconstruction and Segmentation for low latency Cardiac\nOutput monitoring' (FReSCO).\n</p>\n<p>Methods: Deep artifact suppression and segmentation U-Nets were independently\ntrained. Breath hold spiral PCMR data (n=516) was synthetically undersampled\nusing a variable density spiral sampling pattern and gridded to create aliased\ndata for training of the artifact suppression U-net. A subset of the data\n(n=96) was segmented and used to train the segmentation U-net. Real-time spiral\nPCMR was prospectively acquired and then reconstructed and segmented using the\ntrained models (FReSCO) at low latency at the scanner in 10 healthy subjects\nduring rest, exercise and recovery periods. CO obtained via FReSCO was compared\nto a reference rest CO and rest and exercise Compressed Sensing (CS) CO.\n</p>\n<p>Results: FReSCO was demonstrated prospectively at the scanner. Beat-to-beat\nheartrate, stroke volume and CO could be visualized with a mean latency of\n622ms. No significant differences were noted when compared to reference at rest\n(Bias = -0.21+-0.50 L/min, p=0.246) or CS at peak exercise (Bias=0.12+-0.48\nL/min, p=0.458).\n</p>\n<p>Conclusion: FReSCO was successfully demonstrated for real-time monitoring of\nCO during exercise and could provide a convenient tool for assessment of the\nhemodynamic response to a range of stressors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaubert_O/0/1/0/all/0/1\">Olivier Jaubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montalt_Tordera_J/0/1/0/all/0/1\">Javier Montalt-Tordera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">James Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_D/0/1/0/all/0/1\">Daniel Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1\">Simon Arridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeden_J/0/1/0/all/0/1\">Jennifer Steeden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthurangu_V/0/1/0/all/0/1\">Vivek Muthurangu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient-VDVAE: Less is more. (arXiv:2203.13751v1 [cs.LG])","link":"http://arxiv.org/abs/2203.13751","description":"<p>Hierarchical VAEs have emerged in recent years as a reliable option for\nmaximum likelihood estimation. However, instability issues and demanding\ncomputational requirements have hindered research progress in the area. We\npresent simple modifications to the Very Deep VAE to make it converge up to\n$2.6\\times$ faster, save up to $20\\times$ in memory load and improve stability\nduring training. Despite these changes, our models achieve comparable or better\nnegative log-likelihood performance than current state-of-the-art models on all\n$7$ commonly used image datasets we evaluated on. We also make an argument\nagainst using 5-bit benchmarks as a way to measure hierarchical VAE's\nperformance due to undesirable biases caused by the 5-bit quantization.\nAdditionally, we empirically demonstrate that roughly $3\\%$ of the hierarchical\nVAE's latent space dimensions is sufficient to encode most of the image\ninformation, without loss of performance, opening up the doors to efficiently\nleverage the hierarchical VAEs' latent space in downstream tasks. We release\nour source code and models at https://github.com/Rayhane-mamah/Efficient-VDVAE .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazami_L/0/1/0/all/0/1\">Louay Hazami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mama_R/0/1/0/all/0/1\">Rayhane Mama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thurairatnam_R/0/1/0/all/0/1\">Ragavan Thurairatnam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of the use of color and its emotional relationship in visual creations based on experiences during the context of the COVID-19 pandemic. (arXiv:2203.13770v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13770","description":"<p>Color is a complex communicative element that helps us understand and\nevaluate our environment. At the level of artistic creation, this component\ninfluences both the formal aspects of the composition and the symbolic weight,\ndirectly affecting the construction and transmission of the message that you\nwant to communicate, creating a specific emotional reaction. During the\nCOVID-19 pandemic, people generated countless images transmitting this event's\nsubjective experiences. Using the repository of images created in the Instagram\naccount CAM (The COVID Art Museum), we propose a methodology to understand the\nuse of color and its emotional relationship in this context. The process\nconsiders two stages in parallel that are then combined. First, emotions are\nextracted and classified from the CAM dataset images through a convolutional\nneural network. Second, we extract the colors and their harmonies through a\nclustering process. Once both processes are completed, we combine the results\ngenerating an expanded discussion on the usage of color, harmonies, and\nemotion. The results indicate that warm colors are prevalent in the sample,\nwith a preference for analog compositions over complementary ones. The\nrelationship between emotions and these compositions shows a trend in positive\nemotions, reinforced by the results of the algorithm a priori and the emotional\nrelationship analysis of the attributes of color (hue, chroma, and lighting).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Martin_C/0/1/0/all/0/1\">C&#xe9;sar Gonz&#xe1;lez-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrasco_M/0/1/0/all/0/1\">Miguel Carrasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oviedo_G/0/1/0/all/0/1\">Germ&#xe1;n Oviedo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion. (arXiv:2203.13777v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13777","description":"<p>Human behavior has the nature of indeterminacy, which requires the pedestrian\ntrajectory prediction system to model the multi-modality of future motion\nstates. Unlike existing stochastic trajectory prediction methods which usually\nuse a latent variable to represent multi-modality, we explicitly simulate the\nprocess of human motion variation from indeterminate to determinate. In this\npaper, we present a new framework to formulate the trajectory prediction task\nas a reverse process of motion indeterminacy diffusion (MID), in which we\nprogressively discard indeterminacy from all the walkable areas until reaching\nthe desired trajectory. This process is learned with a parameterized Markov\nchain conditioned by the observed trajectories. We can adjust the length of the\nchain to control the degree of indeterminacy and balance the diversity and\ndeterminacy of the predictions. Specifically, we encode the history behavior\ninformation and the social interactions as a state embedding and devise a\nTransformer-based diffusion model to capture the temporal dependencies of\ntrajectories. Extensive experiments on the human trajectory prediction\nbenchmarks including the Stanford Drone and ETH/UCY datasets demonstrate the\nsuperiority of our method. Code is available at\nhttps://github.com/gutianpei/MID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1\">Tianpei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-based Safe Landing for UAVs in Populated Areas: Real-time Validation in Virtual Environments. (arXiv:2203.13792v1 [cs.RO])","link":"http://arxiv.org/abs/2203.13792","description":"<p>Safe autonomous landing for Unmanned Aerial Vehicles (UAVs) in populated\nareas is a crucial aspect for successful urban deployment, particularly in\nemergency landing situations. Nonetheless, validating autonomous landing in\nreal scenarios is a challenging task involving a high risk of injuring people.\nIn this work, we propose a framework for real-time safe and thorough evaluation\nof vision-based autonomous landing in populated scenarios, using\nphoto-realistic virtual environments. We propose to use the Unreal graphics\nengine coupled with the AirSim plugin for drone's simulation, and evaluate\nautonomous landing strategies based on visual detection of Safe Landing Zones\n(SLZ) in populated scenarios. Then, we study two different criteria for\nselecting the \"best\" SLZ, and evaluate them during autonomous landing of a\nvirtual drone in different scenarios and conditions, under different\ndistributions of people in urban scenes, including moving people. We evaluate\ndifferent metrics to quantify the performance of the landing strategies,\nestablishing a baseline for comparison with future works in this challenging\ntask, and analyze them through an important number of randomized iterations.\nThe study suggests that the use of the autonomous landing algorithms\nconsiderably helps to prevent accidents involving humans, which may allow to\nunleash the full potential of drones in urban environments near to people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tovanche_Picon_H/0/1/0/all/0/1\">Hector Tovanche-Picon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Trejo_J/0/1/0/all/0/1\">Javier Gonzalez-Trejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_Abad_A/0/1/0/all/0/1\">Angel Flores-Abad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercado_Ravell_D/0/1/0/all/0/1\">Diego Mercado-Ravell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Dynamic-NeRF: Spline-NeRF. (arXiv:2203.13800v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13800","description":"<p>The problem of reconstructing continuous functions over time is important for\nproblems such as reconstructing moving scenes, and interpolating between time\nsteps. Previous approaches that use deep-learning rely on regularization to\nensure that reconstructions are approximately continuous, which works well on\nshort sequences. As sequence length grows, though, it becomes more difficult to\nregularize, and it becomes less feasible to learn only through regularization.\nWe propose a new architecture for function reconstruction based on classical\nBezier splines, which ensures $C^0$ and $C^1$-continuity, where $C^0$\ncontinuity is that $\\forall c:\\lim\\limits_{x\\to c} f(x)\n</p>\n<p>= f(c)$, or more intuitively that there are no breaks at any point in the\nfunction. In order to demonstrate our architecture, we reconstruct dynamic\nscenes using Neural Radiance Fields, but hope it is clear that our approach is\ngeneral and can be applied to a variety of problems. We recover a Bezier spline\n$B(\\beta, t\\in[0,1])$, parametrized by the control points $\\beta$. Using Bezier\nsplines ensures reconstructions have $C^0$ and $C^1$ continuity, allowing for\nguaranteed interpolation over time. We reconstruct $\\beta$ with a multi-layer\nperceptron (MLP), blending machine learning with classical animation\ntechniques. All code is available at https://github.com/JulianKnodt/nerf_atlas,\nand datasets are from prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knodt_J/0/1/0/all/0/1\">Julian Knodt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Playing Lottery Tickets in Style Transfer Models. (arXiv:2203.13802v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13802","description":"<p>Style transfer has achieved great success and attracted a wide range of\nattention from both academic and industrial communities due to its flexible\napplication scenarios. However, the dependence on pretty large VGG based\nautoencoder leads to existing style transfer models have a high parameter\ncomplexities which limits the application for resource-constrained devices.\nUnfortunately, the compression of style transfer model has less been explored.\nIn parallel, study on the lottery ticket hypothesis (LTH) has shown great\npotential in finding extremely sparse matching subnetworks which can achieve on\npar or even better performance than original full networks when trained in\nisolation. In this work, we perform the first empirical study to verify whether\nsuch trainable networks also exist in style transfer models. From a wide range\nof style transfer methods, we choose two of the most popular style transfer\nmodels as the main testbeds, i.e., AdaIN and SANet, representing approaches of\nglobal and local transformation based style transfer respectively. Through\nextensive experiments and comprehensive analysis, we draw the following main\nconclusions. (1) Compared with fixing VGG encoder, style transfer models can\nbenefit more from training the whole network together. (2) Using iterative\nmagnitude pruning, we find the most sparse matching subnetworks at 89.2% in\nAdaIN and 73.7% in SANet, which suggests that style transfer models can play\nlottery tickets too. (3) Feature transformation module should also be pruned to\nget a sparser model without affecting the existence and quality of matching\nsubnetworks. (4) Besides AdaIN and SANet, other models such as LST, MANet,\nAdaAttN and MCCNet can also play lottert tickets, which shows that LTH can be\ngeneralized to various style transfer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_M/0/1/0/all/0/1\">Meihao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatially Multi-conditional Image Generation. (arXiv:2203.13812v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13812","description":"<p>In most scenarios, conditional image generation can be thought of as an\ninversion of the image understanding process. Since generic image understanding\ninvolves the solving of multiple tasks, it is natural to aim at the generation\nof images via multi-conditioning. However, multi-conditional image generation\nis a very challenging problem due to the heterogeneity and the sparsity of the\n(in practice) available conditioning labels. In this work, we propose a novel\nneural architecture to address the problem of heterogeneity and sparsity of the\nspatially multi-conditional labels. Our choice of spatial conditioning, such as\nby semantics and depth, is driven by the promise it holds for better control of\nthe image generation process. The proposed method uses a transformer-like\narchitecture operating pixel-wise, which receives the available labels as input\ntokens to merge them in a learned homogeneous space of labels. The merged\nlabels are then used for image generation via conditional generative\nadversarial training. In this process, the sparsity of the labels is handled by\nsimply dropping the input tokens corresponding to the missing labels at the\ndesired locations, thanks to the proposed pixel-wise operating architecture.\nOur experiments on three benchmark datasets demonstrate the clear superiority\nof our method over the state-of-the-art and the compared baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Ritika Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Versatile Multi-Modal Pre-Training for Human-Centric Perception. (arXiv:2203.13815v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13815","description":"<p>Human-centric perception plays a vital role in vision and graphics. But their\ndata annotations are prohibitively expensive. Therefore, it is desirable to\nhave a versatile pre-train model that serves as a foundation for data-efficient\ndownstream tasks transfer. To this end, we propose the Human-Centric\nMulti-Modal Contrastive Learning framework HCMoCo that leverages the\nmulti-modal nature of human data (e.g. RGB, depth, 2D keypoints) for effective\nrepresentation learning. The objective comes with two main challenges: dense\npre-train for multi-modality data, efficient usage of sparse human priors. To\ntackle the challenges, we design the novel Dense Intra-sample Contrastive\nLearning and Sparse Structure-aware Contrastive Learning targets by\nhierarchically learning a modal-invariant latent space featured with continuous\nand ordinal feature distribution and structure-aware semantic consistency.\nHCMoCo provides pre-train for different modalities by combining heterogeneous\ndatasets, which allows efficient usage of existing task-specific human data.\nExtensive experiments on four downstream tasks of different modalities\ndemonstrate the effectiveness of HCMoCo, especially under data-efficient\nsettings (7.16% and 12% improvement on DensePose Estimation and Human Parsing).\nMoreover, we demonstrate the versatility of HCMoCo by exploring cross-modality\nsupervision and missing-modality inference, validating its strong ability in\ncross-modal association and reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fangzhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling. (arXiv:2203.13817v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13817","description":"<p>Neural fields such as implicit surfaces have recently enabled avatar modeling\nfrom raw scans without explicit temporal correspondences. In this work, we\nexploit autoregressive modeling to further extend this notion to capture\ndynamic effects, such as soft-tissue deformations. Although autoregressive\nmodels are naturally capable of handling dynamics, it is non-trivial to apply\nthem to implicit representations, as explicit state decoding is infeasible due\nto prohibitive memory requirements. In this work, for the first time, we enable\nautoregressive modeling of implicit avatars. To reduce the memory bottleneck\nand efficiently model dynamic implicit surfaces, we introduce the notion of\narticulated observer points, which relate implicit states to the explicit\nsurface of a parametric human body model. We demonstrate that encoding implicit\nsurfaces as a set of height fields defined on articulated observer points leads\nto significantly better generalization compared to a latent representation. The\nexperiments show that our approach outperforms the state of the art, achieving\nplausible dynamic deformations even for unseen motions.\nhttps://zqbai-jeremy.github.io/autoavatar\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Ziqian Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1\">Javier Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1\">Michael Zollh&#xf6;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection and Tracking of Multiple Mice Using Part Proposal Networks. (arXiv:1906.02831v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1906.02831","description":"<p>The study of mouse social behaviours has been increasingly undertaken in\nneuroscience research. However, automated quantification of mouse behaviours\nfrom the videos of interacting mice is still a challenging problem, where\nobject tracking plays a key role in locating mice in their living spaces.\nArtificial markers are often applied for multiple mice tracking, which are\nintrusive and consequently interfere with the movements of mice in a dynamic\nenvironment. In this paper, we propose a novel method to continuously track\nseveral mice and individual parts without requiring any specific tagging.\nFirstly, we propose an efficient and robust deep learning based mouse part\ndetection scheme to generate part candidates. Subsequently, we propose a novel\nBayesian Integer Linear Programming Model that jointly assigns the part\ncandidates to individual targets with necessary geometric constraints whilst\nestablishing pair-wise association between the detected parts. There is no\npublicly available dataset in the research community that provides a\nquantitative test-bed for the part detection and tracking of multiple mice, and\nwe here introduce a new challenging Multi-Mice PartsTrack dataset that is made\nof complex behaviours and actions. Finally, we evaluate our proposed approach\nagainst several baselines on our new datasets, where the results show that our\nmethod outperforms the other state-of-the-art approaches in terms of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1\">Lei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiangyuan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crookes_D/0/1/0/all/0/1\">Danny Crookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Model to Transfer? Finding the Needle in the Growing Haystack. (arXiv:2010.06402v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.06402","description":"<p>Transfer learning has been recently popularized as a data-efficient\nalternative to training models from scratch, in particular for computer vision\ntasks where it provides a remarkably solid baseline. The emergence of rich\nmodel repositories, such as TensorFlow Hub, enables the practitioners and\nresearchers to unleash the potential of these models across a wide range of\ndownstream tasks. As these repositories keep growing exponentially, efficiently\nselecting a good model for the task at hand becomes paramount. We provide a\nformalization of this problem through a familiar notion of regret and introduce\nthe predominant strategies, namely task-agnostic (e.g. ranking models by their\nImageNet performance) and task-aware search strategies (such as linear or kNN\nevaluation). We conduct a large-scale empirical study and show that both\ntask-agnostic and task-aware methods can yield high regret. We then propose a\nsimple and computationally efficient hybrid search strategy which outperforms\nthe existing approaches. We highlight the practical benefits of the proposed\nsolution on a set of 19 diverse vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Renggli_C/0/1/0/all/0/1\">Cedric Renggli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_A/0/1/0/all/0/1\">Andr&#xe9; Susano Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rimanic_L/0/1/0/all/0/1\">Luka Rimanic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1\">Joan Puigcerver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1\">Carlos Riquelme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Increasing Efficiency of Frank-Wolfe Adversarial Training. (arXiv:2012.12368v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.12368","description":"<p>Deep neural networks are easily fooled by small perturbations known as\nadversarial attacks. Adversarial Training (AT) is a technique that\napproximately solves a robust optimization problem to minimize the worst-case\nloss and is widely regarded as the most effective defense. Due to the high\ncomputation time for generating strong adversarial examples in the AT process,\nsingle-step approaches have been proposed to reduce training time. However,\nthese methods suffer from catastrophic overfitting where adversarial accuracy\ndrops during training, and although improvements have been proposed, they\nincrease training time and robustness is far from that of multi-step AT. We\ndevelop a theoretical framework for adversarial training with FW optimization\n(FW-AT) that reveals a geometric connection between the loss landscape and the\n$\\ell_2$ distortion of $\\ell_\\infty$ FW attacks. We analytically show that high\ndistortion of FW attacks is equivalent to small gradient variation along the\nattack path. It is then experimentally demonstrated on various deep neural\nnetwork architectures that $\\ell_\\infty$ attacks against robust models achieve\nnear maximal distortion, while standard networks have lower distortion. It is\nexperimentally shown that catastrophic overfitting is strongly correlated with\nlow distortion of FW attacks. This mathematical transparency differentiates FW\nfrom Projected Gradient Descent (PGD) optimization. To demonstrate the utility\nof our theoretical framework we develop FW-AT-Adapt, a novel adversarial\ntraining algorithm which uses a simple distortion measure to adapt the number\nof attack steps during training to increase efficiency without compromising\nrobustness. FW-AT-Adapt provides training time on par with single-step fast AT\nmethods and closes the gap between fast AT methods and multi-step PGD-AT with\nminimal loss in adversarial accuracy in white-box and black-box settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jay Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13450","description":"<p>Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. However, in many cases, users only\nhave access to rendered 2D images instead of 3D meshes. Unfortunately,\nretrieving messages from 2D renderings of 3D meshes is still challenging and\nunderexplored. We introduce a novel end-to-end learning framework to solve this\nproblem through: 1) an encoder to covertly embed messages in both mesh geometry\nand textures; 2) a differentiable renderer to render watermarked 3D objects\nfrom different camera angles and under varied lighting conditions; 3) a decoder\nto recover the messages from 2D rendered images. From our experiments, we show\nthat our model can learn to embed information visually imperceptible to humans,\nand to retrieve the embedded information from 2D renderings that undergo 3D\ndistortions. In addition, we demonstrate that our method can also work with\nother renderers, such as ray tracers and real-time renderers with and without\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_I/0/1/0/all/0/1\">Innfarn Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stava_O/0/1/0/all/0/1\">Ondrej Stava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens. (arXiv:2105.15168v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.15168","description":"<p>Transformers have offered a new methodology of designing neural networks for\nvisual recognition. Compared to convolutional networks, Transformers enjoy the\nability of referring to global features at each stage, yet the attention module\nbrings higher computational overhead that obstructs the application of\nTransformers to process high-resolution visual data. This paper aims to\nalleviate the conflict between efficiency and flexibility, for which we propose\na specialized token for each region that serves as a messenger (MSG). Hence, by\nmanipulating these MSG tokens, one can flexibly exchange visual information\nacross regions and the computational complexity is reduced. We then integrate\nthe MSG token into a multi-scale architecture named MSG-Transformer. In\nstandard image classification and object detection, MSG-Transformer achieves\ncompetitive performance and the inference on both GPU and CPU is accelerated.\nCode is available at https://github.com/hustvl/MSG-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIFT Matching by Context Exposed. (arXiv:2106.09584v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09584","description":"<p>This paper investigates how to step up local image descriptor matching by\nexploiting matching context information. Two main contexts are identified,\noriginated respectively from the descriptor space and from the keypoint space.\nThe former is generally used to design the actual matching strategy while the\nlatter to filter matches according to the local spatial consistency. On this\nbasis, a new matching strategy and a novel local spatial filter, named\nrespectively blob matching and Delaunay Triangulation Matching (DTM) are\ndevised. Blob matching provides a general matching framework by merging\ntogether several strategies, including rank-based pre-filtering as well as\nmany-to-many and symmetric matching, enabling to achieve a global improvement\nupon each individual strategy. DTM alternates between Delaunay triangulation\ncontractions and expansions to figure out and adjust keypoint neighborhood\nconsistency. Experimental evaluation shows that DTM is comparable or better\nthan the state-of-the-art in terms of matching accuracy and robustness.\nEvaluation is carried out according to a new benchmark devised for analyzing\nthe matching pipeline in terms of correct correspondences on both planar and\nnon-planar scenes, including several state-of-the-art methods as well as the\ncommon SIFT matching approach for reference. This evaluation can be of\nassistance for future research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1\">Fabio Bellavia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Feature Learning for Contrastive Multi-view Clustering. (arXiv:2106.11193v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11193","description":"<p>Multi-view clustering can explore common semantics from multiple views and\nhas attracted increasing attention. However, existing works punish multiple\nobjectives in the same feature space, where they ignore the conflict between\nlearning consistent common semantics and reconstructing inconsistent\nview-private information. In this paper, we propose a new framework of\nmulti-level feature learning for contrastive multi-view clustering to address\nthe aforementioned issue. Our method learns different levels of features from\nthe raw features, including low-level features, high-level features, and\nsemantic labels/features in a fusion-free manner, so that it can effectively\nachieve the reconstruction objective and the consistency objectives in\ndifferent feature spaces. Specifically, the reconstruction objective is\nconducted on the low-level features. Two consistency objectives based on\ncontrastive learning are conducted on the high-level features and the semantic\nlabels, respectively. They make the high-level features effectively explore the\ncommon semantics and the semantic labels achieve the multi-view clustering. As\na result, the proposed framework can reduce the adverse influence of\nview-private information. Extensive experiments on public datasets demonstrate\nthat our method achieves state-of-the-art clustering effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Huayi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recall@k Surrogate Loss with Large Batches and Similarity Mixup. (arXiv:2108.11179v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11179","description":"<p>This work focuses on learning deep visual representation models for retrieval\nby exploring the interplay between a new loss function, the batch size, and a\nnew regularization approach. Direct optimization, by gradient descent, of an\nevaluation metric, is not possible when it is non-differentiable, which is the\ncase for recall in retrieval. A differentiable surrogate loss for the recall is\nproposed in this work. Using an implementation that sidesteps the hardware\nconstraints of the GPU memory, the method trains with a very large batch size,\nwhich is essential for metrics computed on the entire retrieval database. It is\nassisted by an efficient mixup regularization approach that operates on\npairwise scalar similarities and virtually increases the batch size further.\nThe suggested method achieves state-of-the-art performance in several image\nretrieval benchmarks when used for deep metric learning. For instance-level\nrecognition, the method outperforms similar approaches that train using an\napproximation of average precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1\">Yash Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05488","description":"<p>Estimating the articulated 3D hand-object pose from a single RGB image is a\nhighly ambiguous and challenging problem, requiring large-scale datasets that\ncontain diverse hand poses, object types, and camera viewpoints. Most\nreal-world datasets lack these diversities. In contrast, data synthesis can\neasily ensure those diversities separately. However, constructing both valid\nand diverse hand-object interactions and efficiently learning from the vast\nsynthetic data is still challenging. To address the above issues, we propose\nArtiBoost, a lightweight online data enhancement method. ArtiBoost can cover\ndiverse hand-object poses and camera viewpoints through sampling in a\nComposited hand-object Configuration and Viewpoint space (CCV-space) and can\nadaptively enrich the current hard-discernable items by loss-feedback and\nsample re-weighting. ArtiBoost alternatively performs data exploration and\nsynthesis within a learning pipeline, and those synthetic data are blended into\nreal-world source data for training. We apply ArtiBoost on a simple learning\nbaseline network and witness the performance boost on several hand-object\nbenchmarks. Our models and code are available at\nhttps://github.com/lixiny/ArtiBoost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Bone Length Attack on Action Recognition. (arXiv:2109.05830v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05830","description":"<p>Skeleton-based action recognition models have recently been shown to be\nvulnerable to adversarial attacks. Compared to adversarial attacks on images,\nperturbations to skeletons are typically bounded to a lower dimension of\napproximately 100 per frame. This lower-dimensional setting makes it more\ndifficult to generate imperceptible perturbations. Existing attacks resolve\nthis by exploiting the temporal structure of the skeleton motion so that the\nperturbation dimension increases to thousands. In this paper, we show that\nadversarial attacks can be performed on skeleton-based action recognition\nmodels, even in a significantly low-dimensional setting without any temporal\nmanipulation. Specifically, we restrict the perturbations to the lengths of the\nskeleton's bones, which allows an adversary to manipulate only approximately 30\neffective dimensions. We conducted experiments on the NTU RGB+D and HDM05\ndatasets and demonstrate that the proposed attack successfully deceived models\nwith sometimes greater than 90% success rate by small perturbations.\nFurthermore, we discovered an interesting phenomenon: in our low-dimensional\nsetting, the adversarial training with the bone length attack shares a similar\nproperty with data augmentation, and it not only improves the adversarial\nrobustness but also improves the classification accuracy on the original data.\nThis is an interesting counterexample of the trade-off between adversarial\nrobustness and clean accuracy, which has been widely observed in studies on\nadversarial training in the high-dimensional regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_N/0/1/0/all/0/1\">Nariki Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kera_H/0/1/0/all/0/1\">Hiroshi Kera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_K/0/1/0/all/0/1\">Kazuhiko Kawamoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering. (arXiv:2109.08029v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08029","description":"<p>Integrating outside knowledge for reasoning in visio-linguistic tasks such as\nvisual question answering (VQA) is an open problem. Given that pretrained\nlanguage models have been shown to include world knowledge, we propose to use a\nunimodal (text-only) train and inference procedure based on automatic\noff-the-shelf captioning of images and pretrained language models. Our results\non a visual question answering task which requires external knowledge (OK-VQA)\nshow that our text-only model outperforms pretrained multimodal (image-text)\nmodels of comparable number of parameters. In contrast, our model is less\neffective in a standard VQA task (VQA 2.0) confirming that our text-only method\nis specially effective for tasks requiring external knowledge. In addition, we\nshow that increasing the language model's size improves notably its\nperformance, yielding results comparable to the state-of-the-art with our\nlargest model, significantly outperforming current multimodal systems, even\nthough augmented with external knowledge. Our qualitative analysis on OK-VQA\nreveals that automatic captions often fail to capture relevant information in\nthe images, which seems to be balanced by the better inference ability of the\ntext-only language models. Our work opens up possibilities to further improve\ninference in visio-linguistic tasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salaberria_A/0/1/0/all/0/1\">Ander Salaberria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azkune_G/0/1/0/all/0/1\">Gorka Azkune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.14181","description":"<p>Automated segmentation of pathological regions of interest aids medical image\ndiagnostics and follow-up care. However, accurate pathological segmentations\nrequire high quality of annotated data that can be both cost and time intensive\nto generate. In this work, we propose an automated two-step method that detects\na minimal image subset required to train segmentation models by evaluating the\nquality of medical images from 3D image stacks using a U-net++ model. These\nimages that represent a lack of quality training can then be annotated and used\nto fully train a U-net-based segmentation model. The proposed QU-net++ model\ndetects lack of quality training based on the disagreement in segmentations\nproduced from the final two output layers. The proposed model isolates around\n10% of images per 3D stack and can scale across imaging modalities to segment\ncysts in OCT images and ground glass opacity in Lung CT images with Dice scores\nin the range 0.56-0.72. Thus, the proposed method can be applied for cost\neffective multi-modal pathology segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sohini Roychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07991","description":"<p>This paper presents contrastive-tuning, a simple method employing contrastive\ntraining to align image and text models while still taking advantage of their\npre-training. In our empirical study we find that locked pre-trained image\nmodels with unlocked text models work best. We call this instance of\ncontrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model\nto read out good representations from a pre-trained image model for new tasks.\nA LiT model gains the capability of zero-shot transfer to new vision tasks,\nsuch as image classification or retrieval. The proposed LiT is widely\napplicable; it works reliably with multiple pre-training methods (supervised\nand unsupervised) and across diverse architectures (ResNet, Vision Transformers\nand MLP-Mixer) using three different image-text datasets. With the\ntransformer-based pre-trained ViT-g/14 model, the LiT model achieves 84.5%\nzero-shot transfer accuracy on the ImageNet test set, and 81.1% on the\nchallenging out-of-distribution ObjectNet test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08918","description":"<p>Recent works with an implicit neural function shed light on representing\nimages in arbitrary resolution. However, a standalone multi-layer perceptron\nshows limited performance in learning high-frequency components. In this paper,\nwe propose a Local Texture Estimator (LTE), a dominant-frequency estimator for\nnatural images, enabling an implicit function to capture fine details while\nreconstructing images in a continuous manner. When jointly trained with a deep\nsuper-resolution (SR) architecture, LTE is capable of characterizing image\ntextures in 2D Fourier space. We show that an LTE-based neural function\nachieves favorable performance against existing deep SR methods within an\narbitrary-scale factor. Furthermore, we demonstrate that our implementation\ntakes the shortest running time compared to previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kyong Hwan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trustworthy Long-Tailed Classification. (arXiv:2111.09030v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.09030","description":"<p>Classification on long-tailed distributed data is a challenging problem,\nwhich suffers from serious class-imbalance and accordingly unpromising\nperformance especially on tail classes. Recently, the ensembling based methods\nachieve the state-of-the-art performance and show great potential. However,\nthere are two limitations for current methods. First, their predictions are not\ntrustworthy for failure-sensitive applications. This is especially harmful for\nthe tail classes where the wrong predictions is basically frequent. Second,\nthey assign unified numbers of experts to all samples, which is redundant for\neasy samples with excessive computational cost. To address these issues, we\npropose a Trustworthy Long-tailed Classification (TLC) method to jointly\nconduct classification and uncertainty estimation to identify hard samples in a\nmulti-expert framework. Our TLC obtains the evidence-based uncertainty (EvU)\nand evidence for each expert, and then combines these uncertainties and\nevidences under the Dempster-Shafer Evidence Theory (DST). Moreover, we propose\na dynamic expert engagement to reduce the number of engaged experts for easy\nsamples and achieve efficiency while maintaining promising performances.\nFinally, we conduct comprehensive experiments on the tasks of classification,\ntail detection, OOD detection and failure prediction. The experimental results\nshow that the proposed TLC outperforms existing methods and is trustworthy with\nreliable uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bolian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zongbo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haining Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoxeR: Box-Attention for 2D and 3D Transformers. (arXiv:2111.13087v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13087","description":"<p>In this paper, we propose a simple attention mechanism, we call\nbox-attention. It enables spatial interaction between grid features, as sampled\nfrom boxes of interest, and improves the learning capability of transformers\nfor several vision tasks. Specifically, we present BoxeR, short for Box\nTransformer, which attends to a set of boxes by predicting their transformation\nfrom a reference window on an input feature map. The BoxeR computes attention\nweights on these boxes by considering its grid structure. Notably, BoxeR-2D\nnaturally reasons about box information within its attention module, making it\nsuitable for end-to-end instance detection and segmentation tasks. By learning\ninvariance to rotation in the box-attention module, BoxeR-3D is capable of\ngenerating discriminative information from a bird's-eye view plane for 3D\nend-to-end object detection. Our experiments demonstrate that the proposed\nBoxeR-2D achieves state-of-the-art results on COCO detection and instance\nsegmentation. Besides, BoxeR-3D improves over the end-to-end 3D object\ndetection baseline and already obtains a compelling performance for the vehicle\ncategory of Waymo Open, without any class-specific optimization. Code is\navailable at https://github.com/kienduynguyen/BoxeR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy-Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jihong Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Booij_O/0/1/0/all/0/1\">Olaf Booij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Inverse Transform Sampling For Efficient Vision Transformers. (arXiv:2111.15667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15667","description":"<p>While state-of-the-art vision transformer models achieve promising results\nfor image classification, they are computationally expensive and require many\nGFLOPs. Although the GFLOPs of a vision transformer can be decreased by\nreducing the number of tokens in the network, there is no setting that is\noptimal for all input images. In this work, we, therefore, introduce a\ndifferentiable parameter-free Adaptive Token Sampling (ATS) module, which can\nbe plugged into any existing vision transformer architecture. ATS empowers\nvision transformers by scoring and adaptively sampling significant tokens. As a\nresult, the number of tokens is not constant anymore and varies for each input\nimage. By integrating ATS as an additional layer within current transformer\nblocks, we can convert them into much more efficient vision transformers with\nan adaptive number of tokens. Since ATS is a parameter-free module, it can be\nadded to off-the-shelf pre-trained vision transformers as a plug-and-play\nmodule, thus reducing their GFLOPs without any additional training. Moreover,\ndue to its differentiable design, one can also train a vision transformer\nequipped with ATS. We evaluate our module on both image and video\nclassification tasks by adding it to multiple SOTA vision transformers. Our\nproposed module improves the SOTA by reducing the computational cost (GFLOPs)\nby 2x while preserving the accuracy of SOTA models on ImageNet, Kinetics-400,\nand Kinetics-600 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_F/0/1/0/all/0/1\">Farnoush Rezaei Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sunando Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joze_H/0/1/0/all/0/1\">Hamid Reza Vaezi Joze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommerlade_E/0/1/0/all/0/1\">Eric Sommerlade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Structured Dictionary Perspective on Implicit Neural Representations. (arXiv:2112.01917v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.01917","description":"<p>Implicit neural representations (INRs) have recently emerged as a promising\nalternative to classical discretized representations of signals. Nevertheless,\ndespite their practical success, we still do not understand how INRs represent\nsignals. We propose a novel unified perspective to theoretically analyse INRs.\nLeveraging results from harmonic analysis and deep learning theory, we show\nthat most INR families are analogous to structured signal dictionaries whose\natoms are integer harmonics of the set of initial mapping frequencies. This\nstructure allows INRs to express signals with an exponentially increasing\nfrequency support using a number of parameters that only grows linearly with\ndepth. We also explore the inductive bias of INRs exploiting recent results\nabout the empirical neural tangent kernel (NTK). Specifically, we show that the\neigenfunctions of the NTK can be seen as dictionary atoms whose inner product\nwith the target signal determines the final performance of their\nreconstruction. In this regard, we reveal that meta-learning has a reshaping\neffect on the NTK analogous to dictionary learning, building dictionary atoms\nas a combination of the examples seen during meta-training. Our results permit\nto design and tune novel INR architectures, but can also be of interest for the\nwider deep learning theory community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuce_G/0/1/0/all/0/1\">Gizem Y&#xfc;ce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_Jimenez_G/0/1/0/all/0/1\">Guillermo Ortiz-Jim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besbinar_B/0/1/0/all/0/1\">Beril Besbinar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-Local Context Network for Person Search. (arXiv:2112.02500v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02500","description":"<p>Person search aims to jointly localize and identify a query person from\nnatural, uncropped images, which has been actively studied in the computer\nvision community over the past few years. In this paper, we delve into the rich\ncontext information globally and locally surrounding the target person, which\nwe refer to scene and group context, respectively. Unlike previous works that\ntreat the two types of context individually, we exploit them in a unified\nglobal-local context network (GLCNet) with the intuitive aim of feature\nenhancement. Specifically, re-ID embeddings and context features are enhanced\nsimultaneously in a multi-stage fashion, ultimately leading to enhanced,\ndiscriminative features for person search. We conduct the experiments on two\nperson search benchmarks (i.e., CUHK-SYSU and PRW) as well as extend our\napproach to a more challenging setting (i.e., character search on MovieNet).\nExtensive experimental results demonstrate the consistent improvement of the\nproposed GLCNet over the state-of-the-art methods on the three datasets. Our\nsource codes, pre-trained models, and the new setting for character search are\navailable at: https://github.com/ZhengPeng7/GLCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaogang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs. (arXiv:2112.02789v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02789","description":"<p>Recent neural human representations can produce high-quality multi-view\nrendering but require using dense multi-view inputs and costly training. They\nare hence largely limited to static models as training each frame is\ninfeasible. We present HumanNeRF - a generalizable neural representation - for\nhigh-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet\nassists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated\npixel-alignment feature across multi-view inputs along with a pose embedded\nnon-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can\nalready produce reasonable rendering on sparse video inputs of unseen subjects\nand camera settings. To further improve the rendering quality, we augment our\nsolution with an appearance blending module for combining the benefits of both\nneural volumetric rendering and neural texture blending. Extensive experiments\non various multi-view dynamic human datasets demonstrate the generalizability\nand effectiveness of our approach in synthesizing photo-realistic free-view\nhumans under challenging motions and with very sparse camera view inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiakai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Pei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover's Distance Improves Out-Of-Distribution Face Identification. (arXiv:2112.04016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04016","description":"<p>Face identification (FI) is ubiquitous and drives many high-stake decisions\nmade by law enforcement. State-of-the-art FI approaches compare two images by\ntaking the cosine similarity between their image embeddings. Yet, such an\napproach suffers from poor out-of-distribution (OOD) generalization to new\ntypes of images (e.g., when a query face is masked, cropped, or rotated) not\nincluded in the training set or the gallery. Here, we propose a re-ranking\napproach that compares two faces using the Earth Mover's Distance on the deep,\nspatial features of image patches. Our extra comparison stage explicitly\nexamines image similarity at a fine-grained level (e.g., eyes to eyes) and is\nmore robust to OOD perturbations and occlusions than traditional FI.\nInterestingly, without finetuning feature extractors, our method consistently\nimproves the accuracy on all tested OOD queries: masked, cropped, rotated, and\nadversarial while obtaining similar results on in-distribution images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hai Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Statistics Mixing Regularization for Generative Adversarial Networks. (arXiv:2112.04120v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04120","description":"<p>In generative adversarial networks, improving discriminators is one of the\nkey components for generation performance. As image classifiers are biased\ntoward texture and debiasing improves accuracy, we investigate 1) if the\ndiscriminators are biased, and 2) if debiasing the discriminators will improve\ngeneration performance. Indeed, we find empirical evidence that the\ndiscriminators are sensitive to the style (e.g., texture and color) of images.\nAs a remedy, we propose feature statistics mixing regularization (FSMR) that\nencourages the discriminator's prediction to be invariant to the styles of\ninput images. Specifically, we generate a mixed feature of an original and a\nreference image in the discriminator's feature space and we apply\nregularization so that the prediction for the mixed feature is consistent with\nthe prediction for the original image. We conduct extensive experiments to\ndemonstrate that our regularization leads to reduced sensitivity to style and\nconsistently improves the performance of various GAN architectures on nine\ndatasets. In addition, adding FSMR to recently-proposed augmentation-based GAN\nmethods further improves image quality. Our code is available at\nhttps://github.com/naver-ai/FSMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1\">Youngjung Uh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning. (arXiv:2112.04731v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04731","description":"<p>Class Incremental Learning (CIL) aims at learning a multi-class classifier in\na phase-by-phase manner, in which only data of a subset of the classes are\nprovided at each phase. Previous works mainly focus on mitigating forgetting in\nphases after the initial one. However, we find that improving CIL at its\ninitial phase is also a promising direction. Specifically, we experimentally\nshow that directly encouraging CIL Learner at the initial phase to output\nsimilar representations as the model jointly trained on all classes can greatly\nboost the CIL performance. Motivated by this, we study the difference between a\nna\\\"ively-trained initial-phase model and the oracle model. Specifically, since\none major difference between these two models is the number of training\nclasses, we investigate how such difference affects the model representations.\nWe find that, with fewer training classes, the data representations of each\nclass lie in a long and narrow region; with more training classes, the\nrepresentations of each class scatter more uniformly. Inspired by this\nobservation, we propose Class-wise Decorrelation (CwD) that effectively\nregularizes representations of each class to scatter more uniformly, thus\nmimicking the model jointly trained with all classes (i.e., the oracle model).\nOur CwD is simple to implement and easy to plug into existing methods.\nExtensive experiments on various benchmark datasets show that CwD consistently\nand significantly improves the performance of existing state-of-the-art methods\nby around 1\\% to 3\\%. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kuangqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Sketch for All: One-Shot Personalized Sketch Segmentation. (arXiv:2112.10838v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10838","description":"<p>We present the first one-shot personalized sketch segmentation method. We aim\nto segment all sketches belonging to the same category provisioned with a\nsingle sketch with a given part annotation while (i) preserving the parts\nsemantics embedded in the exemplar, and (ii) being robust to input style and\nabstraction. We refer to this scenario as personalized. With that, we\nimportantly enable a much-desired personalization capability for downstream\nfine-grained sketch analysis tasks. To train a robust segmentation module, we\ndeform the exemplar sketch to each of the available sketches of the same\ncategory. Our method generalizes to sketches not observed during training. Our\ncentral contribution is a sketch-specific hierarchical deformation network.\nGiven a multi-level sketch-strokes encoding obtained via a graph convolutional\nnetwork, our method estimates rigid-body transformation from the target to the\nexemplar, on the upper level. Finer deformation from the exemplar to the\nglobally warped target sketch is further obtained through stroke-wise\ndeformations, on the lower level. Both levels of deformation are guided by mean\nsquared distances between the keypoints learned without supervision, ensuring\nthat the stroke semantics are preserved. We evaluate our method against the\nstate-of-the-art segmentation and perceptual grouping baselines re-purposed for\nthe one-shot setting and against two few-shot 3D shape segmentation methods. We\nshow that our method outperforms all the alternatives by more than $10\\%$ on\naverage. Ablation studies further demonstrate that our method is robust to\npersonalization: changes in input part semantics and style differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_A/0/1/0/all/0/1\">Anran Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1\">Yulia Gryaditskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02409","description":"<p>Synthetic Aperture Radar (SAR) images are a valuable asset for a wide variety\nof tasks. In the last few years, many websites have been offering them for free\nin the form of easy to manage products, favoring their widespread diffusion and\nresearch work in the SAR field. The drawback of these opportunities is that\nsuch images might be exposed to forgeries and manipulations by malicious users,\nraising new concerns about their integrity and trustworthiness. Up to now, the\nmultimedia forensics literature has proposed various techniques to localize\nmanipulations in natural photographs, but the integrity assessment of SAR\nimages was never investigated. This task poses new challenges, since SAR images\nare generated with a processing chain completely different from that of natural\nphotographs. This implies that many forensics methods developed for natural\nimages are not guaranteed to succeed. In this paper, we investigate the problem\nof amplitude SAR imagery splicing localization. Our goal is to localize regions\nof an amplitude SAR image that have been copied and pasted from another image,\npossibly undergoing some kind of editing in the process. To do so, we leverage\na Convolutional Neural Network (CNN) to extract a fingerprint highlighting\ninconsistencies in the processing traces of the analyzed input. Then, we\nexamine this fingerprint to produce a binary tampering mask indicating the\npixel region under splicing attack. Results show that our proposed method,\ntailored to the nature of SAR signals, provides better performances than\nstate-of-the-art forensic tools developed for natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cannas_E/0/1/0/all/0/1\">Edoardo Daniele Cannas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonettini_N/0/1/0/all/0/1\">Nicol&#xf2; Bonettini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandelli_S/0/1/0/all/0/1\">Sara Mandelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Contrastive Learning is Provably (almost) Principal Component Analysis. (arXiv:2201.12680v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12680","description":"<p>We show that Contrastive Learning (CL) under a family of loss functions\n(including InfoNCE) has a game-theoretical formulation, where the \\emph{max\nplayer} finds representation to maximize contrastiveness, and the \\emph{min\nplayer} puts weights on pairs of samples with similar representation. We show\nthat the max player who does \\emph{representation learning} reduces to\nPrincipal Component Analysis for deep linear network, and almost all local\nminima are global, recovering optimal PCA solutions. Experiments show that the\nformulation yields comparable (or better) performance on CIFAR10 and STL-10\nwhen extending beyond InfoNCE, yielding novel contrastive losses. Furthermore,\nwe extend our theoretical analysis to 2-layer ReLU networks, showing its\ndifference from linear ones, and proving that feature composition is preferred\nover picking single dominant feature under strong augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.07508","description":"<p>In this paper, we tackle the problem of blind image super-resolution(SR) with\na reformulated degradation model and two novel modules. Following the common\npractices of blind SR, our method proposes to improve both the kernel\nestimation as well as the kernel-based high-resolution image restoration. To be\nmore specific, we first reformulate the degradation model such that the\ndeblurring kernel estimation can be transferred into the low-resolution space.\nOn top of this, we introduce a dynamic deep linear filter module. Instead of\nlearning a fixed kernel for all images, it can adaptively generate deblurring\nkernel weights conditional on the input and yield a more robust kernel\nestimation. Subsequently, a deep constrained least square filtering module is\napplied to generate clean features based on the reformulation and estimated\nkernel. The deblurred feature and the low input image feature are then fed into\na dual-path structured SR network and restore the final high-resolution result.\nTo evaluate our method, we further conduct evaluations on several benchmarks,\nincluding Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed\nmethod achieves better accuracy and visual improvements against\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1\">Ziwei Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Object Localization as Domain Adaption. (arXiv:2203.01714v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01714","description":"<p>Weakly supervised object localization (WSOL) focuses on localizing objects\nonly with the supervision of image-level classification masks. Most previous\nWSOL methods follow the classification activation map (CAM) that localizes\nobjects based on the classification structure with the multi-instance learning\n(MIL) mechanism. However, the MIL mechanism makes CAM only activate\ndiscriminative object parts rather than the whole object, weakening its\nperformance for localizing objects. To avoid this problem, this work provides a\nnovel perspective that models WSOL as a domain adaption (DA) task, where the\nscore estimator trained on the source/image domain is tested on the\ntarget/pixel domain to locate objects. Under this perspective, a DA-WSOL\npipeline is designed to better engage DA approaches into WSOL to enhance\nlocalization performance. It utilizes a proposed target sampling strategy to\nselect different types of target samples. Based on these types of target\nsamples, domain adaption localization (DAL) loss is elaborated. It aligns the\nfeature distribution between the two domains by DA and makes the estimator\nperceive target domain cues by Universum regularization. Experiments show that\nour pipeline outperforms SOTA methods on multi benchmarks. Code are released at\n\\url{https://github.com/zh460045050/DA-WSOL_CVPR2022}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yunfei You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01785","description":"<p>Deep neural networks are able to memorize noisy labels easily with a softmax\ncross-entropy (CE) loss. Previous studies attempted to address this issue focus\non incorporating a noise-robust loss function to the CE loss. However, the\nmemorization issue is alleviated but still remains due to the non-robust CE\nloss. To address this issue, we focus on learning robust contrastive\nrepresentations of data on which the classifier is hard to memorize the label\nnoise under the CE loss. We propose a novel contrastive regularization function\nto learn such representations over noisy data where label noise does not\ndominate the representation learning. By theoretically investigating the\nrepresentations induced by the proposed regularization function, we reveal that\nthe learned representations keep information related to true labels and discard\ninformation related to corrupted labels. Moreover, our theoretical results also\nindicate that the learned representations are robust to the label noise. The\neffectiveness of this method is demonstrated with experiments on benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLeod_A/0/1/0/all/0/1\">A. Ian McLeod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network. (arXiv:2203.01824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01824","description":"<p>3D room layout estimation by a single panorama using deep neural networks has\nmade great progress. However, previous approaches can not obtain efficient\ngeometry awareness of room layout with the only latitude of boundaries or\nhorizon-depth. We present that using horizon-depth along with room height can\nobtain omnidirectional-geometry awareness of room layout in both horizontal and\nvertical directions. In addition, we propose a planar-geometry aware loss\nfunction with normals and gradients of normals to supervise the planeness of\nwalls and turning of corners. We propose an efficient network, LGT-Net, for\nroom layout estimation, which contains a novel Transformer architecture called\nSWG-Transformer to model geometry relations. SWG-Transformer consists of\n(Shifted) Window Blocks and Global Blocks to combine the local and global\ngeometry relations. Moreover, we design a novel relative position embedding of\nTransformer to enhance the spatial identification ability for the panorama.\nExperiments show that the proposed LGT-Net achieves better performance than\ncurrent state-of-the-arts (SOTA) on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhigang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhongzheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.02651","description":"<p>Conventional NAS-based pruning algorithms aim to find the sub-network with\nthe best validation performance. However, validation performance does not\nsuccessfully represent test performance, i.e., potential performance. Also,\nalthough fine-tuning the pruned network to restore the performance drop is an\ninevitable process, few studies have handled this issue. This paper proposes a\nnovel sub-network search and fine-tuning method that is named Ensemble\nKnowledge Guidance (EKG). First, we experimentally prove that the fluctuation\nof the loss landscape is an effective metric to evaluate the potential\nperformance. In order to search a sub-network with the smoothest loss landscape\nat a low cost, we propose a pseudo-supernet built by an ensemble sub-network\nknowledge distillation. Next, we propose a novel fine-tuning that re-uses the\ninformation of the search phase. We store the interim sub-networks, that is,\nthe by-products of the search phase, and transfer their knowledge into the\npruned network. Note that EKG is easy to be plugged-in and computationally\nefficient. For example, in the case of ResNet-50, about 45% of FLOPS is removed\nwithout any performance drop in only 315 GPU hours. The implemented code is\navailable at https://github.com/sseung0703/EKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Byung Cheol Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02668","description":"<p>It has been widely known that CAM (Class Activation Map) usually only\nactivates discriminative object regions and falsely includes lots of\nobject-related backgrounds. As only a fixed set of image-level object labels\nare available to the WSSS (weakly supervised semantic segmentation) model, it\ncould be very difficult to suppress those diverse background regions consisting\nof open set objects. In this paper, we propose a novel Cross Language Image\nMatching (CLIMS) framework, based on the recently introduced Contrastive\nLanguage-Image Pre-training (CLIP) model, for WSSS. The core idea of our\nframework is to introduce natural language supervision to activate more\ncomplete object regions and suppress closely-related open background regions.\nIn particular, we design object, background region and text label matching\nlosses to guide the model to excite more reasonable object regions for CAM of\neach category. In addition, we design a co-occurring background suppression\nloss to prevent the model from activating closely-related background regions,\nwith a predefined set of class-related background text descriptions. These\ndesigns enable the proposed CLIMS to generate a more complete and compact\nactivation map for the target objects. Extensive experiments on PASCAL VOC2012\ndataset show that our CLIMS significantly outperforms the previous\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xianxu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04132","description":"<p>Autonomous systems and humans are increasingly sharing the same space. Robots\nwork side by side or even hand in hand with humans to balance each other's\nlimitations. Such cooperative interactions are ever more sophisticated. Thus,\nthe ability to reason not just about a human's center of gravity position, but\nalso its granular motion is an important prerequisite for human-robot\ninteraction. Though, many algorithms ignore the multimodal nature of humans or\nneglect uncertainty in their motion forecasts. We present Motron, a multimodal,\nprobabilistic, graph-structured model, that captures human's multimodality\nusing probabilistic methods while being able to output deterministic\nmaximum-likelihood motions and corresponding confidence values for each mode.\nOur model aims to be tightly integrated with the robotic\nplanning-control-interaction loop; outputting physically feasible human motions\nand being computationally efficient. We demonstrate the performance of our\nmodel on several challenging real-world motion forecasting datasets,\noutperforming a wide array of generative/variational methods while providing\nstate-of-the-art single-output motions if required. Both using significantly\nless computational power than state-of-the art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_T/0/1/0/all/0/1\">Tim Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryll_M/0/1/0/all/0/1\">Markus Ryll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05154","description":"<p>Defense models against adversarial attacks have grown significantly, but the\nlack of practical evaluation methods has hindered progress. Evaluation can be\ndefined as looking for defense models' lower bound of robustness given a budget\nnumber of iterations and a test dataset. A practical evaluation method should\nbe convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and\nreliable (i.e., approaching the lower bound of robustness). Towards this\ntarget, we propose a parameter-free Adaptive Auto Attack (A$^3$) evaluation\nmethod which addresses the efficiency and reliability in a test-time-training\nfashion. Specifically, by observing that adversarial examples to a specific\ndefense model follow some regularities in their starting points, we design an\nAdaptive Direction Initialization strategy to speed up the evaluation.\nFurthermore, to approach the lower bound of robustness under the budget number\nof iterations, we propose an online statistics-based discarding strategy that\nautomatically identifies and abandons hard-to-attack images. Extensive\nexperiments demonstrate the effectiveness of our A$^3$. Particularly, we apply\nA$^3$ to nearly 50 widely-used defense models. By consuming much fewer\niterations than existing methods, i.e., $1/10$ on average (10$\\times$ speed\nup), we achieve lower robust accuracy in all cases. Notably, we won\n$\\textbf{first place}$ out of 1681 teams in CVPR 2021 White-box Adversarial\nAttacks on Defense Models competitions with this method. Code is available at:\n$\\href{https://github.com/liuye6666/adaptive_auto_attack}{https://github.com/liuye6666/adaptive\\_auto\\_attack}$\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance. (arXiv:2203.06749v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06749","description":"<p>In May 2021, the site runnersworld.com published that participation in\nultra-distance races has increased by 1,676% in the last 23 years. Moreover,\nnearly 41% of those runners participate in more than one race per year. The\ndevelopment of wearable devices has undoubtedly contributed to motivating\nparticipants by providing performance measures in real-time. However, we\nbelieve there is room for improvement, particularly from the organizers point\nof view. This work aims to determine how the runners performance can be\nquantified and predicted by considering a non-invasive technique focusing on\nthe ultra-running scenario. In this sense, participants are captured when they\npass through a set of locations placed along the race track. Each footage is\nconsidered an input to an I3D ConvNet to extract the participant's running gait\nin our work. Furthermore, weather and illumination capture conditions or\nocclusions may affect these footages due to the race staff and other runners.\nTo address this challenging task, we have tracked and codified the\nparticipant's running gait at some RPs and removed the context intending to\nensure a runner-of-interest proper evaluation. The evaluation suggests that the\nfeatures extracted by an I3D ConvNet provide enough information to estimate the\nparticipant's performance along the different race tracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freire_Obregon_D/0/1/0/all/0/1\">David Freire-Obreg&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzo_Navarro_J/0/1/0/all/0/1\">Javier Lorenzo-Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castrillon_Santana_M/0/1/0/all/0/1\">Modesto Castrill&#xf3;n-Santana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization. (arXiv:2203.07740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07740","description":"<p>Arbitrary style transfer (AST) and domain generalization (DG) are important\nyet challenging visual learning tasks, which can be cast as a feature\ndistribution matching problem. With the assumption of Gaussian feature\ndistribution, conventional feature distribution matching methods usually match\nthe mean and standard deviation of features. However, the feature distributions\nof real-world data are usually much more complicated than Gaussian, which\ncannot be accurately matched by using only the first-order and second-order\nstatistics, while it is computationally prohibitive to use high-order\nstatistics for distribution matching. In this work, we, for the first time to\nour best knowledge, propose to perform Exact Feature Distribution Matching\n(EFDM) by exactly matching the empirical Cumulative Distribution Functions\n(eCDFs) of image features, which could be implemented by applying the Exact\nHistogram Matching (EHM) in the image feature space. Particularly, a fast EHM\nalgorithm, named Sort-Matching, is employed to perform EFDM in a plug-and-play\nmanner with minimal cost. The effectiveness of our proposed EFDM method is\nverified on a variety of AST and DG tasks, demonstrating new state-of-the-art\nresults. Codes are available at https://github.com/YBZh/EFDM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.08368","description":"<p>The exponentially large discrete search space in mixed-precision quantization\n(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous\nworks usually resort to iterative search methods on the training set, which\nconsume hundreds or even thousands of GPU-hours. In this study, we reveal that\nsome unique learnable parameters in quantization, namely the scale factors in\nthe quantizer, can serve as importance indicators of a layer, reflecting the\ncontribution of that layer to the final accuracy at certain bit-widths. These\nimportance indicators naturally perceive the numerical transformation during\nquantization-aware training, which can precisely and correctly provide\nquantization sensitivity metrics of layers. However, a deep network always\ncontains hundreds of such indicators, and training them one by one would lead\nto an excessive time cost. To overcome this issue, we propose a joint training\nscheme that can obtain all indicators at once. It considerably speeds up the\nindicators training process by parallelizing the original sequential training\nprocesses. With these learned importance indicators, we formulate the MPQ\nsearch problem as a one-time integer linear programming (ILP) problem. That\navoids the iterative search and significantly reduces search time without\nlimiting the bit-width search space. For example, MPQ search on ResNet18 with\nour indicators takes only 0.06 seconds. Also, extensive experiments show our\napproach can achieve SOTA accuracy on ImageNet for far-ranging models with\nvarious constraints (e.g., BitOps, compress rate).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_K/0/1/0/all/0/1\">Kai Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DATA: Domain-Aware and Task-Aware Self-supervised Learning. (arXiv:2203.09041v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09041","description":"<p>The paradigm of training models on massive data without label through\nself-supervised learning (SSL) and finetuning on many downstream tasks has\nbecome a trend recently. However, due to the high training costs and the\nunconsciousness of downstream usages, most self-supervised learning methods\nlack the capability to correspond to the diversities of downstream scenarios,\nas there are various data domains, different vision tasks and latency\nconstraints on models. Neural architecture search (NAS) is one universally\nacknowledged fashion to conquer the issues above, but applying NAS on SSL seems\nimpossible as there is no label or metric provided for judging model selection.\nIn this paper, we present DATA, a simple yet effective NAS approach specialized\nfor SSL that provides Domain-Aware and Task-Aware pre-training. Specifically,\nwe (i) train a supernet which could be deemed as a set of millions of networks\ncovering a wide range of model scales without any label, (ii) propose a\nflexible searching mechanism compatible with SSL that enables finding networks\nof different computation costs, for various downstream vision tasks and data\ndomains without explicit metric provided. Instantiated With MoCo v2, our method\nachieves promising results across a wide range of computation costs on\ndownstream tasks, including image classification, object detection and semantic\nsegmentation. DATA is orthogonal to most existing SSL methods and endows them\nthe ability of customization on downstream needs. Extensive experiments on\nother SSL methods demonstrate the generalizability of the proposed method. Code\nis released at https://github.com/GAIA-vision/GAIA-ssl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qing Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Junran Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxie Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiajun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Haoran Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Perceptual Model for Estimating the Quality of Visual Speech. (arXiv:2203.10117v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.10117","description":"<p>Generating realistic lip motions to simulate speech production is key for\ndriving natural character animations from audio. Previous research has shown\nthat traditional metrics used to optimize and assess models for generating lip\nmotions from speech are not a good indicator of subjective opinion of animation\nquality. Yet, running repetitive subjective studies for assessing the quality\nof animations can be time-consuming and difficult to replicate. In this work,\nwe seek to understand the relationship between perturbed lip motion and\nsubjective opinion of lip motion quality. Specifically, we adjust the degree of\narticulation for lip motion sequences and run a user-study to examine how this\nadjustment impacts the perceived quality of lip motion. We then train a model\nusing the scores collected from our user-study to automatically predict the\nsubjective quality of an animated sequence. Our results show that (1) users\nscore lip motions with slight over-articulation the highest in terms of\nperceptual quality; (2) under-articulation had a more detrimental effect on\nperceived quality of lip motion compared to the effect of over-articulation;\nand (3) we can automatically estimate the subjective perceptual score for a\ngiven lip motion sequences with low error rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldeneh_Z/0/1/0/all/0/1\">Zakaria Aldeneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedzechkina_M/0/1/0/all/0/1\">Masha Fedzechkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seto_S/0/1/0/all/0/1\">Skyler Seto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metcalf_K/0/1/0/all/0/1\">Katherine Metcalf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarabia_M/0/1/0/all/0/1\">Miguel Sarabia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobald_B/0/1/0/all/0/1\">Barry-John Theobald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.11692","description":"<p>This manuscript describes the panoptic segmentation method we devised for our\nsubmission to the CONIC challenge at ISBI 2022. Key features of our method are\na weighted loss that we specifically engineered for semantic segmentation of\nhighly imbalanced cell types, and an existing state-of-the art nuclei instance\nsegmentation model, which we combine in a Hovernet-like architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rumberger_J/0/1/0/all/0/1\">Josef Lorenz Rumberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumann_E/0/1/0/all/0/1\">Elias Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirsch_P/0/1/0/all/0/1\">Peter Hirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainmueller_D/0/1/0/all/0/1\">Dagmar Kainmueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Portrait Delighting. (arXiv:2203.12088v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12088","description":"<p>We present a deep neural network for removing undesirable shading features\nfrom an unconstrained portrait image, recovering the underlying texture. Our\ntraining scheme incorporates three regularization strategies: masked loss, to\nemphasize high-frequency shading features; soft-shadow loss, which improves\nsensitivity to subtle changes in lighting; and shading-offset estimation, to\nsupervise separation of shading and texture. Our method demonstrates improved\ndelighting quality and generalization when compared with the state-of-the-art.\nWe further demonstrate how our delighting method can enhance the performance of\nlight-sensitive computer vision tasks such as face relighting and semantic\nparsing, allowing them to handle extreme lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_J/0/1/0/all/0/1\">Joshua Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalmers_A/0/1/0/all/0/1\">Andrew Chalmers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_T/0/1/0/all/0/1\">Taehyun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. (arXiv:2203.12208v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12208","description":"<p>Recent studies in deepfake detection have yielded promising results when the\ntraining and testing face forgeries are from the same dataset. However, the\nproblem remains challenging when one tries to generalize the detector to\nforgeries created by unseen methods in the training dataset. This work\naddresses the generalizable deepfake detection from a simple principle: a\ngeneralizable representation should be sensitive to diverse types of forgeries.\nFollowing this principle, we propose to enrich the \"diversity\" of forgeries by\nsynthesizing augmented forgeries with a pool of forgery configurations and\nstrengthen the \"sensitivity\" to the forgeries by enforcing the model to predict\nthe forgery configurations. To effectively explore the large forgery\naugmentation space, we further propose to use the adversarial training strategy\nto dynamically synthesize the most challenging forgeries to the current model.\nThrough extensive experiments, we show that the proposed strategies are\nsurprisingly effective (see Figure 1), and they could achieve superior\nperformance than the current state-of-the-art methods. Code is available at\n\\url{https://github.com/liangchen527/SLADD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12818","description":"<p>In this paper we describe our approach to the arousal and valence track of\nthe 3rd Workshop and Competition on Affective Behavior Analysis in-the-wild\n(ABAW). We extracted facial features using OpenFace and used them to train a\nmultiple output random forest regressor. Our approach performed comparable to\nthe baseline approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hinduja_S/0/1/0/all/0/1\">Saurabh Hinduja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canavan_S/0/1/0/all/0/1\">Shaun Canavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jivnani_L/0/1/0/all/0/1\">Liza Jivnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannat_S/0/1/0/all/0/1\">Sk Rahatul Jannat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">V Sri Chakra Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization. (arXiv:2203.12870v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12870","description":"<p>Direct estimating the 6-DoF object pose from a single color image is\nchallenging, and post-refinement is generally needed to achieve high-precision\nestimation. In this paper, we propose a framework based on a recurrent neural\nnetwork (RNN) for object pose refinement, which is robust to erroneous initial\nposes and occlusions. During the recurrent iterations, object pose refinement\nis formulated as a non-linear least squares problem based on the estimated\ncorrespondence field (between a rendered image and the observed image). The\nproblem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm\nfor end-toend training. The correspondence field estimation and pose refinement\nare conducted alternatively in each iteration to recover accurate object poses.\nFurthermore, to improve the robustness to occlusions, we introduce a\nconsistencycheck mechanism based on the learned descriptors of the 3D model and\nobserved 2D image, which downweights the unreliable correspondences during pose\noptimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and\nYCB-Video datasets validate the effectiveness of our method and demonstrate\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kwan-Yee Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13032","description":"<p>In this paper, we briefly introduce our submission to the Valence-Arousal\nEstimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)\ncompetition. Our method utilizes the multi-modal information, i.e., the visual\nand audio information, and employs a temporal encoder to model the temporal\ncontext in the videos. Besides, a smooth processor is applied to get more\nreasonable predictions, and a model ensemble strategy is used to improve the\nperformance of our proposed method. The experiment results show that our method\nachieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation\nset of the Aff-Wild2 dataset, which prove the effectiveness of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Liyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaopei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tenggan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuanyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yannan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1\">Fengsheng Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13052","description":"<p>Facial expression recognition plays an important role in human-computer\ninteraction. In this paper, we propose the Coarse-to-Fine Cascaded network with\nSmooth Predicting (CFC-SP) to improve the performance of facial expression\nrecognition. CFC-SP contains two core components, namely Coarse-to-Fine\nCascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups\nseveral similar emotions to form a rough category, and then employs a network\nto conduct a coarse but accurate classification. Later, an additional network\nfor these grouped emotions is further used to obtain fine-grained predictions.\nFor SP, it improves the recognition capability of the model by capturing both\nuniversal and unique expression features. To be specific, the universal\nfeatures denote the general characteristic of facial emotions within a period\nand the unique features denote the specific characteristic at this moment.\nExperiments on Aff-Wild2 show the effectiveness of the proposed CFSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fanglei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zichang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhongsong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory. (arXiv:2203.13055v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13055","description":"<p>Driving 3D characters to dance following a piece of music is highly\nchallenging due to the spatial constraints applied to poses by choreography\nnorms. In addition, the generated dance sequence also needs to maintain\ntemporal coherency with different music genres. To tackle these challenges, we\npropose a novel music-to-dance framework, Bailando, with two powerful\ncomponents: 1) a choreographic memory that learns to summarize meaningful\ndancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic\nGenerative Pre-trained Transformer (GPT) that composes these units to a fluent\ndance coherent to the music. With the learned choreographic memory, dance\ngeneration is realized on the quantized units that meet high choreography\nstandards, such that the generated dancing sequences are confined within the\nspatial constraints. To achieve synchronized alignment between diverse motion\ntempos and music beats, we introduce an actor-critic-based reinforcement\nlearning scheme to the GPT with a newly-designed beat-align reward function.\nExtensive experiments on the standard benchmark demonstrate that our proposed\nframework achieves state-of-the-art performance both qualitatively and\nquantitatively. Notably, the learned choreographic memory is shown to discover\nhuman-interpretable dancing-style poses in an unsupervised manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siyao_L/0/1/0/all/0/1\">Li Siyao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1\">Tianpei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Preliminary Research on Space Situational Awareness Based on Event Cameras. (arXiv:2203.13093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13093","description":"<p>Event camera is a new type of sensor that is different from traditional\ncameras. Each pixel is triggered asynchronously by an event. The trigger event\nis the change of the brightness irradiated on the pixel. If the increment or\ndecrement is higher than a certain threshold, the event is output. Compared\nwith traditional cameras, event cameras have the advantages of high temporal\nresolution, low latency, high dynamic range, low bandwidth and low power\nconsumption. We carried out a series of observation experiments in a simulated\nspace lighting environment. The experimental results show that the event camera\ncan give full play to the above advantages in space situational awareness. This\narticle first introduces the basic principles of the event camera, then\nanalyzes its advantages and disadvantages, then introduces the observation\nexperiment and analyzes the experimental results, and finally, a workflow of\nspace situational awareness based on event cameras is given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengju Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yongfeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuqiang Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IA-FaceS: A Bidirectional Method for Semantic Face Editing. (arXiv:2203.13097v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13097","description":"<p>Semantic face editing has achieved substantial progress in recent years.\nKnown as a growingly popular method, latent space manipulation performs face\nediting by changing the latent code of an input face to liberate users from\npainting skills. However, previous latent space manipulation methods usually\nencode an entire face into a single low-dimensional embedding, which constrains\nthe reconstruction capacity and the control flexibility of facial components,\nsuch as eyes and nose. This paper proposes IA-FaceS as a bidirectional method\nfor disentangled face attribute manipulation as well as flexible, controllable\ncomponent editing without the need for segmentation masks or sketches in the\noriginal image. To strike a balance between the reconstruction capacity and the\ncontrol flexibility, the encoder is designed as a multi-head structure to yield\nembeddings for reconstruction and control, respectively: a high-dimensional\ntensor with spatial properties for consistent reconstruction and four\nlow-dimensional facial component embeddings for semantic face editing.\nManipulating the separate component embeddings can help achieve disentangled\nattribute manipulation and flexible control of facial components. To further\ndisentangle the highly-correlated components, a component adaptive modulation\n(CAM) module is proposed for the decoder. The semantic single-eye editing is\ndeveloped for the first time without any input visual guidance, such as\nsegmentation masks or sketches. According to the experimental results, IA-FaceS\nestablishes a good balance between maintaining image details and performing\nflexible face manipulation. Both quantitative and qualitative results indicate\nthat the proposed method outperforms the other techniques in reconstruction,\nface attribute manipulation, and component transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Shikui Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation. (arXiv:2203.12917v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2203.12917","description":"<p>We propose WarpingGAN, an effective and efficient 3D point cloud generation\nnetwork. Unlike existing methods that generate point clouds by directly\nlearning the mapping functions between latent codes and 3D shapes, Warping-GAN\nlearns a unified local-warping function to warp multiple identical pre-defined\npriors (i.e., sets of points uniformly distributed on regular 3D grids) into 3D\nshapes driven by local structure-aware semantics. In addition, we also\ningeniously utilize the principle of the discriminator and tailor a stitching\nloss to eliminate the gaps between different partitions of a generated shape\ncorresponding to different priors for boosting quality. Owing to the novel\ngenerating mechanism, WarpingGAN, a single lightweight network after one-time\ntraining, is capable of efficiently generating uniformly distributed 3D point\nclouds with various resolutions. Extensive experimental results demonstrate the\nsuperiority of our WarpingGAN over state-of-the-art methods in terms of\nquantitative metrics, visual quality, and efficiency. The source code is\npublicly available at https://github.com/yztang4/WarpingGAN.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yingzhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yue Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yiming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}