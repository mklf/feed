{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Machine Explanations and Human Understanding. (arXiv:2202.04092v1 [cs.AI])","link":"http://arxiv.org/abs/2202.04092","description":"<p>Explanations are hypothesized to improve human understanding of machine\nlearning models and achieve a variety of desirable outcomes, ranging from model\ndebugging to enhancing human decision making. However, empirical studies have\nfound mixed and even negative results. An open question, therefore, is under\nwhat conditions explanations can improve human understanding and in what way.\nUsing adapted causal diagrams, we provide a formal characterization of the\ninterplay between machine explanations and human understanding, and show how\nhuman intuitions play a central role in enabling human understanding.\nSpecifically, we identify three core concepts of interest that cover all\nexisting quantitative measures of understanding in the context of human-AI\ndecision making: task decision boundary, model decision boundary, and model\nerror. Our key result is that without assumptions about task-specific\nintuitions, explanations may potentially improve human understanding of model\ndecision boundary, but they cannot improve human understanding of task decision\nboundary or model error. To achieve complementary human-AI performance, we\narticulate possible ways on how explanations need to work with human\nintuitions. For instance, human intuitions about the relevance of features\n(e.g., education is more important than age in predicting a person's income)\ncan be critical in detecting model error. We validate the importance of human\nintuitions in shaping the outcome of machine explanations with empirical\nhuman-subject studies. Overall, our work provides a general framework along\nwith actionable implications for future algorithmic development and empirical\nexperiments of machine explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chacha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Reasoning for Task Oriented Dialogue Systems. (arXiv:2202.04161v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04161","description":"<p>In recent years, large pretrained models have been used in dialogue systems\nto improve successful task completion rates. However, lack of reasoning\ncapabilities of dialogue platforms make it difficult to provide relevant and\nfluent responses, unless the designers of a conversational experience spend a\nconsiderable amount of time implementing these capabilities in external rule\nbased modules. In this work, we propose a novel method to fine-tune pretrained\ntransformer models such as Roberta and T5. to reason over a set of facts in a\ngiven dialogue context. Our method includes a synthetic data generation\nmechanism which helps the model learn logical relations, such as comparison\nbetween list of numerical values, inverse relations (and negation), inclusion\nand exclusion for categorical attributes, and application of a combination of\nattributes over both numerical and categorical values, and spoken form for\nnumerical values, without need for additional training dataset. We show that\nthe transformer based model can perform logical reasoning to answer questions\nwhen the dialogue context contains all the required information, otherwise it\nis able to extract appropriate constraints to pass to downstream components\n(e.g. a knowledge base) when partial information is available. We observe that\ntransformer based models such as UnifiedQA-T5 can be fine-tuned to perform\nlogical reasoning (such as numerical and categorical attributes' comparison)\nover attributes that been seen in training time (e.g., accuracy of 90\\%+ for\ncomparison of smaller than $k_{\\max}$=5 values over heldout test dataset).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beygi_S/0/1/0/all/0/1\">Sajjad Beygi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1\">Maryam Fazel-Zarandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1\">Alessandra Cervone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_P/0/1/0/all/0/1\">Prakash Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonnalagadda_S/0/1/0/all/0/1\">Siddhartha Reddy Jonnalagadda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models. (arXiv:2202.04173v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04173","description":"<p>Pre-trained language models (LMs) are shown to easily generate toxic\nlanguage. In this work, we systematically explore domain-adaptive training to\nreduce the toxicity of language models. We conduct this study on three\ndimensions: training corpus, model size, and parameter efficiency. For the\ntraining corpus, we propose to leverage the generative power of LMs and\ngenerate nontoxic datasets for domain-adaptive training, which mitigates the\nexposure bias and is shown to be more data-efficient than using a curated\npre-training corpus. We demonstrate that the self-generation method\nconsistently outperforms the existing baselines across various model sizes on\nboth automatic and human evaluations, even when it uses a 1/3 smaller training\ncorpus. We then comprehensively study detoxifying LMs with parameter sizes\nranging from 126M up to 530B (3x larger than GPT-3), a scale that has never\nbeen studied before. We find that i) large LMs have similar toxicity levels as\nsmaller ones given the same pre-training corpus, and ii) large LMs require more\nendeavor to detoxify. We also explore parameter-efficient training methods for\ndetoxification. We demonstrate that adding and training adapter-only layers in\nLMs not only saves a lot of parameters but also achieves a better trade-off\nbetween toxicity and perplexity than whole model adaptation for the large-scale\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Police Text Analysis: Topic Modeling and Spatial Relative Density Estimation. (arXiv:2202.04176v1 [cs.LG])","link":"http://arxiv.org/abs/2202.04176","description":"<p>We analyze a large corpus of police incident narrative documents in\nunderstanding the spatial distribution of the topics. The motivation for doing\nthis is that police narratives in each incident report contains very\nfine-grained information that is richer than the category that is manually\nassigned by the police. Our approach is to split the corpus into topics using\ntwo different unsupervised machine learning algorithms - Latent Dirichlet\nAllocation and Non-negative Matrix Factorization. We validate the performance\nof each learned topic model using model coherence. Then, using a k-nearest\nneighbors density ratio estimation (kNN-DRE) approach that we propose, we\nestimate the spatial density ratio per topic and use this for data discovery\nand analysis of each topic, allowing for insights into the described incidents\nat scale. We provide a qualitative assessment of each topic and highlight some\nkey benefits for using our kNN-DRE model for estimating spatial trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huestis_Mitchell_S/0/1/0/all/0/1\">Sarah Huestis-Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yao Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Executable Formal Model of the VHDL in Isabelle/HOL. (arXiv:2202.04192v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04192","description":"<p>In the hardware design process, hardware components are usually described in\na hardware description language. Most of the hardware description languages,\nsuch as Verilog and VHDL, do not have mathematical foundation and hence are not\nfit for formal reasoning about the design. To enable formal reasoning in one of\nthe most commonly used description language VHDL, we define a formal model of\nthe VHDL language in Isabelle/HOL. Our model targets the functional part of\nVHDL designs used in industry, specifically the design of the LEON3 processor's\ninteger unit. We cover a wide range of features in the VHDL language that are\nusually not modelled in the literature and define a novel operational semantics\nfor it. Furthermore, our model can be exported to OCaml code for execution,\nturning the formal model into a VHDL simulator. We have tested our simulator\nagainst simple designs used in the literature, as well as the div32 module in\nthe LEON3 design. The Isabelle/HOL code is publicly available:\nhttps://zhehou.github.io/apps/VHDLModel.zip\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_W/0/1/0/all/0/1\">Wilayat Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhe Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanan_D/0/1/0/all/0/1\">David Sanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nebhen_J/0/1/0/all/0/1\">Jamel Nebhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiu_A/0/1/0/all/0/1\">Alwen Tiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Difference Captioning with Pre-training and Contrastive Learning. (arXiv:2202.04298v1 [cs.MM])","link":"http://arxiv.org/abs/2202.04298","description":"<p>The Image Difference Captioning (IDC) task aims to describe the visual\ndifferences between two similar images with natural language. The major\nchallenges of this task lie in two aspects: 1) fine-grained visual differences\nthat require learning stronger vision and language association and 2) high-cost\nof manual annotations that leads to limited supervised data. To address these\nchallenges, we propose a new modeling framework following the\npre-training-finetuning paradigm. Specifically, we design three self-supervised\ntasks and contrastive learning strategies to align visual differences and text\ndescriptions at a fine-grained level. Moreover, we propose a data expansion\nstrategy to utilize extra cross-task supervision information, such as data for\nfine-grained image classification, to alleviate the limitation of available\nsupervised IDC data. Extensive experiments on two IDC benchmark datasets,\nCLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed\nmodeling framework. The codes and models will be released at\nhttps://github.com/yaolinli/IDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Linli Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?. (arXiv:2202.04306v1 [cs.AI])","link":"http://arxiv.org/abs/2202.04306","description":"<p>The task of Outside Knowledge Visual Question Answering (OKVQA) requires an\nautomatic system to answer natural language questions about pictures and images\nusing external knowledge. We observe that many visual questions, which contain\ndeictic referential phrases referring to entities in the image, can be\nrewritten as \"non-grounded\" questions and can be answered by existing\ntext-based question answering systems. This allows for the reuse of existing\ntext-based Open Domain Question Answering (QA) Systems for visual question\nanswering. In this work, we propose a potentially data-efficient approach that\nreuses existing systems for (a) image analysis, (b) question rewriting, and (c)\ntext-based question answering to answer such visual questions. Given an image\nand a question pertaining to that image (a visual question), we first extract\nthe entities present in the image using pre-trained object and scene\nclassifiers. Using these detected entities, the visual questions can be\nrewritten so as to be answerable by open domain QA systems. We explore two\nrewriting strategies: (1) an unsupervised method using BERT for masking and\nrewriting, and (2) a weakly supervised approach that combines adaptive\nrewriting and reinforcement learning techniques to use the implicit feedback\nfrom the QA system. We test our strategies on the publicly available OKVQA\ndataset and obtain a competitive performance with state-of-the-art models while\nusing only 10% of the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Abhijit Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1\">Avinesh P.V.S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwardhan_S/0/1/0/all/0/1\">Siddharth Patwardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sachin Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04350","description":"<p>Large pre-trained language models drastically changed the natural language\nprocessing(NLP) landscape. Nowadays, they represent the go-to framework to\ntackle diverse NLP tasks, even with a limited number of annotations. However,\nusing those models in production, either in the cloud or at the edge, remains a\nchallenge due to the memory footprint and/or inference costs. As an\nalternative, recent work on efficient NLP has shown that small weight-efficient\nmodels can reach competitive performance at a fraction of the costs. Here, we\nintroduce pNLP-Mixer, an embbedding-free model based on the MLP-Mixer\narchitecture that achieves high weight-efficiency thanks to a novel\nlinguistically informed projection layer. We evaluate our model on two\nmulti-lingual semantic parsing datasets, MTOP and multiATIS. On MTOP our\npNLP-Mixer almost matches the performance of mBERT, which has 38 times more\nparameters, and outperforms the state-of-the-art of tiny models (pQRNN) with 3\ntimes fewer parameters. On a long-sequence classification task (Hyperpartisan)\nour pNLP-Mixer without pretraining outperforms RoBERTa, which has 100 times\nmore parameters, demonstrating the potential of this architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1\">Francesco Fusco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascual_D/0/1/0/all/0/1\">Damian Pascual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staar_P/0/1/0/all/0/1\">Peter Staar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Merit-based Fusion of NLP Techniques for Instant Feedback on Water Quality from Twitter Text. (arXiv:2202.04462v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04462","description":"<p>This paper focuses on an important environmental challenge; namely, water\nquality by analyzing the potential of social media as an immediate source of\nfeedback. The main goal of the work is to automatically analyze and retrieve\nsocial media posts relevant to water quality with particular attention to posts\ndescribing different aspects of water quality, such as watercolor, smell,\ntaste, and related illnesses. To this aim, we propose a novel framework\nincorporating different preprocessing, data augmentation, and classification\ntechniques. In total, three different Neural Networks (NNs) architectures,\nnamely (i) Bidirectional Encoder Representations from Transformers (BERT), (ii)\nRobustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and (iii) custom\nLong short-term memory (LSTM) model, are employed in a merit-based fusion\nscheme. For merit-based weight assignment to the models, several optimization\nand search techniques are compared including a Particle Swarm Optimization\n(PSO), a Genetic Algorithm (GA), Brute Force (BF), Nelder-Mead, and Powell's\noptimization methods. We also provide an evaluation of the individual models\nwhere the highest F1-score of 0.81 is obtained with the BERT model. In\nmerit-based fusion, overall better results are obtained with BF achieving an\nF1-score score of 0.852.\n</p>\n<p>We also provide comparison against existing methods, where a significant\nimprovement for our proposed solutions is obtained. We believe such rigorous\nanalysis of this relatively new topic will provide a baseline for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Khubaib Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayub_M/0/1/0/all/0/1\">Muhammad Asif Ayub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_J/0/1/0/all/0/1\">Jebran Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Training Data with Language Models: Towards Zero-Shot Language Understanding. (arXiv:2202.04538v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04538","description":"<p>Pretrained language models (PLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks: Unidirectional PLMs (e.g., GPT) are\nwell known for their superior text generation capabilities; bidirectional PLMs\n(e.g., BERT) have been the prominent choice for natural language understanding\n(NLU) tasks. While both types of models have achieved promising few-shot\nlearning performance, their potential for zero-shot learning has been\nunderexplored. In this paper, we present a simple approach that uses both types\nof PLMs for fully zero-shot learning of NLU tasks without requiring any\ntask-specific data: A unidirectional PLM generates class-conditioned texts\nguided by prompts, which are used as the training data for fine-tuning a\nbidirectional PLM. With quality training data selected based on the generation\nprobability and regularization techniques (label smoothing and temporal\nensembling) applied to the fine-tuning stage for better generalization and\nstability, our approach demonstrates strong performance across seven\nclassification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and\n92.8 on SST-2), significantly outperforming zero-shot prompting methods and\nachieving even comparable results to strong few-shot approaches using 32\ntraining samples per class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations. (arXiv:2202.04582v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04582","description":"<p>Topic models have been the prominent tools for automatic topic discovery from\ntext corpora. Despite their effectiveness, topic models suffer from several\nlimitations including the inability of modeling word ordering information in\ndocuments, the difficulty of incorporating external linguistic knowledge, and\nthe lack of both accurate and efficient inference methods for approximating the\nintractable posterior. Recently, pretrained language models (PLMs) have brought\nastonishing performance improvements to a wide variety of tasks due to their\nsuperior representations of text. Interestingly, there have not been standard\napproaches to deploy PLMs for topic discovery as better alternatives to topic\nmodels. In this paper, we begin by analyzing the challenges of using PLM\nrepresentations for topic discovery, and then propose a joint latent space\nlearning and clustering framework built upon PLM embeddings. In the latent\nspace, topic-word and document-topic distributions are jointly modeled so that\nthe discovered topics can be interpreted by coherent and distinctive terms and\nmeanwhile serve as meaningful summaries of the documents. Our model effectively\nleverages the strong representation power and superb linguistic features\nbrought by PLMs for topic discovery, and is conceptually simpler than topic\nmodels. On two benchmark datasets in different domains, our model generates\nsignificantly more coherent and diverse topics than strong topic models, and\noffers better topic-wise document representations, based on both automatic and\nhuman evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Probabilistic Generative Grammar for Semantic Parsing. (arXiv:1606.06361v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1606.06361","description":"<p>Domain-general semantic parsing is a long-standing goal in natural language\nprocessing, where the semantic parser is capable of robustly parsing sentences\nfrom domains outside of which it was trained. Current approaches largely rely\non additional supervision from new domains in order to generalize to those\ndomains. We present a generative model of natural language utterances and\nlogical forms and demonstrate its application to semantic parsing. Our approach\nrelies on domain-independent supervision to generalize to new domains. We\nderive and implement efficient algorithms for training, parsing, and sentence\ngeneration. The work relies on a novel application of hierarchical Dirichlet\nprocesses (HDPs) for structured prediction, which we also present in this\nmanuscript.\n</p>\n<p>This manuscript is an excerpt of chapter 4 from the Ph.D. thesis of Saparov\n(2022), where the model plays a central role in a larger natural language\nunderstanding system.\n</p>\n<p>This manuscript provides a new simplified and more complete presentation of\nthe work first introduced in Saparov, Saraswat, and Mitchell (2017). The\ndescription and proofs of correctness of the training algorithm, parsing\nalgorithm, and sentence generation algorithm are much simplified in this new\npresentation. We also describe the novel application of hierarchical Dirichlet\nprocesses for structured prediction. In addition, we extend the earlier work\nwith a new model of word morphology, which utilizes the comprehensive\nmorphological data from Wiktionary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1\">Abulhair Saparov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Analyses of Social Biases in Wikipedia Bios. (arXiv:2101.00078v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00078","description":"<p>Social biases on Wikipedia, a widely-read global platform, could greatly\ninfluence public opinion. While prior research has examined man/woman gender\nbias in biography articles, possible influences of other demographic attributes\nlimit conclusions. In this work, we present a methodology for analyzing\nWikipedia pages about people that isolates dimensions of interest (e.g.,\ngender), from other attributes (e.g., occupation). Given a target corpus for\nanalysis (e.g.~biographies about women), we present a method for constructing a\ncomparison corpus that matches the target corpus in as many attributes as\npossible, except the target one. We develop evaluation metrics to measure how\nwell the comparison corpus aligns with the target corpus and then examine how\narticles about gender and racial minorities (cis. women, non-binary people,\ntransgender women, and transgender men; African American, Asian American, and\nHispanic/Latinx American people) differ from other articles. In addition to\nidentifying suspect social biases, our results show that failing to control for\ncovariates can result in different conclusions and veil biases. Our\ncontributions include methodology that facilitates further analyses of bias in\nWikipedia articles, findings that can aid Wikipedia editors in reducing biases,\nand a framework and evaluation metrics to guide future work in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Z. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Neural Coreference Resolution Revisited: A Simple yet Effective Baseline. (arXiv:2107.01700v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01700","description":"<p>Since the first end-to-end neural coreference resolution model was\nintroduced, many extensions to the model have been proposed, ranging from using\nhigher-order inference to directly optimizing evaluation metrics using\nreinforcement learning. Despite improving the coreference resolution\nperformance by a large margin, these extensions add substantial extra\ncomplexity to the original model. Motivated by this observation and the recent\nadvances in pre-trained Transformer language models, we propose a simple yet\neffective baseline for coreference resolution. Even though our model is a\nsimplified version of the original neural coreference resolution model, it\nachieves impressive performance, outperforming all recent extended works on the\npublic English OntoNotes benchmark. Our work provides evidence for the\nnecessity of carefully justifying the complexity of existing or newly proposed\nmodels, as introducing a conceptual or practical simplification to an existing\nmodel can still yield competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doo Soon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acyclic and Cyclic Reversing Computations in Petri Nets. (arXiv:2108.02167v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02167","description":"<p>Reversible computations constitute an unconventional form of computing where\nany sequence of performed operations can be undone by executing in reverse\norder at any point during a computation. It has been attracting increasing\nattention as it provides opportunities for low-power computation, being at the\nsame time essential or eligible in various applications. In recent work, we\nhave proposed a structural way of translating Reversing Petri Nets (RPNs) - a\ntype of Petri nets that embeds reversible computation, to bounded Coloured\nPetri Nets (CPNs) - an extension of traditional Petri Nets, where tokens carry\ndata values. Three reversing semantics are possible in RPNs: backtracking\n(reversing of the lately executed action), causal reversing (action can be\nreversed only when all its effects have been undone) and out of causal\nreversing (any previously performed action can be reversed). In this paper, we\nextend the RPN to CPN translation with formal proofs of correctness. Moreover,\nthe possibility of introduction of cycles to RPNs is discussed. We analyze\nwhich type of cycles could be allowed in RPNs to ensure consistency with the\ncurrent semantics. It emerged that the most interesting case related to cycles\nin RPNs occurs in causal semantics, where various interpretations of dependency\nresult in different net's behaviour during reversing. Three definitions of\ndependence are presented and discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barylska_K/0/1/0/all/0/1\">Kamila Barylska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogolinska_A/0/1/0/all/0/1\">Anna Gogoli&#x144;ska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuned Language Models Are Zero-Shot Learners. (arXiv:2109.01652v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01652","description":"<p>This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n</p>\n<p>We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Y. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calculating Question Similarity is Enough: A New Method for KBQA Tasks. (arXiv:2111.07658v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07658","description":"<p>Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with the help of an external knowledge base. The core idea is to find\nthe link between the internal knowledge behind questions and known triples of\nthe knowledge base. Traditional KBQA task pipelines contain several steps,\nincluding entity recognition, entity linking, answering selection, etc. In this\nkind of pipeline methods, errors in any procedure will inevitably propagate to\nthe final prediction. To address this challenge, this paper proposes a Corpus\nGeneration - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for\nthe KBQA task. The major novelty lies in the design of the new method, wherein\nour approach, the knowledge enhanced T5 (kT5) model aims to generate natural\nlanguage QA pairs based on Knowledge Graph triples and directly solve the QA by\nretrieving the synthetic dataset. The new method can extract more information\nabout the entities from PLM to improve accuracy and simplify the processes. We\ntest our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that\nour method improves the performance of KBQA and the out straight-forward method\nis competitive with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Ledell Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Job Titles from Job Descriptions with Multi-label Text Classification. (arXiv:2112.11052v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11052","description":"<p>Finding a suitable job and hunting for eligible candidates are important to\njob seeking and human resource agencies. With the vast information about job\ndescriptions, employees and employers need assistance to automatically detect\njob titles based on job description texts. In this paper, we propose the\nmulti-label classification approach for predicting relevant job titles from job\ndescription texts, and implement the Bi-GRU-LSTM-CNN with different pre-trained\nlanguage models to apply for the job titles prediction problem. The BERT with\nmultilingual pre-trained model obtains the highest result by F1-scores on both\ndevelopment and test sets, which are 62.20% on the development set, and 47.44%\non the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Trung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Hanh Hong Phuc Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Scaling Laws for Routed Language Models. (arXiv:2202.01169v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01169","description":"<p>The performance of a language model has been shown to be effectively modeled\nas a power-law in its parameter count. Here we study the scaling behaviors of\nRouting Networks: architectures that conditionally use only a subset of their\nparameters while processing an input. For these models, parameter count and\ncomputational requirement form two independent axes along which an increase\nleads to better performance. In this work we derive and justify scaling laws\ndefined on these two variables which generalize those known for standard\nlanguage models and describe the performance of a wide range of routing\narchitectures trained via three different techniques. Afterwards we provide two\napplications of these laws: first deriving an Effective Parameter Count along\nwhich all models scale at the same rate, and then using the scaling\ncoefficients to give a quantitative comparison of the three routing techniques\nconsidered. Our analysis derives from an extensive evaluation of Routing\nNetworks across five orders of magnitude of size, including models with\nhundreds of experts and hundreds of billions of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_A/0/1/0/all/0/1\">Aidan Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Diego de las Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_A/0/1/0/all/0/1\">Aurelia Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paganini_M/0/1/0/all/0/1\">Michela Paganini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damoc_B/0/1/0/all/0/1\">Bogdan Damoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hechtman_B/0/1/0/all/0/1\">Blake Hechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessche_G/0/1/0/all/0/1\">George van den Driessche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Matthew Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassirer_A/0/1/0/all/0/1\">Albin Cassirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Chris Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchatskaya_E/0/1/0/all/0/1\">Elena Buchatskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1\">David Budden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1\">Jack Rae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavukcuoglu_K/0/1/0/all/0/1\">Koray Kavukcuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selecting Seed Words for Wordle using Character Statistics. (arXiv:2202.03457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03457","description":"<p>Wordle, a word guessing game rose to global popularity in the January of\n2022. The goal of the game is to guess a five-letter English word within six\ntries. Each try provides the player with hints by means of colour changing\ntiles which inform whether or not a given character is part of the solution as\nwell as, in cases where it is part of the solution, whether or not it is in the\ncorrect placement. Numerous attempts have been made to find the best starting\nword and best strategy to solve the daily wordle. This study uses character\nstatistics of five-letter words to determine the best three starting words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Multi-Token Fairness in Text Classification. (arXiv:2202.03792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03792","description":"<p>The counterfactual token generation has been limited to perturbing only a\nsingle token in texts that are generally short and single sentences. These\ntokens are often associated with one of many sensitive attributes. With limited\ncounterfactuals generated, the goal to achieve invariant nature for machine\nlearning classification models towards any sensitive attribute gets bounded,\nand the formulation of Counterfactual Fairness gets narrowed. In this paper, we\novercome these limitations by solving root problems and opening bigger domains\nfor understanding. We have curated a resource of sensitive tokens and their\ncorresponding perturbation tokens, even extending the support beyond\ntraditionally used sensitive attributes like Age, Gender, Race to Nationality,\nDisability, and Religion. The concept of Counterfactual Generation has been\nextended to multi-token support valid over all forms of texts and documents. We\ndefine the method of generating counterfactuals by perturbing multiple\nsensitive tokens as Counterfactual Multi-token Generation. The method has been\nconceptualized to showcase significant performance improvement over\nsingle-token methods and validated over multiple benchmark datasets. The\nemendation in counterfactual generation propagates in achieving improved\nCounterfactual Multi-token Fairness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lohia_P/0/1/0/all/0/1\">Pranay Lohia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04003","description":"<p>ROUGE is a standard automatic evaluation metric based on n-grams for\nsequence-to-sequence tasks, while cross-entropy loss is an essential objective\nof neural network language model that optimizes at a unigram level. We present\ndifferentiable n-gram objectives, attempting to alleviate the discrepancy\nbetween training criterion and evaluating criterion. The objective maximizes\nthe probabilistic weight of matched sub-sequences, and the novelty of our work\nis the objective weights the matched sub-sequences equally and does not ceil\nthe number of matched sub-sequences by the ground truth count of n-grams in\nreference sequence. We jointly optimize cross-entropy loss and the proposed\nobjective, providing decent ROUGE score enhancement over abstractive\nsummarization dataset CNN/DM and XSum, outperforming alternative n-gram\nobjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training. (arXiv:2202.03751v1 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2202.03751","description":"<p>Denoising diffusion probabilistic models (diffusion models for short) require\na large number of iterations in inference to achieve the generation quality\nthat matches or surpasses the state-of-the-art generative models, which\ninvariably results in slow inference speed. Previous approaches aim to optimize\nthe choice of inference schedule over a few iterations to speed up inference.\nHowever, this results in reduced generation quality, mainly because the\ninference process is optimized separately, without jointly optimizing with the\ntraining process. In this paper, we propose InferGrad, a diffusion model for\nvocoder that incorporates inference process into training, to reduce the\ninference iterations while maintaining high generation quality. More\nspecifically, during training, we generate data from random noise through a\nreverse process under inference schedules with a few iterations, and impose a\nloss to minimize the gap between the generated and ground-truth data samples.\nThen, unlike existing approaches, the training of InferGrad considers the\ninference process. The advantages of InferGrad are demonstrated through\nexperiments on the LJSpeech dataset showing that InferGrad achieves better\nvoice quality than the baseline WaveGrad under same conditions while\nmaintaining the same voice quality as the baseline but with $3$x speedup ($2$\niterations for InferGrad vs $6$ iterations for WaveGrad).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zehua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_S/0/1/0/all/0/1\">Shifeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandic_D/0/1/0/all/0/1\">Danilo Mandic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Detecting and Localizing Copy-Move and Image-Splicing Forgery. (arXiv:2202.04069v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04069","description":"<p>In the world of fake news and deepfakes, there have been an alarmingly large\nnumber of cases of images being tampered with and published in newspapers, used\nin court, and posted on social media for defamation purposes. Detecting these\ntampered images is an important task and one we try to tackle. In this paper,\nwe focus on the methods to detect if an image has been tampered with using both\nDeep Learning and Image transformation methods and comparing the performances\nand robustness of each method. We then attempt to identify the tampered area of\nthe image and predict the corresponding mask. Based on the results, suggestions\nand approaches are provided to achieve a more robust framework to detect and\nidentify the forgeries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Aditya Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Anshuman Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent gaze information in highly dynamic decision-tasks. (arXiv:2202.04072v1 [cs.HC])","link":"http://arxiv.org/abs/2202.04072","description":"<p>Digitization is penetrating more and more areas of life. Tasks are\nincreasingly being completed digitally, and are therefore not only fulfilled\nfaster, more efficiently but also more purposefully and successfully. The rapid\ndevelopments in the field of artificial intelligence in recent years have\nplayed a major role in this, as they brought up many helpful approaches to\nbuild on. At the same time, the eyes, their movements, and the meaning of these\nmovements are being progressively researched. The combination of these\ndevelopments has led to exciting approaches. In this dissertation, I present\nsome of these approaches which I worked on during my Ph.D.\n</p>\n<p>First, I provide insight into the development of models that use artificial\nintelligence to connect eye movements with visual expertise. This is\ndemonstrated for two domains or rather groups of people: athletes in\ndecision-making actions and surgeons in arthroscopic procedures. The resulting\nmodels can be considered as digital diagnostic models for automatic expertise\nrecognition. Furthermore, I show approaches that investigate the\ntransferability of eye movement patterns to different expertise domains and\nsubsequently, important aspects of techniques for generalization. Finally, I\naddress the temporal detection of confusion based on eye movement data. The\nresults suggest the use of the resulting model as a clock signal for possible\ndigital assistance options in the training of young professionals. An\ninteresting aspect of my research is that I was able to draw on very valuable\ndata from DFB youth elite athletes as well as on long-standing experts in\narthroscopy. In particular, the work with the DFB data attracted the interest\nof radio and print media, namely DeutschlandFunk Nova and SWR DasDing. All\nresulting articles presented here have been published in internationally\nrenowned journals or at conferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosp_B/0/1/0/all/0/1\">Benedikt Hosp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms. (arXiv:2202.04073v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04073","description":"<p>Developing and validating artificial intelligence models in medical imaging\nrequires datasets that are large, granular, and diverse. To date, the majority\nof publicly available breast imaging datasets lack in one or more of these\nareas. Models trained on these data may therefore underperform on patient\npopulations or pathologies that have not previously been encountered. The EMory\nBrEast imaging Dataset (EMBED) addresses these gaps by providing 3650,000 2D\nand DBT screening and diagnostic mammograms for 116,000 women divided equally\nbetween White and African American patients. The dataset also contains 40,000\nannotated lesions linked to structured imaging descriptors and 61 ground truth\npathologic outcomes grouped into six severity classes. Our goal is to share\nthis dataset with research partners to aid in development and validation of\nbreast AI models that will serve all patients fairly and help decrease bias in\nmedical AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeong_J/0/1/0/all/0/1\">Jiwoong J. Jeong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vey_B/0/1/0/all/0/1\">Brianna L. Vey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reddy_A/0/1/0/all/0/1\">Ananth Reddy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_T/0/1/0/all/0/1\">Thomas Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santos_T/0/1/0/all/0/1\">Thiago Santos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Correa_R/0/1/0/all/0/1\">Ramon Correa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dutt_R/0/1/0/all/0/1\">Raman Dutt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mosunjac_M/0/1/0/all/0/1\">Marina Mosunjac</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oprea_Ilies_G/0/1/0/all/0/1\">Gabriela Oprea-Ilies</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_G/0/1/0/all/0/1\">Geoffrey Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_M/0/1/0/all/0/1\">Minjae Woo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McAdams_C/0/1/0/all/0/1\">Christopher R. McAdams</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Newell_M/0/1/0/all/0/1\">Mary S. Newell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Gichoya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_H/0/1/0/all/0/1\">Hari Trivedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-level Contrastive Learning and Consistency Constraint for Semi-supervised Medical Image Segmentation. (arXiv:2202.04074v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04074","description":"<p>Semi-supervised learning (SSL), which aims at leveraging a few labeled images\nand a large number of unlabeled images for network training, is beneficial for\nrelieving the burden of data annotation in medical image segmentation.\nAccording to the experience of medical imaging experts, local attributes such\nas texture, luster and smoothness are very important factors for identifying\ntarget objects like lesions and polyps in medical images. Motivated by this, we\npropose a cross-level constrastive learning scheme to enhance representation\ncapacity for local features in semi-supervised medical image segmentation.\nCompared to existing image-wise, patch-wise and point-wise constrastive\nlearning algorithms, our devised method is capable of exploring more complex\nsimilarity cues, namely the relational characteristics between global\npoint-wise and local patch-wise representations. Additionally, for fully making\nuse of cross-level semantic relations, we devise a novel consistency constraint\nthat compares the predictions of patches against those of the full image. With\nthe help of the cross-level contrastive learning and consistency constraint,\nthe unlabelled data can be effectively explored to improve segmentation\nperformance on two medical image datasets for polyp and skin lesion\nsegmentation respectively. Code of our approach is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xinkai Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">De-Jun Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1\">Xutao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint-bone Fusion Graph Convolutional Network for Semi-supervised Skeleton Action Recognition. (arXiv:2202.04075v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04075","description":"<p>In recent years, graph convolutional networks (GCNs) play an increasingly\ncritical role in skeleton-based human action recognition. However, most\nGCN-based methods still have two main limitations: 1) They only consider the\nmotion information of the joints or process the joints and bones separately,\nwhich are unable to fully explore the latent functional correlation between\njoints and bones for action recognition. 2) Most of these works are performed\nin the supervised learning way, which heavily relies on massive labeled\ntraining data. To address these issues, we propose a semi-supervised\nskeleton-based action recognition method which has been rarely exploited\nbefore. We design a novel correlation-driven joint-bone fusion graph\nconvolutional network (CD-JBF-GCN) as an encoder and use a pose prediction head\nas a decoder to achieve semi-supervised learning. Specifically, the CD-JBF-GC\ncan explore the motion transmission between the joint stream and the bone\nstream, so that promoting both streams to learn more discriminative feature\nrepresentations. The pose prediction based auto-encoder in the self-supervised\ntraining stage allows the network to learn motion representation from unlabeled\ndata, which is essential for action recognition. Extensive experiments on two\npopular datasets, i.e. NTU-RGB+D and Kinetics-Skeleton, demonstrate that our\nmodel achieves the state-of-the-art performance for semi-supervised\nskeleton-based action recognition and is also useful for fully-supervised\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces. (arXiv:2202.04101v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04101","description":"<p>Photoplethysmography (PPG) signals have become a key technology in many\nfields such as medicine, well-being, or sports. Our work proposes a set of\npipelines to extract remote PPG signals (rPPG) from the face, robustly,\nreliably, and in a configurable manner. We identify and evaluate the possible\nchoices in the critical steps of unsupervised rPPG methodologies. We evaluate a\nstate-of-the-art processing pipeline in six different datasets, incorporating\nimportant corrections in the methodology that ensure reproducible and fair\ncomparisons. In addition, we extend the pipeline by proposing three novel\nideas; 1) a new method to stabilize the detected face based on a rigid mesh\nnormalization; 2) a new method to dynamically select the different regions in\nthe face that provide the best raw signals, and 3) a new RGB to rPPG\ntransformation method called Orthogonal Matrix Image Transformation (OMIT)\nbased on QR decomposition, that increases robustness against compression\nartifacts. We show that all three changes introduce noticeable improvements in\nretrieving rPPG signals from faces, obtaining state-of-the-art results compared\nwith unsupervised, non-learning-based methodologies, and in some databases,\nvery close to supervised, learning-based methods. We perform a comparative\nstudy to quantify the contribution of each proposed idea. In addition, we\ndepict a series of observations that could help in future implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casado_C/0/1/0/all/0/1\">Constantino &#xc1;lvarez Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1\">Miguel Bordallo L&#xf3;pez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangle Saliency Detection into Cascaded Detail Modeling and Body Filling. (arXiv:2202.04112v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04112","description":"<p>Salient object detection has been long studied to identify the most visually\nattractive objects in images/videos. Recently, a growing amount of approaches\nhave been proposed all of which rely on the contour/edge information to improve\ndetection performance. The edge labels are either put into the loss directly or\nused as extra supervision. The edge and body can also be learned separately and\nthen fused afterward. Both methods either lead to high prediction errors near\nthe edge or cannot be trained in an end-to-end manner. Another problem is that\nexisting methods may fail to detect objects of various sizes due to the lack of\nefficient and effective feature fusion mechanisms. In this work, we propose to\ndecompose the saliency detection task into two cascaded sub-tasks, \\emph{i.e.},\ndetail modeling and body filling. Specifically, the detail modeling focuses on\ncapturing the object edges by supervision of explicitly decomposed detail label\nthat consists of the pixels that are nested on the edge and near the edge. Then\nthe body filling learns the body part which will be filled into the detail map\nto generate more accurate saliency map. To effectively fuse the features and\nhandle objects at different scales, we have also proposed two novel multi-scale\ndetail attention and body attention blocks for precise detail and body\nmodeling. Experimental results show that our method achieves state-of-the-art\nperformances on six public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Untrimmed Action Anticipation. (arXiv:2202.04132v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04132","description":"<p>Egocentric action anticipation consists in predicting a future action the\ncamera wearer will perform from egocentric video. While the task has recently\nattracted the attention of the research community, current approaches assume\nthat the input videos are \"trimmed\", meaning that a short video sequence is\nsampled a fixed time before the beginning of the action. We argue that, despite\nthe recent advances in the field, trimmed action anticipation has a limited\napplicability in real-world scenarios where it is important to deal with\n\"untrimmed\" video inputs and it cannot be assumed that the exact moment in\nwhich the action will begin is known at test time. To overcome such\nlimitations, we propose an untrimmed action anticipation task, which, similarly\nto temporal action detection, assumes that the input video is untrimmed at test\ntime, while still requiring predictions to be made before the actions actually\ntake place. We design an evaluation procedure for methods designed to address\nthis novel task, and compare several baselines on the EPIC-KITCHENS-100\ndataset. Experiments show that the performance of current models designed for\ntrimmed action anticipation is very limited and more research on this task is\nrequired.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodin_I/0/1/0/all/0/1\">Ivan Rodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavroeidis_D/0/1/0/all/0/1\">Dimitrios Mavroeidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04175","description":"<p>Multi-institutional efforts can facilitate training of deep MRI\nreconstruction models, albeit privacy risks arise during cross-site sharing of\nimaging data. Federated learning (FL) has recently been introduced to address\nprivacy concerns by enabling distributed training without transfer of imaging\ndata. Existing FL methods for MRI reconstruction employ conditional models to\nmap from undersampled to fully-sampled acquisitions via explicit knowledge of\nthe imaging operator. Since conditional models generalize poorly across\ndifferent acceleration rates or sampling densities, imaging operators must be\nfixed between training and testing, and they are typically matched across\nsites. To improve generalization and flexibility in multi-institutional\ncollaborations, here we introduce a novel method for MRI reconstruction based\non Federated learning of Generative IMage Priors (FedGIMP). FedGIMP leverages a\ntwo-stage approach: cross-site learning of a generative MRI prior, and\nsubject-specific injection of the imaging operator. The global MRI prior is\nlearned via an unconditional adversarial model that synthesizes high-quality MR\nimages based on latent variables. Specificity in the prior is preserved via a\nmapper subnetwork that produces site-specific latents. During inference, the\nprior is combined with subject-specific imaging operators to enable\nreconstruction, and further adapted to individual test samples by minimizing\ndata-consistency loss. Comprehensive experiments on multi-institutional\ndatasets clearly demonstrate enhanced generalization performance of FedGIMP\nagainst site-specific and federated methods based on conditional models, as\nwell as traditional reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Elmas_G/0/1/0/all/0/1\">Gokberk Elmas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1\">Salman UH Dar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1\">Yilmaz Korkmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ceyani_E/0/1/0/all/0/1\">Emir Ceyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Susam_B/0/1/0/all/0/1\">Burak Susam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransformNet: Self-supervised representation learning through predicting geometric transformations. (arXiv:2202.04181v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04181","description":"<p>Deep neural networks need a big amount of training data, while in the real\nworld there is a scarcity of data available for training purposes. To resolve\nthis issue unsupervised methods are used for training with limited data. In\nthis report, we describe the unsupervised semantic feature learning approach\nfor recognition of the geometric transformation applied to the input data. The\nbasic concept of our approach is that if someone is unaware of the objects in\nthe images, he/she would not be able to quantitatively predict the geometric\ntransformation that was applied to them. This self supervised scheme is based\non pretext task and the downstream task. The pretext classification task to\nquantify the geometric transformations should force the CNN to learn high-level\nsalient features of objects useful for image classification. In our baseline\nmodel, we define image rotations by multiples of 90 degrees. The CNN trained on\nthis pretext task will be used for the classification of images in the CIFAR-10\ndataset as a downstream task. we run the baseline method using various models,\nincluding ResNet, DenseNet, VGG-16, and NIN with a varied number of rotations\nin feature extracting and fine-tuning settings. In extension of this baseline\nmodel we experiment with transformations other than rotation in pretext task.\nWe compare performance of selected models in various settings with different\ntransformations applied to images,various data augmentation techniques as well\nas using different optimizers. This series of different type of experiments\nwill help us demonstrate the recognition accuracy of our self-supervised model\nwhen applied to a downstream task of classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hashim_S/0/1/0/all/0/1\">Sayed Hashim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Muhammad Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskGIT: Masked Generative Image Transformer. (arXiv:2202.04200v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04200","description":"<p>Generative transformers have experienced rapid popularity growth in the\ncomputer vision community in synthesizing high-fidelity and high-resolution\nimages. The best generative transformer models so far, however, still treat an\nimage naively as a sequence of tokens, and decode an image sequentially\nfollowing the raster scan ordering (i.e. line-by-line). We find this strategy\nneither optimal nor efficient. This paper proposes a novel image synthesis\nparadigm using a bidirectional transformer decoder, which we term MaskGIT.\nDuring training, MaskGIT learns to predict randomly masked tokens by attending\nto tokens in all directions. At inference time, the model begins with\ngenerating all tokens of an image simultaneously, and then refines the image\niteratively conditioned on the previous generation. Our experiments demonstrate\nthat MaskGIT significantly outperforms the state-of-the-art transformer model\non the ImageNet dataset, and accelerates autoregressive decoding by up to 64x.\nBesides, we illustrate that MaskGIT can be easily extended to various image\nediting tasks, such as inpainting, extrapolation, and image manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Event-Based Tracking and Detection for Maritime Environments. (arXiv:2202.04231v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04231","description":"<p>Event cameras are ideal for object tracking applications due to their ability\nto capture fast-moving objects while mitigating latency and data redundancy.\nExisting event-based clustering and feature tracking approaches for\nsurveillance and object detection work well in the majority of cases, but fall\nshort in a maritime environment. Our application of maritime vessel detection\nand tracking requires a process that can identify features and output a\nconfidence score representing the likelihood that the feature was produced by a\nvessel, which may trigger a subsequent alert or activate a classification\nsystem. However, the maritime environment presents unique challenges such as\nthe tendency of waves to produce the majority of events, demanding the majority\nof computational processing and producing false positive detections. By\nfiltering redundant events and analyzing the movement of each event cluster, we\ncan identify and track vessels while ignoring shorter lived and erratic\nfeatures such as those produced by waves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aelmore_S/0/1/0/all/0/1\">Stephanie Aelmore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_R/0/1/0/all/0/1\">Richard C. Ordonez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parameswaran_S/0/1/0/all/0/1\">Shibin Parameswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauger_J/0/1/0/all/0/1\">Justin Mauger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations. (arXiv:2202.04235v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04235","description":"<p>Model robustness against adversarial examples of single perturbation type\nsuch as the $\\ell_{p}$-norm has been widely studied, yet its generalization to\nmore realistic scenarios involving multiple semantic perturbations and their\ncomposition remains largely unexplored. In this paper, we firstly propose a\nnovel method for generating composite adversarial examples. By utilizing\ncomponent-wise projected gradient descent and automatic attack-order\nscheduling, our method can find the optimal attack composition. We then propose\n\\textbf{generalized adversarial training} (\\textbf{GAT}) to extend model\nrobustness from $\\ell_{p}$-norm to composite semantic perturbations, such as\nthe combination of Hue, Saturation, Brightness, Contrast, and Rotation. The\nresults on ImageNet and CIFAR-10 datasets show that GAT can be robust not only\nto any single attack but also to any combination of multiple attacks. GAT also\noutperforms baseline $\\ell_{\\infty}$-norm bounded adversarial training\napproaches by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Yun Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiung_L/0/1/0/all/0/1\">Lei Hsiung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1\">Tsung-Yi Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Robust Convolutional Neural Networks with Relevant Feature Focusing via Explanations. (arXiv:2202.04237v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04237","description":"<p>Existing image recognition techniques based on convolutional neural networks\n(CNNs) basically assume that the training and test datasets are sampled from\ni.i.d distributions. However, this assumption is easily broken in the real\nworld because of the distribution shift that occurs when the co-occurrence\nrelations between objects and backgrounds in input images change. Under this\ntype of distribution shift, CNNs learn to focus on features that are not\ntask-relevant, such as backgrounds from the training data, and degrade their\naccuracy on the test data. To tackle this problem, we propose relevant feature\nfocusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via\nexplanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc\nexplanation modules, it can be easily applied to off-the-shelf CNNs.\nFurthermore, ReFF requires no additional inference cost at test time because it\nis only used for regularization while training. We demonstrate that CNNs\ntrained with ReFF focus on features relevant to the target task and that ReFF\nimproves the test-time accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adachi_K/0/1/0/all/0/1\">Kazuki Adachi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1\">Shin&#x27;ya Yamaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multiscale spatiotemporal approach for smallholder irrigation detection. (arXiv:2202.04239v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04239","description":"<p>In presenting an irrigation detection methodology that leverages multiscale\nsatellite imagery of vegetation abundance, this paper introduces a process to\nsupplement limited ground-collected labels and ensure classifier applicability\nin an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced\nVegetation Index (EVI) timeseries characterizes native vegetation phenologies\nat regional scale to provide the basis for a continuous phenology map that\nguides supplementary label collection over irrigated and non-irrigated\nagriculture. Subsequently, validated dry season greening and senescence cycles\nobserved in 10m Sentinel-2 imagery are used to train a suite of classifiers for\nautomated detection of potential smallholder irrigation. Strategies to improve\nmodel robustness are demonstrated, including a method of data augmentation that\nrandomly shifts training samples; and an assessment of classifier types that\nproduce the best performance in withheld target regions. The methodology is\napplied to detect smallholder irrigation in two states in the Ethiopian\nhighlands, Tigray and Amhara. Results show that a transformer-based neural\nnetwork architecture allows for the most robust prediction performance in\nwithheld regions, followed closely by a CatBoost random forest model. Over\nwithheld ground-collection survey labels, the transformer-based model achieves\n96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated\nsamples. Over a larger set of samples independently collected via the\nintroduced method of label supplementation, non-irrigated and irrigated labels\nare predicted with 98.3% and 95.5% accuracy, respectively. The detection model\nis then deployed over Tigray and Amhara, revealing crop rotation patterns and\nyear-over-year irrigated area change. Predictions suggest that irrigated area\nin these two states has decreased by approximately 40% from 2020 to 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conlon_T/0/1/0/all/0/1\">Terence Conlon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_C/0/1/0/all/0/1\">Christopher Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_V/0/1/0/all/0/1\">Vijay Modi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distillation with Contrast is All You Need for Self-Supervised Point Cloud Representation Learning. (arXiv:2202.04241v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04241","description":"<p>In this paper, we propose a simple and general framework for self-supervised\npoint cloud representation learning. Human beings understand the 3D world by\nextracting two levels of information and establishing the relationship between\nthem. One is the global shape of an object, and the other is the local\nstructures of it. However, few existing studies in point cloud representation\nlearning explored how to learn both global shapes and local-to-global\nrelationships without a specified network architecture. Inspired by how human\nbeings understand the world, we utilize knowledge distillation to learn both\nglobal shape information and the relationship between global shape and local\nstructures. At the same time, we combine contrastive learning with knowledge\ndistillation to make the teacher network be better updated. Our method achieves\nthe state-of-the-art performance on linear classification and multiple other\ndownstream tasks. Especially, we develop a variant of ViT for 3D point cloud\nfeature extraction, which also achieves comparable results with existing\nbackbones when combined with our framework, and visualization of the attention\nmaps show that our model does understand the point cloud by combining the\nglobal shape information and multiple local structural information, which is\nconsistent with the inspiration of our representation learning method. Our code\nwill be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kexue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manning Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-Aware Transformer For Occluded Person Re-identification. (arXiv:2202.04243v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04243","description":"<p>Recently, occluded person re-identification(Re-ID) remains a challenging task\nthat people are frequently obscured by other people or obstacles, especially in\na crowd massing situation. In this paper, we propose a self-supervised deep\nlearning method to improve the location performance for human parts through\noccluded person Re-ID. Unlike previous works, we find that motion information\nderived from the photos of various human postures can help identify major human\nbody components. Firstly, a motion-aware transformer encoder-decoder\narchitecture is designed to obtain keypoints heatmaps and part-segmentation\nmaps. Secondly, an affine transformation module is utilized to acquire motion\ninformation from the keypoint detection branch. Then the motion information\nwill support the segmentation branch to achieve refined human part segmentation\nmaps, and effectively divide the human body into reasonable groups. Finally,\nseveral cases demonstrate the efficiency of the proposed model in\ndistinguishing different representative parts of the human body, which can\navoid the background and occlusion disturbs. Our method consistently achieves\nstate-of-the-art results on several popular datasets, including occluded,\npartial, and holistic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhekun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GiraffeDet: A Heavy-Neck Paradigm for Object Detection. (arXiv:2202.04256v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04256","description":"<p>In conventional object detection frameworks, a backbone body inherited from\nimage recognition models extracts deep latent features and then a neck module\nfuses these latent features to capture information at different scales. As the\nresolution in object detection is much larger than in image recognition, the\ncomputational cost of the backbone often dominates the total inference cost.\nThis heavy-backbone design paradigm is mostly due to the historical legacy when\ntransferring image recognition models to object detection rather than an\nend-to-end optimized design for object detection. In this work, we show that\nsuch paradigm indeed leads to sub-optimal object detection models. To this end,\nwe propose a novel heavy-neck paradigm, GiraffeDet, a giraffe-like network for\nefficient object detection. The GiraffeDet uses an extremely lightweight\nbackbone and a very deep and large neck module which encourages dense\ninformation exchange among different spatial scales as well as different levels\nof latent semantics simultaneously. This design paradigm allows detectors to\nprocess the high-level semantic information and low-level spatial information\nat the same priority even in the early stage of the network, making it more\neffective in detection tasks. Numerical evaluations on multiple popular object\ndetection benchmarks show that GiraffeDet consistently outperforms previous\nSOTA models across a wide spectrum of resource constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Detection without Model Information. (arXiv:2202.04271v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04271","description":"<p>Most prior state-of-the-art adversarial detection works assume that the\nunderlying vulnerable model is accessible, i,e., the model can be trained or\nits outputs are visible. However, this is not a practical assumption due to\nfactors like model encryption, model information leakage and so on. In this\nwork, we propose a model independent adversarial detection method using a\nsimple energy function to distinguish between adversarial and natural inputs.\nWe train a standalone detector independent of the underlying model, with\nsequential layer-wise training to increase the energy separation corresponding\nto natural and adversarial inputs. With this, we perform energy\ndistribution-based adversarial detection. Our method achieves state-of-the-art\ndetection performance (ROC-AUC &gt; 0.9) across a wide range of gradient, score\nand decision-based adversarial attacks on CIFAR10, CIFAR100 and TinyImagenet\ndatasets. Compared to prior approaches, our method requires ~10-100x less\nnumber of operations and parameters for adversarial detection. Further, we show\nthat our detection method is transferable across different datasets and\nadversarial attacks. For reproducibility, we provide code in the supplementary\nmaterial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1\">Abhishek Moitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation. (arXiv:2202.04287v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04287","description":"<p>Open compound domain adaptation (OCDA) has emerged as a practical adaptation\nsetting which considers a single labeled source domain against a compound of\nmulti-modal unlabeled target data in order to generalize better on novel unseen\ndomains. We hypothesize that an improved disentanglement of domain-related and\ntask-related factors of dense intermediate layer features can greatly aid OCDA.\nPrior-arts attempt this indirectly by employing adversarial domain\ndiscriminators on the spatial CNN output. However, we find that latent features\nderived from the Fourier-based amplitude spectrum of deep CNN features hold a\nmore tractable mapping with domain discrimination. Motivated by this, we\npropose a novel feature space Amplitude Spectrum Transformation (AST). During\nadaptation, we employ the AST auto-encoder for two purposes. First, carefully\nmined source-target instance pairs undergo a simulation of cross-domain feature\nstylization (AST-Sim) at a particular layer by altering the AST-latent. Second,\nAST operating at a later layer is tasked to normalize (AST-Norm) the domain\ncontent by fixing its latent to a mean prototype. Our simplified adaptation\ntechnique is not only clustering-free but also free from complex adversarial\nalignment. We achieve leading performance against the prior arts on the OCDA\nscene segmentation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_J/0/1/0/all/0/1\">Jogendra Nath Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Akshay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1\">Suvaansh Bhambri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Bootstrap for Combating Label Noise. (arXiv:2202.04291v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04291","description":"<p>Deep neural networks are powerful tools for representation learning, but can\neasily overfit to noisy labels which are prevalent in many real-world\nscenarios. Generally, noisy supervision could stem from variation among\nlabelers, label corruption by adversaries, etc. To combat such label noises,\none popular line of approach is to apply customized weights to the training\ninstances, so that the corrupted examples contribute less to the model\nlearning. However, such learning mechanisms potentially erase important\ninformation about the data distribution and therefore yield suboptimal results.\nTo leverage useful information from the corrupted instances, an alternative is\nthe bootstrapping loss, which reconstructs new training targets on-the-fly by\nincorporating the network's own predictions (i.e., pseudo-labels).\n</p>\n<p>In this paper, we propose a more generic learnable loss objective which\nenables a joint reweighting of instances and labels at once. Specifically, our\nmethod dynamically adjusts the per-sample importance weight between the real\nobserved labels and pseudo-labels, where the weights are efficiently determined\nin a meta process. Compared to the previous instance reweighting methods, our\napproach concurrently conducts implicit relabeling, and thereby yield\nsubstantial improvements with almost no extra cost. Extensive experimental\nresults demonstrated the strengths of our approach over existing methods on\nmultiple natural and medical image benchmark datasets, including CIFAR-10,\nCIFAR-100, ISIC2019 and Clothing 1M. The code is publicly available at\nhttps://github.com/yuyinzhou/L2B.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Difference Captioning with Pre-training and Contrastive Learning. (arXiv:2202.04298v1 [cs.MM])","link":"http://arxiv.org/abs/2202.04298","description":"<p>The Image Difference Captioning (IDC) task aims to describe the visual\ndifferences between two similar images with natural language. The major\nchallenges of this task lie in two aspects: 1) fine-grained visual differences\nthat require learning stronger vision and language association and 2) high-cost\nof manual annotations that leads to limited supervised data. To address these\nchallenges, we propose a new modeling framework following the\npre-training-finetuning paradigm. Specifically, we design three self-supervised\ntasks and contrastive learning strategies to align visual differences and text\ndescriptions at a fine-grained level. Moreover, we propose a data expansion\nstrategy to utilize extra cross-task supervision information, such as data for\nfine-grained image classification, to alleviate the limitation of available\nsupervised IDC data. Extensive experiments on two IDC benchmark datasets,\nCLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed\nmodeling framework. The codes and models will be released at\nhttps://github.com/yaolinli/IDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Linli Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?. (arXiv:2202.04306v1 [cs.AI])","link":"http://arxiv.org/abs/2202.04306","description":"<p>The task of Outside Knowledge Visual Question Answering (OKVQA) requires an\nautomatic system to answer natural language questions about pictures and images\nusing external knowledge. We observe that many visual questions, which contain\ndeictic referential phrases referring to entities in the image, can be\nrewritten as \"non-grounded\" questions and can be answered by existing\ntext-based question answering systems. This allows for the reuse of existing\ntext-based Open Domain Question Answering (QA) Systems for visual question\nanswering. In this work, we propose a potentially data-efficient approach that\nreuses existing systems for (a) image analysis, (b) question rewriting, and (c)\ntext-based question answering to answer such visual questions. Given an image\nand a question pertaining to that image (a visual question), we first extract\nthe entities present in the image using pre-trained object and scene\nclassifiers. Using these detected entities, the visual questions can be\nrewritten so as to be answerable by open domain QA systems. We explore two\nrewriting strategies: (1) an unsupervised method using BERT for masking and\nrewriting, and (2) a weakly supervised approach that combines adaptive\nrewriting and reinforcement learning techniques to use the implicit feedback\nfrom the QA system. We test our strategies on the publicly available OKVQA\ndataset and obtain a competitive performance with state-of-the-art models while\nusing only 10% of the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Abhijit Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1\">Avinesh P.V.S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwardhan_S/0/1/0/all/0/1\">Siddharth Patwardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sachin Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Motion In-betweening. (arXiv:2202.04307v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04307","description":"<p>Motion in-betweening (MIB) is a process of generating intermediate skeletal\nmovement between the given start and target poses while preserving the\nnaturalness of the motion, such as periodic footstep motion while walking.\nAlthough state-of-the-art MIB methods are capable of producing plausible\nmotions given sparse key-poses, they often lack the controllability to generate\nmotions satisfying the semantic contexts required in practical applications. We\nfocus on the method that can handle pose or semantic conditioned MIB tasks\nusing a unified model. We also present a motion augmentation method to improve\nthe quality of pose-conditioned motion generation via defining a distribution\nover smooth trajectories. Our proposed method outperforms the existing\nstate-of-the-art MIB method in pose prediction errors while providing\nadditional controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_T/0/1/0/all/0/1\">Taehyun Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungyoun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_J/0/1/0/all/0/1\">Jungdam Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungjoon Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor Graph Structure Fusion Hashing for Cross-Modal Similarity Search. (arXiv:2202.04327v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04327","description":"<p>Cross-modal hashing still has some challenges needed to address: (1) most\nexisting CMH methods take graphs as input to model data distribution. These\nmethods omit to consider the correlation of graph structure among multiple\nmodalities; (2) most existing CMH methods ignores considering the fusion\naffinity among multi-modalities data; (3) most existing CMH methods relax the\ndiscrete constraints to solve the optimization objective, significantly\ndegrading the retrieval performance. To solve the above limitations, we propose\na novel Anchor Graph Structure Fusion Hashing (AGSFH). AGSFH constructs the\nanchor graph structure fusion matrix from different anchor graphs of multiple\nmodalities with the Hadamard product, which can fully exploit the geometric\nproperty of underlying data structure. Based on the anchor graph structure\nfusion matrix, AGSFH attempts to directly learn an intrinsic anchor graph,\nwhere the structure of the intrinsic anchor graph is adaptively tuned so that\nthe number of components of the intrinsic graph is exactly equal to the number\nof clusters. Besides, AGSFH preserves the anchor fusion affinity into the\ncommon binary Hamming space. Furthermore, a discrete optimization framework is\ndesigned to learn the unified binary codes. Extensive experimental results on\nthree public social datasets demonstrate the superiority of AGSFH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1\">Masoumeh Zareapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhonglong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Feature Rotation for Multimodal Image Style Transfer. (arXiv:2202.04426v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04426","description":"<p>Recently, style transfer is a research area that attracts a lot of attention,\nwhich transfers the style of an image onto a content target. Extensive research\non style transfer has aimed at speeding up processing or generating\nhigh-quality stylized images. Most approaches only produce an output from a\ncontent and style image pair, while a few others use complex architectures and\ncan only produce a certain number of outputs. In this paper, we propose a\nsimple method for representing style features in many ways called Deep Feature\nRotation (DFR), while not only producing diverse outputs but also still\nachieving effective stylization compared to more complex methods. Our approach\nis representative of the many ways of augmentation for intermediate feature\nembedding without consuming too much computational expense. We also analyze our\nmethod by visualizing output in different rotation weights. Our code is\navailable at https://github.com/sonnguyen129/deep-feature-rotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Son Truong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuyen_N/0/1/0/all/0/1\">Nguyen Quang Tuyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phuc_N/0/1/0/all/0/1\">Nguyen Hong Phuc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Guided Day-Night Visual Localization in Urban Scenes. (arXiv:2202.04445v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04445","description":"<p>We introduce Object-Guided Localization (OGuL) based on a novel method of\nlocal-feature matching. Direct matching of local features is sensitive to\nsignificant changes in illumination. In contrast, object detection often\nsurvives severe changes in lighting conditions. The proposed method first\ndetects semantic objects and establishes correspondences of those objects\nbetween images. Object correspondences provide local coarse alignment of the\nimages in the form of a planar homography. These homographies are consequently\nused to guide the matching of local features. Experiments on standard urban\nlocalization datasets (Aachen, Extended-CMU-Season, RobotCar-Season) show that\nOGuL significantly improves localization results with as simple local features\nas SIFT, and its performance competes with the state-of-the-art CNN-based\nmethods trained for day-to-night localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benbihi_A/0/1/0/all/0/1\">Assia Benbihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradalier_C/0/1/0/all/0/1\">C&#xe9;dric Pradalier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ond&#x159;ej Chum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the intended action using internal simulation of perception. (arXiv:2202.04466v1 [cs.AI])","link":"http://arxiv.org/abs/2202.04466","description":"<p>This article proposes an architecture, which allows the prediction of\nintention by internally simulating perceptual states represented by action\npattern vectors. To this end, associative self-organising neural networks\n(A-SOM) is utilised to build a hierarchical cognitive architecture for\nrecognition and simulation of the skeleton based human actions. The abilities\nof the proposed architecture in recognising and predicting actions is evaluated\nin experiments using three different datasets of 3D actions. Based on the\nexperiments of this article, applying internally simulated perceptual states\nrepresented by action pattern vectors improves the performance of the\nrecognition task in all experiments. Furthermore, internal simulation of\nperception addresses the problem of having limited access to the sensory input,\nand also the future prediction of the consecutive perceptual sequences. The\nperformance of the system is compared and discussed with similar architecture\nusing self-organizing neural networks (SOM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRAT-Pred: Vehicle Trajectory Prediction with Crystal Graph Convolutional Neural Networks and Multi-Head Self-Attention. (arXiv:2202.04488v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04488","description":"<p>Predicting the motion of surrounding vehicles is essential for autonomous\nvehicles, as it governs their own motion plan. Current state-of-the-art vehicle\nprediction models heavily rely on map information. In reality, however, this\ninformation is not always available. We therefore propose CRAT-Pred, a\nmulti-modal and non-rasterization-based trajectory prediction model,\nspecifically designed to effectively model social interactions between\nvehicles, without relying on map information. CRAT-Pred applies a graph\nconvolution method originating from the field of material science to vehicle\nprediction, allowing to efficiently leverage edge features, and combines it\nwith multi-head self-attention. Compared to other map-free approaches, the\nmodel achieves state-of-the-art performance with a significantly lower number\nof model parameters. In addition to that, we quantitatively show that the\nself-attention mechanism is able to learn social interactions between vehicles,\nwith the weights representing a measurable interaction score. The source code\nis publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1\">Julian Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_J/0/1/0/all/0/1\">Julian Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritschneder_F/0/1/0/all/0/1\">Franz Gritschneder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Blind Quality Assessment for Laparoscopic Videos using Neural Networks. (arXiv:2202.04517v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04517","description":"<p>Video quality assessment is a challenging problem having a critical\nsignificance in the context of medical imaging. For instance, in laparoscopic\nsurgery, the acquired video data suffers from different kinds of distortion\nthat not only hinder surgery performance but also affect the execution of\nsubsequent tasks in surgical navigation and robotic surgeries. For this reason,\nwe propose in this paper neural network-based approaches for distortion\nclassification as well as quality prediction. More precisely, a Residual\nNetwork (ResNet) based approach is firstly developed for simultaneous ranking\nand classification task. Then, this architecture is extended to make it\nappropriate for the quality prediction task by using an additional Fully\nConnected Neural Network (FCNN). To train the overall architecture (ResNet and\nFCNN models), transfer learning and end-to-end learning approaches are\ninvestigated. Experimental results, carried out on a new laparoscopic video\nquality database, have shown the efficiency of the proposed methods compared to\nrecent conventional and deep learning based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_Z/0/1/0/all/0/1\">Zohaib Amjad Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beghdadi_A/0/1/0/all/0/1\">Azeddine Beghdadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaaniche_M/0/1/0/all/0/1\">Mounir Kaaniche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheikh_F/0/1/0/all/0/1\">Faouzi Alaya Cheikh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gharbi_O/0/1/0/all/0/1\">Osama Gharbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04533","description":"<p>Emerging Metaverse applications demand reliable, accurate, and photorealistic\nreproductions of human hands to perform sophisticated operations as if in the\nphysical world. While real human hand represents one of the most intricate\ncoordination between bones, muscle, tendon, and skin, state-of-the-art\ntechniques unanimously focus on modeling only the skeleton of the hand. In this\npaper, we present NIMBLE, a novel parametric hand model that includes the\nmissing key components, bringing 3D hand model to a new level of realism. We\nfirst annotate muscles, bones and skins on the recent Magnetic Resonance\nImaging hand (MRI-Hand) dataset and then register a volumetric template hand\nonto individual poses and subjects within the dataset. NIMBLE consists of 20\nbones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin\nmesh. Via iterative shape registration and parameter learning, it further\nproduces shape blend shapes, pose blend shapes, and a joint regressor. We\ndemonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.\nBy enforcing the inner bones and muscles to match anatomic and kinematic rules,\nNIMBLE can animate 3D hands to new poses at unprecedented realism. To model the\nappearance of skin, we further construct a photometric HandStage to acquire\nhigh-quality textures and normal maps to model wrinkles and palm print.\nFinally, NIMBLE also benefits learning-based hand pose and shape estimation by\neither synthesizing rich data or acting directly as a differentiable layer in\nthe inference network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zesong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingwenqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04595","description":"<p>Neural image compression have reached or out-performed traditional methods\n(such as JPEG, BPG, WebP). However,their sophisticated network structures with\ncascaded convolution layers bring heavy computational burden for practical\ndeployment. In this paper, we explore the structural sparsity in neural image\ncompression network to obtain real-time acceleration without any specialized\nhardware design or algorithm. We propose a simple plug-in adaptive binary\nchannel masking(ABCM) to judge the importance of each convolution channel and\nintroduce sparsity during training. During inference, the unimportant channels\nare pruned to obtain slimmer network and less computation. We implement our\nmethod into three neural image compression networks with different entropy\nmodels to verify its effectiveness and generalization, the experiment results\nshow that up to 7x computation reduction and 3x acceleration can be achieved\nwith negligible performance drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yin_S/0/1/0/all/0/1\">Shanzhi Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1\">Fanyang Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Wen Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Youneng Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yongsheng Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distance Estimation and Animal Tracking for Wildlife Camera Trapping. (arXiv:2202.04613v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04613","description":"<p>The ongoing biodiversity crysis calls for accurate estimation of animal\ndensity and abundance to identify, for example, sources of biodiversity decline\nand effectiveness of conservation interventions. Camera traps together with\nabundance estimation methods are often employed for this purpose. The necessary\ndistances between camera and observed animal are traditionally derived in a\nlaborious, fully manual or semi-automatic process. Both approaches require\nreference image material, which is both difficult to acquire and not available\nfor existing datasets. In this study, we propose a fully automatic approach to\nestimate camera-to-animal distances, based on monocular depth estimation (MDE),\nand without the need of reference image material. We leverage state-of-the-art\nrelative MDE and a novel alignment procedure to estimate metric distances. We\nevaluate the approach on a zoo scenario dataset unseen during training. We\nachieve a mean absolute distance estimation error of only 0.9864 meters at a\nprecision of 90.3% and recall of 63.8%, while completely eliminating the\npreviously required manual effort for biodiversity researchers. The code will\nbe made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johanns_P/0/1/0/all/0/1\">Peter Johanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haucke_T/0/1/0/all/0/1\">Timm Haucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhage_V/0/1/0/all/0/1\">Volker Steinhage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Redundancy in the Bottleneck Representation of the Autoencoders. (arXiv:2202.04629v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04629","description":"<p>Autoencoders are a type of unsupervised neural networks, which can be used to\nsolve various tasks, e.g., dimensionality reduction, image compression, and\nimage denoising. An AE has two goals: (i) compress the original input to a\nlow-dimensional space at the bottleneck of the network topology using an\nencoder, (ii) reconstruct the input from the representation at the bottleneck\nusing a decoder. Both encoder and decoder are optimized jointly by minimizing a\ndistortion-based loss which implicitly forces the model to keep only those\nvariations of input data that are required to reconstruct the and to reduce\nredundancies. In this paper, we propose a scheme to explicitly penalize feature\nredundancies in the bottleneck representation. To this end, we propose an\nadditional loss term, based on the pair-wise correlation of the neurons, which\ncomplements the standard reconstruction loss forcing the encoder to learn a\nmore diverse and richer representation of the input. We tested our approach\nacross different tasks: dimensionality reduction using three different dataset,\nimage compression using the MNIST dataset, and image denoising using fashion\nMNIST. The experimental results show that the proposed loss leads consistently\nto superior performance compared to the standard AE loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laakom_F/0/1/0/all/0/1\">Firas Laakom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1\">Jenni Raitoharju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-Level Region Contrast for Object Detection Pre-Training. (arXiv:2202.04639v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04639","description":"<p>In this work we present point-level region contrast, a self-supervised\npre-training approach for the task of object detection. This approach is\nmotivated by the two key factors in detection: localization and recognition.\nWhile accurate localization favors models that operate at the pixel- or\npoint-level, correct recognition typically relies on a more holistic,\nregion-level view of objects. Incorporating this perspective in pre-training,\nour approach performs contrastive learning by directly sampling individual\npoint pairs from different regions. Compared to an aggregated representation\nper region, our approach is more robust to the change in input region quality,\nand further enables us to implicitly improve initial region assignments via\nonline knowledge distillation during training. Both advantages are important\nwhen dealing with imperfect regions encountered in the unsupervised setting.\nExperiments show point-level region contrast improves on state-of-the-art\npre-training methods for object detection and segmentation across multiple\ntasks and datasets, and we provide extensive ablation studies and\nvisualizations to aid understanding. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yutong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributionally Robust Deep Learning using Hardness Weighted Sampling. (arXiv:2001.02658v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2001.02658","description":"<p>Limiting failures of machine learning systems is of paramount importance for\nsafety-critical applications. In order to improve the robustness of machine\nlearning systems, Distributionally Robust Optimization (DRO) has been proposed\nas a generalization of Empirical Risk Minimization (ERM). However, its use in\ndeep learning has been severely restricted due to the relative inefficiency of\nthe optimizers available for DRO in comparison to the wide-spread variants of\nStochastic Gradient Descent (SGD) optimizers for ERM. We propose SGD with\nhardness weighted sampling, a principled and efficient optimization method for\nDRO in machine learning that is particularly suited in the context of deep\nlearning. Similar to a hard example mining strategy in practice, the proposed\nalgorithm is straightforward to implement and computationally as efficient as\nSGD-based optimizers used for deep learning, requiring minimal overhead\ncomputation. In contrast to typical ad hoc hard mining approaches, we prove the\nconvergence of our DRO algorithm for over-parameterized deep learning networks\nwith ReLU activation and a finite number of layers and parameters. Our\nexperiments on fetal brain 3D MRI segmentation and brain tumor segmentation in\nMRI demonstrate the feasibility and the usefulness of our approach. Using our\nhardness weighted sampling for training a state-of-the-art deep learning\npipeline leads to improved robustness to anatomical variabilities in automatic\nfetal brain 3D MRI segmentation using deep learning and to improved robustness\nto the image protocol variations in brain tumor segmentation. Our code is\navailable at https://github.com/LucasFidon/HardnessWeightedSampler.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deprest_T/0/1/0/all/0/1\">Thomas Deprest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emam_D/0/1/0/all/0/1\">Doaa Emam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guffens_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Guffens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mufti_N/0/1/0/all/0/1\">Nada Mufti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elslander_E/0/1/0/all/0/1\">Esther Van Elslander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1\">Ernst Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebner_M/0/1/0/all/0/1\">Michael Ebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prayer_D/0/1/0/all/0/1\">Daniela Prayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasprian_G/0/1/0/all/0/1\">Gregor Kasprian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melbourne_A/0/1/0/all/0/1\">Andrew Melbourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langs_G/0/1/0/all/0/1\">Georg Langs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling possible reconstructions of undersampled acquisitions in MR imaging. (arXiv:2010.00042v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.00042","description":"<p>Undersampling the k-space during MR acquisitions saves time, however results\nin an ill-posed inversion problem, leading to an infinite set of images as\npossible solutions. Traditionally, this is tackled as a reconstruction problem\nby searching for a single \"best\" image out of this solution set according to\nsome chosen regularization or prior. This approach, however, misses the\npossibility of other solutions and hence ignores the uncertainty in the\ninversion process. In this paper, we propose a method that instead returns\nmultiple images which are possible under the acquisition model and the chosen\nprior to capture the uncertainty in the inversion process. To this end, we\nintroduce a low dimensional latent space and model the posterior distribution\nof the latent vectors given the acquisition data in k-space, from which we can\nsample in the latent space and obtain the corresponding images. We use a\nvariational autoencoder for the latent model and the Metropolis adjusted\nLangevin algorithm for the sampling. We evaluate our method on two datasets;\nwith images from the Human Connectome Project and in-house measured multi-coil\nimages. We compare to five alternative methods. Results indicate that the\nproposed method produces images that match the measured k-space data better\nthan the alternatives, while showing realistic structural variability.\nFurthermore, in contrast to the compared methods, the proposed method yields\nhigher uncertainty in the undersampled phase encoding direction, as expected.\n</p>\n<p>Keywords: Magnetic Resonance image reconstruction, uncertainty estimation,\ninverse problems, sampling, MCMC, deep learning, unsupervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tezcan_K/0/1/0/all/0/1\">Kerem C. Tezcan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karani_N/0/1/0/all/0/1\">Neerav Karani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumgartner_C/0/1/0/all/0/1\">Christian F. Baumgartner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural Networks. (arXiv:2010.09343v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.09343","description":"<p>Recent learning-based LiDAR odometry methods have demonstrated their\ncompetitiveness. However, most methods still face two substantial challenges:\n1) the 2D projection representation of LiDAR data cannot effectively encode 3D\nstructures from the point clouds; 2) the needs for a large amount of labeled\ndata for training limit the application scope of these methods. In this paper,\nwe propose a self-supervised LiDAR odometry method, dubbed SelfVoxeLO, to\ntackle these two difficulties. Specifically, we propose a 3D convolution\nnetwork to process the raw LiDAR data directly, which extracts features that\nbetter encode the 3D geometric patterns. To suit our network to self-supervised\nlearning, we design several novel loss functions that utilize the inherent\nproperties of LiDAR point clouds. Moreover, an uncertainty-aware mechanism is\nincorporated in the loss functions to alleviate the interference of moving\nobjects/noises. We evaluate our method's performances on two large-scale\ndatasets, i.e., KITTI and Apollo-SouthBay. Our method outperforms\nstate-of-the-art unsupervised methods by 27%/32% in terms of\ntranslational/rotational errors on the KITTI dataset and also performs well on\nthe Apollo-SouthBay dataset. By including more unlabelled training data, our\nmethod can further improve performance comparable to the supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kwan-Yee Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Composition Net for Visual Relationship Prediction. (arXiv:2012.05473v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05473","description":"<p>We present a novel Tensor Composition Net (TCN) to predict visual\nrelationships in images. Visual Relationship Prediction (VRP) provides a more\nchallenging test of image understanding than conventional image tagging and is\ndifficult to learn due to a large label-space and incomplete annotation. The\nkey idea of our TCN is to exploit the low-rank property of the visual\nrelationship tensor, so as to leverage correlations within and across objects\nand relations and make a structured prediction of all visual relationships in\nan image. To show the effectiveness of our model, we first empirically compare\nour model with Multi-Label Image Classification (MLIC) methods, eXtreme\nMulti-label Classification (XMC) methods, and VRD methods. We then show that\nthanks to our tensor (de)composition layer, our model can predict visual\nrelationships which have not been seen in the training dataset. We finally show\nour TCN's image-level visual relationship prediction provides a simple and\nefficient mechanism for relation-based image-retrieval even compared with VRD\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_Y/0/1/0/all/0/1\">Yuting Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced softmax cross-entropy for incremental learning with and without memory. (arXiv:2103.12532v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.12532","description":"<p>When incrementally trained on new classes, deep neural networks are subject\nto catastrophic forgetting which leads to an extreme deterioration of their\nperformance on the old classes while learning the new ones. Using a small\nmemory containing few samples from past classes has shown to be an effective\nmethod to mitigate catastrophic forgetting. However, due to the limited size of\nthe replay memory, there is a large imbalance between the number of samples for\nthe new and the old classes in the training dataset resulting in bias in the\nfinal model. To address this issue, we propose to use the Balanced Softmax\nCross-Entropy and show that it can be seamlessly combined with state-of-the-art\napproaches for class-incremental learning in order to improve their accuracy\nwhile also potentially decreasing the computational cost of the training\nprocedure. We further extend this approach to the more demanding\nclass-incremental learning without memory setting and achieve competitive\nresults with memory-based approaches. Experiments on the challenging ImageNet,\nImageNet-Subset and CIFAR100 benchmarks with various settings demonstrate the\nbenefits of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jodelet_Q/0/1/0/all/0/1\">Quentin Jodelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1\">Tsuyoshi Murata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leaning Compact and Representative Features for Cross-Modality Person Re-Identification. (arXiv:2103.14210v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14210","description":"<p>This paper pays close attention to the cross-modality visible-infrared person\nre-identification (VI Re-ID) task, which aims to match pedestrian samples\nbetween visible and infrared modes. In order to reduce the modality-discrepancy\nbetween samples from different cameras, most existing works usually use\nconstraints based on Euclidean metric. Because of the Euclidean based distance\nmetric strategy cannot effectively measure the internal angles between the\nembedded vectors, the existing solutions cannot learn the angularly\ndiscriminative feature embedding. Since the most important factor affecting the\nclassification task based on embedding vector is whether there is an angularly\ndiscriminative feature space, in this paper, we present a new loss function\ncalled Enumerate Angular Triplet (EAT) loss. Also, motivated by the knowledge\ndistillation, to narrow down the features between different modalities before\nfeature embedding, we further present a novel Cross-Modality Knowledge\nDistillation (CMKD) loss. Benefit from the above two considerations, the\nembedded features are discriminative enough in a way to tackle\nmodality-discrepancy problem. The experimental results on RegDB and SYSU-MM01\ndatasets have demonstrated that the proposed method is superior to the other\nmost advanced methods in terms of impressive performance. Code is available at\nhttps://github.com/IVIPLab/LCCRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hao Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Meng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MODS -- A USV-oriented object detection and obstacle segmentation benchmark. (arXiv:2105.02359v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02359","description":"<p>Small-sized unmanned surface vehicles (USV) are coastal water devices with a\nbroad range of applications such as environmental control and surveillance. A\ncrucial capability for autonomous operation is obstacle detection for timely\nreaction and collision avoidance, which has been recently explored in the\ncontext of camera-based visual scene interpretation. Owing to curated datasets,\nsubstantial advances in scene interpretation have been made in a related field\nof unmanned ground vehicles. However, the current maritime datasets do not\nadequately capture the complexity of real-world USV scenes and the evaluation\nprotocols are not standardised, which makes cross-paper comparison of different\nmethods difficult and hinders the progress. To address these issues, we\nintroduce a new obstacle detection benchmark MODS, which considers two major\nperception tasks: maritime object detection and the more general maritime\nobstacle segmentation. We present a new diverse maritime evaluation dataset\ncontaining approximately 81k stereo images synchronized with an on-board IMU,\nwith over 60k objects annotated. We propose a new obstacle segmentation\nperformance evaluation protocol that reflects the detection accuracy in a way\nmeaningful for practical USV navigation. Nineteen recent state-of-the-art\nobject detection and obstacle segmentation methods are evaluated using the\nproposed protocol, creating a benchmark to facilitate development of the field.\nThe proposed dataset, as well as evaluation routines, are made publicly\navailable at vicos.si/resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bovcon_B/0/1/0/all/0/1\">Borja Bovcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhovic_J/0/1/0/all/0/1\">Jon Muhovi&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vranac_D/0/1/0/all/0/1\">Du&#x161;ko Vranac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozetic_D/0/1/0/all/0/1\">Dean Mozeti&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pers_J/0/1/0/all/0/1\">Janez Per&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v3 [cs.NI] UPDATED)","link":"http://arxiv.org/abs/2105.11166","description":"<p>State-of-the-art performance for many emerging edge applications is achieved\nby deep neural networks (DNNs). Often, the employed DNNs are location- and\ntime-dependent, and the parameters of a specific DNN must be delivered from an\nedge server to the edge device rapidly and efficiently to carry out\ntime-sensitive inference tasks. This can be considered as a joint\nsource-channel coding (JSCC) problem, in which the goal is not to recover the\nDNN coefficients with the minimal distortion, but in a manner that provides the\nhighest accuracy in the downstream task. For this purpose we introduce AirNet,\na novel training and analog transmission method to deliver DNNs over the air.\nWe first train the DNN with noise injection to counter the wireless channel\nnoise. We also employ pruning to identify the most significant DNN parameters\nthat can be delivered within the available channel bandwidth, knowledge\ndistillation, and non-linear bandwidth expansion to provide better error\nprotection for the most important network parameters. We show that AirNet\nachieves significantly higher test accuracy compared to the separation-based\nalternative, and exhibits graceful degradation with channel quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1\">Mikolaj Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains. (arXiv:2105.14391v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14391","description":"<p>Tracking the 6D pose of objects in video sequences is important for robot\nmanipulation. This work presents se(3)-TrackNet, a data-driven optimization\napproach for long term, 6D pose tracking. It aims to identify the optimal\nrelative pose given the current RGB-D observation and a synthetic image\nconditioned on the previous best estimate and the object's model. The key\ncontribution in this context is a novel neural network architecture, which\nappropriately disentangles the feature encoding to help reduce domain shift,\nand an effective 3D orientation representation via Lie Algebra. Consequently,\neven when the network is trained solely with synthetic data can work\neffectively over real images. Comprehensive experiments over multiple\nbenchmarks show se(3)-TrackNet achieves consistently robust estimates and\noutperforms alternatives, even though they have been trained with real images.\nThe approach runs in real time at 90.9Hz. Code, data and supplementary video\nfor this project are available at\nhttps://github.com/wenbowen123/iros20-6d-pose-tracking\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitash_C/0/1/0/all/0/1\">Chaitanya Mitash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LENAS: Learning-based Neural Architecture Search and Ensemble for 3D Radiotherapy Dose Prediction. (arXiv:2106.06733v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.06733","description":"<p>Radiation therapy treatment planning is a complex process, as the target dose\nprescription and normal tissue sparing are conflicting objectives. In order to\nreduce human planning time efforts and improve the quality of treatment\nplanning, knowledge-based planning (KBP) is in high demand. In this study, we\npropose a novel learning-based ensemble approach, named LENAS, which integrates\nneural architecture search (NAS) with knowledge distillation for 3D\nradiotherapy dose prediction. Specifically, the prediction network first\nexhaustively searches each block from an enormous architecture space. Then,\nmultiple architectures with promising performance and a large diversity are\nselected. To reduce the inference time, we adopt the teacher-student paradigm\nby treating the combination of diverse outputs from multiple learned networks\nas supervisions to guide the student network training. In addition, we apply\nadversarial learning to optimize the student network to recover the knowledge\nin teacher networks. To the best of our knowledge, this is the first attempt to\ninvestigate NAS and knowledge distillation in ensemble learning, especially in\nthe field of medical image analysis. The proposed method has been evaluated on\ntwo public datasets, i.e., the OpenKBP and AIMIS dataset. Extensive\nexperimental results demonstrate the effectiveness of our method and its\nsuperior performance to the state-of-the-art methods. In addition, several\nin-depth analysis and empirical guidelines are derived for ensemble learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanfei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13797","description":"<p>Transformer recently has presented encouraging progress in computer vision.\nIn this work, we present new baselines by improving the original Pyramid Vision\nTransformer (PVTv1) by adding three designs, including (1) linear complexity\nattention layer, (2) overlapping patch embedding, and (3) convolutional\nfeed-forward network. With these modifications, PVTv2 reduces the computational\ncomplexity of PVTv1 to linear and achieves significant improvements on\nfundamental vision tasks such as classification, detection, and segmentation.\nNotably, the proposed PVTv2 achieves comparable or better performances than\nrecent works such as Swin Transformer. We hope this work will facilitate\nstate-of-the-art Transformer researches in computer vision. Code is available\nat https://github.com/whai362/PVT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PU-Flow: a Point Cloud Upsampling Networkwith Normalizing Flows. (arXiv:2107.05893v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05893","description":"<p>Point cloud upsampling aims to generate dense point clouds from given sparse\nones, which is a challenging task due to the irregular and unordered nature of\npoint sets. To address this issue, we present a novel deep learning-based\nmodel, called PU-Flow, which incorporates normalizing flows and weight\nprediction techniques to produce dense points uniformly distributed on the\nunderlying surface. Specifically, we exploit the invertible characteristics of\nnormalizing flows to transform points between Euclidean and latent spaces and\nformulate the upsampling process as ensemble of neighbouring points in a latent\nspace, where the ensemble weights are adaptively learned from local geometric\ncontext. Extensive experiments show that our method is competitive and, in most\ntest cases, it outperforms state-of-the-art methods in terms of reconstruction\nquality, proximity-to-surface accuracy, and computation efficiency. The source\ncode will be publicly available at https://github.com/unknownue/pu-flow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Aihua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zihui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yaqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Pose Transfer with Augmented Disentangled Feature Consistency. (arXiv:2107.10984v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10984","description":"<p>Deep generative models have made great progress in synthesizing images with\narbitrary human poses and transferring poses of one person to others. Though\nmany different methods have been proposed to generate images with high visual\nfidelity, the main challenge remains and comes from two fundamental issues:\npose ambiguity and appearance inconsistency. To alleviate the current\nlimitations and improve the quality of the synthesized images, we propose a\npose transfer network with augmented Disentangled Feature Consistency (DFC-Net)\nto facilitate human pose transfer. Given a pair of images containing the source\nand target person, DFC-Net extracts pose and static information from the source\nand target respectively, then synthesizes an image of the target person with\nthe desired pose from the source. Moreover, DFC-Net leverages disentangled\nfeature consistency losses in the adversarial training to strengthen the\ntransfer coherence and integrates a keypoint amplifier to enhance the pose\nfeature extraction. With the help of the disentangled feature consistency\nlosses, we further propose a novel data augmentation scheme that introduces\nunpaired support data with the augmented consistency constraints to improve the\ngenerality and robustness of DFC-Net. Extensive experimental results on\nMixamo-Pose and EDN-10k have demonstrated DFC-Net achieves state-of-the-art\nperformance on pose transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Gangyi Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet-Based Network For High Dynamic Range Imaging. (arXiv:2108.01434v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.01434","description":"<p>High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images\nhas been suffering from ghosting artifacts caused by scene and objects motion.\nExisting methods, such as optical flow based and end-to-end deep learning based\nsolutions, are error-prone either in detail restoration or ghosting artifacts\nremoval. Comprehensive empirical evidence shows that ghosting artifacts caused\nby large foreground motion are mainly low-frequency signals and the details are\nmainly high-frequency signals. In this work, we propose a novel\nfrequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion\nin the frequency domain, and Discrete Wavelet Transform (DWT) is used to\ndecompose inputs into different frequency bands. The low-frequency signals are\nused to avoid specific ghosting artifacts, while the high-frequency signals are\nused for preserving details. Using a U-Net as the backbone, we propose two\nnovel modules: merging module and frequency-guided upsampling module. The\nmerging module applies the attention mechanism to the low-frequency components\nto deal with the ghost caused by large foreground motion. The frequency-guided\nupsampling module reconstructs details from multiple frequency-specific\ncomponents with rich details. In addition, a new RAW dataset is created for\ntraining and evaluating multi-frame HDR imaging algorithms in the RAW domain.\nExtensive experiments are conducted on public datasets and our RAW dataset,\nshowing that the proposed FHDRNet achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_T/0/1/0/all/0/1\">Tianhong Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1\">Xilei Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_S/0/1/0/all/0/1\">Shanxin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FBSNet: A Fast Bilateral Symmetrical Network for Real-Time Semantic Segmentation. (arXiv:2109.00699v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00699","description":"<p>Real-time semantic segmentation, which can be visually understood as the\npixel-level classification task on the input image, currently has broad\napplication prospects, especially in the fast-developing fields of autonomous\ndriving and drone navigation. However, the huge burden of calculation together\nwith redundant parameters are still the obstacles to its technological\ndevelopment. In this paper, we propose a Fast Bilateral Symmetrical Network\n(FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a\nsymmetrical encoder-decoder structure with two branches, semantic information\nbranch, and spatial detail branch. The semantic information branch is the main\nbranch with deep network architecture to acquire the contextual information of\nthe input image and meanwhile acquire sufficient receptive field. While spatial\ndetail branch is a shallow and simple network used to establish local\ndependencies of each pixel for preserving details, which is essential for\nrestoring the original resolution during the decoding phase. Meanwhile, a\nfeature aggregation module (FAM) is designed to effectively combine the output\nfeatures of the two branches. The experimental results of Cityscapes and CamVid\nshow that the proposed FBSNet can strike a good balance between accuracy and\nefficiency. Specifically, it obtains 70.9\\% and 68.9\\% mIoU along with the\ninference speed of 90 fps and 120 fps on these two test datasets, respectively,\nwith only 0.62 million parameters on a single RTX 2080Ti GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huimin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trident Pyramid Networks: The importance of processing at the feature pyramid level for better object detection. (arXiv:2110.04004v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04004","description":"<p>Feature pyramids have become ubiquitous in multi-scale computer vision tasks\nsuch as object detection. Based on their importance, we divide a computer\nvision network into three parts: a backbone (generating a feature pyramid), a\ncore (refining the feature pyramid) and a head (generating the final output).\nMost existing networks operating on feature pyramids, named cores, are shallow\nand mostly focus on communication-based processing in the form of top-down and\nbottom-up operations. We present a new core architecture called Trident Pyramid\nNetwork (TPN), that allows for a deeper design and for a better balance between\ncommunication-based processing and self-processing. We show consistent\nimprovements when using our TPN core on the COCO object detection benchmark,\noutperforming the popular BiFPN baseline by 0.5 AP. Additionally, we\nempirically show that it is more beneficial to put additional computation into\nthe TPN core, rather than into the backbone, by outperforming a ResNet-101+FPN\nbaseline with our ResNet-50+TPN network by 1.7 AP, while operating under\nsimilar computation budgets. This emphasizes the importance of performing\ncomputation at the feature pyramid level in modern-day object detection\nsystems. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picron_C/0/1/0/all/0/1\">C&#xe9;dric Picron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v3 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2110.06986","description":"<p>In this paper, we propose a new deep unfolding neural network based on the\nADMM algorithm for analysis Compressed Sensing. The proposed network jointly\nlearns a redundant analysis operator for sparsification and reconstructs the\nsignal of interest. We compare our proposed network with a state-of-the-art\nunfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we\nconsider not only image, but also speech datasets as test examples.\nComputational experiments demonstrate that our proposed network outperforms the\nstate-of-the-art deep unfolding network, consistently for both real-world image\nand speech datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kouni_V/0/1/0/all/0/1\">Vasiliki Kouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandropoulos_G/0/1/0/all/0/1\">George C. Alexandropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfAnFace: Bridging the infant-adult domain gap in facial landmark estimation in the wild. (arXiv:2110.08935v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08935","description":"<p>We lay the groundwork for research in the algorithmic comprehension of infant\nfaces, in anticipation of applications from healthcare to psychology,\nespecially in the early prediction of developmental disorders. Specifically, we\nintroduce the first-ever dataset of infant faces annotated with facial landmark\ncoordinates and pose attributes, demonstrate the inadequacies of existing\nfacial landmark estimation algorithms in the infant domain, and train new\nstate-of-the-art models that significantly improve upon those algorithms using\ndomain adaptation techniques. We touch on the closely related task of facial\ndetection for infants, and also on a challenging case study of infrared baby\nmonitor images gathered by our lab as part of in-field research into the\naforementioned developmental issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Michael Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shaotong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1\">Lingfei Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prateek_G/0/1/0/all/0/1\">Gulati Prateek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_Mette_R/0/1/0/all/0/1\">Rebecca Schwartz-Mette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_M/0/1/0/all/0/1\">Marie Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_E/0/1/0/all/0/1\">Emily Zimmerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Quantity in Percentage of Factory Machines Using Computer Vision and Mathematical Methods. (arXiv:2111.05080v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05080","description":"<p>Computer vision has been thriving since AI development was gaining thrust.\nUsing deep learning techniques has been the most popular way which computer\nscientists thought the solution of. However, deep learning techniques tend to\nshow lower performance than manual processing. Using deep learning is not\nalways the answer to a problem related to computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seunghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_J/0/1/0/all/0/1\">Jihoon Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyeob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLOSH: Set LOcality Sensitive Hashing via Sliced-Wasserstein Embeddings. (arXiv:2112.05872v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.05872","description":"<p>Learning from set-structured data is an essential problem with many\napplications in machine learning and computer vision. This paper focuses on\nnon-parametric and data-independent learning from set-structured data using\napproximate nearest neighbor (ANN) solutions, particularly locality-sensitive\nhashing. We consider the problem of set retrieval from an input set query. Such\nretrieval problem requires: 1) an efficient mechanism to calculate the\ndistances/dissimilarities between sets, and 2) an appropriate data structure\nfor fast nearest neighbor search. To that end, we propose Sliced-Wasserstein\nset embedding as a computationally efficient \"set-2-vector\" mechanism that\nenables downstream ANN, with theoretical guarantees. The set elements are\ntreated as samples from an unknown underlying distribution, and the\nSliced-Wasserstein distance is used to compare sets. We demonstrate the\neffectiveness of our algorithm, denoted as Set-LOcality Sensitive Hashing\n(SLOSH), on various set retrieval datasets and compare our proposed embedding\nwith standard set embedding approaches, including Generalized Mean (GeM)\nembedding/pooling, Featurewise Sort Pooling (FSPool), and Covariance Pooling\nand show consistent improvement in retrieval results. The code for replicating\nour results is available here:\n\\href{https://github.com/mint-vu/SLOSH}{https://github.com/mint-vu/SLOSH}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuzhe Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltoggio_A/0/1/0/all/0/1\">Andrea Soltoggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1\">Soheil Kolouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persistent Object Identification Leveraging Non-Visual Markers. (arXiv:2112.06809v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06809","description":"<p>Our objective is to locate and provide a unique identifier for each mouse in\na cluttered home-cage environment through time, as a precursor to automated\nbehaviour recognition for biological research. This is a very challenging\nproblem due to (i) the lack of distinguishing visual features for each mouse,\nand (ii) the close confines of the scene with constant occlusion, making\nstandard visual tracking approaches unusable. However, a coarse estimate of\neach mouse's location is available from a unique RFID implant, so there is the\npotential to optimally combine information from (weak) tracking with coarse\ninformation on identity. To achieve our objective, we make the following key\ncontributions: (a) the formulation of the object identification problem as an\nassignment problem (solved using Integer Linear Programming), and (b) a novel\nprobabilistic model of the affinity between tracklets and RFID data. The latter\nis a crucial part of the model, as it provides a principled probabilistic\ntreatment of object detections given coarse localisation. Our approach achieves\n77% accuracy on this animal identification problem, and is able to reject\nspurious detections when the animals are hidden.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camilleri_M/0/1/0/all/0/1\">Michael P. J. Camilleri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bains_R/0/1/0/all/0/1\">Rasneer S. Bains</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher K. I. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional generative data-free knowledge distillation. (arXiv:2112.15358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15358","description":"<p>Knowledge distillation has made remarkable achievements in model compression.\nHowever, most existing methods demand original training data, while real data\nin practice are often unavailable due to privacy, security and transmission\nlimitation. To address this problem, we propose a conditional generative\ndata-free knowledge distillation (CGDD) framework to train efficient portable\nnetwork without any real data. In this framework, except using the knowledge\nextracted from teacher model, we introduce preset labels as additional\nauxiliary information to train the generator. Then, the trained generator can\nproduce meaningful training samples of specified category as required. In order\nto promote distillation process, except using conventional distillation loss,\nwe treat preset label as ground truth label so that student network is directly\nsupervised by the category of synthetic training sample. Moreover, we force\nstudent network to mimic the attention maps of teacher model and further\nimprove its performance. To verify the superiority of our method, we design a\nnew evaluation metric is called as relative accuracy to directly compare the\neffectiveness of different distillation methods. Trained portable network\nlearned with proposed data-free distillation method obtains 99.63%, 99.07% and\n99.84% relative accuracy on CIFAR10, CIFAR100 and Caltech101, respectively. The\nexperimental results demonstrate the superiority of proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Ling Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Libo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.04584","description":"<p>Automatic segmentation of lung lesions associated with COVID-19 in CT images\nrequires large amount of annotated volumes. Annotations mandate expert\nknowledge and are time-intensive to obtain through fully manual segmentation\nmethods. Additionally, lung lesions have large inter-patient variations, with\nsome pathologies having similar visual appearance as healthy lung tissues. This\nposes a challenge when applying existing semi-automatic interactive\nsegmentation techniques for data labelling. To address these challenges, we\npropose an efficient convolutional neural networks (CNNs) that can be learned\nonline while the annotator provides scribble-based interaction. To accelerate\nlearning from only the samples labelled through user-interactions, a\npatch-based approach is used for training the network. Moreover, we use\nweighted cross-entropy loss to address the class imbalance that may result from\nuser-interactions. During online inference, the learned network is applied to\nthe whole input volume using a fully convolutional approach. We compare our\nproposed method with state-of-the-art using synthetic scribbles and show that\nit outperforms existing methods on the task of annotating lung lesions\nassociated with COVID-19, achieving 16% higher Dice score while reducing\nexecution time by 3$\\times$ and requiring 9000 lesser scribbles-based labelled\nvoxels. Due to the online learning aspect, our approach adapts quickly to user\ninput, resulting in high quality segmentation labels. Source code for ECONet is\navailable at: https://github.com/masadcv/ECONet-MONAILabel\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asad_M/0/1/0/all/0/1\">Muhammad Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network. (arXiv:2201.08954v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08954","description":"<p>Synthetic aperture radar (SAR) image change detection is a vital yet\nchallenging task in the field of remote sensing image analysis. Most previous\nworks adopt a self-supervised method which uses pseudo-labeled samples to guide\nsubsequent training and testing. However, deep networks commonly require many\nhigh-quality samples for parameter optimization. The noise in pseudo-labels\ninevitably affects the final change detection performance. To solve the\nproblem, we propose a Graph-based Knowledge Supplement Network (GKSNet). To be\nmore specific, we extract discriminative information from the existing labeled\ndataset as additional knowledge, to suppress the adverse effects of noisy\nsamples to some extent. Afterwards, we design a graph transfer module to\ndistill contextual information attentively from the labeled dataset to the\ntarget dataset, which bridges feature correlation between datasets. To validate\nthe proposed method, we conducted extensive experiments on four SAR datasets,\nwhich demonstrated the superiority of the proposed GKSNet as compared to\nseveral state-of-the-art baselines. Our codes are available at\nhttps://github.com/summitgao/SAR_CD_GKSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Background Invariant Classification on Infrared Imagery by Data Efficient Training and Reducing Bias in CNNs. (arXiv:2201.09144v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09144","description":"<p>Even though convolutional neural networks can classify objects in images very\naccurately, it is well known that the attention of the network may not always\nbe on the semantically important regions of the scene. It has been observed\nthat networks often learn background textures which are not relevant to the\nobject of interest. In turn this makes the networks susceptible to variations\nand changes in the background which negatively affect their performance. We\npropose a new two-step training procedure called split training to reduce this\nbias in CNNs on both Infrared imagery and RGB data. Our split training\nprocedure has two steps: using MSE loss first train the layers of the network\non images with background to match the activations of the same network when it\nis trained using images without background; then with these layers frozen,\ntrain the rest of the network with cross-entropy loss to classify the objects.\nOur training method outperforms the traditional training procedure in both a\nsimple CNN architecture, and deep CNNs like VGG and Densenet which use lots of\nhardware resources, and learns to mimic human vision which focuses more on\nshape and structure than background with higher accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arif_M/0/1/0/all/0/1\">Maliha Arif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_C/0/1/0/all/0/1\">Calvin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahalanobis_A/0/1/0/all/0/1\">Abhijit Mahalanobis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02832","description":"<p>Convolutional Neural Networks have demonstrated human-level performance in\nthe classification of melanoma and other skin lesions, but evident performance\ndisparities between differing skin tones should be addressed before widespread\ndeployment. In this work, we utilise a modified variational autoencoder to\nuncover skin tone bias in datasets commonly used as benchmarks. We propose an\nefficient yet effective algorithm for automatically labelling the skin tone of\nlesion images, and use this to annotate the benchmark ISIC dataset. We\nsubsequently use two leading bias unlearning techniques to mitigate skin tone\nbias. Our experimental results provide evidence that our skin tone detection\nalgorithm outperforms existing solutions and that unlearning skin tone improves\ngeneralisation and can reduce the performance disparity between melanoma\ndetection in lighter and darker skin tones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning for Complex Data through Ensemble-based Self-Supervised Learning. (arXiv:2202.03126v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03126","description":"<p>Self-supervised learning deals with problems that have little or no available\nlabeled data. Recent work has shown impressive results when underlying classes\nhave significant semantic differences. One important dataset in which this\ntechnique thrives is ImageNet, as intra-class distances are substantially lower\nthan inter-class distances. However, this is not the case for several critical\ntasks, and general self-supervised learning methods fail to learn\ndiscriminative features when classes have closer semantics, thus requiring more\nrobust strategies. We propose a strategy to tackle this problem, and to enable\nlearning from unlabeled data even when samples from different classes are not\nprominently diverse. We approach the problem by leveraging a novel\nensemble-based clustering strategy where clusters derived from different\nconfigurations are combined to generate a better grouping for the data samples\nin a fully-unsupervised way. This strategy allows clusters with different\ndensities and higher variability to emerge, which in turn reduces intra-class\ndiscrepancies, without requiring the burden of finding an optimal configuration\nper dataset. We also consider different Convolutional Neural Networks to\ncompute distances between samples. We refine these distances by performing\ncontext analysis and group them to capture complementary information. We\nconsider two applications to validate our pipeline: Person Re-Identification\nand Text Authorship Verification. These are challenging applications\nconsidering that classes are semantically close to each other and that training\nand test sets have disjoint identities. Our method is robust across different\nmodalities and outperforms state-of-the-art results with a fully-unsupervised\nsolution without any labeling or human intervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertocco_G/0/1/0/all/0/1\">Gabriel Bertocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theofilo_A/0/1/0/all/0/1\">Ant&#xf4;nio The&#xf3;filo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andalo_F/0/1/0/all/0/1\">Fernanda Andal&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSSNet: Planarity-sensible Semantic Segmentation of Large-scale Urban Meshes. (arXiv:2202.03209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03209","description":"<p>We introduce a novel deep learning-based framework to interpret 3D urban\nscenes represented as textured meshes. Based on the observation that object\nboundaries typically align with the boundaries of planar regions, our framework\nachieves semantic segmentation in two steps: planarity-sensible\nover-segmentation followed by semantic classification. The over-segmentation\nstep generates an initial set of mesh segments that capture the planar and\nnon-planar regions of urban scenes. In the subsequent classification step, we\nconstruct a graph that encodes geometric and photometric features of the\nsegments in its nodes and multi-scale contextual features in its edges. The\nfinal semantic segmentation is obtained by classifying the segments using a\ngraph convolutional network. Experiments and comparisons on a large semantic\nurban mesh benchmark demonstrate that our approach outperforms the\nstate-of-the-art methods in terms of boundary quality and mean IoU\n(intersection over union). Besides, we also introduce several new metrics for\nevaluating mesh over-segmentation methods dedicated for semantic segmentation,\nand our proposed over-segmentation approach outperforms state-of-the-art\nmethods on all metrics. Our source code will be released when the paper is\naccepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weixiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boom_B/0/1/0/all/0/1\">Bas Boom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1\">Hugo Ledoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03586","description":"<p>As the use of deep learning in high impact domains becomes ubiquitous, it is\nincreasingly important to assess the resilience of models. One such high impact\ndomain is that of face recognition, with real world applications involving\nimages affected by various degradations, such as motion blur or high exposure.\nMoreover, images captured across different attributes, such as gender and race,\ncan also challenge the robustness of a face recognition algorithm. While\ntraditional summary statistics suggest that the aggregate performance of face\nrecognition models has continued to improve, these metrics do not directly\nmeasure the robustness or fairness of the models. Visual Psychophysics\nSensitivity Analysis (VPSA) [1] provides a way to pinpoint the individual\ncauses of failure by way of introducing incremental perturbations in the data.\nHowever, perturbations may affect subgroups differently. In this paper, we\npropose a new fairness evaluation based on robustness in the form of a generic\nframework that extends VPSA. With this framework, we can analyze the ability of\na model to perform fairly for different subgroups of a population affected by\nperturbations, and pinpoint the exact failure modes for a subgroup by measuring\ntargeted robustness. With the increasing focus on the fairness of models, we\nuse face recognition as an example application of our framework and propose to\ncompactly visualize the fairness analysis of a model via AUC matrices. We\nanalyze the performance of common face recognition models and empirically show\nthat certain subgroups are at a disadvantage when images are perturbed, thereby\nuncovering trends that were not visible using the model's performance on\nsubgroups without perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Aparna R. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1\">Xavier Suau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_N/0/1/0/all/0/1\">Nivedha Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1\">Luca Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Understand Masked Autoencoders. (arXiv:2202.03670v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03670","description":"<p>\"Masked Autoencoders (MAE) Are Scalable Vision Learners\" revolutionizes the\nself-supervised learning method in that it not only achieves the\nstate-of-the-art for image pre-training, but is also a milestone that bridges\nthe gap between visual and linguistic masked autoencoding (BERT-style)\npre-trainings. However, to our knowledge, to date there are no theoretical\nperspectives to explain the powerful expressivity of MAE. In this paper, we,\nfor the first time, propose a unified theoretical framework that provides a\nmathematical understanding for MAE. Specifically, we explain the patch-based\nattention approaches of MAE using an integral kernel under a non-overlapping\ndomain decomposition setting. To help the research community to further\ncomprehend the main reasons of the great success of MAE, based on our\nframework, we pose five questions and answer them with mathematical rigor using\ninsights from operator theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Relational Domain Adaptation. (arXiv:2202.03628v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2202.03628","description":"<p>Existing domain adaptation methods tend to treat every domain equally and\nalign them all perfectly. Such uniform alignment ignores topological structures\namong different domains; therefore it may be beneficial for nearby domains, but\nnot necessarily for distant domains. In this work, we relax such uniform\nalignment by using a domain graph to encode domain adjacency, e.g., a graph of\nstates in the US with each state as a domain and each edge indicating\nadjacency, thereby allowing domains to align flexibly based on the graph\nstructure. We generalize the existing adversarial learning framework with a\nnovel graph discriminator using encoding-conditioned graph embeddings.\nTheoretical analysis shows that at equilibrium, our method recovers classic\ndomain adaptation when the graph is a clique, and achieves non-trivial\nalignment for other types of graphs. Empirical results show that our approach\nsuccessfully generalizes uniform alignment, naturally incorporates domain\ninformation represented by graphs, and improves upon existing domain adaptation\nmethods on both synthetic and real-world datasets. Code will soon be available\nat https://github.com/Wang-ML-Lab/GRDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+he_H/0/1/0/all/0/1\">Hao he</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Guang-He Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}