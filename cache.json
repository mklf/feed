{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"UrduFake@FIRE2020: Shared Track on Fake News Identification in Urdu. (arXiv:2207.12406v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12406","description":"<p>This paper gives the overview of the first shared task at FIRE 2020 on fake\nnews detection in the Urdu language. This is a binary classification task in\nwhich the goal is to identify fake news using a dataset composed of 900\nannotated news articles for training and 400 news articles for testing. The\ndataset contains news in five domains: (i) Health, (ii) Sports, (iii) Showbiz,\n(iv) Technology, and (v) Business. 42 teams from 6 different countries (India,\nChina, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams\nsubmitted their experimental results. The participants used various machine\nlearning methods ranging from feature-based traditional machine learning to\nneural network techniques. The best performing system achieved an F-score value\nof 0.90, showing that the BERT-based approach outperforms other machine\nlearning classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amjad_M/0/1/0/all/0/1\">Maaz Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhila_A/0/1/0/all/0/1\">Alisa Zhila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Speaker Diarization that is Agnostic to Language, Overlap-Aware, and Tuning Free. (arXiv:2207.12504v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12504","description":"<p>Podcasts are conversational in nature and speaker changes are frequent --\nrequiring speaker diarization for content understanding. We propose an\nunsupervised technique for speaker diarization without relying on\nlanguage-specific components. The algorithm is overlap-aware and does not\nrequire information about the number of speakers. Our approach shows 79%\nimprovement on purity scores (34% on F-score) against the Google Cloud Platform\nsolution on podcast data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanveer_M/0/1/0/all/0/1\">M. Iftekhar Tanveer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casabuena_D/0/1/0/all/0/1\">Diego Casabuena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlgren_J/0/1/0/all/0/1\">Jussi Karlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Rosie Jones</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialCrowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit. (arXiv:2207.12551v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12551","description":"<p>Dialog system developers need high-quality data to train, fine-tune and\nassess their systems. They often use crowdsourcing for this since it provides\nlarge quantities of data from many workers. However, the data may not be of\nsufficiently good quality. This can be due to the way that the requester\npresents a task and how they interact with the workers. This paper introduces\nDialCrowd 2.0 to help requesters obtain higher quality data by, for example,\npresenting tasks more clearly and facilitating effective communication with\nworkers. DialCrowd 2.0 guides developers in creating improved Human\nIntelligence Tasks (HITs) and is directly applicable to the workflows used\ncurrently by developers and researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_J/0/1/0/all/0/1\">Jessica Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey Bigham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Innovations in Neural Data-to-text Generation. (arXiv:2207.12571v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12571","description":"<p>The neural boom that has sparked natural language processing (NLP) research\nthrough the last decade has similarly led to significant innovations in\ndata-to-text generation (DTG). This survey offers a consolidated view into the\nneural DTG paradigm with a structured examination of the approaches, benchmark\ndatasets, and evaluation protocols. This survey draws boundaries separating DTG\nfrom the rest of the natural language generation (NLG) landscape, encompassing\nan up-to-date synthesis of the literature, and highlighting the stages of\ntechnological adoption from within and outside the greater NLG umbrella. With\nthis holistic view, we highlight promising avenues for DTG research that not\nonly focus on the design of linguistically capable systems but also systems\nthat exhibit fairness and accountability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mandar Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogineni_A/0/1/0/all/0/1\">Ajay Gogineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12576","description":"<p>While vision-and-language models perform well on tasks such as visual\nquestion answering, they struggle when it comes to basic human commonsense\nreasoning skills. In this work, we introduce WinoGAViL: an online game to\ncollect vision-and-language associations, (e.g., werewolves to a full moon),\nused as a dynamic benchmark to evaluate state-of-the-art models. Inspired by\nthe popular card game Codenames, a spymaster gives a textual cue related to\nseveral visual candidates, and another player has to identify them. Human\nplayers are rewarded for creating associations that are challenging for a rival\nAI model but still solvable by other human players. We use the game to collect\n3.5K instances, finding that they are intuitive for humans (&gt;90% Jaccard index)\nbut challenging for state-of-the-art AI models, where the best model (ViLT)\nachieves a score of 52%, succeeding mostly where the cue is visually salient.\nOur analysis as well as the feedback we collect from players indicate that the\ncollected associations require diverse reasoning skills, including general\nknowledge, common sense, abstraction, and more. We release the dataset, the\ncode and the interactive game, aiming to allow future data collection that can\nbe used to develop models with better association abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guetta_N/0/1/0/all/0/1\">Nitzan Bitton Guetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1\">Ron Yosef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training. (arXiv:2207.12661v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12661","description":"<p>Large-scale multi-modal contrastive pre-training has demonstrated great\nutility to learn transferable features for a range of downstream tasks by\nmapping multiple modalities into a shared embedding space. Typically, this has\nemployed separate encoders for each modality. However, recent work suggests\nthat transformers can support learning across multiple modalities and allow\nknowledge sharing. Inspired by this, we investigate a variety of\nModality-Shared Contrastive Language-Image Pre-training (MS-CLIP) frameworks.\nMore specifically, we question how many parameters of a transformer model can\nbe shared across modalities during contrastive pre-training, and rigorously\nexamine architectural design choices that position the proportion of parameters\nshared along a spectrum. In studied conditions, we observe that a mostly\nunified encoder for vision and language signals outperforms all other\nvariations that separate more parameters. Additionally, we find that\nlight-weight modality-specific parallel modules further improve performance.\nExperimental results show that the proposed MS-CLIP approach outperforms\nvanilla CLIP by up to 13\\% relative in zero-shot ImageNet classification\n(pre-trained on YFCC-100M), while simultaneously supporting a reduction of\nparameters. In addition, our approach outperforms vanilla CLIP by 1.6 points in\nlinear probing on a collection of 24 downstream vision tasks. Furthermore, we\ndiscover that sharing parameters leads to semantic concepts from different\nmodalities being encoded more closely in the embedding space, facilitating the\ntransferring of common semantic structure (e.g., attention patterns) from\nlanguage to vision. Code is available at\n\\href{https://github.com/Hxyou/MSCLIP}{URL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advanced Conditional Variational Autoencoders (A-CVAE): Towards interpreting open-domain conversation generation via disentangling latent feature representation. (arXiv:2207.12696v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12696","description":"<p>Currently end-to-end deep learning based open-domain dialogue systems remain\nblack box models, making it easy to generate irrelevant contents with\ndata-driven models. Specifically, latent variables are highly entangled with\ndifferent semantics in the latent space due to the lack of priori knowledge to\nguide the training. To address this problem, this paper proposes to harness the\ngenerative model with a priori knowledge through a cognitive approach involving\nmesoscopic scale feature disentanglement. Particularly, the model integrates\nthe macro-level guided-category knowledge and micro-level open-domain dialogue\ndata for the training, leveraging the priori knowledge into the latent space,\nwhich enables the model to disentangle the latent variables within the\nmesoscopic scale. Besides, we propose a new metric for open-domain dialogues,\nwhich can objectively evaluate the interpretability of the latent space\ndistribution. Finally, we validate our model on different datasets and\nexperimentally demonstrate that our model is able to generate higher quality\nand more interpretable dialogues than other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jingbo Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoxia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automated News Bias Classifier Using Caenorhabditis Elegans Inspired Recursive Feedback Network Architecture. (arXiv:2207.12724v1 [cs.NE])","link":"http://arxiv.org/abs/2207.12724","description":"<p>Traditional approaches to classify the political bias of news articles have\nfailed to generate accurate, generalizable results. Existing networks premised\non CNNs and DNNs lack a model to identify and extrapolate subtle indicators of\nbias like word choice, context, and presentation. In this paper, we propose a\nnetwork architecture that achieves human-level accuracy in assigning bias\nclassifications to articles. The underlying model is based on a novel Mesh\nNeural Network (MNN),this structure enables feedback and feedforward synaptic\nconnections between any two neurons in the mesh. The MNN ontains six network\nconfigurations that utilize Bernoulli based random sampling, pre-trained DNNs,\nand a network modelled after the C. Elegans nematode. The model is trained on\nover ten-thousand articles scraped from AllSides.com which are labelled to\nindicate political bias. The parameters of the network are then evolved using a\ngenetic algorithm suited to the feedback neural structure. Finally, the best\nperforming model is applied to five popular news sources in the United States\nover a fifty-day trial to quantify political biases in the articles they\ndisplay. We hope our project can spur research into biological solutions for\nNLP tasks and provide accurate tools for citizens to understand subtle biases\nin the articles they consume.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_A/0/1/0/all/0/1\">Agastya Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_N/0/1/0/all/0/1\">Natarajan S</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable User Dialogue Act Augmentation for Dialogue State Tracking. (arXiv:2207.12757v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12757","description":"<p>Prior work has demonstrated that data augmentation is useful for improving\ndialogue state tracking. However, there are many types of user utterances,\nwhile the prior method only considered the simplest one for augmentation,\nraising the concern about poor generalization capability. In order to better\ncover diverse dialogue acts and control the generation quality, this paper\nproposes controllable user dialogue act augmentation (CUDA-DST) to augment user\nutterances with diverse behaviors. With the augmented data, different state\ntrackers gain improvement and show better robustness, achieving the\nstate-of-the-art performance on MultiWOZ 2.1\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chun-Mao Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_M/0/1/0/all/0/1\">Ming-Hao Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Effective Neural Sentence Encoders from Automatically Mined Paraphrases. (arXiv:2207.12759v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12759","description":"<p>Sentence embeddings are commonly used in text clustering and semantic\nretrieval tasks. State-of-the-art sentence representation methods are based on\nartificial neural networks fine-tuned on large collections of manually labeled\nsentence pairs. Sufficient amount of annotated data is available for\nhigh-resource languages such as English or Chinese. In less popular languages,\nmultilingual models have to be used, which offer lower performance. In this\npublication, we address this problem by proposing a method for training\neffective language-specific sentence encoders without manually labeled data.\nOur approach is to automatically construct a dataset of paraphrase pairs from\nsentence-aligned bilingual text corpora. We then use the collected data to\nfine-tune a Transformer language model with an additional recurrent pooling\nlayer. Our sentence encoder can be trained in less than a day on a single\ngraphics card, achieving high performance on a diverse set of sentence-level\ntasks. We evaluate our method on eight linguistic tasks in Polish, comparing it\nwith the best available multilingual sentence encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dadas_S/0/1/0/all/0/1\">S&#x142;awomir Dadas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariant and Invariant Grounding for Video Question Answering. (arXiv:2207.12783v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12783","description":"<p>Video Question Answering (VideoQA) is the task of answering the natural\nlanguage questions about a video. Producing an answer requires understanding\nthe interplay across visual scenes in video and linguistic semantics in\nquestion. However, most leading VideoQA models work as black boxes, which make\nthe visual-linguistic alignment behind the answering process obscure. Such\nblack-box nature calls for visual explainability that reveals ``What part of\nthe video should the model look at to answer the question?''. Only a few works\npresent the visual explanations in a post-hoc fashion, which emulates the\ntarget model's answering process via an additional method. Nonetheless, the\nemulation struggles to faithfully exhibit the visual-linguistic alignment\nduring answering.\n</p>\n<p>Instead of post-hoc explainability, we focus on intrinsic interpretability to\nmake the answering process transparent. At its core is grounding the\nquestion-critical cues as the causal scene to yield answers, while rolling out\nthe question-irrelevant information as the environment scene. Taking a causal\nlook at VideoQA, we devise a self-interpretable framework, Equivariant and\nInvariant Grounding for Interpretable VideoQA (EIGV). Specifically, the\nequivariant grounding encourages the answering to be sensitive to the semantic\nchanges in the causal scene and question; in contrast, the invariant grounding\nenforces the answering to be insensitive to the changes in the environment\nscene. By imposing them on the answering process, EIGV is able to distinguish\nthe causal scene from the environment information, and explicitly present the\nvisual-linguistic alignment. Extensive experiments on three benchmark datasets\njustify the superiority of EIGV in terms of accuracy and visual\ninterpretability over the leading baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yicong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junbin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extreme compression of sentence-transformer ranker models: faster inference, longer battery life, and less storage on edge devices. (arXiv:2207.12852v1 [cs.LG])","link":"http://arxiv.org/abs/2207.12852","description":"<p>Modern search systems use several large ranker models with transformer\narchitectures. These models require large computational resources and are not\nsuitable for usage on devices with limited computational resources. Knowledge\ndistillation is a popular compression technique that can reduce the resource\nneeds of such models, where a large teacher model transfers knowledge to a\nsmall student model. To drastically reduce memory requirements and energy\nconsumption, we propose two extensions for a popular sentence-transformer\ndistillation procedure: generation of an optimal size vocabulary and\ndimensionality reduction of the embedding dimension of teachers prior to\ndistillation. We evaluate these extensions on two different types of ranker\nmodels. This results in extremely compressed student models whose analysis on a\ntest dataset shows the significance and utility of our proposed extensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaulwar_A/0/1/0/all/0/1\">Amit Chaulwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_L/0/1/0/all/0/1\">Lukas Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krajewski_M/0/1/0/all/0/1\">Maciej Krajewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichel_F/0/1/0/all/0/1\">Felix Reichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundbaek_L/0/1/0/all/0/1\">Leif-Nissen Lundb&#xe6;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_M/0/1/0/all/0/1\">Michael Huth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matejczyk_B/0/1/0/all/0/1\">Bartlomiej Matejczyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Collaborative Filtering Recommender with Prompt-Based Sentiment Analysis. (arXiv:2207.12883v1 [cs.IR])","link":"http://arxiv.org/abs/2207.12883","description":"<p>Collaborative Filtering(CF) recommender is a crucial application in the\nonline market and ecommerce. However, CF recommender has been proven to suffer\nfrom persistent problems related to sparsity of the user rating that will\nfurther lead to a cold-start issue. Existing methods address the data sparsity\nissue by applying token-level sentiment analysis that translate text review\ninto sentiment scores as a complement of the user rating. In this paper, we\nattempt to optimize the sentiment analysis with advanced NLP models including\nBERT and RoBERTa, and experiment on whether the CF recommender has been further\nenhanced. We build the recommenders on the Amazon US Reviews dataset, and tune\nthe pretrained BERT and RoBERTa with the traditional fine-tuned paradigm as\nwell as the new prompt-based learning paradigm. Experimental result shows that\nthe recommender enhanced with the sentiment ratings predicted by the fine-tuned\nRoBERTa has the best performance, and achieved 30.7% overall gain by comparing\nMAP, NDCG and precision at K to the baseline recommender. Prompt-based learning\nparadigm, although superior to traditional fine-tune paradigm in pure sentiment\nanalysis, fail to further improve the CF recommender.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_E/0/1/0/all/0/1\">Elliot Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zheyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning structures of the French clinical language:development and validation of word embedding models using 21 million clinical reports from electronic health records. (arXiv:2207.12940v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12940","description":"<p>Background\n</p>\n<p>Clinical studies using real-world data may benefit from exploiting clinical\nreports, a particularly rich albeit unstructured medium. To that end, natural\nlanguage processing can extract relevant information. Methods based on transfer\nlearning using pre-trained language models have achieved state-of-the-art\nresults in most NLP applications; however, publicly available models lack\nexposure to speciality-languages, especially in the medical field.\n</p>\n<p>Objective\n</p>\n<p>We aimed to evaluate the impact of adapting a language model to French\nclinical reports on downstream medical NLP tasks.\n</p>\n<p>Methods\n</p>\n<p>We leveraged a corpus of 21M clinical reports collected from August 2017 to\nJuly 2021 at the Greater Paris University Hospitals (APHP) to produce two\nCamemBERT architectures on speciality language: one retrained from scratch and\nthe other using CamemBERT as its initialisation. We used two French annotated\nmedical datasets to compare our language models to the original CamemBERT\nnetwork, evaluating the statistical significance of improvement with the\nWilcoxon test.\n</p>\n<p>Results\n</p>\n<p>Our models pretrained on clinical reports increased the average F1-score on\nAPMed (an APHP-specific task) by 3 percentage points to 91%, a statistically\nsignificant improvement. They also achieved performance comparable to the\noriginal CamemBERT on QUAERO. These results hold true for the fine-tuned and\nfrom-scratch versions alike, starting from very few pre-training samples.\n</p>\n<p>Conclusions\n</p>\n<p>We confirm previous literature showing that adapting generalist pre-train\nlanguage models such as CamenBERT on speciality corpora improves their\nperformance for downstream clinical NLP tasks. Our results suggest that\nretraining from scratch does not induce a statistically significant performance\ngain compared to fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dura_B/0/1/0/all/0/1\">Basile Dura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jean_C/0/1/0/all/0/1\">Charline Jean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tannier_X/0/1/0/all/0/1\">Xavier Tannier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calliger_A/0/1/0/all/0/1\">Alice Calliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bey_R/0/1/0/all/0/1\">Romain Bey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuraz_A/0/1/0/all/0/1\">Antoine Neuraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flicoteaux_R/0/1/0/all/0/1\">R&#xe9;mi Flicoteaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark. (arXiv:2207.13005v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13005","description":"<p>Modern Entity Linking (EL) systems entrench a popularity bias, yet there is\nno dataset focusing on tail and emerging entities in languages other than\nEnglish. We present Hansel, a new benchmark in Chinese that fills the vacancy\nof non-English few-shot and zero-shot EL challenges. The test set of Hansel is\nhuman annotated and reviewed, created with a novel method for collecting\nzero-shot EL datasets. It covers 10K diverse documents in news, social media\nposts and other web articles, with Wikidata as its target Knowledge Base. We\ndemonstrate that the existing state-of-the-art EL system performs poorly on\nHansel (R@1 of 36.6% on Few-Shot). We then establish a strong baseline that\nscores a R@1 of 46.2% on Few-Shot and 76.6% on Zero-Shot on our dataset. We\nalso show that our baseline achieves competitive results on TAC-KBP2015 Chinese\nEntity Linking task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Z/0/1/0/all/0/1\">Zifei Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsStories: Illustrating articles with visual summaries. (arXiv:2207.13061v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13061","description":"<p>Recent self-supervised approaches have used large-scale image-text datasets\nto learn powerful representations that transfer to many tasks without\nfinetuning. These methods often assume that there is one-to-one correspondence\nbetween its images and their (short) captions. However, many tasks require\nreasoning about multiple images and long text narratives, such as describing\nnews articles with visual summaries. Thus, we explore a novel setting where the\ngoal is to learn a self-supervised visual-language representation that is\nrobust to varying text length and the number of images. In addition, unlike\nprior work which assumed captions have a literal relation to the image, we\nassume images only contain loose illustrative correspondence with the text. To\nexplore this problem, we introduce a large-scale multimodal dataset containing\nover 31M articles, 22M images and 1M videos. We show that state-of-the-art\nimage-text alignment methods are not robust to longer narratives with multiple\nimages. Finally, we introduce an intuitive baseline that outperforms these\nmethods on zero-shot image-set retrieval by 10% on the GoodNews dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Reuben Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1\">JP Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sud_A/0/1/0/all/0/1\">Avneesh Sud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_T/0/1/0/all/0/1\">Thomas Leung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TI-CNN: Convolutional Neural Networks for Fake News Detection. (arXiv:1806.00749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1806.00749","description":"<p>With the development of social networks, fake news for various commercial and\npolitical purposes has been appearing in large numbers and gotten widespread in\nthe online world. With deceptive words, people can get infected by the fake\nnews very easily and will share them without any fact-checking. For instance,\nduring the 2016 US president election, various kinds of fake news about the\ncandidates widely spread through both official news media and the online social\nnetworks. These fake news is usually released to either smear the opponents or\nsupport the candidate on their side. The erroneous information in the fake news\nis usually written to motivate the voters' irrational emotion and enthusiasm.\nSuch kinds of fake news sometimes can bring about devastating effects, and an\nimportant goal in improving the credibility of online social networks is to\nidentify the fake news timely. In this paper, we propose to study the fake news\ndetection problem. Automatic fake news identification is extremely hard, since\npure model based fact-checking for news is still an open problem, and few\nexisting models can be applied to solve the problem. With a thorough\ninvestigation of a fake news data, lots of useful explicit features are\nidentified from both the text words and images used in the fake news. Besides\nthe explicit features, there also exist some hidden patterns in the words and\nimages used in fake news, which can be captured with a set of latent features\nextracted via the multiple convolutional layers in our model. A model named as\nTI-CNN (Text and Image information based Convolutinal Neural Network) is\nproposed in this paper. By projecting the explicit and latent features into a\nunified feature space, TI-CNN is trained with both the text and image\ninformation simultaneously. Extensive experiments carried on the real-world\nfake news datasets have demonstrate the effectiveness of TI-CNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Qingcai Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advanced Semantics for Commonsense Knowledge Extraction. (arXiv:2011.00905v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2011.00905","description":"<p>Commonsense knowledge (CSK) about concepts and their properties is useful for\nAI applications such as robust chatbots. Prior works like ConceptNet, TupleKB\nand others compiled large CSK collections, but are restricted in their\nexpressiveness to subject-predicate-object (SPO) triples with simple concepts\nfor S and monolithic strings for P and O. Also, these projects have either\nprioritized precision or recall, but hardly reconcile these complementary\ngoals. This paper presents a methodology, called Ascent, to automatically build\na large-scale knowledge base (KB) of CSK assertions, with advanced\nexpressiveness and both better precision and recall than prior works. Ascent\ngoes beyond triples by capturing composite concepts with subgroups and aspects,\nand by refining assertions with semantic facets. The latter are important to\nexpress temporal and spatial validity of assertions and further qualifiers.\nAscent combines open information extraction with judicious cleaning using\nlanguage models. Intrinsic evaluation shows the superior size and quality of\nthe Ascent KB, and an extrinsic evaluation for QA-support tasks underlines the\nbenefits of Ascent. A web interface, data and code can be found at\nhttps://ascent.mpi-inf.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On joint training with interfaces for spoken language understanding. (arXiv:2106.15919v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15919","description":"<p>Spoken language understanding (SLU) systems extract both text transcripts and\nsemantics associated with intents and slots from input speech utterances. SLU\nsystems usually consist of (1) an automatic speech recognition (ASR) module,\n(2) an interface module that exposes relevant outputs from ASR, and (3) a\nnatural language understanding (NLU) module. Interfaces in SLU systems carry\ninformation on text transcriptions or richer information like neural embeddings\nfrom ASR to NLU. In this paper, we study how interfaces affect joint-training\nfor spoken language understanding. Most notably, we obtain the state-of-the-art\nresults on the publicly available 50-hr SLURP dataset. We first leverage\nlarge-size pretrained ASR and NLU models that are connected by a text\ninterface, and then jointly train both models via a sequence loss function. For\nscenarios where pretrained models are not utilized, the best results are\nobtained through a joint sequence loss training using richer neural interfaces.\nFinally, we show the overall diminishing impact of leveraging pretrained models\nwith increased training data size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Gautam Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1\">Pranav Dheram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Bryan Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chul Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1\">Bach Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Text Extractive Summarization Based on Graph and Pre-trained Language Model Attention. (arXiv:2110.04878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04878","description":"<p>Representing a text as a graph for obtaining automatic text summarization has\nbeen investigated for over ten years. With the development of attention or\nTransformer on natural language processing (NLP), it is possible to make a\nconnection between the graph and attention structure for a text. In this paper,\nan attention matrix between the sentences of the whole text is adopted as a\nweighted adjacent matrix of a fully connected graph of the text, which can be\nproduced through the pre-training language model. The GCN is further applied to\nthe text graph model for classifying each node and finding out the salient\nsentences from the text. It is demonstrated by the experimental results on two\ntypical datasets that our proposed model can achieve a competitive result in\ncomparison with sate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuan-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jinwen Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Visual Question Generation. (arXiv:2110.08226v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08226","description":"<p>In traditional Visual Question Generation (VQG), most images have multiple\nconcepts (e.g. objects and categories) for which a question could be generated,\nbut models are trained to mimic an arbitrary choice of concept as given in\ntheir training data. This makes training difficult and also poses issues for\nevaluation -- multiple valid questions exist for most images but only one or a\nfew are captured by the human references. We present Guiding Visual Question\nGeneration - a variant of VQG which conditions the question generator on\ncategorical information based on expectations on the type of question and the\nobjects it should explore. We propose two variants: (i) an explicitly guided\nmodel that enables an actor (human or automated) to select which objects and\ncategories to generate a question for; and (ii) an implicitly guided model that\nlearns which objects and categories to condition on, based on discrete latent\nvariables. The proposed models are evaluated on an answer-category augmented\nVQA dataset and our quantitative results show a substantial improvement over\nthe current state of the art (over 9 BLEU-4 increase). Human evaluation\nvalidates that guidance helps the generation of questions that are\ngrammatically coherent and relevant to the given image and objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vedd_N/0/1/0/all/0/1\">Nihir Vedd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Theories on Styles to their Transfer in Text: Bridging the Gap with a Hierarchical Survey. (arXiv:2110.15871v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15871","description":"<p>Humans are naturally endowed with the ability to write in a particular style.\nThey can, for instance, re-phrase a formal letter in an informal way, convey a\nliteral message with the use of figures of speech or edit a novel by mimicking\nthe style of some well-known authors. Automating this form of creativity\nconstitutes the goal of style transfer. As a natural language generation task,\nstyle transfer aims at rewriting existing texts, and specifically, it creates\nparaphrases that exhibit some desired stylistic attributes. From a practical\nperspective, it envisions beneficial applications, like chatbots that modulate\ntheir communicative style to appear empathetic, or systems that automatically\nsimplify technical articles for a non-expert audience. Several style-aware\nparaphrasing methods have attempted to tackle style transfer. A handful of\nsurveys give a methodological overview of the field, but they do not support\nresearchers to focus on specific styles. With this paper, we aim at providing a\ncomprehensive discussion of the styles that have received attention in the\ntransfer task. We organize them in a hierarchy, highlighting the challenges for\nthe definition of each of them, and pointing out gaps in the current research\nlandscape. The hierarchy comprises two main groups. One encompasses styles that\npeople modulate arbitrarily, along the lines of registers and genres. The other\ngroup corresponds to unintentionally expressed styles, due to an author's\npersonal characteristics. Hence, our review shows how these groups relate to\none another, and where specific styles, including some that have not yet been\nexplored, belong in the hierarchy. Moreover, we summarize the methods employed\nfor different stylistic families, hinting researchers towards those that would\nbe the most fitting for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velutharambath_A/0/1/0/all/0/1\">Aswathy Velutharambath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pay More Attention to History: A Context Modelling Strategy for Conversational Text-to-SQL. (arXiv:2112.08735v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08735","description":"<p>Conversational text-to-SQL aims at converting multi-turn natural language\nqueries into their corresponding SQL (Structured Query Language)\nrepresentations. One of the most intractable problems of conversational\ntext-to-SQL is modelling the semantics of multi-turn queries and gathering the\nproper information required for the current query. This paper shows that\nexplicitly modelling the semantic changes by adding each turn and the\nsummarization of the whole context can bring better performance on converting\nconversational queries into SQLs. In particular, we propose two conversational\nmodelling tasks in both turn grain and conversation grain. These two tasks\nsimply work as auxiliary training tasks to help with multi-turn conversational\nsemantic parsing. We conducted empirical studies and achieved new\nstate-of-the-art results on the large-scale open-domain conversational\ntext-to-SQL dataset. The results demonstrate that the proposed mechanism\nsignificantly improves the performance of multi-turn semantic parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanchu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yutian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Adaptive Deep Clustering Pipeline to Inform Text Labeling at Scale. (arXiv:2202.01211v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01211","description":"<p>Mining the latent intentions from large volumes of natural language inputs is\na key step to help data analysts design and refine Intelligent Virtual\nAssistants (IVAs) for customer service and sales support. We created a flexible\nand scalable clustering pipeline within the Verint Intent Manager (VIM) that\nintegrates the fine-tuning of language models, a high performing k-NN library\nand community detection techniques to help analysts quickly surface and\norganize relevant user intentions from conversational texts. The fine-tuning\nstep is necessary because pre-trained language models cannot encode texts to\nefficiently surface particular clustering structures when the target texts are\nfrom an unseen domain or the clustering task is not topic detection. We\ndescribe the pipeline and demonstrate its performance and ability to scale on\nthree real-world text mining tasks. As deployed in the VIM application, this\nclustering pipeline produces high quality results, improving the performance of\ndata analysts and reducing the time it takes to surface intentions from\ncustomer service data, thereby reducing the time it takes to build and deploy\nIVAs in new domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_I/0/1/0/all/0/1\">Ian Beaver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing the ICBe Dataset: Very High Recall and Precision Event Extraction from Narratives about International Crises. (arXiv:2202.07081v2 [stat.AP] UPDATED)","link":"http://arxiv.org/abs/2202.07081","description":"<p>How do international crises unfold? We conceptualize of international\nrelations as a strategic chess game between adversaries and develop a\nsystematic way to measure pieces, moves, and gambits accurately and\nconsistently over a hundred years of history. We introduce a new ontology and\ndataset of international events called ICBe based on a very high-quality corpus\nof narratives from the International Crisis Behavior (ICB) Project. We\ndemonstrate that ICBe has higher coverage, recall, and precision than existing\nstate of the art datasets and conduct two detailed case studies of the Cuban\nMissile Crisis (1962) and Crimea-Donbas Crisis (2014). We further introduce two\nnew event visualizations (event icongraphy and crisis maps), an automated\nbenchmark for measuring event recall using natural language processing\n(sythnetic narratives), and an ontology reconstruction task for objectively\nmeasuring event precision. We make the data, online appendix, replication\nmaterial, and visualizations of every historical episode available at a\ncompanion website www.crisisevents.org and the github repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Douglass_R/0/1/0/all/0/1\">Rex W. Douglass</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scherer_T/0/1/0/all/0/1\">Thomas Leo Scherer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gannon_J/0/1/0/all/0/1\">J. Andr&#xe9;s Gannon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gartzke_E/0/1/0/all/0/1\">Erik Gartzke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lindsay_J/0/1/0/all/0/1\">Jon Lindsay</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carcelli_S/0/1/0/all/0/1\">Shannon Carcelli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wilkenfeld_J/0/1/0/all/0/1\">Jonathan Wilkenfeld</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Quinn_D/0/1/0/all/0/1\">David M. Quinn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aiken_C/0/1/0/all/0/1\">Catherine Aiken</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Navarro_J/0/1/0/all/0/1\">Jose Miguel Cabezas Navarro</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lund_N/0/1/0/all/0/1\">Neil Lund</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Murauskaite_E/0/1/0/all/0/1\">Egle Murauskaite</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Partridge_D/0/1/0/all/0/1\">Diana Partridge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task. (arXiv:2202.13174v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13174","description":"<p>Biomedical machine reading comprehension (biomedical-MRC) aims to comprehend\ncomplex biomedical narratives and assist healthcare professionals in retrieving\ninformation from them. The high performance of modern neural network-based MRC\nsystems depends on high-quality, large-scale, human-annotated training\ndatasets. In the biomedical domain, a crucial challenge in creating such\ndatasets is the requirement for domain knowledge, inducing the scarcity of\nlabeled data and the need for transfer learning from the labeled\ngeneral-purpose (source) domain to the biomedical (target) domain. However,\nthere is a discrepancy in marginal distributions between the general-purpose\nand biomedical domains due to the variances in topics. Therefore,\ndirect-transferring of learned representations from a model trained on a\ngeneral-purpose domain to the biomedical domain can hurt the model's\nperformance. We present an adversarial learning-based domain adaptation\nframework for the biomedical machine reading comprehension task (BioADAPT-MRC),\na neural network-based method to address the discrepancies in the marginal\ndistributions between the general and biomedical domain datasets. BioADAPT-MRC\nrelaxes the need for generating pseudo labels for training a well-performing\nbiomedical-MRC model. We extensively evaluate the performance of BioADAPT-MRC\nby comparing it with the best existing methods on three widely used benchmark\nbiomedical-MRC datasets -- BioASQ-7b, BioASQ-8b, and BioASQ-9b. Our results\nsuggest that without using any synthetic or human-annotated data from the\nbiomedical domain, BioADAPT-MRC can achieve state-of-the-art performance on\nthese datasets. Availability: BioADAPT-MRC is freely available as an\nopen-source project at \\url{https://github.com/mmahbub/BioADAPT-MRC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1\">Maria Mahbub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudarshan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1\">Edmon Begoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_G/0/1/0/all/0/1\">Gregory D Peterson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PACS: A Dataset for Physical Audiovisual CommonSense Reasoning. (arXiv:2203.11130v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.11130","description":"<p>In order for AI to be safely deployed in real-world scenarios such as\nhospitals, schools, and the workplace, it must be able to robustly reason about\nthe physical world. Fundamental to this reasoning is physical common sense:\nunderstanding the physical properties and affordances of available objects, how\nthey can be manipulated, and how they interact with other objects. Physical\ncommonsense reasoning is fundamentally a multi-sensory task, since physical\nproperties are manifested through multiple modalities - two of them being\nvision and acoustics. Our paper takes a step towards real-world physical\ncommonsense reasoning by contributing PACS: the first audiovisual benchmark\nannotated for physical commonsense attributes. PACS contains 13,400\nquestion-answer pairs, involving 1,377 unique physical commonsense questions\nand 1,526 videos. Our dataset provides new opportunities to advance the\nresearch field of physical reasoning by bringing audio as a core component of\nthis multimodal problem. Using PACS, we evaluate multiple state-of-the-art\nmodels on our new challenging task. While some models show promising results\n(70% accuracy), they all fall short of human performance (95% accuracy). We\nconclude the paper by demonstrating the importance of multimodal reasoning and\nproviding possible avenues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samuel Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Speech Emotion Recognition Transformers for Linguistic Knowledge. (arXiv:2204.00400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00400","description":"<p>Large, pre-trained neural networks consisting of self-attention layers\n(transformers) have recently achieved state-of-the-art results on several\nspeech emotion recognition (SER) datasets. These models are typically\npre-trained in self-supervised manner with the goal to improve automatic speech\nrecognition performance -- and thus, to understand linguistic information. In\nthis work, we investigate the extent in which this information is exploited\nduring SER fine-tuning. Using a reproducible methodology based on open-source\ntools, we synthesise prosodically neutral speech utterances while varying the\nsentiment of the text. Valence predictions of the transformer model are very\nreactive to positive and negative sentiment content, as well as negations, but\nnot to intensifiers or reducers, while none of those linguistic features impact\narousal or dominance. These findings show that transformers can successfully\nleverage linguistic information to improve their valence predictions, and that\nlinguistic analysis should be included in their testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Triantafyllopoulos_A/0/1/0/all/0/1\">Andreas Triantafyllopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Johannes Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wierstorf_H/0/1/0/all/0/1\">Hagen Wierstorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Maximilian Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichel_U/0/1/0/all/0/1\">Uwe Reichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyben_F/0/1/0/all/0/1\">Florian Eyben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burkhardt_F/0/1/0/all/0/1\">Felix Burkhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.12793","description":"<p>In this work, we focus on the task of generating SPARQL queries from natural\nlanguage questions, which can then be executed on Knowledge Graphs (KGs). We\nassume that gold entity and relations have been provided, and the remaining\ntask is to arrange them in the right order along with SPARQL vocabulary, and\ninput tokens to produce the correct SPARQL query. Pre-trained Language Models\n(PLMs) have not been explored in depth on this task so far, so we experiment\nwith BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings,\nlooking for new baselines in the PLM era for this task, on DBpedia and Wikidata\nKGs. We show that T5 requires special input tokenisation, but produces state of\nthe art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms\ntask-specific models from previous works. Moreover, the methods enable semantic\nparsing for questions where a part of the input needs to be copied to the\noutput query, thus enabling a new paradigm in KG semantic parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_P/0/1/0/all/0/1\">Pranav Ajit Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_J/0/1/0/all/0/1\">Jivat Neet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language statistics at different spatial, temporal, and grammatical scales. (arXiv:2207.00709v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.00709","description":"<p>Statistical linguistics has advanced considerably in recent decades as data\nhas become available. This has allowed researchers to study how statistical\nproperties of languages change over time. In this work, we use data from\nTwitter to explore English and Spanish considering the rank diversity at\ndifferent scales: temporal (from 3 to 96 hour intervals), spatial (from 3km to\n3000+km radii), and grammatical (from monograms to pentagrams). We find that\nall three scales are relevant. However, the greatest changes come from\nvariations in the grammatical scale. At the lowest grammatical scale\n(monograms), the rank diversity curves are most similar, independently on the\nvalues of other scales, languages, and countries. As the grammatical scale\ngrows, the rank diversity curves vary more depending on the temporal and\nspatial scales, as well as on the language and country. We also study the\nstatistics of Twitter-specific tokens: emojis, hashtags, and user mentions.\nThese particular type of tokens show a sigmoid kind of behaviour as a rank\ndiversity function. Our results are helpful to quantify aspects of language\nstatistics that seem universal and what may lead to variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Puig_F/0/1/0/all/0/1\">Fernanda S&#xe1;nchez-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Aranda_R/0/1/0/all/0/1\">Rogelio Lozano-Aranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Mendez_D/0/1/0/all/0/1\">Dante P&#xe9;rez-M&#xe9;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colman_E/0/1/0/all/0/1\">Ewan Colman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_Guzman_A/0/1/0/all/0/1\">Alfredo J. Morales-Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_C/0/1/0/all/0/1\">Carlos Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_P/0/1/0/all/0/1\">Pedro Juan Rivera Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershenson_C/0/1/0/all/0/1\">Carlos Gershenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. (arXiv:2207.04429v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2207.04429","description":"<p>Goal-conditioned policies for robotic navigation can be trained on large,\nunannotated datasets, providing for good generalization to real-world settings.\nHowever, particularly in vision-based settings where specifying goals requires\nan image, this makes for an unnatural interface. Language provides a more\nconvenient modality for communication with robots, but contemporary methods\ntypically require expensive supervision, in the form of trajectories annotated\nwith language descriptions. We present a system, LM-Nav, for robotic navigation\nthat enjoys the benefits of training on unannotated large datasets of\ntrajectories, while still providing a high-level interface to the user. Instead\nof utilizing a labeled instruction following dataset, we show that such a\nsystem can be constructed entirely out of pre-trained models for navigation\n(ViNG), image-language association (CLIP), and language modeling (GPT-3),\nwithout requiring any fine-tuning or language-annotated robot data. We\ninstantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon\nnavigation through complex, outdoor environments from natural language\ninstructions. For videos of our experiments, code release, and an interactive\nColab notebook that runs in your browser, please check out our project page\nhttps://sites.google.com/view/lmnav\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Dhruv Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Blazej Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sockeye 3: Fast Neural Machine Translation with PyTorch. (arXiv:2207.05851v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05851","description":"<p>Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine\nTranslation (NMT). Now based on PyTorch, Sockeye 3 provides faster model\nimplementations and more advanced features with a further streamlined codebase.\nThis enables broader experimentation with faster iteration, efficient training\nof stronger and faster models, and the flexibility to move new ideas quickly\nfrom research to production. When running comparable models, Sockeye 3 is up to\n126% faster than other PyTorch implementations on GPUs and up to 292% faster on\nCPUs. Sockeye 3 is open source software released under the Apache 2.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hieber_F/0/1/0/all/0/1\">Felix Hieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denkowski_M/0/1/0/all/0/1\">Michael Denkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domhan_T/0/1/0/all/0/1\">Tobias Domhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_B/0/1/0/all/0/1\">Barbara Darques Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Celina Dong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_C/0/1/0/all/0/1\">Cuong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Ke Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_B/0/1/0/all/0/1\">Benjamin Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadejde_M/0/1/0/all/0/1\">Maria Nadejde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakew_S/0/1/0/all/0/1\">Surafel Lakew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Currey_A/0/1/0/all/0/1\">Anna Currey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform Libraries. (arXiv:2207.06803v2 [cs.MS] UPDATED)","link":"http://arxiv.org/abs/2207.06803","description":"<p>Discrete Fourier Transform (DFT) libraries are one of the most critical\nsoftware components for scientific computing. Inspired by FFTW, a widely used\nlibrary for DFT HPC calculations, we apply compiler technologies for the\ndevelopment of HPC Fourier transform libraries. In this work, we introduce\nFFTc, a domain-specific language, based on Multi-Level Intermediate\nRepresentation (MLIR), for expressing Fourier Transform algorithms. We present\nthe initial design, implementation, and preliminary results of FFTc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yifei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1\">Artur Podobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersson_M/0/1/0/all/0/1\">M&#xe5;ns I. Andersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Color Coding of Large Value Ranges Applied to Meteorological Data. (arXiv:2207.12399v1 [eess.IV])","link":"http://arxiv.org/abs/2207.12399","description":"<p>This paper presents a novel color scheme designed to address the challenge of\nvisualizing data series with large value ranges, where scale transformation\nprovides limited support. We focus on meteorological data, where the presence\nof large value ranges is common. We apply our approach to meteorological\nscatterplots, as one of the most common plots used in this domain area. Our\napproach leverages the numerical representation of mantissa and exponent of the\nvalues to guide the design of novel \"nested\" color schemes, able to emphasize\ndifferences between magnitudes. Our user study evaluates the new designs, the\nstate of the art color scales and representative color schemes used in the\nanalysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess\naccuracy, time and confidence in the context of discrimination (comparison) and\ninterpretation (reading) tasks. Our proposed color scheme significantly\noutperforms the others in interpretation tasks, while showing comparable\nperformances in discrimination tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Braun_D/0/1/0/all/0/1\">Daniel Braun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebell_K/0/1/0/all/0/1\">Kerstin Ebell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schemann_V/0/1/0/all/0/1\">Vera Schemann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pelchmann_L/0/1/0/all/0/1\">Laura Pelchmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crewell_S/0/1/0/all/0/1\">Susanne Crewell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borgo_R/0/1/0/all/0/1\">Rita Borgo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landesberger_T/0/1/0/all/0/1\">Tatiana von Landesberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Shape Sequence of Human Comparison and Classification using Current and Varifolds. (arXiv:2207.12485v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12485","description":"<p>In this paper we address the task of the comparison and the classification of\n3D shape sequences of human. The non-linear dynamics of the human motion and\nthe changing of the surface parametrization over the time make this task very\nchallenging. To tackle this issue, we propose to embed the 3D shape sequences\nin an infinite dimensional space, the space of varifolds, endowed with an inner\nproduct that comes from a given positive definite kernel. More specifically,\nour approach involves two steps: 1) the surfaces are represented as varifolds,\nthis representation induces metrics equivariant to rigid motions and invariant\nto parametrization; 2) the sequences of 3D shapes are represented by Gram\nmatrices derived from their infinite dimensional Hankel matrices. The problem\nof comparison of two 3D sequences of human is formulated as a comparison of two\nGram-Hankel matrices. Extensive experiments on CVSSP3D and Dyna datasets show\nthat our method is competitive with state-of-the-art in 3D human sequence\nmotion retrieval. Code for the experiments is available at\nhttps://github.com/CRISTAL-3DSAM/HumanComparisonVarifolds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1\">Emery Pierson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arguillere_S/0/1/0/all/0/1\">Sylvain Arguillere</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuriCam: Video Super-Resolution and Colorization Using Key Frames. (arXiv:2207.12496v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12496","description":"<p>We present NeuriCam, a key-frame video super-resolution and colorization\nbased system, to achieve low-power video capture from dual-mode IOT cameras.\nOur idea is to design a dual-mode camera system where the first mode is low\npower (1.1~mW) but only outputs gray-scale, low resolution and noisy video and\nthe second mode consumes much higher power (100~mW) but outputs color and\nhigher resolution images. To reduce total energy consumption, we heavily duty\ncycle the high power mode to output an image only once every second. The data\nfrom this camera system is then wirelessly streamed to a nearby plugged-in\ngateway, where we run our real-time neural network decoder to reconstruct a\nhigher resolution color video. To achieve this, we introduce an attention\nfeature filter mechanism that assigns different weights to different features,\nbased on the correlation between the feature map and contents of the input\nframe at each spatial location. We design a wireless hardware prototype using\noff-the-shelf cameras and address practical issues including packet loss and\nperspective mismatch. Our evaluation shows that our dual-camera hardware\nreduces camera energy consumption while achieving an average gray-scale PSNR\ngain of 3.7~dB over prior video super resolution methods and 5.6~dB RGB gain\nover existing color propagation methods. Open-source code:\nhttps://github.com/vb000/NeuriCam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veluri_B/0/1/0/all/0/1\">Bandhav Veluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Ali Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pernu_C/0/1/0/all/0/1\">Collin Pernu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Joshua Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1\">Michael Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gollakota_S/0/1/0/all/0/1\">Shyamnath Gollakota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based algorithm for assessment of knee osteoarthritis severity in radiographs matches performance of radiologists. (arXiv:2207.12521v1 [eess.IV])","link":"http://arxiv.org/abs/2207.12521","description":"<p>A fully-automated deep learning algorithm matched performance of radiologists\nin assessment of knee osteoarthritis severity in radiographs using the\nKellgren-Lawrence grading system.\n</p>\n<p>To develop an automated deep learning-based algorithm that jointly uses\nPosterior-Anterior (PA) and Lateral (LAT) views of knee radiographs to assess\nknee osteoarthritis severity according to the Kellgren-Lawrence grading system.\n</p>\n<p>We used a dataset of 9739 exams from 2802 patients from Multicenter\nOsteoarthritis Study (MOST). The dataset was divided into a training set of\n2040 patients, a validation set of 259 patients and a test set of 503 patients.\nA novel deep learning-based method was utilized for assessment of knee OA in\ntwo steps: (1) localization of knee joints in the images, (2) classification\naccording to the KL grading system. Our method used both PA and LAT views as\nthe input to the model. The scores generated by the algorithm were compared to\nthe grades provided in the MOST dataset for the entire test set as well as\ngrades provided by 5 radiologists at our institution for a subset of the test\nset.\n</p>\n<p>The model obtained a multi-class accuracy of 71.90% on the entire test set\nwhen compared to the ratings provided in the MOST dataset. The quadratic\nweighted Kappa coefficient for this set was 0.9066. The average quadratic\nweighted Kappa between all pairs of radiologists from our institution who took\na part of study was 0.748. The average quadratic-weighted Kappa between the\nalgorithm and the radiologists at our institution was 0.769.\n</p>\n<p>The proposed model performed demonstrated equivalency of KL classification to\nMSK radiologists, but clearly superior reproducibility. Our model also agreed\nwith radiologists at our institution to the same extent as the radiologists\nwith each other. The algorithm could be used to provide reproducible assessment\nof knee osteoarthritis severity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Swiecicki_A/0/1/0/all/0/1\">Albert Swiecicki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1\">Nianyi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ODonnell_J/0/1/0/all/0/1\">Jonathan O&#x27;Donnell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Said_N/0/1/0/all/0/1\">Nicholas Said</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jichen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mather_R/0/1/0/all/0/1\">Richard C. Mather</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiranek_W/0/1/0/all/0/1\">William A. Jiranek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trainability Preserving Neural Structured Pruning. (arXiv:2207.12534v1 [cs.LG])","link":"http://arxiv.org/abs/2207.12534","description":"<p>Several recent works empirically find finetuning learning rate is critical to\nthe final performance in neural network structured pruning. Further researches\nfind that the network trainability broken by pruning answers for it, thus\ncalling for an urgent need to recover trainability before finetuning. Existing\nattempts propose to exploit weight orthogonalization to achieve dynamical\nisometry for improved trainability. However, they only work for linear MLP\nnetworks. How to develop a filter pruning method that maintains or recovers\ntrainability and is scalable to modern deep networks remains elusive. In this\npaper, we present trainability preserving pruning (TPP), a regularization-based\nstructured pruning method that can effectively maintain trainability during\nsparsification. Specifically, TPP regularizes the gram matrix of convolutional\nkernels so as to de-correlate the pruned filters from the kept filters. Beside\nthe convolutional layers, we also propose to regularize the BN parameters for\nbetter preserving trainability. Empirically, TPP can compete with the\nground-truth dynamical isometry recovery method on linear MLP networks. On\nnon-linear networks (ResNet56/VGG19, CIFAR datasets), it outperforms the other\ncounterpart solutions by a large margin. Moreover, TPP can also work\neffectively with modern deep networks (ResNets) on ImageNet, delivering\nencouraging performance in comparison to many top-performing filter pruning\nmethods. To our best knowledge, this is the first approach that effectively\nmaintains trainability during pruning for the large-scale deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning. (arXiv:2207.12535v1 [cs.CR])","link":"http://arxiv.org/abs/2207.12535","description":"<p>Semi-supervised learning (SSL) leverages both labeled and unlabeled data to\ntrain machine learning (ML) models. State-of-the-art SSL methods can achieve\ncomparable performance to supervised learning by leveraging much fewer labeled\ndata. However, most existing works focus on improving the performance of SSL.\nIn this work, we take a different angle by studying the training data privacy\nof SSL. Specifically, we propose the first data augmentation-based membership\ninference attacks against ML models trained by SSL. Given a data sample and the\nblack-box access to a model, the goal of membership inference attack is to\ndetermine whether the data sample belongs to the training dataset of the model.\nOur evaluation shows that the proposed attack can consistently outperform\nexisting membership inference attacks and achieves the best performance against\nthe model trained by SSL. Moreover, we uncover that the reason for membership\nleakage in SSL is different from the commonly believed one in supervised\nlearning, i.e., overfitting (the gap between training and testing accuracy). We\nobserve that the SSL model is well generalized to the testing data (with almost\n0 overfitting) but ''memorizes'' the training data by giving a more confident\nprediction regardless of its correctness. We also explore early stopping as a\ncountermeasure to prevent membership inference attacks against SSL. The results\nshow that early stopping can mitigate the membership inference attack, but with\nthe cost of model's utility degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinlei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation. (arXiv:2207.12537v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12537","description":"<p>3D Human body pose and shape estimation within a temporal sequence can be\nquite critical for understanding human behavior. Despite the significant\nprogress in human pose estimation in the recent years, which are often based on\nsingle images or videos, human motion estimation on live stream videos is still\na rarely-touched area considering its special requirements for real-time output\nand temporal consistency. To address this problem, we present a temporally\nembedded 3D human body pose and shape estimation (TePose) method to improve the\naccuracy and temporal consistency of pose estimation in live stream videos.\nTePose uses previous predictions as a bridge to feedback the error for better\nestimation in the current frame and to learn the correspondence between data\nframes and predictions in the history. A multi-scale spatio-temporal graph\nconvolutional network is presented as the motion discriminator for adversarial\ntraining using datasets without any 3D labeling. We propose a sequential data\nloading strategy to meet the special start-to-end data processing requirement\nof live stream. We demonstrate the importance of each proposed module with\nextensive experiments. The results show the effectiveness of TePose on\nwidely-used human pose benchmarks with state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhouping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inter-Frame Compression for Dynamic Point Cloud Geometry Coding. (arXiv:2207.12554v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12554","description":"<p>Efficient point cloud compression is essential for applications like virtual\nand mixed reality, autonomous driving, and cultural heritage. In this paper, we\npropose a deep learning-based inter-frame encoding scheme for dynamic point\ncloud geometry compression. We propose a lossy geometry compression scheme that\npredicts the latent representation of the current frame using the previous\nframe by employing a novel prediction network. Our proposed network utilizes\nsparse convolutions with hierarchical multiscale 3D feature learning to encode\nthe current frame using the previous frame. We employ convolution on target\ncoordinates to map the latent representation of the previous frame to the\ndownsampled coordinates of the current frame to predict the current frame's\nfeature embedding. Our framework transmits the residual of the predicted\nfeatures and the actual features by compressing them using a learned\nprobabilistic factorized entropy model. At the receiver, the decoder\nhierarchically reconstructs the current frame by progressively rescaling the\nfeature embedding. We compared our model to the state-of-the-art Video-based\nPoint Cloud Compression (V-PCC) and Geometry-based Point Cloud Compression\n(G-PCC) schemes standardized by the Moving Picture Experts Group (MPEG). Our\nmethod achieves more than 91% BD-Rate Bjontegaard Delta Rate) reduction against\nG-PCC, more than 62% BD-Rate reduction against V-PCC intra-frame encoding mode,\nand more than 52% BD-Rate savings against V-PCC P-frame-based inter-frame\nencoding mode using HEVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_A/0/1/0/all/0/1\">Anique Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auwera_G/0/1/0/all/0/1\">Geert Van der Auwera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v1 [cs.LG])","link":"http://arxiv.org/abs/2207.12559","description":"<p>In this paper, we develop four spiking neural network (SNN) models for two\nstatic American Sign Language (ASL) hand gesture classification tasks, i.e.,\nthe ASL Alphabet and ASL Digits. The SNN models are deployed on Intel's\nneuromorphic platform, Loihi, and then compared against equivalent deep neural\nnetwork (DNN) models deployed on an edge computing device, the Intel Neural\nCompute Stick 2 (NCS2). We perform a comprehensive comparison between the two\nsystems in terms of accuracy, latency, power consumption, and energy. The best\nDNN model achieves an accuracy of 99.6% on the ASL Alphabet dataset, whereas\nthe best performing SNN model has an accuracy of 99.44%. For the ASL-Digits\ndataset, the best SNN model outperforms all of its DNN counterparts with 99.52%\naccuracy. Moreover, our obtained experimental results show that the Loihi\nneuromorphic hardware implementations achieve up to 14.67x and 4.09x reduction\nin power consumption and energy, respectively, when compared to NCS2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_M/0/1/0/all/0/1\">MohammedReza Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandarana_P/0/1/0/all/0/1\">Peyton Chandarana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seekings_J/0/1/0/all/0/1\">James Seekings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrix_S/0/1/0/all/0/1\">Sara Hendrix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zand_R/0/1/0/all/0/1\">Ramtin Zand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing Far in the Dark with Patterned Flash. (arXiv:2207.12570v1 [eess.IV])","link":"http://arxiv.org/abs/2207.12570","description":"<p>Flash illumination is widely used in imaging under low-light environments.\nHowever, illumination intensity falls off with propagation distance\nquadratically, which poses significant challenges for flash imaging at a long\ndistance. We propose a new flash technique, named ``patterned flash'', for\nflash imaging at a long distance. Patterned flash concentrates optical power\ninto a dot array. Compared with the conventional uniform flash where the signal\nis overwhelmed by the noise everywhere, patterned flash provides stronger\nsignals at sparsely distributed points across the field of view to ensure the\nsignals at those points stand out from the sensor noise. This enables\npost-processing to resolve important objects and details. Additionally, the\npatterned flash projects texture onto the scene, which can be treated as a\nstructured light system for depth perception. Given the novel system, we\ndevelop a joint image reconstruction and depth estimation algorithm with a\nconvolutional neural network. We build a hardware prototype and test the\nproposed flash technique on various scenes. The experimental results\ndemonstrate that our patterned flash has significantly better performance at\nlong distances in low-light environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Z/0/1/0/all/0/1\">Zhanghao Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nayar_S/0/1/0/all/0/1\">Shree Nayar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating a Visual LEGO Manual to a Machine-Executable Plan. (arXiv:2207.12572v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12572","description":"<p>We study the problem of translating an image-based, step-by-step assembly\nmanual created by human designers into machine-interpretable instructions. We\nformulate this problem as a sequential prediction task: at each step, our model\nreads the manual, locates the components to be added to the current shape, and\ninfers their 3D poses. This task poses the challenge of establishing a 2D-3D\ncorrespondence between the manual image and the real 3D object, and 3D pose\nestimation for unseen 3D objects, since a new component to be added in a step\ncan be an object built from previous steps. To address these two challenges, we\npresent a novel learning-based framework, the Manual-to-Executable-Plan Network\n(MEPNet), which reconstructs the assembly steps from a sequence of manual\nimages. The key idea is to integrate neural 2D keypoint detection modules and\n2D-3D projection algorithms for high-precision prediction and strong\ngeneralization to unseen components. The MEPNet outperforms existing methods on\nthree newly collected LEGO manual datasets and a Minecraft house dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruocheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiayuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chin-Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v1 [cs.CL])","link":"http://arxiv.org/abs/2207.12576","description":"<p>While vision-and-language models perform well on tasks such as visual\nquestion answering, they struggle when it comes to basic human commonsense\nreasoning skills. In this work, we introduce WinoGAViL: an online game to\ncollect vision-and-language associations, (e.g., werewolves to a full moon),\nused as a dynamic benchmark to evaluate state-of-the-art models. Inspired by\nthe popular card game Codenames, a spymaster gives a textual cue related to\nseveral visual candidates, and another player has to identify them. Human\nplayers are rewarded for creating associations that are challenging for a rival\nAI model but still solvable by other human players. We use the game to collect\n3.5K instances, finding that they are intuitive for humans (&gt;90% Jaccard index)\nbut challenging for state-of-the-art AI models, where the best model (ViLT)\nachieves a score of 52%, succeeding mostly where the cue is visually salient.\nOur analysis as well as the feedback we collect from players indicate that the\ncollected associations require diverse reasoning skills, including general\nknowledge, common sense, abstraction, and more. We release the dataset, the\ncode and the interactive game, aiming to allow future data collection that can\nbe used to develop models with better association abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guetta_N/0/1/0/all/0/1\">Nitzan Bitton Guetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1\">Ron Yosef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compiler-Aware Neural Architecture Search for On-Mobile Real-time Super-Resolution. (arXiv:2207.12577v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12577","description":"<p>Deep learning-based super-resolution (SR) has gained tremendous popularity in\nrecent years because of its high image quality performance and wide application\nscenarios. However, prior methods typically suffer from large amounts of\ncomputations and huge power consumption, causing difficulties for real-time\ninference, especially on resource-limited platforms such as mobile devices. To\nmitigate this, we propose a compiler-aware SR neural architecture search (NAS)\nframework that conducts depth search and per-layer width search with adaptive\nSR blocks. The inference speed is directly taken into the optimization along\nwith the SR loss to derive SR models with high image quality while satisfying\nthe real-time inference requirement. Instead of measuring the speed on mobile\ndevices at each iteration during the search process, a speed model incorporated\nwith compiler optimizations is leveraged to predict the inference latency of\nthe SR block with various width configurations for faster convergence. With the\nproposed framework, we achieve real-time SR inference for implementing 720p\nresolution with competitive SR performance (in terms of PSNR and SSIM) on\nGPU/DSP of mobile platforms (Samsung Galaxy S21).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yushu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RenderNet: Visual Relocalization Using Virtual Viewpoints in Large-Scale Indoor Environments. (arXiv:2207.12579v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12579","description":"<p>Visual relocalization has been a widely discussed problem in 3D vision: given\na pre-constructed 3D visual map, the 6 DoF (Degrees-of-Freedom) pose of a query\nimage is estimated. Relocalization in large-scale indoor environments enables\nattractive applications such as augmented reality and robot navigation.\nHowever, appearance changes fast in such environments when the camera moves,\nwhich is challenging for the relocalization system. To address this problem, we\npropose a virtual view synthesis-based approach, RenderNet, to enrich the\ndatabase and refine poses regarding this particular scenario. Instead of\nrendering real images which requires high-quality 3D models, we opt to directly\nrender the needed global and local features of virtual viewpoints and apply\nthem in the subsequent image retrieval and feature matching operations\nrespectively. The proposed method can largely improve the performance in\nlarge-scale indoor environments, e.g., achieving an improvement of 7.1\\% and\n12.2\\% on the Inloc dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shitao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1\">Kejie Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Le Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zilong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TGCF: Texture guided color fusion for impressionism oil painting style rendering. (arXiv:2207.12585v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12585","description":"<p>As a major branch of Non-Photorealistic Rendering (NPR), image stylization\nmainly uses the computer algorithms to render a photo into an artistic\npainting. Recent work has shown that the extraction of style information such\nas stroke texture and color of the target style image is the key to image\nstylization. Given its stroke texture and color characteristics, a new stroke\nrendering method is proposed, which fully considers the tonal characteristics\nand the representative color of the original oil painting, in order to fit the\ntone of the original oil painting image into the stylized image and make it\nclose to the artist's creative effect. The experiments have validated the\nefficacy of the proposed model. This method would be more suitable for the\nworks of pointillism painters with a relatively uniform sense of direction,\nespecially for natural scenes. When the original painting brush strokes have a\nclearer sense of direction, using this method to simulate brushwork texture\nfeatures can be less satisfactory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1\">Jing Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yijun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-displacement 3D Object Tracking with Hybrid Non-local Optimization. (arXiv:2207.12620v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12620","description":"<p>Optimization-based 3D object tracking is known to be precise and fast, but\nsensitive to large inter-frame displacements. In this paper we propose a fast\nand effective non-local 3D tracking method. Based on the observation that\nerroneous local minimum are mostly due to the out-of-plane rotation, we propose\na hybrid approach combining non-local and local optimizations for different\nparameters, resulting in efficient non-local search in the 6D pose space. In\naddition, a precomputed robust contour-based tracking method is proposed for\nthe pose optimization. By using long search lines with multiple candidate\ncorrespondences, it can adapt to different frame displacements without the need\nof coarse-to-fine search. After the pre-computation, pose updates can be\nconducted very fast, enabling the non-local optimization to run in real time.\nOur method outperforms all previous methods for both small and large\ndisplacements. For large displacements, the accuracy is greatly improved\n($81.7\\% \\;\\text{v.s.}\\; 19.4\\%$). At the same time, real-time speed ($&gt;$50fps)\ncan be achieved with only CPU. The source code is available at\n\\url{https://github.com/cvbubbles/nonlocal-3dtracking}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xuhui Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xinran Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xueying Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Attention Network for Compressed Video Referring Object Segmentation. (arXiv:2207.12622v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12622","description":"<p>Referring video object segmentation aims to segment the object referred by a\ngiven language expression. Existing works typically require compressed video\nbitstream to be decoded to RGB frames before being segmented, which increases\ncomputation and storage requirements and ultimately slows the inference down.\nThis may hamper its application in real-world computing resource limited\nscenarios, such as autonomous cars and drones. To alleviate this problem, in\nthis paper, we explore the referring object segmentation task on compressed\nvideos, namely on the original video data flow. Besides the inherent difficulty\nof the video referring object segmentation task itself, obtaining\ndiscriminative representation from compressed video is also rather challenging.\nTo address this problem, we propose a multi-attention network which consists of\ndual-path dual-attention module and a query-based cross-modal Transformer\nmodule. Specifically, the dual-path dual-attention module is designed to\nextract effective representation from compressed data in three modalities,\ni.e., I-frame, Motion Vector and Residual. The query-based cross-modal\nTransformer firstly models the correlation between linguistic and visual\nmodalities, and then the fused multi-modality features are used to guide object\nqueries to generate a content-aware dynamic kernel and to predict final\nsegmentation masks. Different from previous works, we propose to learn just one\nkernel, which thus removes the complicated post mask-matching procedure of\nexisting methods. Extensive promising experimental results on three challenging\ndatasets show the effectiveness of our method compared against several\nstate-of-the-art methods which are proposed for processing RGB data. Source\ncode is available at: https://github.com/DexiangHong/MANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dexiang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_L/0/1/0/all/0/1\">Laiyun Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guorong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchy Aware Features for Reducing Mistake Severity. (arXiv:2207.12646v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12646","description":"<p>Label hierarchies are often available apriori as part of biological taxonomy\nor language datasets WordNet. Several works exploit these to learn hierarchy\naware features in order to improve the classifier to make semantically\nmeaningful mistakes while maintaining or reducing the overall error. In this\npaper, we propose a novel approach for learning Hierarchy Aware Features (HAF)\nthat leverages classifiers at each level of the hierarchy that are constrained\nto generate predictions consistent with the label hierarchy. The classifiers\nare trained by minimizing a Jensen-Shannon Divergence with target soft labels\nobtained from the fine-grained classifiers. Additionally, we employ a simple\ngeometric loss that constrains the feature space geometry to capture the\nsemantic structure of the label space. HAF is a training time approach that\nimproves the mistakes while maintaining top-1 error, thereby, addressing the\nproblem of cross-entropy loss that treats all mistakes as equal. We evaluate\nHAF on three hierarchical datasets and achieve state-of-the-art results on the\niNaturalist-19 and CIFAR-100 datasets. The source code is available at\nhttps://github.com/07Agarg/HAF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ashima Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sani_D/0/1/0/all/0/1\">Depanshu Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1\">Saket Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12647","description":"<p>Existing visual question answering methods tend to capture the spurious\ncorrelations from visual and linguistic modalities, and fail to discover the\ntrue casual mechanism that facilitates reasoning truthfully based on the\ndominant visual evidence and the correct question intention. Additionally, the\nexisting methods usually ignore the complex event-level understanding in\nmulti-modal settings that requires a strong cognitive capability of causal\ninference to jointly model cross-modal event temporality, causality, and\ndynamics. In this work, we focus on event-level visual question answering from\na new perspective, i.e., cross-modal causal relational reasoning, by\nintroducing causal intervention methods to mitigate the spurious correlations\nand discover the true causal structures for the integration of visual and\nlinguistic modalities. Specifically, we propose a novel event-level visual\nquestion answering framework named Cross-Modal Causal RelatIonal Reasoning\n(CMCIR), to achieve robust casuality-aware visual-linguistic question\nanswering. To uncover the causal structures for visual and linguistic\nmodalities, the novel Causality-aware Visual-Linguistic Reasoning (CVLR) module\nis proposed to collaboratively disentangle the visual and linguistic spurious\ncorrelations via elaborately designed front-door and back-door causal\nintervention modules. To discover the fine-grained interactions between\nlinguistic semantics and spatial-temporal representations, we build a novel\nSpatial-Temporal Transformer (STT) that builds the multi-modal co-occurrence\ninteractions between visual and linguistic content. Extensive experiments on\nlarge-scale event-level urban dataset SUTD-TrafficQA and three benchmark\nreal-world datasets TGIF-QA, MSVD-QA, and MSRVTT-QA demonstrate the\neffectiveness of our CMCIR for discovering visual-linguistic causal structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Accurate Skeleton-Based Two-Person Interaction Recognition Using Inter- and Intra-body Graphs. (arXiv:2207.12648v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12648","description":"<p>Skeleton-based two-person interaction recognition has been gaining increasing\nattention as advancements are made in pose estimation and graph convolutional\nnetworks. Although the accuracy has been gradually improving, the increasing\ncomputational complexity makes it more impractical for a real-world\nenvironment. There is still room for accuracy improvement as the conventional\nmethods do not fully represent the relationship between inter-body joints. In\nthis paper, we propose a lightweight model for accurately recognizing\ntwo-person interactions. In addition to the architecture, which incorporates\nmiddle fusion, we introduce a factorized convolution technique to reduce the\nweight parameters of the model. We also introduce a network stream that\naccounts for relative distance changes between inter-body joints to improve\naccuracy. Experiments using two large-scale datasets, NTU RGB+D 60 and 120,\nshow that our method simultaneously achieved the highest accuracy and\nrelatively low computational complexity compared with the conventional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ito_Y/0/1/0/all/0/1\">Yoshiki Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Quan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morita_K/0/1/0/all/0/1\">Kenichi Morita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1\">Tomoaki Yoshinaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetric Scalable Cross-modal Hashing. (arXiv:2207.12650v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12650","description":"<p>Cross-modal hashing is a successful method to solve large-scale multimedia\nretrieval issue. A lot of matrix factorization-based hashing methods are\nproposed. However, the existing methods still struggle with a few problems,\nsuch as how to generate the binary codes efficiently rather than directly relax\nthem to continuity. In addition, most of the existing methods choose to use an\n$n\\times n$ similarity matrix for optimization, which makes the memory and\ncomputation unaffordable. In this paper we propose a novel Asymmetric Scalable\nCross-Modal Hashing (ASCMH) to address these issues. It firstly introduces a\ncollective matrix factorization to learn a common latent space from the\nkernelized features of different modalities, and then transforms the similarity\nmatrix optimization to a distance-distance difference problem minimization with\nthe help of semantic labels and common latent space. Hence, the computational\ncomplexity of the $n\\times n$ asymmetric optimization is relieved. In the\ngeneration of hash codes we also employ an orthogonal constraint of label\ninformation, which is indispensable for search accuracy. So the redundancy of\ncomputation can be much reduced. For efficient optimization and scalable to\nlarge-scale datasets, we adopt the two-step approach rather than optimizing\nsimultaneously. Extensive experiments on three benchmark datasets: Wiki,\nMIRFlickr-25K, and NUS-WIDE, demonstrate that our ASCMH outperforms the\nstate-of-the-art cross-modal hashing methods in terms of accuracy and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Deep Learning Assist Automatic Identification of Layered Pigments From XRF Data?. (arXiv:2207.12651v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12651","description":"<p>X-ray fluorescence spectroscopy (XRF) plays an important role for elemental\nanalysis in a wide range of scientific fields, especially in cultural heritage.\nXRF imaging, which uses a raster scan to acquire spectra across artworks,\nprovides the opportunity for spatial analysis of pigment distributions based on\ntheir elemental composition. However, conventional XRF-based pigment\nidentification relies on time-consuming elemental mapping by expert\ninterpretations of measured spectra. To reduce the reliance on manual work,\nrecent studies have applied machine learning techniques to cluster similar XRF\nspectra in data analysis and to identify the most likely pigments.\nNevertheless, it is still challenging for automatic pigment identification\nstrategies to directly tackle the complex structure of real paintings, e.g.\npigment mixtures and layered pigments. In addition, pixel-wise pigment\nidentification based on XRF imaging remains an obstacle due to the high noise\nlevel compared with averaged spectra. Therefore, we developed a\ndeep-learning-based end-to-end pigment identification framework to fully\nautomate the pigment identification process. In particular, it offers high\nsensitivity to the underlying pigments and to the pigments with a low\nconcentration, therefore enabling satisfying results in mapping the pigments\nbased on single-pixel XRF spectrum. As case studies, we applied our framework\nto lab-prepared mock-up paintings and two 19th-century paintings: Paul\nGauguin's Po\\`emes Barbares (1896) that contains layered pigments with an\nunderlying painting, and Paul Cezanne's The Bathers (1899-1904). The pigment\nidentification results demonstrated that our model achieved comparable results\nto the analysis by elemental mapping, suggesting the generalizability and\nstability of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bingjie/0/1/0/all/0/1\">Bingjie</a> (Jenny)Xu, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_P/0/1/0/all/0/1\">Pengxiao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermeulen_M/0/1/0/all/0/1\">Marc Vermeulen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGeachy_A/0/1/0/all/0/1\">Alicia McGeachy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kate Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eremin_K/0/1/0/all/0/1\">Katherine Eremin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayner_G/0/1/0/all/0/1\">Georgina Rayner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verri_G/0/1/0/all/0/1\">Giovanni Verri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willomitzer_F/0/1/0/all/0/1\">Florian Willomitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfeld_M/0/1/0/all/0/1\">Matthias Alfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tumblin_J/0/1/0/all/0/1\">Jack Tumblin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos Katsaggelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_M/0/1/0/all/0/1\">Marc Walton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection. (arXiv:2207.12654v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12654","description":"<p>Existing approaches for unsupervised point cloud pre-training are constrained\nto either scene-level or point/voxel-level instance discrimination. Scene-level\nmethods tend to lose local details that are crucial for recognizing the road\nobjects, while point/voxel-level methods inherently suffer from limited\nreceptive field that is incapable of perceiving large objects or context\nenvironments. Considering region-level representations are more suitable for 3D\nobject detection, we devise a new unsupervised point cloud pre-training\nframework, called ProposalContrast, that learns robust 3D representations by\ncontrasting region proposals. Specifically, with an exhaustive set of region\nproposals sampled from each point cloud, geometric point relations within each\nproposal are modeled for creating expressive proposal representations. To\nbetter accommodate 3D detection properties, ProposalContrast optimizes with\nboth inter-cluster and inter-proposal separation, i.e., sharpening the\ndiscriminativeness of proposal representations across semantic classes and\nobject instances. The generalizability and transferability of ProposalContrast\nare verified on various 3D detectors (i.e., PV-RCNN, CenterPoint, PointPillars\nand PointRCNN) and datasets (i.e., KITTI, Waymo and ONCE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junbo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised 3D Object Detection with Proficient Teachers. (arXiv:2207.12655v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12655","description":"<p>Dominated point cloud-based 3D object detectors in autonomous driving\nscenarios rely heavily on the huge amount of accurately labeled samples,\nhowever, 3D annotation in the point cloud is extremely tedious, expensive and\ntime-consuming. To reduce the dependence on large supervision, semi-supervised\nlearning (SSL) based approaches have been proposed. The Pseudo-Labeling\nmethodology is commonly used for SSL frameworks, however, the low-quality\npredictions from the teacher model have seriously limited its performance. In\nthis work, we propose a new Pseudo-Labeling framework for semi-supervised 3D\nobject detection, by enhancing the teacher model to a proficient one with\nseveral necessary designs. First, to improve the recall of pseudo labels, a\nSpatialtemporal Ensemble (STE) module is proposed to generate sufficient seed\nboxes. Second, to improve the precision of recalled boxes, a Clusteringbased\nBox Voting (CBV) module is designed to get aggregated votes from the clustered\nseed boxes. This also eliminates the necessity of sophisticated thresholds to\nselect pseudo labels. Furthermore, to reduce the negative influence of wrongly\npseudo-labeled samples during the training, a soft supervision signal is\nproposed by considering Box-wise Contrastive Learning (BCL). The effectiveness\nof our model is verified on both ONCE and Waymo datasets. For example, on ONCE,\nour approach significantly improves the baseline by 9.51 mAP. Moreover, with\nhalf annotations, our model outperforms the oracle model with full annotations\non Waymo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junbo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Network and Spatiotemporal Transformer Attention for 3D Video Object Detection from Point Clouds. (arXiv:2207.12659v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12659","description":"<p>Previous works for LiDAR-based 3D object detection mainly focus on the\nsingle-frame paradigm. In this paper, we propose to detect 3D objects by\nexploiting temporal information in multiple frames, i.e., the point cloud\nvideos. We empirically categorize the temporal information into short-term and\nlong-term patterns. To encode the short-term data, we present a Grid Message\nPassing Network (GMPNet), which considers each grid (i.e., the grouped points)\nas a node and constructs a k-NN graph with the neighbor grids. To update\nfeatures for a grid, GMPNet iteratively collects information from its\nneighbors, thus mining the motion cues in grids from nearby frames. To further\naggregate the long-term frames, we propose an Attentive Spatiotemporal\nTransformer GRU (AST-GRU), which contains a Spatial Transformer Attention (STA)\nmodule and a Temporal Transformer Attention (TTA) module. STA and TTA enhance\nthe vanilla GRU to focus on small objects and better align the moving objects.\nOur overall framework supports both online and offline video object detection\nin point clouds. We implement our algorithm based on prevalent anchor-based and\nanchor-free detectors. The evaluation results on the challenging nuScenes\nbenchmark show the superior performance of our method, achieving the 1st on the\nleaderboard without any bells and whistles, by the time the paper is submitted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junbo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training. (arXiv:2207.12661v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12661","description":"<p>Large-scale multi-modal contrastive pre-training has demonstrated great\nutility to learn transferable features for a range of downstream tasks by\nmapping multiple modalities into a shared embedding space. Typically, this has\nemployed separate encoders for each modality. However, recent work suggests\nthat transformers can support learning across multiple modalities and allow\nknowledge sharing. Inspired by this, we investigate a variety of\nModality-Shared Contrastive Language-Image Pre-training (MS-CLIP) frameworks.\nMore specifically, we question how many parameters of a transformer model can\nbe shared across modalities during contrastive pre-training, and rigorously\nexamine architectural design choices that position the proportion of parameters\nshared along a spectrum. In studied conditions, we observe that a mostly\nunified encoder for vision and language signals outperforms all other\nvariations that separate more parameters. Additionally, we find that\nlight-weight modality-specific parallel modules further improve performance.\nExperimental results show that the proposed MS-CLIP approach outperforms\nvanilla CLIP by up to 13\\% relative in zero-shot ImageNet classification\n(pre-trained on YFCC-100M), while simultaneously supporting a reduction of\nparameters. In addition, our approach outperforms vanilla CLIP by 1.6 points in\nlinear probing on a collection of 24 downstream vision tasks. Furthermore, we\ndiscover that sharing parameters leads to semantic concepts from different\nmodalities being encoded more closely in the embedding space, facilitating the\ntransferring of common semantic structure (e.g., attention patterns) from\nlanguage to vision. Code is available at\n\\href{https://github.com/Hxyou/MSCLIP}{URL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Kendall Shape Space Approach to 3D Shape Estimation from 2D Landmarks. (arXiv:2207.12687v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12687","description":"<p>3D shapes provide substantially more information than 2D images. However, the\nacquisition of 3D shapes is sometimes very difficult or even impossible in\ncomparison with acquiring 2D images, making it necessary to derive the 3D shape\nfrom 2D images. Although this is, in general, a mathematically ill-posed\nproblem, it might be solved by constraining the problem formulation using prior\ninformation. Here, we present a new approach based on Kendall's shape space to\nreconstruct 3D shapes from single monocular 2D images. The work is motivated by\nan application to study the feeding behavior of the basking shark, an\nendangered species whose massive size and mobility render 3D shape data nearly\nimpossible to obtain, hampering understanding of their feeding behaviors and\necology. 2D images of these animals in feeding position, however, are readily\navailable. We compare our approach with state-of-the-art shape-based\napproaches, both on human stick models and on shark head skeletons. Using a\nsmall set of training shapes, we show that the Kendall shape space approach is\nsubstantially more robust than previous methods and results in plausible\nshapes. This is essential for the motivating application in which specimens are\nrare and therefore only few training shapes are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paskin_M/0/1/0/all/0/1\">Martha Paskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_D/0/1/0/all/0/1\">Daniel Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_M/0/1/0/all/0/1\">Mason N. Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tycowicz_C/0/1/0/all/0/1\">Christoph von Tycowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CENet: Toward Concise and Efficient LiDAR Semantic Segmentation for Autonomous Driving. (arXiv:2207.12691v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12691","description":"<p>Accurate and fast scene understanding is one of the challenging task for\nautonomous driving, which requires to take full advantage of LiDAR point clouds\nfor semantic segmentation. In this paper, we present a \\textbf{concise} and\n\\textbf{efficient} image-based semantic segmentation network, named\n\\textbf{CENet}. In order to improve the descriptive power of learned features\nand reduce the computational as well as time complexity, our CENet integrates\nthe convolution with larger kernel size instead of MLP, carefully-selected\nactivation functions, and multiple auxiliary segmentation heads with\ncorresponding loss functions into architecture. Quantitative and qualitative\nexperiments conducted on publicly available benchmarks, SemanticKITTI and\nSemanticPOSS, demonstrate that our pipeline achieves much better mIoU and\ninference performance compared with state-of-the-art models. The code will be\navailable at https://github.com/huixiancheng/CENet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hui-Xian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xian-Feng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guo-Qiang Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Deep Learning and Machine Learning Models and Frameworks for Skin Lesion Classification. (arXiv:2207.12715v1 [eess.IV])","link":"http://arxiv.org/abs/2207.12715","description":"<p>The incidence rate for skin cancer has been steadily increasing throughout\nthe world, leading to it being a serious issue. Diagnosis at an early stage has\nthe potential to drastically reduce the harm caused by the disease, however,\nthe traditional biopsy is a labor-intensive and invasive procedure. In\naddition, numerous rural communities do not have easy access to hospitals and\ndo not prefer visiting one for what they feel might be a minor issue. Using\nmachine learning and deep learning for skin cancer classification can increase\naccessibility and reduce the discomforting procedures involved in the\ntraditional lesion detection process. These models can be wrapped in web or\nmobile apps and serve a greater population. In this paper, two such models are\ntested on the benchmark HAM10000 dataset of common skin lesions. They are\nRandom Forest with Stratified K-Fold Validation, and MobileNetV2 (throughout\nthe rest of the paper referred to as MobileNet). The MobileNet model was\ntrained separately using both TensorFlow and PyTorch frameworks. A side-by-side\ncomparison of both deep learning and machine learning models and a comparison\nof the same deep learning model on different frameworks for skin lesion\ndiagnosis in a resource-constrained mobile environment has not been conducted\nbefore. The results indicate that each of these models fares better at\ndifferent classification tasks. For greater overall recall, accuracy, and\ndetection of malignant melanoma, the TensorFlow MobileNet was the better\nchoice. However, for detecting noncancerous skin lesions, the PyTorch MobileNet\nproved to be better. Random Forest was the better algorithm when it came to\nhaving a low computational cost with moderate correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhosale_S/0/1/0/all/0/1\">Soham Bhosale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MV-FCOS3D++: Multi-View Camera-Only 4D Object Detection with Pretrained Monocular Backbones. (arXiv:2207.12716v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12716","description":"<p>In this technical report, we present our solution, dubbed MV-FCOS3D++, for\nthe Camera-Only 3D Detection track in Waymo Open Dataset Challenge 2022. For\nmulti-view camera-only 3D detection, methods based on bird-eye-view or 3D\ngeometric representations can leverage the stereo cues from overlapped regions\nbetween adjacent views and directly perform 3D detection without hand-crafted\npost-processing. However, it lacks direct semantic supervision for 2D\nbackbones, which can be complemented by pretraining simple monocular-based\ndetectors. Our solution is a multi-view framework for 4D detection following\nthis paradigm. It is built upon a simple monocular detector FCOS3D++,\npretrained only with object annotations of Waymo, and converts multi-view\nfeatures to a 3D grid space to detect 3D objects thereon. A dual-path neck for\nsingle-frame understanding and temporal stereo matching is devised to\nincorporate multi-frame information. Our method finally achieves 49.75% mAPL\nwith a single model and wins 2nd place in the WOD challenge, without any\nLiDAR-based depth supervision during training. The code will be released at\nhttps://github.com/Tai-Wang/Depth-from-Motion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1\">Qing Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional neural networks and multi-threshold analysis for contamination detection in the apparel industry. (arXiv:2207.12720v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12720","description":"<p>Quality control of apparel items is mandatory in modern textile industry, as\nconsumer's awareness and expectations about the highest possible standard is\nconstantly increasing in favor of sustainable and ethical textile products.\nSuch a level of quality is achieved by checking the product throughout its life\ncycle, from raw materials to boxed stock. Checks may include color shading\ntests, fasteners fatigue tests, fabric weigh tests, contamination tests, etc.\nThis work deals specifically with the automatic detection of contaminations\ngiven by small parts in the finished product such as raw material like little\nstones and plastic bits or materials from the construction process, like a\nwhole needle or a clip. Identification is performed by a two-level processing\nof X-ray images of the items: in the first, a multi-threshold analysis\nrecognizes the contaminations by gray level and shape attributes; the second\nlevel consists of a deep learning classifier that has been trained to\ndistinguish between true positives and false positives. The automatic detector\nwas successfully deployed in an actual production plant, since the results\nsatisfy the technical specification of the process, namely a number of false\nnegatives smaller than 3% and a number of false positives smaller than 15%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boresta_M/0/1/0/all/0/1\">Marco Boresta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_T/0/1/0/all/0/1\">Tommaso Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santis_A/0/1/0/all/0/1\">Alberto De Santis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\textbf{P$^2$A}$: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos. (arXiv:2207.12730v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12730","description":"<p>While deep learning has been widely used for video analytics, such as video\nclassification and action detection, dense action detection with fast-moving\nsubjects from sports videos is still challenging. In this work, we release yet\nanother sports video dataset $\\textbf{P$^2$A}$ for $\\underline{P}$ing\n$\\underline{P}$ong-$\\underline{A}$ction detection, which consists of 2,721\nvideo clips collected from the broadcasting videos of professional table tennis\nmatches in World Table Tennis Championships and Olympiads. We work with a crew\nof table tennis professionals and referees to obtain fine-grained action labels\n(in 14 classes) for every ping-pong action that appeared in the dataset and\nformulate two sets of action detection problems - action localization and\naction recognition. We evaluate a number of commonly-seen action recognition\n(e.g., TSM, TSN, Video SwinTransformer, and Slowfast) and action localization\nmodels (e.g., BSN, BSN++, BMN, TCANet), using $\\textbf{P$^2$A}$ for both\nproblems, under various settings. These models can only achieve 48% area under\nthe AR-AN curve for localization and 82% top-one accuracy for recognition since\nthe ping-pong actions are dense with fast-moving subjects but broadcasting\nvideos are with only 25 FPS. The results confirm that $\\textbf{P$^2$A}$ is\nstill a challenging task and can be used as a benchmark for action detection\nfrom videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feixiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution Learning Based on Evolutionary Algorithm Assisted Deep Neural Networks for Imbalanced Image Classification. (arXiv:2207.12744v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12744","description":"<p>To address the trade-off problem of quality-diversity for the generated\nimages in imbalanced classification tasks, we research on over-sampling based\nmethods at the feature level instead of the data level and focus on searching\nthe latent feature space for optimal distributions. On this basis, we propose\nan iMproved Estimation Distribution Algorithm based Latent featUre Distribution\nEvolution (MEDA_LUDE) algorithm, where a joint learning procedure is programmed\nto make the latent features both optimized and evolved by the deep neural\nnetworks and the evolutionary algorithm, respectively. We explore the effect of\nthe Large-margin Gaussian Mixture (L-GM) loss function on distribution learning\nand design a specialized fitness function based on the similarities among\nsamples to increase diversity. Extensive experiments on benchmark based\nimbalanced datasets validate the effectiveness of our proposed algorithm, which\ncan generate images with both quality and diversity. Furthermore, the MEDA_LUDE\nalgorithm is also applied to the industrial field and successfully alleviates\nthe imbalanced issue in fabric defect classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yudi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_K/0/1/0/all/0/1\">Kuangrong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chaochen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Bing Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Criteria Comparative Learning for Real-scene Image Super-Resolution. (arXiv:2207.12767v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12767","description":"<p>Real-scene image super-resolution aims to restore real-world low-resolution\nimages into their high-quality versions. A typical RealSR framework usually\nincludes the optimization of multiple criteria which are designed for different\nimage properties, by making the implicit assumption that the ground-truth\nimages can provide a good trade-off between different criteria. However, this\nassumption could be easily violated in practice due to the inherent contrastive\nrelationship between different image properties. Contrastive learning (CL)\nprovides a promising recipe to relieve this problem by learning discriminative\nfeatures using the triplet contrastive losses. Though CL has achieved\nsignificant success in many computer vision tasks, it is non-trivial to\nintroduce CL to RealSR due to the difficulty in defining valid positive image\npairs in this case. Inspired by the observation that the contrastive\nrelationship could also exist between the criteria, in this work, we propose a\nnovel training paradigm for RealSR, named Criteria Comparative Learning\n(Cria-CL), by developing contrastive losses defined on criteria instead of\nimage patches. In addition, a spatial projector is proposed to obtain a good\nview for Cria-CL in RealSR. Our experiments demonstrate that compared with the\ntypical weighted regression strategy, our method achieves a significant\nimprovement under similar parameter settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yukai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation. (arXiv:2207.12770v1 [eess.IV])","link":"http://arxiv.org/abs/2207.12770","description":"<p>Medical image segmentation can be implemented using Deep Learning methods\nwith fast and efficient segmentation networks. Single-board computers (SBCs)\nare difficult to use to train deep networks due to their memory and processing\nlimitations. Specific hardware such as Google's Edge TPU makes them suitable\nfor real time predictions using complex pre-trained networks. In this work, we\nstudy the performance of two SBCs, with and without hardware acceleration for\nfundus image segmentation, though the conclusions of this study can be applied\nto the segmentation by deep neural networks of other types of medical images.\nTo test the benefits of hardware acceleration, we use networks and datasets\nfrom a previous published work and generalize them by testing with a dataset\nwith ultrasound thyroid images. We measure prediction times in both SBCs and\ncompare them with a cloud based TPU system. The results show the feasibility of\nMachine Learning accelerated SBCs for optic disc and cup segmentation obtaining\ntimes below 25 milliseconds per image using Edge TPUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Civit_Masot_J/0/1/0/all/0/1\">Javier Civit-Masot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luna_Perejon_F/0/1/0/all/0/1\">Francisco Luna-Perejon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corral_J/0/1/0/all/0/1\">Jose Maria Rodriguez Corral</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dominguez_Morales_M/0/1/0/all/0/1\">Manuel Dominguez-Morales</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morgado_Estevez_A/0/1/0/all/0/1\">Arturo Morgado-Estevez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Civit_A/0/1/0/all/0/1\">Anton Civit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Static and Dynamic Concepts for Self-supervised Video Representation Learning. (arXiv:2207.12795v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12795","description":"<p>In this paper, we propose a novel learning scheme for self-supervised video\nrepresentation learning. Motivated by how humans understand videos, we propose\nto first learn general visual concepts then attend to discriminative local\nareas for video understanding. Specifically, we utilize static frame and frame\ndifference to help decouple static and dynamic concepts, and respectively align\nthe concept distributions in latent space. We add diversity and fidelity\nregularizations to guarantee that we learn a compact set of meaningful\nconcepts. Then we employ a cross-attention mechanism to aggregate detailed\nlocal features of different concepts, and filter out redundant concepts with\nlow activations to perform local concept contrast. Extensive experiments\ndemonstrate that our method distills meaningful static and dynamic concepts to\nguide video understanding, and obtains state-of-the-art results on UCF-101,\nHMDB-51, and Diving-48.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuangrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Aware Universum Inspired Re-Balance Learning for Long-Tailed Recognition. (arXiv:2207.12808v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12808","description":"<p>Data augmentation for minority classes is an effective strategy for\nlong-tailed recognition, thus developing a large number of methods. Although\nthese methods all ensure the balance in sample quantity, the quality of the\naugmented samples is not always satisfactory for recognition, being prone to\nsuch problems as over-fitting, lack of diversity, semantic drift, etc. For\nthese issues, we propose the Class-aware Universum Inspired Re-balance\nLearning(CaUIRL) for long-tailed recognition, which endows the Universum with\nclass-aware ability to re-balance individual minority classes from both sample\nquantity and quality. In particular, we theoretically prove that the\nclassifiers learned by CaUIRL are consistent with those learned under the\nbalanced condition from a Bayesian perspective. In addition, we further develop\na higher-order mixup approach, which can automatically generate class-aware\nUniversum(CaU) data without resorting to any external data. Unlike the\ntraditional Universum, such generated Universum additionally takes the domain\nsimilarity, class separability, and sample diversity into account. Extensive\nexperiments on benchmark datasets demonstrate the surprising advantages of our\nmethod, especially the top1 accuracy in minority classes is improved by 1.9% 6%\ncompared to the state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Enhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_C/0/1/0/all/0/1\">Chuanxing Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songcan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation. (arXiv:2207.12817v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12817","description":"<p>Body language is an eye-catching social signal and its automatic analysis can\nsignificantly advance artificial intelligence systems to understand and\nactively participate in social interactions. While computer vision has made\nimpressive progress in low-level tasks like head and body pose estimation, the\ndetection of more subtle behaviors such as gesturing, grooming, or fumbling is\nnot well explored. In this paper we present BBSI, the first set of annotations\nof complex Bodily Behaviors embedded in continuous Social Interactions in a\ngroup setting. Based on previous work in psychology, we manually annotated 26\nhours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15\ndistinct body language classes. We present comprehensive descriptive statistics\non the resulting dataset as well as results of annotation quality evaluations.\nFor automatic detection of these behaviors, we adapt the Pyramid Dilated\nAttention Network (PDAN), a state-of-the-art approach for human action\ndetection. We perform experiments using four variants of spatial-temporal\nfeatures as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment\nNetworks, Temporal Shift Module and Swin Transformer. Results are promising and\nindicate a great room for improvement in this difficult task. Representing a\nkey piece in the puzzle towards automatic understanding of social behavior,\nBBSI is fully available to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balazia_M/0/1/0/all/0/1\">Michal Balazia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1\">Philipp M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanczos_A/0/1/0/all/0/1\">&#xc1;kos Levente T&#xe1;nczos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liechtenstein_A/0/1/0/all/0/1\">August von Liechtenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Fran&#xe7;ois Br&#xe9;mond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning. (arXiv:2207.12819v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12819","description":"<p>State-of-the-art deep neural networks are still struggling to address the\ncatastrophic forgetting problem in continual learning. In this paper, we\npropose one simple paradigm (named as S-Prompting) and two concrete approaches\nto highly reduce the forgetting degree in one of the most typical continual\nlearning scenarios, i.e., domain increment learning (DIL). The key idea of the\nparadigm is to learn prompts independently across domains with pre-trained\ntransformers, avoiding the use of exemplars that commonly appear in\nconventional methods. This results in a win-win game where the prompting can\nachieve the best for each domain. The independent prompting across domains only\nrequests one single cross-entropy loss for training and one simple K-NN\noperation as a domain identifier for inference. The learning paradigm derives\nan image prompt learning approach and a brand-new language-image prompt\nlearning approach. Owning an excellent scalability (0.03% parameter increase\nper domain), the best of our approaches achieves a remarkable relative\nimprovement (an average of about 30%) over the best of the state-of-the-art\nexemplar-free methods for three standard DIL tasks, and even surpasses the best\nof them relatively by about 6% in average when they use exemplars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaopeng Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Human-Scene Interaction Synthesis with Semantic Control. (arXiv:2207.12824v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12824","description":"<p>Synthesizing natural interactions between virtual humans and their 3D\nenvironments is critical for numerous applications, such as computer games and\nAR/VR experiences. Our goal is to synthesize humans interacting with a given 3D\nscene controlled by high-level semantic specifications as pairs of action\ncategories and object instances, e.g., \"sit on the chair\". The key challenge of\nincorporating interaction semantics into the generation framework is to learn a\njoint representation that effectively captures heterogeneous information,\nincluding human body articulation, 3D object geometry, and the intent of the\ninteraction. To address this challenge, we design a novel transformer-based\ngenerative model, in which the articulated 3D human body surface points and 3D\nobjects are jointly encoded in a unified latent space, and the semantics of the\ninteraction between the human and objects are embedded via positional encoding.\nFurthermore, inspired by the compositional nature of interactions that humans\ncan simultaneously interact with multiple objects, we define interaction\nsemantics as the composition of varying numbers of atomic action-object pairs.\nOur proposed generative model can naturally incorporate varying numbers of\natomic interactions, which enables synthesizing compositional human-scene\ninteractions without requiring composite interaction data. We extend the PROX\ndataset with interaction semantic labels and scene instance segmentation to\nevaluate our method and demonstrate that our method can generate realistic\nhuman-scene interactions with semantic control. Our perceptual study shows that\nour synthesized virtual humans can naturally interact with 3D scenes,\nconsiderably outperforming existing methods. We name our method COINS, for\nCOmpositional INteraction Synthesis with Semantic Control. Code and data are\navailable at https://github.com/zkf1997/COINS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning. (arXiv:2207.12833v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12833","description":"<p>Eye trackers can provide visual guidance to sonographers during ultrasound\n(US) scanning. Such guidance is potentially valuable for less experienced\noperators to improve their scanning skills on how to manipulate the probe to\nachieve the desired plane. In this paper, a multimodal guidance approach\n(Multimodal-GuideNet) is proposed to capture the stepwise dependency between a\nreal-world US video signal, synchronized gaze, and probe motion within a\nunified framework. To understand the causal relationship between gaze movement\nand probe motion, our model exploits multitask learning to jointly learn two\nrelated tasks: predicting gaze movements and probe signals that an experienced\nsonographer would perform in routine obstetric scanning. The two tasks are\nassociated by a modality-aware spatial graph to detect the co-occurrence among\nthe multi-modality inputs and share useful cross-modal information. Instead of\na deterministic scanning path, Multimodal-GuideNet allows for scanning\ndiversity by estimating the probability distribution of real scans. Experiments\nperformed with three typical obstetric scanning examinations show that the new\napproach outperforms single-task learning for both probe motion guidance and\ngaze movement prediction. Multimodal-GuideNet also provides a visual guidance\nsignal with an error rate of less than 10 pixels for a 224x288 US image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1\">Qianhui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Clare Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drukker_L/0/1/0/all/0/1\">Lior Drukker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papageorghiou_A/0/1/0/all/0/1\">Aris T. Papageorghiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KinePose: A temporally optimized inverse kinematics technique for 6DOF human pose estimation with biomechanical constraints. (arXiv:2207.12841v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12841","description":"<p>Computer vision/deep learning-based 3D human pose estimation methods aim to\nlocalize human joints from images and videos. Pose representation is normally\nlimited to 3D joint positional/translational degrees of freedom (3DOFs),\nhowever, a further three rotational DOFs (6DOFs) are required for many\npotential biomechanical applications. Positional DOFs are insufficient to\nanalytically solve for joint rotational DOFs in a 3D human skeletal model.\nTherefore, we propose a temporal inverse kinematics (IK) optimization technique\nto infer joint orientations throughout a biomechanically informed, and\nsubject-specific kinematic chain. For this, we prescribe link directions from a\nposition-based 3D pose estimate. Sequential least squares quadratic programming\nis used to solve a minimization problem that involves both frame-based pose\nterms, and a temporal term. The solution space is constrained using joint DOFs,\nand ranges of motion (ROMs). We generate 3D pose motion sequences to assess the\nIK approach both for general accuracy, and accuracy in boundary cases. Our\ntemporal algorithm achieves 6DOF pose estimates with low Mean Per Joint Angular\nSeparation (MPJAS) errors (3.7{\\deg}/joint overall, &amp; 1.6{\\deg}/joint for lower\nlimbs). With frame-by-frame IK we obtain low errors in the case of bent elbows\nand knees, however, motion sequences with phases of extended/straight limbs\nresults in ambiguity in twist angle. With temporal IK, we reduce ambiguity for\nthese poses, resulting in lower average errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gildea_K/0/1/0/all/0/1\">Kevin Gildea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercadal_Baudart_C/0/1/0/all/0/1\">Clara Mercadal-Baudart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blythman_R/0/1/0/all/0/1\">Richard Blythman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolic_A/0/1/0/all/0/1\">Aljosa Smolic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simms_C/0/1/0/all/0/1\">Ciaran Simms</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Video Transformers in Action Recognition. (arXiv:2207.12842v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12842","description":"<p>Over the last few years, Unsupervised Domain Adaptation (UDA) techniques have\nacquired remarkable importance and popularity in computer vision. However, when\ncompared to the extensive literature available for images, the field of videos\nis still relatively unexplored. On the other hand, the performance of a model\nin action recognition is heavily affected by domain shift. In this paper, we\npropose a simple and novel UDA approach for video action recognition. Our\napproach leverages recent advances on spatio-temporal transformers to build a\nrobust source model that better generalises to the target domain. Furthermore,\nour architecture learns domain invariant features thanks to the introduction of\na novel alignment loss term derived from the Information Bottleneck principle.\nWe report results on two video action recognition benchmarks for UDA, showing\nstate-of-the-art performance on HMDB$\\leftrightarrow$UCF, as well as on\nKinetics$\\rightarrow$NEC-Drone, which is more challenging. This demonstrates\nthe effectiveness of our method in handling different levels of domain shift.\nThe source code is available at https://github.com/vturrisi/UDAVT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1\">Victor G. Turrisi da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zara_G/0/1/0/all/0/1\">Giacomo Zara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1\">Paolo Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_Santos_T/0/1/0/all/0/1\">Thiago Oliveira-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murino_V/0/1/0/all/0/1\">Vittorio Murino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bessel Equivariant Networks for Inversion of Transmission Effects in Multi-Mode Optical Fibres. (arXiv:2207.12849v1 [physics.optics])","link":"http://arxiv.org/abs/2207.12849","description":"<p>We develop a new type of model for solving the task of inverting the\ntransmission effects of multi-mode optical fibres through the construction of\nan $\\mathrm{SO}^{+}(2,1)$-equivariant neural network. This model takes\nadvantage of the of the azimuthal correlations known to exist in fibre speckle\npatterns and naturally accounts for the difference in spatial arrangement\nbetween input and speckle patterns. In addition, we use a second\npost-processing network to remove circular artifacts, fill gaps, and sharpen\nthe images, which is required due to the nature of optical fibre transmission.\nThis two stage approach allows for the inspection of the predicted images\nproduced by the more robust physically motivated equivariant model, which could\nbe useful in a safety-critical application, or by the output of both models,\nwhich produces high quality images. Further, this model can scale to previously\nunachievable resolutions of imaging with multi-mode optical fibres and is\ndemonstrated on $256 \\times 256$ pixel images. This is a result of improving\nthe trainable parameter requirement from $\\mathcal{O}(N^4)$ to\n$\\mathcal{O}(m)$, where $N$ is pixel size and $m$ is number of fibre modes.\nFinally, this model generalises to new images, outside of the set of training\ndata classes, better than previous models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Mitton_J/0/1/0/all/0/1\">Joshua Mitton</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mekhail_S/0/1/0/all/0/1\">Simon Peter Mekhail</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Padgett_M/0/1/0/all/0/1\">Miles Padgett</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Faccio_D/0/1/0/all/0/1\">Daniele Faccio</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Aversa_M/0/1/0/all/0/1\">Marco Aversa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Smart City Security: Violence and Weaponized Violence Detection using DCNN. (arXiv:2207.12850v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12850","description":"<p>In this ever connected society, CCTVs have had a pivotal role in enforcing\nsafety and security of the citizens by recording unlawful activities for the\nauthorities to take actions. In a smart city context, using Deep Convolutional\nNeural Networks (DCNN) to detection violence and weaponized violence from CCTV\nvideos will provide an additional layer of security by ensuring real-time\ndetection around the clock. In this work, we introduced a new specialised\ndataset by gathering real CCTV footage of both weaponized and non-weaponized\nviolence as well as non-violence videos from YouTube. We also proposed a novel\napproach in merging consecutive video frames into a single salient image which\nwill then be the input to the DCNN. Results from multiple DCNN architectures\nhave proven the effectiveness of our method by having the highest accuracy of\n99\\%. We also take into consideration the efficiency of our methods through\nseveral parameter trade-offs to ensure smart city sustainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aremu_T/0/1/0/all/0/1\">Toluwani Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhiyuan_L/0/1/0/all/0/1\">Li Zhiyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameeri_R/0/1/0/all/0/1\">Reem Alameeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloqaily_M/0/1/0/all/0/1\">Moayad Aloqaily</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1\">Mohsen Guizani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually explaining 3D-CNN predictions for video classification with an adaptive occlusion sensitivity analysis. (arXiv:2207.12859v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12859","description":"<p>This paper proposes a method for visually explaining the decision-making\nprocess of 3D convolutional neural networks (CNN) with a temporal extension of\nocclusion sensitivity analysis. The key idea here is to occlude a specific\nvolume of data by a 3D mask in an input 3D temporal-spatial data space and then\nmeasure the change degree in the output score. The occluded volume data that\nproduces a larger change degree is regarded as a more critical element for\nclassification. However, while the occlusion sensitivity analysis is commonly\nused to analyze single image classification, it is not so straightforward to\napply this idea to video classification as a simple fixed cuboid cannot deal\nwith the motions. To this end, we adapt the shape of a 3D occlusion mask to\ncomplicated motions of target objects. Our flexible mask adaptation is\nperformed by considering the temporal continuity and spatial co-occurrence of\nthe optical flows extracted from the input video data. We further propose to\napproximate our method by using the first-order partial derivative of the score\nwith respect to an input image to reduce its computational cost. We demonstrate\nthe effectiveness of our method through various and extensive comparisons with\nthe conventional methods in terms of the deletion/insertion metric and the\npointing metric on the UCF-101. The code is available at:\nhttps://github.com/uchiyama33/AOSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchiyama_T/0/1/0/all/0/1\">Tomoki Uchiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogi_N/0/1/0/all/0/1\">Naoya Sogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niinuma_K/0/1/0/all/0/1\">Koichiro Niinuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukui_K/0/1/0/all/0/1\">Kazuhiro Fukui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair. (arXiv:2207.12863v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12863","description":"<p>During the generation of invisible backdoor attack poisoned data, the feature\nspace transformation operation tends to cause the loss of some poisoned\nfeatures and weakens the mapping relationship between source images with\ntriggers and target labels, resulting in the need for a higher poisoning rate\nto achieve the corresponding backdoor attack success rate. To solve the above\nproblems, we propose the idea of feature repair for the first time and\nintroduce the blind watermark technique to repair the poisoned features lost\nduring the generation of poisoned data. Under the premise of ensuring\nconsistent labeling, we propose a low-poisoning rate invisible backdoor attack\nbased on feature repair, named FRIB. Benefiting from the above design concept,\nthe new method enhances the mapping relationship between the source images with\ntriggers and the target labels, and increases the degree of misleading DNNs,\nthus achieving a high backdoor attack success rate with a very low poisoning\nrate. Ultimately, the detailed experimental results show that the goal of\nachieving a high success rate of backdoor attacks with a very low poisoning\nrate is achieved on all MNIST, CIFAR10, GTSRB, and ImageNet datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Hui Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiugui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xiangyun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition. (arXiv:2207.12866v1 [eess.AS])","link":"http://arxiv.org/abs/2207.12866","description":"<p>In this article gesture recognition and speech recognition applications are\nimplemented on embedded systems with Tiny Machine Learning (TinyML). It\nfeatures 3-axis accelerometer, 3-axis gyroscope and 3-axis magnetometer. The\ngesture recognition,provides an innovative approach nonverbal communication. It\nhas wide applications in human-computer interaction and sign language. Here in\nthe implementation of hand gesture recognition, TinyML model is trained and\ndeployed from EdgeImpulse framework for hand gesture recognition and based on\nthe hand movements, Arduino Nano 33 BLE device having 6-axis IMU can find out\nthe direction of movement of hand. The Speech is a mode of communication.\nSpeech recognition is a way by which the statements or commands of human speech\nis understood by the computer which reacts accordingly. The main aim of speech\nrecognition is to achieve communication between man and machine. Here in the\nimplementation of speech recognition, TinyML model is trained and deployed from\nEdgeImpulse framework for speech recognition and based on the keywords\npronounced by human, Arduino Nano 33 BLE device having built-in microphone can\nmake an RGB LED glow like red, green or blue based on keyword pronounced. The\nresults of each application are obtained and listed in the results section and\ngiven the analysis upon the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+V_V/0/1/0/all/0/1\">Viswanatha V</a>, <a href=\"http://arxiv.org/find/eess/1/au:+C_R/0/1/0/all/0/1\">Ramachandra A.C</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_R/0/1/0/all/0/1\">Raghavendra Prasanna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kakarla_P/0/1/0/all/0/1\">Prem Chowdary Kakarla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+PJ_V/0/1/0/all/0/1\">Viveka Simha PJ</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohan_N/0/1/0/all/0/1\">Nishant Mohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Probabilistic U-Net for medical image segementation. (arXiv:2207.12872v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12872","description":"<p>We propose the Generalized Probabilistic U-Net, which extends the\nProbabilistic U-Net by allowing more general forms of the Gaussian distribution\nas the latent space distribution that can better approximate the uncertainty in\nthe reference segmentations. We study the effect the choice of latent space\ndistribution has on capturing the uncertainty in the reference segmentations\nusing the LIDC-IDRI dataset. We show that the choice of distribution affects\nthe sample diversity of the predictions and their overlap with respect to the\nreference segmentations. For the LIDC-IDRI dataset, we show that using a\nmixture of Gaussians results in a statistically significant improvement in the\ngeneralized energy distance (GED) metric with respect to the standard\nProbabilistic U-Net. We have made our implementation available at\nhttps://github.com/ishaanb92/GeneralizedProbabilisticUNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_I/0/1/0/all/0/1\">Ishaan Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P.W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of road traffic crashes based on collision estimation. (arXiv:2207.12886v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12886","description":"<p>This paper introduces a framework based on computer vision that can detect\nroad traffic crashes (RCTs) by using the installed surveillance/CCTV camera and\nreport them to the emergency in real-time with the exact location and time of\noccurrence of the accident. The framework is built of five modules. We start\nwith the detection of vehicles by using YOLO architecture; The second module is\nthe tracking of vehicles using MOSSE tracker, Then the third module is a new\napproach to detect accidents based on collision estimation. Then the fourth\nmodule for each vehicle, we detect if there is a car accident or not based on\nthe violent flow descriptor (ViF) followed by an SVM classifier for crash\nprediction. Finally, in the last stage, if there is a car accident, the system\nwill send a notification to the emergency by using a GPS module that provides\nus with the location, time, and date of the accident to be sent to the\nemergency with the help of the GSM module. The main objective is to achieve\nhigher accuracy with fewer false alarms and to implement a simple system based\non pipelining technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Essam_M/0/1/0/all/0/1\">Mohamed Essam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_N/0/1/0/all/0/1\">Nagia M. Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ismail_M/0/1/0/all/0/1\">Mohamed A. Ismail</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection. (arXiv:2207.12888v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12888","description":"<p>Visual question answering (VQA) often requires an understanding of visual\nconcepts and language semantics, which relies on external knowledge. Most\nexisting methods exploit pre-trained language models or/and unstructured text,\nbut the knowledge in these resources are often incomplete and noisy. Some\nmethods prefer to use knowledge graphs (KGs) which often have intensive\nstructured knowledge, but the research is still quite preliminary. In this\npaper, we propose LaKo, a knowledge-driven VQA method via Late\nKnowledge-to-text Injection. To effectively incorporate an external KG, we\ntransfer triples into text and propose a late injection mechanism. Finally we\naddress VQA as a text generation task with an effective encoder-decoder\nparadigm. In the evaluation with OKVQA datasets, our method achieves\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yuxia Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Image Registration using a Training-Time Privileged Third Modality. (arXiv:2207.12901v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12901","description":"<p>In this work, we consider the task of pairwise cross-modality image\nregistration, which may benefit from exploiting additional images available\nonly at training time from an additional modality that is different to those\nbeing registered. As an example, we focus on aligning intra-subject\nmultiparametric Magnetic Resonance (mpMR) images, between T2-weighted (T2w)\nscans and diffusion-weighted scans with high b-value (DWI$_{high-b}$). For the\napplication of localising tumours in mpMR images, diffusion scans with zero\nb-value (DWI$_{b=0}$) are considered easier to register to T2w due to the\navailability of corresponding features. We propose a learning from privileged\nmodality algorithm, using a training-only imaging modality DWI$_{b=0}$, to\nsupport the challenging multi-modality registration problems. We present\nexperimental results based on 369 sets of 3D multiparametric MRI images from\n356 prostate cancer patients and report, with statistical significance, a\nlowered median target registration error of 4.34 mm, when registering the\nholdout DWI$_{high-b}$ and T2w image pairs, compared with that of 7.96 mm\nbefore registration. Results also show that the proposed learning-based\nregistration networks enabled efficient registration with comparable or better\naccuracy, compared with a classical iterative algorithm and other tested\nlearning-based methods with/without the additional modality. These compared\nalgorithms also failed to produce any significantly improved alignment between\nDWI$_{high-b}$ and T2w in this challenging application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_D/0/1/0/all/0/1\">David Atkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syer_T/0/1/0/all/0/1\">Tom Syer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punwani_S/0/1/0/all/0/1\">Shonit Punwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction. (arXiv:2207.12909v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12909","description":"<p>Recent work achieved impressive progress towards joint reconstruction of\nhands and manipulated objects from monocular color images. Existing methods\nfocus on two alternative representations in terms of either parametric meshes\nor signed distance fields (SDFs). On one side, parametric models can benefit\nfrom prior knowledge at the cost of limited shape deformations and mesh\nresolutions. Mesh models, hence, may fail to precisely reconstruct details such\nas contact surfaces of hands and objects. SDF-based methods, on the other side,\ncan represent arbitrary details but are lacking explicit priors. In this work\nwe aim to improve SDF models using priors provided by parametric\nrepresentations. In particular, we propose a joint learning framework that\ndisentangles the pose and the shape. We obtain hand and object poses from\nparametric models and use them to align SDFs in 3D space. We show that such\naligned SDFs better focus on reconstructing shape details and improve\nreconstruction accuracy both for hands and objects. We evaluate our method and\ndemonstrate significant improvements over the state of the art on the\nchallenging ObMan and DexYCB benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zerui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasson_Y/0/1/0/all/0/1\">Yana Hasson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Guide to Image and Video based Small Object Detection using Deep Learning : Case Study of Maritime Surveillance. (arXiv:2207.12926v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12926","description":"<p>Small object detection (SOD) in optical images and videos is a challenging\nproblem that even state-of-the-art generic object detection methods fail to\naccurately localize and identify such objects. Typically, small objects appear\nin real-world due to large camera-object distance. Because small objects occupy\nonly a small area in the input image (e.g., less than 10%), the information\nextracted from such a small area is not always rich enough to support decision\nmaking. Multidisciplinary strategies are being developed by researchers working\nat the interface of deep learning and computer vision to enhance the\nperformance of SOD deep learning based methods. In this paper, we provide a\ncomprehensive review of over 160 research papers published between 2017 and\n2022 in order to survey this growing subject. This paper summarizes the\nexisting literature and provide a taxonomy that illustrates the broad picture\nof current research. We investigate how to improve the performance of small\nobject detection in maritime environments, where increasing performance is\ncritical. By establishing a connection between generic and maritime SOD\nresearch, future directions have been identified. In addition, the popular\ndatasets that have been used for SOD for generic and maritime applications are\ndiscussed, and also well-known evaluation metrics for the state-of-the-art\nmethods on some of the datasets are provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rekavandi_A/0/1/0/all/0/1\">Aref Miri Rekavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seghouane_A/0/1/0/all/0/1\">Abd-Krim Seghouane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefs_S/0/1/0/all/0/1\">Stephen Hoefs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Reliable Online Method for Joint Estimation of Focal Length and Camera Rotation. (arXiv:2207.12934v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12934","description":"<p>Linear perspectivecues deriving from regularities of the built environment\ncan be used to recalibrate both intrinsic and extrinsic camera parameters\nonline, but these estimates can be unreliable due to irregularities in the\nscene, uncertainties in line segment estimation and background clutter. Here we\naddress this challenge through four initiatives. First, we use the PanoContext\npanoramic image dataset [27] to curate a novel and realistic dataset of planar\nprojections over a broad range of scenes, focal lengths and camera poses.\nSecond, we use this novel dataset and the YorkUrbanDB [4] to systematically\nevaluate the linear perspective deviation measures frequently found in the\nliterature and show that the choice of deviation measure and likelihood model\nhas a huge impact on reliability. Third, we use these findings to create a\nnovel system for online camera calibration we call fR, and show that it\noutperforms the prior state of the art, substantially reducing error in\nestimated camera rotation and focal length. Our fourth contribution is a novel\nand efficient approach to estimating uncertainty that can dramatically improve\nonline reliability for performance-critical applications by strategically\nselecting which frames to use for recalibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elder_J/0/1/0/all/0/1\">James H. Elder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation for Autonomous Driving: Model Evaluation, Dataset Generation, Perspective Comparison, and Real-Time Capability. (arXiv:2207.12939v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12939","description":"<p>Environmental perception is an important aspect within the field of\nautonomous vehicles that provides crucial information about the driving domain,\nincluding but not limited to identifying clear driving areas and surrounding\nobstacles. Semantic segmentation is a widely used perception method for\nself-driving cars that associates each pixel of an image with a predefined\nclass. In this context, several segmentation models are evaluated regarding\naccuracy and efficiency. Experimental results on the generated dataset confirm\nthat the segmentation model FasterSeg is fast enough to be used in realtime on\nlowpower computational (embedded) devices in self-driving cars. A simple method\nis also introduced to generate synthetic training data for the model. Moreover,\nthe accuracy of the first-person perspective and the bird's eye view\nperspective are compared. For a $320 \\times 256$ input in the first-person\nperspective, FasterSeg achieves $65.44\\,\\%$ mean Intersection over Union\n(mIoU), and for a $320 \\times 256$ input from the bird's eye view perspective,\nFasterSeg achieves $64.08\\,\\%$ mIoU. Both perspectives achieve a frame rate of\n$247.11$ Frames per Second (FPS) on the NVIDIA Jetson AGX Xavier. Lastly, the\nframe rate and the accuracy with respect to the arithmetic 16-bit Floating\nPoint (FP16) and 32-bit Floating Point (FP32) of both perspectives are measured\nand compared on the target hardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cakir_S/0/1/0/all/0/1\">Senay Cakir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauss_M/0/1/0/all/0/1\">Marcel Gau&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Happeler_K/0/1/0/all/0/1\">Kai H&#xe4;ppeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounajjar_Y/0/1/0/all/0/1\">Yassine Ounajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinle_F/0/1/0/all/0/1\">Fabian Heinle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchthaler_R/0/1/0/all/0/1\">Reiner Marchthaler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Generalizable Latent Representations for Novel Degradations in Super Resolution. (arXiv:2207.12941v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12941","description":"<p>Typical methods for blind image super-resolution (SR) focus on dealing with\nunknown degradations by directly estimating them or learning the degradation\nrepresentations in a latent space. A potential limitation of these methods is\nthat they assume the unknown degradations can be simulated by the integration\nof various handcrafted degradations (e.g., bicubic downsampling), which is not\nnecessarily true. The real-world degradations can be beyond the simulation\nscope by the handcrafted degradations, which are referred to as novel\ndegradations. In this work, we propose to learn a latent representation space\nfor degradations, which can be generalized from handcrafted (base) degradations\nto novel degradations. The obtained representations for a novel degradation in\nthis latent space are then leveraged to generate degraded images consistent\nwith the novel degradation to compose paired training data for SR model.\nFurthermore, we perform variational inference to match the posterior of\ndegradations in latent representation space with a prior distribution (e.g.,\nGaussian distribution). Consequently, we are able to sample more high-quality\nrepresentations for a novel degradation to augment the training data for SR\nmodel. We conduct extensive experiments on both synthetic and real-world\ndatasets to validate the effectiveness and advantages of our method for blind\nsuper-resolution with novel degradations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMF: Adaptable Weighting Fusion with Multiple Fine-tuning for Image Classification. (arXiv:2207.12944v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12944","description":"<p>Fine-tuning is widely applied in image classification tasks as a transfer\nlearning approach. It re-uses the knowledge from a source task to learn and\nobtain a high performance in target tasks. Fine-tuning is able to alleviate the\nchallenge of insufficient training data and expensive labelling of new data.\nHowever, standard fine-tuning has limited performance in complex data\ndistributions. To address this issue, we propose the Adaptable Multi-tuning\nmethod, which adaptively determines each data sample's fine-tuning strategy. In\nthis framework, multiple fine-tuning settings and one policy network are\ndefined. The policy network in Adaptable Multi-tuning can dynamically adjust to\nan optimal weighting to feed different samples into models that are trained\nusing different fine-tuning strategies. Our method outperforms the standard\nfine-tuning approach by 1.69%, 2.79% on the datasets FGVC-Aircraft, and\nDescribable Texture, yielding comparable performance on the datasets Stanford\nCars, CIFAR-10, and Fashion-MNIST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1\">Jo Plested</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caldwell_S/0/1/0/all/0/1\">Sabrina Caldwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Text Block Detection towards Scene Text Understanding. (arXiv:2207.12955v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12955","description":"<p>Most existing scene text detectors focus on detecting characters or words\nthat only capture partial text messages due to missing contextual information.\nFor a better understanding of text in scenes, it is more desired to detect\ncontextual text blocks (CTBs) which consist of one or multiple integral text\nunits (e.g., characters, words, or phrases) in natural reading order and\ntransmit certain complete text messages. This paper presents contextual text\ndetection, a new setup that detects CTBs for better understanding of texts in\nscenes. We formulate the new setup by a dual detection task which first detects\nintegral text units and then groups them into a CTB. To this end, we design a\nnovel scene text clustering technique that treats integral text units as tokens\nand groups them (belonging to the same CTB) into an ordered token sequence. In\naddition, we create two datasets SCUT-CTW-Context and ReCTS-Context to\nfacilitate future research, where each CTB is well annotated by an ordered\nsequence of integral text units. Further, we introduce three metrics that\nmeasure contextual text detection in local accuracy, continuity, and global\naccuracy. Extensive experiments show that our method accurately detects CTBs\nwhich effectively facilitates downstream tasks such as text classification and\ntranslation. The project is available at\nhttps://sg-vilab.github.io/publication/xue2022contextual/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chuhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation. (arXiv:2207.12964v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12964","description":"<p>Incremental few-shot semantic segmentation (IFSS) targets at incrementally\nexpanding model's capacity to segment new class of images supervised by only a\nfew samples. However, features learned on old classes could significantly\ndrift, causing catastrophic forgetting. Moreover, few samples for pixel-level\nsegmentation on new classes lead to notorious overfitting issues in each\nlearning session. In this paper, we explicitly represent class-based knowledge\nfor semantic segmentation as a category embedding and a hyper-class embedding,\nwhere the former describes exclusive semantical properties, and the latter\nexpresses hyper-class knowledge as class-shared semantic properties. Aiming to\nsolve IFSS problems, we present EHNet, i.e., Embedding adaptive-update and\nHyper-class representation Network from two aspects. First, we propose an\nembedding adaptive-update strategy to avoid feature drift, which maintains old\nknowledge by hyper-class representation, and adaptively update category\nembeddings with a class-attention scheme to involve new classes learned in\nindividual sessions. Second, to resist overfitting issues caused by few\ntraining samples, a hyper-class embedding is learned by clustering all category\nembeddings for initialization and aligned with category embedding of the new\nclass for enhancement, where learned knowledge assists to learn new knowledge,\nthus alleviating performance dependence on training data scale. Significantly,\nthese two designs provide representation capability for classes with sufficient\nsemantics and limited biases, enabling to perform segmentation tasks requiring\nhigh semantic dependence. Experiments on PASCAL-5i and COCO datasets show that\nEHNet achieves new state-of-the-art performance with remarkable advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guangchen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yirui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Shaohua Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nondestructive Quality Control in Powder Metallurgy using Hyperspectral Imaging. (arXiv:2207.12966v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12966","description":"<p>Measuring the purity in the metal powder is critical for preserving the\nquality of additive manufacturing products. Contamination is one of the most\nheadache problems which can be caused by multiple reasons and lead to the\nas-built components cracking and malfunctions. Existing methods for\nmetallurgical condition assessment are mostly time-consuming and mainly focus\non the physical integrity of structure rather than material composition.\nThrough capturing spectral data from a wide frequency range along with the\nspatial information, hyperspectral imaging (HSI) can detect minor differences\nin terms of temperature, moisture and chemical composition. Therefore, HSI can\nprovide a unique way to tackle this challenge. In this paper, with the use of a\nnear-infrared HSI camera, applications of HSI for the non-destructive\ninspection of metal powders are introduced. Technical assumptions and solutions\non three step-by-step case studies are presented in detail, including powder\ncharacterization, contamination detection, and band selection analysis.\nExperimental results have fully demonstrated the great potential of HSI and\nrelated AI techniques for NDT of powder metallurgy, especially the potential to\nsatisfy the industrial manufacturing environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yijun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jinchang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">He Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFiner: A Full-Scale Refinement Approach for Multiple Object Tracking. (arXiv:2207.12967v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12967","description":"<p>Multiple object tracking (MOT) is the task containing detection and\nassociation. Plenty of trackers have achieved competitive performance.\nUnfortunately, for the lack of informative exchange on these subtasks, they are\noften biased toward one of the two and remain underperforming in complex\nscenarios, such as the expected false negatives and mistaken trajectories of\ntargets when passing each other. In this paper, we propose TransFiner, a\ntransformer-based post-refinement approach for MOT. It is a generic attachment\nframework that leverages the images and tracking results (locations and class\npredictions) from the original tracker as inputs, which are then used to launch\nTransFiner powerfully. Moreover, TransFiner depends on query pairs, which\nproduce pairs of detection and motion through the fusion decoder and achieve\ncomprehensive tracking improvement. We also provide targeted refinement by\nlabeling query pairs according to different refinement levels. Experiments show\nthat our design is effective, on the MOT17 benchmark, we elevate the\nCenterTrack from 67.8% MOTA and 64.7% IDF1 to 71.5% MOTA and 66.8% IDF1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiale Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Every Thing in the Wild. (arXiv:2207.12978v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12978","description":"<p>Current multi-category Multiple Object Tracking (MOT) metrics use class\nlabels to group tracking results for per-class evaluation. Similarly, MOT\nmethods typically only associate objects with the same class predictions. These\ntwo prevalent strategies in MOT implicitly assume that the classification\nperformance is near-perfect. However, this is far from the case in recent\nlarge-scale MOT datasets, which contain large numbers of classes with many rare\nor semantically similar categories. Therefore, the resulting inaccurate\nclassification leads to sub-optimal tracking and inadequate benchmarking of\ntrackers. We address these issues by disentangling classification from\ntracking. We introduce a new metric, Track Every Thing Accuracy (TETA),\nbreaking tracking measurement into three sub-factors: localization,\nassociation, and classification, allowing comprehensive benchmarking of\ntracking performance even under inaccurate classification. TETA also deals with\nthe challenging incomplete annotation problem in large-scale tracking datasets.\nWe further introduce a Track Every Thing tracker (TETer), that performs\nassociation using Class Exemplar Matching (CEM). Our experiments show that TETA\nevaluates trackers more comprehensively, and TETer achieves significant\nimprovements on the challenging large-scale datasets BDD100K and TAO compared\nto the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Thomas E. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient One Pass Self-distillation with Zipf's Label Smoothing. (arXiv:2207.12980v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12980","description":"<p>Self-distillation exploits non-uniform soft supervision from itself during\ntraining and improves performance without any runtime cost. However, the\noverhead during training is often overlooked, and yet reducing time and memory\noverhead during training is increasingly important in the giant models' era.\nThis paper proposes an efficient self-distillation method named Zipf's Label\nSmoothing (Zipf's LS), which uses the on-the-fly prediction of a network to\ngenerate soft supervision that conforms to Zipf distribution without using any\ncontrastive samples or auxiliary parameters. Our idea comes from an empirical\nobservation that when the network is duly trained the output values of a\nnetwork's final softmax layer, after sorting by the magnitude and averaged\nacross samples, should follow a distribution reminiscent to Zipf's Law in the\nword frequency statistics of natural languages. By enforcing this property on\nthe sample level and throughout the whole training period, we find that the\nprediction accuracy can be greatly improved. Using ResNet50 on the INAT21\nfine-grained classification dataset, our technique achieves +3.61% accuracy\ngain compared to the vanilla baseline, and 0.88% more gain against the previous\nlabel smoothing or self-distillation strategies. The implementation is publicly\navailable at https://github.com/megvii-research/zipfls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiajun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_Z/0/1/0/all/0/1\">Zhaodong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Borui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Deep Neural Networks for Point Clouds using Gradient-based Visualisations. (arXiv:2207.12984v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12984","description":"<p>Explaining decisions made by deep neural networks is a rapidly advancing\nresearch topic. In recent years, several approaches have attempted to provide\nvisual explanations of decisions made by neural networks designed for\nstructured 2D image input data. In this paper, we propose a novel approach to\ngenerate coarse visual explanations of networks designed to classify\nunstructured 3D data, namely point clouds. Our method uses gradients flowing\nback to the final feature map layers and maps these values as contributions of\nthe corresponding points in the input point cloud. Due to dimensionality\ndisagreement and lack of spatial consistency between input points and final\nfeature maps, our approach combines gradients with points dropping to compute\nexplanations of different parts of the point cloud iteratively. The generality\nof our approach is tested on various point cloud classification networks,\nincluding 'single object' networks PointNet, PointNet++, DGCNN, and a 'scene'\nnetwork VoteNet. Our method generates symmetric explanation maps that highlight\nimportant regions and provide insight into the decision-making process of\nnetwork architectures. We perform an exhaustive evaluation of trust and\ninterpretability of our explanation method against comparative approaches using\nquantitative, quantitative and human studies. All our code is implemented in\nPyTorch and will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tayyub_J/0/1/0/all/0/1\">Jawad Tayyub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarmad_M/0/1/0/all/0/1\">Muhammad Sarmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonborn_N/0/1/0/all/0/1\">Nicolas Sch&#xf6;nborn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular 3D Object Detection with Depth from Motion. (arXiv:2207.12988v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12988","description":"<p>Perceiving 3D objects from monocular inputs is crucial for robotic systems,\ngiven its economy compared to multi-sensor settings. It is notably difficult as\na single image can not provide any clues for predicting absolute depth values.\nMotivated by binocular methods for 3D object detection, we take advantage of\nthe strong geometry structure provided by camera ego-motion for accurate object\ndepth estimation and detection. We first make a theoretical analysis on this\ngeneral two-view case and notice two challenges: 1) Cumulative errors from\nmultiple estimations that make the direct prediction intractable; 2) Inherent\ndilemmas caused by static cameras and matching ambiguity. Accordingly, we\nestablish the stereo correspondence with a geometry-aware cost volume as the\nalternative for depth estimation and further compensate it with monocular\nunderstanding to address the second problem. Our framework, named Depth from\nMotion (DfM), then uses the established geometry to lift 2D image features to\nthe 3D space and detects 3D objects thereon. We also present a pose-free DfM to\nmake it usable when the camera pose is unavailable. Our framework outperforms\nstate-of-the-art methods by a large margin on the KITTI benchmark. Detailed\nquantitative and qualitative analyses also validate our theoretical\nconclusions. The code will be released at\nhttps://github.com/Tai-Wang/Depth-from-Motion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V$^2$L: Leveraging Vision and Vision-language Models into Large-scale Product Retrieval. (arXiv:2207.12994v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12994","description":"<p>Product retrieval is of great importance in the ecommerce domain. This paper\nintroduces our 1st-place solution in eBay eProduct Visual Search Challenge\n(FGVC9), which is featured for an ensemble of about 20 models from vision\nmodels and vision-language models. While model ensemble is common, we show that\ncombining the vision models and vision-language models brings particular\nbenefits from their complementarity and is a key factor to our superiority.\nSpecifically, for the vision models, we use a two-stage training pipeline which\nfirst learns from the coarse labels provided in the training set and then\nconducts fine-grained self-supervised training, yielding a coarse-to-fine\nmetric learning manner. For the vision-language models, we use the textual\ndescription of the training image as the supervision signals for fine-tuning\nthe image-encoder (feature extractor). With these designs, our solution\nachieves 0.7623 MAR@10, ranking the first place among all the competitors. The\ncode is available at: \\href{https://github.com/WangWenhao0716/V2L}{V$^2$L}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Efficient Segmentation of Cross-domain Medical Images. (arXiv:2207.12995v1 [cs.CV])","link":"http://arxiv.org/abs/2207.12995","description":"<p>Efficient medical image segmentation aims to provide accurate pixel-wise\nprediction for the medical images with the lightweight implementation\nframework. However, lightweight frameworks generally fail to achieve high\nperformance, and suffer from the poor generalizable ability on cross-domain\ntasks.In this paper, we propose a generalizable knowledge distillation method\nfor robust and efficient segmentation of cross-domain medical images.\nPrimarily, we propose the Model-Specific Alignment Networks (MSAN) to provide\nthe domain-invariant representations which are regularized by a Pre-trained\nSemantic AutoEncoder (P-SAE). Meanwhile, a customized Alignment Consistency\nTraining (ACT) strategy is designed to promote the MSAN training. With the\ndomain-invariant representative vectors in MSAN, we propose two generalizable\nknowledge distillation schemes, Dual Contrastive Graph Distillation (DCGD) and\nDomain-Invariant Cross Distillation (DICD). Specifically, in DCGD, two types of\nimplicit contrastive graphs are designed to represent the intra-coupling and\ninter-coupling semantic correlations from the perspective of data distribution.\nIn DICD, the domain-invariant semantic vectors from the two models (i.e.,\nteacher and student) are leveraged to cross-reconstruct features by the header\nexchange of MSAN, which achieves generalizable improvement for both the encoder\nand decoder in the student model. Furthermore, a metric named Fr\\'echet\nSemantic Distance (FSD) is tailored to verify the effectiveness of the\nregularized domain-invariant features. Extensive experiments conducted on the\nLiver and Retinal Vessel Segmentation datasets demonstrate the priority of our\nmethod, in terms of performance and generalization on lightweight frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuojie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Min Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological Optimized Convolutional Visual Recurrent Network for Brain Tumor Segmentation and Classification. (arXiv:2207.13021v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13021","description":"<p>In today's world of health care, brain tumor (BT) detection has become a\ncommon occurrence. However, the manual BT classification approach is\ntime-consuming and only available at a few diagnostic centres. So Deep\nConvolutional Neural Network (DCNN) is introduced in the medical field for\nmaking accurate diagnoses and aiding in the patient's treatment before surgery.\nBut these networks have problems such as overfitting and being unable to\nextract necessary features for classification. To overcome these problems, we\ndeveloped the TDA-IPH and Convolutional Transfer learning and Visual Recurrent\nlearning with Elephant Herding Optimization hyper-parameter tuning (CTVR-EHO)\nmodels for BT segmentation and classification. Initially, the Topological Data\nAnalysis based Improved Persistent Homology (TDA-IPH) is designed to segment\nthe BT image. Then, from the segmented image, features are extracted\nsimultaneously using TL via the AlexNet model and Bidirectional Visual Long\nShort Term Memory (Bi-VLSTM). Elephant Herding Optimization (EHO) is used to\ntune the hyper parameters of both networks to get an optimal result. Finally,\nextracted features are concatenated and classified using the softmax activation\nlayer. The simulation result of this proposed CTVR-EHO and TDA-IPH method is\nanalysed based on some metrics such as precision, accuracy, recall, loss, and F\nscore. When compared to other existing BT segmentation and classification\nmodels, the proposed CTVR-EHO and TDA-IPH approaches show high accuracy\n(99.8%), high recall (99.23%), high precision (99.67%), and high F score\n(99.59%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Joshi_D/0/1/0/all/0/1\">Dhananjay Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nagwanshi_K/0/1/0/all/0/1\">Kapil Kumar Nagwanshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choubey_N/0/1/0/all/0/1\">Nitin S. Choubey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajput_N/0/1/0/all/0/1\">Naveen Singh Rajput</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Pair based Self-Similarity Learning for Unsupervised Person Re-identification. (arXiv:2207.13035v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13035","description":"<p>Person re-identification (re-ID) is of great importance to video surveillance\nsystems by estimating the similarity between a pair of cross-camera person\nshorts. Current methods for estimating such similarity require a large number\nof labeled samples for supervised training. In this paper, we present a\npseudo-pair based self-similarity learning approach for unsupervised person\nre-ID without human annotations. Unlike conventional unsupervised re-ID methods\nthat use pseudo labels based on global clustering, we construct patch surrogate\nclasses as initial supervision, and propose to assign pseudo labels to images\nthrough the pairwise gradient-guided similarity separation. This can cluster\nimages in pseudo pairs, and the pseudos can be updated during training. Based\non pseudo pairs, we propose to improve the generalization of similarity\nfunction via a novel self-similarity learning:it learns local discriminative\nfeatures from individual images via intra-similarity, and discovers the patch\ncorrespondence across images via inter-similarity. The intra-similarity\nlearning is based on channel attention to detect diverse local features from an\nimage. The inter-similarity learning employs a deformable convolution with a\nnon-local block to align patches for cross-image similarity. Experimental\nresults on several re-ID benchmark datasets demonstrate the superiority of the\nproposed method over the state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Deyin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dapeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jialie Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Resolution-Adaptive Representations for Cross-Resolution Person Re-Identification. (arXiv:2207.13037v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13037","description":"<p>The cross-resolution person re-identification (CRReID) problem aims to match\nlow-resolution (LR) query identity images against high resolution (HR) gallery\nimages. It is a challenging and practical problem since the query images often\nsuffer from resolution degradation due to the different capturing conditions\nfrom real-world cameras. To address this problem, state-of-the-art (SOTA)\nsolutions either learn the resolution-invariant representation or adopt\nsuper-resolution (SR) module to recover the missing information from the LR\nquery. This paper explores an alternative SR-free paradigm to directly compare\nHR and LR images via a dynamic metric, which is adaptive to the resolution of a\nquery image. We realize this idea by learning resolution-adaptive\nrepresentations for cross-resolution comparison. Specifically, we propose two\nresolution-adaptive mechanisms. The first one disentangles the\nresolution-specific information into different sub-vectors in the penultimate\nlayer of the deep neural networks, and thus creates a varying-length\nrepresentation. To better extract resolution-dependent information, we further\npropose to learn resolution-adaptive masks for intermediate residual feature\nblocks. A novel progressive learning strategy is proposed to train those masks\nproperly. These two mechanisms are combined to boost the performance of CRReID.\nExperimental results show that the proposed method is superior to existing\napproaches and achieves SOTA performance on multiple CRReID benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models. (arXiv:2207.13038v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13038","description":"<p>Novel architectures have recently improved generative image synthesis leading\nto excellent visual quality in various tasks. Of particular note is the field\nof ``AI-Art'', which has seen unprecedented growth with the emergence of\npowerful multimodal models such as CLIP. By combining speech and image\nsynthesis models, so-called ``prompt-engineering'' has become established, in\nwhich carefully selected and composed sentences are used to achieve a certain\nvisual style in the synthesized image. In this note, we present an alternative\napproach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set\nof nearest neighbors is retrieved from an external database during training for\neach training instance, and the diffusion model is conditioned on these\ninformative samples. During inference (sampling), we replace the retrieval\ndatabase with a more specialized database that contains, for example, only\nimages of a particular visual style. This provides a novel way to prompt a\ngeneral trained model after training and thereby specify a particular visual\nstyle. As shown by our experiments, this approach is superior to specifying the\nvisual style within the text prompt. We open-source code and model weights at\nhttps://github.com/CompVis/latent-diffusion .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1\">Andreas Blattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient High-Resolution Deep Learning: A Survey. (arXiv:2207.13050v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13050","description":"<p>Cameras in modern devices such as smartphones, satellites and medical\nequipment are capable of capturing very high resolution images and videos. Such\nhigh-resolution data often need to be processed by deep learning models for\ncancer detection, automated road navigation, weather prediction, surveillance,\noptimizing agricultural processes and many other applications. Using\nhigh-resolution images and videos as direct inputs for deep learning models\ncreates many challenges due to their high number of parameters, computation\ncost, inference latency and GPU memory consumption. Simple approaches such as\nresizing the images to a lower resolution are common in the literature,\nhowever, they typically significantly decrease accuracy. Several works in the\nliterature propose better alternatives in order to deal with the challenges of\nhigh-resolution data and improve accuracy and speed while complying with\nhardware limitations and time restrictions. This survey describes such\nefficient high-resolution deep learning methods, summarizes real-world\napplications of high-resolution deep learning, and provides comprehensive\ninformation about available high-resolution datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1\">Arian Bakhtiarnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsStories: Illustrating articles with visual summaries. (arXiv:2207.13061v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13061","description":"<p>Recent self-supervised approaches have used large-scale image-text datasets\nto learn powerful representations that transfer to many tasks without\nfinetuning. These methods often assume that there is one-to-one correspondence\nbetween its images and their (short) captions. However, many tasks require\nreasoning about multiple images and long text narratives, such as describing\nnews articles with visual summaries. Thus, we explore a novel setting where the\ngoal is to learn a self-supervised visual-language representation that is\nrobust to varying text length and the number of images. In addition, unlike\nprior work which assumed captions have a literal relation to the image, we\nassume images only contain loose illustrative correspondence with the text. To\nexplore this problem, we introduce a large-scale multimodal dataset containing\nover 31M articles, 22M images and 1M videos. We show that state-of-the-art\nimage-text alignment methods are not robust to longer narratives with multiple\nimages. Finally, we introduce an intuitive baseline that outperforms these\nmethods on zero-shot image-set retrieval by 10% on the GoodNews dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Reuben Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1\">JP Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sud_A/0/1/0/all/0/1\">Avneesh Sud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_T/0/1/0/all/0/1\">Thomas Leung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis. (arXiv:2207.13064v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13064","description":"<p>As tools for content editing mature, and artificial intelligence (AI) based\nalgorithms for synthesizing media grow, the presence of manipulated content\nacross online media is increasing. This phenomenon causes the spread of\nmisinformation, creating a greater need to distinguish between \"real'' and\n\"manipulated'' content. To this end, we present VideoSham, a dataset consisting\nof 826 videos (413 real and 413 manipulated). Many of the existing deepfake\ndatasets focus exclusively on two types of facial manipulations -- swapping\nwith a different subject's face or altering the existing face. VideoSham, on\nthe other hand, contains more diverse, context-rich, and human-centric,\nhigh-resolution videos manipulated using a combination of 6 different spatial\nand temporal attacks. Our analysis shows that state-of-the-art manipulation\ndetection algorithms only work for a few specific attacks and do not scale well\non VideoSham. We performed a user study on Amazon Mechanical Turk with 1200\nparticipants to understand if they can differentiate between the real and\nmanipulated videos in VideoSham. Finally, we dig deeper into the strengths and\nweaknesses of performances by humans and SOTA-algorithms to identify gaps that\nneed to be filled with better AI algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1\">Trisha Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Ritwik Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Viswanathan Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeFakePro: Decentralized DeepFake Attacks Detection using ENF Authentication. (arXiv:2207.13070v1 [cs.CR])","link":"http://arxiv.org/abs/2207.13070","description":"<p>Advancements in generative models, like Deepfake allows users to imitate a\ntargeted person and manipulate online interactions. It has been recognized that\ndisinformation may cause disturbance in society and ruin the foundation of\ntrust. This article presents DeFakePro, a decentralized consensus\nmechanism-based Deepfake detection technique in online video conferencing\ntools. Leveraging Electrical Network Frequency (ENF), an environmental\nfingerprint embedded in digital media recording, affords a consensus mechanism\ndesign called Proof-of-ENF (PoENF) algorithm. The similarity in ENF signal\nfluctuations is utilized in the PoENF algorithm to authenticate the media\nbroadcasted in conferencing tools. By utilizing the video conferencing setup\nwith malicious participants to broadcast deep fake video recordings to other\nparticipants, the DeFakePro system verifies the authenticity of the incoming\nmedia in both audio and video channels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagothu_D/0/1/0/all/0/1\">Deeraj Nagothu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ronghua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blasch_E/0/1/0/all/0/1\">Erik Blasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aved_A/0/1/0/all/0/1\">Alexander Aved</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DETRs with Hybrid Matching. (arXiv:2207.13080v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13080","description":"<p>One-to-one set matching is a key design for DETR to establish its end-to-end\ncapability, so that object detection does not require a hand-crafted NMS\n(non-maximum suppression) method to remove duplicate detections. This\nend-to-end signature is important for the versatility of DETR, and it has been\ngeneralized to a wide range of visual problems, including instance/semantic\nsegmentation, human pose estimation, and point cloud/multi-view-images based\ndetection, etc. However, we note that because there are too few queries\nassigned as positive samples, the one-to-one set matching significantly reduces\nthe training efficiency of positive samples. This paper proposes a simple yet\neffective method based on a hybrid matching scheme that combines the original\none-to-one matching branch with auxiliary queries that use one-to-many matching\nloss during training. This hybrid strategy has been shown to significantly\nimprove training efficiency and improve accuracy. In inference, only the\noriginal one-to-one match branch is used, thus maintaining the end-to-end merit\nand the same inference efficiency of DETR. The method is named\n$\\mathcal{H}$-DETR, and it shows that a wide range of representative DETR\nmethods can be consistently improved across a wide range of visual tasks,\nincluding Deformable-DETR, 3DETR/PETRv2, PETR, and TransTrack, among others.\nCode will be available at: https://github.com/HDETR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Ding Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haodi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaopei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haojun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Agnostic and Post-hoc Unseen Distribution Detection. (arXiv:2207.13083v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13083","description":"<p>Despite the recent advances in out-of-distribution(OOD) detection, anomaly\ndetection, and uncertainty estimation tasks, there do not exist a task-agnostic\nand post-hoc approach. To address this limitation, we design a novel\nclustering-based ensembling method, called Task Agnostic and Post-hoc Unseen\nDistribution Detection (TAPUDD) that utilizes the features extracted from the\nmodel trained on a specific task. Explicitly, it comprises of TAP-Mahalanobis,\nwhich clusters the training datasets' features and determines the minimum\nMahalanobis distance of the test sample from all clusters. Further, we propose\nthe Ensembling module that aggregates the computation of iterative\nTAP-Mahalanobis for a different number of clusters to provide reliable and\nefficient cluster computation. Through extensive experiments on synthetic and\nreal-world datasets, we observe that our approach can detect unseen samples\neffectively across diverse tasks and performs better or on-par with the\nexisting baselines. To this end, we eliminate the necessity of determining the\noptimal value of the number of clusters and demonstrate that our method is more\nviable for large-scale classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dua_R/0/1/0/all/0/1\">Radhika Dua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group DETR: Fast Training Convergence with Decoupled One-to-Many Label Assignment. (arXiv:2207.13085v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13085","description":"<p>Detection Transformer (DETR) relies on One-to-One label assignment, i.e.,\nassigning one ground-truth (gt) object to only one positive object query, for\nend-to-end object detection and lacks the capability of exploiting multiple\npositive queries. We present a novel DETR training approach, named {\\em Group\nDETR}, to support multiple positive queries. To be specific, we decouple the\npositives into multiple independent groups and keep only one positive per gt\nobject in each group. We make simple modifications during training: (i) adopt\n$K$ groups of object queries; (ii) conduct decoder self-attention on each group\nof object queries with the same parameters; (iii) perform One-to-One label\nassignment for each group, leading to $K$ positive object queries for each gt\nobject. In inference, we only use one group of object queries, making no\nmodifications to both architecture and processes. We validate the effectiveness\nof the proposed approach on DETR variants, including Conditional DETR,\nDAB-DETR, DN-DETR, and DINO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What you get is not always what you see: pitfalls in solar array assessment using overhead imagery. (arXiv:1902.10895v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1902.10895","description":"<p>Effective integration planning for small, distributed solar photovoltaic (PV)\narrays into electric power grids requires access to high quality data: the\nlocation and power capacity of individual solar PV arrays. Unfortunately,\nnational databases of small-scale solar PV do not exist; those that do are\nlimited in their spatial resolution, typically aggregated up to state or\nnational levels. While several promising approaches for solar PV detection have\nbeen published, strategies for evaluating the performance of these models are\noften highly heterogeneous from study to study. The resulting comparison of\nthese methods for practical applications for energy assessments becomes\nchallenging and may imply that the reported performance evaluations are overly\noptimistic. The heterogeneity comes in many forms, each of which we explore in\nthis work: the level of spatial aggregation, the validation of ground truth,\ninconsistencies in the training and validation datasets, and the degree of\ndiversity of the locations and sensors from which the training and validation\ndata originate. For each, we discuss emerging practices from the literature to\naddress them or suggest directions of future research. As part of our\ninvestigation, we evaluate solar PV identification performance in two large\nregions. Our findings suggest that traditional performance evaluation of the\nautomated identification of solar PV from satellite imagery may be optimistic\ndue to common limitations in the validation process. The takeaways from this\nwork are intended to inform and catalyze the large-scale practical application\nof automated solar PV assessment techniques by energy researchers and\nprofessionals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_K/0/1/0/all/0/1\">Kyle Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malof_J/0/1/0/all/0/1\">Jordan M. Malof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bohao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Streltsov_A/0/1/0/all/0/1\">Artem Streltsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1\">K. Sydny Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoen_B/0/1/0/all/0/1\">Ben Hoen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Signal Models for Data Augmentation in Deep Learning ATR. (arXiv:2012.09284v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.09284","description":"<p>Automatic Target Recognition (ATR) algorithms classify a given Synthetic\nAperture Radar (SAR) image into one of the known target classes using a set of\ntraining images available for each class. Recently, learning methods have shown\nto achieve state-of-the-art classification accuracy if abundant training data\nis available, sampled uniformly over the classes, and their poses. In this\npaper, we consider the task of ATR with a limited set of training images. We\npropose a data augmentation approach to incorporate domain knowledge and\nimprove the generalization power of a data-intensive learning algorithm, such\nas a Convolutional neural network (CNN). The proposed data augmentation method\nemploys a limited persistence sparse modeling approach, capitalizing on\ncommonly observed characteristics of wide-angle synthetic aperture radar (SAR)\nimagery. Specifically, we exploit the sparsity of the scattering centers in the\nspatial domain and the smoothly-varying structure of the scattering\ncoefficients in the azimuthal domain to solve the ill-posed problem of\nover-parametrized model fitting. Using this estimated model, we synthesize new\nimages at poses and sub-pixel translations not available in the given data to\naugment CNN's training data. The experimental results show that for the\ntraining data starved region, the proposed method provides a significant gain\nin the resulting ATR algorithm's generalization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_T/0/1/0/all/0/1\">Tushar Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugavanam_N/0/1/0/all/0/1\">Nithin Sugavanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertin_E/0/1/0/all/0/1\">Emre Ertin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Object Detection via Probabilistic Ensembling. (arXiv:2104.02904v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02904","description":"<p>Object detection with multimodal inputs can improve many safety-critical\nsystems such as autonomous vehicles (AVs). Motivated by AVs that operate in\nboth day and night, we study multimodal object detection with RGB and thermal\ncameras, since the latter provides much stronger object signatures under poor\nillumination. We explore strategies for fusing information from different\nmodalities. Our key contribution is a probabilistic ensembling technique,\nProbEn, a simple non-learned method that fuses together detections from\nmulti-modalities. We derive ProbEn from Bayes' rule and first principles that\nassume conditional independence across modalities. Through probabilistic\nmarginalization, ProbEn elegantly handles missing modalities when detectors do\nnot fire on the same object. Importantly, ProbEn also notably improves\nmultimodal detection even when the conditional independence assumption does not\nhold, e.g., fusing outputs from other fusion methods (both off-the-shelf and\ntrained in-house). We validate ProbEn on two benchmarks containing both aligned\n(KAIST) and unaligned (FLIR) multimodal images, showing that ProbEn outperforms\nprior work by more than 13% in relative performance!\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jinghao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zelin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertz_C/0/1/0/all/0/1\">Christoph Mertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Shu Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.03746","description":"<p>Contrastive learning (CL) methods effectively learn data representations\nwithout label supervision, where the encoder contrasts each positive sample\nover multiple negative samples via a one-vs-many softmax cross-entropy loss. By\nleveraging large amounts of unlabeled image data, recent CL methods have\nachieved promising results when pre-trained on ImageNet, a well-curated data\nset with balanced image classes. However, they tend to yield worse performance\nwhen pre-trained on images in the wild. In this paper, to further improve the\nperformance of CL and enhance its robustness on uncurated data sets, we propose\na doubly CL strategy that contrasts the positive (negative) samples of a query\nwithin themselves before deciding how strongly to pull (push) them. We realize\nthis strategy with contrastive attraction and contrastive repulsion (CACR),\nwhich makes the query not only exert a greater force to attract more distant\npositive samples but also do so to repel closer negative samples. Theoretical\nanalysis reveals that CACR generalizes CL's behavior by taking into\nconsideration the differences between the distributions of the\npositive/negative samples, which in general are sampled independently of the\nquery, and their true conditional distributions given the query. We demonstrate\nthis unique intra-positive attraction and intra-negative repulsion mechanism,\nwhich helps remove the need to assume uniform prior distributions on both the\ndata and their latent representation, is particularly beneficial when data sets\nare less curated. Extensive large-scale experiments on a number of standard\nvision tasks show that CACR not only consistently outperforms existing CL\nmethods on benchmark data sets in representation learning, but also shows\nbetter robustness when generalized to pre-training on imbalanced image data\nsets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huangjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Visual Features to Hierarchical Semantic Topics for Image Paragraph Captioning. (arXiv:2105.04143v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04143","description":"<p>Observing a set of images and their corresponding paragraph-captions, a\nchallenging task is to learn how to produce a semantically coherent paragraph\nto describe the visual content of an image. Inspired by recent successes in\nintegrating semantic topics into this task, this paper develops a plug-and-play\nhierarchical-topic-guided image paragraph generation framework, which couples a\nvisual extractor with a deep topic model to guide the learning of a language\nmodel. To capture the correlations between the image and text at multiple\nlevels of abstraction and learn the semantic topics from images, we design a\nvariational inference network to build the mapping from image features to\ntextual captions. To guide the paragraph generation, the learned hierarchical\ntopics and visual features are integrated into the language model, including\nLong Short-Term Memory (LSTM) and Transformer, and jointly optimized.\nExperiments on public datasets demonstrate that the proposed models, which are\ncompetitive with many state-of-the-art approaches in terms of standard\nevaluation metrics, can be used to both distill interpretable multi-layer\nsemantic topics and generate diverse and coherent captions. We release our code\nat https://github.com/DandanGuo1993/VTCM-based-image-paragraph-caption.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dandan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ruiying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zequn Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Contrastive Learning. (arXiv:2105.09401v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.09401","description":"<p>With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and are characterized with multiple\nlabels, thus exhibiting the co-existence of multiple types of heterogeneity.\nAlthough state-of-the-art techniques are good at modeling complex heterogeneity\nwith sufficient label information, such label information can be quite\nexpensive to obtain in real applications. Recently, researchers pay great\nattention to contrastive learning due to its prominent performance by utilizing\nrich unlabeled data. However, existing work on contrastive learning is not able\nto address the problem of false negative pairs, i.e., some `negative' pairs may\nhave similar representations if they have the same label. To overcome the\nissues, in this paper, we propose a unified heterogeneous learning framework,\nwhich combines both the weighted unsupervised contrastive loss and the weighted\nsupervised contrastive loss to model multiple types of heterogeneity. We first\nprovide a theoretical analysis showing that the vanilla contrastive learning\nloss easily leads to the sub-optimal solution in the presence of false negative\npairs, whereas the proposed weighted loss could automatically adjust the weight\nbased on the similarity of the learned representations to mitigate this issue.\nExperimental results on real-world data sets demonstrate the effectiveness and\nthe efficiency of the proposed framework modeling multiple types of\nheterogeneity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yada Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Refinement with Joint Optimization of Visual Points and Lines. (arXiv:2110.03940v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03940","description":"<p>High-precision camera re-localization technology in a pre-established 3D\nenvironment map is the basis for many tasks, such as Augmented Reality,\nRobotics and Autonomous Driving. The point-based visual re-localization\napproaches are well-developed in recent decades, but are insufficient in some\nfeature-less cases. In this paper, we design a complete pipeline for camera\npose refinement with points and lines, which contains the innovatively designed\nline extracting CNN named VLSE, the line matching and the pose optimization\napproaches. We adopt a novel line representation and customize a hybrid\nconvolution block based on the Stacked Hourglass network, to detect accurate\nand stable line features on images. Then we apply a geometric-based strategy to\nobtain precise 2D-3D line correspondences using epipolar constraint and\nreprojection filtering. A following point-line joint cost function is\nconstructed to optimize the camera pose with the initial coarse pose from the\npure point-based localization. Sufficient experiments are conducted on open\ndatasets, i.e, line extractor on Wireframe and YorkUrban, localization\nperformance on InLoc duc1 and duc2, to confirm the effectiveness of our\npoint-line joint pose optimization method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jixiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Y/0/1/0/all/0/1\">Yishan Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuzhou Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuchen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Haikuan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jijunnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Corgis Important for Honeycomb Classification: Adversarial Attacks on Concept-based Explainability Tools. (arXiv:2110.07120v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.07120","description":"<p>Methods for model explainability have become increasingly critical for\ntesting the fairness and soundness of deep learning. Concept-based\ninterpretability techniques, which use a small set of human-interpretable\nconcept exemplars in order to measure the influence of a concept on a model's\ninternal representation of input, are an important thread in this line of\nresearch. In this work we show that these explainability methods can suffer the\nsame vulnerability to adversarial attacks as the models they are meant to\nanalyze. We demonstrate this phenomenon on two well-known concept-based\ninterpretability methods: TCAV and faceted feature visualization. We show that\nby carefully perturbing the examples of the concept that is being investigated,\nwe can radically change the output of the interpretability method. The attacks\nthat we propose can either induce positive interpretations (polka dots are an\nimportant concept for a model when classifying zebras) or negative\ninterpretations (stripes are not an important factor in identifying images of a\nzebra). Our work highlights the fact that in safety-critical applications,\nthere is need for security around not only the machine learning pipeline but\nalso the model interpretation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Davis Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Visual Question Generation. (arXiv:2110.08226v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08226","description":"<p>In traditional Visual Question Generation (VQG), most images have multiple\nconcepts (e.g. objects and categories) for which a question could be generated,\nbut models are trained to mimic an arbitrary choice of concept as given in\ntheir training data. This makes training difficult and also poses issues for\nevaluation -- multiple valid questions exist for most images but only one or a\nfew are captured by the human references. We present Guiding Visual Question\nGeneration - a variant of VQG which conditions the question generator on\ncategorical information based on expectations on the type of question and the\nobjects it should explore. We propose two variants: (i) an explicitly guided\nmodel that enables an actor (human or automated) to select which objects and\ncategories to generate a question for; and (ii) an implicitly guided model that\nlearns which objects and categories to condition on, based on discrete latent\nvariables. The proposed models are evaluated on an answer-category augmented\nVQA dataset and our quantitative results show a substantial improvement over\nthe current state of the art (over 9 BLEU-4 increase). Human evaluation\nvalidates that guidance helps the generation of questions that are\ngrammatically coherent and relevant to the given image and objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vedd_N/0/1/0/all/0/1\">Nihir Vedd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Teaching by Label Synthesis. (arXiv:2110.14432v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.14432","description":"<p>In this paper, we consider the problem of iterative machine teaching, where a\nteacher provides examples sequentially based on the current iterative learner.\nIn contrast to previous methods that have to scan over the entire pool and\nselect teaching examples from it in each iteration, we propose a label\nsynthesis teaching framework where the teacher randomly selects input teaching\nexamples (e.g., images) and then synthesizes suitable outputs (e.g., labels)\nfor them. We show that this framework can avoid costly example selection while\nstill provably achieving exponential teachability. We propose multiple novel\nteaching algorithms in this framework. Finally, we empirically demonstrate the\nvalue of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sliced Recursive Transformer. (arXiv:2111.05297v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05297","description":"<p>We present a neat yet effective recursive operation on vision transformers\nthat can improve parameter utilization without involving additional parameters.\nThis is achieved by sharing weights across the depth of transformer networks.\nThe proposed method can obtain a substantial gain (~2%) simply using naive\nrecursive operation, requires no special or sophisticated knowledge for\ndesigning principles of networks, and introduces minimal computational overhead\nto the training procedure. To reduce the additional computation caused by\nrecursive operation while maintaining the superior accuracy, we propose an\napproximating method through multiple sliced group self-attentions across\nrecursive layers which can reduce the cost consumption by 10~30% with minimal\nperformance loss. We call our model Sliced Recursive Transformer (SReT), a\nnovel and parameter-efficient vision transformer design that is compatible with\na broad range of other designs for efficient ViT architectures. Our best model\nestablishes significant improvement on ImageNet-1K over state-of-the-art\nmethods while containing fewer parameters. The proposed weight sharing\nmechanism by sliced recursion structure allows us to build a transformer with\nmore than 100 or even 1000 shared layers with ease while keeping a compact size\n(13~15M), to avoid optimization difficulties when the model is too large. The\nflexible scalability has shown great potential for scaling up models and\nconstructing extremely deep vision transformers. Code is available at\nhttps://github.com/szq0214/SReT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stacked BNAS: Rethinking Broad Convolutional Neural Network for Neural Architecture Search. (arXiv:2111.07722v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07722","description":"<p>Different from other deep scalable architecture based NAS approaches, Broad\nNeural Architecture Search (BNAS) proposes a broad one which consists of\nconvolution and enhancement blocks, dubbed Broad Convolutional Neural Network\n(BCNN) as search space for amazing efficiency improvement. BCNN reuses the\ntopologies of cells in convolution block, so that BNAS can employ few cells for\nefficient search. Moreover, multi-scale feature fusion and knowledge embedding\nare proposed to improve the performance of BCNN with shallow topology. However,\nBNAS suffers some drawbacks: 1) insufficient representation diversity for\nfeature fusion and enhancement, and 2) time consuming of knowledge embedding\ndesign by human expert.\n</p>\n<p>In this paper, we propose Stacked BNAS whose search space is a developed\nbroad scalable architecture named Stacked BCNN, with better performance than\nBNAS. On the one hand, Stacked BCNN treats mini-BCNN as the basic block to\npreserve comprehensive representation and deliver powerful feature extraction\nability. On the other hand, we propose Knowledge Embedding Search (KES) to\nlearn appropriate knowledge embeddings. Experimental results show that 1)\nStacked BNAS obtains better performance than BNAS, 2) KES contributes to reduce\nthe parameters of learned architecture with satisfactory performance, and 3)\nStacked BNAS delivers state-of-the-art efficiency of 0.02 GPU days.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zixiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nannan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongbin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems. (arXiv:2111.09999v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09999","description":"<p>Deep neural networks are vulnerable to attacks from adversarial inputs and,\nmore recently, Trojans to misguide or hijack the model's decision. We expose\nthe existence of an intriguing class of spatially bounded, physically\nrealizable, adversarial examples -- Universal NaTuralistic adversarial paTches\n-- we call TnTs, by exploring the superset of the spatially bounded adversarial\nexample space and the natural input space within generative adversarial\nnetworks. Now, an adversary can arm themselves with a patch that is\nnaturalistic, less malicious-looking, physically realizable, highly effective\nachieving high attack success rates, and universal. A TnT is universal because\nany input image captured with a TnT in the scene will: i) misguide a network\n(untargeted attack); or ii) force the network to make a malicious decision\n(targeted attack). Interestingly, now, an adversarial patch attacker has the\npotential to exert a greater level of control -- the ability to choose a\nlocation-independent, natural-looking patch as a trigger in contrast to being\nconstrained to noisy perturbations -- an ability is thus far shown to be only\npossible with Trojan attack methods needing to interfere with the model\nbuilding processes to embed a backdoor at the risk discovery; but, still\nrealize a patch deployable in the physical world. Through extensive experiments\non the large-scale visual classification task, ImageNet with evaluations across\nits entire validation set of 50,000 images, we demonstrate the realistic threat\nfrom TnTs and the robustness of the attack. We show a generalization of the\nattack to create patches achieving higher attack success rates than existing\nstate-of-the-art methods. Our results show the generalizability of the attack\nto different visual classification tasks (CIFAR-10, GTSRB, PubFig) and multiple\nstate-of-the-art deep neural networks such as WideResnet50, Inception-V3 and\nVGG-16.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_B/0/1/0/all/0/1\">Bao Gia Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Minhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1\">Ehsan Abbasnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_D/0/1/0/all/0/1\">Damith C. Ranasinghe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-slimmed Vision Transformer. (arXiv:2111.12624v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12624","description":"<p>Vision transformers (ViTs) have become the popular structures and\noutperformed convolutional neural networks (CNNs) on various vision tasks.\nHowever, such powerful transformers bring a huge computation burden, because of\nthe exhausting token-to-token comparison. The previous works focus on dropping\ninsignificant tokens to reduce the computational cost of ViTs. But when the\ndropping ratio increases, this hard manner will inevitably discard the vital\ntokens, which limits its efficiency. To solve the issue, we propose a generic\nself-slimmed learning approach for vanilla ViTs, namely SiT. Specifically, we\nfirst design a novel Token Slimming Module (TSM), which can boost the inference\nefficiency of ViTs by dynamic token aggregation. As a general method of token\nhard dropping, our TSM softly integrates redundant tokens into fewer\ninformative ones. It can dynamically zoom visual attention without cutting off\ndiscriminative token relations in the images, even with a high slimming ratio.\nFurthermore, we introduce a concise Feature Recalibration Distillation (FRD)\nframework, wherein we design a reverse version of TSM (RTSM) to recalibrate the\nunstructured token in a flexible auto-encoder manner. Due to the similar\nstructure between teacher and student, our FRD can effectively leverage\nstructure knowledge for better convergence. Finally, we conduct extensive\nexperiments to evaluate our SiT. It demonstrates that our method can speed up\nViTs by 1.7x with negligible accuracy drop, and even speed up ViTs by 3.6x\nwhile maintaining 97% of their performance. Surprisingly, by simply arming\nLV-ViT with our SiT, we achieve new state-of-the-art performance on ImageNet.\nCode is available at https://github.com/Sense-X/SiT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Zhuofan Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_B/0/1/0/all/0/1\">Biao Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Token Sampling For Efficient Vision Transformers. (arXiv:2111.15667v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15667","description":"<p>While state-of-the-art vision transformer models achieve promising results in\nimage classification, they are computationally expensive and require many\nGFLOPs. Although the GFLOPs of a vision transformer can be decreased by\nreducing the number of tokens in the network, there is no setting that is\noptimal for all input images. In this work, we therefore introduce a\ndifferentiable parameter-free Adaptive Token Sampler (ATS) module, which can be\nplugged into any existing vision transformer architecture. ATS empowers vision\ntransformers by scoring and adaptively sampling significant tokens. As a\nresult, the number of tokens is not constant anymore and varies for each input\nimage. By integrating ATS as an additional layer within the current transformer\nblocks, we can convert them into much more efficient vision transformers with\nan adaptive number of tokens. Since ATS is a parameter-free module, it can be\nadded to the off-the-shelf pre-trained vision transformers as a plug and play\nmodule, thus reducing their GFLOPs without any additional training. Moreover,\ndue to its differentiable design, one can also train a vision transformer\nequipped with ATS. We evaluate the efficiency of our module in both image and\nvideo classification tasks by adding it to multiple SOTA vision transformers.\nOur proposed module improves the SOTA by reducing their computational costs\n(GFLOPs) by 2X, while preserving their accuracy on the ImageNet, Kinetics-400,\nand Kinetics-600 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_F/0/1/0/all/0/1\">Farnoush Rezaei Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sunando Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joze_H/0/1/0/all/0/1\">Hamid Reza Vaezi Joze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommerlade_E/0/1/0/all/0/1\">Eric Sommerlade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-guided Deblurring of Unknown Exposure Time Videos. (arXiv:2112.06988v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06988","description":"<p>Motion deblurring is a highly ill-posed problem due to the loss of motion\ninformation in the blur degradation process. Since event cameras can capture\napparent motion with a high temporal resolution, several attempts have explored\nthe potential of events for guiding deblurring. These methods generally assume\nthat the exposure time is the same as the reciprocal of the video frame rate.\nHowever, this is not true in real situations, and the exposure time might be\nunknown and dynamically varies depending on the video shooting\nenvironment(e.g., illumination condition). In this paper, we address the\nevent-guided motion deblurring assuming dynamically variable unknown exposure\ntime of the frame-based camera. To this end, we first derive a new formulation\nfor event-guided motion deblurring by considering the exposure and readout time\nin the video frame acquisition process. We then propose a novel end-to-end\nlearning framework for event-guided motion deblurring. In particular, we design\na novel Exposure Time-based Event Selection(ETES) module to selectively use\nevent features by estimating the cross-modal correlation between the features\nfrom blurred frames and the events. Moreover, we propose a feature fusion\nmodule to fuse the selected features from events and blur frames effectively.\nWe conduct extensive experiments on various datasets and demonstrate that our\nmethod achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAFIM: Targeted Adversarial Attacks against Facial Image Manipulations. (arXiv:2112.09151v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09151","description":"<p>Face manipulation methods can be misused to affect an individual's privacy or\nto spread disinformation. To this end, we introduce a novel data-driven\napproach that produces image-specific perturbations which are embedded in the\noriginal images. The key idea is that these protected images prevent face\nmanipulation by causing the manipulation model to produce a predefined\nmanipulation target (uniformly colored output image in our case) instead of the\nactual manipulation. In addition, we propose to leverage differentiable\ncompression approximation, hence making generated perturbations robust to\ncommon image compression. In order to prevent against multiple manipulation\nmethods simultaneously, we further propose a novel attention-based fusion of\nmanipulation-specific perturbations. Compared to traditional adversarial\nattacks that optimize noise patterns for each image individually, our\ngeneralized model only needs a single forward pass, thus running orders of\nmagnitude faster and allowing for easy integration in image processing stacks,\neven on resource-constrained devices like smartphones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1\">Shivangi Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markhasin_L/0/1/0/all/0/1\">Lev Markhasin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Niessner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning of Semantic and Visual Representations for Text Tracking. (arXiv:2112.14976v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14976","description":"<p>Semantic representation is of great benefit to the video text tracking(VTT)\ntask that requires simultaneously classifying, detecting, and tracking texts in\nthe video. Most existing approaches tackle this task by appearance similarity\nin continuous frames, while ignoring the abundant semantic features. In this\npaper, we explore to robustly track video text with contrastive learning of\nsemantic and visual representations. Correspondingly, we present an end-to-end\nvideo text tracker with Semantic and Visual Representations(SVRep), which\ndetects and tracks texts by exploiting the visual and semantic relationships\nbetween different texts in a video sequence. Besides, with a light-weight\narchitecture, SVRep achieves state-of-the-art performance while maintaining\ncompetitive inference speed. Specifically, with a backbone of ResNet-18, SVRep\nachieves an ${\\rm ID_{F1}}$ of $\\textbf{65.9\\%}$, running at $\\textbf{16.7}$\nFPS, on the ICDAR2015(video) dataset with $\\textbf{8.6\\%}$ improvement than the\nprevious state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Size Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning. (arXiv:2201.03529v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.03529","description":"<p>Transfer-learning methods aim to improve performance in a data-scarce target\ndomain using a model pretrained on a data-rich source domain. A cost-efficient\nstrategy, linear probing, involves freezing the source model and training a new\nclassification head for the target domain. This strategy is outperformed by a\nmore costly but state-of-the-art method -- fine-tuning all parameters of the\nsource model to the target domain -- possibly because fine-tuning allows the\nmodel to leverage useful information from intermediate layers which is\notherwise discarded by the later pretrained layers. We explore the hypothesis\nthat these intermediate layers might be directly exploited. We propose a\nmethod, Head-to-Toe probing (Head2Toe), that selects features from all layers\nof the source model to train a classification head for the target-domain. In\nevaluations on the VTAB-1k, Head2Toe matches performance obtained with\nfine-tuning on average while reducing training and storage cost hundred folds\nor more, but critically, for out-of-distribution transfer, Head2Toe outperforms\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PT4AL: Using Self-Supervised Pretext Tasks for Active Learning. (arXiv:2201.07459v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07459","description":"<p>Labeling a large set of data is expensive. Active learning aims to tackle\nthis problem by asking to annotate only the most informative data from the\nunlabeled set. We propose a novel active learning approach that utilizes\nself-supervised pretext tasks and a unique data sampler to select data that are\nboth difficult and representative. We discover that the loss of a simple\nself-supervised pretext task, such as rotation prediction, is closely\ncorrelated to the downstream task loss. Before the active learning iterations,\nthe pretext task learner is trained on the unlabeled set, and the unlabeled\ndata are sorted and split into batches by their pretext task losses. In each\nactive learning iteration, the main task model is used to sample the most\nuncertain data in a batch to be annotated. We evaluate our method on various\nimage classification and segmentation benchmarks and achieve compelling\nperformances on CIFAR10, Caltech-101, ImageNet, and Cityscapes. We further show\nthat our method performs well on imbalanced datasets, and can be an effective\nsolution to the cold-start problem where active learning performance is\naffected by the randomly sampled initial labeled set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">John Seon Keun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image reconstruction algorithms in radio interferometry: from handcrafted to learned regularization denoisers. (arXiv:2202.12959v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.12959","description":"<p>We introduce a new class of iterative image reconstruction algorithms for\nradio interferometry, at the interface of convex optimization and deep\nlearning, inspired by plug-and-play methods. The approach consists in learning\na prior image model by training a deep neural network (DNN) as a denoiser, and\nsubstituting it for the handcrafted proximal regularization operator of an\noptimization algorithm. The proposed AIRI (``AI for Regularization in\nradio-interferometric Imaging'') framework, for imaging complex intensity\nstructure with diffuse and faint emission from visibility data, inherits the\nrobustness and interpretability of optimization, and the learning power and\nspeed of networks. Our approach relies on three steps. Firstly, we design a low\ndynamic range training database from optical intensity images. Secondly, we\ntrain a DNN denoiser at a noise level inferred from the signal-to-noise ratio\nof the data. We use training losses enhanced with a nonexpansiveness term\nensuring algorithm convergence, and including on-the-fly database dynamic range\nenhancement via exponentiation. Thirdly, we plug the learned denoiser into the\nforward-backward optimization algorithm, resulting in a simple iterative\nstructure alternating a denoising step with a gradient-descent data-fidelity\nstep. We have validated AIRI against CLEAN, optimization algorithms of the SARA\nfamily, and a DNN trained to reconstruct the image directly from visibility\ndata. Simulation results show that AIRI is competitive in imaging quality with\nSARA and its unconstrained forward-backward-based version uSARA, while\nproviding significant acceleration. CLEAN remains faster but offers lower\nquality. The end-to-end DNN offers further acceleration, but with far lower\nquality than AIRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Terris_M/0/1/0/all/0/1\">Matthieu Terris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dabbech_A/0/1/0/all/0/1\">Arwa Dabbech</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Chao Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wiaux_Y/0/1/0/all/0/1\">Yves Wiaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Wilderness Characteristics Using Explainable Machine Learning in Satellite Imagery. (arXiv:2203.00379v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00379","description":"<p>Wilderness areas offer important ecological and social benefits and there are\nurgent reasons to discover where their positive characteristics and ecological\nfunctions are present and able to flourish. We apply a novel explainable\nmachine learning technique to satellite images which show wild and\nanthropogenic areas in Fennoscandia. Occluding certain activations in an\ninterpretable artificial neural network we complete a comprehensive sensitivity\nanalysis regarding wild and anthropogenic characteristics. This enables us to\npredict detailed and high-resolution sensitivity maps highlighting these\ncharacteristics. Our artificial neural network provides an interpretable\nactivation space increasing confidence in our method. Within the activation\nspace, regions are semantically arranged. Our approach advances explainable\nmachine learning for remote sensing, offers opportunities for comprehensive\nanalyses of existing wilderness, and has practical relevance for conservation\nefforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stomberg_T/0/1/0/all/0/1\">Timo T. Stomberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_T/0/1/0/all/0/1\">Taylor Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonhardt_J/0/1/0/all/0/1\">Johannes Leonhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1\">Immanuel Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Matters: A Weakly Supervised Vision-Language Pre-training Approach for Scene Text Detection and Spotting. (arXiv:2203.03911v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03911","description":"<p>Recently, Vision-Language Pre-training (VLP) techniques have greatly\nbenefited various vision-language tasks by jointly learning visual and textual\nrepresentations, which intuitively helps in Optical Character Recognition (OCR)\ntasks due to the rich visual and textual information in scene text images.\nHowever, these methods cannot well cope with OCR tasks because of the\ndifficulty in both instance-level text encoding and image-text pair acquisition\n(i.e. images and captured texts in them). This paper presents a weakly\nsupervised pre-training method, oCLIP, which can acquire effective scene text\nrepresentations by jointly learning and aligning visual and textual\ninformation. Our network consists of an image encoder and a character-aware\ntext encoder that extract visual and textual features, respectively, as well as\na visual-textual decoder that models the interaction among textual and visual\nfeatures for learning effective scene text representations. With the learning\nof textual features, the pre-trained model can attend texts in images well with\ncharacter awareness. Besides, these designs enable the learning from weakly\nannotated texts (i.e. partial texts in images without text bounding boxes)\nwhich mitigates the data annotation constraint greatly. Experiments over the\nweakly annotated images in ICDAR2019-LSVT show that our pre-trained model\nimproves F-score by +2.5\\% and +4.8\\% while transferring its weights to other\ntext detection and spotting networks, respectively. In addition, the proposed\nmethod outperforms existing pre-training techniques consistently across\nmultiple public datasets (e.g., +3.2\\% and +1.3\\% for Total-Text and CTW1500).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chuhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yu Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer. (arXiv:2203.03952v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03952","description":"<p>Recently, vision transformers started to show impressive results which\noutperform large convolution based models significantly. However, in the area\nof small models for mobile or resource constrained devices, ConvNet still has\nits own advantages in both performance and model complexity. We propose\nParC-Net, a pure ConvNet based backbone model that further strengthens these\nadvantages by fusing the merits of vision transformers into ConvNets.\nSpecifically, we propose position aware circular convolution (ParC), a\nlight-weight convolution op which boasts a global receptive field while\nproducing location sensitive features as in local convolutions. We combine the\nParCs and squeeze-exictation ops to form a meta-former like model block, which\nfurther has the attention mechanism like transformers. The aforementioned block\ncan be used in plug-and-play manner to replace relevant blocks in ConvNets or\ntransformers. Experiment results show that the proposed ParC-Net achieves\nbetter performance than popular light-weight ConvNets and vision transformer\nbased models in common vision tasks and datasets, while having fewer parameters\nand faster inference speed. For classification on ImageNet-1k, ParC-Net\nachieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11%\nparameters and 13% computational cost but gaining 0.2% higher accuracy and 23%\nfaster inference speed (on ARM based Rockchip RK3288) compared with MobileViT,\nand uses only 0.5 times parameters but gaining 2.7% accuracy compared with\nDeIT. On MS-COCO object detection and PASCAL VOC segmentation tasks, ParC-Net\nalso shows better performance. Source code is available at\nhttps://github.com/hkzhang91/ParC-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynthStrip: Skull-Stripping for Any Brain Image. (arXiv:2203.09974v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.09974","description":"<p>The removal of non-brain signal from magnetic resonance imaging (MRI) data,\nknown as skull-stripping, is an integral component of many neuroimage analysis\nstreams. Despite their abundance, popular classical skull-stripping methods are\nusually tailored to images with specific acquisition properties, namely\nnear-isotropic resolution and T1-weighted (T1w) MRI contrast, which are\nprevalent in research settings. As a result, existing tools tend to adapt\npoorly to other image types, such as stacks of thick slices acquired with fast\nspin-echo (FSE) MRI that are common in the clinic. While learning-based\napproaches for brain extraction have gained traction in recent years, these\nmethods face a similar burden, as they are only effective for image types seen\nduring the training procedure. To achieve robust skull-stripping across a\nlandscape of imaging protocols, we introduce SynthStrip, a rapid,\nlearning-based brain-extraction tool. By leveraging anatomical segmentations to\ngenerate an entirely synthetic training dataset with anatomies, intensity\ndistributions, and artifacts that far exceed the realistic range of medical\nimages, SynthStrip learns to successfully generalize to a variety of real\nacquired brain images, removing the need for training data with target\ncontrasts. We demonstrate the efficacy of SynthStrip for a diverse set of image\nacquisitions and resolutions across subject populations, ranging from newborn\nto adult. We show substantial improvements in accuracy over popular\nskull-stripping baselines -- all with a single trained model. Our method and\nlabeled evaluation data are available at https://w3id.org/synthstrip.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hoopes_A/0/1/0/all/0/1\">Andrew Hoopes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mora_J/0/1/0/all/0/1\">Jocelyn S. Mora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1\">Malte Hoffmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PACS: A Dataset for Physical Audiovisual CommonSense Reasoning. (arXiv:2203.11130v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.11130","description":"<p>In order for AI to be safely deployed in real-world scenarios such as\nhospitals, schools, and the workplace, it must be able to robustly reason about\nthe physical world. Fundamental to this reasoning is physical common sense:\nunderstanding the physical properties and affordances of available objects, how\nthey can be manipulated, and how they interact with other objects. Physical\ncommonsense reasoning is fundamentally a multi-sensory task, since physical\nproperties are manifested through multiple modalities - two of them being\nvision and acoustics. Our paper takes a step towards real-world physical\ncommonsense reasoning by contributing PACS: the first audiovisual benchmark\nannotated for physical commonsense attributes. PACS contains 13,400\nquestion-answer pairs, involving 1,377 unique physical commonsense questions\nand 1,526 videos. Our dataset provides new opportunities to advance the\nresearch field of physical reasoning by bringing audio as a core component of\nthis multimodal problem. Using PACS, we evaluate multiple state-of-the-art\nmodels on our new challenging task. While some models show promising results\n(70% accuracy), they all fall short of human performance (95% accuracy). We\nconclude the paper by demonstrating the importance of multimodal reasoning and\nproviding possible avenues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samuel Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Semantic Segmentation Grounded in Visual Concepts. (arXiv:2203.13868v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13868","description":"<p>Unsupervised semantic segmentation requires assigning a label to every pixel\nwithout any human annotations. Despite recent advances in self-supervised\nrepresentation learning for individual images, unsupervised semantic\nsegmentation with pixel-level representations is still a challenging task and\nremains underexplored. In this work, we propose a self-supervised pixel\nrepresentation learning method for semantic segmentation by using visual\nconcepts (i.e., groups of pixels with semantic meanings, such as parts,\nobjects, and scenes) extracted from images. To guide self-supervised learning,\nwe leverage three types of relationships between pixels and concepts, including\nthe relationships between pixels and local concepts, local and global concepts,\nas well as the co-occurrence of concepts. We evaluate the learned pixel\nembeddings and visual concepts on three datasets, including PASCAL VOC 2012,\nCOCO 2017, and DAVIS 2017. Our results show that the proposed method gains\nconsistent and substantial improvements over recent unsupervised semantic\nsegmentation approaches, and also demonstrate that visual concepts can reveal\ninsights into image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surmeier_W/0/1/0/all/0/1\">William Surmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekar_A/0/1/0/all/0/1\">Arvind Kumar Shekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1\">Liang Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification. (arXiv:2203.16983v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16983","description":"<p>Self-supervised learning (SSL) has drawn increasing attention in pathological\nimage analysis in recent years. Compared to contrastive learning which requires\ncareful design, masked autoencoders (MAE) building SSL from a generative\nparadigm probably is a simpler method. In this paper, we introduce MAE and\nverify the effect of visible patches for pathological image classification.\nBased on it, a novel SD-MAE model is proposed to enable a self-distillation\naugmented SSL on top of the raw MAE. Besides the reconstruction loss on masked\nimage patches, SD-MAE further imposes the self-distillation loss on visible\npatches. It transfers knowledge brought by the global attention of the decoder\nto the encoder which only uses local attention. We apply SD-MAE on two public\npathological image datasets. Experiments demonstrate that SD-MAE performs\nhighly competitive when compared with other SSL methods. Our code will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhineng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xieping Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Graph Variational Autoencoders for Indoor Furniture layout Generation. (arXiv:2204.04867v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04867","description":"<p>We present a structured graph variational autoencoder for generating the\nlayout of indoor 3D scenes. Given the room type (e.g., living room or library)\nand the room layout (e.g., room elements such as floor and walls), our\narchitecture generates a collection of objects (e.g., furniture items such as\nsofa, table and chairs) that is consistent with the room type and layout. This\nis a challenging problem because the generated scene should satisfy multiple\nconstrains, e.g., each object must lie inside the room and two objects cannot\noccupy the same volume. To address these challenges, we propose a deep\ngenerative model that encodes these relationships as soft constraints on an\nattributed graph (e.g., the nodes capture attributes of room and furniture\nelements, such as class, pose and size, and the edges capture geometric\nrelationships such as relative orientation). The architecture consists of a\ngraph encoder that maps the input graph to a structured latent space, and a\ngraph decoder that generates a furniture graph, given a latent code and the\nroom graph. The latent space is modeled with auto-regressive priors, which\nfacilitates the generation of highly structured scenes. We also propose an\nefficient training procedure that combines matching and constrained learning.\nExperiments on the 3D-FRONT dataset show that our method produces scenes that\nare diverse and are adapted to the room layout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_A/0/1/0/all/0/1\">Aditya Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Paul Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Rene Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06681","description":"<p>The MINSU(Mobile Inventory and Scanning Unit) algorithm uses the\ncomputational vision analysis method to record the residual quantity/fullness\nof the cabinet. To do so, it goes through a five-step method: object detection,\nforeground subtraction, K-means clustering, percentage estimation, and\ncounting. The input image goes through the object detection method to analyze\nthe specific position of the cabinets in terms of coordinates. After doing so,\nit goes through the foreground subtraction method to make the image more\nfocus-able to the cabinet itself by removing the background (some manual work\nmay have to be done such as selecting the parts that were not grab cut by the\nalgorithm). In the K-means clustering method, the multi-colored image turns\ninto a 3 colored monotonous image for quicker and more accurate analysis. At\nlast, the image goes through percentage estimation and counting. In these two\nmethods, the proportion that the material inside the cabinet is found in\npercentage which then is used to approximate the number of materials inside.\nHad this project been successful, the residual quantity management could solve\nthe problem addressed earlier in the introduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_J/0/1/0/all/0/1\">Jihoon Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Byungkon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyeob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seunghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images. (arXiv:2204.12454v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12454","description":"<p>Multiple Instance Learning (MIL) methods have become increasingly popular for\nclassifying giga-pixel sized Whole-Slide Images (WSIs) in digital pathology.\nMost MIL methods operate at a single WSI magnification, by processing all the\ntissue patches. Such a formulation induces high computational requirements, and\nconstrains the contextualization of the WSI-level representation to a single\nscale. A few MIL methods extend to multiple scales, but are computationally\nmore demanding. In this paper, inspired by the pathological diagnostic process,\nwe propose ZoomMIL, a method that learns to perform multi-level zooming in an\nend-to-end manner. ZoomMIL builds WSI representations by aggregating\ntissue-context information from multiple magnifications. The proposed method\noutperforms the state-of-the-art MIL methods in WSI classification on two large\ndatasets, while significantly reducing the computational demands with regard to\nFloating-Point Operations (FLOPs) and processing time by up to 40x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thandiackal_K/0/1/0/all/0/1\">Kevin Thandiackal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pati_P/0/1/0/all/0/1\">Pushpak Pati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaume_G/0/1/0/all/0/1\">Guillaume Jaume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrani_M/0/1/0/all/0/1\">Maria Gabrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation. (arXiv:2204.13132v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13132","description":"<p>Unsupervised domain adaptation (UDA) aims to adapt a model trained on the\nsource domain (e.g. synthetic data) to the target domain (e.g. real-world data)\nwithout requiring further annotations on the target domain. This work focuses\non UDA for semantic segmentation as real-world pixel-wise annotations are\nparticularly expensive to acquire. As UDA methods for semantic segmentation are\nusually GPU memory intensive, most previous methods operate only on downscaled\nimages. We question this design as low-resolution predictions often fail to\npreserve fine details. The alternative of training with random crops of\nhigh-resolution images alleviates this problem but falls short in capturing\nlong-range, domain-robust context information. Therefore, we propose HRDA, a\nmulti-resolution training approach for UDA, that combines the strengths of\nsmall high-resolution crops to preserve fine segmentation details and large\nlow-resolution crops to capture long-range context dependencies with a learned\nscale attention, while maintaining a manageable GPU memory footprint. HRDA\nenables adapting small objects and preserving fine segmentation details. It\nsignificantly improves the state-of-the-art performance by 5.5 mIoU for\nGTA-to-Cityscapes and 4.9 mIoU for Synthia-to-Cityscapes, resulting in\nunprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available\nat https://github.com/lhoyer/HRDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_L/0/1/0/all/0/1\">Lukas Hoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Extrapolation in Space and Time. (arXiv:2205.02084v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02084","description":"<p>Novel view synthesis (NVS) and video prediction (VP) are typically considered\ndisjoint tasks in computer vision. However, they can both be seen as ways to\nobserve the spatial-temporal world: NVS aims to synthesize a scene from a new\npoint of view, while VP aims to see a scene from a new point of time. These two\ntasks provide complementary signals to obtain a scene representation, as\nviewpoint changes from spatial observations inform depth, and temporal\nobservations inform the motion of cameras and individual objects. Inspired by\nthese observations, we propose to study the problem of Video Extrapolation in\nSpace and Time (VEST). We propose a model that leverages the self-supervision\nand the complementary cues from both tasks, while existing methods can only\nsolve one of them. Experiments show that our method achieves performance better\nthan or comparable to several state-of-the-art NVS and VP methods on indoor and\noutdoor real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance. (arXiv:2205.05677v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05677","description":"<p>Marker-less monocular 3D human motion capture (MoCap) with scene interactions\nis a challenging research topic relevant for extended reality, robotics and\nvirtual avatar generation. Due to the inherent depth ambiguity of monocular\nsettings, 3D motions captured with existing methods often contain severe\nartefacts such as incorrect body-scene inter-penetrations, jitter and body\nfloating. To tackle these issues, we propose HULC, a new approach for 3D human\nMoCap which is aware of the scene geometry. HULC estimates 3D poses and dense\nbody-environment surface contacts for improved 3D localisations, as well as the\nabsolute scale of the subject. Furthermore, we introduce a 3D pose trajectory\noptimisation based on a novel pose manifold sampling that resolves erroneous\nbody-environment inter-penetrations. Although the proposed method requires less\nstructured inputs compared to existing scene-aware monocular MoCap algorithms,\nit produces more physically-plausible poses: HULC significantly and\nconsistently outperforms the existing approaches in various experiments and on\ndifferent metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Representation Learning with Deep Latent Particles. (arXiv:2205.15821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15821","description":"<p>We propose a new representation of visual data that disentangles object\nposition from appearance. Our method, termed Deep Latent Particles (DLP),\ndecomposes the visual input into low-dimensional latent ``particles'', where\neach particle is described by its spatial location and features of its\nsurrounding region. To drive learning of such representations, we follow a\nVAE-based approach and introduce a prior for particle positions based on a\nspatial-softmax architecture, and a modification of the evidence lower bound\nloss inspired by the Chamfer distance between particles. We demonstrate that\nour DLP representations are useful for downstream tasks such as unsupervised\nkeypoint (KP) detection, image manipulation, and video prediction for scenes\ncomposed of multiple dynamic objects. In addition, we show that our\nprobabilistic interpretation of the problem naturally provides uncertainty\nestimates for particle locations, which can be used for model selection, among\nother tasks. Videos and code are available:\nhttps://taldatech.github.io/deep-latent-particles-web/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daniel_T/0/1/0/all/0/1\">Tal Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1\">Aviv Tamar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation Using Bounding Boxes. (arXiv:2206.01203v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01203","description":"<p>Current 3D segmentation methods heavily rely on large-scale point-cloud\ndatasets, which are notoriously laborious to annotate. Few attempts have been\nmade to circumvent the need for dense per-point annotations. In this work, we\nlook at weakly-supervised 3D semantic instance segmentation. The key idea is to\nleverage 3D bounding box labels which are easier and faster to annotate.\nIndeed, we show that it is possible to train dense segmentation models using\nonly bounding box labels. At the core of our method, \\name{}, lies a deep\nmodel, inspired by classical Hough voting, that directly votes for bounding box\nparameters, and a clustering method specifically tailored to bounding box\nvotes. This goes beyond commonly used center votes, which would not fully\nexploit the bounding box annotations. On ScanNet test, our weakly supervised\nmodel attains leading performance among other weakly supervised approaches (+18\nmAP@50). Remarkably, it also achieves 97% of the mAP@50 score of current fully\nsupervised models. To further illustrate the practicality of our work, we train\nBox2Mask on the recently released ARKitScenes dataset which is annotated with\n3D bounding boxes only, and show, for the first time, compelling 3D instance\nsegmentation masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chibane_J/0/1/0/all/0/1\">Julian Chibane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1\">Francis Engelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tuan Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01714","description":"<p>Large text-guided diffusion models, such as DALLE-2, are able to generate\nstunning photorealistic images given natural language descriptions. While such\nmodels are highly flexible, they struggle to understand the composition of\ncertain concepts, such as confusing the attributes of different objects or\nrelations between objects. In this paper, we propose an alternative structured\napproach for compositional generation using diffusion models. An image is\ngenerated by composing a set of diffusion models, with each of them modeling a\ncertain component of the image. To do this, we interpret diffusion models as\nenergy-based models in which the data distributions defined by the energy\nfunctions may be explicitly combined. The proposed method can generate scenes\nat test time that are substantially more complex than those seen in training,\ncomposing sentence descriptions, object relations, human facial attributes, and\neven generalizing to new combinations that are rarely seen in the real world.\nWe further illustrate how our approach may be used to compose pre-trained\ntext-guided diffusion models and generate photorealistic images containing all\nthe details described in the input descriptions, including the binding of\ncertain object attributes that have been shown difficult for DALLE-2. These\nresults point to the effectiveness of the proposed method in promoting\nstructured generalization for visual generation. Project page:\nhttps://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Implies Generalization via Data-Dependent Generalization Bounds. (arXiv:2206.13497v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.13497","description":"<p>This paper proves that robustness implies generalization via data-dependent\ngeneralization bounds. As a result, robustness and generalization are shown to\nbe connected closely in a data-dependent manner. Our bounds improve previous\nbounds in two directions, to solve an open problem that has seen little\ndevelopment since 2010. The first is to reduce the dependence on the covering\nnumber. The second is to remove the dependence on the hypothesis space. We\npresent several examples, including ones for lasso and deep learning, in which\nour bounds are provably preferable. The experiments on real-world data and\ntheoretical models demonstrate near-exponential improvements in various\nsituations. To achieve these improvements, we do not require additional\nassumptions on the unknown distribution; instead, we only incorporate an\nobservable and computable property of the training samples. A key technical\ninnovation is an improved concentration bound for multinomial random variables\nthat is of independent interest beyond robustness and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luh_K/0/1/0/all/0/1\">Kyle Luh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaoyang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Intraluminal Navigation of a Soft Robot using Deep-Learning-based Visual Servoing. (arXiv:2207.00401v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2207.00401","description":"<p>Navigation inside luminal organs is an arduous task that requires\nnon-intuitive coordination between the movement of the operator's hand and the\ninformation obtained from the endoscopic video. The development of tools to\nautomate certain tasks could alleviate the physical and mental load of doctors\nduring interventions, allowing them to focus on diagnosis and decision-making\ntasks. In this paper, we present a synergic solution for intraluminal\nnavigation consisting of a 3D printed endoscopic soft robot that can move\nsafely inside luminal structures. Visual servoing, based on Convolutional\nNeural Networks (CNNs) is used to achieve the autonomous navigation task. The\nCNN is trained with phantoms and in-vivo data to segment the lumen, and a\nmodel-less approach is presented to control the movement in constrained\nenvironments. The proposed robot is validated in anatomical phantoms in\ndifferent path configurations. We analyze the movement of the robot using\ndifferent metrics such as task completion time, smoothness, error in the\nsteady-state, and mean and maximum error. We show that our method is suitable\nto navigate safely in hollow environments and conditions which are different\nthan the ones the network was originally trained on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazo_J/0/1/0/all/0/1\">Jorge F. Lazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chun-Feng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_B/0/1/0/all/0/1\">Benoit Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catellani_M/0/1/0/all/0/1\">Michele Catellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathelin_M/0/1/0/all/0/1\">Michel de Mathelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrigno_G/0/1/0/all/0/1\">Giancarlo Ferrigno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breedveld_P/0/1/0/all/0/1\">Paul Breedveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankelman_J/0/1/0/all/0/1\">Jenny Dankelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1\">Elena De Momi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Ground-Truth Depth Image Generation via Overfit Training of Point Cloud Registration using Local Frame Sets. (arXiv:2207.07016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07016","description":"<p>Accurate three-dimensional perception is a fundamental task in several\ncomputer vision applications. Recently, commercial RGB-depth (RGB-D) cameras\nhave been widely adopted as single-view depth-sensing devices owing to their\nefficient depth-sensing abilities. However, the depth quality of most RGB-D\nsensors remains insufficient owing to the inherent noise from a single-view\nenvironment. Recently, several studies have focused on the single-view depth\nenhancement of RGB-D cameras. Recent research has proposed deep-learning-based\napproaches that typically train networks using high-quality supervised depth\ndatasets, which indicates that the quality of the ground-truth (GT) depth\ndataset is a top-most important factor for accurate system; however, such\nhigh-quality GT datasets are difficult to obtain. In this study, we developed a\nnovel method for high-quality GT depth generation based on an RGB-D stream\ndataset. First, we defined consecutive depth frames in a local spatial region\nas a local frame set. Then, the depth frames were aligned to a certain frame in\nthe local frame set using an unsupervised point cloud registration scheme. The\nregistration parameters were trained based on an overfit-training scheme, which\nwas primarily used to construct a single GT depth image for each frame set. The\nfinal GT depth dataset was constructed using several local frame sets, and each\nlocal frame set was trained independently. The primary advantage of this study\nis that a high-quality GT depth dataset can be constructed under various\nscanning environments using only the RGB-D stream dataset. Moreover, our\nproposed method can be used as a new benchmark GT dataset for accurate\nperformance evaluations. We evaluated our GT dataset on previously benchmarked\nGT depth datasets and demonstrated that our method is superior to\nstate-of-the-art depth enhancement frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minchang Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yeong-Gil Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1\">Minyoung Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10257","description":"<p>Over the years, 2D GANs have achieved great successes in photorealistic\nportrait generation. However, they lack 3D understanding in the generation\nprocess, thus they suffer from multi-view inconsistency problem. To alleviate\nthe issue, many 3D-aware GANs have been proposed and shown notable results, but\n3D GANs struggle with editing semantic attributes. The controllability and\ninterpretability of 3D GANs have not been much explored. In this work, we\npropose two solutions to overcome these weaknesses of 2D GANs and 3D-aware\nGANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of\ndiscovering semantic attributes during training and controlling them in an\nunsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN\nto obtain a high-fidelity 3D-controllable generator. Unlike existing\nlatent-based methods allowing implicit pose control, the proposed\n3D-controllable StyleGAN enables explicit pose control over portrait\ngeneration. This distillation allows direct compatibility between 3D control\nand many StyleGAN-based techniques (e.g., inversion and stylization), and also\nbrings an advantage in terms of computational resources. Our codes are\navailable at https://github.com/jgkwak95/SURF-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jeong-gi Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">Dongsik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-temporal Analysis for Automated Concrete Workability Estimation. (arXiv:2207.11635v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.11635","description":"<p>Concrete workability measure is mostly determined based on subjective\nassessment of a certified assessor with visual inspections. The potential human\nerror in measuring the workability and the resulting unnecessary adjustments\nfor the workability is a major challenge faced by the construction industry,\nleading to significant costs, material waste and delay. In this paper, we try\nto apply computer vision techniques to observe the concrete mixing process and\nestimate the workability. Specifically, we collected the video data and then\nbuilt three different deep neural networks for spatial-temporal regression. The\npilot study demonstrates a practical application with computer vision\ntechniques to estimate the concrete workability during the mixing process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Litao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirivivatnanon_V/0/1/0/all/0/1\">Vute Sirivivatnanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nezhad_A/0/1/0/all/0/1\">Ali Nezhad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-Aperture Feature Adaptation in Single Image Super-resolution Model for Light Field Imaging. (arXiv:2207.11894v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.11894","description":"<p>With the availability of commercial Light Field (LF) cameras, LF imaging has\nemerged as an up and coming technology in computational photography. However,\nthe spatial resolution is significantly constrained in commercial microlens\nbased LF cameras because of the inherent multiplexing of spatial and angular\ninformation. Therefore, it becomes the main bottleneck for other applications\nof light field cameras. This paper proposes an adaptation module in a\npretrained Single Image Super Resolution (SISR) network to leverage the\npowerful SISR model instead of using highly engineered light field imaging\ndomain specific Super Resolution models. The adaption module consists of a Sub\naperture Shift block and a fusion block. It is an adaptation in the SISR\nnetwork to further exploit the spatial and angular information in LF images to\nimprove the super resolution performance. Experimental validation shows that\nthe proposed method outperforms existing light field super resolution\nalgorithms. It also achieves PSNR gains of more than 1 dB across all the\ndatasets as compared to the same pretrained SISR models for scale factor 2, and\nPSNR gains 0.6 to 1 dB for scale factor 4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Aupendu Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nehra_S/0/1/0/all/0/1\">Suresh Nehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_J/0/1/0/all/0/1\">Jayanta Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_P/0/1/0/all/0/1\">Prabir Kumar Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Boxes: Boosting End-to-End Scene Text Recognition by Adjusting Annotated Bounding Boxes via Reinforcement Learning. (arXiv:2207.11934v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.11934","description":"<p>Text detection and recognition are essential components of a modern OCR\nsystem. Most OCR approaches attempt to obtain accurate bounding boxes of text\nat the detection stage, which is used as the input of the text recognition\nstage. We observe that when using tight text bounding boxes as input, a text\nrecognizer frequently fails to achieve optimal performance due to the\ninconsistency between bounding boxes and deep representations of text\nrecognition. In this paper, we propose Box Adjuster, a reinforcement\nlearning-based method for adjusting the shape of each text bounding box to make\nit more compatible with text recognition models. Additionally, when dealing\nwith cross-domain problems such as synthetic-to-real, the proposed method\nsignificantly reduces mismatches in domain distribution between the source and\ntarget domains. Experiments demonstrate that the performance of end-to-end text\nrecognition systems can be improved when using the adjusted bounding boxes as\nthe ground truths for training. Specifically, on several benchmark datasets for\nscene text understanding, the proposed method outperforms state-of-the-art text\nspotters by an average of 2.0% F-Score on end-to-end text recognition tasks and\n4.6% F-Score on domain adaptation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jingqun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1\">Wenming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiena Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation. (arXiv:2207.11984v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.11984","description":"<p>Existing self-supervised monocular depth estimation methods can get rid of\nexpensive annotations and achieve promising results. However, these methods\nsuffer from severe performance degradation when directly adopting a model\ntrained on a fixed resolution to evaluate at other different resolutions. In\nthis paper, we propose a resolution adaptive self-supervised monocular depth\nestimation method (RA-Depth) by learning the scale invariance of the scene\ndepth. Specifically, we propose a simple yet efficient data augmentation method\nto generate images with arbitrary scales for the same scene. Then, we develop a\ndual high-resolution network that uses the multi-path encoder and decoder with\ndense interactions to aggregate multi-scale features for accurate depth\ninference. Finally, to explicitly learn the scale invariance of the scene\ndepth, we formulate a cross-scale depth consistency loss on depth predictions\nwith different scales. Extensive experiments on the KITTI, Make3D and NYU-V2\ndatasets demonstrate that RA-Depth not only achieves state-of-the-art\nperformance, but also exhibits a good ability of resolution adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Le Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yikai Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Siamese Transformer Network for Single Object Tracking on Point Clouds. (arXiv:2207.11995v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.11995","description":"<p>Siamese network based trackers formulate 3D single object tracking as\ncross-correlation learning between point features of a template and a search\narea. Due to the large appearance variation between the template and search\narea during tracking, how to learn the robust cross correlation between them\nfor identifying the potential target in the search area is still a challenging\nproblem. In this paper, we explicitly use Transformer to form a 3D Siamese\nTransformer network for learning robust cross correlation between the template\nand the search area of point clouds. Specifically, we develop a Siamese point\nTransformer network to learn shape context information of the target. Its\nencoder uses self-attention to capture non-local information of point clouds to\ncharacterize the shape information of the object, and the decoder utilizes\ncross-attention to upsample discriminative point features. After that, we\ndevelop an iterative coarse-to-fine correlation network to learn the robust\ncross correlation between the template and the search area. It formulates the\ncross-feature augmentation to associate the template with the potential target\nin the search area via cross attention. To further enhance the potential\ntarget, it employs the ego-feature augmentation that applies self-attention to\nthe local k-NN graph of the feature space to aggregate target features.\nExperiments on the KITTI, nuScenes, and Waymo datasets show that our method\nachieves state-of-the-art performance on the 3D single object tracking task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Le Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Linghua Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_K/0/1/0/all/0/1\">Kaihao Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Riemannian Geometry Approach for Minimizing Distortion and its Applications. (arXiv:2207.12038v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.12038","description":"<p>Given an affine transformation $T$, we define its Fisher distortion\n$Dist_F(T)$. We show that the Fisher distortion has Riemannian metric structure\nand provide an algorithm for finding mean distorting transformation -- namely\n-- for a given set $\\{T_{i}\\}_{i=1}^N$ of affine transformations, find an\naffine transformation $T$ that minimize the overall distortion\n$\\sum_{i=1}^NDist_F^{2}(T^{-1}T_{i}).$ The mean distorting transformation can\nbe useful in some fields -- in particular, we apply it for rendering affine\npanoramas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozeri_D/0/1/0/all/0/1\">Dror Ozeri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Decorrelation with Potential Energy Ranking. (arXiv:2207.12194v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.12194","description":"<p>Machine learning systems, especially the methods based on deep learning,\nenjoy great success in modern computer vision tasks under experimental\nsettings. Generally, these classic deep learning methods are built on the\n\\emph{i.i.d.} assumption, supposing the training and test data are drawn from a\nsimilar distribution independently and identically. However, the aforementioned\n\\emph{i.i.d.} assumption is in general unavailable in the real-world scenario,\nand as a result, leads to sharp performance decay of deep learning algorithms.\nBehind this, domain shift is one of the primary factors to be blamed. In order\nto tackle this problem, we propose using \\textbf{Po}tential \\textbf{E}nergy\n\\textbf{R}anking (PoER) to decouple the object feature and the domain feature\n(\\emph{i.e.,} appearance feature) in given images, promoting the learning of\nlabel-discriminative features while filtering out the irrelevant correlations\nbetween the objects and the background. PoER helps the neural networks to\ncapture label-related features which contain the domain information first in\nshallow layers and then distills the label-discriminative representations out\nprogressively, enforcing the neural networks to be aware of the characteristic\nof objects and background which is vital to the generation of domain-invariant\nfeatures. PoER reports superior performance on domain generalization\nbenchmarks, improving the average top-1 accuracy by at least 1.20\\% compared to\nthe existing methods. Moreover, we use PoER in the ECCV 2022 NICO\nChallenge\\footnote{https://nicochallenge.com}, achieving top place with only a\nvanilla ResNet-18. The code has been made available at\nhttps://github.com/ForeverPs/PoER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiaxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Shiming Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Deblurring using Dual Camera Fusion on Mobile Phones. (arXiv:2207.11617v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2207.11617","description":"<p>Motion blur of fast-moving subjects is a longstanding problem in photography\nand very common on mobile phones due to limited light collection efficiency,\nparticularly in low-light conditions. While we have witnessed great progress in\nimage deblurring in recent years, most methods require significant\ncomputational power and have limitations in processing high-resolution photos\nwith severe local motions. To this end, we develop a novel face deblurring\nsystem based on the dual camera fusion technique for mobile phones. The system\ndetects subject motion to dynamically enable a reference camera, e.g.,\nultrawide angle camera commonly available on recent premium phones, and\ncaptures an auxiliary photo with faster shutter settings. While the main shot\nis low noise but blurry, the reference shot is sharp but noisy. We learn ML\nmodels to align and fuse these two shots and output a clear photo without\nmotion blur. Our algorithm runs efficiently on Google Pixel 6, which takes 463\nms overhead per shot. Our experiments demonstrate the advantage and robustness\nof our system against alternative single-image, multi-frame, face-specific, and\nvideo deblurring algorithms as well as commercial products. To the best of our\nknowledge, our work is the first mobile solution for face motion deblurring\nthat works reliably and robustly over thousands of images in diverse motion and\nlighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1\">YiChang Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lun-Cheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaotong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Sung-Fang Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krainin_M/0/1/0/all/0/1\">Michael Krainin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chia-Kai Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}