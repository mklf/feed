{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-01T01:30:00Z","channels":[{"title":"cs.AI updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.AI","description":"Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Astrocytes mediate analogous memory in a multi-layer neuron-astrocytic network. (arXiv:2108.13414v1 [q-bio.NC])","link":"http://arxiv.org/abs/2108.13414","description":"<p>Modeling the neuronal processes underlying short-term working memory remains\nthe focus of many theoretical studies in neuroscience. Here we propose a\nmathematical model of spiking neuron network (SNN) demonstrating how a piece of\ninformation can be maintained as a robust activity pattern for several seconds\nthen completely disappear if no other stimuli come. Such short-term memory\ntraces are preserved due to the activation of astrocytes accompanying the SNN.\nThe astrocytes exhibit calcium transients at a time scale of seconds. These\ntransients further modulate the efficiency of synaptic transmission and, hence,\nthe firing rate of neighboring neurons at diverse timescales through\ngliotransmitter release. We show how such transients continuously encode\nfrequencies of neuronal discharges and provide robust short-term storage of\nanalogous information. This kind of short-term memory can keep operative\ninformation for seconds, then completely forget it to avoid overlapping with\nforthcoming patterns. The SNN is inter-connected with the astrocytic layer by\nlocal inter-cellular diffusive connections. The astrocytes are activated only\nwhen the neighboring neurons fire quite synchronously, e.g. when an information\npattern is loaded. For illustration, we took greyscale photos of people's faces\nwhere the grey level encoded the level of applied current stimulating the\nneurons. The astrocyte feedback modulates (facilitates) synaptic transmission\nby varying the frequency of neuronal firing. We show how arbitrary patterns can\nbe loaded, then stored for a certain interval of time, and retrieved if the\nappropriate clue pattern is applied to the input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Tsybina_Y/0/1/0/all/0/1\">Yuliya Tsybina</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kastalskiy_I/0/1/0/all/0/1\">Innokentiy Kastalskiy</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Krivonosov_M/0/1/0/all/0/1\">Mikhail Krivonosov</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zaikin_A/0/1/0/all/0/1\">Alexey Zaikin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kazantsev_V/0/1/0/all/0/1\">Victor Kazantsev</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gorban_A/0/1/0/all/0/1\">Alexander Gorban</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gordleeva_S/0/1/0/all/0/1\">Susanna Gordleeva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback. (arXiv:2108.13454v1 [cs.IR])","link":"http://arxiv.org/abs/2108.13454","description":"<p>Dense retrieval systems conduct first-stage retrieval using embedded\nrepresentations and simple similarity metrics to match a query to documents.\nIts effectiveness depends on encoded embeddings to capture the semantics of\nqueries and documents, a challenging task due to the shortness and ambiguity of\nsearch queries. This paper proposes ANCE-PRF, a new query encoder that uses\npseudo relevance feedback (PRF) to improve query representations for dense\nretrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top\nretrieved documents from a dense retrieval model, ANCE, and it learns to\nproduce better query embeddings directly from relevance labels. It also keeps\nthe document index unchanged to reduce overhead. ANCE-PRF significantly\noutperforms ANCE and other recent dense retrieval systems on several datasets.\nAnalysis shows that the PRF encoder effectively captures the relevant and\ncomplementary information from PRF documents, while ignoring the noise with its\nlearned attention mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">HongChien Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Want To Reduce Labeling Cost? GPT-3 Can Help. (arXiv:2108.13487v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13487","description":"<p>Data annotation is a time-consuming and labor-intensive process for many NLP\ntasks. Although there exist various methods to produce pseudo data labels, they\nare often task-specific and require a decent amount of labeled data to start\nwith. Recently, the immense language model GPT-3 with 175 billion parameters\nhas achieved tremendous improvement across many few-shot learning tasks. In\nthis paper, we explore ways to leverage GPT-3 as a low-cost data labeler to\ntrain other models. We find that, to make the downstream model achieve the same\nperformance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use\nlabels from GPT-3 than using labels from humans. Furthermore, we propose a\nnovel framework of combining pseudo labels from GPT-3 with human labels, which\nleads to even better performance with limited labeling budget. These results\npresent a cost-effective data labeling methodology that is generalizable to\nmany practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoWhy: Addressing Challenges in Expressing and Validating Causal Assumptions. (arXiv:2108.13518v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13518","description":"<p>Estimation of causal effects involves crucial assumptions about the\ndata-generating process, such as directionality of effect, presence of\ninstrumental variables or mediators, and whether all relevant confounders are\nobserved. Violation of any of these assumptions leads to significant error in\nthe effect estimate. However, unlike cross-validation for predictive models,\nthere is no global validator method for a causal estimate. As a result,\nexpressing different causal assumptions formally and validating them (to the\nextent possible) becomes critical for any analysis. We present DoWhy, a\nframework that allows explicit declaration of assumptions through a causal\ngraph and provides multiple validation tests to check a subset of these\nassumptions. Our experience with DoWhy highlights a number of open questions\nfor future research: developing new ways beyond causal graphs to express\nassumptions, the role of causal discovery in learning relevant parts of the\ngraph, and developing validation tests that can better detect errors, both for\naverage and conditional treatment effects. DoWhy is available at\nhttps://github.com/microsoft/dowhy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1\">Vasilis Syrgkanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1\">Emre K&#x131;c&#x131;man</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Label Smoothing To Regularize Large-Scale Graph Training. (arXiv:2108.13555v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13555","description":"<p>Graph neural networks (GNNs), which learn the node representations by\nrecursively aggregating information from its neighbors, have become a\npredominant computational tool in many domains. To handle large-scale graphs,\nmost of the existing methods partition the input graph into multiple sub-graphs\n(e.g., through node clustering) and apply batch training to save memory cost.\nHowever, such batch training will lead to label bias within each batch, and\nthen result in over-confidence in model predictions. Since the connected nodes\nwith positively related labels tend to be assigned together, the traditional\ncross-entropy minimization process will attend on the predictions of biased\nclasses in the batch, and may intensify the overfitting issue. To overcome the\nlabel bias problem, we propose the adaptive label smoothing (ALS) method to\nreplace the one-hot hard labels with smoothed ones, which learns to allocate\nlabel confidences from the biased classes to the others. Specifically, ALS\npropagates node labels to aggregate the neighborhood label distribution in a\npre-processing step, and then updates the optimal smoothed labels online to\nadapt to specific graph structure. Experiments on the real-world datasets\ndemonstrate that ALS can be generally applied to the main scalable learning\nframeworks to calibrate the biased labels and improve generalization\nperformances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaixiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zirui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Soo-Hyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero Shot on the Cold-Start Problem: Model-Agnostic Interest Learning for Recommender Systems. (arXiv:2108.13592v1 [cs.IR])","link":"http://arxiv.org/abs/2108.13592","description":"<p>User behavior has been validated to be effective in revealing personalized\npreferences for commercial recommendations. However, few user-item interactions\ncan be collected for new users, which results in a null space for their\ninterests, i.e., the cold-start dilemma. In this paper, a two-tower framework,\nnamely, the model-agnostic interest learning (MAIL) framework, is proposed to\naddress the cold-start recommendation (CSR) problem for recommender systems. In\nMAIL, one unique tower is constructed to tackle the CSR from a zero-shot view,\nand the other tower focuses on the general ranking task. Specifically, the\nzero-shot tower first performs cross-modal reconstruction with dual\nauto-encoders to obtain virtual behavior data from highly aligned hidden\nfeatures for new users; and the ranking tower can then output recommendations\nfor users based on the completed data by the zero-shot tower. Practically, the\nranking tower in MAIL is model-agnostic and can be implemented with any\nembedding-based deep models. Based on the co-training of the two towers, the\nMAIL presents an end-to-end method for recommender systems that shows an\nincremental performance improvement. The proposed method has been successfully\ndeployed on the live recommendation system of NetEase Cloud Music to achieve a\nclick-through rate improvement of 13% to 15% for millions of users. Offline\nexperiments on real-world datasets also show its superior performance in CSR.\nOur code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_P/0/1/0/all/0/1\">Philip J. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_P/0/1/0/all/0/1\">Pingjun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tingting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chuanjiang Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-balanced Learning For Domain Generalization. (arXiv:2108.13597v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13597","description":"<p>Domain generalization aims to learn a prediction model on multi-domain source\ndata such that the model can generalize to a target domain with unknown\nstatistics. Most existing approaches have been developed under the assumption\nthat the source data is well-balanced in terms of both domain and class.\nHowever, real-world training data collected with different composition biases\noften exhibits severe distribution gaps for domain and class, leading to\nsubstantial performance degradation. In this paper, we propose a self-balanced\ndomain generalization framework that adaptively learns the weights of losses to\nalleviate the bias caused by different distributions of the multi-domain source\ndata. The self-balanced scheme is based on an auxiliary reweighting network\nthat iteratively updates the weight of loss conditioned on the domain and class\ninformation by leveraging balanced meta data. Experimental results demonstrate\nthe effectiveness of our method overwhelming state-of-the-art works for domain\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dongbo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13602","description":"<p>Adversarial training (AT) is one of the most reliable methods for defending\nagainst adversarial attacks in machine learning. Variants of this method have\nbeen used as regularization mechanisms to achieve SOTA results on NLP\nbenchmarks, and they have been found to be useful for transfer learning and\ncontinual learning. We search for the reasons for the effectiveness of AT by\ncontrasting vanilla and adversarially fine-tuned BERT models. We identify\npartial preservation of BERT's syntactic abilities during fine-tuning as the\nkey to the success of AT. We observe that adversarially fine-tuned models\nremain more faithful to BERT's language modeling behavior and are more\nsensitive to the word order. As concrete examples of syntactic abilities, an\nadversarially fine-tuned model could have an advantage of up to 38% on anaphora\nagreement and up to 11% on dependency parsing. Our analysis demonstrates that\nvanilla fine-tuning oversimplifies the sentence representation by focusing\nheavily on one or a few label-indicative words. AT, however, moderates the\neffect of these influential words and encourages representational diversity.\nThis allows for a more hierarchical representation of a sentence and leads to\nthe mitigation of BERT's loss of syntactic abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_J/0/1/0/all/0/1\">Javid Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation Fault: A Cheap Defense Against Adversarial Machine Learning. (arXiv:2108.13617v1 [cs.CR])","link":"http://arxiv.org/abs/2108.13617","description":"<p>Recently published attacks against deep neural networks (DNNs) have stressed\nthe importance of methodologies and tools to assess the security risks of using\nthis technology in critical systems. Efficient techniques for detecting\nadversarial machine learning helps establishing trust and boost the adoption of\ndeep learning in sensitive and security systems. In this paper, we propose a\nnew technique for defending deep neural network classifiers, and convolutional\nones in particular. Our defense is cheap in the sense that it requires less\ncomputation power despite a small cost to pay in terms of detection accuracy.\nThe work refers to a recently published technique called ML-LOO. We replace the\ncostly pixel by pixel leave-one-out approach of ML-LOO by adopting\ncoarse-grained leave-one-out. We evaluate and compare the efficiency of\ndifferent segmentation algorithms for this task. Our results show that a large\ngain in efficiency is possible, even though penalized by a marginal decrease in\ndetection accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bared_D/0/1/0/all/0/1\">Doha Al Bared</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1\">Mohamed Nassar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spike time displacement based error backpropagation in convolutional spiking neural networks. (arXiv:2108.13621v1 [cs.NE])","link":"http://arxiv.org/abs/2108.13621","description":"<p>We recently proposed the STiDi-BP algorithm, which avoids backward recursive\ngradient computation, for training multi-layer spiking neural networks (SNNs)\nwith single-spike-based temporal coding. The algorithm employs a linear\napproximation to compute the derivative of the spike latency with respect to\nthe membrane potential and it uses spiking neurons with piecewise linear\npostsynaptic potential to reduce the computational cost and the complexity of\nneural processing. In this paper, we extend the STiDi-BP algorithm to employ it\nin deeper and convolutional architectures. The evaluation results on the image\nclassification task based on two popular benchmarks, MNIST and Fashion-MNIST\ndatasets with the accuracies of respectively 99.2% and 92.8%, confirm that this\nalgorithm has been applicable in deep SNNs. Another issue we consider is the\nreduction of memory storage and computational cost. To do so, we consider a\nconvolutional SNN (CSNN) with two sets of weights: real-valued weights that are\nupdated in the backward pass and their signs, binary weights, that are employed\nin the feedforward process. We evaluate the binary CSNN on two datasets of\nMNIST and Fashion-MNIST and obtain acceptable performance with a negligible\naccuracy drop with respect to real-valued weights (about $0.6%$ and $0.8%$\ndrops, respectively).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirsadeghi_M/0/1/0/all/0/1\">Maryam Mirsadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalchian_M/0/1/0/all/0/1\">Majid Shalchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1\">Saeed Reza Kheradpisheh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1\">Timoth&#xe9;e Masquelier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When are Deep Networks really better than Random Forests at small sample sizes?. (arXiv:2108.13637v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13637","description":"<p>Random forests (RF) and deep networks (DN) are two of the most popular\nmachine learning methods in the current scientific literature and yield\ndiffering levels of performance on different data modalities. We wish to\nfurther explore and establish the conditions and domains in which each approach\nexcels, particularly in the context of sample size and feature dimension. To\naddress these issues, we tested the performance of these approaches across\ntabular, image, and audio settings using varying model parameters and\narchitectures. Our focus is on datasets with at most 10,000 samples, which\nrepresent a large fraction of scientific and biomedical datasets. In general,\nwe found RF to excel at tabular and structured data (image and audio) with\nsmall sample sizes, whereas DN performed better on structured data with larger\nsample sizes. Although we plan to continue updating this technical report in\nthe coming months, we believe the current preliminary results may be of\ninterest to others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoyin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainsworth_M/0/1/0/all/0/1\">Michael Ainsworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Chung Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusmanov_M/0/1/0/all/0/1\">Madi Kusmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1\">Sambit Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies. (arXiv:2108.13643v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13643","description":"<p>Recently, deep reinforcement learning (DRL) methods have achieved impressive\nperformance on tasks in a variety of domains. However, neural network policies\nproduced with DRL methods are not human-interpretable and often have difficulty\ngeneralizing to novel scenarios. To address these issues, prior works explore\nlearning programmatic policies that are more interpretable and structured for\ngeneralization. Yet, these works either employ limited policy representations\n(e.g. decision trees, state machines, or predefined program templates) or\nrequire stronger supervision (e.g. input/output state pairs or expert\ndemonstrations). We present a framework that instead learns to synthesize a\nprogram, which details the procedure to solve a task in a flexible and\nexpressive manner, solely from reward signals. To alleviate the difficulty of\nlearning to compose programs to induce the desired agent behavior from scratch,\nwe propose to first learn a program embedding space that continuously\nparameterizes diverse behaviors in an unsupervised manner and then search over\nthe learned program embedding space to yield a program that maximizes the\nreturn for a given task. Experimental results demonstrate that the proposed\nframework not only learns to reliably synthesize task-solving programs but also\noutperforms DRL and program synthesis baselines while producing interpretable\nand more generalizable policies. We also justify the necessity of the proposed\ntwo-stage learning scheme as well as analyze various methods for learning the\nprogram embedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_D/0/1/0/all/0/1\">Dweep Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jesse Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shao-Hua Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joseph J. Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Graph Neural Network with Multi-view Representation Learning. (arXiv:2108.13650v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13650","description":"<p>Graph neural networks for heterogeneous graph embedding is to project nodes\ninto a low-dimensional space by exploring the heterogeneity and semantics of\nthe heterogeneous graph. However, on the one hand, most of existing\nheterogeneous graph embedding methods either insufficiently model the local\nstructure under specific semantic, or neglect the heterogeneity when\naggregating information from it. On the other hand, representations from\nmultiple semantics are not comprehensively integrated to obtain versatile node\nembeddings. To address the problem, we propose a Heterogeneous Graph Neural\nNetwork with Multi-View Representation Learning (named MV-HetGNN) for\nheterogeneous graph embedding by introducing the idea of multi-view\nrepresentation learning. The proposed model consists of node feature\ntransformation, view-specific ego graph encoding and auto multi-view fusion to\nthoroughly learn complex structural and semantic information for generating\ncomprehensive node representations. Extensive experiments on three real-world\nheterogeneous graph datasets show that the proposed MV-HetGNN model\nconsistently outperforms all the state-of-the-art GNN baselines in various\ndownstream tasks, e.g., node classification, node clustering, and link\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zezhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feida Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Classes through Word Attribution. (arXiv:2108.13653v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13653","description":"<p>In recent years, several methods have been proposed for explaining individual\npredictions of deep learning models, yet there has been little study of how to\naggregate these predictions to explain how such models view classes as a whole\nin text classification tasks. In this work, we propose a method for explaining\nclasses using deep learning models and the Integrated Gradients feature\nattribution technique by aggregating explanations of individual examples in\ntext classification to general descriptions of the classes. We demonstrate the\napproach on Web register (genre) classification using the XML-R model and the\nCorpus of Online Registers of English (CORE), finding that the method\nidentifies plausible and discriminative keywords characterizing all but the\nsmallest class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ronnqvist_S/0/1/0/all/0/1\">Samuel R&#xf6;nnqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myntti_A/0/1/0/all/0/1\">Amanda Myntti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrolainen_A/0/1/0/all/0/1\">Aki-Juhani Kyr&#xf6;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1\">Sampo Pyysalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laippala_V/0/1/0/all/0/1\">Veronika Laippala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13679","description":"<p>In this paper, we propose to formulate the task-oriented dialogue system as\nthe purely natural language generation task, so as to fully leverage the\nlarge-scale pre-trained models like GPT-2 and simplify complicated\ndelexicalization prepossessing. However, directly applying this method heavily\nsuffers from the dialogue entity inconsistency caused by the removal of\ndelexicalized tokens, as well as the catastrophic forgetting problem of the\npre-trained model during fine-tuning, leading to unsatisfactory performance. To\nalleviate these problems, we design a novel GPT-Adapter-CopyNet network, which\nincorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve\nbetter performance on transfer learning and dialogue entity generation.\nExperimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ\ndataset demonstrate that our proposed approach significantly outperforms\nbaseline models with a remarkable performance on automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phy-Q: A Benchmark for Physical Reasoning. (arXiv:2108.13696v1 [cs.AI])","link":"http://arxiv.org/abs/2108.13696","description":"<p>Humans are well-versed in reasoning about the behaviors of physical objects\nwhen choosing actions to accomplish tasks, while it remains a major challenge\nfor AI. To facilitate research addressing this problem, we propose a new\nbenchmark that requires an agent to reason about physical scenarios and take an\naction accordingly. Inspired by the physical knowledge acquired in infancy and\nthe capabilities required for robots to operate in real-world environments, we\nidentify 15 essential physical scenarios. For each scenario, we create a wide\nvariety of distinct task templates, and we ensure all the task templates within\nthe same scenario can be solved by using one specific physical rule. By having\nsuch a design, we evaluate two distinct levels of generalization, namely the\nlocal generalization and the broad generalization. We conduct an extensive\nevaluation with human players, learning agents with varying input types and\narchitectures, and heuristic agents with different strategies. The benchmark\ngives a Phy-Q (physical reasoning quotient) score that reflects the physical\nreasoning ability of the agents. Our evaluation shows that 1) all agents fail\nto reach human performance, and 2) learning agents, even with good local\ngeneralization ability, struggle to learn the underlying physical reasoning\nrules and fail to generalize broadly. We encourage the development of\nintelligent agents with broad generalization abilities in physical domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Cheng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_V/0/1/0/all/0/1\">Vimukthini Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamage_C/0/1/0/all/0/1\">Chathura Gamage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikonova_E/0/1/0/all/0/1\">Ekaterina Nikonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1\">Jochen Renz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Multi-Reference Learning for Image Super-Resolution. (arXiv:2108.13697v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13697","description":"<p>This paper proposes a novel Attention-based Multi-Reference Super-resolution\nnetwork (AMRSR) that, given a low-resolution image, learns to adaptively\ntransfer the most similar texture from multiple reference images to the\nsuper-resolution output whilst maintaining spatial coherence. The use of\nmultiple reference images together with attention-based sampling is\ndemonstrated to achieve significantly improved performance over\nstate-of-the-art reference super-resolution approaches on multiple benchmark\ndatasets. Reference super-resolution approaches have recently been proposed to\novercome the ill-posed problem of image super-resolution by providing\nadditional information from a high-resolution reference image. Multi-reference\nsuper-resolution extends this approach by providing a more diverse pool of\nimage features to overcome the inherent information deficit whilst maintaining\nmemory efficiency. A novel hierarchical attention-based sampling approach is\nintroduced to learn the similarity between low-resolution image features and\nmultiple reference images based on a perceptual loss. Ablation demonstrates the\ncontribution of both multi-reference and hierarchical attention-based sampling\nto overall performance. Perceptual and quantitative ground-truth evaluation\ndemonstrates significant improvement in performance even when the reference\nimages deviate significantly from the target image. The project website can be\nfound at https://marcopesavento.github.io/AMRSR/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pesavento_M/0/1/0/all/0/1\">Marco Pesavento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volino_M/0/1/0/all/0/1\">Marco Volino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_A/0/1/0/all/0/1\">Adrian Hilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TNNT: The Named Entity Recognition Toolkit. (arXiv:2108.13700v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13700","description":"<p>Extraction of categorised named entities from text is a complex task given\nthe availability of a variety of Named Entity Recognition (NER) models and the\nunstructured information encoded in different source document formats.\nProcessing the documents to extract text, identifying suitable NER models for a\ntask, and obtaining statistical information is important in data analysis to\nmake informed decisions. This paper presents TNNT, a toolkit that automates the\nextraction of categorised named entities from unstructured information encoded\nin source documents, using diverse state-of-the-art Natural Language Processing\n(NLP) tools and NER models. TNNT integrates 21 different NER models as part of\na Knowledge Graph Construction Pipeline (KGCP) that takes a document set as\ninput and processes it based on the defined settings, applying the selected\nblocks of NER models to output the results. The toolkit generates all results\nwith an integrated summary of the extracted entities, enabling enhanced data\nanalysis to support the KGCP, and also, to aid further NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1\">Sandaru Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_S/0/1/0/all/0/1\">Sergio J. Rodr&#xed;guez M&#xe9;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuecheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omran_P/0/1/0/all/0/1\">Pouya G. Omran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_K/0/1/0/all/0/1\">Kerry Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_A/0/1/0/all/0/1\">Armin Haller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemIE: Semantically-aware Image Extrapolation. (arXiv:2108.13702v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13702","description":"<p>We propose a semantically-aware novel paradigm to perform image extrapolation\nthat enables the addition of new object instances. All previous methods are\nlimited in their capability of extrapolation to merely extending the already\nexisting objects in the image. However, our proposed approach focuses not only\non (i) extending the already present objects but also on (ii) adding new\nobjects in the extended region based on the context. To this end, for a given\nimage, we first obtain an object segmentation map using a state-of-the-art\nsemantic segmentation method. The, thus, obtained segmentation map is fed into\na network to compute the extrapolated semantic segmentation and the\ncorresponding panoptic segmentation maps. The input image and the obtained\nsegmentation maps are further utilized to generate the final extrapolated\nimage. We conduct experiments on Cityscapes and ADE20K-bedroom datasets and\nshow that our method outperforms all baselines in terms of FID, and similarity\nin object co-occurrence statistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_B/0/1/0/all/0/1\">Bholeshwar Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Soumya Ranjan Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Abhishek Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1\">Aniruddha Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Hrituraj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1\">Kuldeep Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Robustness of Off-Policy Evaluation. (arXiv:2108.13703v1 [stat.ML])","link":"http://arxiv.org/abs/2108.13703","description":"<p>Off-policy Evaluation (OPE), or offline evaluation in general, evaluates the\nperformance of hypothetical policies leveraging only offline log data. It is\nparticularly useful in applications where the online interaction involves high\nstakes and expensive setting such as precision medicine and recommender\nsystems. Since many OPE estimators have been proposed and some of them have\nhyperparameters to be tuned, there is an emerging challenge for practitioners\nto select and tune OPE estimators for their specific application.\nUnfortunately, identifying a reliable estimator from results reported in\nresearch papers is often difficult because the current experimental procedure\nevaluates and compares the estimators' performance on a narrow set of\nhyperparameters and evaluation policies. Therefore, it is difficult to know\nwhich estimator is safe and reliable to use. In this work, we develop\nInterpretable Evaluation for Offline Evaluation (IEOE), an experimental\nprocedure to evaluate OPE estimators' robustness to changes in hyperparameters\nand/or evaluation policies in an interpretable manner. Then, using the IEOE\nprocedure, we perform extensive evaluation of a wide variety of existing\nestimators on Open Bandit Dataset, a large-scale public real-world dataset for\nOPE. We demonstrate that our procedure can evaluate the estimators' robustness\nto the hyperparamter choice, helping us avoid using unsafe estimators. Finally,\nwe apply IEOE to real-world e-commerce platform data and demonstrate how to use\nour protocol in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Saito_Y/0/1/0/all/0/1\">Yuta Saito</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Udagawa_T/0/1/0/all/0/1\">Takuma Udagawa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kiyohara_H/0/1/0/all/0/1\">Haruka Kiyohara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mogi_K/0/1/0/all/0/1\">Kazuki Mogi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Narita_Y/0/1/0/all/0/1\">Yusuke Narita</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tateno_K/0/1/0/all/0/1\">Kei Tateno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13741","description":"<p>Recent researches have demonstrated that BERT shows potential in a wide range\nof natural language processing tasks. It is adopted as an encoder for many\nstate-of-the-art automatic summarizing systems, which achieve excellent\nperformance. However, so far, there is not much work done for Vietnamese. In\nthis paper, we showcase how BERT can be implemented for extractive text\nsummarization in Vietnamese. We introduce a novel comparison between different\nmultilingual and monolingual BERT models. The experiment results indicate that\nmonolingual models produce promising results compared to other multilingual\nmodels and previous text summarizing models for Vietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quoc_H/0/1/0/all/0/1\">Huy To Quoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Horn Non-Clausal Class and its Polynomiality. (arXiv:2108.13744v1 [cs.AI])","link":"http://arxiv.org/abs/2108.13744","description":"<p>The expressiveness of propositional non-clausal (NC) formulas is\nexponentially richer than that of clausal formulas. Yet, clausal efficiency\noutperforms non-clausal one. Indeed, a major weakness of the latter is that,\nwhile Horn clausal formulas, along with Horn algorithms, are crucial for the\nhigh efficiency of clausal reasoning, no Horn-like formulas in non-clausal form\nhad been proposed. To overcome such weakness, we define the hybrid class\n$\\mathbb{H_{NC}}$ of Horn Non-Clausal (Horn-NC) formulas, by adequately lifting\nthe Horn pattern to NC form, and argue that $\\mathbb{H_{NC}}$, along with\nfuture Horn-NC algorithms, shall increase non-clausal efficiency just as the\nHorn class has increased clausal efficiency. Secondly, we: (i) give the\ncompact, inductive definition of $\\mathbb{H_{NC}}$; (ii) prove that\nsyntactically $\\mathbb{H_{NC}}$ subsumes the Horn class but semantically both\nclasses are equivalent, and (iii) characterize the non-clausal formulas\nbelonging to $\\mathbb{H_{NC}}$. Thirdly, we define the Non-Clausal\nUnit-Resolution calculus, $UR_{NC}$, and prove that it checks the\nsatisfiability of $\\mathbb{H_{NC}}$ in polynomial time. This fact, to our\nknowledge, makes $\\mathbb{H_{NC}}$ the first characterized polynomial class in\nNC reasoning. Finally, we prove that $\\mathbb{H_{NC}}$ is linearly\nrecognizable, and also that it is both strictly succincter and exponentially\nricher than the Horn class. We discuss that in NC automated reasoning, e.g.\nsatisfiability solving, theorem proving, logic programming, etc., can directly\nbenefit from $\\mathbb{H_{NC}}$ and $UR_{NC}$ and that, as a by-product of its\nproved properties, $\\mathbb{H_{NC}}$ arises as a new alternative to analyze\nHorn functions and implication systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imaz_G/0/1/0/all/0/1\">Gonzalo E. Imaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The five Is: Key principles for interpretable and safe conversational AI. (arXiv:2108.13766v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13766","description":"<p>In this position paper, we present five key principles, namely\ninterpretability, inherent capability to explain, independent data, interactive\nlearning, and inquisitiveness, for the development of conversational AI that,\nunlike the currently popular black box approaches, is transparent and\naccountable. At present, there is a growing concern with the use of black box\nstatistical language models: While displaying impressive average performance,\nsuch systems are also prone to occasional spectacular failures, for which there\nis no clear remedy. In an effort to initiate a discussion on possible\nalternatives, we outline and exemplify how our five principles enable the\ndevelopment of conversational AI systems that are transparent and thus safer\nfor use. We also present some of the challenges inherent in the implementation\nof those principles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahde_M/0/1/0/all/0/1\">Mattias Wahde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virgolin_M/0/1/0/all/0/1\">Marco Virgolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Intelligence Algorithms for Natural Language Processing and the Semantic Web Ontology Learning. (arXiv:2108.13772v1 [cs.AI])","link":"http://arxiv.org/abs/2108.13772","description":"<p>Evolutionary clustering algorithms have considered as the most popular and\nwidely used evolutionary algorithms for minimising optimisation and practical\nproblems in nearly all fields. In this thesis, a new evolutionary clustering\nalgorithm star (ECA*) is proposed. Additionally, a number of experiments were\nconducted to evaluate ECA* against five state-of-the-art approaches. For this,\n32 heterogeneous and multi-featured datasets were used to examine their\nperformance using internal and external clustering measures, and to measure the\nsensitivity of their performance towards dataset features in the form of\noperational framework. The results indicate that ECA* overcomes its competitive\ntechniques in terms of the ability to find the right clusters. Based on its\nsuperior performance, exploiting and adapting ECA* on the ontology learning had\na vital possibility. In the process of deriving concept hierarchies from\ncorpora, generating formal context may lead to a time-consuming process.\nTherefore, formal context size reduction results in removing uninterested and\nerroneous pairs, taking less time to extract the concept lattice and concept\nhierarchies accordingly. In this premise, this work aims to propose a framework\nto reduce the ambiguity of the formal context of the existing framework using\nan adaptive version of ECA*. In turn, an experiment was conducted by applying\n385 sample corpora from Wikipedia on the two frameworks to examine the\nreduction of formal context size, which leads to yield concept lattice and\nconcept hierarchy. The resulting lattice of formal context was evaluated to the\noriginal one using concept lattice-invariants. Accordingly, the homomorphic\nbetween the two lattices preserves the quality of resulting concept hierarchies\nby 89% in contrast to the basic ones, and the reduced concept lattice inherits\nthe structural relation of the original one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1\">Bryar A. Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing the IEEE AV Test Challenge with Scenic and VerifAI. (arXiv:2108.13796v1 [cs.SE])","link":"http://arxiv.org/abs/2108.13796","description":"<p>This paper summarizes our formal approach to testing autonomous vehicles\n(AVs) in simulation for the IEEE AV Test Challenge. We demonstrate a systematic\ntesting framework leveraging our previous work on formally-driven simulation\nfor intelligent cyber-physical systems. First, to model and generate\ninteractive scenarios involving multiple agents, we used Scenic, a\nprobabilistic programming language for specifying scenarios. A Scenic program\ndefines an abstract scenario as a distribution over configurations of physical\nobjects and their behaviors over time. Sampling from an abstract scenario\nyields many different concrete scenarios which can be run as test cases for the\nAV. Starting from a Scenic program encoding an abstract driving scenario, we\ncan use the VerifAI toolkit to search within the scenario for failure cases\nwith respect to multiple AV evaluation metrics. We demonstrate the\neffectiveness of our testing framework by identifying concrete failure\nscenarios for an open-source autopilot, Apollo, starting from a variety of\nrealistic traffic scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viswanadha_K/0/1/0/all/0/1\">Kesav Viswanadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indaheng_F/0/1/0/all/0/1\">Francis Indaheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Justin Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Edward Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalvan_E/0/1/0/all/0/1\">Ellen Kalvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pant_Y/0/1/0/all/0/1\">Yash Pant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fremont_D/0/1/0/all/0/1\">Daniel J. Fremont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1\">Sanjit A. Seshia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chi-square Loss for Softmax: an Echo of Neural Network Structure. (arXiv:2108.13822v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13822","description":"<p>Softmax working with cross-entropy is widely used in classification, which\nevaluates the similarity between two discrete distribution columns (predictions\nand true labels). Inspired by chi-square test, we designed a new loss function\ncalled chi-square loss, which is also works for Softmax. Chi-square loss has a\nstatistical background. We proved that it is unbiased in optimization, and\nclarified its using conditions (its formula determines that it must work with\nlabel smoothing). In addition, we studied the sample distribution of this loss\nfunction by visualization and found that the distribution is related to the\nneural network structure, which is distinct compared to cross-entropy. In the\npast, the influence of structure was often ignored when visualizing. Chi-square\nloss can notice changes in neural network structure because it is very strict,\nand we explained the reason for this strictness. We also studied the influence\nof label smoothing and discussed the relationship between label smoothing and\ntraining accuracy and stability. Since the chi-square loss is very strict, the\nperformance will degrade when dealing samples of very many classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meiqing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PACE: Posthoc Architecture-Agnostic Concept Extractor for Explaining CNNs. (arXiv:2108.13828v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13828","description":"<p>Deep CNNs, though have achieved the state of the art performance in image\nclassification tasks, remain a black-box to a human using them. There is a\ngrowing interest in explaining the working of these deep models to improve\ntheir trustworthiness. In this paper, we introduce a Posthoc\nArchitecture-agnostic Concept Extractor (PACE) that automatically extracts\nsmaller sub-regions of the image called concepts relevant to the black-box\nprediction. PACE tightly integrates the faithfulness of the explanatory\nframework to the black-box model. To the best of our knowledge, this is the\nfirst work that extracts class-specific discriminative concepts in a posthoc\nmanner automatically. The PACE framework is used to generate explanations for\ntwo different CNN architectures trained for classifying the AWA2 and\nImagenet-Birds datasets. Extensive human subject experiments are conducted to\nvalidate the human interpretability and consistency of the explanations\nextracted by PACE. The results from these experiments suggest that over 72% of\nthe concepts extracted by PACE are human interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamakshi_V/0/1/0/all/0/1\">Vidhya Kamakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1\">Uday Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Common Testing Terminology for Software Engineering and Artificial Intelligence Experts. (arXiv:2108.13837v1 [cs.SE])","link":"http://arxiv.org/abs/2108.13837","description":"<p>Analytical quality assurance, especially testing, is an integral part of\nsoftware-intensive system development. With the increased usage of Artificial\nIntelligence (AI) and Machine Learning (ML) as part of such systems, this\nbecomes more difficult as well-understood software testing approaches cannot be\napplied directly to the AI-enabled parts of the system. The required adaptation\nof classical testing approaches and development of new concepts for AI would\nbenefit from a deeper understanding and exchange between AI and software\nengineering experts. A major obstacle on this way, we see in the different\nterminologies used in the two communities. As we consider a mutual\nunderstanding of the testing terminology as a key, this paper contributes a\nmapping between the most important concepts from classical software testing and\nAI testing. In the mapping, we highlight differences in relevance and naming of\nthe mapped concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jockel_L/0/1/0/all/0/1\">Lisa J&#xf6;ckel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_T/0/1/0/all/0/1\">Thomas Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klas_M/0/1/0/all/0/1\">Michael Kl&#xe4;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauer_M/0/1/0/all/0/1\">Marc P. Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gross_J/0/1/0/all/0/1\">Janek Gro&#xdf;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery. (arXiv:2108.13844v1 [eess.IV])","link":"http://arxiv.org/abs/2108.13844","description":"<p>Fiducial markers are commonly used in navigation assisted minimally invasive\nspine surgery (MISS) and they help transfer image coordinates into real world\ncoordinates. In practice, these markers might be located outside the\nfield-of-view (FOV), due to the limited detector sizes of C-arm cone-beam\ncomputed tomography (CBCT) systems used in intraoperative surgeries. As a\nconsequence, reconstructed markers in CBCT volumes suffer from artifacts and\nhave distorted shapes, which sets an obstacle for navigation. In this work, we\npropose two fiducial marker detection methods: direct detection from distorted\nmarkers (direct method) and detection after marker recovery (recovery method).\nFor direct detection from distorted markers in reconstructed volumes, an\nefficient automatic marker detection method using two neural networks and a\nconventional circle detection algorithm is proposed. For marker recovery, a\ntask-specific learning strategy is proposed to recover markers from severely\ntruncated data. Afterwards, a conventional marker detection algorithm is\napplied for position detection. The two methods are evaluated on simulated data\nand real data, both achieving a marker registration error smaller than 0.2 mm.\nOur experiments demonstrate that the direct method is capable of detecting\ndistorted markers accurately and the recovery method with task-specific\nlearning has high robustness and generalizability on various data sets. In\naddition, the task-specific learning is able to reconstruct other structures of\ninterest accurately, e.g. ribs for image-guided needle biopsy, from severely\ntruncated data, which empowers CBCT systems with new potential applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_F/0/1/0/all/0/1\">Fuxin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreher_B/0/1/0/all/0/1\">Bj&#xf6;rn Kreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keil_H/0/1/0/all/0/1\">Holger Keil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andreas_M/0/1/0/all/0/1\">Maier Andreas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Domain Adaptation for Question Answering using Limited Text Corpora. (arXiv:2108.13854v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13854","description":"<p>Question generation has recently shown impressive results in customizing\nquestion answering (QA) systems to new domains. These approaches circumvent the\nneed for manually annotated training data from the new domain and, instead,\ngenerate synthetic question-answer pairs that are used for training. However,\nexisting methods for question generation rely on large amounts of synthetically\ngenerated datasets and costly computational resources, which render these\ntechniques widely inaccessible when the text corpora is of limited size. This\nis problematic as many niche domains rely on small text corpora, which\nnaturally restricts the amount of synthetic data that can be generated. In this\npaper, we propose a novel framework for domain adaptation called contrastive\ndomain adaptation for QA (CAQA). Specifically, CAQA combines techniques from\nquestion generation and domain-invariant learning to answer out-of-domain\nquestions in settings with limited text corpora. Here, we train a QA system on\nboth source data and generated data from the target domain with a contrastive\nadaptation loss that is incorporated in the training objective. By combining\ntechniques from question generation and domain-invariant learning, our model\nachieved considerable improvements compared to state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhenrui Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kratzwald_B/0/1/0/all/0/1\">Bernhard Kratzwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1\">Stefan Feuerriegel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRP-FED: Addressing Client Imbalance in Federated Learning via Global-Regularized Personalization. (arXiv:2108.13858v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13858","description":"<p>Since data is presented long-tailed in reality, it is challenging for\nFederated Learning (FL) to train across decentralized clients as practical\napplications. We present Global-Regularized Personalization (GRP-FED) to tackle\nthe data imbalanced issue by considering a single global model and multiple\nlocal models for each client. With adaptive aggregation, the global model\ntreats multiple clients fairly and mitigates the global long-tailed issue. Each\nlocal model is learned from the local data and aligns with its distribution for\ncustomization. To prevent the local model from just overfitting, GRP-FED\napplies an adversarial discriminator to regularize between the learned\nglobal-local features. Extensive results show that our GRP-FED improves under\nboth global and local scenarios on real-world MIT-BIH and synthesis CIFAR-10\ndatasets, achieving comparable performance and addressing client imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yen-Hsiu Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Shenda Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Derun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Moxian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images. (arXiv:2108.13865v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13865","description":"<p>In this paper, we present InSeGAN, an unsupervised 3D generative adversarial\nnetwork (GAN) for segmenting (nearly) identical instances of rigid objects in\ndepth images. Using an analysis-by-synthesis approach, we design a novel GAN\narchitecture to synthesize a multiple-instance depth image with independent\ncontrol over each instance. InSeGAN takes in a set of code vectors (e.g.,\nrandom noise vectors), each encoding the 3D pose of an object that is\nrepresented by a learned implicit object template. The generator has two\ndistinct modules. The first module, the instance feature generator, uses each\nencoded pose to transform the implicit template into a feature map\nrepresentation of each object instance. The second module, the depth image\nrenderer, aggregates all of the single-instance feature maps output by the\nfirst module and generates a multiple-instance depth image. A discriminator\ndistinguishes the generated multiple-instance depth images from the\ndistribution of true depth images. To use our model for instance segmentation,\nwe propose an instance pose encoder that learns to take in a generated depth\nimage and reproduce the pose code vectors for all of the object instances. To\nevaluate our approach, we introduce a new synthetic dataset, \"Insta-10\",\nconsisting of 100,000 depth images, each with 5 instances of an object from one\nof 10 classes. Our experiments on Insta-10, as well as on real-world noisy\ndepth images, show that InSeGAN achieves state-of-the-art performance, often\noutperforming prior methods by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_G/0/1/0/all/0/1\">Goncalo Dias Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddarth Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_T/0/1/0/all/0/1\">Tim K. Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_A/0/1/0/all/0/1\">Alan Sullivan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13875","description":"<p>Scenario-based question answering (SQA) requires retrieving and reading\nparagraphs from a large corpus to answer a question which is contextualized by\na long scenario description. Since a scenario contains both keyphrases for\nretrieval and much noise, retrieval for SQA is extremely difficult. Moreover,\nit can hardly be supervised due to the lack of relevance labels of paragraphs\nfor SQA. To meet the challenge, in this paper we propose a joint\nretriever-reader model called JEEVES where the retriever is implicitly\nsupervised only using QA labels via a novel word weighting mechanism. JEEVES\nsignificantly outperforms a variety of strong baselines on multiple-choice\nquestions in three SQA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13897","description":"<p>The MS MARCO ranking dataset has been widely used for training deep learning\nmodels for IR tasks, achieving considerable effectiveness on diverse zero-shot\nscenarios. However, this type of resource is scarce in other languages than\nEnglish. In this work we present mMARCO, a multilingual version of the MS MARCO\npassage ranking dataset comprising 8 languages that was created using machine\ntranslation. We evaluated mMARCO by fine-tuning mono and multilingual\nre-ranking models on it. Experimental results demonstrate that multilingual\nmodels fine-tuned on our translated dataset achieve superior effectiveness than\nmodels fine-tuned on the original English version alone. Also, our distilled\nmultilingual re-ranker is competitive with non-distilled models while having\n5.4 times fewer parameters. The translated datasets as well as fine-tuned\nmodels are available at https://github.com/unicamp-dl/mMARCO.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Henrique Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campiotti_I/0/1/0/all/0/1\">Israel Campiotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Retrieval Augmented Generation for Zero-shot Slot Filling. (arXiv:2108.13934v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13934","description":"<p>Automatically inducing high quality knowledge graphs from a given collection\nof documents still remains a challenging problem in AI. One way to make headway\nfor this problem is through advancements in a related task known as slot\nfilling. In this task, given an entity query in form of [Entity, Slot, ?], a\nsystem is asked to fill the slot by generating or extracting the missing value\nexploiting evidence extracted from relevant passage(s) in the given document\ncollection. The recent works in the field try to solve this task in an\nend-to-end fashion using retrieval-based language models. In this paper, we\npresent a novel approach to zero-shot slot filling that extends dense passage\nretrieval with hard negatives and robust training procedures for retrieval\naugmented generation models. Our model reports large improvements on both T-REx\nand zsRE slot filling datasets, improving both passage retrieval and slot value\ngeneration, and ranking at the top-1 position in the KILT leaderboard.\nMoreover, we demonstrate the robustness of our system showing its domain\nadaptation capability on a new variant of the TACRED dataset for slot filling,\nthrough a combination of zero/few-shot learning. We release the source code and\npre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward AI-enhanced online-characterization and shaping of ultrashort X-ray free-electron laser pulses. (arXiv:2108.13979v1 [physics.data-an])","link":"http://arxiv.org/abs/2108.13979","description":"<p>X-ray free-electron lasers (XFELs) as the world`s most brilliant light\nsources provide ultrashort X-ray pulses with durations typically on the order\nof femtoseconds. Recently, they have approached and entered the attosecond\nregime, which holds new promises for single-molecule imaging and studying\nnonlinear and ultrafast phenomena like localized electron dynamics. The\ntechnological evolution of XFELs toward well-controllable light sources for\nprecise metrology of ultrafast processes was, however, hampered by the\ndiagnostic capabilities for characterizing X-ray pulses at the attosecond\nfrontier. In this regard, the spectroscopic technique of photoelectron angular\nstreaking has successfully proven how to non-destructively retrieve the exact\ntime-energy structure of XFEL pulses on a single-shot basis. By using\nartificial intelligence algorithms, in particular convolutional neural\nnetworks, we here show how this technique can be leveraged from its\nproof-of-principle stage toward routine diagnostics at XFELs, thus enhancing\nand refining their scientific access in all related disciplines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Dingel_K/0/1/0/all/0/1\">Kristina Dingel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Otto_T/0/1/0/all/0/1\">Thorsten Otto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Marder_L/0/1/0/all/0/1\">Lutz Marder</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Funke_L/0/1/0/all/0/1\">Lars Funke</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Held_A/0/1/0/all/0/1\">Arne Held</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Savio_S/0/1/0/all/0/1\">Sara Savio</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hans_A/0/1/0/all/0/1\">Andreas Hans</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hartmann_G/0/1/0/all/0/1\">Gregor Hartmann</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Meier_D/0/1/0/all/0/1\">David Meier</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Viefhaus_J/0/1/0/all/0/1\">Jens Viefhaus</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ehresmann_A/0/1/0/all/0/1\">Arno Ehresmann</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ilchen_M/0/1/0/all/0/1\">Markus Ilchen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Helml_W/0/1/0/all/0/1\">Wolfram Helml</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Deception into CyberBattleSim for Autonomous Defense. (arXiv:2108.13980v1 [cs.CR])","link":"http://arxiv.org/abs/2108.13980","description":"<p>Deceptive elements, including honeypots and decoys, were incorporated into\nthe Microsoft CyberBattleSim experimentation and research platform. The\ndefensive capabilities of the deceptive elements were tested using\nreinforcement learning based attackers in the provided capture the flag\nenvironment. The attacker's progress was found to be dependent on the number\nand location of the deceptive elements. This is a promising step toward\nreproducibly testing attack and defense algorithms in a simulated enterprise\nnetwork with deceptive defensive elements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walter_E/0/1/0/all/0/1\">Erich Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferguson_Walter_K/0/1/0/all/0/1\">Kimberly Ferguson-Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridley_A/0/1/0/all/0/1\">Ahmad Ridley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Mitosis against Domain Shift using a Fused Detector and Deep Ensemble Classification Model for MIDOG Challenge. (arXiv:2108.13983v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13983","description":"<p>Mitotic figure count is an important marker of tumor proliferation and has\nbeen shown to be associated with patients' prognosis. Deep learning based\nmitotic figure detection methods have been utilized to automatically locate the\ncell in mitosis using hematoxylin \\&amp; eosin (H\\&amp;E) stained images. However, the\nmodel performance deteriorates due to the large variation of color tone and\nintensity in H\\&amp;E images. In this work, we proposed a two stage mitotic figure\ndetection framework by fusing a detector and a deep ensemble classification\nmodel. To alleviate the impact of color variation in H\\&amp;E images, we utilize\nboth stain normalization and data augmentation, aiding model to learn color\nirrelevant features. The proposed model obtains an F1 score of 0.7550 on the\npreliminary testing set released by the MIDOG challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingtang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yujie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhibin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yubo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Subsampling Based Method for Causal Discovery on Discrete Data. (arXiv:2108.13984v1 [stat.ML])","link":"http://arxiv.org/abs/2108.13984","description":"<p>Inferring causal directions on discrete and categorical data is an important\nyet challenging problem. Even though the additive noise models (ANMs) approach\ncan be adapted to the discrete data, the functional structure assumptions make\nit not applicable on categorical data. Inspired by the principle that the cause\nand mechanism are independent, various methods have been developed, leveraging\nindependence tests such as the distance correlation measure. In this work, we\ntake an alternative perspective and propose a subsampling-based method to test\nthe independence between the generating schemes of the cause and that of the\nmechanism. Our methodology works for both discrete and categorical data and\ndoes not imply any functional model on the data, making it a more flexible\napproach. To demonstrate the efficacy of our methodology, we compare it with\nexisting baselines over various synthetic data and real data experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Goddard_A/0/1/0/all/0/1\">Austin Goddard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepTaskAPT: Insider APT detection using Task-tree based Deep Learning. (arXiv:2108.13989v1 [cs.CR])","link":"http://arxiv.org/abs/2108.13989","description":"<p>APT, known as Advanced Persistent Threat, is a difficult challenge for cyber\ndefence. These threats make many traditional defences ineffective as the\nvulnerabilities exploited by these threats are insiders who have access to and\nare within the network. This paper proposes DeepTaskAPT, a heterogeneous\ntask-tree based deep learning method to construct a baseline model based on\nsequences of tasks using a Long Short-Term Memory (LSTM) neural network that\ncan be applied across different users to identify anomalous behaviour. Rather\nthan applying the model to sequential log entries directly, as most current\napproaches do, DeepTaskAPT applies a process tree based task generation method\nto generate sequential log entries for the deep learning model. To assess the\nperformance of DeepTaskAPT, we use a recently released synthetic dataset, DARPA\nOperationally Transparent Computing (OpTC) dataset and a real-world dataset,\nLos Alamos National Laboratory (LANL) dataset. Both of them are composed of\nhost-based data collected from sensors. Our results show that DeepTaskAPT\noutperforms similar approaches e.g. DeepLog and the DeepTaskAPT baseline model\ndemonstrate its capability to detect malicious traces in various attack\nscenarios while having high accuracy and low false-positive rates. To the best\nof knowledge this is the very first attempt of using recently introduced OpTC\ndataset for cyber threat detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1\">Mohammad Mamun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kevin Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13990","description":"<p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4 data sets with consistent observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdieh_M/0/1/0/all/0/1\">Mahdis Mahdieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantization of Generative Adversarial Networks for Efficient Inference: a Methodological Study. (arXiv:2108.13996v1 [cs.AI])","link":"http://arxiv.org/abs/2108.13996","description":"<p>Generative adversarial networks (GANs) have an enormous potential impact on\ndigital content creation, e.g., photo-realistic digital avatars, semantic\ncontent editing, and quality enhancement of speech and images. However, the\nperformance of modern GANs comes together with massive amounts of computations\nperformed during the inference and high energy consumption. That complicates,\nor even makes impossible, their deployment on edge devices. The problem can be\nreduced with quantization -- a neural network compression technique that\nfacilitates hardware-friendly inference by replacing floating-point\ncomputations with low-bit integer ones. While quantization is well established\nfor discriminative models, the performance of modern quantization techniques in\napplication to GANs remains unclear. GANs generate content of a more complex\nstructure than discriminative models, and thus quantization of GANs is\nsignificantly more challenging. To tackle this problem, we perform an extensive\nexperimental study of state-of-art quantization techniques on three diverse GAN\narchitectures, namely StyleGAN, Self-Attention GAN, and CycleGAN. As a result,\nwe discovered practical recipes that allowed us to successfully quantize these\nmodels for inference with 4/8-bit weights and 8-bit activations while\npreserving the quality of the original full-precision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andreev_P/0/1/0/all/0/1\">Pavel Andreev</a> (1, 2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Fritzler_A/0/1/0/all/0/1\">Alexander Fritzler</a> (1, 2, 4), <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a> (1, 3, 5) ((1) Higher School of Economics, (2) Skolkovo Institute of Science and Technology, (3) Samsung AI Center Moscow, (4) Yandex, (5) Samsung-HSE Laboratory)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First return, then explore. (arXiv:2004.12919v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2004.12919","description":"<p>The promise of reinforcement learning is to solve complex sequential decision\nproblems autonomously by specifying a high-level reward function only. However,\nreinforcement learning algorithms struggle when, as is often the case, simple\nand intuitive rewards provide sparse and deceptive feedback. Avoiding these\npitfalls requires thoroughly exploring the environment, but creating algorithms\nthat can do so remains one of the central challenges of the field. We\nhypothesise that the main impediment to effective exploration originates from\nalgorithms forgetting how to reach previously visited states (\"detachment\") and\nfrom failing to first return to a state before exploring from it\n(\"derailment\"). We introduce Go-Explore, a family of algorithms that addresses\nthese two challenges directly through the simple principles of explicitly\nremembering promising states and first returning to such states before\nintentionally exploring. Go-Explore solves all heretofore unsolved Atari games\nand surpasses the state of the art on all hard-exploration games, with orders\nof magnitude improvements on the grand challenges Montezuma's Revenge and\nPitfall. We also demonstrate the practical potential of Go-Explore on a\nsparse-reward pick-and-place robotics task. Additionally, we show that adding a\ngoal-conditioned policy can further improve Go-Explore's exploration efficiency\nand enable it to handle stochasticity throughout training. The substantial\nperformance gains from Go-Explore suggest that the simple principles of\nremembering states, returning to them, and exploring from them are a powerful\nand general approach to exploration, an insight that may prove critical to the\ncreation of truly intelligent learning agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ecoffet_A/0/1/0/all/0/1\">Adrien Ecoffet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huizinga_J/0/1/0/all/0/1\">Joost Huizinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1\">Joel Lehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1\">Kenneth O. Stanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape Defense Against Adversarial Attacks. (arXiv:2008.13336v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.13336","description":"<p>Humans rely heavily on shape information to recognize objects. Conversely,\nconvolutional neural networks (CNNs) are biased more towards texture. This is\nperhaps the main reason why CNNs are vulnerable to adversarial examples. Here,\nwe explore how shape bias can be incorporated into CNNs to improve their\nrobustness. Two algorithms are proposed, based on the observation that edges\nare invariant to moderate imperceptible perturbations. In the first one, a\nclassifier is adversarially trained on images with the edge map as an\nadditional channel. At inference time, the edge map is recomputed and\nconcatenated to the image. In the second algorithm, a conditional GAN is\ntrained to translate the edge maps, from clean and/or perturbed images, into\nclean images. Inference is done over the generated image corresponding to the\ninput's edge map. Extensive experiments over 10 datasets demonstrate the\neffectiveness of the proposed algorithms against FGSM and $\\ell_\\infty$ PGD-40\nattacks. Further, we show that a) edge information can also benefit other\nadversarial training methods, and b) CNNs trained on edge-augmented inputs are\nmore robust against natural image corruptions such as motion blur, impulse\nnoise and JPEG compression, than CNNs trained solely on RGB images. From a\nbroader perspective, our study suggests that CNNs do not adequately account for\nimage structures that are crucial for robustness. Code is available\nat:~\\url{https://github.com/aliborji/Shapedefence.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blindfolded Attackers Still Threatening: Strict Black-Box Adversarial Attacks on Graphs. (arXiv:2012.06757v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.06757","description":"<p>Adversarial attacks on graphs have attracted considerable research interests.\nExisting works assume the attacker is either (partly) aware of the victim\nmodel, or able to send queries to it. These assumptions are, however,\nunrealistic. To bridge the gap between theoretical graph attacks and real-world\nscenarios, in this work, we propose a novel and more realistic setting: strict\nblack-box graph attack, in which the attacker has no knowledge about the victim\nmodel at all and is not allowed to send any queries. To design such an attack\nstrategy, we first propose a generic graph filter to unify different families\nof graph-based models. The strength of attacks can then be quantified by the\nchange in the graph filter before and after attack. By maximizing this change,\nwe are able to find an effective attack strategy, regardless of the underlying\nmodel. To solve this optimization problem, we also propose a relaxation\ntechnique and approximation theories to reduce the difficulty as well as the\ncomputational expense. Experiments demonstrate that, even with no exposure to\nthe model, the Macro-F1 drops 6.4% in node classification and 29.5% in graph\nclassification, which is a significant result compared with existent works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Ensemble Learning under the Era of Deep Learning. (arXiv:2101.08387v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.08387","description":"<p>Due to the dominant position of deep learning (mostly deep neural networks)\nin various artificial intelligence applications, recently, ensemble learning\nbased on deep neural networks (ensemble deep learning) has shown significant\nperformances in improving the generalization of learning system. However, since\nmodern deep neural networks usually have millions to billions of parameters,\nthe time and space overheads for training multiple base deep learners and\ntesting with the ensemble deep learner are far greater than that of traditional\nensemble learning. Though several algorithms of fast ensemble deep learning\nhave been proposed to promote the deployment of ensemble deep learning in some\napplications, further advances still need to be made for many applications in\nspecific fields, where the developing time and computing resources are usually\nrestricted or the data to be processed is of large dimensionality. An urgent\nproblem needs to be solved is how to take the significant advantages of\nensemble deep learning while reduce the required time and space overheads so\nthat many more applications in specific fields can benefit from it. For the\nalleviation of this problem, it is essential to know about how ensemble\nlearning has developed under the era of deep learning. Thus, in this article,\nwe present discussions focusing on data analyses of published works,\nmethodologies, recent advances and unattainability of traditional ensemble\nlearning and ensemble deep learning. We hope this article will be helpful to\nrealize the technical challenges faced by future developments of ensemble\nlearning under the era of deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Haijun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Ning Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding self-supervised Learning Dynamics without Contrastive Pairs. (arXiv:2102.06810v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.06810","description":"<p>While contrastive approaches of self-supervised learning (SSL) learn\nrepresentations by minimizing the distance between two augmented views of the\nsame data point (positive pairs) and maximizing views from different data\npoints (negative pairs), recent \\emph{non-contrastive} SSL (e.g., BYOL and\nSimSiam) show remarkable performance {\\it without} negative pairs, with an\nextra learnable predictor and a stop-gradient operation. A fundamental question\narises: why do these methods not collapse into trivial representations? We\nanswer this question via a simple theoretical study and propose a novel\napproach, DirectPred, that \\emph{directly} sets the linear predictor based on\nthe statistics of its inputs, without gradient training. On ImageNet, it\nperforms comparably with more complex two-layer non-linear predictors that\nemploy BatchNorm and outperforms a linear predictor by $2.5\\%$ in 300-epoch\ntraining (and $5\\%$ in 60-epoch). DirectPred is motivated by our theoretical\nstudy of the nonlinear learning dynamics of non-contrastive SSL in simple\nlinear networks. Our study yields conceptual insights into how non-contrastive\nSSL methods learn, how they avoid representational collapse, and how multiple\nfactors, like predictor networks, stop-gradients, exponential moving averages,\nand weight decay all come into play. Our simple theory recapitulates the\nresults of real-world ablation studies in both STL-10 and ImageNet. Code is\nreleased https://github.com/facebookresearch/luckmatters/tree/master/ssl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dont Just Divide; Polarize and Conquer!. (arXiv:2102.11872v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.11872","description":"<p>In data containing heterogeneous subpopulations, classification performance\nbenefits from incorporating the knowledge of cluster structure in the\nclassifier. Previous methods for such combined clustering and classification\nare either 1) classifier-specific and not generic, or 2) independently perform\nclustering and classifier training, which may not form clusters that can\npotentially benefit classifier performance. The question of how to perform\nclustering to improve the performance of classifiers trained on the clusters\nhas received scant attention in previous literature, despite its importance in\nseveral real-world applications. In this paper, we design a simple and\nefficient classification algorithm called Clustering Aware Classification\n(CAC), to find clusters that are well suited for being used as training\ndatasets by classifiers for each underlying subpopulation. Our experiments on\nsynthetic and real benchmark datasets demonstrate the efficacy of CAC over\nprevious methods for combined clustering and classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shivin Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Siddharth Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_L/0/1/0/all/0/1\">Lim Jun Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_V/0/1/0/all/0/1\">Vaibhav Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020. (arXiv:2104.10201v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.10201","description":"<p>This paper presents the results and insights from the black-box optimization\n(BBO) challenge at NeurIPS 2020 which ran from July-October, 2020. The\nchallenge emphasized the importance of evaluating derivative-free optimizers\nfor tuning the hyperparameters of machine learning models. This was the first\nblack-box optimization challenge with a machine learning emphasis. It was based\non tuning (validation set) performance of standard machine learning models on\nreal datasets. This competition has widespread impact as black-box optimization\n(e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost\nevery machine learning project as well as many applications outside of machine\nlearning. The final leaderboard was determined using the optimization\nperformance on held-out (hidden) objective functions, where the optimizers ran\nwithout human intervention. Baselines were set using the default settings of\nseveral open-source black-box optimization packages as well as random search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Ryan Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eriksson_D/0/1/0/all/0/1\">David Eriksson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCourt_M/0/1/0/all/0/1\">Michael McCourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiili_J/0/1/0/all/0/1\">Juha Kiili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laaksonen_E/0/1/0/all/0/1\">Eero Laaksonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effects of Air Quality on the Spread of the COVID-19 Pandemic in Italy: An Artificial Intelligence Approach. (arXiv:2104.12546v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2104.12546","description":"<p>The COVID-19 pandemic considerably affects public health systems around the\nworld. The lack of knowledge about the virus, the extension of this phenomenon,\nand the speed of the evolution of the infection are all factors that highlight\nthe necessity of employing new approaches to study these events. Artificial\nintelligence techniques may be useful in analyzing data related to areas\naffected by the virus. The aim of this work is to investigate any possible\nrelationships between air quality and confirmed cases of COVID-19 in Italian\ndistricts. Specifically, we report an analysis of the correlation between daily\nCOVID-19 cases and environmental factors, such as temperature, relative\nhumidity, and atmospheric pollutants. Our analysis confirms a significant\nassociation of some environmental parameters with the spread of the virus. This\nsuggests that machine learning models trained on the environmental parameters\nto predict the number of future infected cases may be accurate. Predictive\nmodels may be useful for helping institutions in making decisions for\nprotecting the population and contrasting the pandemic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loreggia_A/0/1/0/all/0/1\">Andrea Loreggia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passarelli_A/0/1/0/all/0/1\">Anna Passarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pini_M/0/1/0/all/0/1\">Maria Silvia Pini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curious Representation Learning for Embodied Intelligence. (arXiv:2105.01060v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01060","description":"<p>Self-supervised representation learning has achieved remarkable success in\nrecent years. By subverting the need for supervised labels, such approaches are\nable to utilize the numerous unlabeled images that exist on the Internet and in\nphotographic datasets. Yet to build truly intelligent agents, we must construct\nrepresentation learning algorithms that can learn not only from datasets but\nalso learn from environments. An agent in a natural environment will not\ntypically be fed curated data. Instead, it must explore its environment to\nacquire the data it will learn from. We propose a framework, curious\nrepresentation learning (CRL), which jointly learns a reinforcement learning\npolicy and a visual representation model. The policy is trained to maximize the\nerror of the representation learner, and in doing so is incentivized to explore\nits environment. At the same time, the learned representation becomes stronger\nand stronger as the policy feeds it ever harder data to learn from. Our learned\nrepresentations enable promising transfer to downstream navigation tasks,\nperforming better than or comparably to ImageNet pretraining without using any\nsupervision at all. In addition, despite being trained in simulation, our\nlearned representations can obtain interpretable results on real images. Code\nis available at https://yilundu.github.io/crl/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Entropy Guided Node Embedding Dimension Selection for Graph Neural Networks. (arXiv:2105.03178v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.03178","description":"<p>Graph representation learning has achieved great success in many areas,\nincluding e-commerce, chemistry, biology, etc. However, the fundamental problem\nof choosing the appropriate dimension of node embedding for a given graph still\nremains unsolved. The commonly used strategies for Node Embedding Dimension\nSelection (NEDS) based on grid search or empirical knowledge suffer from heavy\ncomputation and poor model performance. In this paper, we revisit NEDS from the\nperspective of minimum entropy principle. Subsequently, we propose a novel\nMinimum Graph Entropy (MinGE) algorithm for NEDS with graph data. To be\nspecific, MinGE considers both feature entropy and structure entropy on graphs,\nwhich are carefully designed according to the characteristics of the rich\ninformation in them. The feature entropy, which assumes the embeddings of\nadjacent nodes to be more similar, connects node features and link topology on\ngraphs. The structure entropy takes the normalized degree as basic unit to\nfurther measure the higher-order structure of graphs. Based on them, we design\nMinGE to directly calculate the ideal node embedding dimension for any graph.\nFinally, comprehensive experiments with popular Graph Neural Networks (GNNs) on\nbenchmark datasets demonstrate the effectiveness and generalizability of our\nproposed MinGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gongxu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uniform Convergence, Adversarial Spheres and a Simple Remedy. (arXiv:2105.03491v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.03491","description":"<p>Previous work has cast doubt on the general framework of uniform convergence\nand its ability to explain generalization in neural networks. By considering a\nspecific dataset, it was observed that a neural network completely\nmisclassifies a projection of the training data (adversarial set), rendering\nany existing generalization bound based on uniform convergence vacuous. We\nprovide an extensive theoretical investigation of the previously studied data\nsetting through the lens of infinitely-wide models. We prove that the Neural\nTangent Kernel (NTK) also suffers from the same phenomenon and we uncover its\norigin. We highlight the important role of the output bias and show\ntheoretically as well as empirically how a sensible choice completely mitigates\nthe problem. We identify sharp phase transitions in the accuracy on the\nadversarial set and study its dependency on the training sample size. As a\nresult, we are able to characterize critical sample sizes beyond which the\neffect disappears. Moreover, we study decompositions of a neural network into a\nclean and noisy part by considering its canonical decomposition into its\ndifferent eigenfunctions and show empirically that for too small bias the\nadversarial phenomenon still persists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1\">Gregor Bachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Can Robots Trust Each Other For Better Cooperation? A Relative Needs Entropy Based Robot-Robot Trust Assessment Model. (arXiv:2105.07443v2 [cs.MA] UPDATED)","link":"http://arxiv.org/abs/2105.07443","description":"<p>Cooperation in multi-agent and multi-robot systems can help agents build\nvarious formations, shapes, and patterns presenting corresponding functions and\npurposes adapting to different situations. Relationships between agents such as\ntheir spatial proximity and functional similarities could play a crucial role\nin cooperation between agents. Trust level between agents is an essential\nfactor in evaluating their relationships' reliability and stability, much as\npeople do. This paper proposes a new model called Relative Needs Entropy (RNE)\nto assess trust between robotic agents. RNE measures the distance of needs\ndistribution between individual agents or groups of agents. To exemplify its\nutility, we implement and demonstrate our trust model through experiments\nsimulating a heterogeneous multi-robot grouping task in a persistent urban\nsearch and rescue mission consisting of tasks at two levels of difficulty. The\nresults suggest that RNE trust-Based grouping of robots can achieve better\nperformance and adaptability for diverse task execution compared to the\nstate-of-the-art energy-based or distance-based grouping models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasuraman_R/0/1/0/all/0/1\">Ramviyas Parasuraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03158","description":"<p>Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing the generalization\ncapabilities of a model, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data over regularizing\nthe objective to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation (C1) and\na taxonomy for existing works (C2), this survey is concerned with data\naugmentation methods for textual classification and aims to achieve a concise\nand comprehensive overview for researchers and practitioners (C3). Derived from\nthe taxonomy, we divided more than 100 methods into 12 different groupings and\nprovide state-of-the-art references expounding which methods are highly\npromising (C4). Finally, research perspectives that may constitute a building\nblock for future work are given (C5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutually improved endoscopic image synthesis and landmark detection in unpaired image-to-image translation. (arXiv:2107.06941v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06941","description":"<p>The CycleGAN framework allows for unsupervised image-to-image translation of\nunpaired data. In a scenario of surgical training on a physical surgical\nsimulator, this method can be used to transform endoscopic images of phantoms\ninto images which more closely resemble the intra-operative appearance of the\nsame surgical target structure. This can be viewed as a novel augmented reality\napproach, which we coined Hyperrealism in previous work. In this use case, it\nis of paramount importance to display objects like needles, sutures or\ninstruments consistent in both domains while altering the style to a more\ntissue-like appearance. Segmentation of these objects would allow for a direct\ntransfer, however, contouring of these, partly tiny and thin foreground objects\nis cumbersome and perhaps inaccurate. Instead, we propose to use landmark\ndetection on the points when sutures pass into the tissue. This objective is\ndirectly incorporated into a CycleGAN framework by treating the performance of\npre-trained detector models as an additional optimization goal. We show that a\ntask defined on these sparse landmark labels improves consistency of synthesis\nby the generator network in both domains. Comparing a baseline CycleGAN\narchitecture to our proposed extension (DetCycleGAN), mean precision (PPV)\nimproved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by\n+0.4743. Furthermore, it could be shown that by dataset fusion, generated\nintra-operative images can be leveraged as additional training data for the\ndetection network itself. The data is released within the scope of the AdaptOR\nMICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at\nhttps://github.com/Cardio-AI/detcyclegan_pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1\">Lalith Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_G/0/1/0/all/0/1\">Gabriele Romano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1\">Sven Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelm_H/0/1/0/all/0/1\">Halvar Kelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karck_M/0/1/0/all/0/1\">Matthias Karck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1\">Raffaele De Simone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12079","description":"<p>Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation to perform reasoning\nand provide consistent and explainable answers. We illustrate the system using\na COVID-19 vaccine information case study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.13625","description":"<p>Ensuring trusted artificial intelligence (AI) in the real world is an\ncritical challenge. A still largely unexplored task is the determination of the\nmajor factors within the real world that affect the behavior and robustness of\na given AI module (e.g. weather or illumination conditions). Specifically, here\nwe seek to discover the factors that cause AI systems to fail, and to mitigate\ntheir influence. The identification of these factors usually heavily relies on\nthe availability of data that is diverse enough to cover numerous combinations\nof these factors, but the exhaustive collection of this data is onerous and\nsometimes impossible in complex environments. This paper investigates methods\nthat discover and mitigate the effects of semantic sensitive factors within a\ngiven dataset. We also here generalize the definition of fairness, which\nnormally only addresses socially relevant factors, and widen it to deal with --\nmore broadly -- the desensitization of AI systems with regard to all possible\naspects of variation in the domain. The proposed methods which discover these\nmajor factors reduce the potentially onerous demands of collecting a\nsufficiently diverse dataset. In experiments using road sign (GTSRB) and facial\nimagery (CelebA) datasets, we show the promise of these new methods and show\nthat they outperform state of the art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Log-based Anomaly Detection Without Log Parsing. (arXiv:2108.01955v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2108.01955","description":"<p>Software systems often record important runtime information in system logs\nfor troubleshooting purposes. There have been many studies that use log data to\nconstruct machine learning models for detecting system anomalies. Through our\nempirical study, we find that existing log-based anomaly detection approaches\nare significantly affected by log parsing errors that are introduced by 1) OOV\n(out-of-vocabulary) words, and 2) semantic misunderstandings. The log parsing\nerrors could cause the loss of important information for anomaly detection. To\naddress the limitations of existing methods, we propose NeuralLog, a novel\nlog-based anomaly detection approach that does not require log parsing.\nNeuralLog extracts the semantic meaning of raw log messages and represents them\nas semantic vectors. These representation vectors are then used to detect\nanomalies through a Transformer-based classification model, which can capture\nthe contextual information from log sequences. Our experimental results show\nthat the proposed approach can effectively understand the semantic meaning of\nlog messages and achieve accurate anomaly detection results. Overall, NeuralLog\nachieves F1-scores greater than 0.95 on four public datasets, outperforming the\nexisting approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Van-Hoang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02854","description":"<p>Languages differ in terms of the absence or presence of gender features, the\nnumber of gender classes and whether and where gender features are explicitly\nmarked. These cross-linguistic differences can lead to ambiguities that are\ndifficult to resolve, especially for sentence-level MT systems. The\nidentification of ambiguity and its subsequent resolution is a challenging task\nfor which currently there aren't any specific resources or challenge sets\navailable. In this paper, we introduce gENder-IT, an English--Italian challenge\nset focusing on the resolution of natural gender phenomena by providing\nword-level gender tags on the English source side and multiple gender\nalternative translations, where needed, on the Italian target side.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monti_J/0/1/0/all/0/1\">Johanna Monti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CushLEPOR: Customised hLEPOR Metric Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements. (arXiv:2108.09484v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09484","description":"<p>Human evaluation has always been expensive while researchers struggle to\ntrust the automatic metrics. To address this, we propose to customise\ntraditional metrics by taking advantages of the pre-trained language models\n(PLMs) and the limited available human labelled scores. We first re-introduce\nthe hLEPOR metric factors, followed by the Python portable version we developed\nwhich achieved the automatic tuning of the weighting parameters in hLEPOR\nmetric. Then we present the customised hLEPOR (cushLEPOR) which uses LABSE\ndistilled knowledge model to improve the metric agreement with human judgements\nby automatically optimised factor weights regarding the exact MT language pairs\nthat cushLEPOR is deployed to. We also optimise cushLEPOR towards human\nevaluation data based on MQM and pSQM framework on English-German and\nChinese-English language pairs. The experimental investigations show cushLEPOR\nboosts hLEPOR performances towards better agreements to PLMs like LABSE with\nmuch lower cost, and better agreements to human evaluations including MQM and\npSQM scores, and yields much better performances than BLEU (data available at\n\\url{https://github.com/poethan/cushLEPOR}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral Denoising Diffusion Models. (arXiv:2108.11514v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.11514","description":"<p>Denoising diffusion probabilistic models (DDPMs) have emerged as competitive\ngenerative models yet brought challenges to efficient sampling. In this paper,\nwe propose novel bilateral denoising diffusion models (BDDMs), which take\nsignificantly fewer steps to generate high-quality samples. From a bilateral\nmodeling objective, BDDMs parameterize the forward and reverse processes with a\nscore network and a scheduling network, respectively. We show that a new lower\nbound tighter than the standard evidence lower bound can be derived as a\nsurrogate objective for training the two networks. In particular, BDDMs are\nefficient, simple-to-train, and capable of further improving any pre-trained\nDDPM by optimizing the inference noise schedules. Our experiments demonstrated\nthat BDDMs can generate high-fidelity samples with as few as 3 sampling steps\nand produce comparable or even higher quality samples than DDPMs using 1000\nsteps with only 16 sampling steps (a 62x speedup).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Max W. Y. Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why and How Governments Should Monitor AI Development. (arXiv:2108.12427v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2108.12427","description":"<p>In this paper we outline a proposal for improving the governance of\nartificial intelligence (AI) by investing in government capacity to\nsystematically measure and monitor the capabilities and impacts of AI systems.\nIf adopted, this would give governments greater information about the AI\necosystem, equipping them to more effectively direct AI development and\ndeployment in the most societally and economically beneficial directions. It\nwould also create infrastructure that could rapidly identify potential threats\nor harms that could occur as a consequence of changes in the AI ecosystem, such\nas the emergence of strategically transformative capabilities, or the\ndeployment of harmful systems.\n</p>\n<p>We begin by outlining the problem which motivates this proposal: in brief,\ntraditional governance approaches struggle to keep pace with the speed of\nprogress in AI. We then present our proposal for addressing this problem:\ngovernments must invest in measurement and monitoring infrastructure. We\ndiscuss this proposal in detail, outlining what specific things governments\ncould focus on measuring and monitoring, and the kinds of benefits this would\ngenerate for policymaking. Finally, we outline some potential pilot projects\nand some considerations for implementing this in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whittlestone_J/0/1/0/all/0/1\">Jess Whittlestone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation. (arXiv:2108.12582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12582","description":"<p>Despite the remarkable performance of large-scale generative models in\nopen-domain conversation, they are known to be less practical for building\nreal-time conversation systems due to high latency. On the other hand,\nretrieval models could return responses with much lower latency but show\ninferior performance to the large-scale generative models since the\nconversation quality is bounded by the pre-defined response set. To take\nadvantage of both approaches, we propose a new training method called G2R\n(Generative-to-Retrieval distillation) that preserves the efficiency of a\nretrieval model while leveraging the conversational ability of a large-scale\ngenerative model by infusing the knowledge of the generative model into the\nretrieval model. G2R consists of two distinct techniques of distillation: the\ndata-level G2R augments the dialogue dataset with additional responses\ngenerated by the large-scale generative model, and the model-level G2R\ntransfers the response quality score assessed by the generative model to the\nscore of the retrieval model by the knowledge distillation loss. Through\nextensive experiments including human evaluation, we demonstrate that our\nretrieval-based conversation system trained with G2R shows a substantially\nimproved performance compared to the baseline retrieval model while showing\nsignificantly lower inference latency than the large-scale generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seokjun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdenee_E/0/1/0/all/0/1\">Enkhbayar Erdenee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Communication-Computation Efficient Device-Edge Co-Inference via AutoML. (arXiv:2108.13009v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.13009","description":"<p>Device-edge co-inference, which partitions a deep neural network between a\nresource-constrained mobile device and an edge server, recently emerges as a\npromising paradigm to support intelligent mobile applications. To accelerate\nthe inference process, on-device model sparsification and intermediate feature\ncompression are regarded as two prominent techniques. However, as the on-device\nmodel sparsity level and intermediate feature compression ratio have direct\nimpacts on computation workload and communication overhead respectively, and\nboth of them affect the inference accuracy, finding the optimal values of these\nhyper-parameters brings a major challenge due to the large search space. In\nthis paper, we endeavor to develop an efficient algorithm to determine these\nhyper-parameters. By selecting a suitable model split point and a pair of\nencoder/decoder for the intermediate feature vector, this problem is casted as\na sequential decision problem, for which, a novel automated machine learning\n(AutoML) framework is proposed based on deep reinforcement learning (DRL).\nExperiment results on an image classification task demonstrate the\neffectiveness of the proposed framework in achieving a better\ncommunication-computation trade-off and significant inference speedup against\nvarious baseline schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jiawei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DNNFusion: Accelerating Deep Neural Networks Execution with Advanced Operator Fusion. (arXiv:2108.13342v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2108.13342","description":"<p>Deep Neural Networks (DNNs) have emerged as the core enabler of many major\napplications on mobile devices. To achieve high accuracy, DNN models have\nbecome increasingly deep with hundreds or even thousands of operator layers,\nleading to high memory and computational requirements for inference. Operator\nfusion (or kernel/layer fusion) is key optimization in many state-of-the-art\nDNN execution frameworks, such as TensorFlow, TVM, and MNN. However, these\nframeworks usually adopt fusion approaches based on certain patterns that are\ntoo restrictive to cover the diversity of operators and layer connections.\nPolyhedral-based loop fusion techniques, on the other hand, work on a low-level\nview of the computation without operator-level information, and can also miss\npotential fusion opportunities. To address this challenge, this paper proposes\na novel and extensive loop fusion framework called DNNFusion. The basic idea of\nthis work is to work at an operator view of DNNs, but expand fusion\nopportunities by developing a classification of both individual operators and\ntheir combinations. In addition, DNNFusion includes 1) a novel\nmathematical-property-based graph rewriting framework to reduce evaluation\ncosts and facilitate subsequent operator fusion, 2) an integrated fusion plan\ngeneration that leverages the high-level analysis and accurate light-weight\nprofiling, and 3) additional optimizations during fusion code generation.\nDNNFusion is extensively evaluated on 15 DNN models with varied types of tasks,\nmodel sizes, and layer counts. The evaluation results demonstrate that\nDNNFusion finds up to 8.8x higher fusion opportunities, outperforms four\nstate-of-the-art DNN execution frameworks with 9.3x speedup. The memory\nrequirement reduction and speedups can enable the execution of many of the\ntarget models on mobile devices and even make them part of a real-time\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jiexiong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1\">Gagan Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Artificial Intelligence"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Want To Reduce Labeling Cost? GPT-3 Can Help. (arXiv:2108.13487v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13487","description":"<p>Data annotation is a time-consuming and labor-intensive process for many NLP\ntasks. Although there exist various methods to produce pseudo data labels, they\nare often task-specific and require a decent amount of labeled data to start\nwith. Recently, the immense language model GPT-3 with 175 billion parameters\nhas achieved tremendous improvement across many few-shot learning tasks. In\nthis paper, we explore ways to leverage GPT-3 as a low-cost data labeler to\ntrain other models. We find that, to make the downstream model achieve the same\nperformance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use\nlabels from GPT-3 than using labels from humans. Furthermore, we propose a\nnovel framework of combining pseudo labels from GPT-3 with human labels, which\nleads to even better performance with limited labeling budget. These results\npresent a cost-effective data labeling methodology that is generalizable to\nmany practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Exaggeration Detection of Health Science Press Releases. (arXiv:2108.13493v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13493","description":"<p>Public trust in science depends on honest and factual communication of\nscientific papers. However, recent studies have demonstrated a tendency of news\nmedia to misrepresent scientific papers by exaggerating their findings. Given\nthis, we present a formalization of and study into the problem of exaggeration\ndetection in science communication. While there are an abundance of scientific\npapers and popular media articles written about them, very rarely do the\narticles include a direct link to the original paper, making data collection\nchallenging. We address this by curating a set of labeled press\nrelease/abstract pairs from existing expert annotated studies on exaggeration\nin press releases of scientific papers suitable for benchmarking the\nperformance of machine learning models on the task. Using limited data from\nthis and previous studies on exaggeration detection in science, we introduce\nMT-PET, a multi-task version of Pattern Exploiting Training (PET), which\nleverages knowledge from complementary cloze-style QA tasks to improve few-shot\nlearning. We demonstrate that MT-PET outperforms PET and supervised learning\nboth when data is limited, as well as when there is an abundance of data for\nthe main task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1\">Dustin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConVIScope: Visual Analytics for Exploring Patient Conversations. (arXiv:2108.13514v1 [cs.HC])","link":"http://arxiv.org/abs/2108.13514","description":"<p>The proliferation of text messaging for mobile health is generating a large\namount of patient-doctor conversations that can be extremely valuable to health\ncare professionals. We present ConVIScope, a visual text analytic system that\ntightly integrates interactive visualization with natural language processing\nin analyzing patient-doctor conversations. ConVIScope was developed in\ncollaboration with healthcare professionals following a user-centered iterative\ndesign. Case studies with six domain experts suggest the potential utility of\nConVIScope and reveal lessons for further developments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lester_R/0/1/0/all/0/1\">Richard Lester</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Chau_R/0/1/0/all/0/1\">Raymond Chau</a> (3) ((1) Department of Computer Science, University of British Columbia, (2) School of Information Technology, York University, (3) Department of Medicine, University of British Columbia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution. (arXiv:2108.13530v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13530","description":"<p>We consider the task of document-level entity linking (EL), where it is\nimportant to make consistent decisions for entity mentions over the full\ndocument jointly. We aim to leverage explicit \"connections\" among mentions\nwithin the document itself: we propose to join the EL task with that of\ncoreference resolution (coref). This is complementary to related works that\nexploit either (i) implicit document information (e.g., latent relations among\nentity mentions, or general language models) or (ii) connections between the\ncandidate links (e.g, as inferred from the external knowledge base).\nSpecifically, we cluster mentions that are linked via coreference, and enforce\na single EL for all of the clustered mentions together. The latter constraint\nhas the added benefit of increased coverage by joining EL candidate lists for\nthe thus clustered mentions. We formulate the coref+EL problem as a structured\nprediction task over directed trees and use a globally normalized model to\nsolve it. Experimental results on two datasets show a boost of up to +5%\nF1-score on both coref and EL tasks, compared to their standalone counterparts.\nFor a subset of hard cases, with individual mentions lacking the correct EL in\ntheir candidate entity list, we obtain a +50% increase in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaporojets_K/0/1/0/all/0/1\">Klim Zaporojets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Characterization of Divisive Topics Online: Case Studies on Contentiousness in Abortion, Climate Change, and Gun Control. (arXiv:2108.13556v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13556","description":"<p>As public discourse continues to move and grow online, conversations about\ndivisive topics on social media platforms have also increased. These divisive\ntopics prompt both contentious and non-contentious conversations. Although what\ndistinguishes these conversations, often framed as what makes these\nconversations contentious, is known in broad strokes, much less is known about\nthe linguistic signature of these conversations. Prior work has shown that\ncontentious content and structure can be a predictor for this task, however,\nmost of them have been focused on conversation in general, very specific\nevents, or complex structural analysis. Additionally, many models used in prior\nwork have lacked interpret-ability, a key factor in online moderation. Our work\nfills these gaps by focusing on conversations from highly divisive topics\n(abortion, climate change, and gun control), operationalizing a set of novel\nlinguistic and conversational characteristics and user factors, and\nincorporating them to build interpretable models. We demonstrate that such\ncharacteristics can largely improve the performance of prediction on this task,\nand also enable nuanced interpretability. Our case studies on these three\ncontentious topics suggest that certain generic linguistic characteristics are\nhighly correlated with contentiousness in conversations while others\ndemonstrate significant contextual influences on specific divisive topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beel_J/0/1/0/all/0/1\">Jacob Beel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_S/0/1/0/all/0/1\">Sandeep Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP. (arXiv:2108.13587v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13587","description":"<p>Transformers are the dominant architecture in NLP, but their training and\nfine-tuning is still very challenging. In this paper, we present the design and\nimplementation of a visual analytic framework for assisting researchers in such\nprocess, by providing them with valuable insights about the model's intrinsic\nproperties and behaviours. Our framework offers an intuitive overview that\nallows the user to explore different facets of the model (e.g., hidden states,\nattention) through interactive visualization, and allows a suite of built-in\nalgorithms that compute the importance of model components and different parts\nof the input sequence. Case studies and feedback from a user focus group\nindicate that the framework is useful, and suggest several improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyeju Jang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a> (1) ((1) University of British Columbia, (2) Huawei Cananda Technologies Co. Ltd.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13602","description":"<p>Adversarial training (AT) is one of the most reliable methods for defending\nagainst adversarial attacks in machine learning. Variants of this method have\nbeen used as regularization mechanisms to achieve SOTA results on NLP\nbenchmarks, and they have been found to be useful for transfer learning and\ncontinual learning. We search for the reasons for the effectiveness of AT by\ncontrasting vanilla and adversarially fine-tuned BERT models. We identify\npartial preservation of BERT's syntactic abilities during fine-tuning as the\nkey to the success of AT. We observe that adversarially fine-tuned models\nremain more faithful to BERT's language modeling behavior and are more\nsensitive to the word order. As concrete examples of syntactic abilities, an\nadversarially fine-tuned model could have an advantage of up to 38% on anaphora\nagreement and up to 11% on dependency parsing. Our analysis demonstrates that\nvanilla fine-tuning oversimplifies the sentence representation by focusing\nheavily on one or a few label-indicative words. AT, however, moderates the\neffect of these influential words and encourages representational diversity.\nThis allows for a more hierarchical representation of a sentence and leads to\nthe mitigation of BERT's loss of syntactic abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_J/0/1/0/all/0/1\">Javid Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Text Classification of Transliterated Hindi and Malayalam. (arXiv:2108.13620v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13620","description":"<p>Transliteration is very common on social media, but transliterated text is\nnot adequately handled by modern neural models for various NLP tasks. In this\nwork, we combine data augmentation approaches with a Teacher-Student training\nscheme to address this issue in a cross-lingual transfer setting for\nfine-tuning state-of-the-art pre-trained multilingual language models such as\nmBERT and XLM-R. We evaluate our method on transliterated Hindi and Malayalam,\nalso introducing new datasets for benchmarking on real-world scenarios: one on\nsentiment classification in transliterated Malayalam, and another on crisis\ntweet classification in transliterated Hindi and Malayalam (related to the 2013\nNorth India and 2018 Kerala floods). Our method yielded an average improvement\nof +5.6% on mBERT and +4.7% on XLM-R in F1 scores over their strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_J/0/1/0/all/0/1\">Jitin Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_H/0/1/0/all/0/1\">Hemant Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangwala_H/0/1/0/all/0/1\">Huzefa Rangwala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Sliding Window for Meeting Summarization. (arXiv:2108.13629v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13629","description":"<p>Recently abstractive spoken language summarization raises emerging research\ninterest, and neural sequence-to-sequence approaches have brought significant\nperformance improvement. However, summarizing long meeting transcripts remains\nchallenging. Due to the large length of source contents and targeted summaries,\nneural models are prone to be distracted on the context, and produce summaries\nwith degraded quality. Moreover, pre-trained language models with input length\nlimitations cannot be readily applied to long sequences. In this work, we first\nanalyze the linguistic characteristics of meeting transcripts on a\nrepresentative corpus, and find that the sentences comprising the summary\ncorrelate with the meeting agenda. Based on this observation, we propose a\ndynamic sliding window strategy for meeting summarization. Experimental results\nshow that performance benefit from the proposed method, and outputs obtain\nhigher factual consistency than the base model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory. (arXiv:2108.13630v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13630","description":"<p>Lip reading, aiming to recognize spoken sentences according to the given\nvideo of lip movements without relying on the audio stream, has attracted great\ninterest due to its application in many scenarios. Although prior works that\nexplore lip reading have obtained salient achievements, they are all trained in\na non-simultaneous manner where the predictions are generated requiring access\nto the full video. To breakthrough this constraint, we study the task of\nsimultaneous lip reading and devise SimulLR, a simultaneous lip Reading\ntransducer with attention-guided adaptive memory from three aspects: (1) To\naddress the challenge of monotonic alignments while considering the syntactic\nstructure of the generated sentences under simultaneous setting, we build a\ntransducer-based model and design several effective training strategies\nincluding CTC pre-training, model warm-up and curriculum learning to promote\nthe training of the lip reading transducer. (2) To learn better spatio-temporal\nrepresentations for simultaneous encoder, we construct a truncated 3D\nconvolution and time-restricted self-attention layer to perform the\nframe-to-frame interaction within a video segment containing fixed number of\nframes. (3) The history information is always limited due to the storage in\nreal-time scenarios, especially for massive video data. Therefore, we devise a\nnovel attention-guided adaptive memory to organize semantic information of\nhistory segments and enhance the visual representations with acceptable\ncomputation-aware latency. The experiments show that the SimulLR achieves the\ntranslation speedup 9.10$\\times$ compared with the state-of-the-art\nnon-simultaneous methods, and also obtains competitive results, which indicates\nthe effectiveness of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhijie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Classes through Word Attribution. (arXiv:2108.13653v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13653","description":"<p>In recent years, several methods have been proposed for explaining individual\npredictions of deep learning models, yet there has been little study of how to\naggregate these predictions to explain how such models view classes as a whole\nin text classification tasks. In this work, we propose a method for explaining\nclasses using deep learning models and the Integrated Gradients feature\nattribution technique by aggregating explanations of individual examples in\ntext classification to general descriptions of the classes. We demonstrate the\napproach on Web register (genre) classification using the XML-R model and the\nCorpus of Online Registers of English (CORE), finding that the method\nidentifies plausible and discriminative keywords characterizing all but the\nsmallest class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ronnqvist_S/0/1/0/all/0/1\">Samuel R&#xf6;nnqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myntti_A/0/1/0/all/0/1\">Amanda Myntti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrolainen_A/0/1/0/all/0/1\">Aki-Juhani Kyr&#xf6;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1\">Sampo Pyysalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laippala_V/0/1/0/all/0/1\">Veronika Laippala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discretized Integrated Gradients for Explaining Language Models. (arXiv:2108.13654v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13654","description":"<p>As a prominent attribution-based explanation algorithm, Integrated Gradients\n(IG) is widely adopted due to its desirable explanation axioms and the ease of\ngradient computation. It measures feature importance by averaging the model's\noutput gradient interpolated along a straight-line path in the input data\nspace. However, such straight-line interpolated points are not representative\nof text data due to the inherent discreteness of the word embedding space. This\nquestions the faithfulness of the gradients computed at the interpolated points\nand consequently, the quality of the generated explanations. Here we propose\nDiscretized Integrated Gradients (DIG), which allows effective attribution\nalong non-linear interpolation paths. We develop two interpolation strategies\nfor the discrete word embedding space that generates interpolation points that\nlie close to actual words in the embedding space, yielding more faithful\ngradient computation. We demonstrate the effectiveness of DIG over IG through\nexperimental and human evaluations on multiple sentiment classification\ndatasets. We provide the source code of DIG to encourage reproducible research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MELM: Data Augmentation with Masked Entity Language Modeling for Cross-lingual NER. (arXiv:2108.13655v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13655","description":"<p>Data augmentation for cross-lingual NER requires fine-grained control over\ntoken labels of the augmented text. Existing augmentation approach based on\nmasked language modeling may replace a labeled entity with words of a different\nclass, which makes the augmented sentence incompatible with the original label\nsequence, and thus hurts the performance.We propose a data augmentation\nframework with Masked-Entity Language Modeling (MELM) which effectively ensures\nthe replacing entities fit the original labels. Specifically, MELM linearizes\nNER labels into sentence context, and thus the fine-tuned MELM is able to\npredict masked tokens by explicitly conditioning on their labels. Our MELM is\nagnostic to the source of data to be augmented. Specifically, when MELM is\napplied to augment training data of the source language, it achieves up to 3.5%\nF1 score improvement for cross-lingual NER. When unlabeled target data is\navailable and MELM can be further applied to augment pseudo-labeled target\ndata, the performance gain reaches 5.7%. Moreover, MELM consistently\noutperforms multiple baseline methods for data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13658","description":"<p>The understanding of time expressions includes two sub-tasks: recognition and\nnormalization. In recent years, significant progress has been made in the\nrecognition of time expressions while research on normalization has lagged\nbehind. Existing SOTA normalization methods highly rely on rules or grammars\ndesigned by experts, which limits their performance on emerging corpora, such\nas social media texts. In this paper, we model time expression normalization as\na sequence of operations to construct the normalized temporal value, and we\npresent a novel method called ARTime, which can automatically generate\nnormalization rules from training data without expert interventions.\nSpecifically, ARTime automatically captures possible operation sequences from\nannotated data and generates normalization rules on time expressions with\ncommon surface forms. The experimental results show that ARTime can\nsignificantly surpass SOTA methods on the Tweets benchmark, and achieves\ncompetitive results with existing expert-engineered rule methods on the\nTempEval-3 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wentao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gray Cycles of Maximum Length Related to k-Character Substitutions. (arXiv:2108.13659v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13659","description":"<p>Given a word binary relation $\\tau$ we define a $\\tau$-Gray cycle over a\nfinite language $X$ to be a permutation $\\left(w_{[i]}\\right)_{0\\le i\\le\n|X|-1}$ of $X$ such that each word $w_i$ is an image of the previous word\n$w_{i-1}$ by $\\tau$. In that framework, we introduce the complexity measure\n$\\lambda(n)$, equal to the largest cardinality of a language $X$ having words\nof length at most $n$, and such that a $\\tau$-Gray cycle over $X$ exists. The\npresent paper is concerned with the relation $\\tau=\\sigma_k$, the so-called\n$k$-character substitution, where $(u,v)$ belongs to $\\sigma_k$ if, and only\nif, the Hamming distance of $u$ and $v$ is $k$. We compute the bound\n$\\lambda(n)$ for all cases of the alphabet cardinality and the argument $n$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neraud_J/0/1/0/all/0/1\">Jean N&#xe9;raud</a> (LITIS, UNIROUEN)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13679","description":"<p>In this paper, we propose to formulate the task-oriented dialogue system as\nthe purely natural language generation task, so as to fully leverage the\nlarge-scale pre-trained models like GPT-2 and simplify complicated\ndelexicalization prepossessing. However, directly applying this method heavily\nsuffers from the dialogue entity inconsistency caused by the removal of\ndelexicalized tokens, as well as the catastrophic forgetting problem of the\npre-trained model during fine-tuning, leading to unsatisfactory performance. To\nalleviate these problems, we design a novel GPT-Adapter-CopyNet network, which\nincorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve\nbetter performance on transfer learning and dialogue entity generation.\nExperimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ\ndataset demonstrate that our proposed approach significantly outperforms\nbaseline models with a remarkable performance on automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization. (arXiv:2108.13684v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13684","description":"<p>Despite recent progress in abstractive summarization, systems still suffer\nfrom faithfulness errors. While prior work has proposed models that improve\nfaithfulness, it is unclear whether the improvement comes from an increased\nlevel of extractiveness of the model outputs as one naive way to improve\nfaithfulness is to make summarization models more extractive. In this work, we\npresent a framework for evaluating the effective faithfulness of summarization\nsystems, by generating a faithfulnessabstractiveness trade-off curve that\nserves as a control at different operating points on the abstractiveness\nspectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as\nwell as a recently proposed method for improving faithfulness, are both worse\nthan the control at the same level of abstractiveness. Finally, we learn a\nselector to identify the most faithful and abstractive summary for a given\ndocument, and show that this system can attain higher faithfulness scores in\nhuman evaluations while being more abstractive than the baseline system on two\ndatasets. Moreover, we show that our system is able to achieve a better\nfaithfulness-abstractiveness trade-off than the control at the same level of\nabstractiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Grounded Dialogue with Reward-Driven Knowledge Selection. (arXiv:2108.13686v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13686","description":"<p>Knowledge-grounded dialogue is a task of generating a fluent and informative\nresponse based on both conversation context and a collection of external\nknowledge, in which knowledge selection plays an important role and attracts\nmore and more research interest. However, most existing models either select\nonly one knowledge or use all knowledge for responses generation. The former\nmay lose valuable information in discarded knowledge, while the latter may\nbring a lot of noise. At the same time, many approaches need to train the\nknowledge selector with knowledge labels that indicate ground-truth knowledge,\nbut these labels are difficult to obtain and require a large number of manual\nannotations. Motivated by these issues, we propose Knoformer, a dialogue\nresponse generation model based on reinforcement learning, which can\nautomatically select one or more related knowledge from the knowledge pool and\ndoes not need knowledge labels during training. Knoformer is evaluated on two\nknowledge-guided conversation datasets, and achieves state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TNNT: The Named Entity Recognition Toolkit. (arXiv:2108.13700v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13700","description":"<p>Extraction of categorised named entities from text is a complex task given\nthe availability of a variety of Named Entity Recognition (NER) models and the\nunstructured information encoded in different source document formats.\nProcessing the documents to extract text, identifying suitable NER models for a\ntask, and obtaining statistical information is important in data analysis to\nmake informed decisions. This paper presents TNNT, a toolkit that automates the\nextraction of categorised named entities from unstructured information encoded\nin source documents, using diverse state-of-the-art Natural Language Processing\n(NLP) tools and NER models. TNNT integrates 21 different NER models as part of\na Knowledge Graph Construction Pipeline (KGCP) that takes a document set as\ninput and processes it based on the defined settings, applying the selected\nblocks of NER models to output the results. The toolkit generates all results\nwith an integrated summary of the extracted entities, enabling enhanced data\nanalysis to support the KGCP, and also, to aid further NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1\">Sandaru Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_S/0/1/0/all/0/1\">Sergio J. Rodr&#xed;guez M&#xe9;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuecheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omran_P/0/1/0/all/0/1\">Pouya G. Omran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_K/0/1/0/all/0/1\">Kerry Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_A/0/1/0/all/0/1\">Armin Haller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plan-then-Generate: Controlled Data-to-Text Generation via Planning. (arXiv:2108.13740v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13740","description":"<p>Recent developments in neural networks have led to the advance in\ndata-to-text generation. However, the lack of ability of neural models to\ncontrol the structure of generated output can be limiting in certain real-world\napplications. In this study, we propose a novel Plan-then-Generate (PlanGen)\nframework to improve the controllability of neural data-to-text models.\nExtensive experiments and analyses are conducted on two benchmark datasets,\nToTTo and WebNLG. The results show that our model is able to control both the\nintra-sentence and inter-sentence structure of the generated output.\nFurthermore, empirical comparisons against previous state-of-the-art methods\nshow that our model improves the generation quality as well as the output\ndiversity as judged by human and automatic evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandyke_D/0/1/0/all/0/1\">David Vandyke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yimai Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13741","description":"<p>Recent researches have demonstrated that BERT shows potential in a wide range\nof natural language processing tasks. It is adopted as an encoder for many\nstate-of-the-art automatic summarizing systems, which achieve excellent\nperformance. However, so far, there is not much work done for Vietnamese. In\nthis paper, we showcase how BERT can be implemented for extractive text\nsummarization in Vietnamese. We introduce a novel comparison between different\nmultilingual and monolingual BERT models. The experiment results indicate that\nmonolingual models produce promising results compared to other multilingual\nmodels and previous text summarizing models for Vietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quoc_H/0/1/0/all/0/1\">Huy To Quoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Search Engine for Discovery of Biomedical Challenges and Directions. (arXiv:2108.13751v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13751","description":"<p>The ability to keep track of scientific challenges, advances and emerging\ndirections is a fundamental part of research. However, researchers face a flood\nof papers that hinders discovery of important knowledge. In biomedicine, this\ndirectly impacts human lives. To address this problem, we present a novel task\nof extraction and search of scientific challenges and directions, to facilitate\nrapid knowledge discovery. We construct and release an expert-annotated corpus\nof texts sampled from full-length papers, labeled with novel semantic\ncategories that generalize across many types of challenges and directions. We\nfocus on a large corpus of interdisciplinary work relating to the COVID-19\npandemic, ranging from biomedicine to areas such as AI and economics. We apply\na model trained on our data to identify challenges and directions across the\ncorpus and build a dedicated search engine for this information. In studies\nwith researchers, including those working directly on COVID-19, we outperform a\npopular scientific search engine in assisting knowledge discovery. Finally, we\nshow that models trained on our resource generalize to the wider biomedical\ndomain, highlighting its broad utility. We make our data, model and search\nengine publicly available. https://challenges.apps.allenai.org\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_J/0/1/0/all/0/1\">Jon Saad Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shomron_N/0/1/0/all/0/1\">Noam Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience. (arXiv:2108.13759v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13759","description":"<p>Pretrained transformer-based models such as BERT have demonstrated\nstate-of-the-art predictive performance when adapted into a range of natural\nlanguage processing tasks. An open problem is how to improve the faithfulness\nof explanations (rationales) for the predictions of these models. In this\npaper, we hypothesize that salient information extracted a priori from the\ntraining data can complement the task-specific information learned by the model\nduring fine-tuning on a downstream task. In this way, we aim to help BERT not\nto forget assigning importance to informative input tokens when making\npredictions by proposing SaLoss; an auxiliary loss function for guiding the\nmulti-head attention mechanism during training to be close to salient\ninformation extracted a priori using TextRank. Experiments for explanation\nfaithfulness across five datasets, show that models trained with SaLoss\nconsistently provide more faithful explanations across four different feature\nattribution methods compared to vanilla BERT. Using the rationales extracted\nfrom vanilla BERT and SaLoss models to train inherently faithful classifiers,\nwe further show that the latter result in higher predictive performance in\ndownstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1\">George Chrysostomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The five Is: Key principles for interpretable and safe conversational AI. (arXiv:2108.13766v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13766","description":"<p>In this position paper, we present five key principles, namely\ninterpretability, inherent capability to explain, independent data, interactive\nlearning, and inquisitiveness, for the development of conversational AI that,\nunlike the currently popular black box approaches, is transparent and\naccountable. At present, there is a growing concern with the use of black box\nstatistical language models: While displaying impressive average performance,\nsuch systems are also prone to occasional spectacular failures, for which there\nis no clear remedy. In an effort to initiate a discussion on possible\nalternatives, we outline and exemplify how our five principles enable the\ndevelopment of conversational AI systems that are transparent and thus safer\nfor use. We also present some of the challenges inherent in the implementation\nof those principles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahde_M/0/1/0/all/0/1\">Mattias Wahde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virgolin_M/0/1/0/all/0/1\">Marco Virgolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network psychometrics and cognitive network science open new ways for detecting, understanding and tackling the complexity of math anxiety: A review. (arXiv:2108.13800v1 [cs.SI])","link":"http://arxiv.org/abs/2108.13800","description":"<p>Math anxiety is a clinical pathology impairing cognitive processing in\nmath-related contexts. Originally thought to affect only inexperienced,\nlow-achieving students, recent investigations show how math anxiety is vastly\ndiffused even among high-performing learners. This review of data-informed\nstudies outlines math anxiety as a complex system that: (i) cripples\nwell-being, self-confidence and information processing on both conscious and\nsubconscious levels, (ii) can be transmitted by social interactions, like a\npathogen, and worsened by distorted perceptions, (iii) affects roughly 20% of\nstudents in 63 out of 64 worldwide educational systems but correlates weakly\nwith academic performance, and (iv) poses a concrete threat to students'\nwell-being, computational literacy and career prospects in science. These\npatterns underline the crucial need to go beyond performance for estimating\nmath anxiety. Recent advances with network psychometrics and cognitive network\nscience provide ideal frameworks for detecting, interpreting and intervening\nupon such clinical condition. Merging education research, psychology and data\nscience, the approaches reviewed here reconstruct psychological constructs as\ncomplex systems, represented either as multivariate correlation models (e.g.\ngraph exploratory analysis) or as cognitive networks of semantic/emotional\nassociations (e.g. free association networks or forma mentis networks). Not\nonly can these interconnected networks detect otherwise hidden levels of math\nanxiety but - more crucially - they can unveil the specific layout of\ninteracting factors, e.g. key sources and targets, behind math anxiety in a\ngiven cohort. As discussed here, these network approaches open concrete ways\nfor unveiling students' perceptions, emotions and mental well-being, and can\nenable future powerful data-informed interventions untangling math anxiety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TREND: Trigger-Enhanced Relation-Extraction Network for Dialogues. (arXiv:2108.13811v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13811","description":"<p>The goal of dialogue relation extraction (DRE) is to identify the relation\nbetween two entities in a given dialogue. During conversations, speakers may\nexpose their relations to certain entities by some clues, such evidences called\n\"triggers\". However, none of the existing work on DRE tried to detect triggers\nand leverage the information for enhancing the performance. This paper proposes\nTREND, a multi-tasking BERT-based model which learns to identify triggers for\nimproving relation extraction. The experimental results show that the proposed\nmethod achieves the state-of-the-art on the benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Po-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shang-Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Open-Domain Question Answering. (arXiv:2108.13817v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13817","description":"<p>Open-domain Question Answering (ODQA) has achieved significant results in\nterms of supervised learning manner. However, data annotation cannot also be\nirresistible for its huge demand in an open domain. Though unsupervised QA or\nunsupervised Machine Reading Comprehension (MRC) has been tried more or less,\nunsupervised ODQA has not been touched according to our best knowledge. This\npaper thus pioneers the work of unsupervised ODQA by formally introducing the\ntask and proposing a series of key data construction methods. Our exploration\nin this work inspiringly shows unsupervised ODQA can reach up to 86%\nperformance of supervised ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Domain Adaptation for Question Answering using Limited Text Corpora. (arXiv:2108.13854v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13854","description":"<p>Question generation has recently shown impressive results in customizing\nquestion answering (QA) systems to new domains. These approaches circumvent the\nneed for manually annotated training data from the new domain and, instead,\ngenerate synthetic question-answer pairs that are used for training. However,\nexisting methods for question generation rely on large amounts of synthetically\ngenerated datasets and costly computational resources, which render these\ntechniques widely inaccessible when the text corpora is of limited size. This\nis problematic as many niche domains rely on small text corpora, which\nnaturally restricts the amount of synthetic data that can be generated. In this\npaper, we propose a novel framework for domain adaptation called contrastive\ndomain adaptation for QA (CAQA). Specifically, CAQA combines techniques from\nquestion generation and domain-invariant learning to answer out-of-domain\nquestions in settings with limited text corpora. Here, we train a QA system on\nboth source data and generated data from the target domain with a contrastive\nadaptation loss that is incorporated in the training objective. By combining\ntechniques from question generation and domain-invariant learning, our model\nachieved considerable improvements compared to state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhenrui Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kratzwald_B/0/1/0/all/0/1\">Bernhard Kratzwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1\">Stefan Feuerriegel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs. (arXiv:2108.13873v1 [cs.CR])","link":"http://arxiv.org/abs/2108.13873","description":"<p>Machine-learning-as-a-service (MLaaS) has attracted millions of users to\ntheir outperforming sophisticated models. Although published as black-box APIs,\nthe valuable models behind these services are still vulnerable to imitation\nattacks. Recently, a series of works have demonstrated that attackers manage to\nsteal or extract the victim models. Nonetheless, none of the previous stolen\nmodels can outperform the original black-box APIs. In this work, we take the\nfirst step of showing that attackers could potentially surpass victims via\nunsupervised domain adaptation and multi-victim ensemble. Extensive experiments\non benchmark datasets and real-world APIs validate that the imitators can\nsucceed in outperforming the original black-box models. We consider this as a\nmilestone in the research of imitation attack, especially on NLP APIs, as the\nsuperior performance could influence the defense or even publishing strategy of\nAPI providers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13875","description":"<p>Scenario-based question answering (SQA) requires retrieving and reading\nparagraphs from a large corpus to answer a question which is contextualized by\na long scenario description. Since a scenario contains both keyphrases for\nretrieval and much noise, retrieval for SQA is extremely difficult. Moreover,\nit can hardly be supervised due to the lack of relevance labels of paragraphs\nfor SQA. To meet the challenge, in this paper we propose a joint\nretriever-reader model called JEEVES where the retriever is implicitly\nsupervised only using QA labels via a novel word weighting mechanism. JEEVES\nsignificantly outperforms a variety of strong baselines on multiple-choice\nquestions in three SQA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning. (arXiv:2108.13888v1 [cs.CR])","link":"http://arxiv.org/abs/2108.13888","description":"<p>\\textbf{P}re-\\textbf{T}rained \\textbf{M}odel\\textbf{s} have been widely\napplied and recently proved vulnerable under backdoor attacks: the released\npre-trained weights can be maliciously poisoned with certain triggers. When the\ntriggers are activated, even the fine-tuned model will predict pre-defined\nlabels, causing a security threat. These backdoors generated by the poisoning\nmethods can be erased by changing hyper-parameters during fine-tuning or\ndetected by finding the triggers. In this paper, we propose a stronger\nweight-poisoning attack method that introduces a layerwise weight poisoning\nstrategy to plant deeper backdoors; we also introduce a combinatorial trigger\nthat cannot be easily detected. The experiments on text classification tasks\nshow that previous defense methods cannot resist our weight-poisoning method,\nwhich indicates that our method can be widely applied and may provide hints for\nfuture model robustness studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Demin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Like Article, Like Audience: Enforcing Multimodal Correlations for Disinformation Detection. (arXiv:2108.13892v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13892","description":"<p>User-generated content (e.g., tweets and profile descriptions) and shared\ncontent between users (e.g., news articles) reflect a user's online identity.\nThis paper investigates whether correlations between user-generated and\nuser-shared content can be leveraged for detecting disinformation in online\nnews articles. We develop a multimodal learning algorithm for disinformation\ndetection. The latent representations of news articles and user-generated\ncontent allow that during training the model is guided by the profile of users\nwho prefer content similar to the news article that is evaluated, and this\neffect is reinforced if that content is shared among different users. By only\nleveraging user information during model optimization, the model does not rely\non user profiling when predicting an article's veracity. The algorithm is\nsuccessfully applied to three widely used neural classifiers, and results are\nobtained on different datasets. Visualization techniques show that the proposed\nmodel learns feature representations of unseen news articles that better\ndiscriminate between fake and real news texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrotta_D/0/1/0/all/0/1\">Domenico Perrotta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13897","description":"<p>The MS MARCO ranking dataset has been widely used for training deep learning\nmodels for IR tasks, achieving considerable effectiveness on diverse zero-shot\nscenarios. However, this type of resource is scarce in other languages than\nEnglish. In this work we present mMARCO, a multilingual version of the MS MARCO\npassage ranking dataset comprising 8 languages that was created using machine\ntranslation. We evaluated mMARCO by fine-tuning mono and multilingual\nre-ranking models on it. Experimental results demonstrate that multilingual\nmodels fine-tuned on our translated dataset achieve superior effectiveness than\nmodels fine-tuned on the original English version alone. Also, our distilled\nmultilingual re-ranker is competitive with non-distilled models while having\n5.4 times fewer parameters. The translated datasets as well as fine-tuned\nmodels are available at https://github.com/unicamp-dl/mMARCO.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Henrique Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campiotti_I/0/1/0/all/0/1\">Israel Campiotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The emojification of sentiment on social media: Collection and analysis of a longitudinal Twitter sentiment dataset. (arXiv:2108.13898v1 [cs.SI])","link":"http://arxiv.org/abs/2108.13898","description":"<p>Social media, as a means for computer-mediated communication, has been\nextensively used to study the sentiment expressed by users around events or\ntopics. There is however a gap in the longitudinal study of how sentiment\nevolved in social media over the years. To fill this gap, we develop TM-Senti,\na new large-scale, distantly supervised Twitter sentiment dataset with over 184\nmillion tweets and covering a time period of over seven years. We describe and\nassess our methodology to put together a large-scale, emoticon- and emoji-based\nlabelled sentiment analysis dataset, along with an analysis of the resulting\ndataset. Our analysis highlights interesting temporal changes, among others in\nthe increasing use of emojis over emoticons. We publicly release the dataset\nfor further research in tasks including sentiment analysis and text\nclassification of tweets. The dataset can be fully rehydrated including tweet\nmetadata and without missing tweets thanks to the archive of tweets publicly\navailable on the Internet Archive, which the dataset is based on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenjie Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Retrieval Augmented Generation for Zero-shot Slot Filling. (arXiv:2108.13934v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13934","description":"<p>Automatically inducing high quality knowledge graphs from a given collection\nof documents still remains a challenging problem in AI. One way to make headway\nfor this problem is through advancements in a related task known as slot\nfilling. In this task, given an entity query in form of [Entity, Slot, ?], a\nsystem is asked to fill the slot by generating or extracting the missing value\nexploiting evidence extracted from relevant passage(s) in the given document\ncollection. The recent works in the field try to solve this task in an\nend-to-end fashion using retrieval-based language models. In this paper, we\npresent a novel approach to zero-shot slot filling that extends dense passage\nretrieval with hard negatives and robust training procedures for retrieval\naugmented generation models. Our model reports large improvements on both T-REx\nand zsRE slot filling datasets, improving both passage retrieval and slot value\ngeneration, and ranking at the top-1 position in the KILT leaderboard.\nMoreover, we demonstrate the robustness of our system showing its domain\nadaptation capability on a new variant of the TACRED dataset for slot filling,\nthrough a combination of zero/few-shot learning. We release the source code and\npre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools. (arXiv:2108.13961v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13961","description":"<p>In the language domain, as in other domains, neural explainability takes an\never more important role, with feature attribution methods on the forefront.\nMany such methods require considerable computational resources and expert\nknowledge about implementation details and parameter choices. To facilitate\nresearch, we present Thermostat which consists of a large collection of model\nexplanations and accompanying analysis tools. Thermostat allows easy access to\nover 200k explanations for the decisions of prominent state-of-the-art models\nspanning across different NLP tasks, generated with multiple explainers. The\ndataset took over 10k GPU hours (&gt; one year) to compile; compute time that the\ncommunity now saves. The accompanying software tools allow to analyse\nexplanations instance-wise but also accumulatively on corpus level. Users can\ninvestigate and compare models, datasets and explainers without the need to\norchestrate implementation details. Thermostat is fully open source,\ndemocratizes explainability research in the language domain, circumvents\nredundant computations and increases comparability and replicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldhus_N/0/1/0/all/0/1\">Nils Feldhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzenberg_R/0/1/0/all/0/1\">Robert Schwarzenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13990","description":"<p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4 data sets with consistent observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdieh_M/0/1/0/all/0/1\">Mahdis Mahdieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Approach for Mitigating Structural Biases in Natural Language Inference. (arXiv:2108.14006v1 [cs.CL])","link":"http://arxiv.org/abs/2108.14006","description":"<p>Many natural language inference (NLI) datasets contain biases that allow\nmodels to perform well by only using a biased subset of the input, without\nconsidering the remainder features. For instance, models are able to make a\nclassification decision by only using the hypothesis, without learning the true\nrelationship between it and the premise. These structural biases lead\ndiscriminative models to learn unintended superficial features and to\ngeneralize poorly out of the training distribution. In this work, we\nreformulate the NLI task as a generative task, where a model is conditioned on\nthe biased subset of the input and the label and generates the remaining subset\nof the input. We show that by imposing a uniform prior, we obtain a provably\nunbiased model. Through synthetic experiments, we find that this approach is\nhighly robust to large amounts of bias. We then demonstrate empirically on two\ntypes of natural bias that this approach leads to fully unbiased models in\npractice. However, we find that generative models are difficult to train and\nthey generally perform worse than discriminative baselines. We highlight the\ndifficulty of the generative modeling task in the context of NLI as a cause for\nthis worse performance. Finally, by fine-tuning the generative model with a\ndiscriminative objective, we reduce the performance gap between the generative\nmodel and the discriminative baseline, while allowing for a small amount of\nbias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asael_D/0/1/0/all/0/1\">Dimion Asael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_Z/0/1/0/all/0/1\">Zachary Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUMBO: Bridging Response Generation and Facial Expression Synthesis. (arXiv:1905.11240v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1905.11240","description":"<p>Spoken dialogue systems that assist users to solve complex tasks such as\nmovie ticket booking have become an emerging research topic in artificial\nintelligence and natural language processing areas. With a well-designed\ndialogue system as an intelligent personal assistant, people can accomplish\ncertain tasks more easily via natural language interactions. Today there are\nseveral virtual intelligent assistants in the market; however, most systems\nonly focus on textual or vocal interaction. In this paper, we present HUMBO, a\nsystem aiming at generating dialogue responses and simultaneously synthesize\ncorresponding visual expressions on faces for better multimodal interaction.\nHUMBO can (1) let users determine the appearances of virtual assistants by a\nsingle image, and (2) generate coherent emotional utterances and facial\nexpressions on the user-provided image. This is not only a brand new research\ndirection but more importantly, an ultimate step toward more human-like virtual\nassistants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shang-Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Po-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer. (arXiv:2004.06698v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.06698","description":"<p>Visual dialog is a task of answering a sequence of questions grounded in an\nimage using the previous dialog history as context. In this paper, we study how\nto address two fundamental challenges for this task: (1) reasoning over\nunderlying semantic structures among dialog rounds and (2) identifying several\nappropriate answers to the given question. To address these challenges, we\npropose a Sparse Graph Learning (SGL) method to formulate visual dialog as a\ngraph structure learning task. SGL infers inherently sparse dialog structures\nby incorporating binary and score edges and leveraging a new structural loss\nfunction. Next, we introduce a Knowledge Transfer (KT) method that extracts the\nanswer predictions from the teacher model and uses them as pseudo labels. We\npropose KT to remedy the shortcomings of single ground-truth labels, which\nseverely limit the ability of a model to obtain multiple reasonable answers. As\na result, our proposed model significantly improves reasoning capability\ncompared to baseline methods and outperforms the state-of-the-art approaches on\nthe VisDial v1.0 dataset. The source code is available at\nhttps://github.com/gicheonkang/SGLKT-VisDial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Gi-Cheon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junseok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural CRF Model for Sentence Alignment in Text Simplification. (arXiv:2005.02324v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.02324","description":"<p>The success of a text simplification system heavily depends on the quality\nand quantity of complex-simple sentence pairs in the training corpus, which are\nextracted by aligning sentences between parallel articles. To evaluate and\nimprove sentence alignment quality, we create two manually annotated\nsentence-aligned datasets from two commonly used text simplification corpora,\nNewsela and Wikipedia. We propose a novel neural CRF alignment model which not\nonly leverages the sequential nature of sentences in parallel documents but\nalso utilizes a neural sentence pair model to capture semantic similarity.\nExperiments demonstrate that our proposed approach outperforms all the previous\nwork on monolingual sentence alignment task by more than 5 points in F1. We\napply our CRF aligner to construct two new text simplification datasets,\nNewsela-Auto and Wiki-Auto, which are much larger and of better quality\ncompared to the existing datasets. A Transformer-based seq2seq model trained on\nour datasets establishes a new state-of-the-art for text simplification in both\nautomatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Wuwei Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Response Selection with Hierarchical Curriculum Learning. (arXiv:2012.14756v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14756","description":"<p>We study the learning of a matching model for dialogue response selection.\nMotivated by the recent finding that models trained with random negative\nsamples are not ideal in real-world scenarios, we propose a hierarchical\ncurriculum learning framework that trains the matching model in an\n\"easy-to-difficult\" scheme. Our learning framework consists of two\ncomplementary curricula: (1) corpus-level curriculum (CC); and (2)\ninstance-level curriculum (IC). In CC, the model gradually increases its\nability in finding the matching clues between the dialogue context and a\nresponse candidate. As for IC, it progressively strengthens the model's ability\nin identifying the mismatching information between the dialogue context and a\nresponse candidate. Empirical studies on three benchmark datasets with three\nstate-of-the-art matching models demonstrate that the proposed learning\nframework significantly improves the model performance across various\nevaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zibo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_S/0/1/0/all/0/1\">Simon Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Transformer-Based Generation of Radiology Reports. (arXiv:2102.09777v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.09777","description":"<p>Inspired by Curriculum Learning, we propose a consecutive (i.e.,\nimage-to-text-to-text) generation framework where we divide the problem of\nradiology report generation into two steps. Contrary to generating the full\nradiology report from the image at once, the model generates global concepts\nfrom the image in the first step and then reforms them into finer and coherent\ntexts using a transformer architecture. We follow the transformer-based\nsequence-to-sequence paradigm at each step. We improve upon the\nstate-of-the-art on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nooralahzadeh_F/0/1/0/all/0/1\">Farhad Nooralahzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_N/0/1/0/all/0/1\">Nicolas Perez Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frauenfelder_T/0/1/0/all/0/1\">Thomas Frauenfelder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujimoto_K/0/1/0/all/0/1\">Koji Fujimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1\">Michael Krauthammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06979","description":"<p>Learning sentence embeddings often requires a large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong domain adaptation and\npre-training method for sentence embeddings, significantly outperforming other\napproaches like Masked Language Model.\n</p>\n<p>A crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08821","description":"<p>This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances the state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We hypothesize that dropout acts as minimal data augmentation and\nremoving it leads to a representation collapse. Then, we incorporate annotated\npairs from natural language inference datasets into our contrastive learning\nframework, by using \"entailment\" pairs as positives and \"contradiction\" pairs\nas hard negatives. We evaluate SimCSE on standard semantic textual similarity\n(STS) tasks, and our unsupervised and supervised models using BERT-base achieve\nan average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2 and\n2.2 points improvement compared to previous best results. We also show -- both\ntheoretically and empirically -- that contrastive learning objective\nregularizes pre-trained embeddings' anisotropic space to be more uniform, and\nit better aligns positive pairs when supervised signals are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction. (arXiv:2106.04847v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04847","description":"<p>Keyphrase Prediction (KP) task aims at predicting several keyphrases that can\nsummarize the main idea of the given document. Mainstream KP methods can be\ncategorized into purely generative approaches and integrated models with\nextraction and generation. However, these methods either ignore the diversity\namong keyphrases or only weakly capture the relation across tasks implicitly.\nIn this paper, we propose UniKeyphrase, a novel end-to-end learning framework\nthat jointly learns to extract and generate keyphrases. In UniKeyphrase,\nstacked relation layer and bag-of-words constraint are proposed to fully\nexploit the latent semantic relation between extraction and generation in the\nview of model structure and training process, respectively. Experiments on KP\nbenchmarks demonstrate that our joint approach outperforms mainstream methods\nby a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huanqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_D/0/1/0/all/0/1\">Dan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03158","description":"<p>Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing the generalization\ncapabilities of a model, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data over regularizing\nthe objective to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation (C1) and\na taxonomy for existing works (C2), this survey is concerned with data\naugmentation methods for textual classification and aims to achieve a concise\nand comprehensive overview for researchers and practitioners (C3). Derived from\nthe taxonomy, we divided more than 100 methods into 12 different groupings and\nprovide state-of-the-art references expounding which methods are highly\npromising (C4). Finally, research perspectives that may constitute a building\nblock for future work are given (C5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12079","description":"<p>Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation to perform reasoning\nand provide consistent and explainable answers. We illustrate the system using\na COVID-19 vaccine information case study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-Oriented Script Construction. (arXiv:2107.13189v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.13189","description":"<p>The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02854","description":"<p>Languages differ in terms of the absence or presence of gender features, the\nnumber of gender classes and whether and where gender features are explicitly\nmarked. These cross-linguistic differences can lead to ambiguities that are\ndifficult to resolve, especially for sentence-level MT systems. The\nidentification of ambiguity and its subsequent resolution is a challenging task\nfor which currently there aren't any specific resources or challenge sets\navailable. In this paper, we introduce gENder-IT, an English--Italian challenge\nset focusing on the resolution of natural gender phenomena by providing\nword-level gender tags on the English source side and multiple gender\nalternative translations, where needed, on the Italian target side.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monti_J/0/1/0/all/0/1\">Johanna Monti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05921","description":"<p>Detecting online hate is a complex task, and low-performing models have\nharmful consequences when used for sensitive applications such as content\nmoderation. Emoji-based hate is a key emerging challenge for automated\ndetection. We present HatemojiCheck, a test suite of 3,930 short-form\nstatements that allows us to evaluate performance on hateful language expressed\nwith emoji. Using the test suite, we expose weaknesses in existing hate\ndetection models. To address these weaknesses, we create the HatemojiTrain\ndataset using a human-and-model-in-the-loop approach. Models trained on these\n5,912 adversarial examples perform substantially better at detecting\nemoji-based hate, while retaining strong performance on text-only hate. Both\nHatemojiCheck and HatemojiTrain are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertram Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CushLEPOR: Customised hLEPOR Metric Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements. (arXiv:2108.09484v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09484","description":"<p>Human evaluation has always been expensive while researchers struggle to\ntrust the automatic metrics. To address this, we propose to customise\ntraditional metrics by taking advantages of the pre-trained language models\n(PLMs) and the limited available human labelled scores. We first re-introduce\nthe hLEPOR metric factors, followed by the Python portable version we developed\nwhich achieved the automatic tuning of the weighting parameters in hLEPOR\nmetric. Then we present the customised hLEPOR (cushLEPOR) which uses LABSE\ndistilled knowledge model to improve the metric agreement with human judgements\nby automatically optimised factor weights regarding the exact MT language pairs\nthat cushLEPOR is deployed to. We also optimise cushLEPOR towards human\nevaluation data based on MQM and pSQM framework on English-German and\nChinese-English language pairs. The experimental investigations show cushLEPOR\nboosts hLEPOR performances towards better agreements to PLMs like LABSE with\nmuch lower cost, and better agreements to human evaluations including MQM and\npSQM scores, and yields much better performances than BLEU (data available at\n\\url{https://github.com/poethan/cushLEPOR}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sarcasm Detection in Twitter -- Performance Impact while using Data Augmentation: Word Embeddings. (arXiv:2108.09924v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09924","description":"<p>Sarcasm is the use of words usually used to either mock or annoy someone, or\nfor humorous purposes. Sarcasm is largely used in social networks and\nmicroblogging websites, where people mock or censure in a way that makes it\ndifficult even for humans to tell if what is said is what is meant. Failure to\nidentify sarcastic utterances in Natural Language Processing applications such\nas sentiment analysis and opinion mining will confuse classification algorithms\nand generate false results. Several studies on sarcasm detection have utilized\ndifferent learning algorithms. However, most of these learning models have\nalways focused on the contents of expression only, leaving the contextual\ninformation in isolation. As a result, they failed to capture the contextual\ninformation in the sarcastic expression. Moreover, some datasets used in\nseveral studies have an unbalanced dataset which impacting the model result. In\nthis paper, we propose a contextual model for sarcasm identification in twitter\nusing RoBERTa, and augmenting the dataset by applying Global Vector\nrepresentation (GloVe) for the construction of word embedding and context\nlearning to generate more data and balancing the dataset. The effectiveness of\nthis technique is tested with various datasets and data augmentation settings.\nIn particular, we achieve performance gain by 3.2% in the iSarcasm dataset when\nusing data augmentation to increase 20% of data labeled as sarcastic, resulting\nF-score of 40.4% compared to 37.2% without data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Handoyo_A/0/1/0/all/0/1\">Alif Tri Handoyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidayaturrahman/0/1/0/all/0/1\">Hidayaturrahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhartono_D/0/1/0/all/0/1\">Derwin Suhartono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12189","description":"<p>This paper presents Macquarie University's participation to the BioASQ\nSynergy Task, and BioASQ9b Phase B. In each of these tasks, our participation\nfocused on the use of query-focused extractive summarisation to obtain the\nideal answers to medical questions. The Synergy Task is an end-to-end question\nanswering task on COVID-19 where systems are required to return relevant\ndocuments, snippets, and answers to a given question. Given the absence of\ntraining data, we used a query-focused summarisation system that was trained\nwith the BioASQ8b training data set and we experimented with methods to\nretrieve the documents and snippets. Considering the poor quality of the\ndocuments and snippets retrieved by our system, we observed reasonably good\nquality in the answers returned. For phase B of the BioASQ9b task, the relevant\ndocuments and snippets were already included in the test data. Our system split\nthe snippets into candidate sentences and used BERT variants under a sentence\nclassification setup. The system used the question and candidate sentence as\ninput and was trained to predict the likelihood of the candidate sentence being\npart of the ideal answer. The runs obtained either the best or second best\nROUGE-F1 results of all participants to all batches of BioASQ9b. This shows\nthat using BERT in a classification setup is a very strong baseline for the\nidentification of ideal answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molla_D/0/1/0/all/0/1\">Diego Moll&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_U/0/1/0/all/0/1\">Urvashi Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galat_D/0/1/0/all/0/1\">Dima Galat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vincent Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybinski_M/0/1/0/all/0/1\">Maciej Rybinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Table-to-Text Generation with Prototype Memory. (arXiv:2108.12516v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12516","description":"<p>Neural table-to-text generation models have achieved remarkable progress on\nan array of tasks. However, due to the data-hungry nature of neural models,\ntheir performances strongly rely on large-scale training examples, limiting\ntheir applicability in real-world applications. To address this, we propose a\nnew framework: Prototype-to-Generate (P2G), for table-to-text generation under\nthe few-shot scenario. The proposed framework utilizes the retrieved\nprototypes, which are jointly selected by an IR system and a novel prototype\nselector to help the model bridging the structural gap between tables and\ntexts. Experimental results on three benchmark datasets with three\nstate-of-the-art models demonstrate that the proposed framework significantly\nimproves the model performance across various evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_S/0/1/0/all/0/1\">Simon Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation. (arXiv:2108.12582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12582","description":"<p>Despite the remarkable performance of large-scale generative models in\nopen-domain conversation, they are known to be less practical for building\nreal-time conversation systems due to high latency. On the other hand,\nretrieval models could return responses with much lower latency but show\ninferior performance to the large-scale generative models since the\nconversation quality is bounded by the pre-defined response set. To take\nadvantage of both approaches, we propose a new training method called G2R\n(Generative-to-Retrieval distillation) that preserves the efficiency of a\nretrieval model while leveraging the conversational ability of a large-scale\ngenerative model by infusing the knowledge of the generative model into the\nretrieval model. G2R consists of two distinct techniques of distillation: the\ndata-level G2R augments the dialogue dataset with additional responses\ngenerated by the large-scale generative model, and the model-level G2R\ntransfers the response quality score assessed by the generative model to the\nscore of the retrieval model by the knowledge distillation loss. Through\nextensive experiments including human evaluation, we demonstrate that our\nretrieval-based conversation system trained with G2R shows a substantially\nimproved performance compared to the baseline retrieval model while showing\nsignificantly lower inference latency than the large-scale generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seokjun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdenee_E/0/1/0/all/0/1\">Enkhbayar Erdenee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scheduled Sampling Based on Decoding Steps for Neural Machine Translation. (arXiv:2108.12963v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12963","description":"<p>Scheduled sampling is widely used to mitigate the exposure bias problem for\nneural machine translation. Its core motivation is to simulate the inference\nscene during training by replacing ground-truth tokens with predicted tokens,\nthus bridging the gap between training and inference. However, vanilla\nscheduled sampling is merely based on training steps and equally treats all\ndecoding steps. Namely, it simulates an inference scene with uniform error\nrates, which disobeys the real inference scene, where larger decoding steps\nusually have higher error rates due to error accumulations. To alleviate the\nabove discrepancy, we propose scheduled sampling methods based on decoding\nsteps, increasing the selection chance of predicted tokens with the growth of\ndecoding steps. Consequently, we can more realistically simulate the inference\nscene during training, thus better bridging the gap between training and\ninference. Moreover, we investigate scheduled sampling based on both training\nsteps and decoding steps for further improvements. Experimentally, our\napproaches significantly outperform the Transformer baseline and vanilla\nscheduled sampling on three large-scale WMT tasks. Additionally, our approaches\nalso generalize well to the text summarization task on two popular benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2108.12971","description":"<p>A smart contract is a program executed on a blockchain, based on which many\ncryptocurrencies are implemented, and is being used for automating\ntransactions. Due to the large amount of money that smart contracts deal with,\nthere is a surging demand for a method that can statically and formally verify\nthem.\n</p>\n<p>This article describes our type-based static verification tool HELMHOLTZ for\nMichelson, which is a statically typed stack-based language for writing smart\ncontracts that are executed on the blockchain platform Tezos. HELMHOLTZ is\ndesigned on top of our extension of Michelson's type system with refinement\ntypes. HELMHOLTZ takes a Michelson program annotated with a user-defined\nspecification written in the form of a refinement type as input; it then\ntypechecks the program against the specification based on the refinement type\nsystem, discharging the generated verification conditions with the SMT solver\nZ3. We briefly introduce our refinement type system for the core calculus\nMini-Michelson of Michelson, which incorporates the characteristic features\nsuch as compound datatypes (e.g., lists and pairs), higher-order functions, and\ninvocation of another contract. \\HELMHOLTZ{} successfully verifies several\npractical Michelson programs, including one that transfers money to an account\nand that checks a digital signature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_Y/0/1/0/all/0/1\">Yuki Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiromasa Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawata_A/0/1/0/all/0/1\">Akira Kawata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuse_J/0/1/0/all/0/1\">Jun Furuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_A/0/1/0/all/0/1\">Atsushi Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Application of Convolutional Neural Networks for Tomographic Reconstruction of Hyperspectral Images. (arXiv:2108.13458v1 [eess.IV])","link":"http://arxiv.org/abs/2108.13458","description":"<p>A novel method, utilizing convolutional neural networks (CNNs), is proposed\nto reconstruct hyperspectral cubes from computed tomography imaging\nspectrometer (CTIS) images. Current reconstruction algorithms are usually\nsubject to long reconstruction times and mediocre precision in cases of a large\nnumber of spectral channels. The constructed CNNs deliver higher precision and\nshorter reconstruction time than a standard expectation maximization algorithm.\nIn addition, the network can handle two different types of real-world images at\nthe same time -- specifically ColorChecker and carrot spectral images are\nconsidered. This work paves the way toward real-time reconstruction of\nhyperspectral cubes from CTIS images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wei-Chih Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peters_M/0/1/0/all/0/1\">Mads Svanborg Peters</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlebaek_M/0/1/0/all/0/1\">Mads Juul Ahlebaek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frandsen_M/0/1/0/all/0/1\">Mads Toudal Frandsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eriksen_R/0/1/0/all/0/1\">Ren&#xe9; Lynge Eriksen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorgensen_B/0/1/0/all/0/1\">Bjarke J&#xf8;rgensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies. (arXiv:2108.13459v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13459","description":"<p>Generative models for 3D shapes represented by hierarchies of parts can\ngenerate realistic and diverse sets of outputs. However, existing models suffer\nfrom the key practical limitation of modelling shapes holistically and thus\ncannot perform conditional sampling, i.e. they are not able to generate\nvariants on individual parts of generated shapes without modifying the rest of\nthe shape. This is limiting for applications such as 3D CAD design that involve\nadjusting created shapes at multiple levels of detail. To address this, we\nintroduce LSD-StructureNet, an augmentation to the StructureNet architecture\nthat enables re-generation of parts situated at arbitrary positions in the\nhierarchies of its outputs. We achieve this by learning individual,\nprobabilistic conditional decoders for each hierarchy depth. We evaluate\nLSD-StructureNet on the PartNet dataset, the largest dataset of 3D shapes\nrepresented by hierarchies of parts. Our results show that contrarily to\nexisting methods, LSD-StructureNet can perform conditional sampling without\nimpacting inference speed or the realism and diversity of its outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1\">Dominic Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danielyan_A/0/1/0/all/0/1\">Ara Danielyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golparvar_Fard_M/0/1/0/all/0/1\">Mani Golparvar-Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-Cycle Energy Consumption Benchmark for Low-Carbon Computer Vision. (arXiv:2108.13465v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13465","description":"<p>The energy consumption of deep learning models is increasing at a\nbreathtaking rate, which raises concerns due to potential negative effects on\ncarbon neutrality in the context of global warming and climate change. With the\nprogress of efficient deep learning techniques, e.g., model compression,\nresearchers can obtain efficient models with fewer parameters and smaller\nlatency. However, most of the existing efficient deep learning methods do not\nexplicitly consider energy consumption as a key performance indicator.\nFurthermore, existing methods mostly focus on the inference costs of the\nresulting efficient models, but neglect the notable energy consumption\nthroughout the entire life cycle of the algorithm. In this paper, we present\nthe first large-scale energy consumption benchmark for efficient computer\nvision models, where a new metric is proposed to explicitly evaluate the\nfull-cycle energy consumption under different model usage intensity. The\nbenchmark can provide insights for low carbon emission when selecting efficient\ndeep learning algorithms in different model usage scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_D/0/1/0/all/0/1\">Donglin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Ningxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Synthesis via Uncertainty-Driven Attribute Synchronization. (arXiv:2108.13499v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13499","description":"<p>Developing deep neural networks to generate 3D scenes is a fundamental\nproblem in neural synthesis with immediate applications in architectural CAD,\ncomputer graphics, as well as in generating virtual robot training\nenvironments. This task is challenging because 3D scenes exhibit diverse\npatterns, ranging from continuous ones, such as object sizes and the relative\nposes between pairs of shapes, to discrete patterns, such as occurrence and\nco-occurrence of objects with symmetrical relationships. This paper introduces\na novel neural scene synthesis approach that can capture diverse feature\npatterns of 3D scenes. Our method combines the strength of both neural\nnetwork-based and conventional scene synthesis approaches. We use the\nparametric prior distributions learned from training data, which provide\nuncertainties of object attributes and relative attributes, to regularize the\noutputs of feed-forward neural models. Moreover, instead of merely predicting a\nscene layout, our approach predicts an over-complete set of attributes. This\nmethodology allows us to utilize the underlying consistency constraints among\nthe predicted attributes to prune infeasible predictions. Experimental results\nshow that our approach outperforms existing methods considerably. The generated\n3D scenes interpolate the training data faithfully while preserving both\ncontinuous and discrete feature patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zaiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Siming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dead Pixel Test Using Effective Receptive Field. (arXiv:2108.13576v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13576","description":"<p>Deep neural networks have been used in various fields, but their internal\nbehavior is not well known. In this study, we discuss two counterintuitive\nbehaviors of convolutional neural networks (CNNs). First, we evaluated the size\nof the receptive field. Previous studies have attempted to increase or control\nthe size of the receptive field. However, we observed that the size of the\nreceptive field does not describe the classification accuracy. The size of the\nreceptive field would be inappropriate for representing superiority in\nperformance because it reflects only depth or kernel size and does not reflect\nother factors such as width or cardinality. Second, using the effective\nreceptive field, we examined the pixels contributing to the output.\nIntuitively, each pixel is expected to equally contribute to the final output.\nHowever, we found that there exist pixels in a partially dead state with little\ncontribution to the output. We reveal that the reason for this lies in the\narchitecture of CNN and discuss solutions to reduce the phenomenon.\nInterestingly, for general classification tasks, the existence of dead pixels\nimproves the training of CNNs. However, in a task that captures small\nperturbation, dead pixels degrade the performance. Therefore, the existence of\nthese dead pixels should be understood and considered in practical applications\nof CNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Bum Jun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyeyeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyeonah Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong Gu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1\">Wonseok Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sang Woo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Splitting and Aggregation Network for Hyperspectral Face Super-Resolution. (arXiv:2108.13584v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13584","description":"<p>High-resolution (HR) hyperspectral face image plays an important role in face\nrelated computer vision tasks under uncontrolled conditions, such as low-light\nenvironment and spoofing attacks. However, the dense spectral bands of\nhyperspectral face images come at the cost of limited amount of photons reached\na narrow spectral window on average, which greatly reduces the spatial\nresolution of hyperspectral face images. In this paper, we investigate how to\nadapt the deep learning techniques to hyperspectral face image super-resolution\n(HFSR), especially when the training samples are very limited. Benefiting from\nthe amount of spectral bands, in which each band can be seen as an image, we\npresent a spectral splitting and aggregation network (SSANet) for HFSR with\nlimited training samples. In the shallow layers, we split the hyperspectral\nimage into different spectral groups and take each of them as an individual\ntraining sample (in the sense that each group will be fed into the same\nnetwork). Then, we gradually aggregate the neighbor bands at the deeper layers\nto exploit the spectral correlations. By this spectral splitting and\naggregation strategy (SSAS), we can divide the original hyperspectral image\ninto multiple samples to support the efficient training of the network and\neffectively exploit the spectral correlations among spectrum. To cope with the\nchallenge of small training sample size (S3) problem, we propose to expand the\ntraining samples by a self-representation model and symmetry-induced\naugmentation. Experiments show that the introduced SSANet can well model the\njoint correlations of spatial and spectral information. By expanding the\ntraining samples, our proposed method can effectively alleviate the S3 problem.\nThe comparison results demonstrate that our proposed method can outperform the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMAC-Seg: LiDAR Panoptic Segmentation via Sparse Multi-directional Attention Clustering. (arXiv:2108.13588v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13588","description":"<p>Panoptic segmentation aims to address semantic and instance segmentation\nsimultaneously in a unified framework. However, an efficient solution of\npanoptic segmentation in applications like autonomous driving is still an open\nresearch problem. In this work, we propose a novel LiDAR-based panoptic system,\ncalled SMAC-Seg. We present a learnable sparse multi-directional attention\nclustering to segment multi-scale foreground instances. SMAC-Seg is a real-time\nclustering-based approach, which removes the complex proposal network to\nsegment instances. Most existing clustering-based methods use the difference of\nthe predicted and ground truth center offset as the only loss to supervise the\ninstance centroid regression. However, this loss function only considers the\ncentroid of the current object, but its relative position with respect to the\nneighbouring objects is not considered when learning to cluster. Thus, we\npropose to use a novel centroid-aware repel loss as an additional term to\neffectively supervise the network to differentiate each object cluster with its\nneighbours. Our experimental results show that SMAC-Seg achieves\nstate-of-the-art performance among all real-time deployable networks on both\nlarge-scale public SemanticKITTI and nuScenes panoptic segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Enxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1\">Ryan Razani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingbing_L/0/1/0/all/0/1\">Liu Bingbing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIP: Adversarial Iterative Pruning Based on Knowledge Transfer for Convolutional Neural Networks. (arXiv:2108.13591v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13591","description":"<p>With the increase of structure complexity, convolutional neural networks\n(CNNs) take a fair amount of computation cost. Meanwhile, existing research\nreveals the salient parameter redundancy in CNNs. The current pruning methods\ncan compress CNNs with little performance drop, but when the pruning ratio\nincreases, the accuracy loss is more serious. Moreover, some iterative pruning\nmethods are difficult to accurately identify and delete unimportant parameters\ndue to the accuracy drop during pruning. We propose a novel adversarial\niterative pruning method (AIP) for CNNs based on knowledge transfer. The\noriginal network is regarded as the teacher while the compressed network is the\nstudent. We apply attention maps and output features to transfer information\nfrom the teacher to the student. Then, a shallow fully-connected network is\ndesigned as the discriminator to allow the output of two networks to play an\nadversarial game, thereby it can quickly recover the pruned accuracy among\npruning intervals. Finally, an iterative pruning scheme based on the importance\nof channels is proposed. We conduct extensive experiments on the image\nclassification tasks CIFAR-10, CIFAR-100, and ILSVRC-2012 to verify our pruning\nmethod can achieve efficient compression for CNNs even without accuracy loss.\nOn the ILSVRC-2012, when removing 36.78% parameters and 45.55% floating-point\noperations (FLOPs) of ResNet-18, the Top-1 accuracy drop are only 0.66%. Our\nmethod is superior to some state-of-the-art pruning schemes in terms of\ncompressing rate and accuracy. Moreover, we further demonstrate that AIP has\ngood generalization on the object detection task PASCAL VOC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jingfei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_P/0/1/0/all/0/1\">Ping Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiqun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhen Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-balanced Learning For Domain Generalization. (arXiv:2108.13597v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13597","description":"<p>Domain generalization aims to learn a prediction model on multi-domain source\ndata such that the model can generalize to a target domain with unknown\nstatistics. Most existing approaches have been developed under the assumption\nthat the source data is well-balanced in terms of both domain and class.\nHowever, real-world training data collected with different composition biases\noften exhibits severe distribution gaps for domain and class, leading to\nsubstantial performance degradation. In this paper, we propose a self-balanced\ndomain generalization framework that adaptively learns the weights of losses to\nalleviate the bias caused by different distributions of the multi-domain source\ndata. The self-balanced scheme is based on an auxiliary reweighting network\nthat iteratively updates the weight of loss conditioned on the domain and class\ninformation by leveraging balanced meta data. Experimental results demonstrate\nthe effectiveness of our method overwhelming state-of-the-art works for domain\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dongbo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Filter Adaptive Network for Single Image Defocus Deblurring. (arXiv:2108.13610v1 [eess.IV])","link":"http://arxiv.org/abs/2108.13610","description":"<p>We propose a novel end-to-end learning-based approach for single image\ndefocus deblurring. The proposed approach is equipped with a novel Iterative\nFilter Adaptive Network (IFAN) that is specifically designed to handle\nspatially-varying and large defocus blur. For adaptively handling\nspatially-varying blur, IFAN predicts pixel-wise deblurring filters, which are\napplied to defocused features of an input image to generate deblurred features.\nFor effectively managing large blur, IFAN models deblurring filters as stacks\nof small-sized separable filters. Predicted separable deblurring filters are\napplied to defocused features using a novel Iterative Adaptive Convolution\n(IAC) layer. We also propose a training scheme based on defocus disparity\nestimation and reblurring, which significantly boosts the deblurring quality.\nWe demonstrate that our method achieves state-of-the-art performance both\nquantitatively and qualitatively on real-world images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_H/0/1/0/all/0/1\">Hyeongseok Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rim_J/0/1/0/all/0/1\">Jaesung Rim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation Fault: A Cheap Defense Against Adversarial Machine Learning. (arXiv:2108.13617v1 [cs.CR])","link":"http://arxiv.org/abs/2108.13617","description":"<p>Recently published attacks against deep neural networks (DNNs) have stressed\nthe importance of methodologies and tools to assess the security risks of using\nthis technology in critical systems. Efficient techniques for detecting\nadversarial machine learning helps establishing trust and boost the adoption of\ndeep learning in sensitive and security systems. In this paper, we propose a\nnew technique for defending deep neural network classifiers, and convolutional\nones in particular. Our defense is cheap in the sense that it requires less\ncomputation power despite a small cost to pay in terms of detection accuracy.\nThe work refers to a recently published technique called ML-LOO. We replace the\ncostly pixel by pixel leave-one-out approach of ML-LOO by adopting\ncoarse-grained leave-one-out. We evaluate and compare the efficiency of\ndifferent segmentation algorithms for this task. Our results show that a large\ngain in efficiency is possible, even though penalized by a marginal decrease in\ndetection accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bared_D/0/1/0/all/0/1\">Doha Al Bared</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1\">Mohamed Nassar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spike time displacement based error backpropagation in convolutional spiking neural networks. (arXiv:2108.13621v1 [cs.NE])","link":"http://arxiv.org/abs/2108.13621","description":"<p>We recently proposed the STiDi-BP algorithm, which avoids backward recursive\ngradient computation, for training multi-layer spiking neural networks (SNNs)\nwith single-spike-based temporal coding. The algorithm employs a linear\napproximation to compute the derivative of the spike latency with respect to\nthe membrane potential and it uses spiking neurons with piecewise linear\npostsynaptic potential to reduce the computational cost and the complexity of\nneural processing. In this paper, we extend the STiDi-BP algorithm to employ it\nin deeper and convolutional architectures. The evaluation results on the image\nclassification task based on two popular benchmarks, MNIST and Fashion-MNIST\ndatasets with the accuracies of respectively 99.2% and 92.8%, confirm that this\nalgorithm has been applicable in deep SNNs. Another issue we consider is the\nreduction of memory storage and computational cost. To do so, we consider a\nconvolutional SNN (CSNN) with two sets of weights: real-valued weights that are\nupdated in the backward pass and their signs, binary weights, that are employed\nin the feedforward process. We evaluate the binary CSNN on two datasets of\nMNIST and Fashion-MNIST and obtain acceptable performance with a negligible\naccuracy drop with respect to real-valued weights (about $0.6%$ and $0.8%$\ndrops, respectively).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirsadeghi_M/0/1/0/all/0/1\">Maryam Mirsadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalchian_M/0/1/0/all/0/1\">Majid Shalchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1\">Saeed Reza Kheradpisheh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1\">Timoth&#xe9;e Masquelier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory. (arXiv:2108.13630v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13630","description":"<p>Lip reading, aiming to recognize spoken sentences according to the given\nvideo of lip movements without relying on the audio stream, has attracted great\ninterest due to its application in many scenarios. Although prior works that\nexplore lip reading have obtained salient achievements, they are all trained in\na non-simultaneous manner where the predictions are generated requiring access\nto the full video. To breakthrough this constraint, we study the task of\nsimultaneous lip reading and devise SimulLR, a simultaneous lip Reading\ntransducer with attention-guided adaptive memory from three aspects: (1) To\naddress the challenge of monotonic alignments while considering the syntactic\nstructure of the generated sentences under simultaneous setting, we build a\ntransducer-based model and design several effective training strategies\nincluding CTC pre-training, model warm-up and curriculum learning to promote\nthe training of the lip reading transducer. (2) To learn better spatio-temporal\nrepresentations for simultaneous encoder, we construct a truncated 3D\nconvolution and time-restricted self-attention layer to perform the\nframe-to-frame interaction within a video segment containing fixed number of\nframes. (3) The history information is always limited due to the storage in\nreal-time scenarios, especially for massive video data. Therefore, we devise a\nnovel attention-guided adaptive memory to organize semantic information of\nhistory segments and enhance the visual representations with acceptable\ncomputation-aware latency. The experiments show that the SimulLR achieves the\ntranslation speedup 9.10$\\times$ compared with the state-of-the-art\nnon-simultaneous methods, and also obtains competitive results, which indicates\nthe effectiveness of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhijie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Module-Power Prediction from PL Measurements using Deep Learning. (arXiv:2108.13640v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13640","description":"<p>The individual causes for power loss of photovoltaic modules are investigated\nfor quite some time. Recently, it has been shown that the power loss of a\nmodule is, for example, related to the fraction of inactive areas. While these\nareas can be easily identified from electroluminescense (EL) images, this is\nmuch harder for photoluminescence (PL) images. With this work, we close the gap\nbetween power regression from EL and PL images. We apply a deep convolutional\nneural network to predict the module power from PL images with a mean absolute\nerror (MAE) of 4.4% or 11.7WP. Furthermore, we depict that regression maps\ncomputed from the embeddings of the trained network can be used to compute the\nlocalized power loss. Finally, we show that these regression maps can be used\nto identify inactive regions in PL images as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1\">Mathis Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hepp_J/0/1/0/all/0/1\">Johannes Hepp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doll_B/0/1/0/all/0/1\">Bernd Doll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buerhop_Lutz_C/0/1/0/all/0/1\">Claudia Buerhop-Lutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_I/0/1/0/all/0/1\">Ian Marius Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabec_C/0/1/0/all/0/1\">Christoph Brabec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1\">Vincent Christlein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is First Person Vision Challenging for Object Tracking?. (arXiv:2108.13665v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13665","description":"<p>Understanding human-object interactions is fundamental in First Person Vision\n(FPV). Tracking algorithms which follow the objects manipulated by the camera\nwearer can provide useful cues to effectively model such interactions. Visual\ntracking solutions available in the computer vision literature have\nsignificantly improved their performance in the last years for a large variety\nof target objects and tracking scenarios. However, despite a few previous\nattempts to exploit trackers in FPV applications, a methodical analysis of the\nperformance of state-of-the-art trackers in this domain is still missing. In\nthis paper, we fill the gap by presenting the first systematic study of object\ntracking in FPV. Our study extensively analyses the performance of recent\nvisual trackers and baseline FPV trackers with respect to different aspects and\nconsidering a new performance measure. This is achieved through TREK-150, a\nnovel benchmark dataset composed of 150 densely annotated video sequences. Our\nresults show that object tracking in FPV is challenging, which suggests that\nmore research efforts should be devoted to this problem so that tracking could\nbenefit FPV tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunnhofer_M/0/1/0/all/0/1\">Matteo Dunnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheloni_C/0/1/0/all/0/1\">Christian Micheloni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Image Classification with Grad-CAM Consistency. (arXiv:2108.13673v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13673","description":"<p>Consistency training, which exploits both supervised and unsupervised\nlearning with different augmentations on image, is an effective method of\nutilizing unlabeled data in semi-supervised learning (SSL) manner. Here, we\npresent another version of the method with Grad-CAM consistency loss, so it can\nbe utilized in training model with better generalization and adjustability. We\nshow that our method improved the baseline ResNet model with at most 1.44 % and\n0.31 $\\pm$ 0.59 %p accuracy improvement on average with CIFAR-10 dataset. We\nconducted ablation study comparing to using only psuedo-label for consistency\ntraining. Also, we argue that our method can adjust in different environments\nwhen targeted to different units in the model. The code is available:\nhttps://github.com/gimme1dollar/gradcam-consistency-semi-sup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seunghyuk Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Multi-Reference Learning for Image Super-Resolution. (arXiv:2108.13697v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13697","description":"<p>This paper proposes a novel Attention-based Multi-Reference Super-resolution\nnetwork (AMRSR) that, given a low-resolution image, learns to adaptively\ntransfer the most similar texture from multiple reference images to the\nsuper-resolution output whilst maintaining spatial coherence. The use of\nmultiple reference images together with attention-based sampling is\ndemonstrated to achieve significantly improved performance over\nstate-of-the-art reference super-resolution approaches on multiple benchmark\ndatasets. Reference super-resolution approaches have recently been proposed to\novercome the ill-posed problem of image super-resolution by providing\nadditional information from a high-resolution reference image. Multi-reference\nsuper-resolution extends this approach by providing a more diverse pool of\nimage features to overcome the inherent information deficit whilst maintaining\nmemory efficiency. A novel hierarchical attention-based sampling approach is\nintroduced to learn the similarity between low-resolution image features and\nmultiple reference images based on a perceptual loss. Ablation demonstrates the\ncontribution of both multi-reference and hierarchical attention-based sampling\nto overall performance. Perceptual and quantitative ground-truth evaluation\ndemonstrates significant improvement in performance even when the reference\nimages deviate significantly from the target image. The project website can be\nfound at https://marcopesavento.github.io/AMRSR/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pesavento_M/0/1/0/all/0/1\">Marco Pesavento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volino_M/0/1/0/all/0/1\">Marco Volino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_A/0/1/0/all/0/1\">Adrian Hilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Monocular Vanishing Point Detection Exploiting Lane Annotations. (arXiv:2108.13699v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13699","description":"<p>Vanishing points (VPs) play a vital role in various computer vision tasks,\nespecially for recognizing the 3D scenes from an image. In the real-world\nscenario of automobile applications, it is costly to manually obtain the\nexternal camera parameters when the camera is attached to the vehicle or the\nattachment is accidentally perturbed. In this paper we introduce a simple but\neffective end-to-end vanishing point detection. By automatically calculating\nintersection of the extrapolated lane marker annotations, we obtain\ngeometrically consistent VP labels and mitigate human annotation errors caused\nby manual VP labeling. With the calculated VP labels we train end-to-end VP\nDetector via heatmap estimation. The VP Detector realizes higher accuracy than\nthe methods utilizing manual annotation or lane detection, paving the way for\naccurate online camera calibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honda_H/0/1/0/all/0/1\">Hiroto Honda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_M/0/1/0/all/0/1\">Motoki Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karasawa_T/0/1/0/all/0/1\">Takumi Karasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_Y/0/1/0/all/0/1\">Yusuke Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemIE: Semantically-aware Image Extrapolation. (arXiv:2108.13702v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13702","description":"<p>We propose a semantically-aware novel paradigm to perform image extrapolation\nthat enables the addition of new object instances. All previous methods are\nlimited in their capability of extrapolation to merely extending the already\nexisting objects in the image. However, our proposed approach focuses not only\non (i) extending the already present objects but also on (ii) adding new\nobjects in the extended region based on the context. To this end, for a given\nimage, we first obtain an object segmentation map using a state-of-the-art\nsemantic segmentation method. The, thus, obtained segmentation map is fed into\na network to compute the extrapolated semantic segmentation and the\ncorresponding panoptic segmentation maps. The input image and the obtained\nsegmentation maps are further utilized to generate the final extrapolated\nimage. We conduct experiments on Cityscapes and ADE20K-bedroom datasets and\nshow that our method outperforms all baselines in terms of FID, and similarity\nin object co-occurrence statistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_B/0/1/0/all/0/1\">Bholeshwar Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Soumya Ranjan Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Abhishek Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1\">Aniruddha Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Hrituraj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1\">Kuldeep Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning with Compensation: Efficient Channel Pruning for Deep Convolutional Neural Networks. (arXiv:2108.13728v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13728","description":"<p>Channel pruning is a promising technique to compress the parameters of deep\nconvolutional neural networks(DCNN) and to speed up the inference. This paper\naims to address the long-standing inefficiency of channel pruning. Most channel\npruning methods recover the prediction accuracy by re-training the pruned model\nfrom the remaining parameters or random initialization. This re-training\nprocess is heavily dependent on the sufficiency of computational resources,\ntraining data, and human interference(tuning the training strategy). In this\npaper, a highly efficient pruning method is proposed to significantly reduce\nthe cost of pruning DCNN. The main contributions of our method include: 1)\npruning compensation, a fast and data-efficient substitute of re-training to\nminimize the post-pruning reconstruction loss of features, 2)\ncompensation-aware pruning(CaP), a novel pruning algorithm to remove redundant\nor less-weighted channels by minimizing the loss of information, and 3) binary\nstructural search with step constraint to minimize human interference. On\nbenchmarks including CIFAR-10/100 and ImageNet, our method shows competitive\npruning performance among the state-of-the-art retraining-based pruning methods\nand, more importantly, reduces the processing time by 95% and data usage by\n90%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhouyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengzhao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junlin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Duanbing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning on Edge TPUs. (arXiv:2108.13732v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13732","description":"<p>Computing at the edge is important in remote settings, however, conventional\nhardware is not optimized for utilizing deep neural networks. The Google Edge\nTPU is an emerging hardware accelerator that is cost, power and speed\nefficient, and is available for prototyping and production purposes. Here, I\nreview the Edge TPU platform, the tasks that have been accomplished using the\nEdge TPU, and which steps are necessary to deploy a model to the Edge TPU\nhardware. The Edge TPU is not only capable of tackling common computer vision\ntasks, but also surpasses other hardware accelerators, especially when the\nentire model can be deployed to the Edge TPU. Co-embedding the Edge TPU in\ncameras allows a seamless analysis of primary data. In summary, the Edge TPU is\na maturing system that has proven its usability across multiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kist_A/0/1/0/all/0/1\">Andreas M Kist</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-Resolution Appearance Transfer for 4D Human Performances. (arXiv:2108.13739v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13739","description":"<p>A common problem in the 4D reconstruction of people from multi-view video is\nthe quality of the captured dynamic texture appearance which depends on both\nthe camera resolution and capture volume. Typically the requirement to frame\ncameras to capture the volume of a dynamic performance ($&gt;50m^3$) results in\nthe person occupying only a small proportion $&lt;$ 10% of the field of view. Even\nwith ultra high-definition 4k video acquisition this results in sampling the\nperson at less-than standard definition 0.5k video resolution resulting in\nlow-quality rendering. In this paper we propose a solution to this problem\nthrough super-resolution appearance transfer from a static high-resolution\nappearance capture rig using digital stills cameras ($&gt; 8k$) to capture the\nperson in a small volume ($&lt;8m^3$). A pipeline is proposed for super-resolution\nappearance transfer from high-resolution static capture to dynamic video\nperformance capture to produce super-resolution dynamic textures. This\naddresses two key problems: colour mapping between different camera systems;\nand dynamic texture map super-resolution using a learnt model. Comparative\nevaluation demonstrates a significant qualitative and quantitative improvement\nin rendering the 4D performance capture with super-resolution dynamic texture\nappearance. The proposed approach reproduces the high-resolution detail of the\nstatic capture whilst maintaining the appearance dynamics of the captured\nvideo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pesavento_M/0/1/0/all/0/1\">Marco Pesavento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volino_M/0/1/0/all/0/1\">Marco Volino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_A/0/1/0/all/0/1\">Adrian Hilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic labelling of urban point clouds using data fusion. (arXiv:2108.13757v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13757","description":"<p>In this paper we describe an approach to semi-automatically create a labelled\ndataset for semantic segmentation of urban street-level point clouds. We use\ndata fusion techniques using public data sources such as elevation data and\nlarge-scale topographical maps to automatically label parts of the point cloud,\nafter which only limited human effort is needed to check the results and make\namendments where needed. This drastically limits the time needed to create a\nlabelled dataset that is extensive enough to train deep semantic segmentation\nmodels. We apply our method to point clouds of the Amsterdam region, and\nsuccessfully train a RandLA-Net semantic segmentation model on the labelled\ndataset. These results demonstrate the potential of smart data fusion and\nsemantic segmentation for the future of smart city planning and management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bloembergen_D/0/1/0/all/0/1\">Daan Bloembergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eijgenstein_C/0/1/0/all/0/1\">Chris Eijgenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Semantic Feature Pyramid Network with Guided Anchoring for Logo Detection. (arXiv:2108.13775v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13775","description":"<p>Recently, logo detection has received more and more attention for its wide\napplications in the multimedia field, such as intellectual property protection,\nproduct brand management, and logo duration monitoring. Unlike general object\ndetection, logo detection is a challenging task, especially for small logo\nobjects and large aspect ratio logo objects in the real-world scenario. In this\npaper, we propose a novel approach, named Discriminative Semantic Feature\nPyramid Network with Guided Anchoring (DSFP-GA), which can address these\nchallenges via aggregating the semantic information and generating different\naspect ratio anchor boxes. More specifically, our approach mainly consists of\nDiscriminative Semantic Feature Pyramid (DSFP) and Guided Anchoring (GA).\nConsidering that low-level feature maps that are used to detect small logo\nobjects lack semantic information, we propose the DSFP, which can enrich more\ndiscriminative semantic features of low-level feature maps and can achieve\nbetter performance on small logo objects. Furthermore, preset anchor boxes are\nless efficient for detecting large aspect ratio logo objects. We therefore\nintegrate the GA into our method to generate large aspect ratio anchor boxes to\nmitigate this issue. Extensive experimental results on four benchmarks\ndemonstrate the effectiveness of our proposed DSFP-GA. Moreover, we further\nconduct visual analysis and ablation studies to illustrate the advantage of our\nmethod in detecting small and large aspect logo objects. The code and models\ncan be found at https://github.com/Zhangbaisong/DSFP-GA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baisong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Sujuan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Calibrating Neural Radiance Fields. (arXiv:2108.13826v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13826","description":"<p>In this work, we propose a camera self-calibration algorithm for generic\ncameras with arbitrary non-linear distortions. We jointly learn the geometry of\nthe scene and the accurate camera parameters without any calibration objects.\nOur camera model consists of a pinhole model, a fourth order radial distortion,\nand a generic noise model that can learn arbitrary non-linear camera\ndistortions. While traditional self-calibration algorithms mostly rely on\ngeometric constraints, we additionally incorporate photometric consistency.\nThis requires learning the geometry of the scene, and we use Neural Radiance\nFields (NeRF). We also propose a new geometric loss function, viz., projected\nray distance loss, to incorporate geometric consistency for complex non-linear\ncamera models. We validate our approach on standard real image datasets and\ndemonstrate that our model can learn the camera intrinsics and extrinsics\n(pose) from scratch without COLMAP initialization. Also, we show that learning\naccurate camera models in a differentiable manner allows us to improve PSNR\nover baselines. Our module is an easy-to-use plugin that can be applied to NeRF\nvariants to improve performance. The code and data are currently available at\nhttps://github.com/POSTECH-CVLab/SCNeRF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yoonwoo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Seokjun Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1\">Christopher Choy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Animashree Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PACE: Posthoc Architecture-Agnostic Concept Extractor for Explaining CNNs. (arXiv:2108.13828v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13828","description":"<p>Deep CNNs, though have achieved the state of the art performance in image\nclassification tasks, remain a black-box to a human using them. There is a\ngrowing interest in explaining the working of these deep models to improve\ntheir trustworthiness. In this paper, we introduce a Posthoc\nArchitecture-agnostic Concept Extractor (PACE) that automatically extracts\nsmaller sub-regions of the image called concepts relevant to the black-box\nprediction. PACE tightly integrates the faithfulness of the explanatory\nframework to the black-box model. To the best of our knowledge, this is the\nfirst work that extracts class-specific discriminative concepts in a posthoc\nmanner automatically. The PACE framework is used to generate explanations for\ntwo different CNN architectures trained for classifying the AWA2 and\nImagenet-Birds datasets. Extensive human subject experiments are conducted to\nvalidate the human interpretability and consistency of the explanations\nextracted by PACE. The results from these experiments suggest that over 72% of\nthe concepts extracted by PACE are human interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamakshi_V/0/1/0/all/0/1\">Vidhya Kamakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1\">Uday Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery. (arXiv:2108.13844v1 [eess.IV])","link":"http://arxiv.org/abs/2108.13844","description":"<p>Fiducial markers are commonly used in navigation assisted minimally invasive\nspine surgery (MISS) and they help transfer image coordinates into real world\ncoordinates. In practice, these markers might be located outside the\nfield-of-view (FOV), due to the limited detector sizes of C-arm cone-beam\ncomputed tomography (CBCT) systems used in intraoperative surgeries. As a\nconsequence, reconstructed markers in CBCT volumes suffer from artifacts and\nhave distorted shapes, which sets an obstacle for navigation. In this work, we\npropose two fiducial marker detection methods: direct detection from distorted\nmarkers (direct method) and detection after marker recovery (recovery method).\nFor direct detection from distorted markers in reconstructed volumes, an\nefficient automatic marker detection method using two neural networks and a\nconventional circle detection algorithm is proposed. For marker recovery, a\ntask-specific learning strategy is proposed to recover markers from severely\ntruncated data. Afterwards, a conventional marker detection algorithm is\napplied for position detection. The two methods are evaluated on simulated data\nand real data, both achieving a marker registration error smaller than 0.2 mm.\nOur experiments demonstrate that the direct method is capable of detecting\ndistorted markers accurately and the recovery method with task-specific\nlearning has high robustness and generalizability on various data sets. In\naddition, the task-specific learning is able to reconstruct other structures of\ninterest accurately, e.g. ribs for image-guided needle biopsy, from severely\ntruncated data, which empowers CBCT systems with new potential applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_F/0/1/0/all/0/1\">Fuxin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreher_B/0/1/0/all/0/1\">Bj&#xf6;rn Kreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keil_H/0/1/0/all/0/1\">Holger Keil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andreas_M/0/1/0/all/0/1\">Maier Andreas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images. (arXiv:2108.13865v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13865","description":"<p>In this paper, we present InSeGAN, an unsupervised 3D generative adversarial\nnetwork (GAN) for segmenting (nearly) identical instances of rigid objects in\ndepth images. Using an analysis-by-synthesis approach, we design a novel GAN\narchitecture to synthesize a multiple-instance depth image with independent\ncontrol over each instance. InSeGAN takes in a set of code vectors (e.g.,\nrandom noise vectors), each encoding the 3D pose of an object that is\nrepresented by a learned implicit object template. The generator has two\ndistinct modules. The first module, the instance feature generator, uses each\nencoded pose to transform the implicit template into a feature map\nrepresentation of each object instance. The second module, the depth image\nrenderer, aggregates all of the single-instance feature maps output by the\nfirst module and generates a multiple-instance depth image. A discriminator\ndistinguishes the generated multiple-instance depth images from the\ndistribution of true depth images. To use our model for instance segmentation,\nwe propose an instance pose encoder that learns to take in a generated depth\nimage and reproduce the pose code vectors for all of the object instances. To\nevaluate our approach, we introduce a new synthetic dataset, \"Insta-10\",\nconsisting of 100,000 depth images, each with 5 instances of an object from one\nof 10 classes. Our experiments on Insta-10, as well as on real-world noisy\ndepth images, show that InSeGAN achieves state-of-the-art performance, often\noutperforming prior methods by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_G/0/1/0/all/0/1\">Goncalo Dias Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddarth Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_T/0/1/0/all/0/1\">Tim K. Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_A/0/1/0/all/0/1\">Alan Sullivan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-shot domain adaptation for semantic face editing of real world images using StyleALAE. (arXiv:2108.13876v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13876","description":"<p>Semantic face editing of real world facial images is an important application\nof generative models. Recently, multiple works have explored possible\ntechniques to generate such modifications using the latent structure of\npre-trained GAN models. However, such approaches often require training an\nencoder network and that is typically a time-consuming and resource intensive\nprocess. A possible alternative to such a GAN-based architecture can be\nstyleALAE, a latent-space based autoencoder that can generate photo-realistic\nimages of high quality. Unfortunately, the reconstructed image in styleALAE\ndoes not preserve the identity of the input facial image. This limits the\napplication of styleALAE for semantic face editing of images with known\nidentities. In our work, we use a recent advancement in one-shot domain\nadaptation to address this problem. Our work ensures that the identity of the\nreconstructed image is the same as the given input image. We further generate\nsemantic modifications over the reconstructed image by using the latent space\nof the pre-trained styleALAE model. Results show that our approach can generate\nsemantic modifications on any real world facial image while preserving the\nidentity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Ravi Kiran Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shubham_K/0/1/0/all/0/1\">Kumar Shubham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Gopalakrishnan Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandikota_S/0/1/0/all/0/1\">Sriram Gandikota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoche_S/0/1/0/all/0/1\">Sarthak Khoche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayagopi_D/0/1/0/all/0/1\">Dinesh Babu Jayagopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasaraghavan_G/0/1/0/all/0/1\">Gopalakrishnan Srinivasaraghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimation of Air Pollution with Remote Sensing Data: Revealing Greenhouse Gas Emissions from Space. (arXiv:2108.13902v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13902","description":"<p>Air pollution is a major driver of climate change. Anthropogenic emissions\nfrom the burning of fossil fuels for transportation and power generation emit\nlarge amounts of problematic air pollutants, including Greenhouse Gases (GHGs).\nDespite the importance of limiting GHG emissions to mitigate climate change,\ndetailed information about the spatial and temporal distribution of GHG and\nother air pollutants is difficult to obtain. Existing models for surface-level\nair pollution rely on extensive land-use datasets which are often locally\nrestricted and temporally static. This work proposes a deep learning approach\nfor the prediction of ambient air pollution that only relies on remote sensing\ndata that is globally available and frequently updated. Combining optical\nsatellite imagery with satellite-based atmospheric column density air pollution\nmeasurements enables the scaling of air pollution estimates (in this case\nNO$_2$) to high spatial resolution (up to $\\sim$10m) at arbitrary locations and\nadds a temporal component to these estimates. The proposed model performs with\nhigh accuracy when evaluated against air quality measurements from ground\nstations (mean absolute error $&lt;$6$~\\mu g/m^3$). Our results enable the\nidentification and temporal monitoring of major sources of air pollution and\nGHGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheibenreif_L/0/1/0/all/0/1\">Linus Scheibenreif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mommert_M/0/1/0/all/0/1\">Michael Mommert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1\">Damian Borth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Nuclear Instance and Layer Segmentation in Oral Epithelial Dysplasia. (arXiv:2108.13904v1 [eess.IV])","link":"http://arxiv.org/abs/2108.13904","description":"<p>Oral epithelial dysplasia (OED) is a pre-malignant histopathological\ndiagnosis given to lesions of the oral cavity. Predicting OED grade or whether\na case will transition to malignancy is critical for early detection and\nappropriate treatment. OED typically begins in the lower third of the\nepithelium before progressing upwards with grade severity, thus we have\nsuggested that segmenting intra-epithelial layers, in addition to individual\nnuclei, may enable researchers to evaluate important layer-specific\nmorphological features for grade/malignancy prediction. We present HoVer-Net+,\na deep learning framework to simultaneously segment (and classify) nuclei and\n(intra-)epithelial layers in H&amp;E stained slides from OED cases. The proposed\narchitecture consists of an encoder branch and four decoder branches for\nsimultaneous instance segmentation of nuclei and semantic segmentation of the\nepithelial layers. We show that the proposed model achieves the\nstate-of-the-art (SOTA) performance in both tasks, with no additional costs\nwhen compared to previous SOTA methods for each task. To the best of our\nknowledge, ours is the first method for simultaneous nuclear instance\nsegmentation and semantic tissue segmentation, with potential for use in\ncomputational pathology for other similar simultaneous tasks and for future\nstudies into malignancy prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shephard_A/0/1/0/all/0/1\">Adam J. Shephard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bashir_R/0/1/0/all/0/1\">R.M. Saad Bashir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_H/0/1/0/all/0/1\">Hanya Mahmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1\">Syed Ali Khurram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic digital twin data model generation of building energy systems from piping and instrumentation diagrams. (arXiv:2108.13912v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13912","description":"<p>Buildings directly and indirectly emit a large share of current CO2\nemissions. There is a high potential for CO2 savings through modern control\nmethods in building automation systems (BAS) like model predictive control\n(MPC). For a proper control, MPC needs mathematical models to predict the\nfuture behavior of the controlled system. For this purpose, digital twins of\nthe building can be used. However, with current methods in existing buildings,\na digital twin set up is usually labor-intensive. Especially connecting the\ndifferent components of the technical system to an overall digital twin of the\nbuilding is time-consuming. Piping and instrument diagrams (P&amp;ID) can provide\nthe needed information, but it is necessary to extract the information and\nprovide it in a standardized format to process it further.\n</p>\n<p>In this work, we present an approach to recognize symbols and connections of\nP&amp;ID from buildings in a completely automated way. There are various standards\nfor graphical representation of symbols in P&amp;ID of building energy systems.\nTherefore, we use different data sources and standards to generate a holistic\ntraining data set. We apply algorithms for symbol recognition, line recognition\nand derivation of connections to the data sets. Furthermore, the result is\nexported to a format that provides semantics of building energy systems.\n</p>\n<p>The symbol recognition, line recognition and connection recognition show good\nresults with an average precision of 93.7%, which can be used in further\nprocesses like control generation, (distributed) model predictive control or\nfault detection. Nevertheless, the approach needs further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stinner_F/0/1/0/all/0/1\">Florian Stinner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiecek_M/0/1/0/all/0/1\">Martin Wiecek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baranski_M/0/1/0/all/0/1\">Marc Baranski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumpel_A/0/1/0/all/0/1\">Alexander K&#xfc;mpel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1\">Dirk M&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScatSimCLR: self-supervised contrastive learning with pretext task regularization for small-scale datasets. (arXiv:2108.13939v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13939","description":"<p>In this paper, we consider a problem of self-supervised learning for\nsmall-scale datasets based on contrastive loss between multiple views of the\ndata, which demonstrates the state-of-the-art performance in classification\ntask. Despite the reported results, such factors as the complexity of training\nrequiring complex architectures, the needed number of views produced by data\naugmentation, and their impact on the classification accuracy are understudied\nproblems. To establish the role of these factors, we consider an architecture\nof contrastive loss system such as SimCLR, where baseline model is replaced by\ngeometrically invariant \"hand-crafted\" network ScatNet with small trainable\nadapter network and argue that the number of parameters of the whole system and\nthe number of views can be considerably reduced while practically preserving\nthe same classification accuracy. In addition, we investigate the impact of\nregularization strategies using pretext task learning based on an estimation of\nparameters of augmentation transform such as rotation and jigsaw permutation\nfor both traditional baseline models and ScatNet based models. Finally, we\ndemonstrate that the proposed architecture with pretext task learning\nregularization achieves the state-of-the-art classification performance with a\nsmaller number of trainable parameters and with reduced number of views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kinakh_V/0/1/0/all/0/1\">Vitaliy Kinakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taran_O/0/1/0/all/0/1\">Olga Taran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1\">Svyatoslav Voloshynovskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Dataset for Keypoint Detection of quadruped Animals from Images. (arXiv:2108.13958v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13958","description":"<p>In this paper, we studied the problem of localizing a generic set of\nkeypoints across multiple quadruped or four-legged animal species from images.\nDue to the lack of large scale animal keypoint dataset with ground truth\nannotations, we developed a novel dataset, AwA Pose, for keypoint detection of\nquadruped animals from images. Our dataset contains significantly more\nkeypoints per animal and has much more diverse animals than the existing\ndatasets for animal keypoint detection. We benchmarked the dataset with a\nstate-of-the-art deep learning model for different keypoint detection tasks,\nincluding both seen and unseen animal cases. Experimental results showed the\neffectiveness of the dataset. We believe that this dataset will help the\ncomputer vision community in the design and evaluation of improved models for\nthe generalized quadruped animal keypoint detection problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banik_P/0/1/0/all/0/1\">Prianka Banik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xishuang Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DepthTrack : Unveiling the Power of RGBD Tracking. (arXiv:2108.13962v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13962","description":"<p>RGBD (RGB plus depth) object tracking is gaining momentum as RGBD sensors\nhave become popular in many application fields such as robotics.However, the\nbest RGBD trackers are extensions of the state-of-the-art deep RGB trackers.\nThey are trained with RGB data and the depth channel is used as a sidekick for\nsubtleties such as occlusion detection. This can be explained by the fact that\nthere are no sufficiently large RGBD datasets to 1) train deep depth trackers\nand to 2) challenge RGB trackers with sequences for which the depth cue is\nessential. This work introduces a new RGBD tracking dataset - Depth-Track -\nthat has twice as many sequences (200) and scene types (40) than in the largest\nexisting dataset, and three times more objects (90). In addition, the average\nlength of the sequences (1473), the number of deformable objects (16) and the\nnumber of annotated tracking attributes (15) have been increased. Furthermore,\nby running the SotA RGB and RGBD trackers on DepthTrack, we propose a new RGBD\ntracking baseline, namely DeT, which reveals that deep RGBD tracking indeed\nbenefits from genuine training data. The code and dataset is available at\nhttps://github.com/xiaozai/DeT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Song Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapyla_J/0/1/0/all/0/1\">Jani K&#xe4;pyl&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamarainen_J/0/1/0/all/0/1\">Joni-Kristian K&#xe4;m&#xe4;r&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S4-Crowd: Semi-Supervised Learning with Self-Supervised Regularisation for Crowd Counting. (arXiv:2108.13969v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13969","description":"<p>Crowd counting has drawn more attention because of its wide application in\nsmart cities. Recent works achieved promising performance but relied on the\nsupervised paradigm with expensive crowd annotations. To alleviate annotation\ncost, in this work we proposed a semi-supervised learning framework S4-Crowd,\nwhich can leverage both unlabeled/labeled data for robust crowd modelling. In\nthe unsupervised pathway, two self-supervised losses were proposed to simulate\nthe crowd variations such as scale, illumination, etc., based on which and the\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit Gated-Crowd-Recurrent-Unit (GCRU),\nwhich can preserve discriminant crowd information by extracting second-order\nstatistics, yielding pseudo labels with improved quality. A joint loss\nincluding both unsupervised/supervised information was proposed, and a dynamic\nweighting strategy was employed to balance the importance of the unsupervised\nloss and supervised loss at different training stages. We conducted extensive\nexperiments on four popular crowd counting datasets in semi-supervised\nsettings. Experimental results suggested the effectiveness of each proposed\ncomponent in our S4-Crowd framework. Our method also outperformed other\nstate-of-the-art semi-supervised learning approaches on these crowd datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haoran Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Mitosis against Domain Shift using a Fused Detector and Deep Ensemble Classification Model for MIDOG Challenge. (arXiv:2108.13983v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13983","description":"<p>Mitotic figure count is an important marker of tumor proliferation and has\nbeen shown to be associated with patients' prognosis. Deep learning based\nmitotic figure detection methods have been utilized to automatically locate the\ncell in mitosis using hematoxylin \\&amp; eosin (H\\&amp;E) stained images. However, the\nmodel performance deteriorates due to the large variation of color tone and\nintensity in H\\&amp;E images. In this work, we proposed a two stage mitotic figure\ndetection framework by fusing a detector and a deep ensemble classification\nmodel. To alleviate the impact of color variation in H\\&amp;E images, we utilize\nboth stain normalization and data augmentation, aiding model to learn color\nirrelevant features. The proposed model obtains an F1 score of 0.7550 on the\npreliminary testing set released by the MIDOG challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingtang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yujie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhibin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yubo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OARnet: Automated organs-at-risk delineation in Head and Neck CT images. (arXiv:2108.13987v1 [eess.IV])","link":"http://arxiv.org/abs/2108.13987","description":"<p>A 3D deep learning model (OARnet) is developed and used to delineate 28 H&amp;N\nOARs on CT images. OARnet utilizes a densely connected network to detect the\nOAR bounding-box, then delineates the OAR within the box. It reuses information\nfrom any layer to subsequent layers and uses skip connections to combine\ninformation from different dense block levels to progressively improve\ndelineation accuracy. Training uses up to 28 expert manual delineated (MD) OARs\nfrom 165 CTs. Dice similarity coefficient (DSC) and the 95th percentile\nHausdorff distance (HD95) with respect to MD is assessed for 70 other CTs.\nMean, maximum, and root-mean-square dose differences with respect to MD are\nassessed for 56 of the 70 CTs. OARnet is compared with UaNet, AnatomyNet, and\nMulti-Atlas Segmentation (MAS). Wilcoxon signed-rank tests using 95% confidence\nintervals are used to assess significance. Wilcoxon signed ranked tests show\nthat, compared with UaNet, OARnet improves (p&lt;0.05) the DSC (23/28 OARs) and\nHD95 (17/28). OARnet outperforms both AnatomyNet and MAS for DSC (28/28) and\nHD95 (27/28). Compared with UaNet, OARnet improves median DSC up to 0.05 and\nHD95 up to 1.5mm. Compared with AnatomyNet and MAS, OARnet improves median\n(DSC, HD95) by up to (0.08, 2.7mm) and (0.17, 6.3mm). Dosimetrically, OARnet\noutperforms UaNet (Dmax 7/28; Dmean 10/28), AnatomyNet (Dmax 21/28; Dmean\n24/28), and MAS (Dmax 22/28; Dmean 21/28). The DenseNet architecture is\noptimized using a hybrid approach that performs OAR-specific bounding box\ndetection followed by feature recognition. Compared with other auto-delineation\nmethods, OARnet is better than or equal to UaNet for all but one geometric\n(Temporal Lobe L, HD95) and one dosimetric (Eye L, mean dose) endpoint for the\n28 H&amp;N OARs, and is better than or equal to both AnatomyNet and MAS for all\nOARs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Soomro_M/0/1/0/all/0/1\">Mumtaz Hussain Soomro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nourzadeh_H/0/1/0/all/0/1\">Hamidreza Nourzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alves_V/0/1/0/all/0/1\">Victor Gabriel Leandro Alves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_W/0/1/0/all/0/1\">Wookjin Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siebers_J/0/1/0/all/0/1\">Jeffrey V. Siebers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Hands: A Hybrid Model for 3D Hand Reconstruction. (arXiv:2108.13995v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13995","description":"<p>Estimating 3D hand meshes from RGB images robustly is a highly desirable\ntask, made challenging due to the numerous degrees of freedom, and issues such\nas self similarity and occlusions. Previous methods generally either use\nparametric 3D hand models or follow a model-free approach. While the former can\nbe considered more robust, e.g. to occlusions, they are less expressive. We\npropose a hybrid approach, utilizing a deep neural network and differential\nrendering based optimization to demonstrably achieve the best of both worlds.\nIn addition, we explore Virtual Reality (VR) as an application. Most VR\nheadsets are nowadays equipped with multiple cameras, which we can leverage by\nextending our method to the egocentric stereo domain. This extension proves to\nbe more resilient to the above mentioned issues. Finally, as a use-case, we\nshow that the improved image-model alignment can be used to acquire the user's\nhand texture, which leads to a more realistic virtual hand representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seeber_M/0/1/0/all/0/1\">Michael Seeber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poranne_R/0/1/0/all/0/1\">Roi Poranne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUMBO: Bridging Response Generation and Facial Expression Synthesis. (arXiv:1905.11240v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1905.11240","description":"<p>Spoken dialogue systems that assist users to solve complex tasks such as\nmovie ticket booking have become an emerging research topic in artificial\nintelligence and natural language processing areas. With a well-designed\ndialogue system as an intelligent personal assistant, people can accomplish\ncertain tasks more easily via natural language interactions. Today there are\nseveral virtual intelligent assistants in the market; however, most systems\nonly focus on textual or vocal interaction. In this paper, we present HUMBO, a\nsystem aiming at generating dialogue responses and simultaneously synthesize\ncorresponding visual expressions on faces for better multimodal interaction.\nHUMBO can (1) let users determine the appearances of virtual assistants by a\nsingle image, and (2) generate coherent emotional utterances and facial\nexpressions on the user-provided image. This is not only a brand new research\ndirection but more importantly, an ultimate step toward more human-like virtual\nassistants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shang-Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Po-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Adversarial Robustness of Texture and Shape-Biased Models. (arXiv:1911.10364v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.10364","description":"<p>Increasing shape-bias in deep neural networks has been shown to improve\nrobustness to common corruptions and noise. In this paper we analyze the\nadversarial robustness of texture and shape-biased models to Universal\nAdversarial Perturbations (UAPs). We use UAPs to evaluate the robustness of DNN\nmodels with varying degrees of shape-based training. We find that shape-biased\nmodels do not markedly improve adversarial robustness, and we show that\nensembles of texture and shape-biased models can improve universal adversarial\nrobustness while maintaining strong performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1\">Kenneth T. Co</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Gonzalez_L/0/1/0/all/0/1\">Luis Mu&#xf1;oz-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanthan_L/0/1/0/all/0/1\">Leslie Kanthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FASTER: Fast and Safe Trajectory Planner for Navigation in Unknown Environments. (arXiv:2001.04420v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2001.04420","description":"<p>Planning high-speed trajectories for UAVs in unknown environments requires\nalgorithmic techniques that enable fast reaction times to guarantee safety as\nmore information about the environment becomes available. The standard\napproaches that ensure safety by enforcing a \"stop\" condition in the free-known\nspace can severely limit the speed of the vehicle, especially in situations\nwhere much of the world is unknown. Moreover, the ad-hoc time and interval\nallocation scheme usually imposed on the trajectory also leads to conservative\nand slower trajectories. This work proposes FASTER (Fast and Safe Trajectory\nPlanner) to ensure safety without sacrificing speed. FASTER obtains high-speed\ntrajectories by enabling the local planner to optimize in both the free-known\nand unknown spaces. Safety is ensured by always having a safe back-up\ntrajectory in the free-known space. The MIQP formulation proposed also allows\nthe solver to choose the trajectory interval allocation. FASTER is tested\nextensively in simulation and in real hardware, showing flights in unknown\ncluttered environments with velocities up to 7.8m/s, and experiments at the\nmaximum speed of a skid-steer ground robot (2m/s).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tordesillas_J/0/1/0/all/0/1\">Jesus Tordesillas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_B/0/1/0/all/0/1\">Brett T. Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everett_M/0/1/0/all/0/1\">Michael Everett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1\">Jonathan P. How</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Model-agnostic Semantic Segmentation via Target-specific Normalization. (arXiv:2003.12296v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.12296","description":"<p>Semantic segmentation in a supervised learning manner has achieved\nsignificant progress in recent years. However, its performance usually drops\ndramatically due to the data-distribution discrepancy between seen and unseen\ndomains when we directly deploy the trained model to segment the images of\nunseen (or new coming) domains. To this end, we propose a novel domain\ngeneralization framework for the generalizable semantic segmentation task,\nwhich enhances the generalization ability of the model from two different\nviews, including the training paradigm and the test strategy. Concretely, we\nexploit the model-agnostic learning to simulate the domain shift problem, which\ndeals with the domain generalization from the training scheme perspective.\nBesides, considering the data-distribution discrepancy between seen source and\nunseen target domains, we develop the target-specific normalization scheme to\nenhance the generalization ability. Furthermore, when images come one by one in\nthe test stage, we design the image-based memory bank (Image Bank in short)\nwith style-based selection policy to select similar images to obtain more\naccurate statistics of normalization. Extensive experiments highlight that the\nproposed method produces state-of-the-art performance for the domain\ngeneralization of semantic segmentation on multiple benchmark segmentation\ndatasets, i.e., Cityscapes, Mapillary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer. (arXiv:2004.06698v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.06698","description":"<p>Visual dialog is a task of answering a sequence of questions grounded in an\nimage using the previous dialog history as context. In this paper, we study how\nto address two fundamental challenges for this task: (1) reasoning over\nunderlying semantic structures among dialog rounds and (2) identifying several\nappropriate answers to the given question. To address these challenges, we\npropose a Sparse Graph Learning (SGL) method to formulate visual dialog as a\ngraph structure learning task. SGL infers inherently sparse dialog structures\nby incorporating binary and score edges and leveraging a new structural loss\nfunction. Next, we introduce a Knowledge Transfer (KT) method that extracts the\nanswer predictions from the teacher model and uses them as pseudo labels. We\npropose KT to remedy the shortcomings of single ground-truth labels, which\nseverely limit the ability of a model to obtain multiple reasonable answers. As\na result, our proposed model significantly improves reasoning capability\ncompared to baseline methods and outperforms the state-of-the-art approaches on\nthe VisDial v1.0 dataset. The source code is available at\nhttps://github.com/gicheonkang/SGLKT-VisDial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Gi-Cheon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junseok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noticing Motion Patterns: Temporal CNN with a Novel Convolution Operator for Human Trajectory Prediction. (arXiv:2007.00862v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.00862","description":"<p>We propose a Convolutional Neural Network-based approach to learn, detect,and\nextract patterns in sequential trajectory data, known here as Social Pattern\nExtraction Convolution (Social-PEC). A set of experiments carried out on the\nhuman trajectory prediction problem shows that our model performs comparably to\nthe state of the art and outperforms in some cases. More importantly,the\nproposed approach unveils the obscurity in the previous use of pooling layer,\npresenting a way to intuitively explain the decision-making process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dapeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protect, Show, Attend and Tell: Empowering Image Captioning Models with Ownership Protection. (arXiv:2008.11009v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.11009","description":"<p>By and large, existing Intellectual Property (IP) protection on deep neural\nnetworks typically i) focus on image classification task only, and ii) follow a\nstandard digital watermarking framework that was conventionally used to protect\nthe ownership of multimedia and video content. This paper demonstrates that the\ncurrent digital watermarking framework is insufficient to protect image\ncaptioning tasks that are often regarded as one of the frontiers AI problems.\nAs a remedy, this paper studies and proposes two different embedding schemes in\nthe hidden memory state of a recurrent neural network to protect the image\ncaptioning model. From empirical points, we prove that a forged key will yield\nan unusable image captioning model, defeating the purpose of infringement. To\nthe best of our knowledge, this work is the first to propose ownership\nprotection on image captioning task. Also, extensive experiments show that the\nproposed method does not compromise the original image captioning performance\non all common captioning metrics on Flickr30k and MS-COCO datasets, and at the\nsame time it is able to withstand both removal and ambiguity attacks. Code is\navailable at https://github.com/jianhanlim/ipr-imagecaptioning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Jian Han Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1\">Kam Woh Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape Defense Against Adversarial Attacks. (arXiv:2008.13336v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.13336","description":"<p>Humans rely heavily on shape information to recognize objects. Conversely,\nconvolutional neural networks (CNNs) are biased more towards texture. This is\nperhaps the main reason why CNNs are vulnerable to adversarial examples. Here,\nwe explore how shape bias can be incorporated into CNNs to improve their\nrobustness. Two algorithms are proposed, based on the observation that edges\nare invariant to moderate imperceptible perturbations. In the first one, a\nclassifier is adversarially trained on images with the edge map as an\nadditional channel. At inference time, the edge map is recomputed and\nconcatenated to the image. In the second algorithm, a conditional GAN is\ntrained to translate the edge maps, from clean and/or perturbed images, into\nclean images. Inference is done over the generated image corresponding to the\ninput's edge map. Extensive experiments over 10 datasets demonstrate the\neffectiveness of the proposed algorithms against FGSM and $\\ell_\\infty$ PGD-40\nattacks. Further, we show that a) edge information can also benefit other\nadversarial training methods, and b) CNNs trained on edge-augmented inputs are\nmore robust against natural image corruptions such as motion blur, impulse\nnoise and JPEG compression, than CNNs trained solely on RGB images. From a\nbroader perspective, our study suggests that CNNs do not adequately account for\nimage structures that are crucial for robustness. Code is available\nat:~\\url{https://github.com/aliborji/Shapedefence.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Conditional Generative Adversarial Networks: Novel Empirical Losses and Label Input Mechanisms. (arXiv:2011.07466v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.07466","description":"<p>This work proposes the continuous conditional generative adversarial network\n(CcGAN), the first generative model for image generation conditional on\ncontinuous, scalar conditions (termed regression labels). Existing conditional\nGANs (cGANs) are mainly designed for categorical conditions (eg, class labels);\nconditioning on regression labels is mathematically distinct and raises two\nfundamental problems:(P1) Since there may be very few (even zero) real images\nfor some regression labels, minimizing existing empirical versions of cGAN\nlosses (aka empirical cGAN losses) often fails in practice;(P2) Since\nregression labels are scalar and infinitely many, conventional label input\nmethods are not applicable. The proposed CcGAN solves the above problems,\nrespectively, by (S1) reformulating existing empirical cGAN losses to be\nappropriate for the continuous scenario; and (S2) proposing a naive label input\n(NLI) method and an improved label input (ILI) method to incorporate regression\nlabels into the generator and the discriminator. The reformulation in (S1)\nleads to two novel empirical discriminator losses, termed the hard vicinal\ndiscriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL)\nrespectively, and a novel empirical generator loss. The error bounds of a\ndiscriminator trained with HVDL and SVDL are derived under mild assumptions in\nthis work. Two new benchmark datasets (RC-49 and Cell-200) and a novel\nevaluation metric (Sliding Fr\\'echet Inception Distance) are also proposed for\nthis continuous scenario. Our experiments on the Circular 2-D Gaussians, RC-49,\nUTKFace, Cell-200, and Steering Angle datasets show that CcGAN is able to\ngenerate diverse, high-quality samples from the image distribution conditional\non a given regression label. Moreover, in these experiments, CcGAN\nsubstantially outperforms cGAN both visually and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zuheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_W/0/1/0/all/0/1\">William J. Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Image Denoising by Sampling from the Posterior Distribution. (arXiv:2101.09552v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.09552","description":"<p>Image denoising is a well-known and well studied problem, commonly targeting\na minimization of the mean squared error (MSE) between the outcome and the\noriginal image. Unfortunately, especially for severe noise levels, such Minimum\nMSE (MMSE) solutions may lead to blurry output images. In this work we propose\na novel stochastic denoising approach that produces viable and high perceptual\nquality results, while maintaining a small MSE. Our method employs Langevin\ndynamics that relies on a repeated application of any given MMSE denoiser,\nobtaining the reconstructed image by effectively sampling from the posterior\ndistribution. Due to its stochasticity, the proposed algorithm can produce a\nvariety of high-quality outputs for a given noisy input, all shown to be\nlegitimate denoising results. In addition, we present an extension of our\nalgorithm for handling the inpainting problem, recovering missing pixels while\nremoving noise from partially given data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kawar_B/0/1/0/all/0/1\">Bahjat Kawar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaksman_G/0/1/0/all/0/1\">Gregory Vaksman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TLRM: Task-level Relation Module for GNN-based Few-Shot Learning. (arXiv:2101.09840v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09840","description":"<p>Recently, graph neural networks (GNNs) have shown powerful ability to handle\nfew-shot classification problem, which aims at classifying unseen samples when\ntrained with limited labeled samples per class. GNN-based few-shot learning\narchitectures mostly replace traditional metric with a learnable GNN. In the\nGNN, the nodes are set as the samples embedding, and the relationship between\ntwo connected nodes can be obtained by a network, the input of which is the\ndifference of their embedding features. We consider this method of measuring\nrelation of samples only models the sample-to-sample relation, while neglects\nthe specificity of different tasks. That is, this method of measuring relation\ndoes not take the task-level information into account. To this end, we propose\na new relation measure method, namely the task-level relation module (TLRM), to\nexplicitly model the task-level relation of one sample to all the others. The\nproposed module captures the relation representations between nodes by\nconsidering the sample-to-task instead of sample-to-sample embedding features.\nWe conducted extensive experiments on four benchmark datasets: mini-ImageNet,\ntiered-ImageNet, CUB-$200$-$2011$, and CIFAR-FS. Experimental results\ndemonstrate that the proposed module is effective for GNN-based few-shot\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yurong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuan Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding self-supervised Learning Dynamics without Contrastive Pairs. (arXiv:2102.06810v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.06810","description":"<p>While contrastive approaches of self-supervised learning (SSL) learn\nrepresentations by minimizing the distance between two augmented views of the\nsame data point (positive pairs) and maximizing views from different data\npoints (negative pairs), recent \\emph{non-contrastive} SSL (e.g., BYOL and\nSimSiam) show remarkable performance {\\it without} negative pairs, with an\nextra learnable predictor and a stop-gradient operation. A fundamental question\narises: why do these methods not collapse into trivial representations? We\nanswer this question via a simple theoretical study and propose a novel\napproach, DirectPred, that \\emph{directly} sets the linear predictor based on\nthe statistics of its inputs, without gradient training. On ImageNet, it\nperforms comparably with more complex two-layer non-linear predictors that\nemploy BatchNorm and outperforms a linear predictor by $2.5\\%$ in 300-epoch\ntraining (and $5\\%$ in 60-epoch). DirectPred is motivated by our theoretical\nstudy of the nonlinear learning dynamics of non-contrastive SSL in simple\nlinear networks. Our study yields conceptual insights into how non-contrastive\nSSL methods learn, how they avoid representational collapse, and how multiple\nfactors, like predictor networks, stop-gradients, exponential moving averages,\nand weight decay all come into play. Our simple theory recapitulates the\nresults of real-world ablation studies in both STL-10 and ImageNet. Code is\nreleased https://github.com/facebookresearch/luckmatters/tree/master/ssl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05690","description":"<p>In current clinical practice, noisy and artifact-ridden weekly cone-beam\ncomputed tomography (CBCT) images are only used for patient setup during\nradiotherapy. Treatment planning is done once at the beginning of the treatment\nusing high-quality planning CT (pCT) images and manual contours for\norgans-at-risk (OARs) structures. If the quality of the weekly CBCT images can\nbe improved while simultaneously segmenting OAR structures, this can provide\ncritical information for adapting radiotherapy mid-treatment as well as for\nderiving biomarkers for treatment response. Using a novel physics-based data\naugmentation strategy, we synthesize a large dataset of perfectly/inherently\nregistered planning CT and synthetic-CBCT pairs for locally advanced lung\ncancer patient cohort, which are then used in a multitask 3D deep learning\nframework to simultaneously segment and translate real weekly CBCT images to\nhigh-quality planning CT-like images. We compared the synthetic CT and OAR\nsegmentations generated by the model to real planning CT and manual OAR\nsegmentations and showed promising results. The real week 1 (baseline) CBCT\nimages which had an average MAE of 162.77 HU compared to pCT images are\ntranslated to synthetic CT images that exhibit a drastically improved average\nMAE of 29.31 HU and average structural similarity of 92% with the pCT images.\nThe average DICE scores of the 3D organs-at-risk segmentations are: lungs 0.96,\nheart 0.88, spinal cord 0.83 and esophagus 0.66. This approach could allow\nclinicians to adjust treatment plans using only the routine low-quality CBCT\nimages, potentially improving patient outcomes. Our code, data, and pre-trained\nmodels will be made available via our physics-based data augmentation library,\nPhysics-ArX, at https://github.com/nadeemlab/Physics-ArX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1\">Navdeep Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sadegh R Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Si-Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1\">Anthony Yezzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalizing image enhancement for critical visual tasks: improved legibility of papyri using color processing and visual illusions. (arXiv:2104.01106v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01106","description":"<p>Purpose: This article develops theoretical, algorithmic, perceptual, and\ninteraction aspects of script legibility enhancement in the visible light\nspectrum for the purpose of scholarly editing of papyri texts. - Methods: Novel\nlegibility enhancement algorithms based on color processing and visual\nillusions are compared to classic methods in a user experience experiment. -\nResults: (1) The proposed methods outperformed the comparison methods. (2)\nUsers exhibited a broad behavioral spectrum, under the influence of factors\nsuch as personality and social conditioning, tasks and application domains,\nexpertise level and image quality, and affordances of software, hardware, and\ninterfaces. No single enhancement method satisfied all factor configurations.\nTherefore, it is suggested to offer users a broad choice of methods to\nfacilitate personalization, contextualization, and complementarity. (3) A\ndistinction is made between casual and critical vision on the basis of signal\nambiguity and error consequences. The criteria of a paradigm for enhancing\nimages for critical applications comprise: interpreting images skeptically;\napproaching enhancement as a system problem; considering all image structures\nas potential information; and making uncertainty and alternative\ninterpretations explicit, both visually and numerically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atanasiu_V/0/1/0/all/0/1\">Vlad Atanasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marthot_Santaniello_I/0/1/0/all/0/1\">Isabelle Marthot-Santaniello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"De-rendering the World's Revolutionary Artefacts. (arXiv:2104.03954v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03954","description":"<p>Recent works have shown exciting results in unsupervised image de-rendering\n-- learning to decompose 3D shape, appearance, and lighting from single-image\ncollections without explicit supervision. However, many of these assume\nsimplistic material and lighting models. We propose a method, termed RADAR,\nthat can recover environment illumination and surface materials from real\nsingle-image collections, relying neither on explicit 3D supervision, nor on\nmulti-view or multi-light images. Specifically, we focus on rotationally\nsymmetric artefacts that exhibit challenging surface properties including\nspecular reflections, such as vases. We introduce a novel self-supervised\nalbedo discriminator, which allows the model to recover plausible albedo\nwithout requiring any ground-truth during training. In conjunction with a shape\nreconstruction module exploiting rotational symmetry, we present an end-to-end\nlearning framework that is able to de-render the world's revolutionary\nartefacts. We conduct experiments on a real vase dataset and demonstrate\ncompelling decomposition results, allowing for applications including\nfree-viewpoint rendering and relighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1\">Ameesh Makadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1\">Richard Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-on and Outfit Editing. (arXiv:2104.07021v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07021","description":"<p>We proposes a flexible person generation framework called Dressing in Order\n(DiOr), which supports 2D pose transfer, virtual try-on, and several fashion\nediting tasks. The key to DiOr is a novel recurrent generation pipeline to\nsequentially put garments on a person, so that trying on the same garments in\ndifferent orders will result in different looks. Our system can produce\ndressing effects not achievable by existing work, including different\ninteractions of garments (e.g., wearing a top tucked into the bottom or over\nit), as well as layering of multiple garments of the same type (e.g., jacket\nover shirt over t-shirt). DiOr explicitly encodes the shape and texture of each\ngarment, enabling these elements to be edited separately. Joint training on\npose transfer and inpainting helps with detail preservation and coherence of\ngenerated garments. Extensive evaluations show that DiOr outperforms other\nrecent methods like ADGAN in terms of output quality, and handles a wide range\nof editing functions for which there is no direct supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_A/0/1/0/all/0/1\">Aiyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKee_D/0/1/0/all/0/1\">Daniel McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1\">Svetlana Lazebnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection with Prototype-Guided Discriminative Latent Embeddings. (arXiv:2104.14945v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14945","description":"<p>Recent efforts towards video anomaly detection (VAD) try to learn a deep\nautoencoder to describe normal event patterns with small reconstruction errors.\nThe video inputs with large reconstruction errors are regarded as anomalies at\nthe test time. However, these methods sometimes reconstruct abnormal inputs\nwell because of the powerful generalization ability of deep autoencoder. To\naddress this problem, we present a novel approach for anomaly detection, which\nutilizes discriminative prototypes of normal data to reconstruct video frames.\nIn this way, the model will favor the reconstruction of normal events and\ndistort the reconstruction of abnormal events. Specifically, we use a\nprototype-guided memory module to perform discriminative latent embedding. We\nintroduce a new discriminative criterion for the memory module, as well as a\nloss function correspondingly, which can encourage memory items to record the\nrepresentative embeddings of normal data, i.e. prototypes. Besides, we design a\nnovel two-branch autoencoder, which is composed of a future frame prediction\nnetwork and an RGB difference generation network that share the same encoder.\nThe stacked RGB difference contains motion information just like optical flow,\nso our model can learn temporal regularity. We evaluate the effectiveness of\nour method on three benchmark datasets and experimental results demonstrate the\nproposed method outperforms the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuandu Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curious Representation Learning for Embodied Intelligence. (arXiv:2105.01060v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01060","description":"<p>Self-supervised representation learning has achieved remarkable success in\nrecent years. By subverting the need for supervised labels, such approaches are\nable to utilize the numerous unlabeled images that exist on the Internet and in\nphotographic datasets. Yet to build truly intelligent agents, we must construct\nrepresentation learning algorithms that can learn not only from datasets but\nalso learn from environments. An agent in a natural environment will not\ntypically be fed curated data. Instead, it must explore its environment to\nacquire the data it will learn from. We propose a framework, curious\nrepresentation learning (CRL), which jointly learns a reinforcement learning\npolicy and a visual representation model. The policy is trained to maximize the\nerror of the representation learner, and in doing so is incentivized to explore\nits environment. At the same time, the learned representation becomes stronger\nand stronger as the policy feeds it ever harder data to learn from. Our learned\nrepresentations enable promising transfer to downstream navigation tasks,\nperforming better than or comparably to ImageNet pretraining without using any\nsupervision at all. In addition, despite being trained in simulation, our\nlearned representations can obtain interpretable results on real images. Code\nis available at https://yilundu.github.io/crl/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data. (arXiv:2105.01764v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2105.01764","description":"<p>The use of video surveillance in public spaces -- both by government agencies\nand by private citizens -- has attracted considerable attention in recent\nyears, particularly in light of rapid advances in face-recognition technology.\nBut it has been difficult to systematically measure the prevalence and\nplacement of cameras, hampering efforts to assess the implications of\nsurveillance on privacy and public safety. Here, we combine computer vision,\nhuman verification, and statistical analysis to estimate the spatial\ndistribution of surveillance cameras. Specifically, we build a camera detection\nmodel and apply it to 1.6 million street view images sampled from 10 large U.S.\ncities and 6 other major cities around the world, with positive model\ndetections verified by human experts. After adjusting for the estimated recall\nof our model, and accounting for the spatial coverage of our sampled images, we\nare able to estimate the density of surveillance cameras visible from the road.\nAcross the 16 cities we consider, the estimated number of surveillance cameras\nper linear kilometer ranges from 0.2 (in Los Angeles) to 0.9 (in Seoul). In a\ndetailed analysis of the 10 U.S. cities, we find that cameras are concentrated\nin commercial, industrial, and mixed zones, and in neighborhoods with higher\nshares of non-white residents -- a pattern that persists even after adjusting\nfor land use. These results help inform ongoing discussions on the use of\nsurveillance technology, including its potential disparate impacts on\ncommunities of color.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1\">Hao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Keniel Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Sharad Goel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Deep Dynamic Characters. (arXiv:2105.01794v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01794","description":"<p>We propose a deep videorealistic 3D human character model displaying highly\nrealistic shape, motion, and dynamic appearance learned in a new weakly\nsupervised way from multi-view imagery. In contrast to previous work, our\ncontrollable 3D character displays dynamics, e.g., the swing of the skirt,\ndependent on skeletal body motion in an efficient data-driven way, without\nrequiring complex physics simulation. Our character model also features a\nlearned dynamic texture model that accounts for photo-realistic\nmotion-dependent appearance details, as well as view-dependent lighting\neffects. During training, we do not need to resort to difficult dynamic 3D\ncapture of the human; instead we can train our model entirely from multi-view\nvideo in a weakly supervised manner. To this end, we propose a parametric and\ndifferentiable character representation which allows us to model coarse and\nfine dynamic deformations, e.g., garment wrinkles, as explicit space-time\ncoherent mesh geometry that is augmented with high-quality dynamic textures\ndependent on motion and view point. As input to the model, only an arbitrary 3D\nskeleton motion is required, making it directly compatible with the established\n3D animation pipeline. We use a novel graph convolutional network architecture\nto enable motion-dependent deformation learning of body and clothing, including\ndynamics, and a neural generative dynamic texture model creates corresponding\ndynamic texture maps. We show that by merely providing new skeletal motions,\nour model creates motion-dependent surface deformations, physically plausible\ndynamic clothing deformations, as well as video-realistic surface textures at a\nmuch higher level of detail than previous state of the art approaches, and even\nin real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is cell ratio important in deep learning? A robust comparison of deep learning methods for multi-scale cytopathology cell image classification: from convolutional neural networks to visual transformers. (arXiv:2105.07402v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07402","description":"<p>Cervical cancer is a very common and fatal cancer in women. Cytopathology\nimages are often used to screen this cancer. Since there is a possibility of a\nlarge number of errors in manual screening, the computer-aided diagnosis system\nbased on deep learning is developed. The deep learning methods required a fixed\nsize of input images, but the sizes of the clinical medical images are\ninconsistent. The internal cell ratios of the images are suffered while\nresizing it directly. Clinically, the ratios of cells inside cytopathological\nimages provide important information for doctors to diagnose cancer. Therefore,\nit is illogical to resize directly. However, many existing studies resized the\nimages directly and obtained very robust classification results. To find a\nreasonable interpretation, we have conducted a series of comparative\nexperiments. First, the raw data of the SIPaKMeD dataset are preprocessed to\nobtain the standard and scaled datasets. Then, the datasets are resized to 224\n$\\times$ 224 pixels. Finally, twenty-two deep learning models are used to\nclassify standard and scaled datasets. The conclusion is that the deep learning\nmodels are robust to changes in the internal cell ratio of cervical\ncytopathological images. This conclusion is also validated on the Herlev\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahamana_M/0/1/0/all/0/1\">Md Mamunur Rahamana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformation Driven Seq2Seq Longitudinal Tumor and Organs-at-Risk Prediction for Radiotherapy. (arXiv:2106.09076v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09076","description":"<p>Purpose: Radiotherapy presents unique challenges and clinical requirements\nfor longitudinal tumor and organ-at-risk (OAR) prediction during treatment. The\nchallenges include tumor inflammation/edema and radiation-induced changes in\norgan geometry, whereas the clinical requirements demand flexibility in\ninput/output sequence timepoints to update the predictions on rolling basis and\nthe grounding of all predictions in relationship to the pre-treatment imaging\ninformation for response and toxicity assessment in adaptive radiotherapy.\nMethods: To deal with the aforementioned challenges and to comply with the\nclinical requirements, we present a novel 3D sequence-to-sequence model based\non Convolution Long Short Term Memory (ConvLSTM) that makes use of series of\ndeformation vector fields (DVF) between individual timepoints and reference\npre-treatment/planning CTs to predict future anatomical deformations and\nchanges in gross tumor volume as well as critical OARs. High-quality DVF\ntraining data is created by employing hyper-parameter optimization on the\nsubset of the training data with DICE coefficient and mutual information\nmetric. We validated our model on two radiotherapy datasets: a publicly\navailable head-and-neck dataset (28 patients with manually contoured pre-,\nmid-, and post-treatment CTs), and an internal non-small cell lung cancer\ndataset (63 patients with manually contoured planning CT and 6 weekly CBCTs).\nResults: The use of DVF representation and skip connections overcomes the\nblurring issue of ConvLSTM prediction with the traditional image\nrepresentation. The mean and standard deviation of DICE for predictions of lung\nGTV at week 4, 5, and 6 were 0.83$\\pm$0.09, 0.82$\\pm$0.08, and 0.81$\\pm$0.10,\nrespectively, and for post-treatment ipsilateral and contralateral parotids,\nwere 0.81$\\pm$0.06 and 0.85$\\pm$0.02.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sadegh R Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu-Chi Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Single Image Super-resolution Under Complex Noise. (arXiv:2107.00986v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00986","description":"<p>While the researches on single image super-resolution (SISR), especially\nequipped with deep neural networks (DNNs), have achieved tremendous successes\nrecently, they still suffer from two major limitations. Firstly, the real image\ndegradation is usually unknown and highly variant from one to another, making\nit extremely hard to train a single model to handle the general SISR task.\nSecondly, most of current methods mainly focus on the downsampling process of\nthe degradation, but ignore or underestimate the inevitable noise\ncontamination. For example, the commonly-used independent and identically\ndistributed (i.i.d.) Gaussian noise distribution always largely deviates from\nthe real image noise (e.g., camera sensor noise), which limits their\nperformance in real scenarios. To address these issues, this paper proposes a\nmodel-based unsupervised SISR method to deal with the general SISR task with\nunknown degradations. Instead of the traditional i.i.d. Gaussian noise\nassumption, a novel patch-based non-i.i.d. noise modeling method is proposed to\nfit the complex real noise. Besides, a deep generator parameterized by a DNN is\nused to map the latent variable to the high-resolution image, and the\nconventional hyper-Laplacian prior is also elaborately embedded into such\ngenerator to further constrain the image gradients. Finally, a Monte Carlo EM\nalgorithm is designed to solve our model, which provides a general inference\nframework to update the image generator both w.r.t. the latent variable and the\nnetwork parameters. Comprehensive experiments demonstrate that the proposed\nmethod can evidently surpass the current state of the art (SotA) method (about\n1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster\nspeed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zongsheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutually improved endoscopic image synthesis and landmark detection in unpaired image-to-image translation. (arXiv:2107.06941v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06941","description":"<p>The CycleGAN framework allows for unsupervised image-to-image translation of\nunpaired data. In a scenario of surgical training on a physical surgical\nsimulator, this method can be used to transform endoscopic images of phantoms\ninto images which more closely resemble the intra-operative appearance of the\nsame surgical target structure. This can be viewed as a novel augmented reality\napproach, which we coined Hyperrealism in previous work. In this use case, it\nis of paramount importance to display objects like needles, sutures or\ninstruments consistent in both domains while altering the style to a more\ntissue-like appearance. Segmentation of these objects would allow for a direct\ntransfer, however, contouring of these, partly tiny and thin foreground objects\nis cumbersome and perhaps inaccurate. Instead, we propose to use landmark\ndetection on the points when sutures pass into the tissue. This objective is\ndirectly incorporated into a CycleGAN framework by treating the performance of\npre-trained detector models as an additional optimization goal. We show that a\ntask defined on these sparse landmark labels improves consistency of synthesis\nby the generator network in both domains. Comparing a baseline CycleGAN\narchitecture to our proposed extension (DetCycleGAN), mean precision (PPV)\nimproved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by\n+0.4743. Furthermore, it could be shown that by dataset fusion, generated\nintra-operative images can be leveraged as additional training data for the\ndetection network itself. The data is released within the scope of the AdaptOR\nMICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at\nhttps://github.com/Cardio-AI/detcyclegan_pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1\">Lalith Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_G/0/1/0/all/0/1\">Gabriele Romano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1\">Sven Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelm_H/0/1/0/all/0/1\">Halvar Kelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karck_M/0/1/0/all/0/1\">Matthias Karck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1\">Raffaele De Simone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triplet is All You Need with Random Mappings for Unsupervised Visual Representation Learning. (arXiv:2107.10419v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10419","description":"<p>Contrastive self-supervised learning (SSL) has achieved great success in\nunsupervised visual representation learning by maximizing the similarity\nbetween two augmented views of the same image (positive pairs) and\nsimultaneously contrasting other different images (negative pairs). However,\nthis type of methods, such as SimCLR and MoCo, relies heavily on a large number\nof negative pairs and thus requires either large batches or memory banks. In\ncontrast, some recent non-contrastive SSL methods, such as BYOL and SimSiam,\nattempt to discard negative pairs by introducing asymmetry and show remarkable\nperformance. Unfortunately, to avoid collapsed solutions caused by not using\nnegative pairs, these methods require sophisticated asymmetry designs. In this\npaper, we argue that negative pairs are still necessary but one is sufficient,\ni.e., triplet is all you need. A simple triplet-based loss can achieve\nsurprisingly good performance without requiring large batches or asymmetry.\nMoreover, we observe that unsupervised visual representation learning can gain\nsignificantly from randomness. Based on this observation, we propose a simple\nplug-in RandOm MApping (ROMA) strategy by randomly mapping samples into other\nspaces and enforcing these randomly projected samples to satisfy the same\ncorrelation requirement. The proposed ROMA strategy not only achieves the\nstate-of-the-art performance in conjunction with the triplet-based loss, but\nalso can further effectively boost other SSL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuesong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_M/0/1/0/all/0/1\">Meihao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.13625","description":"<p>Ensuring trusted artificial intelligence (AI) in the real world is an\ncritical challenge. A still largely unexplored task is the determination of the\nmajor factors within the real world that affect the behavior and robustness of\na given AI module (e.g. weather or illumination conditions). Specifically, here\nwe seek to discover the factors that cause AI systems to fail, and to mitigate\ntheir influence. The identification of these factors usually heavily relies on\nthe availability of data that is diverse enough to cover numerous combinations\nof these factors, but the exhaustive collection of this data is onerous and\nsometimes impossible in complex environments. This paper investigates methods\nthat discover and mitigate the effects of semantic sensitive factors within a\ngiven dataset. We also here generalize the definition of fairness, which\nnormally only addresses socially relevant factors, and widen it to deal with --\nmore broadly -- the desensitization of AI systems with regard to all possible\naspects of variation in the domain. The proposed methods which discover these\nmajor factors reduce the potentially onerous demands of collecting a\nsufficiently diverse dataset. In experiments using road sign (GTSRB) and facial\nimagery (CelebA) datasets, we show the promise of these new methods and show\nthat they outperform state of the art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTT: Point-Track-Transformer Module for 3D Single Object Tracking in Point Clouds. (arXiv:2108.06455v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06455","description":"<p>3D single object tracking is a key issue for robotics. In this paper, we\npropose a transformer module called Point-Track-Transformer (PTT) for point\ncloud-based 3D single object tracking. PTT module contains three blocks for\nfeature embedding, position encoding, and self-attention feature computation.\nFeature embedding aims to place features closer in the embedding space if they\nhave similar semantic information. Position encoding is used to encode\ncoordinates of point clouds into high dimension distinguishable features.\nSelf-attention generates refined attention features by computing attention\nweights. Besides, we embed the PTT module into the open-source state-of-the-art\nmethod P2B to construct PTT-Net. Experiments on the KITTI dataset reveal that\nour PTT-Net surpasses the state-of-the-art by a noticeable margin (~10\\%).\nAdditionally, PTT-Net could achieve real-time performance (~40FPS) on NVIDIA\n1080Ti GPU. Our code is open-sourced for the robotics community at\nhttps://github.com/shanjiayao/PTT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Jiayao Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sifan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yubo Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation. (arXiv:2108.09897v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09897","description":"<p>This paper addresses weakly supervised amodal instance segmentation, where\nthe goal is to segment both visible and occluded (amodal) object parts, while\ntraining provides only ground-truth visible (modal) segmentations. Following\nprior work, we use data manipulation to generate occlusions in training images\nand thus train a segmenter to predict amodal segmentations of the manipulated\ndata. The resulting predictions on training images are taken as the\npseudo-ground truth for the standard training of Mask-RCNN, which we use for\namodal instance segmentation of test images. For generating the pseudo-ground\ntruth, we specify a new Amodal Segmenter based on Boundary Uncertainty\nestimation (ASBU) and make two contributions. First, while prior work uses the\noccluder's mask, our ASBU uses the occlusion boundary as input. Second, ASBU\nestimates an uncertainty map of the prediction. The estimated uncertainty\nregularizes learning such that lower segmentation loss is incurred on regions\nwith high uncertainty. ASBU achieves significant performance improvement\nrelative to the state of the art on the COCOA and KINS datasets in three tasks:\namodal instance segmentation, amodal completion, and ordering recovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorovic_S/0/1/0/all/0/1\">Sinisa Todorovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11250","description":"<p>A panoptic driving perception system is an essential part of autonomous\ndriving. A high-precision and real-time perception system can assist the\nvehicle in making the reasonable decision while driving. We present a panoptic\ndriving perception network (YOLOP) to perform traffic object detection,\ndrivable area segmentation and lane detection simultaneously. It is composed of\none encoder for feature extraction and three decoders to handle the specific\ntasks. Our model performs extremely well on the challenging BDD100K dataset,\nachieving state-of-the-art on all three tasks in terms of accuracy and speed.\nBesides, we verify the effectiveness of our multi-task learning model for joint\ntraining via ablative studies. To our best knowledge, this is the first work\nthat can process these three visual perception tasks simultaneously in\nreal-time on an embedded device Jetson TX2(23 FPS) and maintain excellent\naccuracy. To facilitate further research, the source codes and pre-trained\nmodels will be released at https://github.com/hustvl/YOLOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Manwen Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-to-Graph Convolutional Network for Deformable Shape Reconstruction from a Single Projection Image. (arXiv:2108.12533v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.12533","description":"<p>Shape reconstruction of deformable organs from two-dimensional X-ray images\nis a key technology for image-guided intervention. In this paper, we propose an\nimage-to-graph convolutional network (IGCN) for deformable shape reconstruction\nfrom a single-viewpoint projection image. The IGCN learns relationship between\nshape/deformation variability and the deep image features based on a\ndeformation mapping scheme. In experiments targeted to the respiratory motion\nof abdominal organs, we confirmed the proposed framework with a regularized\nloss function can reconstruct liver shapes from a single digitally\nreconstructed radiograph with a mean distance error of 3.6mm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nakao_M/0/1/0/all/0/1\">M. Nakao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_F/0/1/0/all/0/1\">F. Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nakamura_M/0/1/0/all/0/1\">M. Nakamura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matsuda_T/0/1/0/all/0/1\">T. Matsuda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-and-Language Navigation: A Survey and Taxonomy. (arXiv:2108.11544v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.11544","description":"<p>An agent that can understand natural-language instruction and carry out\ncorresponding actions in the visual world is one of the long-term challenges of\nArtificial Intelligent (AI). Due to multifarious instructions from humans, it\nrequires the agent can link natural language to vision and action in\nunstructured, previously unseen environments. If the instruction given by human\nis a navigation task, this challenge is called Visual-and-Language Navigation\n(VLN). It is a booming multi-disciplinary field of increasing importance and\nwith extraordinary practicality. Instead of focusing on the details of specific\nmethods, this paper provides a comprehensive survey on VLN tasks and makes a\nclassification carefully according the different characteristics of language\ninstructions in these tasks. According to when the instructions are given, the\ntasks can be divided into single-turn and multi-turn. For single-turn tasks, we\nfurther divided them into goal-orientation and route-orientation based on\nwhether the instructions contain a route. For multi-turn tasks, we divided them\ninto imperative task and interactive task based on whether the agent responses\nto the instructions. This taxonomy enable researchers to better grasp the key\npoint of a specific task and identify directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wansen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinmeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}