{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"mGPT: Few-Shot Learners Go Multilingual. (arXiv:2204.07580v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07580","description":"<p>Recent studies report that autoregressive language models can successfully\nsolve many NLP tasks via zero- and few-shot learning paradigms, which opens up\nnew possibilities for using the pre-trained language models. This paper\nintroduces two autoregressive GPT-like models with 1.3 billion and 13 billion\nparameters trained on 60 languages from 25 language families using Wikipedia\nand Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using\nGPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron\nframeworks allow us to parallelize the training and inference steps\neffectively. The resulting models show performance on par with the recently\nreleased XGLM models by Facebook, covering more languages and enhancing NLP\npossibilities for low resource languages of CIS countries and Russian small\nnations. We detail the motivation for the choices of the architecture design,\nthoroughly describe the data preparation pipeline, and train five small\nversions of the model to choose the most optimal multilingual tokenization\nstrategy. We measure the model perplexity in all covered languages and evaluate\nit on the wide spectre of multilingual tasks, including classification,\ngenerative, sequence labeling and knowledge probing. The models were evaluated\nwith the zero-shot and few-shot methods. Furthermore, we compared the\nclassification tasks with the state-of-the-art multilingual model XGLM. source\ncode and the mGPT XL model are publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shliazhko_O/0/1/0/all/0/1\">Oleh Shliazhko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1\">Alena Fenogenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonova_M/0/1/0/all/0/1\">Maria Tikhonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozlova_A/0/1/0/all/0/1\">Anastasia Kozlova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-BoXBART: Get Instructions into Biomedical Multi-Task Learning. (arXiv:2204.07600v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07600","description":"<p>Single-task models have proven pivotal in solving specific tasks; however,\nthey have limitations in real-world applications where multi-tasking is\nnecessary and domain shifts are exhibited. Recently, instructional prompts have\nshown significant improvement towards multi-task generalization; however, the\neffect of instructional prompts and Multi-Task Learning (MTL) has not been\nsystematically studied in the biomedical domain. Motivated by this, this paper\nexplores the impact of instructional prompts for biomedical MTL. We introduce\nthe BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)\nvarious categories. Using this meta-dataset, we propose a unified model termed\nIn-BoXBART, that can jointly learn all tasks of the BoX without any\ntask-specific modules. To the best of our knowledge, this is the first attempt\nto propose a unified model in the biomedical domain and use instructions to\nachieve generalization across several biomedical tasks. Experimental results\nindicate that the proposed model: 1) outperforms the single-task baseline by\n~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)\nshows ~23% improvement compared to the single-task baseline in few-shot\nlearning (i.e., 32 instances per task) on an average. Our analysis indicates\nthat there is significant room for improvement across tasks in the BoX,\nimplying the scope for future research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_M/0/1/0/all/0/1\">Mirali Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murad_M/0/1/0/all/0/1\">M. Hassan Murad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Adapt Domain Shifts of Moral Values via Instance Weighting. (arXiv:2204.07603v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07603","description":"<p>Classifying moral values in user-generated text from social media is critical\nin understanding community cultures and interpreting user behaviors of social\nmovements. Moral values and language usage can change across the social\nmovements; however, text classifiers are usually trained in source domains of\nexisting social movements and tested in target domains of new social issues\nwithout considering the variations. In this study, we examine domain shifts of\nmoral values and language usage, quantify the effects of domain shifts on the\nmorality classification task, and propose a neural adaptation framework via\ninstance weighting to improve cross-domain classification tasks. The\nquantification analysis suggests a strong correlation between morality shifts,\nlanguage usage, and classification performance. We evaluate the neural\nadaptation framework on a public Twitter data across 7 social movements and\ngain classification improvements up to 12.1\\%. Finally, we release a new data\nof the COVID-19 vaccine labeled with moral values and evaluate our approach on\nthe new target domain. For the case study of the COVID-19 vaccine, our\nadaptation framework achieves up to 5.26\\% improvements over neural baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wormley_A/0/1/0/all/0/1\">Alexandra Wormley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Adam Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate detection of sepsis at ED triage using machine learning with clinical natural language processing. (arXiv:2204.07657v1 [cs.LG])","link":"http://arxiv.org/abs/2204.07657","description":"<p>Sepsis is a life-threatening condition with organ dysfunction and is a\nleading cause of death and critical illness worldwide. Accurate detection of\nsepsis during emergency department triage would allow early initiation of lab\nanalysis, antibiotic administration, and other sepsis treatment protocols. The\npurpose of this study was to determine whether EHR data can be extracted and\nsynthesized with the latest machine learning algorithms (KATE Sepsis) and\nclinical natural language processing to produce accurate sepsis models, and\ncompare KATE Sepsis performance with existing sepsis screening protocols, such\nas SIRS and qSOFA. A machine learning model (KATE Sepsis) was developed using\npatient encounters with triage data from 16 participating hospitals. KATE\nSepsis, SIRS, standard screening (SIRS with source of infection) and qSOFA were\ntested in three settings. Cohort-A was a retrospective analysis on medical\nrecords from a single Site 1. Cohort-B was a prospective analysis of Site 1.\nCohort-C was a retrospective analysis on Site 1 with 15 additional sites.\nAcross all cohorts, KATE Sepsis demonstrates an AUC of 0.94-0.963 with\n73-74.87% TPR and 3.76-7.17% FPR. Standard screening demonstrates an AUC of\n0.682-0.726 with 39.39-51.19% TPR and 2.9-6.02% FPR. The qSOFA protocol\ndemonstrates an AUC of 0.544-0.56, with 10.52-13.18% TPR and 1.22-1.68% FPR.\nFor severe sepsis, across all cohorts, KATE Sepsis demonstrates an AUC of\n0.935-0.972 with 70-82.26% TPR and 4.64-8.62% FPR. For septic shock, across all\ncohorts, KATE Sepsis demonstrates an AUC of 0.96-0.981 with 85.71-89.66% TPR\nand 4.85-8.8% FPR. SIRS, standard screening, and qSOFA demonstrate low AUC and\nTPR for severe sepsis and septic shock detection. KATE Sepsis provided\nsubstantially better sepsis detection performance in triage than commonly used\nscreening protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_O/0/1/0/all/0/1\">Oleksandr Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molander_K/0/1/0/all/0/1\">Karen Molander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunne_R/0/1/0/all/0/1\">Robert Dunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Stephen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masek_K/0/1/0/all/0/1\">Kevin Masek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_E/0/1/0/all/0/1\">Erica Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lisa Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Travers_D/0/1/0/all/0/1\">Debbie Travers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brecher_D/0/1/0/all/0/1\">Deena Brecher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaney_D/0/1/0/all/0/1\">Deb Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_K/0/1/0/all/0/1\">Kyla Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reilly_C/0/1/0/all/0/1\">Christian Reilly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection. (arXiv:2204.07660v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07660","description":"<p>Datasets that capture the connection between vision, language, and affection\nare limited, causing a lack of understanding of the emotional aspect of human\nintelligence. As a step in this direction, the ArtEmis dataset was recently\nintroduced as a large-scale dataset of emotional reactions to images along with\nlanguage explanations of these chosen emotions. We observed a significant\nemotional bias towards instance-rich emotions, making trained neural speakers\nless accurate in describing under-represented emotions. We show that collecting\nnew data, in the same way, is not effective in mitigating this emotional bias.\nTo remedy this problem, we propose a contrastive data collection approach to\nbalance ArtEmis with a new complementary dataset such that a pair of similar\nimages have contrasting emotions (one positive and one negative). We collected\n260,533 instances using the proposed method, we combine them with ArtEmis,\ncreating a second iteration of the dataset. The new combined dataset, dubbed\nArtEmis v2.0, has a balanced distribution of emotions with explanations\nrevealing more fine details in the associated painting. Our experiments show\nthat neural speakers trained on the new dataset improve CIDEr and METEOR\nevaluation metrics by 20% and 7%, respectively, compared to the biased dataset.\nFinally, we also show that the performance per emotion of neural speakers is\nimproved across all the emotion categories, significantly on under-represented\nemotions. The collected dataset and code are available at\nhttps://artemisdataset-v2.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_Y/0/1/0/all/0/1\">Youssef Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Faizan Farooq Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haydarov_K/0/1/0/all/0/1\">Kilichbek Haydarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairly Accurate: Learning Optimal Accuracy vs. Fairness Tradeoffs for Hate Speech Detection. (arXiv:2204.07661v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07661","description":"<p>Recent work has emphasized the importance of balancing competing objectives\nin model training (e.g., accuracy vs. fairness, or competing measures of\nfairness). Such trade-offs reflect a broader class of multi-objective\noptimization (MOO) problems in which optimization methods seek Pareto optimal\ntrade-offs between competing goals. In this work, we first introduce a\ndifferentiable measure that enables direct optimization of group fairness\n(specifically, balancing accuracy across groups) in model training. Next, we\ndemonstrate two model-agnostic MOO frameworks for learning Pareto optimal\nparameterizations over different groups of neural classification models. We\nevaluate our methods on the specific task of hate speech detection, in which\nprior work has shown lack of group fairness across speakers of different\nEnglish dialects. Empirical results across convolutional, sequential, and\ntransformer-based neural architectures show superior empirical accuracy vs.\nfairness trade-offs over prior work. More significantly, our measure enables\nthe Pareto machinery to ensure that each architecture achieves the best\npossible trade-off between fairness and accuracy w.r.t. the dataset, given\nuser-prescribed error tolerance bounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1\">Venelin Kovatchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Soumyajit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spanish Abstract Meaning Representation: Annotation of a General Corpus. (arXiv:2204.07663v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07663","description":"<p>The Abstract Meaning Representation (AMR) formalism, designed originally for\nEnglish, has been adapted to a number of languages. We build on previous work\nproposing the annotation of AMR in Spanish, which resulted in the release of 50\nSpanish AMR annotations for the fictional text \"The Little Prince.\" In this\nwork, we present the first sizable, general annotation project for Spanish\nAbstract Meaning Representation. Our approach to annotation makes use of\nSpanish rolesets from the AnCora-Net lexicon and extends English AMR with\nsemantic features specific to Spanish. In addition to our guidelines, we\nrelease an annotated corpus (586 annotations total, for 486 unique sentences)\nof multiple genres of documents from the \"Abstract Meaning Representation 2.0 -\nFour Translations\" sembank. This corpus is commonly used for evaluation of AMR\nparsing and generation, but does not include gold AMRs; we hope that providing\ngold annotations for this dataset can result in a more complete approach to\ncross-lingual AMR parsing. Finally, we perform a disagreement analysis and\ndiscuss the implications of our work on the adaptability of AMR to languages\nother than English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wein_S/0/1/0/all/0/1\">Shira Wein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donatelli_L/0/1/0/all/0/1\">Lucia Donatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricker_E/0/1/0/all/0/1\">Ethan Ricker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engstrom_C/0/1/0/all/0/1\">Calvin Engstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_A/0/1/0/all/0/1\">Alex Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Fine-tune Twice: Selective Differential Privacy for Large Language Models. (arXiv:2204.07667v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07667","description":"<p>With the increasing adoption of NLP models in real-world products, it becomes\nmore and more important to protect these models from privacy leakage. Because\nprivate information in language data is sparse, previous research formalized a\nSelective-Differential-Privacy (SDP) notion to provide protection for sensitive\ntokens detected by policy functions, and prove its effectiveness on RNN-based\nmodels. But the previous mechanism requires separating the private and public\nmodel parameters and thus cannot be applied on large attention-based models. In\nthis paper, we propose a simple yet effective just-fine-tune-twice privacy\nmechanism to first fine-tune on in-domain redacted data and then on in-domain\nprivate data, to achieve SDP for large Transformer-based language models. We\nalso design explicit and contextual policy functions to provide protections at\ndifferent levels. Experiments show that our models achieve strong performance\nwhile staying robust to the canary insertion attack. We further show that even\nunder low-resource settings with a small amount of in-domain data, SDP can\nstill improve the model utility. We will release the code, data and models to\nfacilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge Distillation. (arXiv:2204.07674v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07674","description":"<p>Knowledge distillation (KD) is an efficient framework for compressing\nlarge-scale pre-trained language models. Recent years have seen a surge of\nresearch aiming to improve KD by leveraging Contrastive Learning, Intermediate\nLayer Distillation, Data Augmentation, and Adversarial Training. In this work,\nwe propose a learning based data augmentation technique tailored for knowledge\ndistillation, called CILDA. To the best of our knowledge, this is the first\ntime that intermediate layer representations of the main task are used in\nimproving the quality of augmented samples. More precisely, we introduce an\naugmentation technique for KD based on intermediate layer matching using\ncontrastive loss to improve masked adversarial data augmentation. CILDA\noutperforms existing state-of-the-art KD approaches on the GLUE benchmark, as\nwell as in an out-of-domain evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haidar_M/0/1/0/all/0/1\">Md Akmal Haidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_K/0/1/0/all/0/1\">Khalil Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation. (arXiv:2204.07675v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07675","description":"<p>Pre-trained language models have demonstrated superior performance in various\nnatural language processing tasks. However, these models usually contain\nhundreds of millions of parameters, which limits their practicality because of\nlatency requirements in real-world applications. Existing methods train small\ncompressed models via knowledge distillation. However, performance of these\nsmall models drops significantly compared with the pre-trained models due to\ntheir reduced model capacity. We propose MoEBERT, which uses a\nMixture-of-Experts structure to increase model capacity and inference speed. We\ninitialize MoEBERT by adapting the feed-forward neural networks in a\npre-trained model into multiple experts. As such, representation power of the\npre-trained model is largely retained. During inference, only one of the\nexperts is activated, such that speed can be improved. We also propose a\nlayer-wise distillation method to train MoEBERT. We validate the efficiency and\neffectiveness of MoEBERT on natural language understanding and question\nanswering tasks. Results show that the proposed method outperforms existing\ntask-specific distillation algorithms. For example, our method outperforms\nprevious approaches by over 2% on the MNLI (mismatched) dataset. Our code is\npublicly available at https://github.com/SimiaoZuo/MoEBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling. (arXiv:2204.07679v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07679","description":"<p>Retrieval-based conversational systems learn to rank response candidates for\na given dialogue context by computing the similarity between their vector\nrepresentations. However, training on a single textual form of the multi-turn\ncontext limits the ability of a model to learn representations that generalize\nto natural perturbations seen during inference. In this paper we propose a\nframework that incorporates augmented versions of a dialogue context into the\nlearning objective. We utilize contrastive learning as an auxiliary objective\nto learn robust dialogue context representations that are invariant to\nperturbations injected through the augmentation method. We experiment with four\nbenchmark dialogue datasets and demonstrate that our framework combines well\nwith existing augmentation methods and can significantly improve over baseline\nBERT-based ranking architectures. Furthermore, we propose a novel data\naugmentation method, ConMix, that adds token level perturbations through\nstochastic mixing of tokens from other contexts in the batch. We show that our\nproposed augmentation method outperforms previous data augmentation approaches,\nand provides dialogue representations that are more robust to common\nperturbations seen during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poddar_L/0/1/0/all/0/1\">Lahari Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinspach_J/0/1/0/all/0/1\">Julia Reinspach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners. (arXiv:2204.07689v1 [cs.LG])","link":"http://arxiv.org/abs/2204.07689","description":"<p>Traditional multi-task learning (MTL) methods use dense networks that use the\nsame set of shared weights across several different tasks. This often creates\ninterference where two or more tasks compete to pull model parameters in\ndifferent directions. In this work, we study whether sparsely activated\nMixture-of-Experts (MoE) improve multi-task learning by specializing some\nweights for learning shared representations and using the others for learning\ntask-specific information. To this end, we devise task-aware gating functions\nto route examples from different tasks to specialized experts which share\nsubsets of network weights conditioned on the task. This results in a sparsely\nactivated multi-task model with a large number of parameters, but with the same\ncomputational cost as that of a dense model. We demonstrate such sparse\nnetworks to improve multi-task learning along three key dimensions: (i)\ntransfer to low-resource tasks from related tasks in the training mixture; (ii)\nsample-efficient generalization to tasks not seen during training by making use\nof task-aware routing from seen related tasks; (iii) robustness to the addition\nof unrelated tasks by avoiding catastrophic forgetting of existing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shashank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subudhi_K/0/1/0/all/0/1\">Krishan Subudhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_E/0/1/0/all/0/1\">Eduardo Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_D/0/1/0/all/0/1\">Damien Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed H. Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes. (arXiv:2204.07693v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07693","description":"<p>Multi-hop Question Answering (QA) is a challenging task since it requires an\naccurate aggregation of information from multiple context paragraphs and a\nthorough understanding of the underlying reasoning chains. Recent work in\nmulti-hop QA has shown that performance can be boosted by first decomposing the\nquestions into simpler, single-hop questions. In this paper, we explore one\nadditional utility of the multi-hop decomposition from the perspective of\nexplainable NLP: to create explanation by probing a neural QA model with them.\nWe hypothesize that in doing so, users will be better able to construct a\nmental model of when the underlying QA system will give the correct answer.\nThrough human participant studies, we verify that exposing the decomposition\nprobes and answers to the probes to users can increase their ability to predict\nsystem performance on a question instance basis. We show that decomposition is\nan effective form of probing QA systems as well as a promising approach to\nexplanation generation. In-depth analyses show the need for improvements in\ndecomposition systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kaige Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Reinforcement Learning for Unsupervised Controlled Text Generation. (arXiv:2204.07696v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07696","description":"<p>Controlled text generation tasks such as unsupervised text style transfer\nhave increasingly adopted the use of Reinforcement Learning (RL). A major\nchallenge in applying RL to such tasks is the sparse reward, which is available\nonly after the full text is generated. Sparse rewards, combined with a large\naction space make RL training sample-inefficient and difficult to converge.\nRecently proposed reward-shaping strategies to address this issue have shown\nonly negligible gains. In contrast, this work proposes a novel approach that\nprovides dense rewards to each generated token. We evaluate our approach by its\nusage in unsupervised text style transfer. Averaged across datasets, our style\ntransfer system improves upon current state-of-art systems by 21\\% on human\nevaluation and 12\\% on automatic evaluation. Upon ablated comparison with the\ncurrent reward shaping approach (the `roll-out strategy'), using dense rewards\nimproves the overall style transfer quality by 22\\% based on human evaluation.\nFurther the RL training is 2.5 times as sample efficient, and 7 times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_B/0/1/0/all/0/1\">Bhargav Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudhakar_A/0/1/0/all/0/1\">Akhilesh Sudhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheswaran_A/0/1/0/all/0/1\">Arjun Maheswaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLCU-ICALL at SemEval-2022 Task 1: Cross-Attention Multitasking Framework for Definition Modeling. (arXiv:2204.07701v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07701","description":"<p>This paper describes the BLCU-ICALL system used in the SemEval-2022 Task 1\nComparing Dictionaries and Word Embeddings, the Definition Modeling subtrack,\nachieving 1st on Italian, 2nd on Spanish and Russian, and 3rd on English and\nFrench. We propose a transformer-based multitasking framework to explore the\ntask. The framework integrates multiple embedding architectures through the\ncross-attention mechanism, and captures the structure of glosses through a\nmasking language model objective. Additionally, we also investigate a simple\nbut effective model ensembling strategy to further improve the robustness. The\nevaluation results show the effectiveness of our solution. We release our code\nat: https://github.com/blcuicall/SemEval2022-Task1-DM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Cunliang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_R/0/1/0/all/0/1\">Ruining Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liner Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaping Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks. (arXiv:2204.07705v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07705","description":"<p>How can we measure the generalization of models to a variety of unseen tasks\nwhen provided with their language instructions? To facilitate progress in this\ngoal, we introduce Natural-Instructions v2, a collection of 1,600+ diverse\nlanguage tasks and their expert written instructions. More importantly, the\nbenchmark covers 70+ distinct task types, such as tagging, in-filling, and\nrewriting. This benchmark is collected with contributions of NLP practitioners\nin the community and through an iterative peer review process to ensure their\nquality. This benchmark enables large-scale evaluation of cross-task\ngeneralization of the models -- training on a subset of tasks and evaluating on\nthe remaining unseen ones. For instance, we are able to rigorously quantify\ngeneralization as a function of various scaling parameters, such as the number\nof observed tasks, the number of instances, and model sizes. As a by-product of\nthese experiments. we introduce Tk-Instruct, an encoder-decoder Transformer\nthat is trained to follow a variety of in-context instructions (plain language\ntask definitions or k-shot examples) which outperforms existing larger models\non our benchmark. We hope this benchmark facilitates future progress toward\nmore general-purpose language understanding models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordi_Y/0/1/0/all/0/1\">Yeganeh Kordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1\">Amirreza Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">Anjana Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1\">Arjun Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhanasekaran_A/0/1/0/all/0/1\">Arut Selvan Dhanasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stap_D/0/1/0/all/0/1\">David Stap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_E/0/1/0/all/0/1\">Eshaan Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamanolakis_G/0/1/0/all/0/1\">Giannis Karamanolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Haizhi Gary Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_I/0/1/0/all/0/1\">Ishan Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1\">Ishani Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_J/0/1/0/all/0/1\">Jacob Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznia_K/0/1/0/all/0/1\">Kirby Kuznia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_K/0/1/0/all/0/1\">Krima Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Maitreya Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_M/0/1/0/all/0/1\">Mirali Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaza_P/0/1/0/all/0/1\">Phani Rohitha Kaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Pulkit Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ravsehaj Singh Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karia_R/0/1/0/all/0/1\">Rushang Karia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampat_S/0/1/0/all/0/1\">Shailaja Keyur Sampat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_S/0/1/0/all/0/1\">Savan Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Sujan Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patro_S/0/1/0/all/0/1\">Sumanta Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_T/0/1/0/all/0/1\">Tanay Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xudong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TVShowGuess: Character Comprehension in Stories as Speaker Guessing. (arXiv:2204.07721v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07721","description":"<p>We propose a new task for assessing machines' skills of understanding\nfictional characters in narrative stories. The task, TVShowGuess, builds on the\nscripts of TV series and takes the form of guessing the anonymous main\ncharacters based on the backgrounds of the scenes and the dialogues. Our human\nstudy supports that this form of task covers comprehension of multiple types of\ncharacter persona, including understanding characters' personalities, facts and\nmemories of personal experience, which are well aligned with the psychological\nand literary theories about the theory of mind (ToM) of human beings on\nunderstanding fictional characters during reading. We further propose new model\narchitectures to support the contextualized encoding of long scene texts.\nExperiments show that our proposed approaches significantly outperform\nbaselines, yet still largely lag behind the (nearly perfect) human performance.\nOur work serves as a first step toward the goal of narrative character\ncomprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1\">Yisi Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1\">Xiangyang Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanton_J/0/1/0/all/0/1\">Jeffrey Stanton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persua: A Visual Interactive System to Enhance the Persuasiveness of Arguments in Online Discussion. (arXiv:2204.07741v1 [cs.HC])","link":"http://arxiv.org/abs/2204.07741","description":"<p>Persuading people to change their opinions is a common practice in online\ndiscussion forums on topics ranging from political campaigns to relationship\nconsultation. Enhancing people's ability to write persuasive arguments could\nnot only practice their critical thinking and reasoning but also contribute to\nthe effectiveness and civility in online communication. It is, however, not an\neasy task in online discussion settings where written words are the primary\ncommunication channel. In this paper, we derived four design goals for a tool\nthat helps users improve the persuasiveness of arguments in online discussions\nthrough a survey with 123 online forum users and interviews with five debating\nexperts. To satisfy these design goals, we analyzed and built a labeled dataset\nof fine-grained persuasive strategies (i.e., logos, pathos, ethos, and\nevidence) in 164 arguments with high ratings on persuasiveness from\nChangeMyView, a popular online discussion forum. We then designed an\ninteractive visual system, Persua, which provides example-based guidance on\npersuasive strategies to enhance the persuasiveness of arguments. In\nparticular, the system constructs portfolios of arguments based on different\npersuasive strategies applied to a given discussion topic. It then presents\nconcrete examples based on the difference between the portfolios of user input\nand high-quality arguments in the dataset. A between-subjects study shows\nsuggestive evidence that Persua encourages users to submit more times for\nfeedback and helps users improve more on the persuasiveness of their arguments\nthan a baseline system. Finally, a set of design considerations was summarized\nto guide future intelligent systems that improve the persuasiveness in text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Meng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nei_F/0/1/0/all/0/1\">Fei Nei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Attention-based Sentence-Level Meta-Embeddings from Contextualised Language Models. (arXiv:2204.07746v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07746","description":"<p>A variety of contextualised language models have been proposed in the NLP\ncommunity, which are trained on diverse corpora to produce numerous Neural\nLanguage Models (NLMs). However, different NLMs have reported different levels\nof performances in downstream NLP applications when used as text\nrepresentations. We propose a sentence-level meta-embedding learning method\nthat takes independently trained contextualised word embedding models and\nlearns a sentence embedding that preserves the complementary strengths of the\ninput source NLMs. Our proposed method is unsupervised and is not tied to a\nparticular downstream task, which makes the learnt meta-embeddings in principle\napplicable to different tasks that require sentence representations.\nSpecifically, we first project the token-level embeddings obtained by the\nindividual NLMs and learn attention weights that indicate the contributions of\nsource embeddings towards their token-level meta-embeddings. Next, we apply\nmean and max pooling to produce sentence-level meta-embeddings from token-level\nmeta-embeddings. Experimental results on semantic textual similarity benchmarks\nshow that our proposed unsupervised sentence-level meta-embedding method\noutperforms previously proposed sentence-level meta-embedding methods as well\nas a supervised baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1\">Keigo Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue. (arXiv:2204.07770v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07770","description":"<p>The goal-oriented document-grounded dialogue aims at responding to the user\nquery based on the dialogue context and supporting document. Existing studies\ntackle this problem by decomposing it into two sub-tasks: knowledge\nidentification and response generation. However, such pipeline methods would\nunavoidably suffer from the error propagation issue. This paper proposes to\nunify these two sub-tasks via sequentially generating the grounding knowledge\nand the response. We further develop a prompt-connected multi-task learning\nstrategy to model the characteristics and connections of different tasks and\nintroduce linear temperature scheduling to reduce the negative effect of\nirrelevant document information. Experimental results demonstrate the\neffectiveness of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TASTEset -- Recipe Dataset and Food Entities Recognition Benchmark. (arXiv:2204.07775v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07775","description":"<p>Food Computing is currently a fast-growing field of research. Natural\nlanguage processing (NLP) is also increasingly essential in this field,\nespecially for recognising food entities. However, there are still only a few\nwell-defined tasks that serve as benchmarks for solutions in this area. We\nintroduce a new dataset -- called \\textit{TASTEset} -- to bridge this gap. In\nthis dataset, Named Entity Recognition (NER) models are expected to find or\ninfer various types of entities helpful in processing recipes, e.g.~food\nproducts, quantities and their units, names of cooking processes, physical\nquality of ingredients, their purpose, taste.\n</p>\n<p>The dataset consists of 700 recipes with more than 13,000 entities to\nextract. We provide a few state-of-the-art baselines of named entity\nrecognition models, which show that our dataset poses a solid challenge to\nexisting models. The best model achieved, on average, 0.95 $F_1$ score,\ndepending on the entity type -- from 0.781 to 0.982. We share the dataset and\nthe task to encourage progress on more in-depth and complex information\nextraction from recipes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Ania Wr&#xf3;blewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaliska_A/0/1/0/all/0/1\">Agnieszka Kaliska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawlowski_M/0/1/0/all/0/1\">Maciej Paw&#x142;owski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_D/0/1/0/all/0/1\">Dawid Wi&#x15b;niewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnowski_W/0/1/0/all/0/1\">Witold Sosnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrynowicz_A/0/1/0/all/0/1\">Agnieszka &#x141;awrynowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimpleBERT: A Pre-trained Model That Learns to Generate Simple Words. (arXiv:2204.07779v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07779","description":"<p>Pre-trained models are widely used in the tasks of natural language\nprocessing nowadays. However, in the specific field of text simplification, the\nresearch on improving pre-trained models is still blank. In this work, we\npropose a continued pre-training method for text simplification. Specifically,\nwe propose a new masked language modeling (MLM) mechanism, which does not\nrandomly mask words but only masks simple words. The new mechanism can make the\nmodel learn to generate simple words. We use a small-scale simple text dataset\nfor continued pre-training and employ two methods to identify simple words from\nthe texts. We choose BERT, a representative pre-trained model, and continue\npre-training it using our proposed method. Finally, we obtain SimpleBERT, which\nsurpasses BERT in both lexical simplification and sentence simplification tasks\nand has achieved state-of-the-art results on multiple datasets. What's more,\nSimpleBERT can replace BERT in existing simplification models without\nmodification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Renliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unification of Discourse Annotation Frameworks. (arXiv:2204.07781v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07781","description":"<p>Discourse information is difficult to represent and annotate. Among the major\nframeworks for annotating discourse information, RST, PDTB and SDRT are widely\ndiscussed and used, each having its own theoretical foundation and focus.\nCorpora annotated under different frameworks vary considerably. To make better\nuse of the existing discourse corpora and achieve the possible synergy of\ndifferent frameworks, it is worthwhile to investigate the systematic relations\nbetween different frameworks and devise methods of unifying the frameworks.\nAlthough the issue of framework unification has been a topic of discussion for\na long time, there is currently no comprehensive approach which considers\nunifying both discourse structure and discourse relations and evaluates the\nunified framework intrinsically and extrinsically. We plan to use automatic\nmeans for the unification task and evaluate the result with structural\ncomplexity and downstream tasks. We will also explore the application of the\nunified framework in multi-task learning and graphical models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingxue Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Hard Negative Entities for Entity Set Expansion. (arXiv:2204.07789v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07789","description":"<p>Entity Set Expansion (ESE) is a promising task which aims to expand entities\nof the target semantic class described by a small seed entity set. Various NLP\nand IR applications will benefit from ESE due to its ability to discover\nknowledge. Although previous ESE methods have achieved great progress, most of\nthem still lack the ability to handle hard negative entities (i.e., entities\nthat are difficult to distinguish from the target entities), since two entities\nmay or may not belong to the same semantic class based on different granularity\nlevels we analyze on. To address this challenge, we devise an entity-level\nmasked language model with contrastive learning to refine the representation of\nentities. In addition, we propose the ProbExpan, a novel probabilistic ESE\nframework utilizing the entity representation obtained by the aforementioned\nlanguage model to expand entities. Extensive experiments and detailed analyses\non three datasets show that our method outperforms previous state-of-the-art\nmethods. The source codes of this paper are available at\nhttps://github.com/geekjuruo/ProbExpan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Inference for Counting on Semi-structured Tables. (arXiv:2204.07803v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07803","description":"<p>Recently, the Natural Language Inference (NLI) task has been studied for\nsemi-structured tables that do not have a strict format. Although neural\napproaches have achieved high performance in various types of NLI, including\nNLI between semi-structured tables and texts, they still have difficulty in\nperforming a numerical type of inference, such as counting. To handle a\nnumerical type of inference, we propose a logical inference system for\nreasoning between semi-structured tables and texts. We use logical\nrepresentations as meaning representations for tables and texts and use model\nchecking to handle a numerical type of inference between texts and tables. To\nevaluate the extent to which our system can perform inference with numerical\ncomparatives, we make an evaluation protocol that focuses on numerical\nunderstanding between semi-structured tables and texts in English. We show that\nour system can more robustly perform inference between tables and texts that\nrequires numerical understanding compared with current neural approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurosawa_T/0/1/0/all/0/1\">Tomoya Kurosawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1\">Hitomi Yanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Classify Open Intent via Soft Labeling and Manifold Mixup. (arXiv:2204.07804v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07804","description":"<p>Open intent classification is a practical yet challenging task in dialogue\nsystems. Its objective is to accurately classify samples of known intents while\nat the same time detecting those of open (unknown) intents. Existing methods\nusually use outlier detection algorithms combined with K-class classifier to\ndetect open intents, where K represents the class number of known intents.\nDifferent from them, in this paper, we consider another way without using\noutlier detection algorithms. Specifically, we directly train a (K+1)-class\nclassifier for open intent classification, where the (K+1)-th class represents\nopen intents. To address the challenge that training a (K+1)-class classifier\nwith training samples of only K classes, we propose a deep model based on Soft\nLabeling and Manifold Mixup (SLMM). In our method, soft labeling is used to\nreshape the label distribution of the known intent samples, aiming at reducing\nmodel's overconfident on known intents. Manifold mixup is used to generate\npseudo samples for open intents, aiming at well optimizing the decision\nboundary of open intents. Experiments on four benchmark datasets demonstrate\nthat our method outperforms previous methods and achieves state-of-the-art\nperformance. All the code and data of this work can be obtained at\nhttps://github.com/zifengcheng/SLMM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zifeng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yafeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qing Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Contrastive Cross-Channel Data Augmentation Framework for Aspect-based Sentiment Analysis. (arXiv:2204.07832v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07832","description":"<p>Aspect-Based Sentiment Analysis is a fine-grained sentiment analysis task,\nwhich focuses on detecting the sentiment polarity towards the aspect in a\nsentence. However, it is always sensitive to the multi-aspect challenge, where\nfeatures of multiple aspects in a sentence will affect each other. To mitigate\nthis issue, we design a novel training framework, called Contrastive\nCross-Channel Data Augmentation (C3DA). A source sentence will be fed a\ndomain-specific generator to obtain some synthetic sentences and is\nconcatenated with these generated sentences to conduct supervised training and\nproposed contrastive training. To be specific, considering the limited ABSA\nlabeled data, we also introduce some parameter-efficient approaches to complete\nsentences generation. This novel generation method consists of an Aspect\nAugmentation Channel (AAC) to generate aspect-specific sentences and a Polarity\nAugmentation (PAC) to generate polarity-inverted sentences. According to our\nextensive experiments, our C3DA framework can outperform those baselines\nwithout any augmentations by about 1\\% on accuracy and Macro-F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Ximing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Cross-Lingual Gaps During Leveraging the Multilingual Sequence-to-Sequence Pretraining for Text Generation. (arXiv:2204.07834v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07834","description":"<p>For multilingual sequence-to-sequence pretrained language models\n(multilingual Seq2Seq PLMs), e.g. mBART, the self-supervised pretraining task\nis trained on a wide range of monolingual languages, e.g. 25 languages from\ncommoncrawl, while the downstream cross-lingual tasks generally progress on a\nbilingual language subset, e.g. English-German, making there exists the\ncross-lingual data discrepancy, namely \\textit{domain discrepancy}, and\ncross-lingual learning objective discrepancy, namely \\textit{task discrepancy},\nbetween the pretrain and finetune stages. To bridge the above cross-lingual\ndomain and task gaps, we extend the vanilla pretrain-finetune pipeline with\nextra code-switching restore task. Specifically, the first stage employs the\nself-supervised code-switching restore task as a pretext task, allowing the\nmultilingual Seq2Seq PLM to acquire some in-domain alignment information. And\nfor the second stage, we continuously fine-tune the model on labeled data\nnormally. Experiments on a variety of cross-lingual NLG tasks, including 12\nbilingual translation tasks, 36 zero-shot translation tasks, and cross-lingual\nsummarization tasks show our model outperforms the strong baseline mBART\nconsistently. Comprehensive analyses indicate our approach could narrow the\ncross-lingual sentence representation distance and improve low-frequency word\ntranslation with trivial computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zan_C/0/1/0/all/0/1\">Changtong Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What If: Generating Code to Answer Simulation Questions. (arXiv:2204.07835v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07835","description":"<p>Many texts, especially in chemistry and biology, describe complex processes.\nWe focus on texts that describe a chemical reaction process and questions that\nask about the process's outcome under different environmental conditions. To\nanswer questions about such processes, one needs to understand the interactions\nbetween the different entities involved in the process and to simulate their\nstate transitions during the process execution under different conditions. A\nstate transition is defined as the memory modification the program does to the\nvariables during the execution. We hypothesize that generating code and\nexecuting it to simulate the process will allow answering such questions. We,\ntherefore, define a domain-specific language (DSL) to represent processes. We\ncontribute to the community a unique dataset curated by chemists and annotated\nby computer scientists. The dataset is composed of process texts, simulation\nquestions, and their corresponding computer codes represented by the DSL.We\npropose a neural program synthesis approach based on reinforcement learning\nwith a novel state-transition semantic reward. The novel reward is based on the\nrun-time semantic similarity between the predicted code and the reference code.\nThis allows simulating complex process transitions and thus answering\nsimulation questions. Our approach yields a significant boost in accuracy for\nsimulation questions: 88\\% accuracy as opposed to 83\\% accuracy of the\nstate-of-the-art neural program synthesis approaches and 54\\% accuracy of\nstate-of-the-art end-to-end text-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peretz_G/0/1/0/all/0/1\">Gal Peretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLISS: Robust Sequence-to-Sequence Learning via Self-Supervised Input Representation. (arXiv:2204.07837v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07837","description":"<p>Data augmentations (DA) are the cores to achieving robust\nsequence-to-sequence learning on various natural language processing (NLP)\ntasks. However, most of the DA approaches force the decoder to make predictions\nconditioned on the perturbed input representation, underutilizing supervised\ninformation provided by perturbed input. In this work, we propose a\nframework-level robust sequence-to-sequence learning approach, named BLISS, via\nself-supervised input representation, which has the great potential to\ncomplement the data-level augmentation approaches. The key idea is to supervise\nthe sequence-to-sequence framework with both the \\textit{supervised}\n(\"input$\\rightarrow$output\") and \\textit{self-supervised} (\"perturbed\ninput$\\rightarrow$input\") information. We conduct comprehensive experiments to\nvalidate the effectiveness of BLISS on various tasks, including machine\ntranslation, grammatical error correction, and text summarization. The results\nshow that BLISS outperforms significantly the vanilla Transformer and\nconsistently works well across tasks than the other five contrastive baselines.\nExtensive analyses reveal that BLISS learns robust representations and rich\nlinguistic knowledge, confirming our claim. Source code will be released upon\npublication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dazhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STRATA: Word Boundaries & Phoneme Recognition From Continuous Urdu Speech using Transfer Learning, Attention, & Data Augmentation. (arXiv:2204.07848v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07848","description":"<p>Phoneme recognition is a largely unsolved problem in NLP, especially for\nlow-resource languages like Urdu. The systems that try to extract the phonemes\nfrom audio speech require hand-labeled phonetic transcriptions. This requires\nexpert linguists to annotate speech data with its relevant phonetic\nrepresentation which is both an expensive and a tedious task. In this paper, we\npropose STRATA, a framework for supervised phoneme recognition that overcomes\nthe data scarcity issue for low resource languages using a seq2seq neural\narchitecture integrated with transfer learning, attention mechanism, and data\naugmentation. STRATA employs transfer learning to reduce the network loss in\nhalf. It uses attention mechanism for word boundaries and frame alignment\ndetection which further reduces the network loss by 4% and is able to identify\nthe word boundaries with 92.2% accuracy. STRATA uses various data augmentation\ntechniques to further reduce the loss by 1.5% and is more robust towards new\nsignals both in terms of generalization and accuracy. STRATA is able to achieve\na Phoneme Error Rate of 16.5% and improves upon the state of the art by 1.1%\nfor TIMIT dataset (English) and 11.5% for CSaLT dataset (Urdu).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naeem_S/0/1/0/all/0/1\">Saad Naeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_O/0/1/0/all/0/1\">Omer Beg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVIBOT: A Smart Chatbot for Assistance and E-Awareness during COVID-19 Pandemic. (arXiv:2204.07851v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07851","description":"<p>The coronavirus pandemic has spread over the past two years in our highly\nconnected and information-dense society. Nonetheless, disseminating accurate\nand up-to-date information on the spread of this pandemic remains a challenge.\nIn this context, opting for a solution based on conversational artificial\nintelligence, also known under the name of the chatbot, is proving to be an\nunavoidable solution, especially since it has already shown its effectiveness\nin fighting the coronavirus crisis in several countries. This work proposes to\ndesign and implement a smart chatbot on the theme of COVID-19, called COVIBOT,\nwhich will be useful in the context of Saudi Arabia. COVIBOT is a\ngenerative-based contextual chatbot, which is built using machine learning APIs\nthat are offered by the cloud-based Azure Cognitive Services. Two versions of\nCOVIBOT are offered: English and Arabic versions. Use cases of COVIBOT are\ntested and validated using a scenario-based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Driss_M/0/1/0/all/0/1\">Maha Driss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almomani_I/0/1/0/all/0/1\">Iman Almomani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahmadi_L/0/1/0/all/0/1\">Leen Alahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhajjam_L/0/1/0/all/0/1\">Linah Alhajjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharbi_R/0/1/0/all/0/1\">Raghad Alharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alanazi_S/0/1/0/all/0/1\">Shahad Alanazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nigam@COLIEE-22: Legal Case Retrieval and Entailment using Cascading of Lexical and Semantic-based models. (arXiv:2204.07853v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07853","description":"<p>This paper describes our submission to the Competition on Legal Information\nExtraction/Entailment 2022 (COLIEE-2022) workshop on case law competition for\ntasks 1 and 2. Task 1 is a legal case retrieval task, which involves reading a\nnew case and extracting supporting cases from the provided case law corpus to\nsupport the decision. Task 2 is the legal case entailment task, which involves\nthe identification of a paragraph from existing cases that entails the decision\nin a relevant case. We employed the neural models Sentence-BERT and Sent2Vec\nfor semantic understanding and the traditional retrieval model BM25 for exact\nmatching in both tasks. As a result, our team (\"nigam\") ranked 5th among all\nthe teams in Tasks 1 and 2. Experimental results indicate that the traditional\nretrieval model BM25 still outperforms neural network-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nigam_S/0/1/0/all/0/1\">Shubham Kumar Nigam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1\">Navansh Goel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?. (arXiv:2204.07931v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07931","description":"<p>Knowledge-grounded conversational models are known to suffer from producing\nfactually invalid statements, a phenomenon commonly called hallucination. In\nthis work, we investigate the underlying causes of this phenomenon: is\nhallucination due to the training data, or to the models? We conduct a\ncomprehensive human study on both existing knowledge-grounded conversational\nbenchmarks and several state-of-the-art models. Our study reveals that the\nstandard benchmarks consist of &gt;60% hallucinated responses, leading to models\nthat not only hallucinate but even amplify hallucinations. Our findings raise\nimportant questions on the quality of existing datasets and models trained\nusing them. We make our annotations publicly available for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milton_S/0/1/0/all/0/1\">Sivan Milton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cross-Task Generalization via Retrieval Augmentation. (arXiv:2204.07937v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07937","description":"<p>Humans can perform unseen tasks by recalling relevant skills that are\nacquired previously and then generalizing them to the target tasks, even if\nthere is no supervision at all. In this paper, we aim to improve such\ncross-task generalization ability of massive multi-task language models such as\nT0 (Sanh et al., 2021) in an unsupervised setting. We propose a\nretrieval-augmentation method named ReCross that takes a few unlabelled\nexamples as queries to retrieve a small subset of upstream data and uses them\nto update the multi-task model for better generalization. Our empirical results\nshow that the proposed ReCross consistently outperforms non-retrieval baselines\nby a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kangmin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Chris Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Beiwen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2204.07955v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07955","description":"<p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment\nAnalysis (MABSA) has attracted increasing attention in recent years. However,\nprevious approaches either (i) use separately pre-trained visual and textual\nmodels, which ignore the crossmodal alignment or (ii) use vision-language\nmodels pre-trained with general pre-training tasks, which are inadequate to\nidentify finegrained aspects, opinions, and their alignments across modalities.\nTo tackle these limitations, we propose a task-specific Vision-Language\nPre-training framework for MABSA (VLPMABSA), which is a unified multimodal\nencoder-decoder architecture for all the pretraining and downstream tasks. We\nfurther design three types of task-specific pre-training tasks from the\nlanguage, vision, and multimodal modalities, respectively. Experimental results\nshow that our approach generally outperforms the state-of-the-art approaches on\nthree MABSA subtasks. Further analysis demonstrates the effectiveness of each\npretraining task. The source code is publicly released at\nhttps://github.com/NUSTM/VLP-MABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1\">Yan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yu_J/0/1/0/all/0/1\">Jianfei yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED. (arXiv:2204.07980v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07980","description":"<p>DocRED is a widely used dataset for document-level relation extraction. In\nthe large-scale annotation, a \\textit{recommend-revise} scheme is adopted to\nreduce the workload. Within this scheme, annotators are provided with candidate\nrelation instances from distant supervision, and they then manually supplement\nand remove relational facts based on the recommendations. However, when\ncomparing DocRED with a subset relabeled from scratch, we find that this scheme\nresults in a considerable amount of false negative samples and an obvious bias\ntowards popular entities and relations. Furthermore, we observe that the models\ntrained on DocRED have low recall on our relabeled dataset and inherit the same\nbias in the training data. Through the analysis of annotators' behaviors, we\nfigure out the underlying reason for the problems above: the scheme actually\ndiscourages annotators from supplementing adequate instances in the revision\nphase. We appeal to future research to take into consideration the issues with\nthe recommend-revise scheme when designing new models and annotation schemes.\nThe relabeled dataset is released at\n\\url{https://github.com/AndrewZhe/Revisit-DocRED}, to serve as a more reliable\ntest set of document RE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Quzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shibo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shengqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Effectively Learning of Knowledge in Continual Pre-training. (arXiv:2204.07994v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07994","description":"<p>Pre-trained language models (PLMs) like BERT have made significant progress\nin various downstream NLP tasks. However, by asking models to do cloze-style\ntests, recent work finds that PLMs are short in acquiring knowledge from\nunstructured text. To understand the internal behaviour of PLMs in retrieving\nknowledge, we first define knowledge-baring (K-B) tokens and knowledge-free\n(K-F) tokens for unstructured text and ask professional annotators to label\nsome samples manually. Then, we find that PLMs are more likely to give wrong\npredictions on K-B tokens and attend less attention to those tokens inside the\nself-attention module. Based on these observations, we develop two solutions to\nhelp the model learn more knowledge from unstructured text in a fully\nself-supervised manner. Experiments on knowledge-intensive tasks show the\neffectiveness of the proposed methods. To our best knowledge, we are the first\nto explore fully self-supervised learning of knowledge in continual\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fuli Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Named Entity Recognition as Holistic Structure Parsing. (arXiv:2204.08006v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08006","description":"<p>As a fundamental natural language processing task and one of core knowledge\nextraction techniques, named entity recognition (NER) is widely used to extract\ninformation from texts for downstream tasks. Nested NER is a branch of NER in\nwhich the named entities (NEs) are nested with each other. However, most of the\nprevious studies on nested NER usually apply linear structure to model the\nnested NEs which are actually accommodated in a hierarchical structure. Thus in\norder to address this mismatch, this work models the full nested NEs in a\nsentence as a holistic structure, then we propose a holistic structure parsing\nalgorithm to disclose the entire NEs once for all. Besides, there is no\nresearch on applying corpus-level information to NER currently. To make up for\nthe loss of this information, we introduce Point-wise Mutual Information (PMI)\nand other frequency features from corpus-aware statistics for even better\nperformance by holistic modeling from sentence-level to corpus-level.\nExperiments show that our model yields promising results on widely-used\nbenchmarks which approach or even achieve state-of-the-art. Further empirical\nstudies show that our proposed corpus-aware features can substantially improve\nNER domain adaptation, which demonstrates the surprising advantage of our\nproposed corpus-level holistic structure modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiOmnia: generative QA corpus on the whole Russian Wikipedia. (arXiv:2204.08009v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08009","description":"<p>The General QA field has been developing the methodology referencing the\nStanford Question answering dataset (SQuAD) as the significant benchmark.\nHowever, compiling factual questions is accompanied by time- and\nlabour-consuming annotation, limiting the training data's potential size. We\npresent the WikiOmnia dataset, a new publicly available set of QA-pairs and\ncorresponding Russian Wikipedia article summary sections, composed with a fully\nautomated generative pipeline. The dataset includes every available article\nfrom Wikipedia for the Russian language. The WikiOmnia pipeline is available\nopen-source and is also tested for creating SQuAD-formatted QA on other\ndomains, like news texts, fiction, and social media. The resulting dataset\nincludes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs\nwith paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for\nruT5-large) and cleaned data with strict automatic verification (over 160,000\nQA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with\nparagraphs for ruT5-large).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pisarevskaya_D/0/1/0/all/0/1\">Dina Pisarevskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pathologies of Pre-trained Language Models in Few-shot Fine-tuning. (arXiv:2204.08039v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08039","description":"<p>Although adapting pre-trained language models with few examples has shown\npromising performance on text classification, there is a lack of understanding\nof where the performance gain comes from. In this work, we propose to answer\nthis question by interpreting the adaptation behavior using post-hoc\nexplanations from model predictions. By modeling feature statistics of\nexplanations, we discover that (1) without fine-tuning, pre-trained models\n(e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although\nfew-shot fine-tuning can mitigate the prediction bias and demonstrate promising\nprediction performance, our analysis shows models gain performance improvement\nby capturing non-task-related features (e.g. stop words) or shallow data\npatterns (e.g. lexical overlaps). These observations alert that pursuing model\nperformance with fewer examples may incur pathological prediction behavior,\nwhich requires further sanity check on model predictions and careful design in\nmodel evaluations in few-shot fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Mixed-initiative Conversational Search Systems via User Simulation. (arXiv:2204.08046v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08046","description":"<p>Clarifying the underlying user information need by asking clarifying\nquestions is an important feature of modern conversational search system.\nHowever, evaluation of such systems through answering prompted clarifying\nquestions requires significant human effort, which can be time-consuming and\nexpensive. In this paper, we propose a conversational User Simulator, called\nUSi, for automatic evaluation of such conversational search systems. Given a\ndescription of an information need, USi is capable of automatically answering\nclarifying questions about the topic throughout the search session. Through a\nset of experiments, including automated natural language generation metrics and\ncrowdsourcing studies, we show that responses generated by USi are both inline\nwith the underlying information need and comparable to human-generated answers.\nMoreover, we make the first steps towards multi-turn interactions, where\nconversational search systems asks multiple questions to the (simulated) user\nwith a goal of clarifying the user need. To this end, we expand on currently\navailable datasets for studying clarifying questions, i.e., Qulac and ClariQ,\nby performing a crowdsourcing-based multi-turn data acquisition. We show that\nour generative, GPT2-based model, is capable of providing accurate and natural\nanswers to unseen clarifying questions in the single-turn setting and discuss\ncapabilities of our model in the multi-turn setting. We provide the code, data,\nand the pre-trained model to be used for further research on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekulic_I/0/1/0/all/0/1\">Ivan Sekuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crestani_F/0/1/0/all/0/1\">Fabio Crestani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Psycho-linguistic Analysis of BitChute. (arXiv:2204.08078v1 [cs.CY])","link":"http://arxiv.org/abs/2204.08078","description":"<p>In order to better support researchers, journalist, and practitioners in\ntheir use of the MeLa-BitChute dataset for exploration and investigative\nreporting, we provide new psycho-linguistic metadata for the videos, comments,\nand channels in the dataset using LIWC22. This paper describes that metadata\nand methods to filter the data using the metadata. In addition, we provide\nbasic analysis and comparison of the language on BitChute to other social media\nplatforms. The MeLa-BitChute dataset and LIWC metadata described in this paper\ncan be found at:\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horne_B/0/1/0/all/0/1\">Benjamin D. Horne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\\`It\\`ak\\'ur\\`oso: Exploiting Cross-Lingual Transferability for Natural Language Generation of Dialogues in Low-Resource, African Languages. (arXiv:2204.08083v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08083","description":"<p>We investigate the possibility of cross-lingual transfer from a\nstate-of-the-art (SoTA) deep monolingual model (DialoGPT) to 6 African\nlanguages and compare with 2 baselines (BlenderBot 90M, another SoTA, and a\nsimple Seq2Seq). The languages are Swahili, Wolof, Hausa, Nigerian Pidgin\nEnglish, Kinyarwanda &amp; Yor\\`ub\\'a. Generation of dialogues is known to be a\nchallenging task for many reasons. It becomes more challenging for African\nlanguages which are low-resource in terms of data. Therefore, we translate a\nsmall portion of the English multi-domain MultiWOZ dataset for each target\nlanguage. Besides intrinsic evaluation (i.e. perplexity), we conduct human\nevaluation of single-turn conversations by using majority votes and measure\ninter-annotator agreement (IAA). The results show that the hypothesis that deep\nmonolingual models learn some abstractions that generalise across languages\nholds. We observe human-like conversations in 5 out of the 6 languages. It,\nhowever, applies to different degrees in different languages, which is\nexpected. The language with the most transferable properties is the Nigerian\nPidgin English, with a human-likeness score of 78.1%, of which 34.4% are\nunanimous. The main contributions of this paper include the representation\n(through the provision of high-quality dialogue data) of under-represented\nAfrican languages and demonstrating the cross-lingual transferability\nhypothesis for dialogue systems. We also provide the datasets and host the\nmodel checkpoints/demos on the HuggingFace hub for public access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anuoluwapo_A/0/1/0/all/0/1\">Aremu Anuoluwapo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_B/0/1/0/all/0/1\">Bukola Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzaaba_H/0/1/0/all/0/1\">Happy Buzaaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_O/0/1/0/all/0/1\">Oyerinde Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rufai_A/0/1/0/all/0/1\">Amina Mardiyyah Rufai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajibade_B/0/1/0/all/0/1\">Benjamin Ajibade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajudeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traore_M/0/1/0/all/0/1\">Mory Moussou Koulibaly Traore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajayi_T/0/1/0/all/0/1\">Tunde Ajayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baruwa_A/0/1/0/all/0/1\">Ahmed Baruwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owoicho_P/0/1/0/all/0/1\">Paul Owoicho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogunremi_T/0/1/0/all/0/1\">Tolulope Ogunremi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngigi_P/0/1/0/all/0/1\">Phylis Ngigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasir_R/0/1/0/all/0/1\">Ruqayya Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"kpfriends at SemEval-2022 Task 2: NEAMER -- Named Entity Augmented Multi-word Expression Recognizer. (arXiv:2204.08102v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08102","description":"<p>We present NEAMER -- Named Entity Augmented Multi-word Expression Recognizer.\nThis system is inspired by non-compositionality characteristics shared between\nNamed Entity and Idiomatic Expressions. We utilize transfer learning and\nlocality features to enhance idiom classification task. This system is our\nsubmission for SemEval Task 2: Multilingual Idiomaticity Detection and Sentence\nEmbedding Subtask A OneShot shared task. We achieve SOTA with F1 0.9395 during\npost-evaluation phase. We also observe improvement in training stability.\nLastly, we experiment with non-compositionality knowledge transfer,\ncross-lingual fine-tuning and locality features, which we also introduce in\nthis paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Min Sik Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monte Carlo Tree Search for Interpreting Stress in Natural Language. (arXiv:2204.08105v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08105","description":"<p>Natural language processing can facilitate the analysis of a person's mental\nstate from text they have written. Previous studies have developed models that\ncan predict whether a person is experiencing a mental health condition from\nsocial media posts with high accuracy. Yet, these models cannot explain why the\nperson is experiencing a particular mental state. In this work, we present a\nnew method for explaining a person's mental state from text using Monte Carlo\ntree search (MCTS). Our MCTS algorithm employs trained classification models to\nguide the search for key phrases that explain the writer's mental state in a\nconcise, interpretable manner. Furthermore, our algorithm can find both\nexplanations that depend on the particular context of the text (e.g., a recent\nbreakup) and those that are context-independent. Using a dataset of Reddit\nposts that exhibit stress, we demonstrate the ability of our MCTS algorithm to\nidentify interpretable explanations for a person's feeling of stress in both a\ncontext-dependent and context-independent manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swanson_K/0/1/0/all/0/1\">Kyle Swanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1\">Joy Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering. (arXiv:2204.08109v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08109","description":"<p>Question answering on knowledge bases (KBQA) poses a unique challenge for\nsemantic parsing research due to two intertwined factors: large search space\nand ambiguities in schema linking. The predominant ranking-based KBQA models,\nwhich rely on a candidate enumeration step to reduce the search space, struggle\nwith flexibility and have impractical online running time. In this paper, we\npresent ArcaneQA, a novel generation-based model that addresses both the large\nsearch space and schema linking in a unified framework with two mutually\nboosting ingredients: we use dynamic program induction to tackle the large\nsearch space and dynamic contextualized encoding to enhance schema linking.\nExperiment results on multiple popular KBQA datasets demonstrate the highly\ncompetitive performance of ArcaneQA in both effectiveness and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models. (arXiv:2204.08110v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08110","description":"<p>English pretrained language models, which make up the backbone of many modern\nNLP systems, require huge amounts of unlabeled training data. These models are\ngenerally presented as being trained only on English text but have been found\nto transfer surprisingly well to other languages. We investigate this\nphenomenon and find that common English pretraining corpora actually contain\nsignificant amounts of non-English text: even when less than 1% of data is not\nEnglish (well within the error rate of strong language classifiers), this leads\nto hundreds of millions of foreign language tokens in large-scale datasets. We\nthen demonstrate that even these small percentages of non-English data\nfacilitate cross-lingual transfer for models trained on them, with target\nlanguage performance strongly correlated to the amount of in-language data seen\nduring pretraining. In light of these findings, we argue that no model is truly\nmonolingual when pretrained at scale, which should be considered when\nevaluating cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HFT-ONLSTM: Hierarchical and Fine-Tuning Multi-label Text Classification. (arXiv:2204.08115v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08115","description":"<p>Many important classification problems in the real-world consist of a large\nnumber of closely related categories in a hierarchical structure or taxonomy.\nHierarchical multi-label text classification (HMTC) with higher accuracy over\nlarge sets of closely related categories organized in a hierarchy or taxonomy\nhas become a challenging problem. In this paper, we present a hierarchical and\nfine-tuning approach based on the Ordered Neural LSTM neural network,\nabbreviated as HFT-ONLSTM, for more accurate level-by-level HMTC. First, we\npresent a novel approach to learning the joint embeddings based on parent\ncategory labels and textual data for accurately capturing the joint features of\nboth category labels and texts. Second, a fine tuning technique is adopted for\ntraining parameters such that the text classification results in the upper\nlevel should contribute to the classification in the lower one. At last, the\ncomprehensive analysis is made based on extensive experiments in comparison\nwith the state-of-the-art hierarchical and flat multi-label text classification\napproaches over two benchmark datasets, and the experimental results show that\nour HFT-ONLSTM approach outperforms these approaches, in particular reducing\ncomputational costs while achieving superior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jingpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yinglong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanvir_A/0/1/0/all/0/1\">Ahmad Tanvir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Beihong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Dense Video Captioning as Sequence Generation. (arXiv:2204.08121v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08121","description":"<p>Dense video captioning aims to identify the events of interest in an input\nvideo, and generate descriptive captions for each event. Previous approaches\nusually follow a two-stage generative process, which first proposes a segment\nfor each event, then renders a caption for each identified segment. Recent\nadvances in large-scale sequence generation pretraining have seen great success\nin unifying task formulation for a great variety of tasks, but so far, more\ncomplex tasks such as dense video captioning are not able to fully utilize this\npowerful paradigm. In this work, we show how to model the two subtasks of dense\nvideo captioning jointly as one sequence generation task, and simultaneously\npredict the events and the corresponding descriptions. Experiments on YouCook2\nand ViTT show encouraging results and indicate the feasibility of training\ncomplex tasks such as end-to-end dense video captioning integrated into\nlarge-scale pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1\">Ashish Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parallel Text Style Transfer with Self-Parallel Supervision. (arXiv:2204.08123v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08123","description":"<p>The performance of existing text style transfer models is severely limited by\nthe non-parallel datasets on which the models are trained. In non-parallel\ndatasets, no direct mapping exists between sentences of the source and target\nstyle; the style transfer models thus only receive weak supervision of the\ntarget sentences during training, which often leads the model to discard too\nmuch style-independent information, or utterly fail to transfer the style. In\nthis work, we propose LaMer, a novel text style transfer framework based on\nlarge-scale language models. LaMer first mines the roughly parallel expressions\nin the non-parallel datasets with scene graphs, and then employs MLE training,\nfollowed by imitation learning refinement, to leverage the intrinsic\nparallelism within the data. On two benchmark tasks (sentiment &amp; formality\ntransfer) and a newly proposed challenging task (political stance transfer),\nour model achieves qualitative advances in transfer accuracy, content\npreservation, and fluency. Further empirical and human evaluations demonstrate\nthat our model not only makes training more efficient, but also generates more\nreadable and diverse expressions than previous models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chongyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation. (arXiv:2204.08128v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08128","description":"<p>Personalized dialogue systems explore the problem of generating responses\nthat are consistent with the user's personality, which has raised much\nattention in recent years. Existing personalized dialogue systems have tried to\nextract user profiles from dialogue history to guide personalized response\ngeneration. Since the dialogue history is usually long and noisy, most existing\nmethods truncate the dialogue history to model the user's personality. Such\nmethods can generate some personalized responses, but a large part of dialogue\nhistory is wasted, leading to sub-optimal performance of personalized response\ngeneration. In this work, we propose to refine the user dialogue history on a\nlarge scale, based on which we can handle more dialogue history and obtain more\nabundant and accurate persona information. Specifically, we design an MSP model\nwhich consists of three personal information refiners and a personalized\nresponse generator. With these multi-level refiners, we can sparsely extract\nthe most valuable information (tokens) from the dialogue history and leverage\nother similar users' data to enhance personalization. Experimental results on\ntwo real-world datasets demonstrate the superiority of our model in generating\nmore informative and personalized responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hanxun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hongjin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ingredient Extraction from Text in the Recipe Domain. (arXiv:2204.08137v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08137","description":"<p>In recent years, there has been an increase in the number of devices with\nvirtual assistants (e.g: Siri, Google Home, Alexa) in our living rooms and\nkitchens. As a result of this, these devices receive several queries about\nrecipes. All these queries will contain terms relating to a \"recipe-domain\"\ni.e: they will contain dish-names, ingredients, cooking times, dietary\npreferences etc. Extracting these recipe-relevant aspects from the query thus\nbecomes important when it comes to addressing the user's information need. Our\nproject focuses on extracting ingredients from such plain-text user utterances.\nOur best performing model was a fine-tuned BERT which achieved an F1-score of\n$95.01$. We have released all our code in a GitHub repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dharawat_A/0/1/0/all/0/1\">Arkin Dharawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doan_C/0/1/0/all/0/1\">Chris Doan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Position Encoding for Transformers. (arXiv:2204.08142v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08142","description":"<p>Recurrent models have been dominating the field of neural machine translation\n(NMT) for the past few years. Transformers \\citep{vaswani2017attention}, have\nradically changed it by proposing a novel architecture that relies on a\nfeed-forward backbone and self-attention mechanism. Although Transformers are\npowerful, they could fail to properly encode sequential/positional information\ndue to their non-recurrent nature. To solve this problem, position embeddings\nare defined exclusively for each time step to enrich word information. However,\nsuch embeddings are fixed after training regardless of the task and the word\nordering system of the source or target language.\n</p>\n<p>In this paper, we propose a novel architecture with new position embeddings\ndepending on the input text to address this shortcoming by taking the order of\ntarget words into consideration. Instead of using predefined position\nembeddings, our solution \\textit{generates} new embeddings to refine each\nword's position information. Since we do not dictate the position of source\ntokens and learn them in an end-to-end fashion, we refer to our method as\n\\textit{dynamic} position encoding (DPE). We evaluated the impact of our model\non multiple datasets to translate from English into German, French, and Italian\nand observed meaningful improvements in comparison to the original Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Joyce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passban_P/0/1/0/all/0/1\">Peyman Passban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning. (arXiv:2204.08143v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08143","description":"<p>Massive false rumors emerging along with breaking news or trending topics\nseverely hinder the truth. Existing rumor detection approaches achieve\npromising performance on the yesterday`s news, since there is enough corpus\ncollected from the same domain for model training. However, they are poor at\ndetecting rumors about unforeseen events especially those propagated in\ndifferent languages due to the lack of training data and prior knowledge (i.e.,\nlow-resource regimes). In this paper, we propose an adversarial contrastive\nlearning framework to detect rumors by adapting the features learned from\nwell-resourced rumor data to that of the low-resourced. Our model explicitly\novercomes the restriction of domain and/or language usage via language\nalignment and a novel supervised contrastive training paradigm. Moreover, we\ndevelop an adversarial augmentation mechanism to further enhance the robustness\nof low-resource rumor representation. Extensive experiments conducted on two\nlow-resource datasets collected from real-world microblog platforms demonstrate\nthat our framework achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingfei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling. (arXiv:2204.08152v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08152","description":"<p>Multi-turn dialogue modeling as a challenging branch of natural language\nunderstanding (NLU), aims to build representations for machines to understand\nhuman dialogues, which provides a solid foundation for multiple downstream\ntasks. Recent studies of dialogue modeling commonly employ pre-trained language\nmodels (PrLMs) to encode the dialogue history as successive tokens, which is\ninsufficient in capturing the temporal characteristics of dialogues. Therefore,\nwe propose Bidirectional Information Decoupling Network (BiDeN) as a universal\ndialogue encoder, which explicitly incorporates both the past and future\ncontexts and can be generalized to a wide range of dialogue-related tasks.\nExperimental results on datasets of different downstream tasks demonstrate the\nuniversality and effectiveness of our BiDeN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on Prompt-based Few-Shot Learning Methods for Belief State Tracking in Task-oriented Dialog Systems. (arXiv:2204.08167v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08167","description":"<p>We tackle the Dialogue Belief State Tracking(DST) problem of task-oriented\nconversational systems. Recent approaches to this problem leveraging\nTransformer-based models have yielded great results. However, training these\nmodels is expensive, both in terms of computational resources and time.\nAdditionally, collecting high quality annotated dialogue datasets remains a\nchallenge for researchers because of the extensive annotation required for\ntraining these models. Driven by the recent success of pre-trained language\nmodels and prompt-based learning, we explore prompt-based few-shot learning for\nDialogue Belief State Tracking. We formulate the DST problem as a 2-stage\nprompt-based language modelling task and train language models for both tasks\nand present a comprehensive empirical analysis of their separate and joint\nperformance. We demonstrate the potential of prompt-based methods in few-shot\nlearning for DST and provide directions for future improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1\">Debjoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santra_B/0/1/0/all/0/1\">Bishal Santra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval. (arXiv:2204.08173v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08173","description":"<p>Entity retrieval--retrieving information about entity mentions in a query--is\na key step in open-domain tasks, such as question answering or fact checking.\nHowever, state-of-the-art entity retrievers struggle to retrieve rare entities\nfor ambiguous mentions due to biases towards popular entities. Incorporating\nknowledge graph types during training could help overcome popularity biases,\nbut there are several challenges: (1) existing type-based retrieval methods\nrequire mention boundaries as input, but open-domain tasks run on unstructured\ntext, (2) type-based methods should not compromise overall performance, and (3)\ntype-based methods should be robust to noisy and missing types. In this work,\nwe introduce TABi, a method to jointly train bi-encoders on knowledge graph\ntypes and unstructured text for entity retrieval for open-domain tasks. TABi\nleverages a type-enforced contrastive loss to encourage entities and queries of\nsimilar types to be close in the embedding space. TABi improves retrieval of\nrare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining\nstrong overall retrieval performance on open-domain tasks in the KILT benchmark\ncompared to state-of-the-art retrievers. TABi is also robust to incomplete type\nsystems, improving rare entity retrieval over baselines with only 5% type\ncoverage of the training dataset. We make our code publicly available at\nhttps://github.com/HazyResearch/tabi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leszczynski_M/0/1/0/all/0/1\">Megan Leszczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Daniel Y. Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mayee F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection using generative-based and mutation-based data augmentation. (arXiv:2204.08198v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08198","description":"<p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse\nsomeone. It is commonly used on social media. The metaphorical and creative\nnature of sarcasm presents a significant difficulty for sentiment analysis\nsystems based on affective computing. The methodology and results of our team,\nUTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in\nthis paper. We put different models, and data augmentation approaches to the\ntest and report on which one works best. The tests begin with traditional\nmachine learning models and progress to transformer-based and attention-based\nmodels. We employed data augmentation based on data mutation and data\ngeneration. Using RoBERTa and mutation-based data augmentation, our best\napproach achieved an F1-sarcastic of 0.38 in the competition's evaluation\nphase. After the competition, we fixed our model's flaws and achieved an\nF1-sarcastic of 0.414.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1\">Arash Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeraati_T/0/1/0/all/0/1\">Tanin Zeraati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrak_B/0/1/0/all/0/1\">Behnam Bahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visio-Linguistic Brain Encoding. (arXiv:2204.08261v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08261","description":"<p>Enabling effective brain-computer interfaces requires understanding how the\nhuman brain encodes stimuli across modalities such as visual, language (or\ntext), etc. Brain encoding aims at constructing fMRI brain activity given a\nstimulus. There exists a plethora of neural encoding models which study brain\nencoding for single mode stimuli: visual (pretrained CNNs) or text (pretrained\nlanguage models). Few recent papers have also obtained separate visual and text\nrepresentation models and performed late-fusion using simple heuristics.\nHowever, previous work has failed to explore: (a) the effectiveness of image\nTransformer models for encoding visual stimuli, and (b) co-attentive\nmulti-modal modeling for visual and text reasoning. In this paper, we\nsystematically explore the efficacy of image Transformers (ViT, DEiT, and BEiT)\nand multi-modal Transformers (VisualBERT, LXMERT, and CLIP) for brain encoding.\nExtensive experiments on two popular datasets, BOLD5000 and Pereira, provide\nthe following insights. (1) To the best of our knowledge, we are the first to\ninvestigate the effectiveness of image and multi-modal Transformers for brain\nencoding. (2) We find that VisualBERT, a multi-modal Transformer, significantly\noutperforms previously proposed single-mode CNNs, image Transformers as well as\nother previously proposed multi-modal models, thereby establishing new\nstate-of-the-art. The supremacy of visio-linguistic models raises the question\nof whether the responses elicited in the visual regions are affected implicitly\nby linguistic processing even when passively viewing images. Future fMRI tasks\ncan verify this computational insight in an appropriate experimental setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_J/0/1/0/all/0/1\">Jashn Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowtula_V/0/1/0/all/0/1\">Vijay Rowtula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapi_R/0/1/0/all/0/1\">Raju S. Bapi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factual Error Correction for Abstractive Summaries Using Entity Retrieval. (arXiv:2204.08263v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08263","description":"<p>Despite the recent advancements in abstractive summarization systems\nleveraged from large-scale datasets and pre-trained language models, the\nfactual correctness of the summary is still insufficient. One line of trials to\nmitigate this problem is to include a post-editing process that can detect and\ncorrect factual errors in the summary. In building such a post-editing system,\nit is strongly required that 1) the process has a high success rate and\ninterpretability and 2) has a fast running time. Previous approaches focus on\nregeneration of the summary using the autoregressive models, which lack\ninterpretability and require high computing resources. In this paper, we\npropose an efficient factual error correction system RFEC based on entities\nretrieval post-editing process. RFEC first retrieves the evidence sentences\nfrom the original document by comparing the sentences with the target summary.\nThis approach greatly reduces the length of text for a system to analyze. Next,\nRFEC detects the entity-level errors in the summaries by considering the\nevidence sentences and substitutes the wrong entities with the accurate\nentities from the evidence sentences. Experimental results show that our\nproposed error correction system shows more competitive performance than\nbaseline methods in correcting the factual errors with a much faster speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Cheoneum Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts. (arXiv:2204.08292v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08292","description":"<p>Inferring spatial relations in natural language is a crucial ability an\nintelligent system should possess. The bAbI dataset tries to capture tasks\nrelevant to this domain (task 17 and 19). However, these tasks have several\nlimitations. Most importantly, they are limited to fixed expressions, they are\nlimited in the number of reasoning steps required to solve them, and they fail\nto test the robustness of models to input that contains irrelevant or redundant\ninformation. In this paper, we present a new Question-Answering dataset called\nStepGame for robust multi-hop spatial reasoning in texts. Our experiments\ndemonstrate that state-of-the-art models on the bAbI dataset struggle on the\nStepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented\nNeural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental\nresults on both datasets show that our model outperforms all the baselines with\nsuperior generalization and robustness performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language. (arXiv:2204.08304v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08304","description":"<p>Patronizing and condescending language (PCL) is everywhere, but rarely is the\nfocus on its use by media towards vulnerable communities. Accurately detecting\nPCL of this form is a difficult task due to limited labeled data and how subtle\nit can be. In this paper, we describe our system for detecting such language\nwhich was submitted to SemEval 2022 Task 4: Patronizing and Condescending\nLanguage Detection. Our approach uses an ensemble of pre-trained language\nmodels, data augmentation, and optimizing the threshold for detection.\nExperimental results on the evaluation dataset released by the competition\nhosts show that our work is reliably able to detect PCL, achieving an F1 score\nof 55.47% on the binary classification task and a macro F1 score of 36.25% on\nthe fine-grained, multi-label detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koleczek_D/0/1/0/all/0/1\">David Koleczek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarlatos_A/0/1/0/all/0/1\">Alex Scarlatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakare_S/0/1/0/all/0/1\">Siddha Karakare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_P/0/1/0/all/0/1\">Preshma Linet Pereira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding. (arXiv:2204.08325v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08325","description":"<p>Due to high data demands of current methods, attention to zero-shot\ncross-lingual spoken language understanding (SLU) has grown, as such approaches\ngreatly reduce human annotation effort. However, existing models solely rely on\nshared parameters, which can only perform implicit alignment across languages.\nWe present Global--Local Contrastive Learning Framework (GL-CLeF) to address\nthis shortcoming. Specifically, we employ contrastive learning, leveraging\nbilingual dictionaries to construct multilingual views of the same utterance,\nthen encourage their representations to be more similar than negative example\npairs, which achieves to explicitly aligned representations of similar\nsentences across languages. In addition, a key step in GL-CLeF is a proposed\nLocal and Global component, which achieves a fine-grained cross-lingual\ntransfer (i.e., sentence-level Local intent transfer, token-level Local slot\ntransfer, and semantic-level Global transfer across intent and slot).\nExperiments on MultiATIS++ show that GL-CLeF achieves the best performance and\nsuccessfully pulls representations of similar sentences across languages\ncloser.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to execute or ask clarification questions. (arXiv:2204.08373v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08373","description":"<p>Collaborative tasks are ubiquitous activities where a form of communication\nis required in order to reach a joint goal. Collaborative building is one of\nsuch tasks. We wish to develop an intelligent builder agent in a simulated\nbuilding environment (Minecraft) that can build whatever users wish to build by\njust talking to the agent. In order to achieve this goal, such agents need to\nbe able to take the initiative by asking clarification questions when further\ninformation is needed. Existing works on Minecraft Corpus Dataset only learn to\nexecute instructions neglecting the importance of asking for clarifications. In\nthis paper, we extend the Minecraft Corpus Dataset by annotating all builder\nutterances into eight types, including clarification questions, and propose a\nnew builder agent model capable of determining when to ask or execute\ninstructions. Experimental results show that our model achieves\nstate-of-the-art performance on the collaborative building task with a\nsubstantial improvement. We also define two new tasks, the learning to ask task\nand the joint learning task. The latter consists of solving both collaborating\nbuilding and learning to ask tasks jointly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Transformer-based End-to-end ASR using BERT. (arXiv:2104.04805v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04805","description":"<p>Transformer-based models have led to significant innovation in classical and\npractical subjects as varied as speech processing, natural language processing,\nand computer vision. On top of the Transformer, attention-based end-to-end\nautomatic speech recognition (ASR) models have recently become popular.\nSpecifically, non-autoregressive modeling, which boasts fast inference and\nperformance comparable to conventional autoregressive methods, is an emerging\nresearch topic. In the context of natural language processing, the\nbidirectional encoder representations from Transformers (BERT) model has\nreceived widespread attention, partially due to its ability to infer\ncontextualized word representations and to enable superior performance for\ndownstream tasks while needing only simple fine-tuning. Motivated by the\nsuccess, we intend to view speech recognition as a downstream task of BERT,\nthus an ASR system is expected to be deduced by performing fine-tuning.\nConsequently, to not only inherit the advantages of non-autoregressive ASR\nmodels but also enjoy the benefits of a pre-trained language model (e.g.,\nBERT), we propose a non-autoregressive Transformer-based end-to-end ASR model\nbased on BERT. We conduct a series of experiments on the AISHELL-1 dataset that\ndemonstrate competitive or superior results for the model when compared to\nstate-of-the-art ASR systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fu-Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Fully Trained to Fully Random Embeddings: Improving Neural Machine Translation with Compact Word Embedding Tables. (arXiv:2104.08677v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08677","description":"<p>Embedding matrices are key components in neural natural language processing\n(NLP) models that are responsible to provide numerical representations of input\ntokens.\\footnote{In this paper words and subwords are referred to as\n\\textit{tokens} and the term \\textit{embedding} only refers to embeddings of\ninputs.} In this paper, we analyze the impact and utility of such matrices in\nthe context of neural machine translation (NMT). We show that detracting\nsyntactic and semantic information from word embeddings and running NMT systems\nwith random embeddings is not as damaging as it initially sounds. We also show\nhow incorporating only a limited amount of task-specific knowledge from\nfully-trained embeddings can boost the performance NMT systems. Our findings\ndemonstrate that in exchange for negligible deterioration in performance, any\nNMT model can be run with partially random embeddings. Working with such\nstructures means a minimal memory requirement as there is no longer need to\nstore large embedding tables, which is a significant gain in industrial and\non-device settings. We evaluated our embeddings in translating {English} into\n{German} and {French} and achieved a $5.3$x compression rate. Despite having a\nconsiderably smaller architecture, our models in some cases are even able to\noutperform state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Krtin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passban_P/0/1/0/all/0/1\">Peyman Passban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_Y/0/1/0/all/0/1\">Yiu Sing Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transductive Learning for Abstractive News Summarization. (arXiv:2104.09500v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09500","description":"<p>Pre-trained and fine-tuned news summarizers are expected to generalize to\nnews articles unseen in the fine-tuning (training) phase. However, these\narticles often contain specifics, such as new events and people, a summarizer\ncould not learn about in training. This applies to scenarios such as a news\npublisher training a summarizer on dated news and summarizing incoming recent\nnews. In this work, we explore the first application of transductive learning\nto summarization where we further fine-tune models on test set inputs.\nSpecifically, we construct pseudo summaries from salient article sentences and\ninput randomly masked articles. Moreover, this approach is also beneficial in\nthe fine-tuning phase, where we jointly predict extractive pseudo references\nand abstractive gold summaries in the training set. We show that our approach\nyields state-of-the-art results on CNN/DM and NYT datasets, improving ROUGE-L\nby 1.05 and 0.74, respectively. Importantly, our approach does not require any\nchanges of the original architecture. Moreover, we show the benefits of\ntransduction from dated to more recent CNN news. Finally, through human and\nautomatic evaluation, we demonstrate improvements in summary abstractiveness\nand coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1\">Arthur Bra&#x17e;inskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sujith Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters. (arXiv:2105.06232v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06232","description":"<p>To diversify and enrich generated dialogue responses, knowledge-grounded\ndialogue has been investigated in recent years. The existing methods tackle the\nknowledge grounding challenge by retrieving the relevant sentences over a large\ncorpus and augmenting the dialogues with explicit extra information. Despite\ntheir success, however, the existing works have drawbacks on the inference\nefficiency. This paper proposes KnowExpert, an end-to-end framework to bypass\nthe explicit retrieval process and inject knowledge into the pre-trained\nlanguage models with lightweight adapters and adapt to the knowledge-grounded\ndialogue task. To the best of our knowledge, this is the first attempt to\ntackle this challenge without retrieval in this task under an open-domain\nchit-chat scenario. The experimental results show that KknowExpert performs\ncomparably with some retrieval-based baselines while being time-efficient in\ninference, demonstrating the potential of our proposed direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoramic-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01263","description":"<p>Response selector is an essential component of generation-based dialogue\nsystems and it aims to pick out an optimal response in a candidate pool to\ncontinue the dialogue. The current state-of-the-art methods are mainly based on\nthe encoding paradigm called Cross-Encoder, which separately encodes each\ncontext-response pair and ranks the responses according to their fitness\nscores. However, Cross-Encoder repeatedly encodes the same lengthy context for\neach response, resulting in high computational costs. Moreover, without\nconsidering the relationship among the candidates, it is difficult to figure\nout which candidate is the best response purely based on the fitness score per\ncandidate. We aim to address these problems through a new paradigm called\nPanoramic-Encoder. The proposed method encodes all candidates and the context\nat once and realizes the mutual interaction using a tailored candidate\nattention mechanism (CAM). It also enables the integration of some effective\ntraining techniques, such as the in-batch negative training, which cannot be\nused in Cross-Encoders. Extensive experiments across four benchmark datasets\nshow that our new method significantly outperforms the current state-of-the-art\nwith lower computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chiyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huachuan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Language to Learn Program Abstractions and Search Heuristics. (arXiv:2106.11053v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11053","description":"<p>Inductive program synthesis, or inferring programs from examples of desired\nbehavior, offers a general paradigm for building interpretable, robust, and\ngeneralizable machine learning systems. Effective program synthesis depends on\ntwo key ingredients: a strong library of functions from which to build\nprograms, and an efficient search strategy for finding programs that solve a\ngiven task. We introduce LAPS (Language for Abstraction and Program Search), a\ntechnique for using natural language annotations to guide joint learning of\nlibraries and neurally-guided search models for synthesis. When integrated into\na state-of-the-art library learning system (DreamCoder), LAPS produces\nhigher-quality libraries and improves search efficiency and generalization on\nthree domains -- string editing, image composition, and abstract reasoning\nabout scenes -- even when no natural language hints are available at test time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Catherine Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1\">Kevin Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation. (arXiv:2107.14600v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14600","description":"<p>It is expensive to evaluate the results of Machine Translation(MT), which\nusually requires manual translation as a reference. Machine Translation Quality\nEstimation (QE) is a task of predicting the quality of machine translations\nwithout relying on any reference. Recently, the emergence of\npredictor-estimator framework which trains the predictor as a feature extractor\nand estimator as a QE predictor, and pre-trained language models(PLM) have\nachieved promising QE performance. However, we argue that there are still gaps\nbetween the predictor and the estimator in both data quality and training\nobjectives, which preclude QE models from benefiting from a large number of\nparallel corpora more directly. Based on previous related work that have\nalleviated gaps to some extent, we propose a novel framework that provides a\nmore accurate direct pretraining for QE tasks. In this framework, a generator\nis trained to produce pseudo data that is closer to the real QE data, and a\nestimator is pretrained on these data with novel objectives that are the same\nas the QE task. Experiments on widely used benchmarks show that our proposed\nframework outperforms existing methods, without using any pretraining models\nsuch as BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Knowledge Base Question Answering: A Survey. (arXiv:2108.06688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06688","description":"<p>Knowledge base question answering (KBQA) aims to answer a question over a\nknowledge base (KB). Early studies mainly focused on answering simple questions\nover KBs and achieved great success. However, their performance on complex\nquestions is still far from satisfactory. Therefore, in recent years,\nresearchers propose a large number of novel methods, which looked into the\nchallenges of answering complex questions. In this survey, we review recent\nadvances on KBQA with the focus on solving complex questions, which usually\ncontain multiple subjects, express compound relations, or involve numerical\noperations. In detail, we begin with introducing the complex KBQA task and\nrelevant background. Then, we describe benchmark datasets for complex KBQA task\nand introduce the construction process of these datasets. Next, we present two\nmainstream categories of methods for complex KBQA, namely semantic\nparsing-based (SP-based) methods and information retrieval-based (IR-based)\nmethods. Specifically, we illustrate their procedures with flow designs and\ndiscuss their major differences and similarities. After that, we summarize the\nchallenges that these two categories of methods encounter when answering\ncomplex questions, and explicate advanced solutions and techniques used in\nexisting work. Finally, we conclude and discuss several promising directions\nrelated to complex KBQA for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gaole He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IsoScore: Measuring the Uniformity of Embedding Space Utilization. (arXiv:2108.07344v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07344","description":"<p>The recent success of distributed word representations has led to an\nincreased interest in analyzing the properties of their spatial distribution.\nSeveral studies have suggested that contextualized word embedding models do not\nisotropically project tokens into vector space. However, current methods\ndesigned to measure isotropy, such as average random cosine similarity and the\npartition score, have not been thoroughly analyzed and are not appropriate for\nmeasuring isotropy. We propose IsoScore: a novel tool that quantifies the\ndegree to which a point cloud uniformly utilizes the ambient vector space.\nUsing rigorously designed tests, we demonstrate that IsoScore is the only tool\navailable in the literature that accurately measures how uniformly distributed\nvariance is across dimensions in vector space. Additionally, we use IsoScore to\nchallenge a number of recent conclusions in the NLP literature that have been\nderived using brittle metrics of isotropy. We caution future studies from using\nexisting tools to measure isotropy in contextualized embedding space as\nresulting conclusions will be misleading or altogether inaccurate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillman_N/0/1/0/all/0/1\">Nate Gillman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayne_T/0/1/0/all/0/1\">Taylor Rayne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Molecular Representation Learning via Contrastive Pre-training. (arXiv:2109.08830v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08830","description":"<p>Molecular representation learning plays an essential role in cheminformatics.\nRecently, language model-based approaches have gained popularity as an\nalternative to traditional expert-designed features to encode molecules.\nHowever, these approaches only utilize a single molecular language for\nrepresentation learning. Motivated by the fact that a given molecule can be\ndescribed using different languages such as Simplified Molecular Line Entry\nSystem (SMILES), The International Union of Pure and Applied Chemistry (IUPAC),\nand The IUPAC International Chemical Identifier (InChI), we propose a\nmultilingual molecular embedding generation approach called MM-Deacon\n(multilingual molecular domain embedding analysis via contrastive learning).\nMM-Deacon is pre-trained using SMILES and IUPAC as two different languages on\nlarge-scale molecules. We evaluated the robustness of our method on seven\nmolecular property prediction tasks from MoleculeNet benchmark, zero-shot\ncross-lingual retrieval, and a drug-drug interaction prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhihui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pramod Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Andy Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_R/0/1/0/all/0/1\">Robin Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Opinion Summarization via Collaborative Decoding. (arXiv:2110.07520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07520","description":"<p>Opinion summarization focuses on generating summaries that reflect popular\nsubjective information expressed in multiple online reviews. While generated\nsummaries offer general and concise information about a particular hotel or\nproduct, the information may be insufficient to help the user compare multiple\ndifferent choices. Thus, the user may still struggle with the question \"Which\none should I pick?\" In this paper, we propose the comparative opinion\nsummarization task, which aims at generating two contrastive summaries and one\ncommon summary from two different candidate sets of reviews. We develop a\ncomparative summarization framework CoCoSum, which consists of two base\nsummarization models that jointly generate contrastive and common summaries.\nExperimental results on a newly created benchmark CoCoTrip show that CoCoSum\ncan produce higher-quality contrastive and common summaries than\nstate-of-the-art opinion summarization models. The dataset and code are\navailable at https://github.com/megagonlabs/cocosum\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelidis_S/0/1/0/all/0/1\">Stefanos Angelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshihiko Suhara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent Classification Using Pre-trained Language Agnostic Embeddings For Low Resource Languages. (arXiv:2110.09264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.09264","description":"<p>Building Spoken Language Understanding (SLU) systems that do not rely on\nlanguage specific Automatic Speech Recognition (ASR) is an important yet less\nexplored problem in language processing. In this paper, we present a\ncomparative study aimed at employing a pre-trained acoustic model to perform\nSLU in low resource scenarios. Specifically, we use three different embeddings\nextracted using Allosaurus, a pre-trained universal phone decoder: (1) Phone\n(2) Panphone, and (3) Allo embeddings. These embeddings are then used in\nidentifying the spoken intent. We perform experiments across three different\nlanguages: English, Sinhala, and Tamil each with different data sizes to\nsimulate high, medium, and low resource scenarios. Our system improves on the\nstate-of-the-art (SOTA) intent classification accuracy by approximately 2.11%\nfor Sinhala and 7.00% for Tamil and achieves competitive results on English.\nFurthermore, we present a quantitative analysis of how the performance scales\nwith the number of training examples used per intent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_H/0/1/0/all/0/1\">Hemant Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sai Krishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Transformers Are More Efficient Language Models. (arXiv:2110.13711v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.13711","description":"<p>Transformer models yield impressive results on many NLP and sequence modeling\ntasks. Remarkably, Transformers can handle long sequences which allows them to\nproduce long coherent outputs: full paragraphs produced by GPT-3 or\nwell-structured images produced by DALL-E. These large language models are\nimpressive but also very inefficient and costly, which limits their\napplications and accessibility. We postulate that having an explicit\nhierarchical architecture is the key to Transformers that efficiently handle\nlong sequences. To verify this claim, we first study different ways to\ndownsample and upsample activations in Transformers so as to make them\nhierarchical. We use the best performing upsampling and downsampling layers to\ncreate Hourglass - a hierarchical Transformer language model. Hourglass\nimproves upon the Transformer baseline given the same amount of computation and\ncan yield the same results as Transformers more efficiently. In particular,\nHourglass sets new state-of-the-art for Transformer models on the ImageNet32\ngeneration task and improves language modeling efficiency on the widely studied\nenwik8 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1\">Piotr Nawrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1\">Szymon Tworkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyrolski_M/0/1/0/all/0/1\">Micha&#x142; Tyrolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1\">&#x141;ukasz Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegedy_C/0/1/0/all/0/1\">Christian Szegedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing in-and-for Design Research. (arXiv:2111.13827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.13827","description":"<p>We review the scholarly contributions that utilise Natural Language\nProcessing (NLP) techniques to support the design process. Using a heuristic\napproach, we gathered 223 articles that are published in 32 journals within the\nperiod 1991-present. We present state-of-the-art NLP in-and-for design research\nby reviewing these articles according to the type of natural language text\nsources: internal reports, design concepts, discourse transcripts, technical\npublications, consumer opinions, and others. Upon summarizing and identifying\nthe gaps in these contributions, we utilise an existing design innovation\nframework to identify the applications that are currently being supported by\nNLP. We then propose a few methodological and theoretical directions for future\nNLP in-and-for design research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1\">L Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1\">Lucienne T. M. Blessing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. (arXiv:2112.07577v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07577","description":"<p>Dense retrieval approaches can overcome the lexical gap and lead to\nsignificantly improved search results. However, they require large amounts of\ntraining data which is not available for most domains. As shown in previous\nwork (Thakur et al., 2021b), the performance of dense retrievers severely\ndegrades under a domain shift. This limits the usage of dense retrieval\napproaches to only a few domains with large training datasets.\n</p>\n<p>In this paper, we propose the novel unsupervised domain adaptation method\nGenerative Pseudo Labeling (GPL), which combines a query generator with pseudo\nlabeling from a cross-encoder. On six representative domain-specialized\ndatasets, we find the proposed GPL can outperform an out-of-the-box\nstate-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL\nrequires less (unlabeled) data from the target domain and is more robust in its\ntraining than previous methods.\n</p>\n<p>We further investigate the role of six recent pre-training methods in the\nscenario of domain adaptation for retrieval tasks, where only three could yield\nimproved results. The best approach, TSDAE (Wang et al., 2021) can be combined\nwith GPL, yielding another average improvement of 1.4 points nDCG@10 across the\nsix tasks. Code and models are available at https://gpl.sbert.net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases. (arXiv:2112.07868v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07868","description":"<p>Detecting social bias in text is challenging due to nuance, subjectivity, and\ndifficulty in obtaining good quality labeled datasets at scale, especially\ngiven the evolving nature of social biases and society. To address these\nchallenges, we propose a few-shot instruction-based method for prompting\npre-trained language models (LMs). We select a few class-balanced exemplars\nfrom a small support repository that are closest to the query to be labeled in\nthe embedding space. We then provide the LM with instruction that consists of\nthis subset of labeled exemplars, the query text to be classified, a definition\nof bias, and prompt it to make a decision. We demonstrate that large LMs used\nin a few-shot context can detect different types of fine-grained biases with\nsimilar and sometimes superior accuracy to fine-tuned models. We observe that\nthe largest 530B parameter model is significantly more effective in detecting\nsocial bias compared to smaller models (achieving at least 13% improvement in\nAUC metric compared to other models). It also maintains a high AUC (dropping\nless than 2%) when the labeled repository is reduced to as few as $100$\nsamples. Large pretrained language models thus make it easier and quicker to\nbuild new bias detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocielnik_R/0/1/0/all/0/1\">Rafal Kocielnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving both domain robustness and domain adaptability in machine translation. (arXiv:2112.08288v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08288","description":"<p>We consider two problems of NMT domain adaptation using meta-learning. First,\nwe want to reach domain robustness, i.e., we want to reach high quality on both\ndomains seen in the training data and unseen domains. Second, we want our\nsystems to be adaptive, i.e., making it possible to finetune systems with just\nhundreds of in-domain parallel sentences. We study the domain adaptability of\nmeta-learning when improving the domain robustness of the model. In this paper,\nwe propose a novel approach, RMLNMT (Robust Meta-Learning Framework for Neural\nMachine Translation Domain Adaptation), which improves the robustness of\nexisting meta-learning models. More specifically, we show how to use a domain\nclassifier in curriculum learning and we integrate the word-level domain mixing\nmodel into the meta-learning framework with a balanced sampling strategy.\nExperiments on English$\\rightarrow$German and English$\\rightarrow$Chinese\ntranslation show that RMLNMT improves in terms of both domain robustness and\ndomain adaptability in seen and unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relationship extraction for knowledge graph creation from biomedical literature. (arXiv:2201.01647v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2201.01647","description":"<p>Biomedical research is growing at such an exponential pace that scientists,\nresearchers, and practitioners are no more able to cope with the amount of\npublished literature in the domain. The knowledge presented in the literature\nneeds to be systematized in such a way that claims and hypotheses can be easily\nfound, accessed, and validated. Knowledge graphs can provide such a framework\nfor semantic knowledge representation from literature. However, in order to\nbuild a knowledge graph, it is necessary to extract knowledge as relationships\nbetween biomedical entities and normalize both entities and relationship types.\nIn this paper, we present and compare a few rule-based and machine\nlearning-based (Naive Bayes, Random Forests as examples of traditional machine\nlearning methods and DistilBERT and T5-based models as examples of modern deep\nlearning transformers) methods for scalable relationship extraction from\nbiomedical literature, and for the integration into the knowledge graphs. We\nexamine how resilient are these various methods to unbalanced and fairly small\ndatasets. Our experiments show that transformer-based models handle well both\nsmall (due to pre-training on a large dataset) and unbalanced datasets. The\nbest performing model was the DistilBERT-based model fine-tuned on balanced\ndata, with a reported F1-score of 0.89.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nikola Milosevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thielemann_W/0/1/0/all/0/1\">Wolfgang Thielemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-based Data Augmentation for Math Word Problems. (arXiv:2201.02489v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02489","description":"<p>It's hard for neural MWP solvers to deal with tiny local variances. In MWP\ntask, some local changes conserve the original semantic while the others may\ntotally change the underlying logic. Currently, existing datasets for MWP task\ncontain limited samples which are key for neural models to learn to\ndisambiguate different kinds of local variances in questions and solve the\nquestions correctly. In this paper, we propose a set of novel data augmentation\napproaches to supplement existing datasets with such data that are augmented\nwith different kinds of local variances, and help to improve the generalization\nability of current neural models. New samples are generated by knowledge guided\nentity replacement, and logic guided problem reorganization. The augmentation\napproaches are ensured to keep the consistency between the new data and their\nlabels. Experimental results have shown the necessity and the effectiveness of\nour methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ailisi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval. (arXiv:2201.02772v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02772","description":"<p>Cross-Modal Retrieval (CMR) is an important research topic across multimodal\ncomputing and information retrieval, which takes one type of data as the query\nto retrieve relevant data of another type. It has been widely used in many\nreal-world applications. Recently, the vision-language pre-trained models\nrepresented by CLIP demonstrate its superiority in learning the visual and\ntextual representations and gain impressive performance on various vision and\nlanguage related tasks. Although CLIP as well as the previous pre-trained\nmodels have shown great performance improvement in the unsupervised CMR, the\nperformance and impact of these pre-trained models on the supervised CMR were\nrarely explored due to the lack of common representation for the multimodal\nclass-level associations. In this paper, we take CLIP as the current\nrepresentative vision-language pre-trained model to conduct a comprehensive\nempirical study. We evaluate its performance and impact on the supervised CMR,\nand attempt to answer several key research questions. To this end, we first\npropose a novel model CLIP4CMR (CLIP enhanced network for Cross-Modal\nRetrieval) that employs the pre-trained CLIP as backbone network to perform the\nsupervised CMR. Then by means of the CLIP4CMR framework, we revisit the design\nof different learning objectives in current CMR methods to provide new insights\non model design. Moreover, we investigate the most concerned aspects in\napplying CMR, including the robustness to modality imbalance and sensitivity to\nhyper-parameters, to provide new perspectives for practical applications.\nThrough extensive experiments, we show that CLIP4CMR achieves the SOTA results\nwith prominent improvements on the benchmark datasets, and can be used as a\nfundamental framework to empirically study the key research issues of the\nsupervised CMR, with significant implications for model design and practical\nconsiderations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhixiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenji Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Many Ways to Be Lonely: Fine-Grained Characterization of Loneliness and Its Potential Changes in COVID-19. (arXiv:2201.07423v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07423","description":"<p>Loneliness has been associated with negative outcomes for physical and mental\nhealth. Understanding how people express and cope with various forms of\nloneliness is critical for early screening and targeted interventions to reduce\nloneliness, particularly among vulnerable groups such as young adults. To\nexamine how different forms of loneliness and coping strategies manifest in\nloneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained\nLoneliness) by using Reddit posts in two young adult-focused forums and two\nloneliness related forums consisting of a diverse age group. We provided\nannotations by trained human annotators for binary and fine-grained loneliness\nclassifications of the posts. Trained on FIG-Loneliness, two BERT-based models\nwere used to understand loneliness forms and authors' coping strategies in\nthese forums. Our binary loneliness classification achieved an accuracy above\n97%, and fine-grained loneliness category classification reached an average\naccuracy of 77% across all labeled categories. With FIG-Loneliness and model\npredictions, we found that loneliness expressions in the young adults related\nforums were distinct from other forums. Those in young adult-focused forums\nwere more likely to express concerns pertaining to peer relationship, and were\npotentially more sensitive to geographical isolation impacted by the COVID-19\npandemic lockdown. Also, we showed that different forms of loneliness have\ndifferential use in coping strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yueyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunfan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leqi_L/0/1/0/all/0/1\">Liu Leqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkielman_P/0/1/0/all/0/1\">Piotr Winkielman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11114","description":"<p>Some neurons in deep networks specialize in recognizing highly specific\nperceptual, structural, or semantic features of inputs. In computer vision,\ntechniques exist for identifying neurons that respond to individual concept\ncategories like colors, textures, and object classes. But these techniques are\nlimited in scope, labeling only a small subset of neurons and behaviors in any\nnetwork. Is a richer characterization of neuron-level computation possible? We\nintroduce a procedure (called MILAN, for mutual-information-guided linguistic\nannotation of neurons) that automatically labels neurons with open-ended,\ncompositional, natural language descriptions. Given a neuron, MILAN generates a\ndescription by searching for a natural language string that maximizes pointwise\nmutual information with the image regions in which the neuron is active. MILAN\nproduces fine-grained descriptions that capture categorical, relational, and\nlogical structure in learned features. These descriptions obtain high agreement\nwith human-generated feature descriptions across a diverse set of model\narchitectures and tasks, and can aid in understanding and controlling learned\nmodels. We highlight three applications of natural language neuron\ndescriptions. First, we use MILAN for analysis, characterizing the distribution\nand importance of neurons selective for attribute, category, and relational\ninformation in vision models. Second, we use MILAN for auditing, surfacing\nneurons sensitive to human faces in datasets designed to obscure them. Finally,\nwe use MILAN for editing, improving robustness in an image classifier by\ndeleting neurons sensitive to text features spuriously correlated with class\nlabels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_E/0/1/0/all/0/1\">Evan Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1\">Sarah Schwettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagashvili_T/0/1/0/all/0/1\">Teona Bagashvili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RerrFact: Reduced Evidence Retrieval Representations for Scientific Claim Verification. (arXiv:2202.02646v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02646","description":"<p>Exponential growth in digital information outlets and the race to publish has\nmade scientific misinformation more prevalent than ever. However, the task to\nfact-verify a given scientific claim is not straightforward even for\nresearchers. Scientific claim verification requires in-depth knowledge and\ngreat labor from domain experts to substantiate supporting and refuting\nevidence from credible scientific sources. The SciFact dataset and\ncorresponding task provide a benchmarking leaderboard to the community to\ndevelop automatic scientific claim verification systems via extracting and\nassimilating relevant evidence rationales from source abstracts. In this work,\nwe propose a modular approach that sequentially carries out binary\nclassification for every prediction subtask as in the SciFact leaderboard. Our\nsimple classifier-based approach uses reduced abstract representations to\nretrieve relevant abstracts. These are further used to train the relevant\nrationale-selection model. Finally, we carry out two-step stance predictions\nthat first differentiate non-relevant rationales and then identify supporting\nor refuting rationales for a given claim. Experimentally, our system RerrFact\nwith no fine-tuning, simple design, and a fraction of model parameters fairs\ncompetitively on the leaderboard against large-scale, modular, and joint\nmodeling approaches. We make our codebase available at\nhttps://github.com/ashishrana160796/RerrFact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Ashish Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_D/0/1/0/all/0/1\">Deepanshu Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_T/0/1/0/all/0/1\">Tirthankar Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Muskaan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_P/0/1/0/all/0/1\">Prashant Singh Rana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion. (arXiv:2202.13785v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.13785","description":"<p>Knowledge graphs store a large number of factual triples while they are still\nincomplete, inevitably. The previous knowledge graph completion (KGC) models\npredict missing links between entities merely relying on fact-view data,\nignoring the valuable commonsense knowledge. The previous knowledge graph\nembedding (KGE) techniques suffer from invalid negative sampling and the\nuncertainty of fact-view link prediction, limiting KGC's performance. To\naddress the above challenges, we propose a novel and scalable Commonsense-Aware\nKnowledge Embedding (CAKE) framework to automatically extract commonsense from\nfactual triples with entity concepts. The generated commonsense augments\neffective self-supervision to facilitate both high-quality negative sampling\n(NS) and joint commonsense and fact-view link prediction. Experimental results\non the KGC task demonstrate that assembling our framework could enhance the\nperformance of the original KGE models, and the proposed commonsense-aware NS\nmodule is superior to other NS techniques. Besides, our proposed framework\ncould be easily adaptive to various KGE models and explain the predicted\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guanglin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration Network for Multilingual Complex Named Entity Recognition. (arXiv:2203.03216v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03216","description":"<p>This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2022 Task 11 Multilingual Complex Named Entity Recognition\n(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to\nimprove the performance of language models for recognizing complex named\nentities. The method first adapts the representations of gazetteer networks to\nthose of language models by minimizing the KL divergence between them. After\nadaptation, these two networks are then integrated for backend supervised named\nentity recognition (NER) training. The proposed method is applied to several\nstate-of-the-art Transformer-based NER models with a gazetteer built from\nWikidata, and shows great generalization ability across them. The final\npredictions are derived from an ensemble of these trained models. Experimental\nresults and detailed analysis verify the effectiveness of the proposed method.\nThe official results show that our system ranked 1st on three tracks (Chinese,\nCode-mixed and Bangla) and 2nd on the other ten tracks in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beiduo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun-Yu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiajun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillNet: A Sparsely Activated Model for General-Purpose Natural Language Understanding. (arXiv:2203.03312v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03312","description":"<p>Prevailing deep models are single-purpose and overspecialize at individual\ntasks. However, when being extended to new tasks, they typically forget\npreviously learned skills and learn from scratch. We address this issue by\nintroducing SkillNet, a general-purpose model that stitches together existing\nskills to learn new tasks more effectively. The key feature of our approach is\nthat it is sparsely activated guided by predefined skills. Different from\ntraditional dense models that always activate all the model parameters,\nSkillNet only activates parts of the model parameters whose skills are relevant\nto the target task. When learning for a new task, our approach precisely\nactivates required skills and also provides an option to add new skills. We\nevaluate on natural language understandings tasks and have the following\nfindings. First, with only one model checkpoint, SkillNet performs better than\ntask-specific fine-tuning and two multi-task learning baselines (i.e., dense\nmodel and Mixture-of-Experts model) on six tasks. Second, sparsely activated\npre-training further improves the overall performance. Third, SkillNet\nsignificantly outperforms baseline systems when being extended to new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons. (arXiv:2203.06063v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06063","description":"<p>Recent studies have shown the advantages of evaluating NLG systems using\npairwise comparisons as opposed to direct assessment. Given $k$ systems, a\nnaive approach for identifying the top-ranked system would be to uniformly\nobtain pairwise comparisons from all ${k \\choose 2}$ pairs of systems. However,\nthis can be very expensive as the number of human annotations required would\ngrow quadratically with $k$. In this work, we introduce Active Evaluation, a\nframework to efficiently identify the top-ranked system by actively choosing\nsystem pairs for comparison using dueling bandit algorithms. We perform\nextensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation\ndatasets spanning 5 tasks and show that the number of human annotations can be\nreduced by 80%. To further reduce the number of human annotations, we propose\nmodel-based dueling bandit algorithms which combine automatic evaluation\nmetrics with human evaluations. Specifically, we eliminate sub-optimal systems\neven before the human annotation process and perform human evaluations only on\ntest examples where the automatic metric is highly uncertain. This reduces the\nnumber of human annotations required further by 89%. In effect, we show that\nidentifying the top-ranked system requires only a few hundred human\nannotations, which grow linearly with $k$. Lastly, we provide practical\nrecommendations and best practices to identify the top-ranked system\nefficiently. Our code has been made publicly available at\nhttps://github.com/akashkm99/duelnlg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohankumar_A/0/1/0/all/0/1\">Akash Kumar Mohankumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALT: um software para an\\'alise de legibilidade de textos em L\\'ingua Portuguesa. (arXiv:2203.12135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12135","description":"<p>In the initial stage of human life, communication, seen as a process of\nsocial interaction, was always the best way to reach consensus between the\nparties. Understanding and credibility in this process are essential for the\nmutual agreement to be validated. But, how to do it so that this communication\nreaches the great mass? This is the main challenge when what is sought is the\ndissemination of information and its approval. In this context, this study\npresents the ALT software, developed from original readability metrics adapted\nto the Portuguese language, available on the web, to reduce communication\ndifficulties. The development of the software was motivated by the theory of\ncommunicative action of Habermas, which uses a multidisciplinary style to\nmeasure the credibility of the discourse in the communication channels used to\nbuild and maintain a safe and healthy relationship with the public.\n</p>\n<p>--\n</p>\n<p>No est\\'agio inicial da vida humana a comunica\\c{c}\\~ao, vista como um\nprocesso de intera\\c{c}\\~ao social, foi sempre o melhor caminho para o consenso\nentre as partes. O entendimento e a credibilidade nesse processo s\\~ao\nfundamentais para que o acordo m\\'utuo seja validado. Mas, como faz\\^e-lo de\nforma que essa comunica\\c{c}\\~ao alcance a grande massa? Esse \\'e o principal\ndesafio quando o que se busca \\'e a difus\\~ao da informa\\c{c}\\~ao e a sua\naprova\\c{c}\\~ao. Nesse contexto, este estudo apresenta o software ALT,\ndesenvolvido a partir de m\\'etricas de legibilidade originais adaptadas para a\nL\\'ingua Portuguesa, dispon\\'ivel na web, para reduzir as dificuldades na\ncomunica\\c{c}\\~ao. O desenvolvimento do software foi motivado pela teoria do\nagir comunicativo de Habermas, que faz uso de um estilo multidisciplinar para\nmedir a credibilidade do discurso nos canais de comunica\\c{c}\\~ao utilizados\npara construir e manter uma rela\\c{c}\\~ao segura e saud\\'avel com o p\\'ublico.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreno_G/0/1/0/all/0/1\">Gleice Carvalho de Lima Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1\">Marco P. M. de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_N/0/1/0/all/0/1\">Nelson Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_A/0/1/0/all/0/1\">Adriana Kroenke Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Classification with Structured Information for Disfluency Detection in Spoken Utterances. (arXiv:2203.16028v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16028","description":"<p>Existing approaches in disfluency detection focus on solving a token-level\nclassification task for identifying and removing disfluencies in text.\nMoreover, most works focus on leveraging only contextual information captured\nby the linear sequences in text, thus ignoring the structured information in\ntext which is efficiently captured by dependency trees. In this paper, building\non the span classification paradigm of entity recognition, we propose a novel\narchitecture for detecting disfluencies in transcripts from spoken utterances,\nincorporating both contextual information through transformers and\nlong-distance structured information captured by dependency trees, through\ngraph convolutional networks (GCNs). Experimental results show that our\nproposed model achieves state-of-the-art results on the widely used English\nSwitchboard for disfluency detection and outperforms prior-art by a significant\nmargin. We make all our codes publicly available on GitHub\n(https://github.com/Sreyan88/Disfluency-Detection-with-Span-Classification)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sonal Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04392","description":"<p>Pretrained language models can be effectively stimulated by textual prompts\nor demonstrations, especially in low-data scenarios. Recent works have focused\non automatically searching discrete or continuous prompts or optimized\nverbalizers, yet studies for the demonstration are still limited. Concretely,\nthe demonstration examples are crucial for an excellent final performance of\nprompt-tuning. In this paper, we propose a novel pluggable, extensible, and\nefficient approach named contrastive demonstration tuning, which is free of\ndemonstration sampling. Furthermore, the proposed approach can be: (i) Plugged\nto any previous prompt-tuning approaches; (ii) Extended to widespread\nclassification tasks with a large number of categories. Experimental results on\n16 datasets illustrate that our method integrated with previous approaches\nLM-BFF and P-tuning can yield better performance. Code is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias. (arXiv:2204.04902v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04902","description":"<p>Media framing bias can lead to increased political polarization, and thus,\nthe need for automatic mitigation methods is growing. We propose a new task, a\nneutral summary generation from multiple news headlines of the varying\npolitical leanings to facilitate balanced and unbiased news reading. In this\npaper, we first collect a new dataset, obtain insights about framing bias\nthrough a case study, and propose a new effective metric and models for the\ntask. Lastly, we conduct experimental analyses to provide insights about\nremaining challenges and future directions. One of the most interesting\nobservations is that generation models can hallucinate not only factually\ninaccurate or unverifiable content, but also politically biased content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification. (arXiv:2204.04952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04952","description":"<p>Text classification struggles to generalize to unseen classes with very few\nlabeled text instances per class. In such a few-shot learning (FSL) setting,\nmetric-based meta-learning approaches have shown promising results. Previous\nstudies mainly aim to derive a prototype representation for each class.\nHowever, they neglect that it is challenging-yet-unnecessary to construct a\ncompact representation which expresses the entire meaning for each class. They\nalso ignore the importance to capture the inter-dependency between query and\nthe support set for few-shot text classification. To deal with these issues, we\npropose a meta-learning based method MGIMN which performs instance-wise\ncomparison followed by aggregation to generate class-wise matching vectors\ninstead of prototype learning. The key of instance-wise comparison is the\ninteractive matching within the class-specific context and episode-specific\ncontext. Extensive experiments demonstrate that the proposed method\nsignificantly outperforms the existing state-of-the-art approaches, under both\nthe standard FSL and generalized FSL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maimaiti_M/0/1/0/all/0/1\">Mieradilijiang Maimaiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanhang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving Deep into Regularity: A Simple but Effective Method for Chinese Named Entity Recognition. (arXiv:2204.05544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05544","description":"<p>Recent years have witnessed the improving performance of Chinese Named Entity\nRecognition (NER) from proposing new frameworks or incorporating word lexicons.\nHowever, the inner composition of entity mentions in character-level Chinese\nNER has been rarely studied. Actually, most mentions of regular types have\nstrong name regularity. For example, entities end with indicator words such as\n\"company\" or \"bank\" usually belong to organization. In this paper, we propose a\nsimple but effective method for investigating the regularity of entity spans in\nChinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON).\nSpecifically, the proposed model consists of two branches: a regularity-aware\nmodule and a regularityagnostic module. The regularity-aware module captures\nthe internal regularity of each span for better entity type prediction, while\nthe regularity-agnostic module is employed to locate the boundary of entities\nand relieve the excessive attention to span regularity. An orthogonality space\nis further constructed to encourage two modules to extract different aspects of\nregularity features. To verify the effectiveness of our method, we conduct\nextensive experiments on three benchmark datasets and a practical medical\ndataset. The experimental results show that our RICON significantly outperforms\nprevious state-of-the-art methods, including various lexicon-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yingjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Jing Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2204.05999","description":"<p>Code is seldom written in a single left-to-right pass and is instead\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\nthat can perform program synthesis (via left-to-right generation) as well as\nediting (via infilling). InCoder is trained to generate code files from a large\ncorpus of permissively licensed code, where regions of code have been randomly\nmasked and moved to the end of each file, allowing code infilling with\nbidirectional context. Our model is the first generative model that is able to\ndirectly perform zero-shot code infilling, which we evaluate on challenging\ntasks such as type inference, comment generation, and variable re-naming. We\nfind that the ability to condition on bidirectional context substantially\nimproves performance on these tasks, while still performing comparably on\nstandard program synthesis benchmarks in comparison to left-to-right only\nmodels pretrained at similar scale. The InCoder models and code are publicly\nreleased. https://sites.google.com/view/incoder-code-models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Freda Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing. (arXiv:2204.06625v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06625","description":"<p>Model ensemble is a popular approach to produce a low-variance and\nwell-generalized model. However, it induces large memory and inference costs,\nwhich are often not affordable for real-world deployment. Existing work has\nresorted to sharing weights among models. However, when increasing the\nproportion of the shared weights, the resulting models tend to be similar, and\nthe benefits of using model ensemble diminish. To retain ensemble benefits\nwhile maintaining a low memory cost, we propose a consistency-regularized\nensemble learning approach based on perturbed models, named CAMERO.\nSpecifically, we share the weights of bottom layers across all models and apply\ndifferent perturbations to the hidden representations for different models,\nwhich can effectively promote the model diversity. Meanwhile, we apply a\nprediction consistency regularizer across the perturbed models to control the\nvariance due to the model diversity. Our experiments using large language\nmodels demonstrate that CAMERO significantly improves the generalization\nperformance of the ensemble model. Specifically, CAMERO outperforms the\nstandard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a\nsignificantly smaller model size (114.2M vs. 880.6M).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals. (arXiv:2204.06644v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.06644","description":"<p>We present an efficient method of pretraining large-scale autoencoding\nlanguage models using training signals generated by an auxiliary model.\nOriginated in ELECTRA, this training strategy has demonstrated\nsample-efficiency to pretrain models at the scale of hundreds of millions of\nparameters. In this work, we conduct a comprehensive empirical study, and\npropose a recipe, namely \"Model generated dEnoising TRaining Objective\"\n(METRO), which incorporates some of the best modeling techniques developed\nrecently to speed up, stabilize, and enhance pretrained language models without\ncompromising model effectiveness. The resultant models, METRO-LM, consisting of\nup to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,\nSuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in\nthat they often outperform previous large models with significantly smaller\nmodel sizes and lower pretraining cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_S/0/1/0/all/0/1\">Saurabh Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"$\\Upsilon$-Net: A Spatiospectral Network for Retinal OCT Segmentation. (arXiv:2204.07613v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07613","description":"<p>Automated segmentation of retinal optical coherence tomography (OCT) images\nhas become an important recent direction in machine learning for medical\napplications. We hypothesize that the anatomic structure of layers and their\nhigh-frequency variation in OCT images make retinal OCT a fitting choice for\nextracting spectral-domain features and combining them with spatial domain\nfeatures. In this work, we present $\\Upsilon$-Net, an architecture that\ncombines the frequency domain features with the image domain to improve the\nsegmentation performance of OCT images. The results of this work demonstrate\nthat the introduction of two branches, one for spectral and one for spatial\ndomain features, brings a very significant improvement in fluid segmentation\nperformance and allows outperformance as compared to the well-known U-Net\nmodel. Our improvement was 13% on the fluid segmentation dice score and 1.9% on\nthe average dice score. Finally, removing selected frequency ranges in the\nspectral domain demonstrates the impact of these features on the fluid\nsegmentation outperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Farshad_A/0/1/0/all/0/1\">Azade Farshad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeganeh_Y/0/1/0/all/0/1\">Yousef Yeganeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gehlbach_P/0/1/0/all/0/1\">Peter Gehlbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Frame Self-Supervised Depth with Transformers. (arXiv:2204.07616v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07616","description":"<p>Multi-frame depth estimation improves over single-frame approaches by also\nleveraging geometric relationships between images via feature matching, in\naddition to learning appearance-based features. In this paper we revisit\nfeature matching for self-supervised monocular depth estimation, and propose a\nnovel transformer architecture for cost volume generation. We use\ndepth-discretized epipolar sampling to select matching candidates, and refine\npredictions through a series of self- and cross-attention layers. These layers\nsharpen the matching probability between pixel features, improving over\nstandard similarity metrics prone to ambiguities and local minima. The refined\ncost volume is decoded into depth estimates, and the whole pipeline is trained\nend-to-end from videos using only a photometric objective. Experiments on the\nKITTI and DDAD datasets show that our DepthFormer architecture establishes a\nnew state of the art in self-supervised monocular depth estimation, and is even\ncompetitive with highly specialized supervised single-frame architectures. We\nalso show that our learned cross-attention network yields representations\ntransferable across datasets, increasing the effectiveness of pre-training\nstrategies. Project page: https://sites.google.com/tri.global/depthformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1\">Sergey Zakharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lagrangian Motion Magnification with Double Sparse Optical Flow Decomposition. (arXiv:2204.07636v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07636","description":"<p>Motion magnification techniques aim at amplifying and hence revealing subtle\nmotion in videos. There are basically two main approaches to reach this goal,\nnamely via Eulerian or Lagrangian techniques. While the first one magnifies\nmotion implicitly by operating directly on image pixels, the Lagrangian\napproach uses optical flow techniques to extract and amplify pixel\ntrajectories. Microexpressions are fast and spatially small facial expressions\nthat are difficult to detect. In this paper, we propose a novel approach for\nlocal Lagrangian motion magnification of facial micromovements. Our\ncontribution is three-fold: first, we fine-tune the recurrent all-pairs field\ntransforms for optical flows (RAFT) deep learning approach for faces by adding\nground truth obtained from the variational dense inverse search (DIS) for\noptical flow algorithm applied to the CASME II video set of faces. This enables\nus to produce optical flows of facial videos in an efficient and sufficiently\naccurate way. Second, since facial micromovements are both local in space and\ntime, we propose to approximate the optical flow field by sparse components\nboth in space and time leading to a double sparse decomposition. Third, we use\nthis decomposition to magnify micro-motions in specific areas of the face,\nwhere we introduce a new forward warping strategy using a triangular splitting\nof the image grid and barycentric interpolation of the RGB vectors at the\ncorners of the transformed triangles. We demonstrate the very good performance\nof our approach by various examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flotho_P/0/1/0/all/0/1\">Philipp Flotho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiss_C/0/1/0/all/0/1\">Cosmas Heiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steidl_G/0/1/0/all/0/1\">Gabriele Steidl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strauss_D/0/1/0/all/0/1\">Daniel J. Strauss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-aided Direct Sparse Odometry. (arXiv:2204.07640v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07640","description":"<p>We introduce EDS, a direct monocular visual odometry using events and frames.\nOur algorithm leverages the event generation model to track the camera motion\nin the blind time between frames. The method formulates a direct probabilistic\napproach of observed brightness increments. Per-pixel brightness increments are\npredicted using a sparse number of selected 3D points and are compared to the\nevents via the brightness increment error to estimate camera motion. The method\nrecovers a semi-dense 3D map using photometric bundle adjustment. EDS is the\nfirst method to perform 6-DOF VO using events and frames with a direct\napproach. By design, it overcomes the problem of changing appearance in\nindirect methods. We also show that, for a target error performance, EDS can\nwork at lower frame rates than state-of-the-art frame-based VO solutions. This\nopens the door to low-power motion-tracking applications where frames are\nsparingly triggered \"on demand\" and our method tracks the motion in between. We\nrelease code and datasets to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hidalgo_Carrio_J/0/1/0/all/0/1\">Javier Hidalgo-Carri&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEarth 2022 -- Multimodal Learning for Earth and Environment Workshop and Challenge. (arXiv:2204.07649v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07649","description":"<p>The Multimodal Learning for Earth and Environment Challenge (MultiEarth 2022)\nwill be the first competition aimed at the monitoring and analysis of\ndeforestation in the Amazon rainforest at any time and in any weather\nconditions. The goal of the Challenge is to provide a common benchmark for\nmultimodal information processing and to bring together the earth and\nenvironmental science communities as well as multimodal representation learning\ncommunities to compare the relative merits of the various multimodal learning\nmethods to deforestation estimation under well-defined and strictly comparable\nconditions. MultiEarth 2022 will have three sub-challenges: 1) matrix\ncompletion, 2) deforestation estimation, and 3) image-to-image translation.\nThis paper presents the challenge guidelines, datasets, and evaluation metrics\nfor the three sub-challenges. Our challenge website is available at\nhttps://sites.google.com/view/rainforest-challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Miriam Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Morgan Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelides_G/0/1/0/all/0/1\">Gregory Angelides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_M/0/1/0/all/0/1\">Mark Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_S/0/1/0/all/0/1\">Sam Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1\">Armando Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perron_T/0/1/0/all/0/1\">Taylor Perron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_B/0/1/0/all/0/1\">Bill Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swenson_B/0/1/0/all/0/1\">Brandon Swenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piou_J/0/1/0/all/0/1\">Jean Piou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Unlearning via Randomized Conditionally Independent Hessians. (arXiv:2204.07655v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07655","description":"<p>Recent legislation has led to interest in machine unlearning, i.e., removing\nspecific training samples from a predictive model as if they never existed in\nthe training dataset. Unlearning may also be required due to\ncorrupted/adversarial data or simply a user's updated privacy requirement. For\nmodels which require no training (k-NN), simply deleting the closest original\nsample can be effective. But this idea is inapplicable to models which learn\nricher representations. Recent ideas leveraging optimization-based updates\nscale poorly with the model dimension d, due to inverting the Hessian of the\nloss function. We use a variant of a new conditional independence coefficient,\nL-CODEC, to identify a subset of the model parameters with the most semantic\noverlap on an individual sample level. Our approach completely avoids the need\nto invert a (possibly) huge matrix. By utilizing a Markov blanket selection, we\npremise that L-CODEC is also suitable for deep unlearning, as well as other\napplications in vision. Compared to alternatives, L-CODEC makes approximate\nunlearning possible in settings that would otherwise be infeasible, including\nvision models used for face recognition, person re-identification and NLP\nmodels that may require unlearning samples identified for exclusion. Code can\nbe found at https://github.com/vsingh-group/LCODEC-deep-unlearning/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Ronak Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Sourav Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikas Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sathya N. Ravi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection. (arXiv:2204.07660v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07660","description":"<p>Datasets that capture the connection between vision, language, and affection\nare limited, causing a lack of understanding of the emotional aspect of human\nintelligence. As a step in this direction, the ArtEmis dataset was recently\nintroduced as a large-scale dataset of emotional reactions to images along with\nlanguage explanations of these chosen emotions. We observed a significant\nemotional bias towards instance-rich emotions, making trained neural speakers\nless accurate in describing under-represented emotions. We show that collecting\nnew data, in the same way, is not effective in mitigating this emotional bias.\nTo remedy this problem, we propose a contrastive data collection approach to\nbalance ArtEmis with a new complementary dataset such that a pair of similar\nimages have contrasting emotions (one positive and one negative). We collected\n260,533 instances using the proposed method, we combine them with ArtEmis,\ncreating a second iteration of the dataset. The new combined dataset, dubbed\nArtEmis v2.0, has a balanced distribution of emotions with explanations\nrevealing more fine details in the associated painting. Our experiments show\nthat neural speakers trained on the new dataset improve CIDEr and METEOR\nevaluation metrics by 20% and 7%, respectively, compared to the biased dataset.\nFinally, we also show that the performance per emotion of neural speakers is\nimproved across all the emotion categories, significantly on under-represented\nemotions. The collected dataset and code are available at\nhttps://artemisdataset-v2.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_Y/0/1/0/all/0/1\">Youssef Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Faizan Farooq Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haydarov_K/0/1/0/all/0/1\">Kilichbek Haydarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Similarity Priors: Neural Collages as Differentiable Fractal Representations. (arXiv:2204.07673v1 [cs.LG])","link":"http://arxiv.org/abs/2204.07673","description":"<p>Many patterns in nature exhibit self-similarity: they can be compactly\ndescribed via self-referential transformations. Said patterns commonly appear\nin natural and artificial objects, such as molecules, shorelines, galaxies and\neven images. In this work, we investigate the role of learning in the automated\ndiscovery of self-similarity and in its utilization for downstream tasks. To\nthis end, we design a novel class of implicit operators, Neural Collages, which\n(1) represent data as the parameters of a self-referential, structured\ntransformation, and (2) employ hypernetworks to amortize the cost of finding\nthese parameters to a single forward pass. We investigate how to leverage the\nrepresentations produced by Neural Collages in various tasks, including data\ncompression and generation. Neural Collages image compressors are orders of\nmagnitude faster than other self-similarity-based algorithms during encoding\nand offer compression rates competitive with implicit methods. Finally, we\nshowcase applications of Neural Collages for fractal art and as deep generative\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1\">Stefano Massaroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kuno Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safe Self-Refinement for Transformer-based Domain Adaptation. (arXiv:2204.07683v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07683","description":"<p>Unsupervised Domain Adaptation (UDA) aims to leverage a label-rich source\ndomain to solve tasks on a related unlabeled target domain. It is a challenging\nproblem especially when a large domain gap lies between the source and target\ndomains. In this paper we propose a novel solution named SSRT (Safe\nSelf-Refinement for Transformer-based domain adaptation), which brings\nimprovement from two aspects. First, encouraged by the success of vision\ntransformers in various vision tasks, we arm SSRT with a transformer backbone.\nWe find that the combination of vision transformer with simple adversarial\nadaptation surpasses best reported Convolutional Neural Network (CNN)-based\nresults on the challenging DomainNet benchmark, showing its strong transferable\nfeature representation. Second, to reduce the risk of model collapse and\nimprove the effectiveness of knowledge transfer between domains with large\ngaps, we propose a Safe Self-Refinement strategy. Specifically, SSRT utilizes\npredictions of perturbed target domain data to refine the model. Since the\nmodel capacity of vision transformer is large and predictions in such\nchallenging tasks can be noisy, a safe training mechanism is designed to\nadaptively adjust learning configuration. Extensive evaluations are conducted\non several widely tested UDA benchmarks and SSRT achieves consistently the best\nperformances, including 85.43% on Office-Home, 88.76% on VisDA-2017 and 45.2%\non DomainNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianshuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Image Classification Using Isotropic Network. (arXiv:2204.07707v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07707","description":"<p>In this paper, we propose a privacy-preserving image classification method\nthat uses encrypted images and an isotropic network such as the vision\ntransformer. The proposed method allows us not only to apply images without\nvisual information to deep neural networks (DNNs) for both training and testing\nbut also to maintain a high classification accuracy. In addition, compressible\nencrypted images, called encryption-then-compression (EtC) images, can be used\nfor both training and testing without any adaptation network. Previously, to\nclassify EtC images, an adaptation network was required before a classification\nnetwork, so methods with an adaptation network have been only tested on small\nimages. To the best of our knowledge, previous privacy-preserving image\nclassification methods have never considered image compressibility and patch\nembedding-based isotropic networks. In an experiment, the proposed\nprivacy-preserving image classification was demonstrated to outperform\nstate-of-the-art methods even when EtC images were used in terms of\nclassification accuracy and robustness against various attacks under the use of\ntwo isotropic networks: vision transformer and ConvMixer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+MaungMaung_A/0/1/0/all/0/1\">AprilPyone MaungMaung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAUSS: Guided Encoder-Decoder Architecture for Hyperspectral Unmixing with Spatial Smoothness. (arXiv:2204.07713v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07713","description":"<p>In recent hyperspectral unmixing (HU) literature, the application of deep\nlearning (DL) has become more prominent, especially with the autoencoder (AE)\narchitecture. We propose a split architecture and use a pseudo-ground truth for\nabundances to guide the `unmixing network' (UN) optimization. Preceding the UN,\nan `approximation network' (AN) is proposed, which will improve the association\nbetween the centre pixel and its neighbourhood. Hence, it will accentuate\nspatial correlation in the abundances as its output is the input to the UN and\nthe reference for the `mixing network' (MN). In the Guided Encoder-Decoder\nArchitecture for Hyperspectral Unmixing with Spatial Smoothness (GAUSS), we\nproposed using one-hot encoded abundances as the pseudo-ground truth to guide\nthe UN; computed using the k-means algorithm to exclude the use of prior HU\nmethods. Furthermore, we release the single-layer constraint on MN by\nintroducing the UN generated abundances in contrast to the standard AE for HU.\nSecondly, we experimented with two modifications on the pre-trained network\nusing the GAUSS method. In GAUSS$_\\textit{blind}$, we have concatenated the UN\nand the MN to back-propagate the reconstruction error gradients to the encoder.\nThen, in the GAUSS$_\\textit{prime}$, abundance results of a signal processing\n(SP) method with reliable abundance results were used as the pseudo-ground\ntruth with the GAUSS architecture. According to quantitative and graphical\nresults for four experimental datasets, the three architectures either\ntranscended or equated the performance of existing HU algorithms from both DL\nand SP domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_Y/0/1/0/all/0/1\">Yasiru Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerasooriya_K/0/1/0/all/0/1\">Kavinga Weerasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godaliyadda_R/0/1/0/all/0/1\">Roshan Godaliyadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herath_V/0/1/0/all/0/1\">Vijitha Herath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_P/0/1/0/all/0/1\">Parakrama Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasundara_D/0/1/0/all/0/1\">Dhananjaya Jayasundara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanayake_L/0/1/0/all/0/1\">Lakshitha Ramanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senarath_N/0/1/0/all/0/1\">Neranjan Senarath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wickramasinghe_D/0/1/0/all/0/1\">Dulantha Wickramasinghe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Performance Limit of Scene Text Recognizer without Human Annotation. (arXiv:2204.07714v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07714","description":"<p>Scene text recognition (STR) attracts much attention over the years because\nof its wide application. Most methods train STR model in a fully supervised\nmanner which requires large amounts of labeled data. Although synthetic data\ncontributes a lot to STR, it suffers from the real-tosynthetic domain gap that\nrestricts model performance. In this work, we aim to boost STR models by\nleveraging both synthetic data and the numerous real unlabeled images,\nexempting human annotation cost thoroughly. A robust consistency regularization\nbased semi-supervised framework is proposed for STR, which can effectively\nsolve the instability issue due to domain inconsistency between synthetic and\nreal images. A character-level consistency regularization is designed to\nmitigate the misalignment between characters in sequence recognition. Extensive\nexperiments on standard text recognition benchmarks demonstrate the\neffectiveness of the proposed method. It can steadily improve existing STR\nmodels, and boost an STR model to achieve new state-of-the-art results. To our\nbest knowledge, this is the first consistency regularization based framework\nthat applies successfully to STR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Caiyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_S/0/1/0/all/0/1\">Seon-Min Rhee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jae-Joon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactiveness Field in Human-Object Interactions. (arXiv:2204.07718v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07718","description":"<p>Human-Object Interaction (HOI) detection plays a core role in activity\nunderstanding. Though recent two/one-stage methods have achieved impressive\nresults, as an essential step, discovering interactive human-object pairs\nremains challenging. Both one/two-stage methods fail to effectively extract\ninteractive pairs instead of generating redundant negative pairs. In this work,\nwe introduce a previously overlooked interactiveness bimodal prior: given an\nobject in an image, after pairing it with the humans, the generated pairs are\neither mostly non-interactive, or mostly interactive, with the former more\nfrequent than the latter. Based on this interactiveness bimodal prior we\npropose the \"interactiveness field\". To make the learned field compatible with\nreal HOI image considerations, we propose new energy constraints based on the\ncardinality and difference in the inherent \"interactiveness field\" underlying\ninteractive versus non-interactive pairs. Consequently, our method can detect\nmore precise pairs and thus significantly boost HOI detection performance,\nwhich is validated on widely-used benchmarks where we achieve decent\nimprovements over state-of-the-arts. Our code is available at\nhttps://github.com/Foruck/Interactiveness-Field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoqian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stress-Testing LiDAR Registration. (arXiv:2204.07719v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07719","description":"<p>Point cloud registration (PCR) is an important task in many fields including\nautonomous driving with LiDAR sensors. PCR algorithms have improved\nsignificantly in recent years, by combining deep-learned features with robust\nestimation methods. These algorithms succeed in scenarios such as indoor scenes\nand object models registration. However, testing in the automotive LiDAR\nsetting, which presents its own challenges, has been limited. The standard\nbenchmark for this setting, KITTI-10m, has essentially been saturated by recent\nalgorithms: many of them achieve near-perfect recall.\n</p>\n<p>In this work, we stress-test recent PCR techniques with LiDAR data. We\npropose a method for selecting balanced registration sets, which are\nchallenging sets of frame-pairs from LiDAR datasets. They contain a balanced\nrepresentation of the different relative motions that appear in a dataset, i.e.\nsmall and large rotations, small and large offsets in space and time, and\nvarious combinations of these.\n</p>\n<p>We perform a thorough comparison of accuracy and run-time on these\nbenchmarks. Perhaps unexpectedly, we find that the fastest and simultaneously\nmost accurate approach is a version of advanced RANSAC. We further improve\nresults with a novel pre-filtering method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drory_A/0/1/0/all/0/1\">Amnon Drory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching Intrinsic Dimensions of Vision Transformers. (arXiv:2204.07722v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07722","description":"<p>It has been shown by many researchers that transformers perform as well as\nconvolutional neural networks in many computer vision tasks. Meanwhile, the\nlarge computational costs of its attention module hinder further studies and\napplications on edge devices. Some pruning methods have been developed to\nconstruct efficient vision transformers, but most of them have considered image\nclassification tasks only. Inspired by these results, we propose SiDT, a method\nfor pruning vision transformer backbones on more complicated vision tasks like\nobject detection, based on the search of transformer dimensions. Experiments on\nCIFAR-100 and COCO datasets show that the backbones with 20\\% or 40\\%\ndimensions/parameters pruned can have similar or even better performance than\nthe unpruned models. Moreover, we have also provided the complexity analysis\nand comparisons with the previous pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fanghui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yingyong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic interpretation for convolutional neural networks: What makes a cat a cat?. (arXiv:2204.07724v1 [cs.LG])","link":"http://arxiv.org/abs/2204.07724","description":"<p>The interpretability of deep neural networks has attracted increasing\nattention in recent years, and several methods have been created to interpret\nthe \"black box\" model. Fundamental limitations remain, however, that impede the\npace of understanding the networks, especially the extraction of understandable\nsemantic space. In this work, we introduce the framework of semantic\nexplainable AI (S-XAI), which utilizes row-centered principal component\nanalysis to obtain the common traits from the best combination of superpixels\ndiscovered by a genetic algorithm, and extracts understandable semantic spaces\non the basis of discovered semantically sensitive neurons and visualization\ntechniques. Statistical interpretation of the semantic space is also provided,\nand the concept of semantic probability is proposed for the first time. Our\nexperimental results demonstrate that S-XAI is effective in providing a\nsemantic interpretation for the CNN, and offers broad usage, including\ntrustworthiness assessment and semantic sample searching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bidirectional Self-Training with Multiple Anisotropic Prototypes for Domain Adaptive Semantic Segmentation. (arXiv:2204.07730v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07730","description":"<p>A thriving trend for domain adaptive segmentation endeavors to generate the\nhigh-quality pseudo labels for target domain and retrain the segmentor on them.\nUnder this self-training paradigm, some competitive methods have sought to the\nlatent-space information, which establishes the feature centroids (a.k.a\nprototypes) of the semantic classes and determines the pseudo label candidates\nby their distances from these centroids. In this paper, we argue that the\nlatent space contains more information to be exploited thus taking one step\nfurther to capitalize on it. Firstly, instead of merely using the source-domain\nprototypes to determine the target pseudo labels as most of the traditional\nmethods do, we bidirectionally produce the target-domain prototypes to degrade\nthose source features which might be too hard or disturbed for the adaptation.\nSecondly, existing attempts simply model each category as a single and\nisotropic prototype while ignoring the variance of the feature distribution,\nwhich could lead to the confusion of similar categories. To cope with this\nissue, we propose to represent each category with multiple and anisotropic\nprototypes via Gaussian Mixture Model, in order to fit the de facto\ndistribution of source domain and estimate the likelihood of target samples\nbased on the probability density. We apply our method on GTA5-&gt;Cityscapes and\nSynthia-&gt;Cityscapes tasks and achieve 61.2 and 62.8 respectively in terms of\nmean IoU, substantially outperforming other competitive self-training methods.\nNoticeably, in some categories which severely suffer from the categorical\nconfusion such as \"truck\" and \"bus\", our method achieves 56.4 and 68.8\nrespectively, which further demonstrates the effectiveness of our design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yulei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Linear Attention for Fast and Accurate Keypoint Matching. (arXiv:2204.07731v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07731","description":"<p>Recently Transformers have provided state-of-the-art performance in sparse\nmatching, crucial to realize high-performance 3D vision applications. Yet,\nthese Transformers lack efficiency due to the quadratic computational\ncomplexity of their attention mechanism. To solve this problem, we employ an\nefficient linear attention for the linear computational complexity. Then, we\npropose a new attentional aggregation that achieves high accuracy by\naggregating both the global and local information from sparse keypoints. To\nfurther improve the efficiency, we propose the joint learning of feature\nmatching and description. Our learning enables simpler and faster matching than\nSinkhorn, often used in matching the learned descriptors from Transformers. Our\nmethod achieves competitive performance with only 0.84M learnable parameters\nagainst the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M\nparameters), on three benchmarks, HPatch, ETH, and Aachen Day-Night.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suwanwimolkul_S/0/1/0/all/0/1\">Suwichaya Suwanwimolkul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komorita_S/0/1/0/all/0/1\">Satoshi Komorita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation. (arXiv:2204.07733v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07733","description":"<p>Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving\nfor its powerful spatial representation ability. It is challenging to estimate\nthe BEV semantic maps from monocular images due to the spatial gap, since it is\nimplicitly required to realize both the perspective-to-BEV transformation and\nsegmentation. We present a novel two-stage Geometry Prior-based Transformation\nframework named GitNet, consisting of (i) the geometry-guided pre-alignment and\n(ii) ray-based transformer. In the first stage, we decouple the BEV\nsegmentation into the perspective image segmentation and geometric prior-based\nmapping, with explicit supervision by projecting the BEV semantic labels onto\nthe image plane to learn visibility-aware features and learnable geometry to\ntranslate into BEV space. Second, the pre-aligned coarse BEV features are\nfurther deformed by ray-based transformers to take visibility knowledge into\naccount. GitNet achieves the leading performance on the challenging nuScenes\nand Argoverse Datasets. The code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shi Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Methods in Deep Learning: An In-Depth Survey. (arXiv:2204.07756v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07756","description":"<p>Inspired by the human cognitive system, attention is a mechanism that\nimitates the human cognitive awareness about specific information, amplifying\ncritical details to focus more on the essential aspects of data. Deep learning\nhas employed attention to boost performance for many applications.\nInterestingly, the same attention design can suit processing different data\nmodalities and can easily be incorporated into large networks. Furthermore,\nmultiple complementary attention mechanisms can be incorporated in one network.\nHence, attention techniques have become extremely attractive. However, the\nliterature lacks a comprehensive survey specific to attention techniques to\nguide researchers in employing attention in their deep models. Note that,\nbesides being demanding in terms of training data and computational resources,\ntransformers only cover a single category in self-attention out of the many\ncategories available. We fill this gap and provide an in-depth survey of 50\nattention techniques categorizing them by their most prominent features. We\ninitiate our discussion by introducing the fundamental concepts behind the\nsuccess of attention mechanism. Next, we furnish some essentials such as the\nstrengths and limitations of each attention category, describe their\nfundamental building blocks, basic formulations with primary usage, and\napplications specifically for computer vision. We also discuss the challenges\nand open questions related to attention mechanism in general. Finally, we\nrecommend possible future research directions for deep attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassanin_M/0/1/0/all/0/1\">Mohammed Hassanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radwan_I/0/1/0/all/0/1\">Ibrahim Radwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad S Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Grounded Indoor 3D Semantic Segmentation in the Wild. (arXiv:2204.07761v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07761","description":"<p>Recent advances in 3D semantic segmentation with deep neural networks have\nshown remarkable success, with rapid performance increase on available\ndatasets. However, current 3D semantic segmentation benchmarks contain only a\nsmall number of categories -- less than 30 for ScanNet and SemanticKITTI, for\ninstance, which are not enough to reflect the diversity of real environments\n(e.g., semantic image understanding covers hundreds to thousands of classes).\nThus, we propose to study a larger vocabulary for 3D semantic segmentation with\na new extended benchmark on ScanNet data with 200 class categories, an order of\nmagnitude more than previously studied. This large number of class categories\nalso induces a large natural class imbalance, both of which are challenging for\nexisting 3D semantic segmentation methods. To learn more robust 3D features in\nthis context, we propose a language-driven pre-training method to encourage\nlearned 3D features that might have limited training examples to lie close to\ntheir pre-trained text embeddings. Extensive experiments show that our approach\nconsistently outperforms state-of-the-art 3D pre-training for 3D semantic\nsegmentation on our proposed benchmark (+9% relative mIoU), including\nlimited-data scenarios with +25% relative mIoU using only 5% annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozenberszki_D/0/1/0/all/0/1\">David Rozenberszki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biometric verification of humans by means of hand geometry. (arXiv:2204.07764v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07764","description":"<p>This paper describes a hand geometry biometric identification system. We have\nacquired a database of 22 people, 10 acquisitions per person, using a\nconventional document scanner. We propose a feature extraction and classifier.\nThe experimental results reveal a maximum identification rate equal to 93.64%,\nand a minimum value of the detection cost function equal to 2.92% using a multi\nlayer perceptron classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks. (arXiv:2204.07780v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07780","description":"<p>Despite the exciting performance, Transformer is criticized for its excessive\nparameters and computation cost. However, compressing Transformer remains as an\nopen problem due to its internal complexity of the layer designs, i.e.,\nMulti-Head Attention (MHA) and Feed-Forward Network (FFN). To address this\nissue, we introduce Group-wise Transformation towards a universal yet\nlightweight Transformer for vision-and-language tasks, termed as\nLW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both\nthe parameters and computations of Transformer, while also preserving its two\nmain properties, i.e., the efficient attention modeling on diverse subspaces of\nMHA, and the expanding-scaling feature transformation of FFN. We apply\nLW-Transformer to a set of Transformer-based networks, and quantitatively\nmeasure them on three vision-and-language tasks and six benchmark datasets.\nExperimental results show that while saving a large number of parameters and\ncomputations, LW-Transformer achieves very competitive performance against the\noriginal Transformer networks for vision-and-language tasks. To examine the\ngeneralization ability, we also apply our optimization strategy to a recently\nproposed image Transformer called Swin-Transformer for image classification,\nwhere the effectiveness can be also confirmed\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UAMD-Net: A Unified Adaptive Multimodal Neural Network for Dense Depth Completion. (arXiv:2204.07791v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07791","description":"<p>Depth prediction is a critical problem in robotics applications especially\nautonomous driving. Generally, depth prediction based on binocular stereo\nmatching and fusion of monocular image and laser point cloud are two mainstream\nmethods. However, the former usually suffers from overfitting while building\ncost volume, and the latter has a limited generalization due to the lack of\ngeometric constraint. To solve these problems, we propose a novel multimodal\nneural network, namely UAMD-Net, for dense depth completion based on fusion of\nbinocular stereo matching and the weak constrain from the sparse point clouds.\nSpecifically, the sparse point clouds are converted to sparse depth map and\nsent to the multimodal feature encoder (MFE) with binocular image, constructing\na cross-modal cost volume. Then, it will be further processed by the multimodal\nfeature aggregator (MFA) and the depth regression layer. Furthermore, the\nexisting multimodal methods ignore the problem of modal dependence, that is,\nthe network will not work when a certain modal input has a problem. Therefore,\nwe propose a new training strategy called Modal-dropout which enables the\nnetwork to be adaptively trained with multiple modal inputs and inference with\nspecific modal inputs. Benefiting from the flexible network structure and\nadaptive training method, our proposed network can realize unified training\nunder various modal input conditions. Comprehensive experiments conducted on\nKITTI depth completion benchmark demonstrate that our method produces robust\nresults and outperforms other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guancheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Huabiao Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring. (arXiv:2204.07820v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07820","description":"<p>Blind image deblurring (BID) remains a challenging and significant task.\nBenefiting from the strong fitting ability of deep learning, paired data-driven\nsupervised BID method has obtained great progress. However, paired data are\nusually synthesized by hand, and the realistic blurs are more complex than\nsynthetic ones, which makes the supervised methods inept at modeling realistic\nblurs and hinders their real-world applications. As such, unsupervised deep BID\nmethod without paired data offers certain advantages, but current methods still\nsuffer from some drawbacks, e.g., bulky model size, long inference time, and\nstrict image resolution and domain requirements. In this paper, we propose a\nlightweight and real-time unsupervised BID baseline, termed Frequency-domain\nContrastive Loss Constrained Lightweight CycleGAN (shortly, FCL-GAN), with\nattractive properties, i.e., no image domain limitation, no image resolution\nlimitation, 25x lighter than SOTA, and 5x faster than SOTA. To guarantee the\nlightweight property and performance superiority, two new collaboration units\ncalled lightweight domain conversion unit(LDCU) and parameter-free\nfrequency-domain contrastive unit(PFCU) are designed. LDCU mainly implements\ninter-domain conversion in lightweight manner. PFCU further explores the\nsimilarity measure, external difference and internal connection between the\nblurred domain and sharp domain images in frequency domain, without involving\nextra parameters. Extensive experiments on several image datasets demonstrate\nthe effectiveness of our FCL-GAN in terms of performance, model size and\nreference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Suiyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Transfer Learning to improve Chest X-Ray pathology detection using limited triplets. (arXiv:2204.07824v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07824","description":"<p>Deep learning approaches applied to medical imaging have reached near-human\nor better-than-human performance on many diagnostic tasks. For instance, the\nCheXpert competition on detecting pathologies in chest x-rays has shown\nexcellent multi-class classification performance. However, training and\nvalidating deep learning models require extensive collections of images and\nstill produce false inferences, as identified by a human-in-the-loop. In this\npaper, we introduce a practical approach to improve the predictions of a\npre-trained model through Few-Shot Learning (FSL). After training and\nvalidating a model, a small number of false inference images are collected to\nretrain the model using \\textbf{\\textit{Image Triplets}} - a false positive or\nfalse negative, a true positive, and a true negative. The retrained FSL model\nproduces considerable gains in performance with only a few epochs and few\nimages. In addition, FSL opens rapid retraining opportunities for\nhuman-in-the-loop systems, where a radiologist can relabel false inferences,\nand the model can be quickly retrained. We compare our retrained model\nperformance with existing FSL approaches in medical imaging that train and\nevaluate models at once.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhimireddy_A/0/1/0/all/0/1\">Ananth Reddy Bhimireddy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1\">John Lee Burns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust and Scalable Attention Guided Deep Learning Framework for Movement Quality Assessment. (arXiv:2204.07840v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07840","description":"<p>Physical rehabilitation programs frequently begin with a brief stay in the\nhospital and continue with home-based rehabilitation. Lack of feedback on\nexercise correctness is a significant issue in home-based rehabilitation.\nAutomated movement quality assessment (MQA) using skeletal movement data\n(hereafter referred to as skeletal data) collected via depth imaging devices\ncan assist with home-based rehabilitation by providing the necessary\nquantitative feedback. This paper aims to use recent advances in deep learning\nto address the problem of MQA. Movement quality score generation is an\nessential component of MQA. We propose three novel skeletal data augmentation\nschemes. We show that using the proposed augmentations for generating movement\nquality scores result in significant performance boosts over existing methods.\nFinally, we propose a novel transformer based architecture for MQA. Four novel\nfeature extractors are proposed and studied that allow the transformer network\nto operate on skeletal data. We show that adding the attention mechanism in the\ndesign of the proposed feature extractor allows the transformer network to pay\nattention to specific body parts that make a significant contribution towards\nexecuting a movement. We report an improvement in movement quality score\nprediction of 12% on UI-PRMD dataset and 21% on KIMORE dataset compared to the\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1\">Aditya Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mansi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muniyandi_M/0/1/0/all/0/1\">Manivannan Muniyandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Few-Shot Object Detection with Meta-Learning Based Cross-Modal Prompting. (arXiv:2204.07841v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07841","description":"<p>We study multimodal few-shot object detection (FSOD) in this paper, using\nboth few-shot visual examples and class semantic information for detection.\nMost of previous works focus on either few-shot or zero-shot object detection,\nignoring the complementarity of visual and semantic information. We first show\nthat meta-learning and prompt-based learning, the most commonly-used methods\nfor few-shot learning and zero-shot transferring from pre-trained\nvision-language models to downstream tasks, are conceptually similar. They both\nreformulate the objective of downstream tasks the same as the pre-training\ntasks, and mostly without tuning the parameters of pre-trained models. Based on\nthis observation, we propose to combine meta-learning with prompt-based\nlearning for multimodal FSOD without fine-tuning, by learning transferable\nclass-agnostic multimodal FSOD models over many-shot base classes.\nSpecifically, to better exploit the pre-trained vision-language models, the\nmeta-learning based cross-modal prompting is proposed to generate soft prompts\nand further used to extract the semantic prototype, conditioned on the few-shot\nvisual examples. Then, the extracted semantic prototype and few-shot visual\nprototype are fused to generate the multimodal prototype for detection. Our\nmodels can efficiently fuse the visual and semantic information at both\ntoken-level and feature-level. We comprehensively evaluate the proposed\nmultimodal FSOD models on multiple few-shot object detection benchmarks,\nachieving promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-guided Object Inpainting. (arXiv:2204.07845v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07845","description":"<p>Previous works on image inpainting mainly focus on inpainting background or\npartially missing objects, while the problem of inpainting an entire missing\nobject remains unexplored. This work studies a new image inpainting task, i.e.\nshape-guided object inpainting. Given an incomplete input image, the goal is to\nfill in the hole by generating an object based on the context and implicit\nguidance given by the hole shape. Since previous methods for image inpainting\nare mainly designed for background inpainting, they are not suitable for this\ntask. Therefore, we propose a new data preparation method and a novel\nContextual Object Generator (CogNet) for the object inpainting task. On the\ndata side, we incorporate object priors into training data by using object\ninstances as holes. The CogNet has a two-stream architecture that combines the\nstandard bottom-up image completion process with a top-down object generation\nprocess. A predictive class embedding module bridges the two streams by\npredicting the class of the missing object from the bottom-up features, from\nwhich a semantic object map is derived as the input of the top-down stream.\nExperiments demonstrate that the proposed method can generate realistic objects\nthat fit the context in terms of both visual appearance and semantic meanings.\nCode can be found at the project page\n\\url{https://zengxianyu.github.io/objpaint}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-organ Segmentation Network with Adversarial Performance Validator. (arXiv:2204.07850v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07850","description":"<p>CT organ segmentation on computed tomography (CT) images becomes a\nsignificant brick for modern medical image analysis, supporting clinic\nworkflows in multiple domains. Previous segmentation methods include 2D\nconvolution neural networks (CNN) based approaches, fed by CT image slices that\nlack the structural knowledge in axial view, and 3D CNN-based methods with the\nexpensive computation cost in multi-organ segmentation applications. This paper\nintroduces an adversarial performance validation network into a 2D-to-3D\nsegmentation framework. The classifier and performance validator competition\ncontribute to accurate segmentation results via back-propagation. The proposed\nnetwork organically converts the 2D-coarse result to 3D high-quality\nsegmentation masks in a coarse-to-fine manner, allowing joint optimization to\nimprove segmentation accuracy. Besides, the structural information of one\nspecific organ is depicted by a statistics-meaningful prior bounding box, which\nis transformed into a global feature leveraging the learning process in 3D fine\nsegmentation. The experiments on the NIH pancreas segmentation dataset\ndemonstrate the proposed network achieves state-of-the-art accuracy on small\norgan segmentation and outperforms the previous best. High accuracy is also\nreported on multi-organ segmentation in a dataset collected by ourselves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Haoyu Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaofeng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Deeper Understanding of Skeleton-based Gait Recognition. (arXiv:2204.07855v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07855","description":"<p>Gait recognition is a promising biometric with unique properties for\nidentifying individuals from a long distance by their walking patterns. In\nrecent years, most gait recognition methods used the person's silhouette to\nextract the gait features. However, silhouette images can lose fine-grained\nspatial information, suffer from (self) occlusion, and be challenging to obtain\nin real-world scenarios. Furthermore, these silhouettes also contain other\nvisual clues that are not actual gait features and can be used for\nidentification, but also to fool the system. Model-based methods do not suffer\nfrom these problems and are able to represent the temporal motion of body\njoints, which are actual gait features. The advances in human pose estimation\nstarted a new era for model-based gait recognition with skeleton-based gait\nrecognition. In this work, we propose an approach based on Graph Convolutional\nNetworks (GCNs) that combines higher-order inputs, and residual networks to an\nefficient architecture for gait recognition. Extensive experiments on the two\npopular gait datasets, CASIA-B and OUMVLP-Pose, show a massive improvement (3x)\nof the state-of-the-art (SotA) on the largest gait dataset OUMVLP-Pose and\nstrong temporal modeling capabilities. Finally, we visualize our method to\nunderstand skeleton-based gait recognition better and to show that we model\nreal gait features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teepe_T/0/1/0/all/0/1\">Torben Teepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilg_J/0/1/0/all/0/1\">Johannes Gilg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_F/0/1/0/all/0/1\">Fabian Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GHM Wavelet Transform for Deep Image Super Resolution. (arXiv:2204.07862v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07862","description":"<p>The GHM multi-level discrete wavelet transform is proposed as preprocessing\nfor image super resolution with convolutional neural networks. Previous works\nperform analysis with the Haar wavelet only. In this work, 37 single-level\nwavelets are experimentally analyzed from Haar, Daubechies, Biorthogonal,\nReverse Biorthogonal, Coiflets, and Symlets wavelet families. All single-level\nwavelets report similar results indicating that the convolutional neural\nnetwork is invariant to choice of wavelet in a single-level filter approach.\nHowever, the GHM multi-level wavelet achieves higher quality reconstructions\nthan the single-level wavelets. Three large data sets are used for the\nexperiments: DIV2K, a dataset of textures, and a dataset of satellite images.\nThe approximate high resolution images are compared using seven objective error\nmeasurements. A convolutional neural network based approach using wavelet\ntransformed images has good results in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lowe_B/0/1/0/all/0/1\">Ben Lowe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salman_H/0/1/0/all/0/1\">Hadi Salman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhan_J/0/1/0/all/0/1\">Justin Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Human Pose Estimation for Free-from and Moving Activities Using WiFi. (arXiv:2204.07878v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07878","description":"<p>This paper presents GoPose, a 3D skeleton-based human pose estimation system\nthat uses WiFi devices at home. Our system leverages the WiFi signals reflected\noff the human body for 3D pose estimation. In contrast to prior systems that\nneed specialized hardware or dedicated sensors, our system does not require a\nuser to wear or carry any sensors and can reuse the WiFi devices that already\nexist in a home environment for mass adoption. To realize such a system, we\nleverage the 2D AoA spectrum of the signals reflected from the human body and\nthe deep learning techniques. In particular, the 2D AoA spectrum is proposed to\nlocate different parts of the human body as well as to enable\nenvironment-independent pose estimation. Deep learning is incorporated to model\nthe complex relationship between the 2D AoA spectrums and the 3D skeletons of\nthe human body for pose tracking. Our evaluation results show GoPose achieves\naround 4.7cm of accuracy under various scenarios including tracking unseen\nactivities and under NLoS scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yili Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping LiDAR and Camera Measurements in a Dual Top-View Grid Representation Tailored for Automated Vehicles. (arXiv:2204.07887v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07887","description":"<p>We present a generic evidential grid mapping pipeline designed for imaging\nsensors such as LiDARs and cameras. Our grid-based evidential model contains\nsemantic estimates for cell occupancy and ground separately. We specify the\nestimation steps for input data represented by point sets, but mainly focus on\ninput data represented by images such as disparity maps or LiDAR range images.\nInstead of relying on an external ground segmentation only, we deduce occupancy\nevidence by analyzing the surface orientation around measurements. We conduct\nexperiments and evaluate the presented method using LiDAR and stereo camera\ndata recorded in real traffic scenarios. Our method estimates cell occupancy\nrobustly and with a high level of detail while maximizing efficiency and\nminimizing the dependency to external processing modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1\">Sven Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirges_S/0/1/0/all/0/1\">Sascha Wirges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SymForce: Symbolic Computation and Code Generation for Robotics. (arXiv:2204.07889v1 [cs.RO])","link":"http://arxiv.org/abs/2204.07889","description":"<p>We present SymForce, a fast symbolic computation and code generation library\nfor robotics applications like computer vision, state estimation, motion\nplanning, and controls. SymForce combines the development speed and flexibility\nof symbolic mathematics with the performance of autogenerated, highly optimized\ncode in C++ or any target runtime language. SymForce provides geometry and\ncamera types, Lie group operations, and branchless singularity handling for\ncreating and analyzing complex symbolic expressions in Python, built on top of\nSymPy. Generated functions can be integrated as factors into our tangent space\nnonlinear optimizer, which is highly optimized for real-time production use. We\nintroduce novel methods to automatically compute tangent space Jacobians,\neliminating the need for bug-prone handwritten derivatives. This workflow\nenables faster runtime code, faster development time, and fewer lines of\nhandwritten code versus the state-of-the-art. Our experiments demonstrate that\nour approach can yield order of magnitude speedups on computational tasks core\nto robotics. Code is available at https://github.com/symforce-org/symforce .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martiros_H/0/1/0/all/0/1\">Hayk Martiros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1\">Aaron Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucki_N/0/1/0/all/0/1\">Nathan Bucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solliday_B/0/1/0/all/0/1\">Bradley Solliday</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_R/0/1/0/all/0/1\">Ryan Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jack Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tung Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattison_D/0/1/0/all/0/1\">Dominic Pattison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Harrison Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomic_T/0/1/0/all/0/1\">Teo Tomic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_P/0/1/0/all/0/1\">Peter Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_G/0/1/0/all/0/1\">Gareth Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanderMey_J/0/1/0/all/0/1\">Josiah VanderMey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Alvin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Samuel Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtz_K/0/1/0/all/0/1\">Kristen Holtz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Action Detection: Analysing Limitations and Challenges. (arXiv:2204.07892v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07892","description":"<p>Beyond possessing large enough size to feed data hungry machines (eg,\ntransformers), what attributes measure the quality of a dataset? Assuming that\nthe definitions of such attributes do exist, how do we quantify among their\nrelative existences? Our work attempts to explore these questions for video\naction detection. The task aims to spatio-temporally localize an actor and\nassign a relevant action class. We first analyze the existing datasets on video\naction detection and discuss their limitations. Next, we propose a new dataset,\nMulti Actor Multi Action (MAMA) which overcomes these limitations and is more\nsuitable for real world applications. In addition, we perform a biasness study\nwhich analyzes a key property differentiating videos from static images: the\ntemporal aspect. This reveals if the actions in these datasets really need the\nmotion information of an actor, or whether they predict the occurrence of an\naction even by looking at a single frame. Finally, we investigate the widely\nheld assumptions on the importance of temporal ordering: is temporal ordering\nimportant for detecting these actions? Such extreme experiments show existence\nof biases which have managed to creep into existing methods inspite of careful\nmodeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modi_R/0/1/0/all/0/1\">Rajat Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Aayush Jung Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Akash Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirupattur_P/0/1/0/all/0/1\">Praveen Tirupattur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh Singh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction. (arXiv:2204.07908v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07908","description":"<p>Existing leading methods for spectral reconstruction (SR) focus on designing\ndeeper or wider convolutional neural networks (CNNs) to learn the end-to-end\nmapping from the RGB image to its hyperspectral image (HSI). These CNN-based\nmethods achieve impressive restoration performance while showing limitations in\ncapturing the long-range dependencies and self-similarity prior. To cope with\nthis problem, we propose a novel Transformer-based method, Multi-stage\nSpectral-wise Transformer (MST++), for efficient spectral reconstruction. In\nparticular, we employ Spectral-wise Multi-head Self-attention (S-MSA) that is\nbased on the HSI spatially sparse while spectrally self-similar nature to\ncompose the basic unit, Spectral-wise Attention Block (SAB). Then SABs build up\nSingle-stage Spectral-wise Transformer (SST) that exploits a U-shaped structure\nto extract multi-resolution contextual information. Finally, our MST++,\ncascaded by several SSTs, progressively improves the reconstruction quality\nfrom coarse to fine. Comprehensive experiments show that our MST++\nsignificantly outperforms other state-of-the-art methods. In the NTIRE 2022\nSpectral Reconstruction Challenge, our approach won the First place. Code and\npre-trained models are publicly available at\nhttps://github.com/caiyuanhao1998/MST-plus-plus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zudi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Goes beyond Multi-modal Fusion in One-stage Referring Expression Comprehension: An Empirical Study. (arXiv:2204.07913v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07913","description":"<p>Most of the existing work in one-stage referring expression comprehension\n(REC) mainly focuses on multi-modal fusion and reasoning, while the influence\nof other factors in this task lacks in-depth exploration. To fill this gap, we\nconduct an empirical study in this paper. Concretely, we first build a very\nsimple REC network called SimREC, and ablate 42 candidate designs/settings,\nwhich covers the entire process of one-stage REC from network design to model\ntraining. Afterwards, we conduct over 100 experimental trials on three\nbenchmark datasets of REC. The extensive experimental results not only show the\nkey factors that affect REC performance in addition to multi-modal fusion,\ne.g., multi-scale features and data augmentation, but also yield some findings\nthat run counter to conventional understanding. For example, as a vision and\nlanguage (V&amp;L) task, REC does is less impacted by language prior. In addition,\nwith a proper combination of these findings, we can improve the performance of\nSimREC by a large margin, e.g., +27.12% on RefCOCO+, which outperforms all\nexisting REC methods. But the most encouraging finding is that with much less\ntraining overhead and parameters, SimREC can still achieve better performance\nthan a set of large-scale pre-trained models, e.g., UNITER and VILLA,\nportraying the special role of REC in existing V&amp;L research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shubin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Multi-grid Methods for Minimizing Curvature Energy. (arXiv:2204.07921v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07921","description":"<p>The geometric high-order regularization methods such as mean curvature and\nGaussian curvature, have been intensively studied during the last decades due\nto their abilities in preserving geometric properties including image edges,\ncorners, and image contrast. However, the dilemma between restoration quality\nand computational efficiency is an essential roadblock for high-order methods.\nIn this paper, we propose fast multi-grid algorithms for minimizing both mean\ncurvature and Gaussian curvature energy functionals without sacrificing the\naccuracy for efficiency. Unlike the existing approaches based on operator\nsplitting and the Augmented Lagrangian method (ALM), no artificial parameters\nare introduced in our formulation, which guarantees the robustness of the\nproposed algorithm. Meanwhile, we adopt the domain decomposition method to\npromote parallel computing and use the fine-to-coarse structure to accelerate\nthe convergence. Numerical experiments are presented on both image denoising\nand CT reconstruction problem to demonstrate the ability to recover image\ntexture and the efficiency of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuping Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerated MRI With Deep Linear Convolutional Transform Learning. (arXiv:2204.07923v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07923","description":"<p>Recent studies show that deep learning (DL) based MRI reconstruction\noutperforms conventional methods, such as parallel imaging and compressed\nsensing (CS), in multiple applications. Unlike CS that is typically implemented\nwith pre-determined linear representations for regularization, DL inherently\nuses a non-linear representation learned from a large database. Another line of\nwork uses transform learning (TL) to bridge the gap between these two\napproaches by learning linear representations from data. In this work, we\ncombine ideas from CS, TL and DL reconstructions to learn deep linear\nconvolutional transforms as part of an algorithm unrolling approach. Using\nend-to-end training, our results show that the proposed technique can\nreconstruct MR images to a level comparable to DL methods, while supporting\nuniform undersampling patterns unlike conventional CS methods. Our proposed\nmethod relies on convex sparse image reconstruction with linear representation\nat inference time, which may be beneficial for characterizing robustness,\nstability and generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gu_H/0/1/0/all/0/1\">Hongyi Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moeller_S/0/1/0/all/0/1\">Steen Moeller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chun_I/0/1/0/all/0/1\">Il Yong Chun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akcakaya_M/0/1/0/all/0/1\">Mehmet Ak&#xe7;akaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleT2F: Generating Human Faces from Textual Description Using StyleGAN2. (arXiv:2204.07924v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07924","description":"<p>AI-driven image generation has improved significantly in recent years.\nGenerative adversarial networks (GANs), like StyleGAN, are able to generate\nhigh-quality realistic data and have artistic control over the output, as well.\nIn this work, we present StyleT2F, a method of controlling the output of\nStyleGAN2 using text, in order to be able to generate a detailed human face\nfrom textual description. We utilize StyleGAN's latent space to manipulate\ndifferent facial features and conditionally sample the required latent code,\nwhich embeds the facial features mentioned in the input text. Our method proves\nto capture the required features correctly and shows consistency between the\ninput text and the output images. Moreover, our method guarantees\ndisentanglement on manipulating a wide range of facial features that\nsufficiently describes a human face.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabae_M/0/1/0/all/0/1\">Mohamed Shawky Sabae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dardir_M/0/1/0/all/0/1\">Mohamed Ahmed Dardir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskarous_R/0/1/0/all/0/1\">Remonda Talaat Eskarous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebbed_M/0/1/0/all/0/1\">Mohamed Ramzy Ebbed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Defense of Subspace Tracker: Orthogonal Embedding for Visual Tracking. (arXiv:2204.07927v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07927","description":"<p>The paper focuses on a classical tracking model, subspace learning, grounded\non the fact that the targets in successive frames are considered to reside in a\nlow-dimensional subspace or manifold due to the similarity in their\nappearances. In recent years, a number of subspace trackers have been proposed\nand obtained impressive results. Inspired by the most recent results that the\ntracking performance is boosted by the subspace with discrimination capability\nlearned over the recently localized targets and their immediately surrounding\nbackground, this work aims at solving such a problem: how to learn a robust\nlow-dimensional subspace to accurately and discriminatively represent these\ntarget and background samples. To this end, a discriminative approach, which\nreliably separates the target from its surrounding background, is injected into\nthe subspace learning by means of joint learning, achieving a\ndimension-adaptive subspace with superior discrimination capability. The\nproposed approach is extensively evaluated and compared with the\nstate-of-the-art trackers on four popular tracking benchmarks. The experimental\nresults demonstrate that the proposed tracker performs competitively against\nits counterparts. In particular, it achieves more than 9% performance increase\ncompared with the state-of-the-art subspace trackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yao Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition. (arXiv:2204.07935v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07935","description":"<p>Subject-invariant facial action unit (AU) recognition remains challenging for\nthe reason that the data distribution varies among subjects. In this paper, we\npropose a causal inference framework for subject-invariant facial action unit\nrecognition. To illustrate the causal effect existing in AU recognition task,\nwe formulate the causalities among facial images, subjects, latent AU semantic\nrelations, and estimated AU occurrence probabilities via a structural causal\nmodel. By constructing such a causal diagram, we clarify the causal effect\namong variables and propose a plug-in causal intervention module, CIS, to\ndeconfound the confounder \\emph{Subject} in the causal diagram. Extensive\nexperiments conducted on two commonly used AU benchmark datasets, BP4D and\nDISFA, show the effectiveness of our CIS, and the model with CIS inserted,\nCISNet, has achieved state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Diqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yun Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wound Severity Classification using Deep Neural Network. (arXiv:2204.07942v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07942","description":"<p>The classification of wound severity is a critical step in wound diagnosis.\nAn effective classifier can help wound professionals categorize wound\nconditions more quickly and affordably, allowing them to choose the best\ntreatment option. This study used wound photos to construct a deep neural\nnetwork-based wound severity classifier that classified them into one of three\nclasses: green, yellow, or red. The green class denotes wounds still in the\nearly stages of healing and are most likely to recover with adequate care.\nWounds in the yellow category require more attention and treatment than those\nin the green category. Finally, the red class denotes the most severe wounds\nthat require prompt attention and treatment. A dataset containing different\ntypes of wound images is designed with the help of wound specialists. Nine deep\nlearning models are used with applying the concept of transfer learning.\nSeveral stacked models are also developed by concatenating these transfer\nlearning models. The maximum accuracy achieved on multi-class classification is\n68.49%. In addition, we achieved 78.79%, 81.40%, and 77.57% accuracies on green\nvs. yellow, green vs. red, and yellow vs. red classifications for binary\nclassifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Anisuzzaman_D/0/1/0/all/0/1\">D. M. Anisuzzaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_Y/0/1/0/all/0/1\">Yash Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niezgoda_J/0/1/0/all/0/1\">Jeffrey Niezgoda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gopalakrishnan_S/0/1/0/all/0/1\">Sandeep Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zeyun Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-Supervised Contrastive Loss and View-Aware-Based Post-Processing for Vehicle Re-Identification. (arXiv:2204.07943v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07943","description":"<p>In this paper, we propose a Global-Supervised Contrastive loss and a\nview-aware-based post-processing (VABPP) method for the field of vehicle\nre-identification. The traditional supervised contrastive loss calculates the\ndistances of features within the batch, so it has the local attribute. While\nthe proposed Global-Supervised Contrastive loss has new properties and has good\nglobal attributes, the positive and negative features of each anchor in the\ntraining process come from the entire training set. The proposed VABPP method\nis the first time that the view-aware-based method is used as a post-processing\nmethod in the field of vehicle re-identification. The advantages of VABPP are\nthat, first, it is only used during testing and does not affect the training\nprocess. Second, as a post-processing method, it can be easily integrated into\nother trained re-id models. We directly apply the view-pair distance scaling\ncoefficient matrix calculated by the model trained in this paper to another\ntrained re-id model, and the VABPP method greatly improves its performance,\nwhich verifies the feasibility of the VABPP method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhijun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xianjing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zaijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lilei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DR-GAN: Distribution Regularization for Text-to-Image Generation. (arXiv:2204.07945v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07945","description":"<p>This paper presents a new Text-to-Image generation model, named Distribution\nRegularization Generative Adversarial Network (DR-GAN), to generate images from\ntext descriptions from improved distribution learning. In DR-GAN, we introduce\ntwo novel modules: a Semantic Disentangling Module (SDM) and a Distribution\nNormalization Module (DNM). SDM combines the spatial self-attention mechanism\nand a new Semantic Disentangling Loss (SDL) to help the generator distill key\nsemantic information for the image generation. DNM uses a Variational\nAuto-Encoder (VAE) to normalize and denoise the image latent distribution,\nwhich can help the discriminator better distinguish synthesized images from\nreal images. DNM also adopts a Distribution Adversarial Loss (DAL) to guide the\ngenerator to align with normalized real image distributions in the latent\nspace. Extensive experiments on two public datasets demonstrated that our\nDR-GAN achieved a competitive performance in the Text-to-Image task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hongchen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated In-vehicle Monitoring System Using 3D Human Pose Estimation and Seat Belt Segmentation. (arXiv:2204.07946v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07946","description":"<p>Recently, along with interest in autonomous vehicles, the importance of\nmonitoring systems for both drivers and passengers inside vehicles has been\nincreasing. This paper proposes a novel in-vehicle monitoring system the\ncombines 3D pose estimation, seat-belt segmentation, and seat-belt status\nclassification networks. Our system outputs various information necessary for\nmonitoring by accurately considering the data characteristics of the in-vehicle\nenvironment. Specifically, the proposed 3D pose estimation directly estimates\nthe absolute coordinates of keypoints for a driver and passengers, and the\nproposed seat-belt segmentation is implemented by applying a structure based on\nthe feature pyramid. In addition, we propose a classification task to\ndistinguish between normal and abnormal states of wearing a seat belt using\nresults that combine 3D pose estimation with seat-belt segmentation. These\ntasks can be learned simultaneously and operate in real-time. Our method was\nevaluated on a private dataset we newly created and annotated. The experimental\nresults show that our method has significantly high performance that can be\napplied directly to real in-vehicle monitoring systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Ginam Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sung-Sik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yeong-Hun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Suk-Ju Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Signatures. (arXiv:2204.07953v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07953","description":"<p>In this work we investigate the use of the Signature Transform in the context\nof Learning. Under this assumption, we advance a supervised framework that\nprovides state-of-the-art classification accuracy with the use of very few\nlabels without the need of credit assignment and with minimal or no\noverfitting. We leverage tools from harmonic analysis by the use of the\nsignature and log-signature and use as a score function RMSE and MAE Signature\nand log-signature. We develop a closed-form equation to compute probably good\noptimal scale factors. Classification is performed at the CPU level orders of\nmagnitude faster than other methods. We report results on AFHQ dataset, Four\nShapes, MNIST and CIFAR10 achieving 100% accuracy on all tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Curto_J/0/1/0/all/0/1\">J. de Curt&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarza_I/0/1/0/all/0/1\">I. de Zarz&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calafate_C/0/1/0/all/0/1\">Carlos T. Calafate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2204.07955v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07955","description":"<p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment\nAnalysis (MABSA) has attracted increasing attention in recent years. However,\nprevious approaches either (i) use separately pre-trained visual and textual\nmodels, which ignore the crossmodal alignment or (ii) use vision-language\nmodels pre-trained with general pre-training tasks, which are inadequate to\nidentify finegrained aspects, opinions, and their alignments across modalities.\nTo tackle these limitations, we propose a task-specific Vision-Language\nPre-training framework for MABSA (VLPMABSA), which is a unified multimodal\nencoder-decoder architecture for all the pretraining and downstream tasks. We\nfurther design three types of task-specific pre-training tasks from the\nlanguage, vision, and multimodal modalities, respectively. Experimental results\nshow that our approach generally outperforms the state-of-the-art approaches on\nthree MABSA subtasks. Further analysis demonstrates the effectiveness of each\npretraining task. The source code is publicly released at\nhttps://github.com/NUSTM/VLP-MABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1\">Yan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yu_J/0/1/0/all/0/1\">Jianfei yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Extendable, Efficient and Effective Transformer-based Object Detector. (arXiv:2204.07962v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07962","description":"<p>Transformers have been widely used in numerous vision problems especially for\nvisual recognition and detection. Detection transformers are the first fully\nend-to-end learning systems for object detection, while vision transformers are\nthe first fully transformer-based architecture for image classification. In\nthis paper, we integrate Vision and Detection Transformers (ViDT) to construct\nan effective and efficient object detector. ViDT introduces a reconfigured\nattention module to extend the recent Swin Transformer to be a standalone\nobject detector, followed by a computationally efficient transformer decoder\nthat exploits multi-scale features and auxiliary techniques essential to boost\nthe detection performance without much increase in computational load. In\naddition, we extend it to ViDT+ to support joint-task learning for object\ndetection and instance segmentation. Specifically, we attach an efficient\nmulti-scale feature fusion layer and utilize two more auxiliary training\nlosses, IoU-aware loss and token labeling loss. Extensive evaluation results on\nthe Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP\nand latency trade-off among existing fully transformer-based object detectors,\nand its extended ViDT+ achieves 53.2AP owing to its high scalability for large\nmodels. The source code and trained models are available at\nhttps://github.com/naver-ai/vidt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFSC: Adaptive Fourier Space Compression for Anomaly Detection. (arXiv:2204.07963v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07963","description":"<p>Anomaly Detection (AD) on medical images enables a model to recognize any\ntype of anomaly pattern without lesion-specific supervised learning. Data\naugmentation based methods construct pseudo-healthy images by \"pasting\" fake\nlesions on real healthy ones, and a network is trained to predict healthy\nimages in a supervised manner. The lesion can be found by difference between\nthe unhealthy input and pseudo-healthy output. However, using only manually\ndesigned fake lesions fail to approximate to irregular real lesions, hence\nlimiting the model generalization. We assume by exploring the intrinsic data\nproperty within images, we can distinguish previously unseen lesions from\nhealthy regions in an unhealthy image. In this study, we propose an Adaptive\nFourier Space Compression (AFSC) module to distill healthy feature for AD. The\ncompression of both magnitude and phase in frequency domain addresses the hyper\nintensity and diverse position of lesions. Experimental results on the BraTS\nand MS-SEG datasets demonstrate an AFSC baseline is able to produce promising\ndetection results, and an AFSC module can be effectively embedded into existing\nAD methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Haote Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunlong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1\">Liyan Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chenxin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection. (arXiv:2204.07964v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07964","description":"<p>Domain adaptive object detection (DAOD) is a promising way to alleviate\nperformance drop of detectors in new scenes. Albeit great effort made in single\nsource domain adaptation, a more generalized task with multiple source domains\nremains not being well explored, due to knowledge degradation during their\ncombination. To address this issue, we propose a novel approach, namely\ntarget-relevant knowledge preservation (TRKP), to unsupervised multi-source\nDAOD. Specifically, TRKP adopts the teacher-student framework, where the\nmulti-head teacher network is built to extract knowledge from labeled source\ndomains and guide the student network to learn detectors in unlabeled target\ndomain. The teacher network is further equipped with an adversarial\nmulti-source disentanglement (AMSD) module to preserve source domain-specific\nknowledge and simultaneously perform cross-domain alignment. Besides, a\nholistic target-relevant mining (HTRM) scheme is developed to re-weight the\nsource images according to the source-target relevance. By this means, the\nteacher network is enforced to capture target-relevant knowledge, thus\nbenefiting decreasing domain shift when mentoring object detection in the\ntarget domain. Extensive experiments are conducted on various widely used\nbenchmarks with new state-of-the-art scores reported, highlighting the\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bingqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint. (arXiv:2204.07965v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07965","description":"<p>Active learning is a promising alternative to alleviate the issue of high\nannotation cost in the computer vision tasks by consciously selecting more\ninformative samples to label. Active learning for object detection is more\nchallenging and existing efforts on it are relatively rare. In this paper, we\npropose a novel hybrid approach to address this problem, where the\ninstance-level uncertainty and diversity are jointly considered in a bottom-up\nmanner. To balance the computational complexity, the proposed approach is\ndesigned as a two-stage procedure. At the first stage, an Entropy-based\nNon-Maximum Suppression (ENMS) is presented to estimate the uncertainty of\nevery image, which performs NMS according to the entropy in the feature space\nto remove predictions with redundant information gains. At the second stage, a\ndiverse prototype (DivProto) strategy is explored to ensure the diversity\nacross images by progressively converting it into the intra-class and\ninter-class diversities of the entropy-based class-specific prototypes.\nExtensive experiments are conducted on MS COCO and Pascal VOC, and the proposed\napproach achieves state of the art results and significantly outperforms the\nother counterparts, highlighting its superiority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmentation Invariance and Adaptive Sampling in Semantic Segmentation of Agricultural Aerial Images. (arXiv:2204.07969v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07969","description":"<p>In this paper, we investigate the problem of Semantic Segmentation for\nagricultural aerial imagery. We observe that the existing methods used for this\ntask are designed without considering two characteristics of the aerial data:\n(i) the top-down perspective implies that the model cannot rely on a fixed\nsemantic structure of the scene, because the same scene may be experienced with\ndifferent rotations of the sensor; (ii) there can be a strong imbalance in the\ndistribution of semantic classes because the relevant objects of the scene may\nappear at extremely different scales (e.g., a field of crops and a small\nvehicle). We propose a solution to these problems based on two ideas: (i) we\nuse together a set of suitable augmentation and a consistency loss to guide the\nmodel to learn semantic representations that are invariant to the photometric\nand geometric shifts typical of the top-down perspective (Augmentation\nInvariance); (ii) we use a sampling method (Adaptive Sampling) that selects the\ntraining images based on a measure of pixel-wise distribution of classes and\nactual network confidence. With an extensive set of experiments conducted on\nthe Agriculture-Vision dataset, we demonstrate that our proposed strategies\nimprove the performance of the current state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavera_A/0/1/0/all/0/1\">Antonio Tavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaudo_E/0/1/0/all/0/1\">Edoardo Arnaudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic spinal curvature measurement on ultrasound spine images using Faster R-CNN. (arXiv:2204.07988v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07988","description":"<p>Ultrasound spine imaging technique has been applied to the assessment of\nspine deformity. However, manual measurements of scoliotic angles on ultrasound\nimages are time-consuming and heavily rely on raters experience. The objectives\nof this study are to construct a fully automatic framework based on Faster\nR-CNN for detecting vertebral lamina and to measure the fitting spinal curves\nfrom the detected lamina pairs. The framework consisted of two closely linked\nmodules: 1) the lamina detector for identifying and locating each lamina pairs\non ultrasound coronal images, and 2) the spinal curvature estimator for\ncalculating the scoliotic angles based on the chain of detected lamina. Two\nhundred ultrasound images obtained from AIS patients were identified and used\nfor the training and evaluation of the proposed method. The experimental\nresults showed the 0.76 AP on the test set, and the Mean Absolute Difference\n(MAD) between automatic and manual measurement which was within the clinical\nacceptance error. Meanwhile the correlation between automatic measurement and\nCobb angle from radiographs was 0.79. The results revealed that our proposed\ntechnique could provide accurate and reliable automatic curvature measurements\non ultrasound spine images for spine deformities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhichao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1\">Liyue Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jing_W/0/1/0/all/0/1\">Wenke Jing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lou_E/0/1/0/all/0/1\">Edmond Lou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PiouCrypt: Decentralized Lattice-based Method for Visual Symmetric Cryptography. (arXiv:2204.08017v1 [cs.CR])","link":"http://arxiv.org/abs/2204.08017","description":"<p>In recent years, establishing secure visual communications has turned into\none of the essential problems for security engineers and researchers. However,\nonly limited novel solutions are provided for image encryption, and limiting\nthe visual cryptography to only limited schemes can bring up negative\nconsequences, especially with emerging quantum computational systems. This\npaper presents a novel algorithm for establishing secure private visual\ncommunication. The proposed method has a layered architecture with several\ncohesive components, and corresponded with an NP-hard problem, despite its\nsymmetric structure. This two-step technique is not limited to gray-scale\npictures, and furthermore, utilizing a lattice structure causes to proposed\nmethod has optimal resistance for the post-quantum era, and is relatively\nsecure from the theoretical dimension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abapour_N/0/1/0/all/0/1\">Navid Abapour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebadpour_M/0/1/0/all/0/1\">Mohsen Ebadpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VDTR: Video Deblurring with Transformer. (arXiv:2204.08023v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08023","description":"<p>Video deblurring is still an unsolved problem due to the challenging\nspatio-temporal modeling process. While existing convolutional neural\nnetwork-based methods show a limited capacity for effective spatial and\ntemporal modeling for video deblurring. This paper presents VDTR, an effective\nTransformer-based model that makes the first attempt to adapt Transformer for\nvideo deblurring. VDTR exploits the superior long-range and relation modeling\ncapabilities of Transformer for both spatial and temporal modeling. However, it\nis challenging to design an appropriate Transformer-based model for video\ndeblurring due to the complicated non-uniform blurs, misalignment across\nmultiple frames and the high computational costs for high-resolution spatial\nmodeling. To address these problems, VDTR advocates performing attention within\nnon-overlapping windows and exploiting the hierarchical structure for\nlong-range dependencies modeling. For frame-level spatial modeling, we propose\nan encoder-decoder Transformer that utilizes multi-scale features for\ndeblurring. For multi-frame temporal modeling, we adapt Transformer to fuse\nmultiple spatial features efficiently. Compared with CNN-based methods, the\nproposed method achieves highly competitive results on both synthetic and\nreal-world video deblurring benchmarks, including DVD, GOPRO, REDS and BSD. We\nhope such a Transformer-based architecture can serve as a powerful alternative\nbaseline for video deblurring and other video restoration tasks. The source\ncode will be available at \\url{https://github.com/ljzycmd/VDTR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Z-axis, X-axis, Weight and Disambiguation Methods for Constructing Local Reference Frame in 3D Registration: An Evaluation. (arXiv:2204.08024v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08024","description":"<p>The local reference frame (LRF), as an independent coordinate system\ngenerated on a local 3D surface, is widely used in 3D local feature descriptor\nconstruction and 3D transformation estimation which are two key steps in the\nlocal method-based surface matching. There are numerous LRF methods have been\nproposed in literatures. In these methods, the x- and z-axis are commonly\ngenerated by different methods or strategies, and some x-axis methods are\nimplemented on the basis of a z-axis being given. In addition, the weight and\ndisambiguation methods are commonly used in these LRF methods. In existing\nevaluations of LRF, each LRF method is evaluated with a complete form. However,\nthe merits and demerits of the z-axis, x-axis, weight and disambiguation\nmethods in LRF construction are unclear. In this paper, we comprehensively\nanalyze the z-axis, x-axis, weight and disambiguation methods in existing LRFs,\nand obtain six z-axis and eight x-axis, five weight and two disambiguation\nmethods. The performance of these methods are comprehensively evaluated on six\nstandard datasets with different application scenarios and nuisances.\nConsidering the evaluation outcomes, the merits and demerits of different\nweight, disambiguation, z- and x-axis methods are analyzed and summarized. The\nexperimental result also shows that some new designed LRF axes present superior\nperformance compared with the state-of-the-art ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xianyong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jiahui Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaobo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xinyi Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chanjuan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Mechanism based Cognition-level Scene Understanding. (arXiv:2204.08027v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08027","description":"<p>Given a question-image input, the Visual Commonsense Reasoning (VCR) model\ncan predict an answer with the corresponding rationale, which requires\ninference ability from the real world. The VCR task, which calls for exploiting\nthe multi-source information as well as learning different levels of\nunderstanding and extensive commonsense knowledge, is a cognition-level scene\nunderstanding task. The VCR task has aroused researchers' interest due to its\nwide range of applications, including visual question answering, automated\nvehicle systems, and clinical decision support. Previous approaches to solving\nthe VCR task generally rely on pre-training or exploiting memory with long\ndependency relationship encoded models. However, these approaches suffer from a\nlack of generalizability and losing information in long sequences. In this\npaper, we propose a parallel attention-based cognitive VCR network PAVCR, which\nfuses visual-textual information efficiently and encodes semantic information\nin parallel to enable the model to capture rich information for cognition-level\ninference. Extensive experiments show that the proposed model yields\nsignificant improvements over existing methods on the benchmark VCR dataset.\nMoreover, the proposed model provides intuitive interpretation into visual\ncommonsense reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quy_T/0/1/0/all/0/1\">Tai Le Quy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1\">Kea Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palade_V/0/1/0/all/0/1\">Vasile Palade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_I/0/1/0/all/0/1\">Israat Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_C/0/1/0/all/0/1\">Chris Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning based Automatic Detection of Dicentric Chromosome. (arXiv:2204.08029v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08029","description":"<p>Automatic detection of dicentric chromosomes is an essential step to estimate\nradiation exposure and development of end to end emergency bio dosimetry\nsystems. During accidents, a large amount of data is required to be processed\nfor extensive testing to formulate a medical treatment plan for the masses,\nwhich requires this process to be automated. Current approaches require human\nadjustments according to the data and therefore need a human expert to\ncalibrate the system. This paper proposes a completely data driven framework\nwhich requires minimum intervention of field experts and can be deployed in\nemergency cases with relative ease. Our approach involves YOLOv4 to detect the\nchromosomes and remove the debris in each image, followed by a classifier that\ndifferentiates between an analysable chromosome and a non-analysable one.\nImages are extracted from YOLOv4 based on the protocols described by\nWHO-BIODOSNET. The analysable chromosome is classified as Monocentric or\nDicentric and an image is accepted for consideration of dose estimation based\non the analysable chromosome count. We report an accuracy in dicentric\nidentification of 94.33% on a 1:1 split of Dicentric and Monocentric\nChromosomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_A/0/1/0/all/0/1\">Angad Singh Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_N/0/1/0/all/0/1\">Nikhil Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Roy Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Adaptive Task-Related Component Analysis Method for SSVEP recognition. (arXiv:2204.08030v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08030","description":"<p>Steady-state visual evoked potential (SSVEP) recognition methods are equipped\nwith learning from the subject's calibration data, and they can achieve extra\nhigh performance in the SSVEP-based brain-computer interfaces (BCIs), however\ntheir performance deteriorate drastically if the calibration trials are\ninsufficient. This study develops a new method to learn from limited\ncalibration data and it proposes and evaluates a novel adaptive data-driven\nspatial filtering approach for enhancing SSVEPs detection. The spatial filter\nlearned from each stimulus utilizes temporal information from the corresponding\nEEG trials. To introduce the temporal information into the overall procedure,\nan multitask learning approach, based on the bayesian framework, is adopted.\nThe performance of the proposed method was evaluated into two publicly\navailable benchmark datasets, and the results demonstrated that our method\noutperform competing methods by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_V/0/1/0/all/0/1\">Vangelis P. Oikonomou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NICO++: Towards Better Benchmarking for Domain Generalization. (arXiv:2204.08040v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08040","description":"<p>Despite the remarkable performance that modern deep neural networks have\nachieved on independent and identically distributed (I.I.D.) data, they can\ncrash under distribution shifts. Most current evaluation methods for domain\ngeneralization (DG) adopt the leave-one-out strategy as a compromise on the\nlimited number of domains. We propose a large-scale benchmark with extensive\nlabeled domains named NICO++{\\ddag} along with more rational evaluation methods\nfor comprehensively evaluating DG algorithms. To evaluate DG datasets, we\npropose two metrics to quantify covariate shift and concept shift,\nrespectively. Two novel generalization bounds from the perspective of data\nconstruction are proposed to prove that limited concept shift and significant\ncovariate shift favor the evaluation capability for generalization. Through\nextensive experiments, NICO++ shows its superior evaluation capability compared\nwith current DG datasets and its contribution in alleviating unfairness caused\nby the leak of oracle knowledge in model selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Hippocampus Segmentation with Transformers. (arXiv:2204.08043v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08043","description":"<p>In clinical settings, where acquisition conditions and patient populations\nchange over time, continual learning is key for ensuring the safe use of deep\nneural networks. Yet most existing work focuses on convolutional architectures\nand image classification. Instead, radiologists prefer to work with\nsegmentation models that outline specific regions-of-interest, for which\nTransformer-based architectures are gaining traction. The self-attention\nmechanism of Transformers could potentially mitigate catastrophic forgetting,\nopening the way for more robust medical image segmentation. In this work, we\nexplore how recently-proposed Transformer mechanisms for semantic segmentation\nbehave in sequential learning scenarios, and analyse how best to adapt\ncontinual learning strategies for this setting. Our evaluation on hippocampus\nsegmentation shows that Transformer mechanisms mitigate catastrophic forgetting\nfor medical image segmentation compared to purely convolutional architectures,\nand demonstrates that regularising ViT modules should be done with caution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ranem_A/0/1/0/all/0/1\">Amin Ranem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration. (arXiv:2204.08058v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08058","description":"<p>Multimodal video-audio-text understanding and generation can benefit from\ndatasets that are narrow but rich. The narrowness allows bite-sized challenges\nthat the research community can make progress on. The richness ensures we are\nmaking progress along the core challenges. To this end, we present a\nlarge-scale video-audio-text dataset MUGEN, collected using the open-sourced\nplatform game CoinRun [11]. We made substantial modifications to make the game\nricher by introducing audio and enabling new interactions. We trained RL agents\nwith different objectives to navigate the game and interact with 13 objects and\ncharacters. This allows us to automatically extract a large collection of\ndiverse videos and associated audio. We sample 375K video clips (3.2s each) and\ncollect text descriptions from human annotators. Each video has additional\nannotations that are extracted automatically from the game engine, such as\naccurate semantic maps for each frame and templated textual descriptions.\nAltogether, MUGEN can help progress research in many tasks in multimodal\nunderstanding and generation. We benchmark representative approaches on tasks\ninvolving video-audio-text retrieval and generation. Our dataset and code are\nreleased at: https://mugen-org.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Thomas Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1\">Sasha Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_I/0/1/0/all/0/1\">Isabelle Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning 3D Semantics from Pose-Noisy 2D Images with Hierarchical Full Attention Network. (arXiv:2204.08084v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08084","description":"<p>We propose a novel framework to learn 3D point cloud semantics from 2D\nmulti-view image observations containing pose error. On the one hand, directly\nlearning from the massive, unstructured and unordered 3D point cloud is\ncomputationally and algorithmically more difficult than learning from\ncompactly-organized and context-rich 2D RGB images. On the other hand, both\nLiDAR point cloud and RGB images are captured in standard automated-driving\ndatasets. This motivates us to conduct a \"task transfer\" paradigm so that 3D\nsemantic segmentation benefits from aggregating 2D semantic cues, albeit pose\nnoises are contained in 2D image observations. Among all difficulties, pose\nnoise and erroneous prediction from 2D semantic segmentation approaches are the\nmain challenges for the task transfer. To alleviate the influence of those\nfactor, we perceive each 3D point using multi-view images and for each single\nimage a patch observation is associated. Moreover, the semantic labels of a\nblock of neighboring 3D points are predicted simultaneously, enabling us to\nexploit the point structure prior to further improve the performance. A\nhierarchical full attention network~(HiFANet) is designed to sequentially\naggregates patch, bag-of-frames and inter-point semantic cues, with\nhierarchical attention mechanism tailored for different level of semantic cues.\nAlso, each preceding attention block largely reduces the feature size before\nfeeding to the next attention block, making our framework slim. Experiment\nresults on Semantic-KITTI show that the proposed framework outperforms existing\n3D point cloud based methods significantly, it requires much less training data\nand exhibits tolerance to pose noise. The code is available at\nhttps://github.com/yuhanghe01/HiFANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Junkun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Compositional Representations for Effective Low-Shot Generalization. (arXiv:2204.08090v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08090","description":"<p>We propose Recognition as Part Composition (RPC), an image encoding approach\ninspired by human cognition. It is based on the cognitive theory that humans\nrecognize complex objects by components, and that they build a small compact\nvocabulary of concepts to represent each instance with. RPC encodes images by\nfirst decomposing them into salient parts, and then encoding each part as a\nmixture of a small number of prototypes, each representing a certain concept.\nWe find that this type of learning inspired by human cognition can overcome\nhurdles faced by deep convolutional networks in low-shot generalization tasks,\nlike zero-shot learning, few-shot learning and unsupervised domain adaptation.\nFurthermore, we find a classifier using an RPC image encoder is fairly robust\nto adversarial attacks, that deep neural networks are known to be prone to.\nGiven that our image encoding principle is based on human cognition, one would\nexpect the encodings to be interpretable by humans, which we find to be the\ncase via crowd-sourcing experiments. Finally, we propose an application of\nthese interpretable encodings in the form of generating synthetic attribute\nannotations for evaluating zero-shot learning methods on new datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengkai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset for Analyzing Various Gaze Zones and Distracted Behaviors of a Driver. (arXiv:2204.08096v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08096","description":"<p>This article presents a synthetic dataset for machine learning models to\ndetect and analyze drivers' various distracted behavior and different gaze\nzones. We collected the data in a stationary vehicle using three in-vehicle\ncameras positioned at locations: on the dashboard, near the rearview mirror,\nand on the top right-side window corner. The dataset contains two activity\ntypes: distracted activities, and gaze zones for each participant and each\nactivity type has two sets: without appearance blocks and with appearance\nblocks such as wearing a hat or sunglasses. The order and duration of each\nactivity for each participant are random. In addition, the dataset contains\nmanual annotations for each activity, having its start and end time annotated.\nResearchers could use this dataset to evaluate the performance of machine\nlearning algorithms for the classification of various distracting activities\nand gaze zones of drivers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammed Shaiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatachalapathy_A/0/1/0/all/0/1\">Archana Venkatachalapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gursoy_S/0/1/0/all/0/1\">Senem Velipasalar Gursoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1\">David Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction. (arXiv:2204.08107v1 [cs.AI])","link":"http://arxiv.org/abs/2204.08107","description":"<p>In this paper we present a novel method for a naive agent to detect novel\nobjects it encounters in an interaction. We train a reinforcement learning\npolicy on a stacking task given a known object type, and then observe the\nresults of the agent attempting to stack various other objects based on the\nsame trained policy. By extracting embedding vectors from a convolutional\nneural net trained over the results of the aforementioned stacking play, we can\ndetermine the similarity of a given object to known object types, and determine\nif the given object is likely dissimilar enough to the known types to be\nconsidered a novel class of object. We present the results of this method on\ntwo datasets gathered using two different policies and demonstrate what\ninformation the agent needs to extract from its environment to make these\nnovelty judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_N/0/1/0/all/0/1\">Nikhil Krishnaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_S/0/1/0/all/0/1\">Sadaf Ghaffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Dense Video Captioning as Sequence Generation. (arXiv:2204.08121v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08121","description":"<p>Dense video captioning aims to identify the events of interest in an input\nvideo, and generate descriptive captions for each event. Previous approaches\nusually follow a two-stage generative process, which first proposes a segment\nfor each event, then renders a caption for each identified segment. Recent\nadvances in large-scale sequence generation pretraining have seen great success\nin unifying task formulation for a great variety of tasks, but so far, more\ncomplex tasks such as dense video captioning are not able to fully utilize this\npowerful paradigm. In this work, we show how to model the two subtasks of dense\nvideo captioning jointly as one sequence generation task, and simultaneously\npredict the events and the corresponding descriptions. Experiments on YouCook2\nand ViTT show encouraging results and indicate the feasibility of training\ncomplex tasks such as end-to-end dense video captioning integrated into\nlarge-scale pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1\">Ashish Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Network with Channel Attention and Post-Processing for Carotid Arteries Vulnerable Plaque Segmentation in Ultrasound Images. (arXiv:2204.08127v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08127","description":"<p>Carotid arteries vulnerable plaques are a crucial factor in the screening of\natherosclerosis by ultrasound technique. However, the plaques are contaminated\nby various noises such as artifact, speckle noise, and manual segmentation may\nbe time-consuming. This paper proposes an automatic convolutional neural\nnetwork (CNN) method for plaque segmentation in carotid ultrasound images using\na small dataset. First, a parallel network with three independent scale\ndecoders is utilized as our base segmentation network, pyramid dilation\nconvolutions are used to enlarge receptive fields in the three segmentation\nsub-networks. Subsequently, the three decoders are merged to be rectified in\nchannels by SENet. Thirdly, in test stage, the initially segmented plaque is\nrefined by the max contour morphology post-processing to obtain the final\nplaque. Moreover, three loss function Dice loss, SSIM loss and cross-entropy\nloss are compared to segment plaques. Test results show that the proposed\nmethod with dice loss function yields a Dice value of 0.820, an IoU of 0.701,\nAcc of 0.969, and modified Hausdorff distance (MHD) of 1.43 for 30 vulnerable\ncases of plaques, it outperforms some of the conventional CNN-based methods on\nthese metrics. Additionally, we apply an ablation experiment to show the\nvalidity of each proposed module. Our study provides some reference for similar\nresearches and may be useful in actual applications for plaque segmentation of\nultrasound carotid arteries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yanchao Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Cancheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_Y/0/1/0/all/0/1\">Yang Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding. (arXiv:2204.08129v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08129","description":"<p>Understanding animals' behaviors is significant for a wide range of\napplications. However, existing animal behavior datasets have limitations in\nmultiple aspects, including limited numbers of animal classes, data samples and\nprovided tasks, and also limited variations in environmental conditions and\nviewpoints. To address these limitations, we create a large and diverse\ndataset, Animal Kingdom, that provides multiple annotated tasks to enable a\nmore thorough understanding of natural animal behaviors. The wild animal\nfootages used in our dataset record different times of the day in extensive\nrange of environments containing variations in backgrounds, viewpoints,\nillumination and weather conditions. More specifically, our dataset contains 50\nhours of annotated videos to localize relevant animal behavior segments in long\nvideos for the video grounding task, 30K video sequences for the fine-grained\nmulti-label action recognition task, and 33K frames for the pose estimation\ntask, which correspond to a diverse range of animals with 850 species across 6\nmajor animal classes. Such a challenging and comprehensive dataset shall be\nable to facilitate the community to develop, adapt, and evaluate various types\nof advanced methods for animal behavior analysis. Moreover, we propose a\nCollaborative Action Recognition (CARe) model that learns general and specific\nfeatures for action recognition with unseen new animals. This method achieves\npromising performance in our experiments. Our dataset can be found at\nhttps://sutdcv.github.io/Animal-Kingdom.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_X/0/1/0/all/0/1\">Xun Long Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1\">Kian Eng Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qichen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_S/0/1/0/all/0/1\">Si Yong Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Weakly-supervised Multiple 3D Hand Mesh Reconstruction from Single Image. (arXiv:2204.08154v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08154","description":"<p>In this paper, we consider the challenging task of simultaneously locating\nand recovering multiple hands from single 2D image. Previous studies either\nfocus on single hand reconstruction or solve this problem in a multi-stage way.\nMoreover, the conventional two-stage pipeline firstly detects hand areas, and\nthen estimates 3D hand pose from each cropped patch. To reduce the\ncomputational redundancy in preprocessing and feature extraction, we propose a\nconcise but efficient single-stage pipeline. Specifically, we design a\nmulti-head auto-encoder structure for multi-hand reconstruction, where each\nhead network shares the same feature map and outputs the hand center, pose and\ntexture, respectively. Besides, we adopt a weakly-supervised scheme to\nalleviate the burden of expensive 3D real-world data annotations. To this end,\nwe propose a series of losses optimized by a stage-wise training scheme, where\na multi-hand dataset with 2D annotations is generated based on the publicly\navailable single hand datasets. In order to further improve the accuracy of the\nweakly supervised model, we adopt several feature consistency constraints in\nboth single and multiple hand settings. Specifically, the keypoints of each\nhand estimated from local features should be consistent with the re-projected\npoints predicted from global features. Extensive experiments on public\nbenchmarks including FreiHAND, HO3D, InterHand2.6M and RHD demonstrate that our\nmethod outperforms the state-of-the-art model-based methods in both\nweakly-supervised and fully-supervised manners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jinwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jialiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOD-CNN: An Effective Convolutional Neural Network for Tiny Object Detection in Sperm Videos. (arXiv:2204.08166v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08166","description":"<p>The detection of tiny objects in microscopic videos is a problematic point,\nespecially in large-scale experiments. For tiny objects (such as sperms) in\nmicroscopic videos, current detection methods face challenges in fuzzy,\nirregular, and precise positioning of objects. In contrast, we present a\nconvolutional neural network for tiny object detection (TOD-CNN) with an\nunderlying data set of high-quality sperm microscopic videos (111 videos, $&gt;$\n278,000 annotated objects), and a graphical user interface (GUI) is designed to\nemploy and test the proposed model effectively. TOD-CNN is highly accurate,\nachieving $85.60\\%$ AP$_{50}$ in the task of real-time sperm detection in\nmicroscopic videos. To demonstrate the importance of sperm detection technology\nin sperm quality analysis, we carry out relevant sperm quality evaluation\nmetrics and compare them with the diagnosis results from medical doctors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shuojia Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-world Deep Local Motion Deblurring. (arXiv:2204.08179v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08179","description":"<p>Most existing deblurring methods focus on removing global blur caused by\ncamera shake, while they cannot well handle local blur caused by object\nmovements. To fill the vacancy of local deblurring in real scenes, we establish\nthe first real local motion blur dataset (ReLoBlur), which is captured by a\nsynchronized beam-splitting photographing system and corrected by a\npost-progressing pipeline. Based on ReLoBlur, we propose a Local Blur-Aware\nGated network (LBAG) and several local blur-aware techniques to bridge the gap\nbetween global and local deblurring: 1) a blur detection approach based on\nbackground subtraction to localize blurred regions; 2) a gate mechanism to\nguide our network to focus on blurred regions; and 3) a blur-aware patch\ncropping strategy to address data imbalance problem. Extensive experiments\nprove the reliability of ReLoBlur dataset, and demonstrate that LBAG achieves\nbetter performance than state-of-the-art global deblurring methods without our\nproposed local blur-aware techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Peng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huajun Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality-Balanced Embedding for Video Retrieval. (arXiv:2204.08182v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08182","description":"<p>Video search has become the main routine for users to discover videos\nrelevant to a text query on large short-video sharing platforms. During\ntraining a query-video bi-encoder model using online search logs, we identify a\nmodality bias phenomenon that the video encoder almost entirely relies on text\nmatching, neglecting other modalities of the videos such as vision, audio. This\nmodality imbalanceresults from a) modality gap: the relevance between a query\nand a video text is much easier to learn as the query is also a piece of text,\nwith the same modality as the video text; b) data bias: most training samples\ncan be solved solely by text matching. Here we share our practices to improve\nthe first retrieval stage including our solution for the modality imbalance\nissue. We propose MBVR (short for Modality Balanced Video Retrieval) with two\nkey components: manually generated modality-shuffled (MS) samples and a dynamic\nmargin (DM) based on visual relevance. They can encourage the video encoder to\npay balanced attentions to each modality. Through extensive experiments on a\nreal world dataset, we show empirically that our method is both effective and\nefficient in solving modality bias problem. We have also deployed our MBVR in a\nlarge video platform and observed statistically significant boost over a highly\noptimized baseline in an A/B test and manual GSB evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_B/0/1/0/all/0/1\">Bingqing Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1\">Qiushi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile Edge. (arXiv:2204.08189v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08189","description":"<p>Adversarial example attack endangers the mobile edge systems such as vehicles\nand drones that adopt deep neural networks for visual sensing. This paper\npresents {\\em Sardino}, an active and dynamic defense approach that renews the\ninference ensemble at run time to develop security against the adaptive\nadversary who tries to exfiltrate the ensemble and construct the corresponding\neffective adversarial examples. By applying consistency check and data fusion\non the ensemble's predictions, Sardino can detect and thwart adversarial\ninputs. Compared with the training-based ensemble renewal, we use HyperNet to\nachieve {\\em one million times} acceleration and per-frame ensemble renewal\nthat presents the highest level of difficulty to the prerequisite exfiltration\nattacks. Moreover, the robustness of the renewed ensembles against adversarial\nexamples is enhanced with adversarial learning for the HyperNet. We design a\nrun-time planner that maximizes the ensemble size in favor of security while\nmaintaining the processing frame rate. Beyond adversarial examples, Sardino can\nalso address the issue of out-of-distribution inputs effectively. This paper\npresents extensive evaluation of Sardino's performance in counteracting\nadversarial examples and applies it to build a real-time car-borne traffic sign\nrecognition system. Live on-road tests show the built system's effectiveness in\nmaintaining frame rate and detecting out-of-distribution inputs due to the\nfalse positives of a preceding YOLO-based traffic sign detector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhenyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Rui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Super-Resolution. (arXiv:2204.08192v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08192","description":"<p>Super-Resolution is the process of generating a high-resolution image from a\nlow-resolution image. A picture may be of lower resolution due to smaller\nspatial resolution, poor camera quality, as a result of blurring, or due to\nother possible degradations. Super-Resolution is the technique to improve the\nquality of a low-resolution photo by boosting its plausible resolution. The\ncomputer vision community has extensively explored the area of\nSuper-Resolution. However, the previous Super-Resolution methods require vast\namounts of data for training. This becomes problematic in domains where very\nfew low-resolution, high-resolution pairs might be available. One of such areas\nis statistical downscaling, where super-resolution is increasingly being used\nto obtain high-resolution climate information from low-resolution data.\nAcquiring high-resolution climate data is extremely expensive and challenging.\nTo reduce the cost of generating high-resolution climate information,\nSuper-Resolution algorithms should be able to train with a limited number of\nlow-resolution, high-resolution pairs. This paper tries to solve the\naforementioned problem by introducing a semi-supervised way to perform\nsuper-resolution that can generate sharp, high-resolution images with as few as\n500 paired examples. The proposed semi-supervised technique can be used as a\nplug-and-play module with any supervised GAN-based Super-Resolution method to\nenhance its performance. We quantitatively and qualitatively analyze the\nperformance of the proposed model and compare it with completely supervised\nmethods as well as other unsupervised techniques. Comprehensive evaluations\nshow the superiority of our method over other methods on different metrics. We\nalso offer the applicability of our approach in statistical downscaling to\nobtain high-resolution climate images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1\">Ankur Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Arbitrary-Scale Point Clouds Upsampling via Implicit Neural Representation. (arXiv:2204.08196v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08196","description":"<p>Point clouds upsampling is a challenging issue to generate dense and uniform\npoint clouds from the given sparse input. Most existing methods either take the\nend-to-end supervised learning based manner, where large amounts of pairs of\nsparse input and dense ground-truth are exploited as supervision information;\nor treat up-scaling of different scale factors as independent tasks, and have\nto build multiple networks to handle upsampling with varying factors. In this\npaper, we propose a novel approach that achieves self-supervised and\nmagnification-flexible point clouds upsampling simultaneously. We formulate\npoint clouds upsampling as the task of seeking nearest projection points on the\nimplicit surface for seed points. To this end, we define two implicit neural\nfunctions to estimate projection direction and distance respectively, which can\nbe trained by two pretext learning tasks. Experimental results demonstrate that\nour self-supervised learning based scheme achieves competitive or even better\nperformance than supervised learning based state-of-the-art methods. The source\ncode is publicly available at https://github.com/xnowbzhao/sapcu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhiwei Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OMG: Observe Multiple Granularities for Natural Language-Based Vehicle Retrieval. (arXiv:2204.08209v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08209","description":"<p>Retrieving tracked-vehicles by natural language descriptions plays a critical\nrole in smart city construction. It aims to find the best match for the given\ntexts from a set of tracked vehicles in surveillance videos. Existing works\ngenerally solve it by a dual-stream framework, which consists of a text\nencoder, a visual encoder and a cross-modal loss function. Although some\nprogress has been made, they failed to fully exploit the information at various\nlevels of granularity. To tackle this issue, we propose a novel framework for\nthe natural language-based vehicle retrieval task, OMG, which Observes Multiple\nGranularities with respect to visual representation, textual representation and\nobjective functions. For the visual representation, target features, context\nfeatures and motion features are encoded separately. For the textual\nrepresentation, one global embedding, three local embeddings and a color-type\nprompt embedding are extracted to represent various granularities of semantic\nfeatures. Finally, the overall framework is optimized by a cross-modal\nmulti-granularity contrastive loss function. Experiments demonstrate the\neffectiveness of our method. Our OMG significantly outperforms all previous\nmethods and ranks the 9th on the 6th AI City Challenge Track2. The codes are\navailable at https://github.com/dyhBUPT/OMG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yunhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1\">Xiangning Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1\">Fei Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Evaluation and Theoretical Analysis for Representation Learning: A Survey. (arXiv:2204.08226v1 [cs.LG])","link":"http://arxiv.org/abs/2204.08226","description":"<p>Representation learning enables us to automatically extract generic feature\nrepresentations from a dataset to solve another machine learning task.\nRecently, extracted feature representations by a representation learning\nalgorithm and a simple predictor have exhibited state-of-the-art performance on\nseveral machine learning tasks. Despite its remarkable progress, there exist\nvarious ways to evaluate representation learning algorithms depending on the\napplication because of the flexibility of representation learning. To\nunderstand the current representation learning, we review evaluation methods of\nrepresentation learning algorithms and theoretical analyses. On the basis of\nour evaluation survey, we also discuss the future direction of representation\nlearning. Note that this survey is the extended version of Nozawa and Sato\n(2022).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nozawa_K/0/1/0/all/0/1\">Kento Nozawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Issei Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training. (arXiv:2204.08227v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08227","description":"<p>The self-supervised Masked Image Modeling (MIM) schema, following\n\"mask-and-reconstruct\" pipeline of recovering contents from masked image, has\nrecently captured the increasing interest in the multimedia community, owing to\nthe excellent ability of learning visual representation from unlabeled data.\nAiming at learning representations with high semantics abstracted, a group of\nworks attempts to reconstruct non-semantic pixels with large-ratio masking\nstrategy, which may suffer from \"over-smoothing\" problem, while others directly\ninfuse semantics into targets in off-line way requiring extra data. Different\nfrom them, we shift the perspective to the Fourier domain which naturally has\nglobal perspective and present a new Masked Image Modeling (MIM), termed\nGeminated Gestalt Autoencoder (Ge$^2$-AE) for visual pre-training.\nSpecifically, we equip our model with geminated decoders in charge of\nreconstructing image contents from both pixel and frequency space, where each\nother serves as not only the complementation but also the reciprocal\nconstraints. Through this way, more robust representations can be learned in\nthe pre-trained encoders, of which the effectiveness is confirmed by the\njuxtaposing experimental results on downstream recognition tasks. We also\nconduct several quantitative and qualitative experiments to investigate the\nlearning behavior of our method. To our best knowledge, this is the first MIM\nwork to solve the visual pre-training through the lens of frequency domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinghua Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1\">Antai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Deqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Multi-view Unsupervised Feature Selection and Graph Learning. (arXiv:2204.08247v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08247","description":"<p>Despite the recent progress, the existing multi-view unsupervised feature\nselection methods mostly suffer from two limitations. First, they generally\nutilize either cluster structure or similarity structure to guide the feature\nselection, neglecting the possibility of a joint formulation with mutual\nbenefits. Second, they often learn the similarity structure by either global\nstructure learning or local structure learning, lacking the capability of graph\nlearning with both global and local structural awareness. In light of this,\nthis paper presents a joint multi-view unsupervised feature selection and graph\nlearning (JMVFG) approach. Particularly, we formulate the multi-view feature\nselection with orthogonal decomposition, where each target matrix is decomposed\ninto a view-specific basis matrix and a view-consistent cluster indicator.\nCross-space locality preservation is incorporated to bridge the cluster\nstructure learning in the projected space and the similarity learning (i.e.,\ngraph learning) in the original space. Further, a unified objective function is\npresented to enable the simultaneous learning of the cluster structure, the\nglobal and local similarity structures, and the multi-view consistency and\ninconsistency, upon which an alternating optimization algorithm is developed\nwith theoretically proved convergence. Extensive experiments demonstrate the\nsuperiority of our approach for both multi-view feature selection and graph\nlearning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Si-Guo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visio-Linguistic Brain Encoding. (arXiv:2204.08261v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08261","description":"<p>Enabling effective brain-computer interfaces requires understanding how the\nhuman brain encodes stimuli across modalities such as visual, language (or\ntext), etc. Brain encoding aims at constructing fMRI brain activity given a\nstimulus. There exists a plethora of neural encoding models which study brain\nencoding for single mode stimuli: visual (pretrained CNNs) or text (pretrained\nlanguage models). Few recent papers have also obtained separate visual and text\nrepresentation models and performed late-fusion using simple heuristics.\nHowever, previous work has failed to explore: (a) the effectiveness of image\nTransformer models for encoding visual stimuli, and (b) co-attentive\nmulti-modal modeling for visual and text reasoning. In this paper, we\nsystematically explore the efficacy of image Transformers (ViT, DEiT, and BEiT)\nand multi-modal Transformers (VisualBERT, LXMERT, and CLIP) for brain encoding.\nExtensive experiments on two popular datasets, BOLD5000 and Pereira, provide\nthe following insights. (1) To the best of our knowledge, we are the first to\ninvestigate the effectiveness of image and multi-modal Transformers for brain\nencoding. (2) We find that VisualBERT, a multi-modal Transformer, significantly\noutperforms previously proposed single-mode CNNs, image Transformers as well as\nother previously proposed multi-modal models, thereby establishing new\nstate-of-the-art. The supremacy of visio-linguistic models raises the question\nof whether the responses elicited in the visual regions are affected implicitly\nby linguistic processing even when passively viewing images. Future fMRI tasks\ncan verify this computational insight in an appropriate experimental setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_J/0/1/0/all/0/1\">Jashn Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowtula_V/0/1/0/all/0/1\">Vijay Rowtula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapi_R/0/1/0/all/0/1\">Raju S. Bapi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised domain adaptation and super resolution on drone images for autonomous dry herbage biomass estimation. (arXiv:2204.08271v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08271","description":"<p>Herbage mass yield and composition estimation is an important tool for dairy\nfarmers to ensure an adequate supply of high quality herbage for grazing and\nsubsequently milk production. By accurately estimating herbage mass and\ncomposition, targeted nitrogen fertiliser application strategies can be\ndeployed to improve localised regions in a herbage field, effectively reducing\nthe negative impacts of over-fertilization on biodiversity and the environment.\nIn this context, deep learning algorithms offer a tempting alternative to the\nusual means of sward composition estimation, which involves the destructive\nprocess of cutting a sample from the herbage field and sorting by hand all\nplant species in the herbage. The process is labour intensive and time\nconsuming and so not utilised by farmers. Deep learning has been successfully\napplied in this context on images collected by high-resolution cameras on the\nground. Moving the deep learning solution to drone imaging, however, has the\npotential to further improve the herbage mass yield and composition estimation\ntask by extending the ground-level estimation to the large surfaces occupied by\nfields/paddocks. Drone images come at the cost of lower resolution views of the\nfields taken from a high altitude and requires further herbage ground-truth\ncollection from the large surfaces covered by drone images. This paper proposes\nto transfer knowledge learned on ground-level images to raw drone images in an\nunsupervised manner. To do so, we use unpaired image style translation to\nenhance the resolution of drone images by a factor of eight and modify them to\nappear closer to their ground-level counterparts. We then ...\n~\\url{www.github.com/PaulAlbert31/Clover_SSL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albert_P/0/1/0/all/0/1\">Paul Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadeldin_M/0/1/0/all/0/1\">Mohamed Saadeldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_B/0/1/0/all/0/1\">Badri Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1\">Jaime Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennessey_D/0/1/0/all/0/1\">Deirdre Hennessey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heavy Rain Face Image Restoration: Integrating Physical Degradation Model and Facial Component Guided Adversarial Learning. (arXiv:2204.08307v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08307","description":"<p>With the recent increase in intelligent CCTVs for visual surveillance, a new\nimage degradation that integrates resolution conversion and synthetic rain\nmodels is required. For example, in heavy rain, face images captured by CCTV\nfrom a distance have significant deterioration in both visibility and\nresolution. Unlike traditional image degradation models (IDM), such as rain\nremoval and superresolution, this study addresses a new IDM referred to as a\nscale-aware heavy rain model and proposes a method for restoring\nhigh-resolution face images (HR-FIs) from low-resolution heavy rain face images\n(LRHR-FI). To this end, a 2-stage network is presented. The first stage\ngenerates low-resolution face images (LR-FIs), from which heavy rain has been\nremoved from the LRHR-FIs to improve visibility. To realize this, an\ninterpretable IDM-based network is constructed to predict physical parameters,\nsuch as rain streaks, transmission maps, and atmospheric light. In addition,\nthe image reconstruction loss is evaluated to enhance the estimates of the\nphysical parameters. For the second stage, which aims to reconstruct the HR-FIs\nfrom the LR-FIs outputted in the first stage, facial component guided\nadversarial learning (FCGAL) is applied to boost facial structure expressions.\nTo focus on informative facial features and reinforce the authenticity of\nfacial components, such as the eyes and nose, a face-parsing-guided generator\nand facial local discriminators are designed for FCGAL. The experimental\nresults verify that the proposed approach based on physical-based network\ndesign and FCGAL can remove heavy rain and increase the resolution and\nvisibility simultaneously. Moreover, the proposed heavy-rain face image\nrestoration outperforms state-of-the-art models of heavy rain removal,\nimage-to-image translation, and superresolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_C/0/1/0/all/0/1\">Chang-Hwan Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1\">Da-Hee Jeong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency in Augmented Reality. (arXiv:2204.08308v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08308","description":"<p>With the rapid development of multimedia technology, Augmented Reality (AR)\nhas become a promising next-generation mobile platform. The primary theory\nunderlying AR is human visual confusion, which allows users to perceive the\nreal-world scenes and augmented contents (virtual-world scenes) simultaneously\nby superimposing them together. To achieve good Quality of Experience (QoE), it\nis important to understand the interaction between two scenarios, and\nharmoniously display AR contents. However, studies on how this superimposition\nwill influence the human visual attention are lacking. Therefore, in this\npaper, we mainly analyze the interaction effect between background (BG) scenes\nand AR contents, and study the saliency prediction problem in AR. Specifically,\nwe first construct a Saliency in AR Dataset (SARD), which contains 450 BG\nimages, 450 AR images, as well as 1350 superimposed images generated by\nsuperimposing BG and AR images in pair with three mixing levels. A large-scale\neye-tracking experiment among 60 subjects is conducted to collect eye movement\ndata. To better predict the saliency in AR, we propose a vector quantized\nsaliency prediction method and generalize it for AR saliency prediction. For\ncomparison, three benchmark methods are proposed and evaluated together with\nour proposed method on our SARD. Experimental results demonstrate the\nsuperiority of our proposed method on both of the common saliency prediction\nproblem and the AR saliency prediction problem over benchmark methods. Our data\ncollection methodology, dataset, benchmark methods, and proposed saliency\nmodels will be publicly available to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Huiyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Danyang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking monocular camera pose and deformation for SLAM inside the human body. (arXiv:2204.08309v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08309","description":"<p>Monocular SLAM in deformable scenes will open the way to multiple medical\napplications like computer-assisted navigation in endoscopy, automatic drug\ndelivery or autonomous robotic surgery. In this paper we propose a novel method\nto simultaneously track the camera pose and the 3D scene deformation, without\nany assumption about environment topology or shape. The method uses an\nillumination-invariant photometric method to track image features and estimates\ncamera motion and deformation combining reprojection error with spatial and\ntemporal regularization of deformations. Our results in simulated colonoscopies\nshow the method's accuracy and robustness in complex scenes under increasing\nlevels of deformation. Our qualitative results in human colonoscopies from\nEndomapper dataset show that the method is able to successfully cope with the\nchallenges of real endoscopies: deformations, low texture and strong\nillumination changes. We also compare with previous tracking methods in simpler\nscenarios from Hamlyn dataset where we obtain competitive performance, without\nneeding any topological assumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan J. Gomez Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">J.M.M Montiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_J/0/1/0/all/0/1\">Juan D. Tardos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Transfer Learning and Ensemble Learning in Image-level Classification for Breast Histopathology. (arXiv:2204.08311v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08311","description":"<p>Background: Breast cancer has the highest prevalence in women globally. The\nclassification and diagnosis of breast cancer and its histopathological images\nhave always been a hot spot of clinical concern. In Computer-Aided Diagnosis\n(CAD), traditional classification models mostly use a single network to extract\nfeatures, which has significant limitations. On the other hand, many networks\nare trained and optimized on patient-level datasets, ignoring the application\nof lower-level data labels.\n</p>\n<p>Method: This paper proposes a deep ensemble model based on image-level labels\nfor the binary classification of benign and malignant lesions of breast\nhistopathological images. First, the BreakHis dataset is randomly divided into\na training, validation and test set. Then, data augmentation techniques are\nused to balance the number of benign and malignant samples. Thirdly,\nconsidering the performance of transfer learning and the complementarity\nbetween each network, VGG-16, Xception, Resnet-50, DenseNet-201 are selected as\nthe base classifiers.\n</p>\n<p>Result: In the ensemble network model with accuracy as the weight, the\nimage-level binary classification achieves an accuracy of $98.90\\%$. In order\nto verify the capabilities of our method, the latest Transformer and Multilayer\nPerception (MLP) models have been experimentally compared on the same dataset.\nOur model wins with a $5\\%-20\\%$ advantage, emphasizing the ensemble model's\nfar-reaching significance in classification tasks.\n</p>\n<p>Conclusion: This research focuses on improving the model's classification\nperformance with an ensemble algorithm. Transfer learning plays an essential\nrole in small datasets, improving training speed and accuracy. Our model has\noutperformed many existing approaches in accuracy, providing a method for the\nfield of auxiliary medical diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuchao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaomin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A high-resolution canopy height model of the Earth. (arXiv:2204.08322v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08322","description":"<p>The worldwide variation in vegetation height is fundamental to the global\ncarbon cycle and central to the functioning of ecosystems and their\nbiodiversity. Geospatially explicit and, ideally, highly resolved information\nis required to manage terrestrial ecosystems, mitigate climate change, and\nprevent biodiversity loss. Here, we present the first global, wall-to-wall\ncanopy height map at 10 m ground sampling distance for the year 2020. No single\ndata source meets these requirements: dedicated space missions like GEDI\ndeliver sparse height data, with unprecedented coverage, whereas optical\nsatellite images like Sentinel-2 offer dense observations globally, but cannot\ndirectly measure vertical structures. By fusing GEDI with Sentinel-2, we have\ndeveloped a probabilistic deep learning model to retrieve canopy height from\nSentinel-2 images anywhere on Earth, and to quantify the uncertainty in these\nestimates. The presented approach reduces the saturation effect commonly\nencountered when estimating canopy height from satellite images, allowing to\nresolve tall canopies with likely high carbon stocks. According to our map,\nonly 5% of the global landmass is covered by trees taller than 30 m. Such data\nplay an important role for conservation, e.g., we find that only 34% of these\ntall canopies are located within protected areas. Our model enables consistent,\nuncertainty-informed worldwide mapping and supports an ongoing monitoring to\ndetect change and inform decision making. The approach can serve ongoing\nefforts in forest conservation, and has the potential to foster advances in\nclimate, carbon, and biodiversity modelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jetz_W/0/1/0/all/0/1\">Walter Jetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Optimal Transport for Comparing Histopathology Datasets. (arXiv:2204.08324v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08324","description":"<p>Scarcity of labeled histopathology data limits the applicability of deep\nlearning methods to under-profiled cancer types and labels. Transfer learning\nallows researchers to overcome the limitations of small datasets by\npre-training machine learning models on larger datasets \\emph{similar} to the\nsmall target dataset. However, similarity between datasets is often determined\nheuristically. In this paper, we propose a principled notion of distance\nbetween histopathology datasets based on a hierarchical generalization of\noptimal transport distances. Our method does not require any training, is\nagnostic to model type, and preserves much of the hierarchical structure in\nhistopathology datasets imposed by tiling. We apply our method to H\\&amp;E stained\nslides from The Cancer Genome Atlas from six different cancer types. We show\nthat our method outperforms a baseline distance in a cancer-type prediction\ntask. Our results also show that our optimal transport distance predicts\ndifficulty of transferability in a tumor vs.~normal prediction setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1\">Anna Yeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rahul G. Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mieloszyk_R/0/1/0/all/0/1\">Rebecca Mieloszyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Melis_D/0/1/0/all/0/1\">David Alvarez-Melis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_G/0/1/0/all/0/1\">Grace Huynh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Data-Efficient GANs in Image Generation. (arXiv:2204.08329v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08329","description":"<p>Generative Adversarial Networks (GANs) have achieved remarkable achievements\nin image synthesis. These successes of GANs rely on large scale datasets,\nrequiring too much cost. With limited training data, how to stable the training\nprocess of GANs and generate realistic images have attracted more attention.\nThe challenges of Data-Efficient GANs (DE-GANs) mainly arise from three\naspects: (i) Mismatch Between Training and Target Distributions, (ii)\nOverfitting of the Discriminator, and (iii) Imbalance Between Latent and Data\nSpaces. Although many augmentation and pre-training strategies have been\nproposed to alleviate these issues, there lacks a systematic survey to\nsummarize the properties, challenges, and solutions of DE-GANs. In this paper,\nwe revisit and define DE-GANs from the perspective of distribution\noptimization. We conclude and analyze the challenges of DE-GANs. Meanwhile, we\npropose a taxonomy, which classifies the existing methods into three\ncategories: Data Selection, GANs Optimization, and Knowledge Sharing. Last but\nnot the least, we attempt to highlight the current problems and the future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment. (arXiv:2204.08332v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08332","description":"<p>This work addresses the Burst Super-Resolution (BurstSR) task using a new\narchitecture, which requires restoring a high-quality image from a sequence of\nnoisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in\nBurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can\nsignificantly improve the capability of extracting inter-frame information and\nreconstruction. To achieve this goal, we propose a Pyramid Flow-Guided\nDeformable Convolution Network (Pyramid FG-DCN) and incorporate Swin\nTransformer Blocks and Groups as our main backbone. More specifically, we\ncombine optical flows and deformable convolutions, hence our BSRT can handle\nmisalignment and aggregate the potential texture information in multi-frames\nmore efficiently. In addition, our Transformer-based structure can capture\nlong-range dependency to further improve the performance. The evaluation on\nboth synthetic and real-world tracks demonstrates that our approach achieves a\nnew state-of-the-art in BurstSR task. Further, our BSRT wins the championship\nin the NTIRE2022 Burst Super-Resolution Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziwei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhihong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Migrating Face Swap to Mobile Devices: A lightweight Framework and A Supervised Training Solution. (arXiv:2204.08339v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08339","description":"<p>Existing face swap methods rely heavily on large-scale networks for adequate\ncapacity to generate visually plausible results, which inhibits its\napplications on resource-constraint platforms. In this work, we propose\nMobileFSGAN, a novel lightweight GAN for face swap that can run on mobile\ndevices with much fewer parameters while achieving competitive performance. A\nlightweight encoder-decoder structure is designed especially for image\nsynthesis tasks, which is only 10.2MB and can run on mobile devices at a\nreal-time speed. To tackle the unstability of training such a small network, we\nconstruct the FSTriplets dataset utilizing facial attribute editing techniques.\nFSTriplets provides source-target-result training triplets, yielding\npixel-level labels thus for the first time making the training process\nsupervised. We also designed multi-scale gradient losses for efficient\nback-propagation, resulting in faster and better convergence. Experimental\nresults show that our model reaches comparable performance towards\nstate-of-the-art methods, while significantly reducing the number of network\nparameters. Codes and the dataset have been released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiming Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiangju Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHSCNet: A Multimodal Hierarchical Shot-aware Convolutional Network for Video Summarization. (arXiv:2204.08352v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08352","description":"<p>Video summarization intends to produce a concise video summary by effectively\ncapturing and combining the most informative parts of the whole content.\nExisting approaches for video summarization regard the task as a frame-wise\nkeyframe selection problem and generally construct the frame-wise\nrepresentation by combining the long-range temporal dependency with the\nunimodal or bimodal information. However, the optimal video summaries need to\nreflect the most valuable keyframe with its own information, and one with\nsemantic power of the whole content. Thus, it is critical to construct a more\npowerful and robust frame-wise representation and predict the frame-level\nimportance score in a fair and comprehensive manner. To tackle the above\nissues, we propose a multimodal hierarchical shot-aware convolutional network,\ndenoted as MHSCNet, to enhance the frame-wise representation via combining the\ncomprehensive available multimodal information. Specifically, we design a\nhierarchical ShotConv network to incorporate the adaptive shot-aware\nframe-level representation by considering the short-range and long-range\ntemporal dependency. Based on the learned shot-aware representations, MHSCNet\ncan predict the frame-level importance score in the local and global view of\nthe video. Extensive experiments on two standard video summarization datasets\ndemonstrate that our proposed method consistently outperforms state-of-the-art\nbaselines. Source code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wujiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qiongxu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_j/0/1/0/all/0/1\">jeff little Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting, Tracking and Counting Motorcycle Rider Traffic Violations on Unconstrained Roads. (arXiv:2204.08364v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08364","description":"<p>In many Asian countries with unconstrained road traffic conditions, driving\nviolations such as not wearing helmets and triple-riding are a significant\nsource of fatalities involving motorcycles. Identifying and penalizing such\nriders is vital in curbing road accidents and improving citizens' safety. With\nthis motivation, we propose an approach for detecting, tracking, and counting\nmotorcycle riding violations in videos taken from a vehicle-mounted dashboard\ncamera. We employ a curriculum learning-based object detector to better tackle\nchallenging scenarios such as occlusions. We introduce a novel trapezium-shaped\nobject boundary representation to increase robustness and tackle the\nrider-motorcycle association. We also introduce an amodal regressor that\ngenerates bounding boxes for the occluded riders. Experimental results on a\nlarge-scale unconstrained driving dataset demonstrate the superiority of our\napproach compared to existing approaches and other ablative variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Aman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dev Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Anbumani Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saluja_R/0/1/0/all/0/1\">Rohit Saluja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Deepfakes with Self-Blended Images. (arXiv:2204.08376v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08376","description":"<p>In this paper, we present novel synthetic training data called self-blended\nimages (SBIs) to detect deepfakes. SBIs are generated by blending pseudo source\nand target images from single pristine images, reproducing common forgery\nartifacts (e.g., blending boundaries and statistical inconsistencies between\nsource and target images). The key idea behind SBIs is that more general and\nhardly recognizable fake samples encourage classifiers to learn generic and\nrobust representations without overfitting to manipulation-specific artifacts.\nWe compare our approach with state-of-the-art methods on FF++, CDF, DFD, DFDC,\nDFDCP, and FFIW datasets by following the standard cross-dataset and\ncross-manipulation protocols. Extensive experiments show that our method\nimproves the model generalization to unknown manipulations and scenes. In\nparticular, on DFDC and DFDCP where existing methods suffer from the domain gap\nbetween the training and test sets, our approach outperforms the baseline by\n4.90% and 11.78% points in the cross-dataset evaluation, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiohara_K/0/1/0/all/0/1\">Kaede Shiohara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamasaki_T/0/1/0/all/0/1\">Toshihiko Yamasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple-environment Self-adaptive Network for Aerial-view Geo-localization. (arXiv:2204.08381v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08381","description":"<p>Aerial-view geo-localization tends to determine an unknown position through\nmatching the drone-view image with the geo-tagged satellite-view image. This\ntask is mostly regarded as an image retrieval problem. The key underpinning\nthis task is to design a series of deep neural networks to learn discriminative\nimage descriptors. However, existing methods meet large performance drops under\nrealistic weather, such as rain and fog, since they do not take the domain\nshift between the training data and multiple test environments into\nconsideration. To minor this domain gap, we propose a Multiple-environment\nSelf-adaptive Network (MuSe-Net) to dynamically adjust the domain shift caused\nby environmental changing. In particular, MuSe-Net employs a two-branch neural\nnetwork containing one multiple-environment style extraction network and one\nself-adaptive feature extraction network. As the name implies, the\nmultiple-environment style extraction network is to extract the\nenvironment-related style information, while the self-adaptive feature\nextraction network utilizes an adaptive modulation module to dynamically\nminimize the environment-related style gap. Extensive experiments on two\nwidely-used benchmarks, i.e., University-1652 and CVUSA, demonstrate that the\nproposed MuSe-Net achieves a competitive result for geo-localization in\nmultiple environments. Furthermore, we observe that the proposed method also\nshows great potential to the unseen extreme weather, such as mixing the fog,\nrain and snow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yaoqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenggang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subspace Nonnegative Matrix Factorization for Feature Representation. (arXiv:2204.08382v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08382","description":"<p>Traditional nonnegative matrix factorization (NMF) learns a new feature\nrepresentation on the whole data space, which means treating all features\nequally. However, a subspace is often sufficient for accurate representation in\npractical applications, and redundant features can be invalid or even harmful.\nFor example, if a camera has some sensors destroyed, then the corresponding\npixels in the photos from this camera are not helpful to identify the content,\nwhich means only the subspace consisting of remaining pixels is worthy of\nattention. This paper proposes a new NMF method by introducing adaptive weights\nto identify key features in the original space so that only a subspace involves\ngenerating the new representation. Two strategies are proposed to achieve this:\nthe fuzzier weighted technique and entropy regularized weighted technique, both\nof which result in an iterative solution with a simple form. Experimental\nresults on several real-world datasets demonstrated that the proposed methods\ncan generate a more accurate feature representation than existing methods. The\ncode developed in this study is available at\nhttps://github.com/WNMF1/FWNMF-ERWNMF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Can Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tingting Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shouliang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yueyang Teng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHSA-Net: Multi-Head Self-Attention Network for Occluded Person Re-Identification. (arXiv:2008.04015v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.04015","description":"<p>This paper presents a novel person re-identification model, named Multi-Head\nSelf-Attention Network (MHSA-Net), to prune unimportant information and capture\nkey local information from person images. MHSA-Net contains two main novel\ncomponents: Multi-Head Self-Attention Branch (MHSAB) and Attention Competition\nMechanism (ACM). The MHSAB adaptively captures key local person information,\nand then produces effective diversity embeddings of an image for the person\nmatching. The ACM further helps filter out attention noise and non-key\ninformation. Through extensive ablation studies, we verified that the\nMulti-Head Self-Attention Branch (MHSAB) and Attention Competition Mechanism\n(ACM) both contribute to the performance improvement of the MHSA-Net. Our\nMHSA-Net achieves competitive performance in the standard and occluded person\nRe-ID tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hongchen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can You Spot the Chameleon? Adversarially Camouflaging Images from Co-Salient Object Detection. (arXiv:2009.09258v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.09258","description":"<p>Co-salient object detection (CoSOD) has recently achieved significant\nprogress and played a key role in retrieval-related tasks. However, it\ninevitably poses an entirely new safety and security issue, i.e., highly\npersonal and sensitive content can potentially be extracting by powerful CoSOD\nmethods. In this paper, we address this problem from the perspective of\nadversarial attacks and identify a novel task: adversarial co-saliency attack.\nSpecially, given an image selected from a group of images containing some\ncommon and salient objects, we aim to generate an adversarial version that can\nmislead CoSOD methods to predict incorrect co-salient regions. Note that,\ncompared with general white-box adversarial attacks for classification, this\nnew task faces two additional challenges: (1) low success rate due to the\ndiverse appearance of images in the group; (2) low transferability across CoSOD\nmethods due to the considerable difference between CoSOD pipelines. To address\nthese challenges, we propose the very first black-box joint adversarial\nexposure and noise attack (Jadena), where we jointly and locally tune the\nexposure and additive perturbations of the image according to a newly designed\nhigh-feature-level contrast-sensitive loss function. Our method, without any\ninformation on the state-of-the-art CoSOD methods, leads to significant\nperformance degradation on various co-saliency detection datasets and makes the\nco-salient objects undetectable. This can have strong practical benefits in\nproperly securing the large number of personal photos currently shared on the\nInternet. Moreover, our method is potential to be utilized as a metric for\nevaluating the robustness of CoSOD methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruijun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Neural Networks via Orthogonal Diversity. (arXiv:2010.12190v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12190","description":"<p>Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the\nimages generated by adversarial attacks, which raises researches on the\nadversarial robustness of DNNs. A series of methods represented by the\nadversarial training and its variants have proven as one of the most effective\ntechniques in enhancing the DNN robustness. Generally, adversarial training\nfocuses on enriching the training data by involving perturbed data. Despite of\nthe efficiency in defending specific attacks, adversarial training is benefited\nfrom the data augmentation, which does not contribute to the robustness of DNN\nitself and usually suffers from accuracy drop on clean data as well as\ninefficiency in unknown attacks. Towards the robustness of DNN itself, we\npropose a novel defense that aims at augmenting the model in order to learn\nfeatures adaptive to diverse inputs, including adversarial examples.\nSpecifically, we introduce multiple paths to augment the network, and impose\northogonality constraints on these paths. In addition, a margin-maximization\nloss is designed to further boost DIversity via Orthogonality (DIO). Extensive\nempirical results on various data sets, architectures, and attacks demonstrate\nthe adversarial robustness of the proposed DIO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qinghua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingwen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1\">Feipeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stretchable Cells Help DARTS Search Better. (arXiv:2011.09300v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09300","description":"<p>Differentiable neural architecture search (DARTS) has gained much success in\ndiscovering flexible and diverse cell types. To reduce the evaluation gap, the\nsupernet is expected to have identical layers with the target network. However,\neven for this consistent search, the searched cells often suffer from poor\nperformance, especially for the supernet with fewer layers, as current DARTS\nmethods are prone to wide and shallow cells, and this topology collapse induces\nsub-optimal searched cells. In this paper, we alleviate this issue by endowing\nthe cells with explicit stretchability, so the search can be directly\nimplemented on our stretchable cells for both operation type and topology\nsimultaneously. Concretely, we introduce a set of topological variables and a\ncombinatorial probabilistic distribution to explicitly model the target\ntopology. With more diverse and complex topologies, our method adapts well for\nvarious layer numbers. Extensive experiments on CIFAR-10 and ImageNet show that\nour stretchable cells obtain better performance with fewer layers and\nparameters. For example, our method can improve DARTS by 0.28\\% accuracy on\nCIFAR-10 dataset with 45\\% parameters reduced or 2.9\\% with similar FLOPs on\nImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuozhuo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Layout Manipulation with High-Resolution Sparse Attention. (arXiv:2012.07288v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.07288","description":"<p>We tackle the problem of semantic image layout manipulation, which aims to\nmanipulate an input image by editing its semantic label map. A core problem of\nthis task is how to transfer visual details from the input images to the new\nsemantic layout while making the resulting image visually realistic. Recent\nwork on learning cross-domain correspondence has shown promising results for\nglobal layout transfer with dense attention-based warping. However, this method\ntends to lose texture details due to the resolution limitation and the lack of\nsmoothness constraint of correspondence. To adapt this paradigm for the layout\nmanipulation task, we propose a high-resolution sparse attention module that\neffectively transfers visual details to new layouts at a resolution up to\n512x512. To further improve visual quality, we introduce a novel generator\narchitecture consisting of a semantic encoder and a two-stage decoder for\ncoarse-to-fine synthesis. Experiments on the ADE20k and Places365 datasets\ndemonstrate that our proposed approach achieves substantial improvements over\nthe existing inpainting and layout manipulation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss. (arXiv:2101.11952v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11952","description":"<p>Boundary discontinuity and its inconsistency to the final detection metric\nhave been the bottleneck for rotating detection regression loss design. In this\npaper, we propose a novel regression loss based on Gaussian Wasserstein\ndistance as a fundamental approach to solve the problem. Specifically, the\nrotated bounding box is converted to a 2-D Gaussian distribution, which enables\nto approximate the indifferentiable rotational IoU induced loss by the Gaussian\nWasserstein distance (GWD) which can be learned efficiently by gradient\nback-propagation. GWD can still be informative for learning even there is no\noverlapping between two rotating bounding boxes which is often the case for\nsmall object detection. Thanks to its three unique properties, GWD can also\nelegantly solve the boundary discontinuity and square-like problem regardless\nhow the bounding box is defined. Experiments on five datasets using different\ndetectors show the effectiveness of our approach. Codes are available at\nhttps://github.com/yangxue0827/RotationDetection and\nhttps://github.com/open-mmlab/mmrotate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Deep ML Architecture by Integrating Visual Simultaneous Localization and Mapping (vSLAM) into Mask R-CNN for Real-time Surgical Video Analysis. (arXiv:2103.16847v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.16847","description":"<p>Seven million people suffer surgical complications each year, but with\nsufficient surgical training and review, 50\\% of these complications could be\nprevented. To improve surgical performance, existing research uses various deep\nlearning (DL) technologies including convolutional neural networks (CNN) and\nrecurrent neural networks (RNN) to automate surgical tool and workflow\ndetection. However, there is room to improve accuracy; real-time analysis is\nalso minimal due to the complexity of CNN. In this research, a novel DL\narchitecture is proposed to integrate visual simultaneous localization and\nmapping (vSLAM) into Mask R-CNN. This architecture, vSLAM-CNN (vCNN), for the\nfirst time, integrates the best of both worlds, inclusive of (1) vSLAM for\nobject detection, by focusing on geometric information for region proposals,\nand (2) CNN for object recognition, by focusing on semantic information for\nimage classification, combining them into one joint end-to-end training\nprocess. This method, using spatio-temporal information in addition to visual\nfeatures, is evaluated on M2CAI 2016 challenge datasets, achieving the\nstate-of-the-art results with 96.8 mAP for tool detection and 97.5 mean Jaccard\nscore for workflow detection, surpassing all previous works, and reaching a 50\nFPS performance, 10x faster than the region-based CNN. A region proposal module\n(RPM) replaces the region proposal network (RPN) in Mask R-CNN, accurately\nplacing bounding boxes and lessening the annotation requirement. Furthermore, a\nMicrosoft HoloLens 2 application is developed to provide an augmented reality\n(AR)-based solution for surgical training and assistance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lan_E/0/1/0/all/0/1\">Ella Selina Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIH: Towards More Accurate Face Alignment via Heatmap in Heatmap. (arXiv:2104.03100v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03100","description":"<p>Heatmap-based regression overcomes the lack of spatial and contextual\ninformation of direct coordinate regression, and has revolutionized the task of\nface alignment. Yet it suffers from quantization errors caused by neglecting\nsubpixel coordinates in image resizing and network downsampling. In this paper,\nwe first quantitatively analyze the quantization error on benchmarks, which\naccounts for more than 1/3 of the whole prediction errors for state-of-the-art\nmethods. To tackle this problem, we propose a novel Heatmap In Heatmap(HIH)\nrepresentation and a coordinate soft-classification (CSC) method, which are\nseamlessly integrated into the classic hourglass network. The HIH\nrepresentation utilizes nested heatmaps to jointly represent the coordinate\nlabel: one heatmap called integer heatmap stands for the integer coordinate,\nand the other heatmap named decimal heatmap represents the subpixel coordinate.\nThe range of a decimal heatmap makes up one pixel in the corresponding integer\nheatmap. Besides, we transfer the offset regression problem to an interval\nclassification task, and CSC regards the confidence of the pixel as the\nprobability of the interval. Meanwhile, CSC applying the distribution loss\nleverage the soft labels generated from the Gaussian distribution function to\nguide the offset heatmap training, which makes it easier to learn the\ndistribution of coordinate offsets. Extensive experiments on challenging\nbenchmark datasets demonstrate that our HIH can achieve state-of-the-art\nresults. In particular, our HIH reaches 4.08 NME (Normalized Mean Error) on\nWFLW, and 3.21 on COFW, which exceeds previous methods by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xing Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving. (arXiv:2105.00373v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.00373","description":"<p>The past few years have witnessed an increasing interest in improving the\nperception performance of LiDARs on autonomous vehicles. While most of the\nexisting works focus on developing new deep learning algorithms or model\narchitectures, we study the problem from the physical design perspective, i.e.,\nhow different placements of multiple LiDARs influence the learning-based\nperception. To this end, we introduce an easy-to-compute information-theoretic\nsurrogate metric to quantitatively and fast evaluate LiDAR placement for 3D\ndetection of different types of objects. We also present a new data collection,\ndetection model training and evaluation framework in the realistic CARLA\nsimulator to evaluate disparate multi-LiDAR configurations. Using several\nprevalent placements inspired by the designs of self-driving companies, we show\nthe correlation between our surrogate metric and object detection performance\nof different representative algorithms on KITTI through extensive experiments,\nvalidating the effectiveness of our LiDAR placement evaluation approach. Our\nresults show that sensor placement is non-negligible in 3D point cloud-based\nobject detection, which will contribute up to 10% performance discrepancy in\nterms of average precision in challenging 3D object detection settings. We\nbelieve that this is one of the first studies to quantitatively investigate the\ninfluence of LiDAR placement on perception performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1\">Sharad Chitlangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnihotri_A/0/1/0/all/0/1\">Akhil Agnihotri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Objects in Clutter. (arXiv:2105.03053v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03053","description":"<p>This paper identifies and addresses a serious design bias of existing salient\nobject detection (SOD) datasets, which unrealistically assume that each image\nshould contain at least one clear and uncluttered salient object. This design\nbias has led to a saturation in performance for state-of-the-art SOD models\nwhen evaluated on existing datasets. However, these models are still far from\nsatisfactory when applied to real-world scenes. Based on our analyses, we\npropose a new high-quality dataset and update the previous saliency benchmark.\nSpecifically, our dataset, called Salient Objects in Clutter~\\textbf{(SOC)},\nincludes images with both salient and non-salient objects from several common\nobject categories. In addition to object category annotations, each salient\nimage is accompanied by attributes that reflect common challenges in common\nscenes, which can help provide deeper insight into the SOD problem. Further,\nwith a given saliency encoder, e.g., the backbone network, existing saliency\nmodels are designed to achieve mapping from the training image set to the\ntraining ground-truth set. We, therefore, argue that improving the dataset can\nyield higher performance gains than focusing only on the decoder design. With\nthis in mind, we investigate several dataset-enhancement strategies, including\nlabel smoothing to implicitly emphasize salient boundaries, random image\naugmentation to adapt saliency models to various scenarios, and self-supervised\nlearning as a regularization strategy to learn from small datasets. Our\nextensive results demonstrate the effectiveness of these tricks. We also\nprovide a comprehensive benchmark for SOD, which can be found in our\nrepository: https://github.com/DengPingFan/SODBenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCN-MIF: Graph Convolutional Network with Multi-Information Fusion for Low-dose CT Denoising. (arXiv:2105.07146v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.07146","description":"<p>Being low-level radiation exposure and less harmful to health, low-dose\ncomputed tomography (LDCT) has been widely adopted in the early screening of\nlung cancer and COVID-19. LDCT images inevitably suffer from the degradation\nproblem caused by complex noises. It was reported that deep learning (DL)-based\nLDCT denoising methods using convolutional neural network (CNN) achieved\nimpressive denoising performance. Although most existing DL-based methods\n(e.g., encoder-decoder framework) can implicitly utilize non-local and\ncontextual information via downsampling operator and 3D CNN, the explicit\nmulti-information (i.e., local, non-local, and contextual) integration may not\nbe explored enough. To address this issue, we propose a novel graph\nconvolutional network-based LDCT denoising model, namely GCN-MIF, to explicitly\nperform multi-information fusion for denoising purpose. Concretely, by\nconstructing intra- and inter-slice graph, the graph convolutional network is\nintroduced to leverage the non-local and contextual relationships among pixels.\nThe traditional CNN is adopted for the extraction of local information.\nFinally, the proposed GCN-MIF model fuses all the extracted local, non-local,\nand contextual information. Extensive experiments show the effectiveness of our\nproposed GCN-MIF model by quantitative and visualized results. Furthermore, a\ndouble-blind reader study on a public clinical dataset is also performed to\nvalidate the usability of denoising results in terms of the structural\nfidelity, the noise suppression, and the overall score. Models and code are\navailable at https://github.com/tonyckc/GCN-MIF_demo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Kecheng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jiayu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1\">Jiang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jixiang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xuelin Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_D/0/1/0/all/0/1\">Dongsheng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bento_M/0/1/0/all/0/1\">Miguel Bento</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pu_X/0/1/0/all/0/1\">Xiaorong Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.09624","description":"<p>Photoacoustic (PA) imaging has the potential to revolutionize functional\nmedical imaging in healthcare due to the valuable information on tissue\nphysiology contained in multispectral photoacoustic measurements. Clinical\ntranslation of the technology requires conversion of the high-dimensional\nacquired data into clinically relevant and interpretable information. In this\nwork, we present a deep learning-based approach to semantic segmentation of\nmultispectral photoacoustic images to facilitate image interpretability.\nManually annotated photoacoustic {and ultrasound} imaging data are used as\nreference and enable the training of a deep learning-based segmentation\nalgorithm in a supervised manner. Based on a validation study with\nexperimentally acquired data from 16 healthy human volunteers, we show that\nautomatic tissue segmentation can be used to create powerful analyses and\nvisualizations of multispectral photoacoustic images. Due to the intuitive\nrepresentation of high-dimensional information, such a preprocessing algorithm\ncould be a valuable means to facilitate the clinical translation of\nphotoacoustic imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1\">Melanie Schellenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1\">Kris Dreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1\">Niklas Holzwarth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schreck_N/0/1/0/all/0/1\">Nicholas Schreck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1\">Janek Gr&#xf6;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D U-NetR: Low Dose Computed Tomography Reconstruction via Deep Learning and 3 Dimensional Convolutions. (arXiv:2105.14130v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14130","description":"<p>In this paper, we introduced a novel deep learning-based reconstruction\ntechnique for low-dose CT imaging using 3 dimensional convolutions to include\nthe sagittal information unlike the existing 2 dimensional networks which\nexploits correlation only in transverse plane. In the proposed reconstruction\ntechnique, sparse and noisy sinograms are back-projected to the image domain\nwith FBP operation, then the denoising process is applied with a U-Net like\n3-dimensional network called 3D U-NetR. The proposed network is trained with\nsynthetic and real chest CT images, and 2D U-Net is also trained with the same\ndataset to show the importance of the third dimension in terms of recovering\nthe fine details. The proposed network shows better quantitative performance on\nSSIM and PSNR, especially in the real chest CT data. More importantly, 3D\nU-NetR captures medically critical visual details that cannot be visualized by\na 2D network on the reconstruction of real CT images with 1/10 of the normal\ndose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunduzalp_D/0/1/0/all/0/1\">Doga Gunduzalp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cengiz_B/0/1/0/all/0/1\">Batuhan Cengiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_M/0/1/0/all/0/1\">Mehmet Ozan Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_I/0/1/0/all/0/1\">Isa Yildirim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence. (arXiv:2106.01883v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01883","description":"<p>Existing rotated object detectors are mostly inherited from the horizontal\ndetection paradigm, as the latter has evolved into a well-developed area.\nHowever, these detectors are difficult to perform prominently in high-precision\ndetection due to the limitation of current regression loss design, especially\nfor objects with large aspect ratios. Taking the perspective that horizontal\ndetection is a special case for rotated object detection, in this paper, we are\nmotivated to change the design of rotation regression loss from induction\nparadigm to deduction methodology, in terms of the relation between rotation\nand horizontal detection. We show that one essential challenge is how to\nmodulate the coupled parameters in the rotation regression loss, as such the\nestimated parameters can influence to each other during the dynamic joint\noptimization, in an adaptive and synergetic way. Specifically, we first convert\nthe rotated bounding box into a 2-D Gaussian distribution, and then calculate\nthe Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the\nregression loss. By analyzing the gradient of each parameter, we show that KLD\n(and its derivatives) can dynamically adjust the parameter gradients according\nto the characteristics of the object. It will adjust the importance (gradient\nweight) of the angle parameter according to the aspect ratio. This mechanism\ncan be vital for high-precision detection as a slight angle error would cause a\nserious accuracy drop for large aspect ratios objects. More importantly, we\nhave proved that KLD is scale invariant. We further show that the KLD loss can\nbe degenerated into the popular $l_{n}$-norm loss for horizontal detection.\nExperimental results on seven datasets using different detectors show its\nconsistent superiority, and codes are available at\nhttps://github.com/yangxue0827/RotationDetection and\nhttps://github.com/open-mmlab/mmrotate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaojiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jirui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track. (arXiv:2106.10829v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10829","description":"<p>This technical report presents our solution to the HACS Temporal Action\nLocalization Challenge 2021, Weakly-Supervised Learning Track. The goal of\nweakly-supervised temporal action localization is to temporally locate and\nclassify action of interest in untrimmed videos given only video-level labels.\nWe adopt the two-stream consensus network (TSCN) as the main framework in this\nchallenge. The TSCN consists of a two-stream base model training procedure and\na pseudo ground truth learning procedure. The base model training encourages\nthe model to predict reliable predictions based on single modality (i.e., RGB\nor optical flow), based on the fusion of which a pseudo ground truth is\ngenerated and in turn used as supervision to train the base models. On the HACS\nv1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our\nmethod achieves 22.20% on the validation set and 21.68% on the testing set in\nterms of average mAP. Our solution ranked the 2nd in this challenge, and we\nhope our method can serve as a baseline for future academic research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuanhao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Zero-Shot\" Point Cloud Upsampling. (arXiv:2106.13765v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13765","description":"<p>Recent supervised point cloud upsampling methods are restricted by the size\nof training data and are limited in terms of covering all object shapes.\nBesides the challenges faced due to data acquisition, the networks also\nstruggle to generalize on unseen records. In this paper, we present an internal\npoint cloud upsampling approach at a holistic level referred to as \"Zero-Shot\"\nPoint Cloud Upsampling (ZSPU). Our approach is data agnostic and relies solely\non the internal information provided by a particular point cloud without\npatching in both self-training and testing phases. This single-stream design\nsignificantly reduces the training time by learning the relation between low\nresolution (LR) point clouds and their high (original) resolution (HR)\ncounterparts. This association will then provide super resolution (SR) outputs\nwhen original point clouds are loaded as input. ZSPU achieves\ncompetitive/superior quantitative and qualitative performances on benchmark\ndatasets when compared with other upsampling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arslanturk_S/0/1/0/all/0/1\">Suzan Arslanturk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Cycle Pruning: Pruning ConvNets Under a Tight Training Budget. (arXiv:2107.02086v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02086","description":"<p>Introducing sparsity in a neural network has been an efficient way to reduce\nits complexity while keeping its performance almost intact. Most of the time,\nsparsity is introduced using a three-stage pipeline: 1) train the model to\nconvergence, 2) prune the model according to some criterion, 3) fine-tune the\npruned model to recover performance. The last two steps are often performed\niteratively, leading to reasonable results but also to a time-consuming and\ncomplex process. In our work, we propose to get rid of the first step of the\npipeline and to combine the two other steps in a single pruning-training cycle,\nallowing the model to jointly learn for the optimal weights while being pruned.\nWe do this by introducing a novel pruning schedule, named One-Cycle Pruning,\nwhich starts pruning from the beginning of the training, and until its very\nend. Adopting such a schedule not only leads to better performing pruned models\nbut also drastically reduces the training budget required to prune a model.\nExperiments are conducted on a variety of architectures (VGG-16 and ResNet-18)\nand datasets (CIFAR-10, CIFAR-100 and Caltech-101), and for relatively high\nsparsity values (80%, 90%, 95% of weights removed). Our results show that\nOne-Cycle Pruning consistently outperforms commonly used pruning schedules such\nas One-Shot Pruning, Iterative Pruning and Automated Gradual Pruning, on a\nfixed training budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hubens_N/0/1/0/all/0/1\">Nathan Hubens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancas_M/0/1/0/all/0/1\">Matei Mancas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_B/0/1/0/all/0/1\">Bernard Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preda_M/0/1/0/all/0/1\">Marius Preda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_T/0/1/0/all/0/1\">Titus Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Large Circular Kernels into CNNs through Neural Architecture Search. (arXiv:2107.02451v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02451","description":"<p>The square kernel is a standard unit for contemporary CNNs, as it fits well\non the tensor computation for convolution operation. However, the retinal\nganglion cells in the biological visual system have approximately concentric\nreceptive fields. Motivated by this observation, we propose to use circular\nkernel with a concentric and isotropic receptive field as an option for the\nconvolution operation. We first propose a simple yet efficient implementation\nof the convolution using circular kernels, and empirically show the significant\nadvantages of large circular kernels over the counterpart square kernels. We\nthen expand the operation space of several typical Neural Architecture Search\n(NAS) methods with the convolutions of large circular kernels. The searched new\nneural architectures do contain large circular kernels and outperform the\noriginal searched models considerably. Our additional analysis also reveals\nthat large circular kernels could help the model to be more robust to the\nrotated or sheared images due to their better rotation invariance. Our work\nshows the potential of designing new convolutional kernels for CNNs, bringing\nup the prospect of expanding the search space of NAS with new variants of\nconvolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yixiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopcroft_J/0/1/0/all/0/1\">John E. Hopcroft</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Classifiers Based Maximum Classifier Discrepancy for Unsupervised Domain Adaptation. (arXiv:2108.00610v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00610","description":"<p>Adversarial training based on the maximum classifier discrepancy between two\nclassifier structures has achieved great success in unsupervised domain\nadaptation tasks for image classification. The approach adopts the structure of\ntwo classifiers, though simple and intuitive, the learned classification\nboundary may not well represent the data property in the new domain. In this\npaper, we propose to extend the structure to multiple classifiers to further\nboost its performance. To this end, we develop a very straightforward approach\nto adding more classifiers. We employ the principle that the classifiers are\ndifferent from each other to construct a discrepancy loss function for multiple\nclassifiers. The proposed construction method of loss function makes it\npossible to add any number of classifiers to the original framework. The\nproposed approach is validated through extensive experimental evaluations. We\ndemonstrate that, on average, adopting the structure of three classifiers\nnormally yields the best performance as a trade-off between accuracy and\nefficiency. With minimum extra computational costs, the proposed approach can\nsignificantly improve the performance of the original algorithm. The source\ncode of the proposed approach can be downloaded from\n\\url{https://github.com/rucv/MMCD\\_DA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiju Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Research on Gender-related Fingerprint Features. (arXiv:2108.08233v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08233","description":"<p>Fingerprint is an important biological feature of human body, which contains\nabundant gender information. At present, the academic research of fingerprint\ngender characteristics is generally at the level of understanding, while the\nstandardization research is quite limited. In this work, we propose a more\nrobust method, Dense Dilated Convolution ResNet (DDC-ResNet) to extract valid\ngender information from fingerprints. By replacing the normal convolution\noperations with the atrous convolution in the backbone, prior knowledge is\nprovided to keep the edge details and the global reception field can be\nextended. We explored the results in 3 ways: 1) The efficiency of the\nDDC-ResNet. 6 typical methods of automatic feature extraction coupling with 9\nmainstream classifiers are evaluated in our dataset with fair implementation\ndetails. Experimental results demonstrate that the combination of our approach\noutperforms other combinations in terms of average accuracy and separate-gender\naccuracy. It reaches 96.5% for average and 0.9752 (males)/0.9548 (females) for\nseparate-gender accuracy. 2) The effect of fingers. It is found that the best\nperformance of classifying gender with separate fingers is achieved by the\nright ring finger. 3) The effect of specific features. Based on the\nobservations of the concentrations of fingerprints visualized by our approach,\nit can be inferred that loops and whorls (level 1), bifurcations (level 2), as\nwell as line shapes (level 3) are connected with gender. Finally, we will open\nsource the dataset that contains 6000 fingerprint images\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huawei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiashu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Huaiguang Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12043","description":"<p>Disentangled representation learning has been proposed as an approach to\nlearning general representations even in the absence of, or with limited,\nsupervision. A good general representation can be fine-tuned for new target\ntasks using modest amounts of data, or used directly in unseen domains\nachieving remarkable performance in the corresponding task. This alleviation of\nthe data and annotation requirements offers tantalising prospects for\napplications in computer vision and healthcare. In this tutorial paper, we\nmotivate the need for disentangled representations, revisit key concepts, and\ndescribe practical building blocks and criteria for learning such\nrepresentations. We survey applications in medical imaging emphasising choices\nmade in exemplar key works, and then discuss links to computer vision\napplications. We conclude by presenting limitations, challenges, and\nopportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pedro Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.05265","description":"<p>Stereoscopy exposits a natural perception of distance in a scene, and its\nmanifestation in 3D world understanding is an intuitive phenomenon. However, an\ninnate rigid calibration of binocular vision sensors is crucial for accurate\ndepth estimation. Alternatively, a monocular camera alleviates the limitation\nat the expense of accuracy in estimating depth, and the challenge exacerbates\nin harsh environmental conditions. Moreover, an optical sensor often fails to\nacquire vital signals in harsh environments, and radar is used instead, which\ngives coarse but more accurate signals. This work explores the utility of\ncoarse signals from radar when fused with fine-grained data from a monocular\ncamera for depth estimation in harsh environmental conditions. A variant of\nfeature pyramid network (FPN) extensively operates on fine-grained image\nfeatures at multiple scales with a fewer number of parameters. FPN feature maps\nare fused with sparse radar features extracted with a Convolutional neural\nnetwork. The concatenated hierarchical features are used to predict the depth\nwith ordinal regression. We performed experiments on the nuScenes dataset, and\nthe proposed architecture stays on top in quantitative evaluations with reduced\nparameters and faster inference. The depth estimation results suggest that the\nproposed techniques can be used as an alternative to stereo depth estimation in\ncritical applications in robotics and self-driving cars. The source code will\nbe available in the following: \\url{https://github.com/MI-Hussain/RVMDE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1\">Muhamamd Ishfaq Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_M/0/1/0/all/0/1\">Muhammad Aasim Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Non-Line-of-Sight Photography. (arXiv:2109.07783v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.07783","description":"<p>Non-line-of-sight (NLOS) imaging is based on capturing the multi-bounce\nindirect reflections from the hidden objects. Active NLOS imaging systems rely\non the capture of the time of flight of light through the scene, and have shown\ngreat promise for the accurate and robust reconstruction of hidden scenes\nwithout the need for specialized scene setups and prior assumptions. Despite\nthat existing methods can reconstruct 3D geometries of the hidden scene with\nexcellent depth resolution, accurately recovering object textures and\nappearance with high lateral resolution remains an challenging problem. In this\nwork, we propose a new problem formulation, called NLOS photography, to\nspecifically address this deficiency. Rather than performing an intermediate\nestimate of the 3D scene geometry, our method follows a data-driven approach\nand directly reconstructs 2D images of a NLOS scene that closely resemble the\npictures taken with a conventional camera from the location of the relay wall.\nThis formulation largely simplifies the challenging reconstruction problem by\nbypassing the explicit modeling of 3D geometry, and enables the learning of a\ndeep model with a relatively small training dataset. The results are NLOS\nreconstructions of unprecedented lateral resolution and image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_J/0/1/0/all/0/1\">Jiayong Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_F/0/1/0/all/0/1\">Fangzhou Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nam_J/0/1/0/all/0/1\">Ji Hyun Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raghavan_S/0/1/0/all/0/1\">Siddeshwar Raghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Velten_A/0/1/0/all/0/1\">Andreas Velten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Instance Segmentation with Automotive Radar Detection Points. (arXiv:2110.01775v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01775","description":"<p>Automotive radar provides reliable environmental perception in all-weather\nconditions with affordable cost, but it hardly supplies semantic and geometry\ninformation due to the sparsity of radar detection points. With the development\nof automotive radar technologies in recent years, instance segmentation becomes\npossible by using automotive radar. Its data contain contexts such as radar\ncross section and micro-Doppler effects, and sometimes can provide detection\nwhen the field of view is obscured. The outcome from instance segmentation\ncould be potentially used as the input of trackers for tracking targets. The\nexisting methods often utilize a clustering-based classification framework,\nwhich fits the need of real-time processing but has limited performance due to\nminimum information provided by sparse radar detection points. In this paper,\nwe propose an efficient method based on clustering of estimated semantic\ninformation to achieve instance segmentation for the sparse radar detection\npoints. In addition, we show that the performance of the proposed approach can\nbe further enhanced by incorporating the visual multi-layer perceptron. The\neffectiveness of the proposed method is verified by experimental results on the\npopular RadarScenes dataset, achieving 89.53% mean coverage and 86.97% mean\naverage precision with the IoU threshold of 0.5, which is superior to other\napproaches in the literature. More significantly, the consumed memory is around\n1MB, and the inference time is less than 40ms, indicating that our proposed\nalgorithm is storage and time efficient. These two criteria ensure the\npracticality of the proposed method in real-world systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weiyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimMIM: A Simple Framework for Masked Image Modeling. (arXiv:2111.09886v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09886","description":"<p>This paper presents SimMIM, a simple framework for masked image modeling. We\nsimplify recently proposed related approaches without special designs such as\nblock-wise masking and tokenization via discrete VAE or clustering. To study\nwhat let the masked image modeling task learn good representations, we\nsystematically study the major components in our framework, and find that\nsimple designs of each component have revealed very strong representation\nlearning performance: 1) random masking of the input image with a moderately\nlarge masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting\nraw pixels of RGB values by direct regression performs no worse than the patch\nclassification approaches with complex designs; 3) the prediction head can be\nas light as a linear layer, with no worse performance than heavier ones. Using\nViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by\npre-training also on this dataset, surpassing previous best approach by +0.6%.\nWhen applied on a larger model of about 650 million parameters, SwinV2-H, it\nachieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We\nalso leverage this approach to facilitate the training of a 3B model\n(SwinV2-G), that by $40\\times$ less data than that in previous practice, we\nachieve the state-of-the-art on four representative vision benchmarks. The code\nand models will be publicly available at https://github.com/microsoft/SimMIM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuliang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion. (arXiv:2111.10332v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10332","description":"<p>Point cloud processing is a challenging task due to its sparsity and\nirregularity. Prior works introduce delicate designs on either local feature\naggregator or global geometric architecture, but few combine both advantages.\nWe propose Dual-Scale Point Cloud Recognition with High-frequency Fusion\n(DSPoint) to extract local-global features by concurrently operating on voxels\nand points. We reverse the conventional design of applying convolution on\nvoxels and attention to points. Specifically, we disentangle point features\nthrough channel dimension for dual-scale processing: one by point-wise\nconvolution for fine-grained geometry parsing, the other by voxel-wise global\nattention for long-range structural exploration. We design a co-attention\nfusion module for feature alignment to blend local-global modalities, which\nconducts inter-scale cross-modality interaction by communicating high-frequency\ncoordinates information. Experiments and ablations on widely-adopted\nModelNet40, ShapeNet, and S3DIS demonstrate the state-of-the-art performance of\nour DSPoint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinben Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kexue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianbo Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GreedyNASv2: Greedier Search with a Greedy Path Filter. (arXiv:2111.12609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12609","description":"<p>Training a good supernet in one-shot NAS methods is difficult since the\nsearch space is usually considerably huge (e.g., $13^{21}$). In order to\nenhance the supernet's evaluation ability, one greedy strategy is to sample\ngood paths, and let the supernet lean towards the good ones and ease its\nevaluation burden as a result. However, in practice the search can be still\nquite inefficient since the identification of good paths is not accurate enough\nand sampled paths still scatter around the whole search space. In this paper,\nwe leverage an explicit path filter to capture the characteristics of paths and\ndirectly filter those weak ones, so that the search can be thus implemented on\nthe shrunk space more greedily and efficiently. Concretely, based on the fact\nthat good paths are much less than the weak ones in the space, we argue that\nthe label of \"weak paths\" will be more confident and reliable than that of\n\"good paths\" in multi-path sampling. In this way, we thus cast the training of\npath filter in the positive and unlabeled (PU) learning paradigm, and also\nencourage a \\textit{path embedding} as better path/operation representation to\nenhance the identification capacity of the learned filter. By dint of this\nembedding, we can further shrink the search space by aggregating similar\noperations with similar embeddings, and the search can be more efficient and\naccurate. Extensive experiments validate the effectiveness of the proposed\nmethod GreedyNASv2. For example, our obtained GreedyNASv2-L achieves $81.1\\%$\nTop-1 accuracy on ImageNet dataset, significantly outperforming the ResNet-50\nstrong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling. (arXiv:2111.12681v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12681","description":"<p>A great challenge in video-language (VidL) modeling lies in the disconnection\nbetween fixed video representations extracted from image/video understanding\nmodels and downstream VidL data. Recent studies try to mitigate this\ndisconnection via end-to-end training. To make it computationally feasible,\nprior works tend to \"imagify\" video inputs, i.e., a handful of sparsely sampled\nframes are fed into a 2D CNN, followed by a simple mean-pooling or\nconcatenation to obtain the overall video representations. Although achieving\npromising results, such simple approaches may lose temporal information that is\nessential for performing downstream VidL tasks. In this work, we present\nVIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video\ntransformer to explicitly model the temporal dynamics of video inputs. Further,\nunlike previous studies that found pre-training tasks on video inputs (e.g.,\nmasked frame modeling) not very effective, we design a new pre-training task,\nMasked Visual-token Modeling (MVM), for better video modeling. Specifically,\nthe original video frame patches are \"tokenized\" into discrete visual tokens,\nand the goal is to recover the original visual tokens based on the masked\npatches. Comprehensive analysis demonstrates the effectiveness of both explicit\ntemporal modeling via video transformer and MVM. As a result, VIOLET achieves\nnew state-of-the-art performance on 5 video question answering tasks and 4\ntext-to-video retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Low-Cost and Efficient Malaria Detection. (arXiv:2111.13656v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13656","description":"<p>Malaria, a fatal but curable disease claims hundreds of thousands of lives\nevery year. Early and correct diagnosis is vital to avoid health complexities,\nhowever, it depends upon the availability of costly microscopes and trained\nexperts to analyze blood-smear slides. Deep learning-based methods have the\npotential to not only decrease the burden of experts but also improve\ndiagnostic accuracy on low-cost microscopes. However, this is hampered by the\nabsence of a reasonable size dataset. One of the most challenging aspects is\nthe reluctance of the experts to annotate the dataset at low magnification on\nlow-cost microscopes. We present a dataset to further the research on malaria\nmicroscopy over the low-cost microscopes at low magnification. Our large-scale\ndataset consists of images of blood-smear slides from several malaria-infected\npatients, collected through microscopes at two different cost spectrums and\nmultiple magnifications. Malarial cells are annotated for the localization and\nlife-stage classification task on the images collected through the high-cost\nmicroscope at high magnification. We design a mechanism to transfer these\nannotations from the high-cost microscope at high magnification to the low-cost\nmicroscope, at multiple magnifications. Multiple object detectors and domain\nadaptation methods are presented as the baselines. Furthermore, a partially\nsupervised domain adaptation method is introduced to adapt the object-detector\nto work on the images collected from the low-cost microscope. The dataset will\nbe made publicly available after publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sultani_W/0/1/0/all/0/1\">Waqas Sultani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawaz_W/0/1/0/all/0/1\">Wajahat Nawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Syed Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danish_M/0/1/0/all/0/1\">Muhammad Sohail Danish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadia_A/0/1/0/all/0/1\">Asma Saadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExCon: Explanation-driven Supervised Contrastive Learning for Image Classification. (arXiv:2111.14271v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14271","description":"<p>Contrastive learning has led to substantial improvements in the quality of\nlearned embedding representations for tasks such as image classification.\nHowever, a key drawback of existing contrastive augmentation methods is that\nthey may lead to the modification of the image content which can yield\nundesired alterations of its semantics. This can affect the performance of the\nmodel on downstream tasks. Hence, in this paper, we ask whether we can augment\nimage data in contrastive learning such that the task-relevant semantic content\nof an image is preserved. For this purpose, we propose to leverage\nsaliency-based explanation methods to create content-preserving masked\naugmentations for contrastive learning. Our novel explanation-driven supervised\ncontrastive learning (ExCon) methodology critically serves the dual goals of\nencouraging nearby image embeddings to have similar content and explanation. To\nquantify the impact of ExCon, we conduct experiments on the CIFAR-100 and the\nTiny ImageNet datasets. We demonstrate that ExCon outperforms vanilla\nsupervised contrastive learning in terms of classification, explanation\nquality, adversarial robustness as well as probabilistic calibration in the\ncontext of distributional shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jongseong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trabelsi_C/0/1/0/all/0/1\">Chiheb Trabelsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yeonjeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1\">Dongsub Shim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions. (arXiv:2112.00246v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00246","description":"<p>Perceiving and interacting with 3D articulated objects, such as cabinets,\ndoors, and faucets, pose particular challenges for future home-assistant robots\nperforming daily tasks in human environments. Besides parsing the articulated\nparts and joint parameters, researchers recently advocate learning manipulation\naffordance over the input shape geometry which is more task-aware and\ngeometrically fine-grained. However, taking only passive observations as\ninputs, these methods ignore many hidden but important kinematic constraints\n(e.g., joint location and limits) and dynamic factors (e.g., joint friction and\nrestitution), therefore losing significant accuracy for test cases with such\nuncertainties. In this paper, we propose a novel framework, named AdaAfford,\nthat learns to perform very few test-time interactions for quickly adapting the\naffordance priors to more accurate instance-specific posteriors. We conduct\nlarge-scale experiments using the PartNet-Mobility dataset and prove that our\nsystem performs better than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1\">Jiaqi Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving GAN Equilibrium by Raising Spatial Awareness. (arXiv:2112.00718v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00718","description":"<p>The success of Generative Adversarial Networks (GANs) is largely built upon\nthe adversarial training between a generator (G) and a discriminator (D). They\nare expected to reach a certain equilibrium where D cannot distinguish the\ngenerated images from the real ones. However, such an equilibrium is rarely\nachieved in practical GAN training, instead, D almost always surpasses G. We\nattribute one of its sources to the information asymmetry between D and G. We\nobserve that D learns its own visual attention when determining whether an\nimage is real or fake, but G has no explicit clue on which regions to focus on\nfor a particular synthesis. To alleviate the issue of D dominating the\ncompetition in GANs, we aim to raise the spatial awareness of G. Randomly\nsampled multi-level heatmaps are encoded into the intermediate layers of G as\nan inductive bias. Thus G can purposefully improve the synthesis of certain\nimage regions. We further propose to align the spatial awareness of G with the\nattention map induced from D. Through this way we effectively lessen the\ninformation gap between D and G. Extensive results show that our method pushes\nthe two-player game in GANs closer to the equilibrium, leading to a better\nsynthesis performance. As a byproduct, the introduced spatial awareness\nfacilitates interactive editing over the output synthesis. Demo video and code\nare available at https://genforce.github.io/eqgan-sa/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Ceyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BA-Net: Bridge Attention for Deep Convolutional Neural Networks. (arXiv:2112.04150v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04150","description":"<p>In recent years, channel attention mechanism has been widely investigated due\nto its great potential in improving the performance of deep convolutional\nneural networks (CNNs) in many vision tasks. However, in most of the existing\nmethods, only the output of the adjacent convolution layer is fed into the\nattention layer for calculating the channel weights. Information from other\nconvolution layers has been ignored. With these observations, a simple\nstrategy, named Bridge Attention Net (BA-Net), is proposed in this paper for\nbetter performance with channel attention mechanisms. The core idea of this\ndesign is to bridge the outputs of the previous convolution layers through skip\nconnections for channel weights generation. Based on our experiment and theory\nanalysis, we find that features from previous layers also contribute to the\nweights significantly. The Comprehensive evaluation demonstrates that the\nproposed approach achieves state-of-the-art(SOTA) performance compared with the\nexisting methods in accuracy and speed. which shows that Bridge Attention\nprovides a new perspective on the design of neural network architectures with\ngreat potential in improving performance. The code is available at\nhttps://github.com/zhaoy376/Bridge-Attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ronghui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel multi-view deep learning approach for BI-RADS and density assessment of mammograms. (arXiv:2112.04490v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.04490","description":"<p>Advanced deep learning (DL) algorithms may predict the patient's risk of\ndeveloping breast cancer based on the Breast Imaging Reporting and Data System\n(BI-RADS) and density standards. Recent studies have suggested that the\ncombination of multi-view analysis improved the overall breast exam\nclassification. In this paper, we propose a novel multi-view DL approach for\nBI-RADS and density assessment of mammograms. The proposed approach first\ndeploys deep convolutional networks for feature extraction on each view\nseparately. The extracted features are then stacked and fed into a Light\nGradient Boosting Machine (LightGBM) classifier to predict BI-RADS and density\nscores. We conduct extensive experiments on both the internal mammography\ndataset and the public dataset Digital Database for Screening Mammography\n(DDSM). The experimental results demonstrate that the proposed approach\noutperforms the single-view classification approach on two benchmark datasets\nby huge F1-score margins (+5% on the internal dataset and +10% on the DDSM\ndataset). These results highlight the vital role of combining multi-view\ninformation to improve the performance of breast cancer risk prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Huyen T. X. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_S/0/1/0/all/0/1\">Sam B. Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_D/0/1/0/all/0/1\">Dung B. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Vision Transformers for Incremental Learning. (arXiv:2112.06103v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06103","description":"<p>This paper proposes a working recipe of using Vision Transformer (ViT) in\nclass incremental learning. Although this recipe only combines existing\ntechniques, developing the combination is not trivial. Firstly, naive\napplication of ViT to replace convolutional neural networks (CNNs) in\nincremental learning results in serious performance degradation. Secondly, we\nnail down three issues of naively using ViT: (a) ViT has very slow convergence\nwhen the number of classes is small, (b) more bias towards new classes is\nobserved in ViT than CNN-based architectures, and (c) the conventional learning\nrate of ViT is too low to learn a good classifier layer. Finally, our solution,\nnamed ViTIL (ViT for Incremental Learning) achieves new state-of-the-art on\nboth CIFAR and ImageNet datasets for all three class incremental learning\nsetups by a clear margin. We believe this advances the knowledge of transformer\nin the incremental learning community. Code will be publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Magnifying Networks for Images with Billions of Pixels. (arXiv:2112.06121v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06121","description":"<p>The shift towards end-to-end deep learning has brought unprecedented advances\nin many areas of computer vision. However, deep neural networks are trained on\nimages with resolutions that rarely exceed $1,000 \\times 1,000$ pixels. The\ngrowing use of scanners that create images with extremely high resolutions\n(average can be $100,000 \\times 100,000$ pixels) thereby presents novel\nchallenges to the field. Most of the published methods preprocess\nhigh-resolution images into a set of smaller patches, imposing an a priori\nbelief on the best properties of the extracted patches (magnification, field of\nview, location, etc.). Herein, we introduce Magnifying Networks (MagNets) as an\nalternative deep learning solution for gigapixel image analysis that does not\nrely on a preprocessing stage nor requires the processing of billions of\npixels. MagNets can learn to dynamically retrieve any part of a gigapixel\nimage, at any magnification level and field of view, in an end-to-end fashion\nwith minimal ground truth (a single global, slide-level label). Our results on\nthe publicly available Camelyon16 and Camelyon17 datasets corroborate to the\neffectiveness and efficiency of MagNets and the proposed optimization framework\nfor whole slide image classification. Importantly, MagNets process far less\npatches from each slide than any of the existing approaches ($10$ to $300$\ntimes less).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimitriou_N/0/1/0/all/0/1\">Neofytos Dimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVIP: Sequence VerIfication for Procedures in Videos. (arXiv:2112.06447v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06447","description":"<p>In this paper, we propose a novel sequence verification task that aims to\ndistinguish positive video pairs performing the same action sequence from\nnegative ones with step-level transformations but still conducting the same\ntask. Such a challenging task resides in an open-set setting without prior\naction detection or segmentation that requires event-level or even frame-level\nannotations. To that end, we carefully reorganize two publicly available\naction-related datasets with step-procedure-task structure. To fully\ninvestigate the effectiveness of any method, we collect a scripted video\ndataset enumerating all kinds of step-level transformations in chemical\nexperiments. Besides, a novel evaluation metric Weighted Distance Ratio is\nintroduced to ensure equivalence for different step-level transformations\nduring evaluation. In the end, a simple but effective baseline based on the\ntransformer encoder with a novel sequence alignment loss is introduced to\nbetter characterize long-term dependency between steps, which outperforms other\naction recognition methods. Codes and data will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yicheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weixin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Dongze Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Value Retrieval with Arbitrary Queries for Form-like Documents. (arXiv:2112.07820v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07820","description":"<p>We propose value retrieval with arbitrary queries for form-like documents to\nreduce human effort of processing forms. Unlike previous methods that only\naddress a fixed set of field items, our method predicts target value for an\narbitrary query based on the understanding of the layout and semantics of a\nform. To further boost model performance, we propose a simple document language\nmodeling (SimpleDLM) strategy to improve document understanding on large-scale\nmodel pre-training. Experimental results show that our method outperforms\nprevious designs significantly and the SimpleDLM further improves our\nperformance on value retrieval by around 17% F1 score compared with the\nstate-of-the-art pre-training method. Code is available at\nhttps://github.com/salesforce/QVR-SimpleDLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Le Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaiah_C/0/1/0/all/0/1\">Chetan Ramaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPTS: Single-Point Text Spotting. (arXiv:2112.07917v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07917","description":"<p>Existing scene text spotting (i.e., end-to-end text detection and\nrecognition) methods rely on costly bounding box annotations (e.g., text-line,\nword-level, or character-level bounding boxes). For the first time, we\ndemonstrate that training scene text spotting models can be achieved with an\nextremely low-cost annotation of a single-point for each instance. We propose\nan end-to-end scene text spotting method that tackles scene text spotting as a\nsequence prediction task. Given an image as input, we formulate the desired\ndetection and recognition results as a sequence of discrete tokens and use an\nauto-regressive Transformer to predict the sequence. The proposed method is\nsimple yet effective, which can achieve state-of-the-art results on widely used\nbenchmarks. Most significantly, we show that the performance is not very\nsensitive to the positions of the point annotation, meaning that it can be much\neasier to be annotated or even be automatically generated than the bounding box\nthat requires precise positions. We believe that such a pioneer attempt\nindicates a significant opportunity for scene text spotting applications of a\nmuch larger scale than previously possible. The code will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Model Pseudo-Labeling for Semi-Supervised Action Recognition. (arXiv:2112.09690v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09690","description":"<p>Semi-supervised action recognition is a challenging but important task due to\nthe high cost of data annotation. A common approach to this problem is to\nassign unlabeled data with pseudo-labels, which are then used as additional\nsupervision in training. Typically in recent work, the pseudo-labels are\nobtained by training a model on the labeled data, and then using confident\npredictions from the model to teach itself. In this work, we propose a more\neffective pseudo-labeling scheme, called Cross-Model Pseudo-Labeling (CMPL).\nConcretely, we introduce a lightweight auxiliary network in addition to the\nprimary backbone, and ask them to predict pseudo-labels for each other. We\nobserve that, due to their different structural biases, these two models tend\nto learn complementary representations from the same video clips. Each model\ncan thus benefit from its counterpart by utilizing cross-model predictions as\nsupervision. Experiments on different data partition protocols demonstrate the\nsignificant improvement of our framework over existing alternatives. For\nexample, CMPL achieves $17.6\\%$ and $25.1\\%$ Top-1 accuracy on Kinetics-400 and\nUCF-101 using only the RGB modality and $1\\%$ labeled data, outperforming our\nbaseline model, FixMatch, by $9.0\\%$ and $10.3\\%$, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Ceyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distill and De-bias: Mitigating Bias in Face Verification using Knowledge Distillation. (arXiv:2112.09786v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09786","description":"<p>Face recognition networks generally demonstrate bias with respect to\nsensitive attributes like gender, skintone etc. For gender and skintone, we\nobserve that the regions of the face that a network attends to vary by the\ncategory of an attribute. This might contribute to bias. Building on this\nintuition, we propose a novel distillation-based approach called Distill and\nDe-bias (D&amp;D) to enforce a network to attend to similar face regions,\nirrespective of the attribute category. In D&amp;D, we train a teacher network on\nimages from one category of an attribute; e.g. light skintone. Then distilling\ninformation from the teacher, we train a student network on images of the\nremaining category; e.g., dark skintone. A feature-level distillation loss\nconstrains the student network to generate teacher-like representations. This\nallows the student network to attend to similar face regions for all attribute\ncategories and enables it to reduce bias. We also propose a second distillation\nstep on top of D&amp;D, called D&amp;D++. Here, we distill the `un-biasedness' of the\nD&amp;D network into a new student network, the D&amp;D++ network, while training this\nnew network on all attribute categories; e.g., both light and dark skintones.\nThis helps us train a network that is less biased for an attribute, while\nobtaining higher face verification performance than D&amp;D. We show that D&amp;D++\noutperforms existing baselines in reducing gender and skintone bias on the\nIJB-C dataset, while obtaining higher face verification performance than\nexisting adversarial de-biasing methods. We evaluate the effectiveness of our\nproposed methods on two state-of-the-art face recognition networks: ArcFace and\nCrystalface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_P/0/1/0/all/0/1\">Prithviraj Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Joshua Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Aniket Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_P/0/1/0/all/0/1\">P. Jonathon Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-aware Image Synthesis via Learning Structural and Textural Representations. (arXiv:2112.10759v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10759","description":"<p>Making generative models 3D-aware bridges the 2D image space and the 3D\nphysical world yet remains challenging. Recent attempts equip a Generative\nAdversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D\ncoordinates to pixel values, as a 3D prior. However, the implicit function in\nNeRF has a very local receptive field, making the generator hard to become\naware of the global structure. Meanwhile, NeRF is built on volume rendering\nwhich can be too costly to produce high-resolution results, increasing the\noptimization difficulty. To alleviate these two problems, we propose a novel\nframework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis,\nthrough explicitly learning a structural representation and a textural\nrepresentation. We first learn a feature volume to represent the underlying\nstructure, which is then converted to a feature field using a NeRF-like model.\nThe feature field is further accumulated into a 2D feature map as the textural\nrepresentation, followed by a neural renderer for appearance synthesis. Such a\ndesign enables independent control of the shape and the appearance. Extensive\nexperiments on a wide range of datasets show that our approach achieves\nsufficiently higher image quality and better 3D control than the previous\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Ceyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape from Polarization for Complex Scenes in the Wild. (arXiv:2112.11377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11377","description":"<p>We present a new data-driven approach with physics-based priors to\nscene-level normal estimation from a single polarization image. Existing shape\nfrom polarization (SfP) works mainly focus on estimating the normal of a single\nobject rather than complex scenes in the wild. A key barrier to high-quality\nscene-level SfP is the lack of real-world SfP data in complex scenes. Hence, we\ncontribute the first real-world scene-level SfP dataset with paired input\npolarization images and ground-truth normal maps. Then we propose a\nlearning-based framework with a multi-head self-attention module and viewing\nencoding, which is designed to handle increasing polarization ambiguities\ncaused by complex materials and non-orthographic projection in scene-level SfP.\nOur trained model can be generalized to far-field outdoor scenes as the\nrelationship between polarized light and surface normals is not affected by\ndistance. Experimental results demonstrate that our approach significantly\noutperforms existing SfP models on two datasets. Our dataset and source code\nwill be publicly available at https://github.com/ChenyangLEI/sfp-wild\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chenyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BDG-Net: Boundary Distribution Guided Network for Accurate Polyp Segmentation. (arXiv:2201.00767v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.00767","description":"<p>Colorectal cancer (CRC) is one of the most common fatal cancer in the world.\nPolypectomy can effectively interrupt the progression of adenoma to\nadenocarcinoma, thus reducing the risk of CRC development. Colonoscopy is the\nprimary method to find colonic polyps. However, due to the different sizes of\npolyps and the unclear boundary between polyps and their surrounding mucosa, it\nis challenging to segment polyps accurately. To address this problem, we design\na Boundary Distribution Guided Network (BDG-Net) for accurate polyp\nsegmentation. Specifically, under the supervision of the ideal Boundary\nDistribution Map (BDM), we use Boundary Distribution Generate Module (BDGM) to\naggregate high-level features and generate BDM. Then, BDM is sent to the\nBoundary Distribution Guided Decoder (BDGD) as complementary spatial\ninformation to guide the polyp segmentation. Moreover, a multi-scale feature\ninteraction strategy is adopted in BDGD to improve the segmentation accuracy of\npolyps with different sizes. Extensive quantitative and qualitative evaluations\ndemonstrate the effectiveness of our model, which outperforms state-of-the-art\nmodels remarkably on five public polyp datasets while maintaining low\ncomputational complexity. Code: https://github.com/zihuanqiu/BDG-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zihuan Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhichuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jie Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Linfeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval. (arXiv:2201.02772v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02772","description":"<p>Cross-Modal Retrieval (CMR) is an important research topic across multimodal\ncomputing and information retrieval, which takes one type of data as the query\nto retrieve relevant data of another type. It has been widely used in many\nreal-world applications. Recently, the vision-language pre-trained models\nrepresented by CLIP demonstrate its superiority in learning the visual and\ntextual representations and gain impressive performance on various vision and\nlanguage related tasks. Although CLIP as well as the previous pre-trained\nmodels have shown great performance improvement in the unsupervised CMR, the\nperformance and impact of these pre-trained models on the supervised CMR were\nrarely explored due to the lack of common representation for the multimodal\nclass-level associations. In this paper, we take CLIP as the current\nrepresentative vision-language pre-trained model to conduct a comprehensive\nempirical study. We evaluate its performance and impact on the supervised CMR,\nand attempt to answer several key research questions. To this end, we first\npropose a novel model CLIP4CMR (CLIP enhanced network for Cross-Modal\nRetrieval) that employs the pre-trained CLIP as backbone network to perform the\nsupervised CMR. Then by means of the CLIP4CMR framework, we revisit the design\nof different learning objectives in current CMR methods to provide new insights\non model design. Moreover, we investigate the most concerned aspects in\napplying CMR, including the robustness to modality imbalance and sensitivity to\nhyper-parameters, to provide new perspectives for practical applications.\nThrough extensive experiments, we show that CLIP4CMR achieves the SOTA results\nwith prominent improvements on the benchmark datasets, and can be used as a\nfundamental framework to empirically study the key research issues of the\nsupervised CMR, with significant implications for model design and practical\nconsiderations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhixiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenji Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore and Match: A New Paradigm for Temporal Video Grounding with Natural Language. (arXiv:2201.10168v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10168","description":"<p>Temporal Video Grounding (TVG) aims to localize time segments in an untrimmed\nvideo according to natural language queries. In this work, we present a new\nparadigm named Explore-and-Match for TVG that seamlessly unifies two streams of\nTVG methods: proposal-free and proposal-based; the former explores the search\nspace to find segments directly, and the latter matches the predefined\nproposals with ground truths. To achieve this goal, we view TVG as a set\nprediction problem and design an end-to-end trainable Language Video\nTransformer (LVTR) that utilizes the architectural strengths of rich\ncontextualization and parallel decoding for set prediction. The overall\ntraining schedule is balanced by two key losses that play different roles,\nnamely temporal localization loss and set guidance loss. These two losses allow\neach proposal to regress the target segment and identify the target query. More\nspecifically, LVTR first explores the search space to diversify the initial\nproposals, and then matches the proposals to the corresponding targets to align\nthem in a fine-grained manner. The Explore-and-Match scheme successfully\ncombines the strengths of two complementary methods without encoding prior\nknowledge (e.g., non-maximum suppression) into the TVG pipeline. As a result,\nLVTR sets new state-of-the-art results on two TVG benchmarks (ActivityCaptions\nand Charades-STA) with double the inference speed. Code is available at\nhttps://github.com/sangminwoo/Explore-and-Match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_I/0/1/0/all/0/1\">Inyong Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sumin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minki Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossRectify: Leveraging Disagreement for Semi-supervised Object Detection. (arXiv:2201.10734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10734","description":"<p>Semi-supervised object detection has recently achieved substantial progress.\nAs a mainstream solution, the self-labeling-based methods train the detector on\nboth labeled data and unlabeled data with pseudo labels predicted by the\ndetector itself, but their performances are always limited. Through\nexperimental analysis, we reveal the underlying reason is that the detector is\nmisguided by the incorrect pseudo labels predicted by itself (dubbed\nself-errors). These self-errors can hurt performance even worse than\nrandom-errors, and can be neither discerned nor rectified during the\nself-labeling process. In this paper, we propose an effective detection\nframework named CrossRectify, to obtain accurate pseudo labels by\nsimultaneously training two detectors with different initial parameters.\nSpecifically, the proposed approach leverages the disagreements between\ndetectors to discern the self-errors and refines the pseudo label quality by\nthe proposed cross-rectifying mechanism. Extensive experiments show that\nCrossRectify achieves outperforming performances over various detector\nstructures on 2D and 3D detection benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chengcheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Compressible and Learnable Image Transformation with Secret Key and Its Applications. (arXiv:2201.11006v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11006","description":"<p>This article presents an overview of image transformation with a secret key\nand its applications. Image transformation with a secret key enables us not\nonly to protect visual information on plain images but also to embed unique\nfeatures controlled with a key into images. In addition, numerous encryption\nmethods can generate encrypted images that are compressible and learnable for\nmachine learning. Various applications of such transformation have been\ndeveloped by using these properties. In this paper, we focus on a class of\nimage transformation referred to as learnable image encryption, which is\napplicable to privacy-preserving machine learning and adversarially robust\ndefense. Detailed descriptions of both transformation algorithms and\nperformances are provided. Moreover, we discuss robustness against various\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MaungMaung_A/0/1/0/all/0/1\">AprilPyone MaungMaung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1\">Yuma Kinoshita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imaizumi_S/0/1/0/all/0/1\">Shoko Imaizumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1\">Sayaka Shiota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11114","description":"<p>Some neurons in deep networks specialize in recognizing highly specific\nperceptual, structural, or semantic features of inputs. In computer vision,\ntechniques exist for identifying neurons that respond to individual concept\ncategories like colors, textures, and object classes. But these techniques are\nlimited in scope, labeling only a small subset of neurons and behaviors in any\nnetwork. Is a richer characterization of neuron-level computation possible? We\nintroduce a procedure (called MILAN, for mutual-information-guided linguistic\nannotation of neurons) that automatically labels neurons with open-ended,\ncompositional, natural language descriptions. Given a neuron, MILAN generates a\ndescription by searching for a natural language string that maximizes pointwise\nmutual information with the image regions in which the neuron is active. MILAN\nproduces fine-grained descriptions that capture categorical, relational, and\nlogical structure in learned features. These descriptions obtain high agreement\nwith human-generated feature descriptions across a diverse set of model\narchitectures and tasks, and can aid in understanding and controlling learned\nmodels. We highlight three applications of natural language neuron\ndescriptions. First, we use MILAN for analysis, characterizing the distribution\nand importance of neurons selective for attribute, category, and relational\ninformation in vision models. Second, we use MILAN for auditing, surfacing\nneurons sensitive to human faces in datasets designed to obscure them. Finally,\nwe use MILAN for editing, improving robustness in an image classifier by\ndeleting neurons sensitive to text features spuriously correlated with class\nlabels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_E/0/1/0/all/0/1\">Evan Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1\">Sarah Schwettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagashvili_T/0/1/0/all/0/1\">Teona Bagashvili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Feature based Cross-slide Registration. (arXiv:2202.09971v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09971","description":"<p>Cross-slide image analysis provides additional information by analysing the\nexpression of different biomarkers as compared to a single slide analysis.\nThese biomarker stained slides are analysed side by side, revealing unknown\nrelations between them. During the slide preparation, a tissue section may be\nplaced at an arbitrary orientation as compared to other sections of the same\ntissue block. The problem is compounded by the fact that tissue contents are\nlikely to change from one section to the next and there may be unique artefacts\non some of the slides. This makes registration of each section to a reference\nsection of the same tissue block an important pre-requisite task before any\ncross-slide analysis. We propose a deep feature based registration (DFBR)\nmethod which utilises data-driven features to estimate the rigid\ntransformation. We adopted a multi-stage strategy for improving the quality of\nregistration. We also developed a visualisation tool to view registered pairs\nof WSIs at different magnifications. With the help of this tool, one can apply\na transformation on the fly without the need to generate transformed source WSI\nin a pyramidal form. We compared the performance of data-driven features with\nthat of hand-crafted features on the COMET dataset. Our approach can align the\nimages with low registration errors. Generally, the success of non-rigid\nregistration is dependent on the quality of rigid registration. To evaluate the\nefficacy of the DFBR method, the first two steps of the ANHIR winner's\nframework are replaced with our DFBR to register challenge provided image\npairs. The modified framework produces comparable results to that of challenge\nwinning team.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awan_R/0/1/0/all/0/1\">Ruqayya Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_N/0/1/0/all/0/1\">Nick Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning. (arXiv:2202.12588v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12588","description":"<p>The expensive annotation cost is notoriously known as the main constraint for\nthe development of the point cloud semantic segmentation technique. Active\nlearning methods endeavor to reduce such cost by selecting and labeling only a\nsubset of the point clouds, yet previous attempts ignore the spatial-structural\ndiversity of the selected samples, inducing the model to select clustered\ncandidates with similar shapes in a local area while missing other\nrepresentative ones in the global environment. In this paper, we propose a new\n3D region-based active learning method to tackle this problem. Dubbed SSDR-AL,\nour method groups the original point clouds into superpoints and incrementally\nselects the most informative and representative ones for label acquisition. We\nachieve the selection mechanism via a graph reasoning network that considers\nboth the spatial and structural diversities of superpoints. To deploy SSDR-AL\nin a more practical scenario, we design a noise-aware iterative labeling\nstrategy to confront the \"noisy annotation\" problem introduced by the previous\n\"dominant labeling\" strategy in superpoints. Extensive experiments on two point\ncloud benchmarks demonstrate the effectiveness of SSDR-AL in the semantic\nsegmentation task. Particularly, SSDR-AL significantly outperforms the baseline\nmethod and reduces the annotation cost by up to 63.0% and 24.0% when achieving\n90% performance of fully supervised learning, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1\">Feifei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yulei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGMR-Net: Attention Guided Multiscale Recovery framework for stroke segmentation. (arXiv:2202.13687v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13687","description":"<p>Automatic and accurate lesion segmentation is critical for clinically\nestimating the lesion statuses of stroke diseases and developing appropriate\ndiagnostic systems. Although existing methods have achieved remarkable results,\nfurther adoption of the models is hindered by: (1) inter-class indistinction,\nthe normal brain tissue resembles the lesion in appearance. (2) intra-class\ninconsistency, large variability exists between different areas of the lesion.\nTo solve these challenges in stroke segmentation, we propose a novel method,\nnamely Attention Guided Multiscale Recovery framework (AGMR-Net) in this paper.\nFirstly, a coarse-grained patch attention module in the encoding is adopted to\nget a patch-based coarse-grained attention map in a multi-stage explicitly\nsupervised way, enabling target spatial context saliency representation with a\npatch-based weighting technique that eliminates the effect of intra-class\ninconsistency. Secondly, to obtain a more detailed boundary partitioning to\nsolve the challenge of the inter-class indistinction, a newly designed\ncross-dimensional feature fusion module is used to capture global contextual\ninformation to further guide the selective aggregation of 2D and 3D features,\nwhich can compensate for the lack of boundary learning capability of 2D\nconvolution. Lastly, in the decoding stage, an innovative designed multi-scale\ndeconvolution upsampling instead of linear interpolation enhances the recovery\nof target space and boundary information. The AGMR-Net is evaluated on the open\ndataset Anatomical Tracings of Lesions-After-Stroke (ATLAS), achieving the\nhighest dice similarity coefficient (DSC) score of 0.594, Hausdorff distance of\n27.005 mm, and average symmetry surface distance of 7.137 mm, which demonstrate\nthat our proposed method outperforms other state-of-the-art methods and has\ngreat potential in the diagnosis of stroke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiuquan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kunpeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuhui Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers. (arXiv:2203.00156v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.00156","description":"<p>As technology advances, the need for safe, efficient, and collaborative\nhuman-robot-teams has become increasingly important. One of the most\nfundamental collaborative tasks in any setting is the object handover.\nHuman-to-robot handovers can take either of two approaches: (1) direct\nhand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach\nensures minimal contact between the human and robot but can also result in\nincreased idle time due to having to wait for the object to first be placed\ndown on a surface. To minimize such idle time, the robot must preemptively\npredict the human intent of where the object will be placed. Furthermore, for\nthe robot to preemptively act in any sort of productive manner, predictions and\nmotion planning must occur in real-time. We introduce a novel\nprediction-planning pipeline that allows the robot to preemptively move towards\nthe human agent's intended placement location using gaze and gestures as model\ninputs. In this paper, we investigate the performance and drawbacks of our\nearly intent predictor-planner as well as the practical benefits of using such\na pipeline through a human-robot case study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_A/0/1/0/all/0/1\">Andrew Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawed_M/0/1/0/all/0/1\">Mohammad Khalid Jawed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Semantic and Instance Segmentation for Colon Nuclei Identification and Counting. (arXiv:2203.00157v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00157","description":"<p>We address the problem of automated nuclear segmentation, classification, and\nquantification from Haematoxylin and Eosin stained histology images, which is\nof great relevance for several downstream computational pathology applications.\nIn this work, we present a solution framed as a simultaneous semantic and\ninstance segmentation framework. Our solution is part of the Colon Nuclei\nIdentification and Counting (CoNIC) Challenge. We first train a semantic and\ninstance segmentation model separately. Our framework uses as backbone HoverNet\nand Cascade Mask-RCNN models. We then ensemble the results with a custom\nNon-Maximum Suppression embedding (NMS). In our framework, the semantic model\ncomputes a class prediction for the cells whilst the instance model provides a\nrefined segmentation. We demonstrate, through our experimental results, that\nour model outperforms the provided baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Chenyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I. Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning based Prediction of MSI using MMR Markers in Colorectal Cancer. (arXiv:2203.00449v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2203.00449","description":"<p>The accurate diagnosis and molecular profiling of colorectal cancers are\ncritical for planning the best treatment options for patients. Microsatellite\ninstability (MSI) or mismatch repair (MMR) status plays a vital role\ninappropriate treatment selection, has prognostic implications and is used to\ninvestigate the possibility of patients having underlying genetic disorders\n(Lynch syndrome). NICE recommends that all CRC patients should be offered\nMMR/microsatellite instability (MSI) testing. Immunohistochemistry is commonly\nused to assess MMR status with subsequent molecular testing performed as\nrequired. This incurs significant extra costs and requires additional\nresources. The introduction of automated methods that can predict MSI or MMR\nstatus from a target image could substantially reduce the cost associated with\nMMR testing. Unlike previous studies on MSI prediction involving training a CNN\nusing coarse labels (Microsatellite Instable vs Microsatellite Stable), we have\nutilised fine-grain MMR labels for training purposes. In this paper, we present\nour work on predicting MSI status in a two-stage process using a single target\nslide either stained with CK8/18 or H\\&amp;E. First, we trained a multi-headed\nconvolutional neural network model where each head was responsible for\npredicting one of the MMR protein expressions. To this end, we performed the\nregistration of MMR stained slides to the target slide as a pre-processing\nstep. In the second stage, statistical features computed from the MMR\nprediction maps were used for the final MSI prediction. Our results\ndemonstrated that MSI classification can be improved by incorporating\nfine-grained MMR labels in comparison to the previous approaches in which only\ncoarse labels were utilised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Awan_R/0/1/0/all/0/1\">Ruqayya Awan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nimir_M/0/1/0/all/0/1\">Mohammed Nimir</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bilal_M/0/1/0/all/0/1\">Mohsin Bilal</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_A/0/1/0/all/0/1\">Andrew Robinson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification. (arXiv:2203.04961v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04961","description":"<p>Early detection of breast cancer in mammography screening via deep-learning\nbased computer-aided detection systems shows promising potential in improving\nthe curability and mortality rates of breast cancer. However, many clinical\ncentres are restricted in the amount and heterogeneity of available data to\ntrain such models to (i) achieve promising performance and to (ii) generalise\nwell across acquisition protocols and domains. As sharing data between centres\nis restricted due to patient privacy concerns, we propose a potential solution:\nsharing trained generative models between centres as substitute for real\npatient data. In this work, we use three well known mammography datasets to\nsimulate three different centres, where one centre receives the trained\ngenerator of Generative Adversarial Networks (GANs) from the two remaining\ncentres in order to augment the size and heterogeneity of its training dataset.\nWe evaluate the utility of this approach on mammography patch classification on\nthe test set of the GAN-receiving centre using two different classification\nmodels, (a) a convolutional neural network and (b) a transformer neural\nnetwork. Our experiments demonstrate that shared GANs notably increase the\nperformance of both transformer and convolutional classification models and\nhighlight this approach as a viable alternative to inter-centre data sharing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breier_B/0/1/0/all/0/1\">Bennet Breier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PseudoProp: Robust Pseudo-Label Generation for Semi-Supervised Object Detection in Autonomous Driving Systems. (arXiv:2203.05983v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05983","description":"<p>Semi-supervised object detection methods are widely used in autonomous\ndriving systems, where only a fraction of objects are labeled. To propagate\ninformation from the labeled objects to the unlabeled ones, pseudo-labels for\nunlabeled objects must be generated. Although pseudo-labels have proven to\nimprove the performance of semi-supervised object detection significantly, the\napplications of image-based methods to video frames result in numerous miss or\nfalse detections using such generated pseudo-labels. In this paper, we propose\na new approach, PseudoProp, to generate robust pseudo-labels by leveraging\nmotion continuity in video frames. Specifically, PseudoProp uses a novel\nbidirectional pseudo-label propagation approach to compensate for misdetection.\nA feature-based fusion technique is also used to suppress inference noise.\nExtensive experiments on the large-scale Cityscapes dataset demonstrate that\nour method outperforms the state-of-the-art semi-supervised object detection\nmethods by 7.4% on mAP75.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chun-Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_J/0/1/0/all/0/1\">Jayanta Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naveen Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision and Deep Learning for Fish Classification in Underwater Habitats: A Survey. (arXiv:2203.06951v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06951","description":"<p>Marine scientists use remote underwater video recording to survey fish\nspecies in their natural habitats. This helps them understand and predict how\nfish respond to climate change, habitat degradation, and fishing pressure. This\ninformation is essential for developing sustainable fisheries for human\nconsumption, and for preserving the environment. However, the enormous volume\nof collected videos makes extracting useful information a daunting and\ntime-consuming task for a human. A promising method to address this problem is\nthe cutting-edge Deep Learning (DL) technology.DL can help marine scientists\nparse large volumes of video promptly and efficiently, unlocking niche\ninformation that cannot be obtained using conventional manual monitoring\nmethods. In this paper, we provide an overview of the key concepts of DL, while\npresenting a survey of literature on fish habitat monitoring with a focus on\nunderwater fish classification. We also discuss the main challenges faced when\ndeveloping DL for underwater image processing and propose approaches to address\nthem. Finally, we provide insights into the marine habitat monitoring research\ndomain and shed light on what the future of DL for underwater image processing\nmay hold. This paper aims to inform a wide range of readers from marine\nscientists who would like to apply DL in their research to computer scientists\nwho would like to survey state-of-the-art DL-based underwater fish habitat\nmonitoring literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_A/0/1/0/all/0/1\">Alzayat Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheaves_M/0/1/0/all/0/1\">Marcus Sheaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1\">Mostafa Rahimi Azghadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Unsupervised Hashing with Latent Semantic Components. (arXiv:2203.09420v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09420","description":"<p>Deep unsupervised hashing has been appreciated in the regime of image\nretrieval. However, most prior arts failed to detect the semantic components\nand their relationships behind the images, which makes them lack discriminative\npower. To make up the defect, we propose a novel Deep Semantic Components\nHashing (DSCH), which involves a common sense that an image normally contains a\nbunch of semantic components with homology and co-occurrence relationships.\nBased on this prior, DSCH regards the semantic components as latent variables\nunder the Expectation-Maximization framework and designs a two-step iterative\nalgorithm with the objective of maximum likelihood of training data. Firstly,\nDSCH constructs a semantic component structure by uncovering the fine-grained\nsemantics components of images with a Gaussian Mixture Modal~(GMM), where an\nimage is represented as a mixture of multiple components, and the semantics\nco-occurrence are exploited. Besides, coarse-grained semantics components, are\ndiscovered by considering the homology relationships between fine-grained\ncomponents, and the hierarchy organization is then constructed. Secondly, DSCH\nmakes the images close to their semantic component centers at both fine-grained\nand coarse-grained levels, and also makes the images share similar semantic\ncomponents close to each other. Extensive experiments on three benchmark\ndatasets demonstrate that the proposed hierarchical semantic components indeed\nfacilitate the hashing model to achieve superior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinghong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaotian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CM-GAN: Image Inpainting with Cascaded Modulation GAN and Object-Aware Training. (arXiv:2203.11947v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11947","description":"<p>Recent image inpainting methods have made great progress but often struggle\nto generate plausible image structures when dealing with large holes in complex\nimages. This is partially due to the lack of effective network structures that\ncan capture both the long-range dependency and high-level semantics of an\nimage. To address these problems, we propose cascaded modulation GAN (CM-GAN),\na new network design consisting of an encoder with Fourier convolution blocks\nthat extract multi-scale feature representations from the input image with\nholes and a StyleGAN-like decoder with a novel cascaded global-spatial\nmodulation block at each scale level. In each decoder block, global modulation\nis first applied to perform coarse semantic-aware structure synthesis, then\nspatial modulation is applied on the output of global modulation to further\nadjust the feature map in a spatially adaptive fashion. In addition, we design\nan object-aware training scheme to prevent the network from hallucinating new\nobjects inside holes, fulfilling the needs of object removal tasks in\nreal-world scenarios. Extensive experiments are conducted to show that our\nmethod significantly outperforms existing methods in both quantitative and\nqualitative evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_C/0/1/0/all/0/1\">Connelly Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirghodsi_S/0/1/0/all/0/1\">Sohrab Amirghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lymphocyte Classification in Hyperspectral Images of Ovarian Cancer Tissue Biopsy Samples. (arXiv:2203.12112v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.12112","description":"<p>Current methods for diagnosing the progression of multiple types of cancer\nwithin patients rely on interpreting stained needle biopsies. This process is\ntime-consuming and susceptible to error throughout the paraffinization,\nHematoxylin and Eosin (H&amp;E) staining, deparaffinization, and annotation stages.\nFourier Transform Infrared (FTIR) imaging has been shown to be a promising\nalternative to staining for appropriately annotating biopsy cores without the\nneed for deparaffinization or H&amp;E staining with the use of Fourier Transform\nInfrared (FTIR) images when combined with machine learning to interpret the\ndense spectral information. We present a machine learning pipeline to segment\nwhite blood cell (lymphocyte) pixels in hyperspectral images of biopsy cores.\nThese cells are clinically important for diagnosis, but some prior work has\nstruggled to incorporate them due to difficulty obtaining precise pixel labels.\nEvaluated methods include Support Vector Machine (SVM), Gaussian Naive Bayes,\nand Multilayer Perceptron (MLP), as well as analyzing the comparatively modern\nconvolutional neural network (CNN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paulson_B/0/1/0/all/0/1\">Benjamin Paulson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colwell_T/0/1/0/all/0/1\">Theodore Colwell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bukowski_N/0/1/0/all/0/1\">Natalia Bukowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weller_J/0/1/0/all/0/1\">Joseph Weller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crisler_A/0/1/0/all/0/1\">Andrew Crisler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cisler_J/0/1/0/all/0/1\">John Cisler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drobek_A/0/1/0/all/0/1\">Alexander Drobek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neuwirth_A/0/1/0/all/0/1\">Alexander Neuwirth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Multimodal Information Fusion for Facial Expression Analysis. (arXiv:2203.12367v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12367","description":"<p>Human affective behavior analysis has received much attention in\nhuman-computer interaction (HCI). In this paper, we introduce our submission to\nthe CVPR 2022 Competition on Affective Behavior Analysis in-the-wild (ABAW). To\nfully exploit affective knowledge from multiple views, we utilize the\nmultimodal features of spoken words, speech prosody, and facial expression,\nwhich are extracted from the video clips in the Aff-Wild2 dataset. Based on\nthese features, we propose a unified transformer-based multimodal framework for\nAction Unit detection and also expression recognition. Specifically, the static\nvision feature is first encoded from the current frame image. At the same time,\nwe clip its adjacent frames by a sliding window and extract three kinds of\nmultimodal features from the sequence of images, audio, and text. Then, we\nintroduce a transformer-based fusion module that integrates the static vision\nfeatures and the dynamic multimodal features. The cross-attention module in the\nfusion module makes the output integrated features focus on the crucial parts\nthat facilitate the downstream detection tasks. We also leverage some data\nbalancing techniques, data augmentation techniques, and postprocessing methods\nto further improve the model performance. In the official test of ABAW3\nCompetition, our model ranks first in the EXPR and AU tracks. The extensive\nquantitative evaluations, as well as ablation studies on the Aff-Wild2 dataset,\nprove the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_F/0/1/0/all/0/1\">Feng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_R/0/1/0/all/0/1\">Rudong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bowen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation. (arXiv:2203.13254v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13254","description":"<p>Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is\na long-standing problem in computer vision. Driven by end-to-end deep learning,\nrecent studies suggest interpreting PnP as a differentiable layer, so that\n2D-3D point correspondences can be partly learned by backpropagating the\ngradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D\npoints from scratch fails to converge with existing approaches, since the\ndeterministic pose is inherently non-differentiable. In this paper, we propose\nthe EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation,\nwhich outputs a distribution of pose on the SE(3) manifold, essentially\nbringing categorical Softmax to the continuous domain. The 2D-3D coordinates\nand corresponding weights are treated as intermediate variables learned by\nminimizing the KL divergence between the predicted and target pose\ndistribution. The underlying principle unifies the existing approaches and\nresembles the attention mechanism. EPro-PnP significantly outperforms\ncompetitive baselines, closing the gap between PnP-based method and the\ntask-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D\nobject detection benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hansheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1\">Wei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Lu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FocalClick: Towards Practical Interactive Image Segmentation. (arXiv:2204.02574v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02574","description":"<p>Interactive segmentation allows users to extract target masks by making\npositive/negative clicks. Although explored by many previous works, there is\nstill a gap between academic approaches and industrial needs: first, existing\nmodels are not efficient enough to work on low power devices; second, they\nperform poorly when used to refine preexisting masks as they could not avoid\ndestroying the correct part. FocalClick solves both issues at once by\npredicting and updating the mask in localized areas. For higher efficiency, we\ndecompose the slow prediction on the entire image into two fast inferences on\nsmall crops: a coarse segmentation on the Target Crop, and a local refinement\non the Focus Crop. To make the model work with preexisting masks, we formulate\na sub-task termed Interactive Mask Correction, and propose Progressive Merge as\nthe solution. Progressive Merge exploits morphological information to decide\nwhere to preserve and where to update, enabling users to refine any preexisting\nmask effectively. FocalClick achieves competitive results against SOTA methods\nwith significantly smaller FLOPs. It also shows significant superiority when\nmaking corrections on preexisting masks. Code and data will be released at\ngithub.com/XavierCHEN34/ClickSEG\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yilei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1\">Manni Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Donglian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks. (arXiv:2204.02824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02824","description":"<p>Face inpainting aims to complete the corrupted regions of the face images,\nwhich requires coordination between the completed areas and the non-corrupted\nareas. Recently, memory-oriented methods illustrate great prospects in the\ngeneration related tasks by introducing an external memory module to improve\nimage coordination. However, such methods still have limitations in restoring\nthe consistency and continuity for specificfacial semantic parts. In this\npaper, we propose the coarse-to-fine Memory-Disentangled Refinement Networks\n(MDRNets) for coordinated face inpainting, in which two collaborative modules\nare integrated, Disentangled Memory Module (DMM) and Mask-Region Enhanced\nModule (MREM). Specifically, the DMM establishes a group of disentangled memory\nblocks to store the semantic-decoupled face representations, which could\nprovide the most relevant information to refine the semantic-level\ncoordination. The MREM involves a masked correlation mining mechanism to\nenhance the feature relationships into the corrupted regions, which could also\nmake up for the correlation loss caused by memory disentanglement. Furthermore,\nto better improve the inter-coordination between the corrupted and\nnon-corrupted regions and enhance the intra-coordination in corrupted regions,\nwe design InCo2 Loss, a pair of similarity based losses to constrain the\nfeature consistency. Eventually, extensive experiments conducted on CelebA-HQ\nand FFHQ datasets demonstrate the superiority of our MDRNets compared with\nprevious State-Of-The-Art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuojie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Dose CT Denoising via Sinogram Inner-Structure Transformer. (arXiv:2204.03163v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.03163","description":"<p>Low-Dose Computed Tomography (LDCT) technique, which reduces the radiation\nharm to human bodies, is now attracting increasing interest in the medical\nimaging field. As the image quality is degraded by low dose radiation, LDCT\nexams require specialized reconstruction methods or denoising algorithms.\nHowever, most of the recent effective methods overlook the inner-structure of\nthe original projection data (sinogram) which limits their denoising ability.\nThe inner-structure of the sinogram represents special characteristics of the\ndata in the sinogram domain. By maintaining this structure while denoising, the\nnoise can be obviously restrained. Therefore, we propose an LDCT denoising\nnetwork namely Sinogram Inner-Structure Transformer (SIST) to reduce the noise\nby utilizing the inner-structure in the sinogram domain. Specifically, we study\nthe CT imaging mechanism and statistical characteristics of sinogram to design\nthe sinogram inner-structure loss including the global and local\ninner-structure for restoring high-quality CT images. Besides, we propose a\nsinogram transformer module to better extract sinogram features. The\ntransformer architecture using a self-attention mechanism can exploit\ninterrelations between projections of different view angles, which achieves an\noutstanding performance in sinogram denoising. Furthermore, in order to improve\nthe performance in the image domain, we propose the image reconstruction module\nto complementarily denoise both in the sinogram and image domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Liutao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhongnian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_R/0/1/0/all/0/1\">Rongjun Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1\">Junyong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Si_H/0/1/0/all/0/1\">Haipeng Si</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Daoqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Transformer. A sparse-aware solution for efficient event data processing. (arXiv:2204.03355v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03355","description":"<p>Event cameras are sensors of great interest for many applications that run in\nlow-resource and challenging environments. They log sparse illumination changes\nwith high temporal resolution and high dynamic range, while they present\nminimal power consumption. However, top-performing methods often ignore\nspecific event-data properties, leading to the development of generic but\ncomputationally expensive algorithms. Efforts toward efficient solutions\nusually do not achieve top-accuracy results for complex tasks. This work\nproposes a novel framework, Event Transformer (EvT), that effectively takes\nadvantage of event-data properties to be highly efficient and accurate. We\nintroduce a new patch-based event representation and a compact transformer-like\narchitecture to process it. EvT is evaluated on different event-based\nbenchmarks for action and gesture recognition. Evaluation results show better\nor comparable accuracy to the state-of-the-art while requiring significantly\nless computation resources, which makes EvT able to work with minimal latency\nboth on GPU and CPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Trajectory-Aware Transformer for Video Super-Resolution. (arXiv:2204.04216v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.04216","description":"<p>Video super-resolution (VSR) aims to restore a sequence of high-resolution\n(HR) frames from their low-resolution (LR) counterparts. Although some progress\nhas been made, there are grand challenges to effectively utilize temporal\ndependency in entire video sequences. Existing approaches usually align and\naggregate video frames from limited adjacent frames (e.g., 5 or 7 frames),\nwhich prevents these approaches from satisfactory results. In this paper, we\ntake one step further to enable effective spatio-temporal learning in videos.\nWe propose a novel Trajectory-aware Transformer for Video Super-Resolution\n(TTVSR). In particular, we formulate video frames into several pre-aligned\ntrajectories which consist of continuous visual tokens. For a query token,\nself-attention is only learned on relevant visual tokens along spatio-temporal\ntrajectories. Compared with vanilla vision Transformers, such a design\nsignificantly reduces the computational cost and enables Transformers to model\nlong-range features. We further propose a cross-scale feature tokenization\nmodule to overcome scale-changing problems that often occur in long-range\nvideos. Experimental results demonstrate the superiority of the proposed TTVSR\nover state-of-the-art models, by extensive quantitative and qualitative\nevaluations in four widely-used video super-resolution benchmarks. Both code\nand pre-trained models can be downloaded at\nhttps://github.com/researchmm/TTVSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chengxu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OutfitTransformer: Learning Outfit Representations for Fashion Recommendation. (arXiv:2204.04812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04812","description":"<p>Learning an effective outfit-level representation is critical for predicting\nthe compatibility of items in an outfit, and retrieving complementary items for\na partial outfit. We present a framework, OutfitTransformer, that uses the\nproposed task-specific tokens and leverages the self-attention mechanism to\nlearn effective outfit-level representations encoding the compatibility\nrelationships between all items in the entire outfit for addressing both\ncompatibility prediction and complementary item retrieval tasks. For\ncompatibility prediction, we design an outfit token to capture a global outfit\nrepresentation and train the framework using a classification loss. For\ncomplementary item retrieval, we design a target item token that additionally\ntakes the target item specification (in the form of a category or text\ndescription) into consideration. We train our framework using a proposed\nset-wise outfit ranking loss to generate a target item embedding given an\noutfit, and a target item specification as inputs. The generated target item\nembedding is then used to retrieve compatible items that match the rest of the\noutfit. Additionally, we adopt a pre-training approach and a curriculum\nlearning strategy to improve retrieval performance. Since our framework learns\nat an outfit-level, it allows us to learn a single embedding capturing\nhigher-order relations among multiple items in the outfit more effectively than\npairwise methods. Experiments demonstrate that our approach outperforms\nstate-of-the-art methods on compatibility prediction, fill-in-the-blank, and\ncomplementary item retrieval tasks. We further validate the quality of our\nretrieval results with a user study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1\">Rohan Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodla_N/0/1/0/all/0/1\">Navaneeth Bodla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasileva_M/0/1/0/all/0/1\">Mariya I. Vasileva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beniwal_A/0/1/0/all/0/1\">Anurag Beniwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Alan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1\">Gerard Medioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Machine Learning Model Evaluation in Pathology. (arXiv:2204.05205v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.05205","description":"<p>Machine Learning has been applied to pathology images in research and\nclinical practice with promising outcomes. However, standard ML models often\nlack the rigorous evaluation required for clinical decisions. Machine learning\ntechniques for natural images are ill-equipped to deal with pathology images\nthat are significantly large and noisy, require expensive labeling, are hard to\ninterpret, and are susceptible to spurious correlations. We propose a set of\npractical guidelines for ML evaluation in pathology that address the above\nconcerns. The paper includes measures for setting up the evaluation framework,\neffectively dealing with variability in labels, and a recommended suite of\ntests to address issues related to domain shift, robustness, and confounding\nvariables. We hope that the proposed framework will bridge the gap between ML\nresearchers and domain experts, leading to wider adoption of ML techniques in\npathology and improving patient outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Javed_S/0/1/0/all/0/1\">Syed Ashar Javed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Juyal_D/0/1/0/all/0/1\">Dinkar Juyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shanis_Z/0/1/0/all/0/1\">Zahil Shanis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakraborty_S/0/1/0/all/0/1\">Shreya Chakraborty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pokkalla_H/0/1/0/all/0/1\">Harsha Pokkalla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prakash_A/0/1/0/all/0/1\">Aaditya Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"medXGAN: Visual Explanations for Medical Classifiers through a Generative Latent Space. (arXiv:2204.05376v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05376","description":"<p>Despite the surge of deep learning in the past decade, some users are\nskeptical to deploy these models in practice due to their black-box nature.\nSpecifically, in the medical space where there are severe potential\nrepercussions, we need to develop methods to gain confidence in the models'\ndecisions. To this end, we propose a novel medical imaging generative\nadversarial framework, medXGAN (medical eXplanation GAN), to visually explain\nwhat a medical classifier focuses on in its binary predictions. By encoding\ndomain knowledge of medical images, we are able to disentangle anatomical\nstructure and pathology, leading to fine-grained visualization through latent\ninterpolation. Furthermore, we optimize the latent space such that\ninterpolation explains how the features contribute to the classifier's output.\nOur method outperforms baselines such as Gradient-Weighted Class Activation\nMapping (Grad-CAM) and Integrated Gradients in localization and explanatory\nability. Additionally, a combination of the medXGAN with Integrated Gradients\ncan yield explanations more robust to noise. The code is available at:\nhttps://avdravid.github.io/medXGAN_page/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dravid_A/0/1/0/all/0/1\">Amil Dravid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiffers_F/0/1/0/all/0/1\">Florian Schiffers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpoofGAN: Synthetic Fingerprint Spoof Images. (arXiv:2204.06498v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06498","description":"<p>A major limitation to advances in fingerprint spoof detection is the lack of\npublicly available, large-scale fingerprint spoof datasets, a problem which has\nbeen compounded by increased concerns surrounding privacy and security of\nbiometric data. Furthermore, most state-of-the-art spoof detection algorithms\nrely on deep networks which perform best in the presence of a large amount of\ntraining data. This work aims to demonstrate the utility of synthetic (both\nlive and spoof) fingerprints in supplying these algorithms with sufficient data\nto improve the performance of fingerprint spoof detection algorithms beyond the\ncapabilities when training on a limited amount of publicly available real\ndatasets. First, we provide details of our approach in modifying a\nstate-of-the-art generative architecture to synthesize high quality live and\nspoof fingerprints. Then, we provide quantitative and qualitative analysis to\nverify the quality of our synthetic fingerprints in mimicking the distribution\nof real data samples. We showcase the utility of our synthetic live and spoof\nfingerprints in training a deep network for fingerprint spoof detection, which\ndramatically boosts the performance across three different evaluation datasets\ncompared to an identical model trained on real data alone. Finally, we\ndemonstrate that only 25% of the original (real) dataset is required to obtain\nsimilar detection performance when augmenting the training dataset with\nsynthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grosz_S/0/1/0/all/0/1\">Steven A. Grosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. (arXiv:2204.07439v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07439","description":"<p>Binary Neural Networks (BNNs) have emerged as a promising solution for\nreducing the memory footprint and compute costs of deep neural networks. BNNs,\non the other hand, suffer from information loss because binary activations are\nlimited to only two values, resulting in reduced accuracy. To improve the\naccuracy, previous studies have attempted to control the distribution of binary\nactivation by manually shifting the threshold of the activation function or\nmaking the shift amount trainable. During the process, they usually depended on\nstatistical information computed from a batch. We argue that using statistical\ndata from a batch fails to capture the crucial information for each input\ninstance in BNN computations, and the differences between statistical\ninformation computed from each instance need to be considered when determining\nthe binary activation threshold of each instance. Based on the concept, we\npropose the Binary Neural Network with INSTAnce-aware threshold (INSTA-BNN),\nwhich decides the activation threshold value considering the difference between\nstatistical data computed from a batch and each instance. The proposed\nINSTA-BNN outperforms the baseline by 2.5% and 2.3% on the ImageNet\nclassification task with comparable computing cost, achieving 68.0% and 71.7%\ntop-1 accuracy on ResNet-18 and MobileNetV1 based models, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Changhun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyungjun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunhyeok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jae-Joon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}