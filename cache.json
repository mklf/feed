{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Likelihood Ratio based Domain Adaptation Method for E2E Models. (arXiv:2201.03655v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03655","description":"<p>End-to-end (E2E) automatic speech recognition models like Recurrent Neural\nNetworks Transducer (RNN-T) are becoming a popular choice for streaming ASR\napplications like voice assistants. While E2E models are very effective at\nlearning representation of the training data they are trained on, their\naccuracy on unseen domains remains a challenging problem. Additionally, these\nmodels require paired audio and text training data, are computationally\nexpensive and are difficult to adapt towards the fast evolving nature of\nconversational speech. In this work, we explore a contextual biasing approach\nusing likelihood-ratio that leverages text data sources to adapt RNN-T model to\nnew domains and entities. We show that this method is effective in improving\nrare words recognition, and results in a relative improvement of 10% in 1-best\nword error rate (WER) and 10% in n-best Oracle WER (n=8) on multiple\nout-of-domain datasets without any degradation on a general dataset. We also\nshow that complementing the contextual biasing adaptation with adaptation of a\nsecond-pass rescoring model gives additive WER improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_C/0/1/0/all/0/1\">Chhavi Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Agnostic Website Embedding and Classification. (arXiv:2201.03677v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03677","description":"<p>Currently, publicly available models for website classification do not offer\nan embedding method and have limited support for languages beyond English. We\nrelease a dataset with more than 1M websites in 92 languages with relative\nlabels collected from Curlie, the largest multilingual crowdsourced Web\ndirectory. The dataset contains 14 website categories aligned across languages.\nAlongside it, we introduce Homepage2Vec, a machine-learned pre-trained model\nfor classifying and embedding websites based on their homepage in a\nlanguage-agnostic way. Homepage2Vec, thanks to its feature set (textual\ncontent, metadata tags, and visual attributes) and recent progress in natural\nlanguage representation, is language-independent by design and can generate\nembeddings representation. We show that Homepage2Vec correctly classifies\nwebsites with a macro-averaged F1-score of 0.90, with stable performance across\nlow- as well as high-resource languages. Feature analysis shows that a small\nsubset of efficiently computable features suffices to achieve high performance\neven with limited computational resources. We make publicly available the\ncurated Curlie dataset aligned across languages, the pre-trained Homepage2Vec\nmodel, and libraries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugeon_S/0/1/0/all/0/1\">Sylvain Lugeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccardi_T/0/1/0/all/0/1\">Tiziano Piccardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informal Persian Universal Dependency Treebank. (arXiv:2201.03679v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03679","description":"<p>This paper presents the phonological, morphological, and syntactic\ndistinctions between formal and informal Persian, showing that these two\nvariants have fundamental differences that cannot be attributed solely to\npronunciation discrepancies. Given that informal Persian exhibits particular\ncharacteristics, any computational model trained on formal Persian is unlikely\nto transfer well to informal Persian, necessitating the creation of dedicated\ntreebanks for this variety. We thus detail the development of the open-source\nInformal Persian Universal Dependency Treebank, a new treebank annotated within\nthe Universal Dependencies scheme. We then investigate the parsing of informal\nPersian by training two dependency parsers on existing formal treebanks and\nevaluating them on out-of-domain data, i.e. the development set of our informal\ntreebank. Our results show that parsers experience a substantial performance\ndrop when we move across the two domains, as they face more unknown tokens and\nstructures and fail to generalize well. Furthermore, the dependency relations\nwhose performance deteriorates the most represent the unique properties of the\ninformal variant. The ultimate goal of this study that demonstrates a broader\nimpact is to provide a stepping-stone to reveal the significance of informal\nvariants of languages, which have been widely overlooked in natural language\nprocessing tools across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabiri_R/0/1/0/all/0/1\">Roya Kabiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_S/0/1/0/all/0/1\">Simin Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. (arXiv:2201.03713v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03713","description":"<p>We introduce CVSS, a massively multilingual-to-English speech-to-speech\ntranslation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21\nlanguages into English. CVSS is derived from the Common Voice speech corpus and\nthe CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the\ntranslation text from CoVoST 2 into speech using state-of-the-art TTS systems.\nTwo versions of translation speeches are provided: 1) CVSS-C: All the\ntranslation speeches are in a single high-quality canonical voice; 2) CVSS-T:\nThe translation speeches are in voices transferred from the corresponding\nsource speeches. In addition, CVSS provides normalized translation text which\nmatches the pronunciation in the translation speech. On each version of CVSS,\nwe built baseline multilingual direct S2ST models and cascade S2ST models,\nverifying the effectiveness of the corpus. To build strong cascade S2ST\nbaselines, we trained an ST model on CoVoST 2, which outperforms the previous\nstate-of-the-art trained on the corpus without extra data by 5.8 BLEU.\nNevertheless, the performance of the direct S2ST models approaches the strong\ncascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU\ndifference on ASR transcribed translation when initialized from matching ST\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Prediction Uncertainty of Pre-trained Language Models by Detecting Uncertain Words in Inputs. (arXiv:2201.03742v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03742","description":"<p>Estimating the predictive uncertainty of pre-trained language models is\nimportant for increasing their trustworthiness in NLP. Although many previous\nworks focus on quantifying prediction uncertainty, there is little work on\nexplaining the uncertainty. This paper pushes a step further on explaining\nuncertain predictions of post-calibrated pre-trained language models. We adapt\ntwo perturbation-based post-hoc interpretation methods, Leave-one-out and\nSampling Shapley, to identify words in inputs that cause the uncertainty in\npredictions. We test the proposed methods on BERT and RoBERTa with three tasks:\nsentiment classification, natural language inference, and paraphrase\nidentification, in both in-domain and out-of-domain settings. Experiments show\nthat both methods consistently capture words in inputs that cause prediction\nuncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prior Knowledge Enhances Radiology Report Generation. (arXiv:2201.03761v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03761","description":"<p>Radiology report generation aims to produce computer-aided diagnoses to\nalleviate the workload of radiologists and has drawn increasing attention\nrecently. However, previous deep learning methods tend to neglect the mutual\ninfluences between medical findings, which can be the bottleneck that limits\nthe quality of generated reports. In this work, we propose to mine and\nrepresent the associations among medical findings in an informative knowledge\ngraph and incorporate this prior knowledge with radiology report generation to\nhelp improve the quality of generated reports. Experiment results demonstrate\nthe superior performance of our proposed method on the IU X-ray dataset with a\nROUGE-L of 0.384$\\pm$0.007 and CIDEr of 0.340$\\pm$0.011. Compared with previous\nworks, our model achieves an average of 1.6% improvement (2.0% and 1.5%\nimprovements in CIDEr and ROUGE-L, respectively). The experiments suggest that\nprior knowledge can bring performance gains to accurate radiology report\ngeneration. We will make the code publicly available at\nhttps://github.com/bionlplab/report_generation_amia2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingquan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_G/0/1/0/all/0/1\">George Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition. (arXiv:2201.03804v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03804","description":"<p>With the rise of deep learning and intelligent vehicle, the smart assistant\nhas become an essential in-car component to facilitate driving and provide\nextra functionalities. In-car smart assistants should be able to process\ngeneral as well as car-related commands and perform corresponding actions,\nwhich eases driving and improves safety. However, there is a data scarcity\nissue for low resource languages, hindering the development of research and\napplications. In this paper, we introduce a new dataset, Cantonese In-car\nAudio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in\nthe Cantonese language with both video and audio data. It consists of 4,984\nsamples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese\nspeakers. Furthermore, we augment our dataset using common in-car background\nnoises to simulate real environments, producing a dataset 10 times larger than\nthe collected one. We provide detailed statistics of both the clean and the\naugmented versions of our dataset. Moreover, we implement two multimodal\nbaselines to demonstrate the validity of CI-AVSR. Experiment results show that\nleveraging the visual signal improves the overall performance of the model.\nAlthough our best model can achieve a considerable quality on the clean test\nset, the speech recognition quality on the noisy data is still inferior and\nremains as an extremely challenging task for real in-car speech recognition\nsystems. The dataset and code will be released at\nhttps://github.com/HLTCHKUST/CI-AVSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_C/0/1/0/all/0/1\">Cheuk Tung Shadow Yiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Robustness to Adversarial Word Substitutions. (arXiv:2201.03829v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03829","description":"<p>Deep-learning-based NLP models are found to be vulnerable to word\nsubstitution perturbations. Before they are widely adopted, the fundamental\nissues of robustness need to be addressed. Along this line, we propose a formal\nframework to evaluate word-level robustness. First, to study safe regions for a\nmodel, we introduce robustness radius which is the boundary where the model can\nresist any perturbation. As calculating the maximum robustness radius is\ncomputationally hard, we estimate its upper and lower bound. We repurpose\nattack methods as ways of seeking upper bound and design a pseudo-dynamic\nprogramming algorithm for a tighter upper bound. Then verification method is\nutilized for a lower bound. Further, for evaluating the robustness of regions\noutside a safe radius, we reexamine robustness from another view:\nquantification. A robustness metric with a rigorous statistical guarantee is\nintroduced to measure the quantification of adversarial examples, which\nindicates the model's susceptibility to perturbations outside the safe radius.\nThe metric helps us figure out why state-of-the-art models like BERT can be\neasily fooled by a few word substitutions, but generalize well in the presence\nof real-world noises.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">FeiFei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turkish Sentiment Analysis Using Machine Learning Methods: Application on Online Food Order Site Reviews. (arXiv:2201.03848v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03848","description":"<p>Satisfaction measurement, which emerges in every sector today, is a very\nimportant factor for many companies. In this study, it is aimed to reach the\nhighest accuracy rate with various machine learning algorithms by using the\ndata on Yemek Sepeti and variations of this data. The accuracy values of each\nalgorithm were calculated together with the various natural language processing\nmethods used. While calculating these accuracy values, the parameters of the\nalgorithms used were tried to be optimized. The models trained in this study on\nlabeled data can be used on unlabeled data and can give companies an idea in\nmeasuring customer satisfaction. It was observed that 3 different natural\nlanguage processing methods applied resulted in approximately 5% accuracy\nincrease in most of the developed models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aktas_O/0/1/0/all/0/1\">&#xd6;zlem Akta&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coskuner_B/0/1/0/all/0/1\">Berkay Co&#x15f;kuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soner_I/0/1/0/all/0/1\">&#x130;lker Soner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild. (arXiv:2201.03857v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03857","description":"<p>This paper presents a new training dataset for automatic genre identification\nGINCO, which is based on 1,125 crawled Slovenian web documents that consist of\n650 thousand words. Each document was manually annotated for genre with a new\nannotation schema that builds upon existing schemata, having primarily clarity\nof labels and inter-annotator agreement in mind. The dataset consists of\nvarious challenges related to web-based data, such as machine translated\ncontent, encoding errors, multiple contents presented in one document etc.,\nenabling evaluation of classifiers in realistic conditions. The initial machine\nlearning experiments on the dataset show that (1) pre-Transformer models are\ndrastically less able to model the phenomena, with macro F1 metrics ranging\naround 0.22, while Transformer-based models achieve scores of around 0.58, and\n(2) multilingual Transformer models work as well on the task as the monolingual\nmodels that were previously proven to be superior to multilingual models on\nstandard NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuzman_T/0/1/0/all/0/1\">Taja Kuzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupnik_P/0/1/0/all/0/1\">Peter Rupnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1\">Nikola Ljube&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis with Deep Learning Models: A Comparative Study on a Decade of Sinhala Language Facebook Data. (arXiv:2201.03941v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03941","description":"<p>The relationship between Facebook posts and the corresponding reaction\nfeature is an interesting subject to explore and understand. To archive this\nend, we test state-of-the-art Sinhala sentiment analysis models against a data\nset containing a decade worth of Sinhala posts with millions of reactions. For\nthe purpose of establishing benchmarks and with the goal of identifying the\nbest model for Sinhala sentiment analysis, we also test, on the same data set\nconfiguration, other deep learning models catered for sentiment analysis. In\nthis study we report that the 3 layer Bidirectional LSTM model achieves an F1\nscore of 84.58% for Sinhala sentiment analysis, surpassing the current\nstate-of-the-art model; Capsule B, which only manages to get an F1 score of\n82.04%. Further, since all the deep learning models show F1 scores above 75% we\nconclude that it is safe to claim that Facebook reactions are suitable to\npredict the sentiment of a text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weeraprameshwara_G/0/1/0/all/0/1\">Gihan Weeraprameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayawickrama_V/0/1/0/all/0/1\">Vihanga Jayawickrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijeratne_Y/0/1/0/all/0/1\">Yudhanjaya Wijeratne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Intensity and its Control for Emotional Voice Conversion. (arXiv:2201.03967v1 [cs.SD])","link":"http://arxiv.org/abs/2201.03967","description":"<p>Emotional voice conversion (EVC) seeks to convert the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In EVC,\nemotions are usually treated as discrete categories overlooking the fact that\nspeech also conveys emotions with various intensity levels that the listener\ncan perceive. In this paper, we aim to explicitly characterize and control the\nintensity of emotion. We propose to disentangle the speaker style from\nlinguistic content and encode the speaker style into a style embedding in a\ncontinuous space that forms the prototype of emotion embedding. We further\nlearn the actual emotion encoder from an emotion-labelled database and study\nthe use of relative attributes to represent fine-grained emotion intensity. To\nensure emotional intelligibility, we incorporate emotion classification loss\nand emotion embedding similarity loss into the training of the EVC network. As\ndesired, the proposed network controls the fine-grained emotion intensity in\nthe output speech. Through both objective and subjective evaluations, we\nvalidate the effectiveness of the proposed network for emotional expressiveness\nand emotion intensity control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rajib Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v1 [cs.LG])","link":"http://arxiv.org/abs/2201.03969","description":"<p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem\ndue to the heterogeneity gap between different modalities and the ambiguity of\nhuman emotional expression. Although there have been many successful attempts\nto construct multimodal representations for MSA, there are still two challenges\nto be addressed: 1) A more robust multimodal representation needs to be\nconstructed to bridge the heterogeneity gap and cope with the complex\nmultimodal interactions, and 2) the contextual dynamics must be modeled\neffectively throughout the information flow. In this work, we propose a\nmultimodal representation model based on Mutual information Maximization and\nMinimization and Identity Embedding (MMMIE). We combine mutual information\nmaximization between modal pairs, and mutual information minimization between\ninput data and corresponding features to mine the modal-invariant and\ntask-related information. Furthermore, Identity Embedding is proposed to prompt\nthe downstream network to perceive the contextual information. Experimental\nresults on two public datasets demonstrate the effectiveness of the proposed\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiahao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training. (arXiv:2201.04026v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04026","description":"<p>Vision-language pre-training has been an emerging and fast-developing\nresearch topic, which transfers multi-modal knowledge from rich-resource\npre-training task to limited-resource downstream tasks. Unlike existing works\nthat predominantly learn a single generic encoder, we present a pre-trainable\nUniversal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language\nperception (e.g., visual question answering) and generation (e.g., image\ncaptioning). Uni-EDEN is a two-stream Transformer based structure, consisting\nof three modules: object and sentence encoders that separately learns the\nrepresentations of each modality, and sentence decoder that enables both\nmulti-modal reasoning and sentence generation via inter-modal interaction.\nConsidering that the linguistic representations of each image can span\ndifferent granularities in this hierarchy including, from simple to\ncomprehensive, individual label, a phrase, and a natural sentence, we pre-train\nUni-EDEN through multi-granular vision-language proxy tasks: Masked Object\nClassification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence\nMatching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is\nendowed with the power of both multi-modal representation extraction and\nlanguage modeling. Extensive experiments demonstrate the compelling\ngeneralizability of Uni-EDEN by fine-tuning it to four vision-language\nperception and generation downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiahao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"rVAD: An Unsupervised Segment-Based Robust Voice Activity Detection Method. (arXiv:1906.03588v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/1906.03588","description":"<p>This paper presents an unsupervised segment-based method for robust voice\nactivity detection (rVAD). The method consists of two passes of denoising\nfollowed by a voice activity detection (VAD) stage. In the first pass,\nhigh-energy segments in a speech signal are detected by using a posteriori\nsignal-to-noise ratio (SNR) weighted energy difference and if no pitch is\ndetected within a segment, the segment is considered as a high-energy noise\nsegment and set to zero. In the second pass, the speech signal is denoised by a\nspeech enhancement method, for which several methods are explored. Next,\nneighbouring frames with pitch are grouped together to form pitch segments, and\nbased on speech statistics, the pitch segments are further extended from both\nends in order to include both voiced and unvoiced sounds and likely non-speech\nparts as well. In the end, a posteriori SNR weighted energy difference is\napplied to the extended pitch segments of the denoised speech signal for\ndetecting voice activity. We evaluate the VAD performance of the proposed\nmethod using two databases, RATS and Aurora-2, which contain a large variety of\nnoise conditions. The rVAD method is further evaluated, in terms of speaker\nverification performance, on the RedDots 2016 challenge database and its\nnoise-corrupted versions. Experiment results show that rVAD is compared\nfavourably with a number of existing methods. In addition, we present a\nmodified version of rVAD where computationally intensive pitch extraction is\nreplaced by computationally efficient spectral flatness calculation. The\nmodified version significantly reduces the computational complexity at the cost\nof moderately inferior VAD performance, which is an advantage when processing a\nlarge amount of data and running on low resource devices. The source code of\nrVAD is made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zheng-Hua Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Achintya kr. Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DensE: An Enhanced Non-commutative Representation for Knowledge Graph Embedding with Adaptive Semantic Hierarchy. (arXiv:2008.04548v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2008.04548","description":"<p>Capturing the composition patterns of relations is a vital task in knowledge\ngraph completion. It also serves as a fundamental step towards multi-hop\nreasoning over learned knowledge. Previously, several rotation-based\ntranslational methods have been developed to model composite relations using\nthe product of a series of complex-valued diagonal matrices. However, these\nmethods tend to make several oversimplified assumptions on the composite\nrelations, e.g., forcing them to be commutative, independent from entities and\nlacking semantic hierarchy. To systematically tackle these problems, we have\ndeveloped a novel knowledge graph embedding method, named DensE, to provide an\nimproved modeling scheme for the complex composition patterns of relations. In\nparticular, our method decomposes each relation into an SO(3) group-based\nrotation operator and a scaling operator in the three dimensional (3-D)\nEuclidean space. This design principle leads to several advantages of our\nmethod: (1) For composite relations, the corresponding diagonal relation\nmatrices can be non-commutative, reflecting a predominant scenario in real\nworld applications; (2) Our model preserves the natural interaction between\nrelational operations and entity embeddings; (3) The scaling operation provides\nthe modeling power for the intrinsic semantic hierarchical structure of\nentities; (4) The enhanced expressiveness of DensE is achieved with high\ncomputational efficiency in terms of both parameter size and training time; and\n(5) Modeling entities in Euclidean space instead of quaternion space keeps the\ndirect geometrical interpretations of relational patterns. Experimental results\non multiple benchmark knowledge graphs show that DensE outperforms the current\nstate-of-the-art models for missing link prediction, especially on composite\nrelations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haonan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hailin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaodong Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularization for Long Named Entity Recognition. (arXiv:2104.07249v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07249","description":"<p>When performing named entity recognition (NER), entity length is variable and\ndependent on a specific domain or dataset. Pre-trained language models (PLMs)\nare used to solve NER tasks and tend to be biased toward dataset patterns such\nas length statistics, surface form, and skewed class distribution. These biases\nhinder the generalization ability of PLMs, which is necessary to address many\nunseen mentions in real-world situations. We propose a novel debiasing method\nRegLER to improve predictions for entities of varying lengths. To close the gap\nbetween evaluation and real-world situations, we evaluated PLMs on partitioned\nbenchmark datasets containing unseen mention sets. Here, RegLER shows\nsignificant improvement over long-named entities that can predict through\ndebiasing on conjunction or special characters within entities. Furthermore,\nthere is a severe class imbalance in most NER datasets, causing easy-negative\nexamples to dominate during training, such as \"The\". Our approach alleviates\nskewed class distribution by reducing the influence of easy-negative examples.\nExtensive experiments on the biomedical and general domains demonstrated the\ngeneralization capabilities of our method. To facilitate reproducibility and\nfuture work, we release our code.\"https://github.com/minstar/RegLER\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minbyul Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational AI Systems for Social Good: Opportunities and Challenges. (arXiv:2105.06457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06457","description":"<p>Conversational artificial intelligence (ConvAI) systems have attracted much\nacademic and commercial attention recently, making significant progress on both\nfronts. However, little existing work discusses how these systems can be\ndeveloped and deployed for social good in real-world applications, with\ncomprehensive case studies and analyses of pros and cons. In this paper, we\nbriefly review the progress the community has made towards better ConvAI\nsystems and reflect on how existing technologies can help advance social good\ninitiatives from various angles that are unique for ConvAI, or not yet become\ncommon knowledge in the community. We further discuss about the challenges\nahead for ConvAI systems to better help us achieve these goals and highlight\nthe risks involved in their development and deployment in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransAug: Translate as Augmentation for Sentence Embeddings. (arXiv:2111.00157v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00157","description":"<p>While contrastive learning greatly advances the representation of sentence\nembeddings, it is still limited by the size of the existing sentence datasets.\nIn this paper, we present TransAug (Translate as Augmentation), which provide\nthe first exploration of utilizing translated sentence pairs as data\naugmentation for text, and introduce a two-stage paradigm to advances the\nstate-of-the-art sentence embeddings. Instead of adopting an encoder trained in\nother languages setting, we first distill a Chinese encoder from a SimCSE\nencoder (pretrained in English), so that their embeddings are close in semantic\nspace, which can be regraded as implicit data augmentation. Then, we only\nupdate the English encoder via cross-lingual contrastive learning and frozen\nthe distilled Chinese encoder. Our approach achieves a new state-of-art on\nstandard semantic textual similarity (STS), outperforming both SimCSE and\nSentence-T5, and the best performance in corresponding tracks on transfer tasks\nevaluated by SentEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaochen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information. (arXiv:2111.04022v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.04022","description":"<p>We study the problem of weakly supervised text classification, which aims to\nclassify text documents into a set of pre-defined categories with category\nsurface names only and without any annotated training document provided. Most\nexisting classifiers leverage textual information in each document. However, in\nmany domains, documents are accompanied by various types of metadata (e.g.,\nauthors, venue, and year of a research paper). These metadata and their\ncombinations may serve as strong category indicators in addition to textual\ncontents. In this paper, we explore the potential of using metadata to help\nweakly supervised text classification. To be specific, we model the\nrelationships between documents and metadata via a heterogeneous information\nnetwork. To effectively capture higher-order structures in the network, we use\nmotifs to describe metadata combinations. We propose a novel framework, named\nMotifClass, which (1) selects category-indicative motif instances, (2)\nretrieves and generates pseudo-labeled training samples based on category names\nand indicative motif instances, and (3) trains a text classifier using the\npseudo training data. Extensive experiments on real-world datasets demonstrate\nthe superior performance of MotifClass to existing weakly supervised text\nclassification approaches. Further analysis shows the benefit of considering\nhigher-order metadata information in our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shweta Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiusi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Randomized Smoothing for Adversarially Robust Speech Recognition. (arXiv:2112.03000v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03000","description":"<p>While Automatic Speech Recognition has been shown to be vulnerable to\nadversarial attacks, defenses against these attacks are still lagging.\nExisting, naive defenses can be partially broken with an adaptive attack. In\nclassification tasks, the Randomized Smoothing paradigm has been shown to be\neffective at defending models. However, it is difficult to apply this paradigm\nto ASR tasks, due to their complexity and the sequential nature of their\noutputs. Our paper overcomes some of these challenges by leveraging\nspeech-specific tools like enhancement and ROVER voting to design an ASR model\nthat is robust to perturbations. We apply adaptive versions of state-of-the-art\nattacks, such as the Imperceptible ASR attack, to our model, and show that our\nstrongest defense is robust to all attacks that use inaudible noise, and can\nonly be broken with very high distortion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olivier_R/0/1/0/all/0/1\">Raphael Olivier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving language models by retrieving from trillions of tokens. (arXiv:2112.04426v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04426","description":"<p>We enhance auto-regressive language models by conditioning on document chunks\nretrieved from a large corpus, based on local similarity with preceding tokens.\nWith a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)\nobtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite\nusing 25$\\times$ fewer parameters. After fine-tuning, RETRO performance\ntranslates to downstream knowledge-intensive tasks such as question answering.\nRETRO combines a frozen Bert retriever, a differentiable encoder and a chunked\ncross-attention mechanism to predict tokens based on an order of magnitude more\ndata than what is typically consumed during training. We typically train RETRO\nfrom scratch, yet can also rapidly RETROfit pre-trained transformers with\nretrieval and still achieve good performance. Our work opens up new avenues for\nimproving language models through explicit memory at unprecedented scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessche_G/0/1/0/all/0/1\">George van den Driessche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lespiau_J/0/1/0/all/0/1\">Jean-Baptiste Lespiau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damoc_B/0/1/0/all/0/1\">Bogdan Damoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_A/0/1/0/all/0/1\">Aidan Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Diego de Las Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_A/0/1/0/all/0/1\">Aurelia Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ring_R/0/1/0/all/0/1\">Roman Ring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Saffron Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggiore_L/0/1/0/all/0/1\">Loren Maggiore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Chris Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassirer_A/0/1/0/all/0/1\">Albin Cassirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andy Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paganini_M/0/1/0/all/0/1\">Michela Paganini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1\">Jack W. Rae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The King is Naked: on the Notion of Robustness for Natural Language Processing. (arXiv:2112.07605v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07605","description":"<p>There is growing evidence that the classical notion of adversarial robustness\noriginally introduced for images has been adopted as a de facto standard by a\nlarge part of the NLP research community. We show that this notion is\nproblematic in the context of NLP as it considers a narrow spectrum of\nlinguistic phenomena. In this paper, we argue for semantic robustness, which is\nbetter aligned with the human concept of linguistic fidelity. We characterize\nsemantic robustness in terms of biases that it is expected to induce in a\nmodel. We study semantic robustness of a range of vanilla and robustly trained\narchitectures using a template-based generative test bed. We complement the\nanalysis with empirical evidence that, despite being harder to implement,\nsemantic robustness can improve performance %gives guarantees for on complex\nlinguistic phenomena where models robust in the classical sense fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1\">Marta Kwiatkowska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Romanian BERTs Using Multiple Teachers. (arXiv:2112.12650v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.12650","description":"<p>Running large-scale pre-trained language models in computationally\nconstrained environments remains a challenging problem yet to be addressed,\nwhile transfer learning from these models has become prevalent in Natural\nLanguage Processing tasks. Several solutions, including knowledge distillation,\nnetwork quantization, or network pruning have been previously proposed;\nhowever, these approaches focus mostly on the English language, thus widening\nthe gap when considering low-resource languages. In this work, we introduce\nthree light and fast versions of distilled BERT models for the Romanian\nlanguage: Distil-BERT-base-ro, Distil-RoBERT-base, and\nDistilMulti-BERT-base-ro. The first two models resulted from the individual\ndistillation of knowledge from two base versions of Romanian BERTs available in\nliterature, while the last one was obtained by distilling their ensemble. To\nour knowledge, this is the first attempt to create publicly available Romanian\ndistilled BERT models, which were thoroughly evaluated on five tasks:\npart-of-speech tagging, named entity recognition, sentiment analysis, semantic\ntextual similarity, and dialect identification. Our experimental results argue\nthat the three distilled models maintain most performance in terms of accuracy\nwith their teachers, while being twice as fast on a GPU and ~35% smaller. In\naddition, we further test the similarity between the predictions of our\nstudents versus their teachers by measuring their label and probability\nloyalty, together with regression loyalty - a new metric introduced in this\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catrina_D/0/1/0/all/0/1\">Darius Catrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1\">Dumitru-Clementin Cercel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dascalu_M/0/1/0/all/0/1\">Mihai Dasc&#x103;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebedea_T/0/1/0/all/0/1\">Traian Rebedea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile P&#x103;i&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufi&#x15f;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Latent Structures in Natural Language Processing: A Survey. (arXiv:2201.00490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.00490","description":"<p>While end-to-end learning with fully differentiable models has enabled\ntremendous success in natural language process (NLP) and machine learning,\nthere have been significant recent interests in learning with latent discrete\nstructures to incorporate better inductive biases for improved end-task\nperformance and better interpretability. This paradigm, however, is not\nstraightforwardly amenable to the mainstream gradient-based optimization\nmethods. This work surveys three main families of methods to learn such models:\nsurrogate gradients, continuous relaxation, and marginal likelihood\nmaximization via sampling. We conclude with a review of applications of these\nmethods and an inspection of the learned latent structure that they induce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Stage Episodic Control for Strategic Exploration in Text Games. (arXiv:2201.01251v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01251","description":"<p>Text adventure games present unique challenges to reinforcement learning\nmethods due to their combinatorially large action spaces and sparse rewards.\nThe interplay of these two factors is particularly demanding because large\naction spaces require extensive exploration, while sparse rewards provide\nlimited feedback. This work proposes to tackle the explore-vs-exploit dilemma\nusing a multi-stage approach that explicitly disentangles these two strategies\nwithin each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins\neach episode using an exploitation policy that imitates a set of promising\ntrajectories from the past, and then switches over to an exploration policy\naimed at discovering novel actions that lead to unseen state spaces. This\npolicy decomposition allows us to combine global decisions about which parts of\nthe game space to return to with curiosity-based local exploration in that\nspace, motivated by how a human may approach these games. Our method\nsignificantly outperforms prior approaches by 27% and 11% average normalized\nscore over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in\nboth deterministic and stochastic settings, respectively. On the game of Zork1,\nin particular, XTX obtains a score of 103, more than a 2x improvement over\nprior methods, and pushes past several known bottlenecks in the game that have\nplagued previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuyls_J/0/1/0/all/0/1\">Jens Tuyls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuSpaCy: an industrial-strength Hungarian natural language processing toolkit. (arXiv:2201.01956v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01956","description":"<p>Although there are a couple of open-source language processing pipelines\navailable for Hungarian, none of them satisfies the requirements of today's NLP\napplications. A language processing pipeline should consist of close to\nstate-of-the-art lemmatization, morphosyntactic analysis, entity recognition\nand word embeddings. Industrial text processing applications have to satisfy\nnon-functional software quality requirements, what is more, frameworks\nsupporting multiple languages are more and more favored. This paper introduces\nHuSpaCy, an industry-ready Hungarian language processing toolkit. The presented\ntool provides components for the most important basic linguistic analysis\ntasks. It is open-source and is available under a permissive license. Our\nsystem is built upon spaCy's NLP components resulting in an easily usable, fast\nyet accurate application. Experiments confirm that HuSpaCy has high accuracy\nwhile maintaining resource-efficient prediction capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orosz_G/0/1/0/all/0/1\">Gy&#xf6;rgy Orosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szanto_Z/0/1/0/all/0/1\">Zsolt Sz&#xe1;nt&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkecz_P/0/1/0/all/0/1\">P&#xe9;ter Berkecz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1\">Gerg&#x151; Szab&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkas_R/0/1/0/all/0/1\">Rich&#xe1;rd Farkas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot and Few-Shot Classification of Biomedical Articles in Context of the COVID-19 Pandemic. (arXiv:2201.03017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03017","description":"<p>MeSH (Medical Subject Headings) is a large thesaurus created by the National\nLibrary of Medicine and used for fine-grained indexing of publications in the\nbiomedical domain. In the context of the COVID-19 pandemic, MeSH descriptors\nhave emerged in relation to articles published on the corresponding topic.\nZero-shot classification is an adequate response for timely labeling of the\nstream of papers with MeSH categories. In this work, we hypothesise that rich\nsemantic information available in MeSH has potential to improve BioBERT\nrepresentations and make them more suitable for zero-shot/few-shot tasks. We\nframe the problem as determining if MeSH term definitions, concatenated with\npaper abstracts are valid instances or not, and leverage multi-task learning to\ninduce the MeSH hierarchy in the representations thanks to a seq2seq task.\nResults establish a baseline on the MedLine and LitCovid datasets, and probing\nshows that the resulting representations convey the hierarchical relations\npresent in MeSH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lupart_S/0/1/0/all/0/1\">Simon Lupart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favre_B/0/1/0/all/0/1\">Benoit Favre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ait_Mokhtar_S/0/1/0/all/0/1\">Salah Ait-Mokhtar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Reproducing BowNet: Learning Representations by Predicting Bags of Visual Words. (arXiv:2201.03556v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03556","description":"<p>This work aims to reproduce results from the CVPR 2020 paper by Gidaris et\nal. Self-supervised learning (SSL) is used to learn feature representations of\nan image using an unlabeled dataset. This work proposes to use bag-of-words\n(BoW) deep feature descriptors as a self-supervised learning target to learn\nrobust, deep representations. BowNet is trained to reconstruct the histogram of\nvisual words (ie. the deep BoW descriptor) of a reference image when presented\na perturbed version of the image as input. Thus, this method aims to learn\nperturbation-invariant and context-aware image features that can be useful for\nfew-shot tasks or supervised downstream tasks. In the paper, the author\ndescribes BowNet as a network consisting of a convolutional feature extractor\n$\\Phi(\\cdot)$ and a Dense-softmax layer $\\Omega(\\cdot)$ trained to predict BoW\nfeatures from images. After BoW training, the features of $\\Phi$ are used in\ndownstream tasks. For this challenge we were trying to build and train a\nnetwork that could reproduce the CIFAR-100 accuracy improvements reported in\nthe original paper. However, we were unsuccessful in reproducing an accuracy\nimprovement comparable to what the authors mentioned.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Harry Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Stone Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_H/0/1/0/all/0/1\">Hisham Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demonstrating The Risk of Imbalanced Datasets in Chest X-ray Image-based Diagnostics by Prototypical Relevance Propagation. (arXiv:2201.03559v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03559","description":"<p>The recent trend of integrating multi-source Chest X-Ray datasets to improve\nautomated diagnostics raises concerns that models learn to exploit\nsource-specific correlations to improve performance by recognizing the source\ndomain of an image rather than the medical pathology. We hypothesize that this\neffect is enforced by and leverages label-imbalance across the source domains,\ni.e, prevalence of a disease corresponding to a source. Therefore, in this\nwork, we perform a thorough study of the effect of label-imbalance in\nmulti-source training for the task of pneumonia detection on the widely used\nChestX-ray14 and CheXpert datasets. The results highlight and stress the\nimportance of using more faithful and transparent self-explaining models for\nautomated diagnosis, thus enabling the inherent detection of spurious learning.\nThey further illustrate that this undesirable effect of learning spurious\ncorrelations can be reduced considerably when ensuring label-balanced source\ndomain datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gautam_S/0/1/0/all/0/1\">Srishti Gautam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hohne_M/0/1/0/all/0/1\">Marina M.-C. H&#xf6;hne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_S/0/1/0/all/0/1\">Stine Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenssen_R/0/1/0/all/0/1\">Robert Jenssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative RAKI with Complex-Valued Convolution for Improved Image Reconstruction with Limited Scan-Specific Training Samples. (arXiv:2201.03560v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03560","description":"<p>MRI scan time reduction is commonly achieved by Parallel Imaging methods,\ntypically based on uniform undersampling of the inverse image space (a.k.a.\nk-space) and simultaneous signal reception with multiple receiver coils. The\nGRAPPA method interpolates missing k-space signals by linear combination of\nadjacent, acquired signals across all coils, and can be described by a\nconvolution in k-space. Recently, a more generalized method called RAKI was\nintroduced. RAKI is a deep-learning method that generalizes GRAPPA with\nadditional convolution layers, on which a non-linear activation function is\napplied. This enables non-linear estimation of missing signals by convolutional\nneural networks. In analogy to GRAPPA, the convolution kernels in RAKI are\ntrained using scan-specific training samples obtained from\nauto-calibration-signals (ACS). RAKI provides superior reconstruction quality\ncompared to GRAPPA, however, often requires much more ACS due to its increased\nnumber of unknown parameters. In order to overcome this limitation, this study\ninvestigates the influence of training data on the reconstruction quality for\nstandard 2D imaging, with particular focus on its amount and contrast\ninformation. Furthermore, an iterative k-space interpolation approach (iRAKI)\nis evaluated, which includes training data augmentation via an initial GRAPPA\nreconstruction, and refinement of convolution filters by iterative training.\nUsing only 18, 20 and 25 ACS lines (8%), iRAKI outperforms RAKI by suppressing\nresidual artefacts occurring at accelerations factors R=4 and R=5, and yields\nstrong noise suppression in comparison to GRAPPA, underlined by quantitative\nquality metrics. Combination with a phase-constraint yields further\nimprovement. Additionally, iRAKI shows better performance than GRAPPA and RAKI\nin case of pre-scan calibration and strongly varying contrast between training-\nand undersampled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dawood_P/0/1/0/all/0/1\">Peter Dawood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blaimer_M/0/1/0/all/0/1\">Martin Blaimer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breuer_F/0/1/0/all/0/1\">Felix Breuer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burd_P/0/1/0/all/0/1\">Paul R. Burd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Homolya_I/0/1/0/all/0/1\">Istv&#xe1;n Homolya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jakob_P/0/1/0/all/0/1\">Peter M. Jakob</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oberberger_J/0/1/0/all/0/1\">Johannes Oberberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image Representations. (arXiv:2201.03597v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03597","description":"<p>In tissue characterization and cancer diagnostics, multimodal imaging has\nemerged as a powerful technique. Thanks to computational advances, large\ndatasets can be exploited to improve diagnosis and discover patterns in\npathologies. However, this requires efficient and scalable image retrieval\nmethods. Cross-modality image retrieval is particularly demanding, as images of\nthe same content captured in different modalities may display little common\ninformation. We propose a content-based image retrieval system (CBIR) for\nreverse (sub-)image search to retrieve microscopy images in one modality given\na corresponding image captured by a different modality, where images are not\naligned and share only few structures. We propose to combine deep learning to\ngenerate representations which embed both modalities in a common space, with\nclassic, fast, and robust feature extractors (SIFT, SURF) to create a\nbag-of-words model for efficient and reliable retrieval. Our\napplication-independent approach shows promising results on a publicly\navailable dataset of brightfield and second harmonic generation microscopy\nimages. We obtain 75.4% and 83.6% top-10 retrieval success for retrieval in one\nor the other direction. Our proposed method significantly outperforms both\ndirect retrieval of the original multimodal (sub-)images, as well as their\ncorresponding generative adversarial network (GAN)-based image-to-image\ntranslations. We establish that the proposed method performs better in\ncomparison with a recent sub-image retrieval toolkit, GAN-based image-to-image\ntranslations, and learnt feature extractors for the downstream task of\ncross-modal image retrieval. We highlight the shortcomings of the latter\nmethods and observe the importance of equivariance and invariance properties of\nthe learnt representations and feature extractors in the CBIR pipeline. Code\nwill be available at github.com/MIDA-group.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Breznik_E/0/1/0/all/0/1\">Eva Breznik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzer_E/0/1/0/all/0/1\">Elisabeth Wetzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindblad_J/0/1/0/all/0/1\">Joakim Lindblad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sladoje_N/0/1/0/all/0/1\">Nata&#x161;a Sladoje</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-query Video Retrieval. (arXiv:2201.03639v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03639","description":"<p>Retrieving target videos based on text descriptions is a task of great\npractical value and has received increasing attention over the past few years.\nIn this paper, we focus on the less-studied setting of multi-query video\nretrieval, where multiple queries are provided to the model for searching over\nthe video archive. We first show that the multi-query retrieval task is more\npragmatic and representative of real-world use cases and better evaluates\nretrieval capabilities of current models, thereby deserving of further\ninvestigation alongside the more prevalent single-query retrieval setup. We\nthen propose several new methods for leveraging multiple queries at training\ntime to improve over simply combining similarity outputs of multiple queries\nfrom regular single-query trained models. Our models consistently outperform\nseveral competitive baselines over three different datasets. For instance,\nRecall@1 can be improved by 4.7 points on MSR-VTT, 4.1 points on MSVD and 11.7\npoints on VATEX over a strong baseline built on the state-of-the-art CLIP4Clip\nmodel. We believe further modeling efforts will bring new insights to this\ndirection and spark new systems that perform better in real-world video\nretrieval applications. Code is available at\nhttps://github.com/princetonvisualai/MQVR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Segmentation with Fully Trainable Gabor Kernels and Pearson's Correlation Coefficient. (arXiv:2201.03644v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03644","description":"<p>The convolutional layer and loss function are two fundamental components in\ndeep learning. Because of the success of conventional deep learning kernels,\nthe less versatile Gabor kernels become less popular despite the fact that they\ncan provide abundant features at different frequencies, orientations, and\nscales with much fewer parameters. For existing loss functions for multi-class\nimage segmentation, there is usually a tradeoff among accuracy, robustness to\nhyperparameters, and manual weight selections for combining different losses.\nTherefore, to gain the benefits of using Gabor kernels while keeping the\nadvantage of automatic feature generation in deep learning, we propose a fully\ntrainable Gabor-based convolutional layer where all Gabor parameters are\ntrainable through backpropagation. Furthermore, we propose a loss function\nbased on the Pearson's correlation coefficient, which is accurate, robust to\nlearning rates, and does not require manual weight selections. Experiments on\n43 3D brain magnetic resonance images with 19 anatomical structures show that,\nusing the proposed loss function with a proper combination of conventional and\nGabor-based kernels, we can train a network with only 1.6 million parameters to\nachieve an average Dice coefficient of 83%. This size is 44 times smaller than\nthe V-Net which has 71 million parameters. This paper demonstrates the\npotentials of using learnable parametric kernels in deep learning for 3D\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1\">Ken C. L. Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Group Robustness in the presence of Partial Group Labels. (arXiv:2201.03668v1 [cs.LG])","link":"http://arxiv.org/abs/2201.03668","description":"<p>Learning invariant representations is an important requirement when training\nmachine learning models that are driven by spurious correlations in the\ndatasets. These spurious correlations, between input samples and the target\nlabels, wrongly direct the neural network predictions resulting in poor\nperformance on certain groups, especially the minority groups. Robust training\nagainst these spurious correlations requires the knowledge of group membership\nfor every sample. Such a requirement is impractical in situations where the\ndata labeling efforts for minority or rare groups are significantly laborious\nor where the individuals comprising the dataset choose to conceal sensitive\ninformation. On the other hand, the presence of such data collection efforts\nresults in datasets that contain partially labeled group information. Recent\nworks have tackled the fully unsupervised scenario where no labels for groups\nare available. Thus, we aim to fill the missing gap in the literature by\ntackling a more realistic setting that can leverage partially available\nsensitive or group information during training. First, we construct a\nconstraint set and derive a high probability bound for the group assignment to\nbelong to the set. Second, we propose an algorithm that optimizes for the\nworst-off group assignments from the constraint set. Through experiments on\nimage and tabular datasets, we show improvements in the minority group's\nperformance while preserving overall aggregate accuracy across groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lokhande_V/0/1/0/all/0/1\">Vishnu Suresh Lokhande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuroplastic graph attention networks for nuclei segmentation in histopathology images. (arXiv:2201.03669v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03669","description":"<p>Modern histopathological image analysis relies on the segmentation of cell\nstructures to derive quantitative metrics required in biomedical research and\nclinical diagnostics. State-of-the-art deep learning approaches predominantly\napply convolutional layers in segmentation and are typically highly customized\nfor a specific experimental configuration; often unable to generalize to\nunknown data. As the model capacity of classical convolutional layers is\nlimited by a finite set of learned kernels, our approach uses a graph\nrepresentation of the image and focuses on the node transitions in multiple\nmagnifications. We propose a novel architecture for semantic segmentation of\ncell nuclei robust to differences in experimental configuration such as\nstaining and variation of cell types. The architecture is comprised of a novel\nneuroplastic graph attention network based on residual graph attention layers\nand concurrent optimization of the graph structure representing multiple\nmagnification levels of the histopathological image. The modification of graph\nstructure, which generates the node features by projection, is as important to\nthe architecture as the graph neural network itself. It determines the possible\nmessage flow and critical properties to optimize attention, graph structure,\nand node updates in a balanced magnification loss. In experimental evaluation,\nour framework outperforms ensembles of state-of-the-art neural networks, with a\nfraction of the neurons typically required, and sets new standards for the\nsegmentation of new nuclei datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alon_Y/0/1/0/all/0/1\">Yoav Alon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrintsGAN: Synthetic Fingerprint Generator. (arXiv:2201.03674v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03674","description":"<p>A major impediment to researchers working in the area of fingerprint\nrecognition is the lack of publicly available, large-scale, fingerprint\ndatasets. The publicly available datasets that do exist contain very few\nidentities and impressions per finger. This limits research on a number of\ntopics, including e.g., using deep networks to learn fixed length fingerprint\nembeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator\ncapable of generating unique fingerprints along with multiple impressions for a\ngiven fingerprint. Using PrintsGAN, we synthesize a database of 525,000\nfingerprints (35,000 distinct fingers, each with 15 impressions). Next, we show\nthe utility of the PrintsGAN generated dataset by training a deep network to\nextract a fixed-length embedding from a fingerprint. In particular, an\nembedding model trained on our synthetic fingerprints and fine-tuned on a small\nnumber of publicly available real fingerprints (25,000 prints from NIST SD302)\nobtains a TAR of 87.03% @ FAR=0.01% on the NIST SD4 database (a boost from\nTAR=73.37% when only trained on NIST SD302). Prevailing synthetic fingerprint\ngeneration methods do not enable such performance gains due to i) lack of\nrealism or ii) inability to generate multiple impressions per finger. We plan\nto release our database of synthetic fingerprints to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosz_S/0/1/0/all/0/1\">Steven A. Grosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NFANet: A Novel Method for Weakly Supervised Water Extraction from High-Resolution Remote Sensing Imagery. (arXiv:2201.03686v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03686","description":"<p>The use of deep learning for water extraction requires precise pixel-level\nlabels. However, it is very difficult to label high-resolution remote sensing\nimages at the pixel level. Therefore, we study how to utilize point labels to\nextract water bodies and propose a novel method called the neighbor feature\naggregation network (NFANet). Compared with pixellevel labels, point labels are\nmuch easier to obtain, but they will lose much information. In this paper, we\ntake advantage of the similarity between the adjacent pixels of a local\nwater-body, and propose a neighbor sampler to resample remote sensing images.\nThen, the sampled images are sent to the network for feature aggregation. In\naddition, we use an improved recursive training algorithm to further improve\nthe extraction accuracy, making the water boundary more natural. Furthermore,\nour method utilizes neighboring features instead of global or local features to\nlearn more representative features. The experimental results show that the\nproposed NFANet method not only outperforms other studied weakly supervised\napproaches, but also obtains similar results as the state-of-the-art ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Leyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bob Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An analysis of reconstruction noise from undersampled 4D flow MRI. (arXiv:2201.03715v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03715","description":"<p>Novel Magnetic Resonance (MR) imaging modalities can quantify hemodynamics\nbut require long acquisition times, precluding its widespread use for early\ndiagnosis of cardiovascular disease. To reduce the acquisition times,\nreconstruction methods from undersampled measurements are routinely used, that\nleverage representations designed to increase image compressibility.\n</p>\n<p>Reconstructed anatomical and hemodynamic images may present visual artifacts.\nAlthough some of these artifact are essentially reconstruction errors, and thus\na consequence of undersampling, others may be due to measurement noise or the\nrandom choice of the sampled frequencies. Said otherwise, a reconstructed image\nbecomes a random variable, and both its bias and its covariance can lead to\nvisual artifacts; the latter leads to spatial correlations that may be\nmisconstrued for visual information. Although the nature of the former has been\nstudied in the literature, the latter has not received as much attention.\n</p>\n<p>In this study, we investigate the theoretical properties of the random\nperturbations arising from the reconstruction process, and perform a number of\nnumerical experiments on simulated and MR aortic flow. Our results show that\nthe correlation length remains limited to two to three pixels when a Gaussian\nundersampling pattern is combined with recovery algorithms based on\n$\\ell_1$-norm minimization. However, the correlation length may increase\nsignificantly for other undersampling patterns, higher undersampling factors\n(i.e., 8x or 16x compression), and different reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Partin_L/0/1/0/all/0/1\">Lauren Partin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schiavazzi_D/0/1/0/all/0/1\">Daniele E. Schiavazzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_C/0/1/0/all/0/1\">Carlos A. Sing Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSA-Net: Tube Self-Attention Network for Action Quality Assessment. (arXiv:2201.03746v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03746","description":"<p>In recent years, assessing action quality from videos has attracted growing\nattention in computer vision community and human computer interaction. Most\nexisting approaches usually tackle this problem by directly migrating the model\nfrom action recognition tasks, which ignores the intrinsic differences within\nthe feature map such as foreground and background information. To address this\nissue, we propose a Tube Self-Attention Network (TSA-Net) for action quality\nassessment (AQA). Specifically, we introduce a single object tracker into AQA\nand propose the Tube Self-Attention Module (TSA), which can efficiently\ngenerate rich spatio-temporal contextual information by adopting sparse feature\ninteractions. The TSA module is embedded in existing video networks to form\nTSA-Net. Overall, our TSA-Net is with the following merits: 1) High\ncomputational efficiency, 2) High flexibility, and 3) The state-of-the art\nperformance. Extensive experiments are conducted on popular action quality\nassessment datasets including AQA-7 and MTL-AQA. Besides, a dataset named Fall\nRecognition in Figure Skating (FR-FS) is proposed to explore the basic action\nassessment in the figure skating scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shunli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Peng Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chixiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reciprocal Adversarial Learning for Brain Tumor Segmentation: A Solution to BraTS Challenge 2021 Segmentation Task. (arXiv:2201.03777v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03777","description":"<p>This paper proposes an adversarial learning based training approach for brain\ntumor segmentation task. In this concept, the 3D segmentation network learns\nfrom dual reciprocal adversarial learning approaches. To enhance the\ngeneralization across the segmentation predictions and to make the segmentation\nnetwork robust, we adhere to the Virtual Adversarial Training approach by\ngenerating more adversarial examples via adding some noise on original patient\ndata. By incorporating a critic that acts as a quantitative subjective referee,\nthe segmentation network learns from the uncertainty information associated\nwith segmentation results. We trained and evaluated network architecture on the\nRSNA-ASNR-MICCAI BraTS 2021 dataset. Our performance on the online validation\ndataset is as follows: Dice Similarity Score of 81.38%, 90.77% and 85.39%;\nHausdorff Distance (95\\%) of 21.83 mm, 5.37 mm, 8.56 mm for the enhancing\ntumor, whole tumor and tumor core, respectively. Similarly, our approach\nachieved a Dice Similarity Score of 84.55%, 90.46% and 85.30%, as well as\nHausdorff Distance (95\\%) of 13.48 mm, 6.32 mm and 16.98 mm on the final test\ndataset. Overall, our proposed approach yielded better performance in\nsegmentation accuracy for each tumor sub-region. Our code implementation is\npublicly available at https://github.com/himashi92/vizviva_brats_2021\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peiris_H/0/1/0/all/0/1\">Himashi Peiris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaolin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egan_G/0/1/0/all/0/1\">Gary Egan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drone Object Detection Using RGB/IR Fusion. (arXiv:2201.03786v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03786","description":"<p>Object detection using aerial drone imagery has received a great deal of\nattention in recent years. While visible light images are adequate for\ndetecting objects in most scenarios, thermal cameras can extend the\ncapabilities of object detection to night-time or occluded objects. As such,\nRGB and Infrared (IR) fusion methods for object detection are useful and\nimportant. One of the biggest challenges in applying deep learning methods to\nRGB/IR object detection is the lack of available training data for drone IR\nimagery, especially at night. In this paper, we develop several strategies for\ncreating synthetic IR images using the AIRSim simulation engine and CycleGAN.\nFurthermore, we utilize an illumination-aware fusion framework to fuse RGB and\nIR images for object detection on the ground. We characterize and test our\nmethods for both simulated and actual data. Our solution is implemented on an\nNVIDIA Jetson Xavier running on an actual drone, requiring about 28\nmilliseconds of processing per RGB/IR image pair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lizhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruhang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakhor_A/0/1/0/all/0/1\">Avideh Zakhor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Beer Bottles using Object Detection and Transfer Learning. (arXiv:2201.03791v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03791","description":"<p>Classification problems are common in Computer Vision. Despite this, there is\nno dedicated work for the classification of beer bottles. As part of the\nchallenge of the master course Deep Learning, a dataset of 5207 beer bottle\nimages and brand labels was created. An image contains exactly one beer bottle.\nIn this paper we present a deep learning model which classifies pictures of\nbeer bottles in a two step approach. As the first step, a Faster-R-CNN detects\nimage sections relevant for classification independently of the brand. In the\nsecond step, the relevant image sections are classified by a ResNet-18. The\nimage section with the highest confidence is returned as class label. We\npropose a model, with which we surpass the classic one step transfer learning\napproach and reached an accuracy of 99.86% during the challenge on the final\ntest dataset. We were able to achieve 100% accuracy after the challenge ended\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hohlfeld_P/0/1/0/all/0/1\">Philipp Hohlfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostermeier_T/0/1/0/all/0/1\">Tobias Ostermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandl_D/0/1/0/all/0/1\">Dominik Brandl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Non-Local Contrastive Attention for Image Super-Resolution. (arXiv:2201.03794v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03794","description":"<p>Non-Local Attention (NLA) brings significant improvement for Single Image\nSuper-Resolution (SISR) by leveraging intrinsic feature correlation in natural\nimages. However, NLA gives noisy information large weights and consumes\nquadratic computation resources with respect to the input size, limiting its\nperformance and application. In this paper, we propose a novel Efficient\nNon-Local Contrastive Attention (ENLCA) to perform long-range visual modeling\nand leverage more relevant non-local features. Specifically, ENLCA consists of\ntwo parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA\nadopts the kernel method to approximate exponential function and obtains linear\ncomputation complexity. For Sparse Aggregation, we multiply inputs by an\namplification factor to focus on informative features, yet the variance of\napproximation increases exponentially. Therefore, contrastive learning is\napplied to further separate relevant and irrelevant features. To demonstrate\nthe effectiveness of ENLCA, we build an architecture called Efficient Non-Local\nContrastive Network (ENLCN) by adding a few of our modules in a simple\nbackbone. Extensive experimental results show that ENLCN reaches superior\nperformance over state-of-the-art approaches on both quantitative and\nqualitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_Y/0/1/0/all/0/1\">Yucheng Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COROLLA: An Efficient Multi-Modality Fusion Framework with Supervised Contrastive Learning for Glaucoma Grading. (arXiv:2201.03795v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03795","description":"<p>Glaucoma is one of the ophthalmic diseases that may cause blindness, for\nwhich early detection and treatment are very important. Fundus images and\noptical coherence tomography (OCT) images are both widely-used modalities in\ndiagnosing glaucoma. However, existing glaucoma grading approaches mainly\nutilize a single modality, ignoring the complementary information between\nfundus and OCT. In this paper, we propose an efficient multi-modality\nsupervised contrastive learning framework, named COROLLA, for glaucoma grading.\nThrough layer segmentation as well as thickness calculation and projection,\nretinal thickness maps are extracted from the original OCT volumes and used as\na replacing modality, resulting in more efficient calculations with less memory\nusage. Given the high structure and distribution similarities across medical\nimage samples, we employ supervised contrastive learning to increase our\nmodels' discriminative power with better convergence. Moreover, feature-level\nfusion of paired fundus image and thickness map is conducted for enhanced\ndiagnosis accuracy. On the GAMMA dataset, our COROLLA framework achieves\noverwhelming glaucoma grading performance compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiyuan Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Huaqing He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptive Person Re-id with Local-enhance and Prototype Dictionary Learning. (arXiv:2201.03803v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03803","description":"<p>The unsupervised domain adaptive person re-identification (re-ID) task has\nbeen a challenge because, unlike the general domain adaptive tasks, there is no\noverlap between the classes of source and target domain data in the person\nre-ID, which leads to a significant domain gap. State-of-the-art unsupervised\nre-ID methods train the neural networks using a memory-based contrastive loss.\nHowever, performing contrastive learning by treating each unlabeled instance as\na class will lead to the problem of class collision, and the updating intensity\nis inconsistent due to the difference in the number of instances of different\ncategories when updating in the memory bank. To address such problems, we\npropose Prototype Dictionary Learning for person re-ID which is able to utilize\nboth source domain data and target domain data by one training stage while\navoiding the problem of class collision and the problem of updating intensity\ninconsistency by cluster-level prototype dictionary learning. In order to\nreduce the interference of domain gap on the model, we propose a local-enhance\nmodule to improve the domain adaptation of the model without increasing the\nnumber of model parameters. Our experiments on two large datasets demonstrate\nthe effectiveness of the prototype dictionary learning. 71.5\\% mAP is achieved\nin the Market-to-Duke task, which is a 2.3\\% improvement compared to the\nstate-of-the-art unsupervised domain adaptive re-ID methods. It achieves 83.9\\%\nmAP in the Duke-to-Market task, which improves by 4.4\\% compared to the\nstate-of-the-art unsupervised adaptive re-ID methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1\">Haopeng Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobileFaceSwap: A Lightweight Framework for Video Face Swapping. (arXiv:2201.03808v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03808","description":"<p>Advanced face swapping methods have achieved appealing results. However, most\nof these methods have many parameters and computations, which makes it\nchallenging to apply them in real-time applications or deploy them on edge\ndevices like mobile phones. In this work, we propose a lightweight\nIdentity-aware Dynamic Network (IDN) for subject-agnostic face swapping by\ndynamically adjusting the model parameters according to the identity\ninformation. In particular, we design an efficient Identity Injection Module\n(IIM) by introducing two dynamic neural network techniques, including the\nweights prediction and weights modulation. Once the IDN is updated, it can be\napplied to swap faces given any target image or video. The presented IDN\ncontains only 0.50M parameters and needs 0.33G FLOPs per frame, making it\ncapable for real-time video face swapping on mobile phones. In addition, we\nintroduce a knowledge distillation-based method for stable training, and a loss\nreweighting module is employed to obtain better synthesized results. Finally,\nour method achieves comparable results with the teacher models and other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhibin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Exploring Pose Estimation as an Auxiliary Learning Task for Visible-Infrared Person Re-identification. (arXiv:2201.03859v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03859","description":"<p>Visible-infrared person re-identification (VI-ReID) has been challenging due\nto the existence of large discrepancies between visible and infrared\nmodalities. Most pioneering approaches reduce intra-class variations and\ninter-modality discrepancies by learning modality-shared and ID-related\nfeatures. However, an explicit modality-shared cue, i.e., body keypoints, has\nnot been fully exploited in VI-ReID. Additionally, existing feature learning\nparadigms imposed constraints on either global features or partitioned feature\nstripes, which neglect the prediction consistency of global and part features.\nTo address the above problems, we exploit Pose Estimation as an auxiliary\nlearning task to assist the VI-ReID task in an end-to-end framework. By jointly\ntraining these two tasks in a mutually beneficial manner, our model learns\nhigher quality modality-shared and ID-related features. On top of it, the\nlearnings of global features and local features are seamlessly synchronized by\nHierarchical Feature Constraint (HFC), where the former supervises the latter\nusing the knowledge distillation strategy. Experimental results on two\nbenchmark VI-ReID datasets show that the proposed method consistently improves\nstate-of-the-art methods by significant margins. Specifically, our method\nachieves nearly 20$\\%$ mAP improvements against the state-of-the-art method on\nthe RegDB dataset. Our intriguing findings highlight the usage of auxiliary\ntask learning in VI-ReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yunqi Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1\">Nianchang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Estimation from EEG -- A Dual Deep Learning Approach Combined with Saliency. (arXiv:2201.03891v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03891","description":"<p>Emotion estimation is an active field of research that has an important\nimpact on the interaction between human and computer. Among the different\nmodality to assess emotion, electroencephalogram (EEG) representing the\nelectrical brain activity presented motivating results during the last decade.\nEmotion estimation from EEG could help in the diagnosis or rehabilitation of\ncertain diseases. In this paper, we propose a dual method considering the\nphysiological knowledge defined by specialists combined with novel deep\nlearning (DL) models initially dedicated to computer vision. The joint learning\nhas been enhanced with model saliency analysis. To present a global approach,\nthe model has been evaluated on four publicly available datasets and achieves\nsimilar results to the state-of-theart approaches and outperforming results for\ntwo of the proposed datasets with a lower standard deviation that reflects\nhigher stability. For sake of reproducibility, the codes and models proposed in\nthis paper are available at github.com/VDelv/Emotion-EEG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delvigne_V/0/1/0/all/0/1\">Victor Delvigne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facchini_A/0/1/0/all/0/1\">Antoine Facchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wannous_H/0/1/0/all/0/1\">Hazem Wannous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ris_L/0/1/0/all/0/1\">Laurence Ris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandeborre_J/0/1/0/all/0/1\">Jean-Philippe Vandeborre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where Is My Mind (looking at)? Predicting Visual Attention from Brain Activity. (arXiv:2201.03902v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03902","description":"<p>Visual attention estimation is an active field of research at the crossroads\nof different disciplines: computer vision, artificial intelligence and\nmedicine. One of the most common approaches to estimate a saliency map\nrepresenting attention is based on the observed images. In this paper, we show\nthat visual attention can be retrieved from EEG acquisition. The results are\ncomparable to traditional predictions from observed images, which is of great\ninterest. For this purpose, a set of signals has been recorded and different\nmodels have been developed to study the relationship between visual attention\nand brain activity. The results are encouraging and comparable with other\napproaches estimating attention with other modalities. The codes and dataset\nconsidered in this paper have been made available at\n\\url{https://figshare.com/s/3e353bd1c621962888ad} to promote research in the\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delvigne_V/0/1/0/all/0/1\">Victor Delvigne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tits_N/0/1/0/all/0/1\">No&#xe9; Tits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisca_L/0/1/0/all/0/1\">Luca La Fisca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubens_N/0/1/0/all/0/1\">Nathan Hubens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_A/0/1/0/all/0/1\">Antoine Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wannous_H/0/1/0/all/0/1\">Hazem Wannous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandeborre_J/0/1/0/all/0/1\">Jean-Philippe Vandeborre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Extraction Framework based on Contrastive Learning with Adaptive Positive and Negative Samples. (arXiv:2201.03942v1 [cs.LG])","link":"http://arxiv.org/abs/2201.03942","description":"<p>In this study, we propose a feature extraction framework based on contrastive\nlearning with adaptive positive and negative samples (CL-FEFA) that is suitable\nfor unsupervised, supervised, and semi-supervised single-view feature\nextraction. CL-FEFA constructs adaptively the positive and negative samples\nfrom the results of feature extraction, which makes it more appropriate and\naccurate. Thereafter, the discriminative features are re extracted to according\nto InfoNCE loss based on previous positive and negative samples, which will\nmake the intra-class samples more compact and the inter-class samples more\ndispersed. At the same time, using the potential structure information of\nsubspace samples to dynamically construct positive and negative samples can\nmake our framework more robust to noisy data. Furthermore, CL-FEFA considers\nthe mutual information between positive samples, that is, similar samples in\npotential structures, which provides theoretical support for its advantages in\nfeature extraction. The final numerical experiments prove that the proposed\nframework has a strong advantage over the traditional feature extraction\nmethods and contrastive learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongjie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering. (arXiv:2201.03965v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03965","description":"<p>In recent years, multi-modal transformers have shown significant progress in\nVision-Language tasks, such as Visual Question Answering (VQA), outperforming\nprevious architectures by a considerable margin. This improvement in VQA is\noften attributed to the rich interactions between vision and language streams.\nIn this work, we investigate the efficacy of co-attention transformer layers in\nhelping the network focus on relevant regions while answering the question. We\ngenerate visual attention maps using the question-conditioned image attention\nscores in these co-attention layers. We evaluate the effect of the following\ncritical components on visual attention of a state-of-the-art VQA model: (i)\nnumber of object region proposals, (ii) question part of speech (POS) tags,\n(iii) question semantics, (iv) number of co-attention layers, and (v) answer\naccuracy. We compare the neural network attention maps against human attention\nmaps both qualitatively and quantitatively. Our findings indicate that\nco-attention transformer modules are crucial in attending to relevant regions\nof the image given a question. Importantly, we observe that the semantic\nmeaning of the question is not what drives visual attention, but specific\nkeywords in the question do. Our work sheds light on the function and\ninterpretation of co-attention transformer layers, highlights gaps in current\nnetworks, and can guide the development of future VQA models and networks that\nsimultaneously process visual and language streams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sikarwar_A/0/1/0/all/0/1\">Ankur Sikarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v1 [cs.LG])","link":"http://arxiv.org/abs/2201.03969","description":"<p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem\ndue to the heterogeneity gap between different modalities and the ambiguity of\nhuman emotional expression. Although there have been many successful attempts\nto construct multimodal representations for MSA, there are still two challenges\nto be addressed: 1) A more robust multimodal representation needs to be\nconstructed to bridge the heterogeneity gap and cope with the complex\nmultimodal interactions, and 2) the contextual dynamics must be modeled\neffectively throughout the information flow. In this work, we propose a\nmultimodal representation model based on Mutual information Maximization and\nMinimization and Identity Embedding (MMMIE). We combine mutual information\nmaximization between modal pairs, and mutual information minimization between\ninput data and corresponding features to mine the modal-invariant and\ntask-related information. Furthermore, Identity Embedding is proposed to prompt\nthe downstream network to perceive the contextual information. Experimental\nresults on two public datasets demonstrate the effectiveness of the proposed\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiahao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image quality measurements and denoising using Fourier Ring Correlations. (arXiv:2201.03992v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03992","description":"<p>Image quality is a nebulous concept with different meanings to different\npeople. To quantify image quality a relative difference is typically calculated\nbetween a corrupted image and a ground truth image. But what metric should we\nuse for measuring this difference? Ideally, the metric should perform well for\nboth natural and scientific images. The structural similarity index (SSIM) is a\ngood measure for how humans perceive image similarities, but is not sensitive\nto differences that are scientifically meaningful in microscopy. In electron\nand super-resolution microscopy, the Fourier Ring Correlation (FRC) is often\nused, but is little known outside of these fields. Here we show that the FRC\ncan equally well be applied to natural images, e.g. the Google Open Images\ndataset. We then define a loss function based on the FRC, show that it is\nanalytically differentiable, and use it to train a U-net for denoising of\nimages. This FRC-based loss function allows the network to train faster and\nachieve similar or better results than when using L1- or L2- based losses. We\nalso investigate the properties and limitations of neural network denoising\nwith the FRC analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kaczmar_Michalska_J/0/1/0/all/0/1\">J. Kaczmar-Michalska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hajizadeh_N/0/1/0/all/0/1\">N.R. Hajizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rzepiela_A/0/1/0/all/0/1\">A.J. Rzepiela</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norrelykke_S/0/1/0/all/0/1\">S.F. N&#xf8;rrelykke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Home-Built Metrology to Analyze Oral Fluid Droplets and Quantify the Efficacy of Masks. (arXiv:2201.03993v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03993","description":"<p>Wearing masks is crucial to preventing the spread of potentially\npathogen-containing droplets, especially amidst the COVID-19 pandemic. However,\nnot all face coverings are equally effective and most experiments evaluating\nmask efficacy are very expensive and complex to operate. In this work, a novel,\nhome-built, low-cost, and accurate metrology to visualize orally-generated\nfluid droplets has been developed. The project includes setup optimization,\ndata collection, data analysis, and applications. The final materials chosen\nwere quinine-containing tonic water, 397-402 nm wavelength UV tube lights, an\niPhone and tripod, string, and a spray bottle. The experiment took place in a\ndark closet with a dark background. During data collection, the test subject\nfirst wets their mouth with an ingestible fluorescent liquid (tonic water) and\nspeaks, sneezes, or coughs under UV darklight. The fluorescence from the tonic\nwater droplets generated can be visualized, recorded by an iPhone 8+ camera in\nslo-mo (240 fps), and analyzed. The software VLC is used for frame separation\nand Fiji/ImageJ is used for image processing and analysis. The dependencies of\noral fluid droplet generation and propagation on different phonics, the\nloudness of speech, and the type of expiratory event were studied in detail and\nestablished using the metrology developed. The efficacy of different types of\nmasks was evaluated and correlated with fabric microstructures. All masks\nblocked droplets to varying extent. Masks with smaller-sized pores and thicker\nmaterial were found to block the most droplets. This low-cost technique can be\neasily constructed at home using materials that total to a cost of less than\n$50. Despite the minimal cost, the method is very accurate and the data is\nquantifiable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhowmik_A/0/1/0/all/0/1\">Ava Tan Bhowmik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity-based Gray-box Adversarial Attack Against Deep Face Recognition. (arXiv:2201.04011v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04011","description":"<p>The majority of adversarial attack techniques perform well against deep face\nrecognition when the full knowledge of the system is revealed\n(\\emph{white-box}). However, such techniques act unsuccessfully in the gray-box\nsetting where the face templates are unknown to the attackers. In this work, we\npropose a similarity-based gray-box adversarial attack (SGADV) technique with a\nnewly developed objective function. SGADV utilizes the dissimilarity score to\nproduce the optimized adversarial example, i.e., similarity-based adversarial\nattack. This technique applies to both white-box and gray-box attacks against\nauthentication systems that determine genuine or imposter users using the\ndissimilarity score. To validate the effectiveness of SGADV, we conduct\nextensive experiments on face datasets of LFW, CelebA, and CelebA-HQ against\ndeep face recognition models of FaceNet and InsightFace in both white-box and\ngray-box settings. The results suggest that the proposed method significantly\noutperforms the existing adversarial attack techniques in the gray-box setting.\nWe hence summarize that the similarity-base approaches to develop the\nadversarial example could satisfactorily cater to the gray-box attack scenarios\nfor de-authentication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhe Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yandan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cunjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tistarell_M/0/1/0/all/0/1\">Massimo Tistarell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Captcha Attack:Turning Captchas Against Humanity. (arXiv:2201.04014v1 [cs.CR])","link":"http://arxiv.org/abs/2201.04014","description":"<p>Nowadays, people generate and share massive content on online platforms\n(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook\nusers posted around 150 thousand photos every minute. Content moderators\nconstantly monitor these online platforms to prevent the spreading of\ninappropriate content (e.g., hate speech, nudity images). Based on deep\nlearning (DL) advances, Automatic Content Moderators (ACM) help human\nmoderators handle high data volume. Despite their advantages, attackers can\nexploit weaknesses of DL components (e.g., preprocessing, model) to affect\ntheir performance. Therefore, an attacker can leverage such techniques to\nspread inappropriate content by evading ACM.\n</p>\n<p>In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that\nallows users to spread inappropriate text online by evading ACM controls. CAPA,\nby generating custom textual CAPTCHAs, exploits ACM's careless design\nimplementations and internal procedures vulnerabilities. We test our attack on\nreal-world ACM, and the results confirm the ferocity of our simple yet\neffective attack, reaching up to a 100% evasion success in most cases. At the\nsame time, we demonstrate the difficulties in designing CAPA mitigations,\nopening new challenges in CAPTCHAs research area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajola_L/0/1/0/all/0/1\">Luca Pajola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tricomi_P/0/1/0/all/0/1\">Pier Paolo Tricomi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04019","description":"<p>The recently proposed MaskFormer \\cite{maskformer} gives a refreshed\nperspective on the task of semantic segmentation: it shifts from the popular\npixel-level classification paradigm to a mask-level classification method. In\nessence, it generates paired probabilities and masks corresponding to category\nsegments and combines them during inference for the segmentation maps. The\nsegmentation quality thus relies on how well the queries can capture the\nsemantic information for categories and their spatial locations within the\nimages. In our study, we find that per-mask classification decoder on top of a\nsingle-scale feature is not effective enough to extract reliable probability or\nmask. To mine for rich semantic information across the feature pyramid, we\npropose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask\napproach semantic segmentation on top of multi-scale features. To efficiently\nutilize image features of different resolutions without incurring too much\ncomputational overheads, PFT uses a multi-scale transformer decoder with\ncross-scale inter-query attention to exchange complimentary information.\nExtensive experimental evaluations and ablations demonstrate the efficacy of\nour framework. In particular, we achieve a 3.2 mIoU improvement on COCO-Stuff\n10K dataset with ResNet-101c compared to MaskFormer. Besides, on ADE20K\nvalidation set, our result with Swin-B backbone matches that of MaskFormer's\nwith a much larger Swin-L backbone in both single-scale and multi-scale\ninference, achieving 54.1 mIoU and 55.3 mIoU respectively. Using a Swin-L\nbackbone, we achieve 56.0 mIoU single-scale result on the ADE20K validation set\nand 57.2 multi-scale result, obtaining state-of-the-art performance on the\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Maoqing Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimization Planning for 3D ConvNets. (arXiv:2201.04021v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04021","description":"<p>It is not trivial to optimally learn a 3D Convolutional Neural Networks (3D\nConvNets) due to high complexity and various options of the training scheme.\nThe most common hand-tuning process starts from learning 3D ConvNets using\nshort video clips and then is followed by learning long-term temporal\ndependency using lengthy clips, while gradually decaying the learning rate from\nhigh to low as training progresses. The fact that such process comes along with\nseveral heuristic settings motivates the study to seek an optimal \"path\" to\nautomate the entire training. In this paper, we decompose the path into a\nseries of training \"states\" and specify the hyper-parameters, e.g., learning\nrate and the length of input clips, in each state. The estimation of the knee\npoint on the performance-epoch curve triggers the transition from one state to\nanother. We perform dynamic programming over all the candidate states to plan\nthe optimal permutation of states, i.e., optimization path. Furthermore, we\ndevise a new 3D ConvNets with a unique design of dual-head classifier to\nimprove spatial and temporal discrimination. Extensive experiments on seven\npublic video recognition benchmarks demonstrate the advantages of our proposal.\nWith the optimization planning, our 3D ConvNets achieves superior results when\ncomparing to the state-of-the-art recognition methods. More remarkably, we\nobtain the top-1 accuracy of 80.5% and 82.7% on Kinetics-400 and Kinetics-600\ndatasets, respectively. Source code is available at\nhttps://github.com/ZhaofanQiu/Optimization-Planning-for-3D-ConvNets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaofan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Condensing a Sequence to One Informative Frame for Video Recognition. (arXiv:2201.04022v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04022","description":"<p>Video is complex due to large variations in motion and rich content in\nfine-grained visual details. Abstracting useful information from such\ninformation-intensive media requires exhaustive computing resources. This paper\nstudies a two-step alternative that first condenses the video sequence to an\ninformative \"frame\" and then exploits off-the-shelf image recognition system on\nthe synthetic frame. A valid question is how to define \"useful information\" and\nthen distill it from a video sequence down to one synthetic frame. This paper\npresents a novel Informative Frame Synthesis (IFS) architecture that\nincorporates three objective tasks, i.e., appearance reconstruction, video\ncategorization, motion estimation, and two regularizers, i.e., adversarial\nlearning, color consistency. Each task equips the synthetic frame with one\nability, while each regularizer enhances its visual quality. With these, by\njointly learning the frame synthesis in an end-to-end manner, the generated\nframe is expected to encapsulate the required spatio-temporal information\nuseful for video analysis. Extensive experiments are conducted on the\nlarge-scale Kinetics dataset. When comparing to baseline methods that map video\nsequence to a single image, IFS shows superior performance. More remarkably,\nIFS consistently demonstrates evident improvements on image-based 2D networks\nand clip-based 3D networks, and achieves comparable performance with the\nstate-of-the-art methods with less computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaofan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yan Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Video Representation Learning with Multi-Faceted Integration. (arXiv:2201.04023v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04023","description":"<p>Video content is multifaceted, consisting of objects, scenes, interactions or\nactions. The existing datasets mostly label only one of the facets for model\ntraining, resulting in the video representation that biases to only one facet\ndepending on the training dataset. There is no study yet on how to learn a\nvideo representation from multifaceted labels, and whether multifaceted\ninformation is helpful for video representation learning. In this paper, we\npropose a new learning framework, MUlti-Faceted Integration (MUFI), to\naggregate facets from different datasets for learning a representation that\ncould reflect the full spectrum of video content. Technically, MUFI formulates\nthe problem as visual-semantic embedding learning, which explicitly maps video\nrepresentation into a rich semantic embedding space, and jointly optimizes\nvideo representation from two perspectives. One is to capitalize on the\nintra-facet supervision between each video and its own label descriptions, and\nthe second predicts the \"semantic representation\" of each video from the facets\nof other datasets as the inter-facet supervision. Extensive experiments\ndemonstrate that learning 3D CNN via our MUFI framework on a union of four\nlarge-scale video datasets plus two image datasets leads to superior capability\nof video representation. The pre-learnt 3D CNN with MUFI also shows clear\nimprovements over other approaches on several downstream video applications.\nMore remarkably, MUFI achieves 98.1%/80.9% on UCF101/HMDB51 for action\nrecognition and 101.5% in terms of CIDEr-D score on MSVD for video captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaofan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Director: An Event-Driven Directing System for Live Broadcasting. (arXiv:2201.04024v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04024","description":"<p>Live video broadcasting normally requires a multitude of skills and expertise\nwith domain knowledge to enable multi-camera productions. As the number of\ncameras keep increasing, directing a live sports broadcast has now become more\ncomplicated and challenging than ever before. The broadcast directors need to\nbe much more concentrated, responsive, and knowledgeable, during the\nproduction. To relieve the directors from their intensive efforts, we develop\nan innovative automated sports broadcast directing system, called Smart\nDirector, which aims at mimicking the typical human-in-the-loop broadcasting\nprocess to automatically create near-professional broadcasting programs in\nreal-time by using a set of advanced multi-view video analysis algorithms.\nInspired by the so-called \"three-event\" construction of sports broadcast, we\nbuild our system with an event-driven pipeline consisting of three consecutive\nnovel components: 1) the Multi-view Event Localization to detect events by\nmodeling multi-view correlations, 2) the Multi-view Highlight Detection to rank\ncamera views by the visual importance for view selection, 3) the\nAuto-Broadcasting Scheduler to control the production of broadcasting videos.\nTo our best knowledge, our system is the first end-to-end automated directing\nsystem for multi-camera sports broadcasting, completely driven by the semantic\nunderstanding of sports events. It is also the first system to solve the novel\nproblem of multi-view joint event detection by cross-view relation modeling. We\nconduct both objective and subjective evaluations on a real-world multi-camera\nsoccer dataset, which demonstrate the quality of our auto-generated videos is\ncomparable to that of the human-directed. Thanks to its faster response, our\nsystem is able to capture more fast-passing and short-duration events which are\nusually missed by human directors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qian Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training. (arXiv:2201.04026v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04026","description":"<p>Vision-language pre-training has been an emerging and fast-developing\nresearch topic, which transfers multi-modal knowledge from rich-resource\npre-training task to limited-resource downstream tasks. Unlike existing works\nthat predominantly learn a single generic encoder, we present a pre-trainable\nUniversal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language\nperception (e.g., visual question answering) and generation (e.g., image\ncaptioning). Uni-EDEN is a two-stream Transformer based structure, consisting\nof three modules: object and sentence encoders that separately learns the\nrepresentations of each modality, and sentence decoder that enables both\nmulti-modal reasoning and sentence generation via inter-modal interaction.\nConsidering that the linguistic representations of each image can span\ndifferent granularities in this hierarchy including, from simple to\ncomprehensive, individual label, a phrase, and a natural sentence, we pre-train\nUni-EDEN through multi-granular vision-language proxy tasks: Masked Object\nClassification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence\nMatching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is\nendowed with the power of both multi-modal representation extraction and\nlanguage modeling. Extensive experiments demonstrate the compelling\ngeneralizability of Uni-EDEN by fine-tuning it to four vision-language\nperception and generation downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiahao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representing Videos as Discriminative Sub-graphs for Action Recognition. (arXiv:2201.04027v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04027","description":"<p>Human actions are typically of combinatorial structures or patterns, i.e.,\nsubjects, objects, plus spatio-temporal interactions in between. Discovering\nsuch structures is therefore a rewarding way to reason about the dynamics of\ninteractions and recognize the actions. In this paper, we introduce a new\ndesign of sub-graphs to represent and encode the discriminative patterns of\neach action in the videos. Specifically, we present MUlti-scale Sub-graph\nLEarning (MUSLE) framework that novelly builds space-time graphs and clusters\nthe graphs into compact sub-graphs on each scale with respect to the number of\nnodes. Technically, MUSLE produces 3D bounding boxes, i.e., tubelets, in each\nvideo clip, as graph nodes and takes dense connectivity as graph edges between\ntubelets. For each action category, we execute online clustering to decompose\nthe graph into sub-graphs on each scale through learning Gaussian Mixture Layer\nand select the discriminative sub-graphs as action prototypes for recognition.\nExtensive experiments are conducted on both Something-Something V1 &amp; V2 and\nKinetics-400 datasets, and superior results are reported when comparing to\nstate-of-the-art methods. More remarkably, our MUSLE achieves to-date the best\nreported accuracy of 65.0% on Something-Something V2 validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaofan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-Focused Contrastive Learning of Video Representations. (arXiv:2201.04029v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04029","description":"<p>Motion, as the most distinct phenomenon in a video to involve the changes\nover time, has been unique and critical to the development of video\nrepresentation learning. In this paper, we ask the question: how important is\nthe motion particularly for self-supervised video representation learning. To\nthis end, we compose a duet of exploiting the motion for data augmentation and\nfeature learning in the regime of contrastive learning. Specifically, we\npresent a Motion-focused Contrastive Learning (MCL) method that regards such\nduet as the foundation. On one hand, MCL capitalizes on optical flow of each\nframe in a video to temporally and spatially sample the tubelets (i.e.,\nsequences of associated frame patches across time) as data augmentations. On\nthe other hand, MCL further aligns gradient maps of the convolutional layers to\noptical flow maps from spatial, temporal and spatio-temporal perspectives, in\norder to ground motion information in feature learning. Extensive experiments\nconducted on R(2+1)D backbone demonstrate the effectiveness of our MCL. On\nUCF101, the linear classifier trained on the representations learnt by MCL\nachieves 81.91% top-1 accuracy, outperforming ImageNet supervised pre-training\nby 6.78%. On Kinetics-400, MCL achieves 66.62% top-1 accuracy under the linear\nprotocol. Code is available at\nhttps://github.com/YihengZhang-CV/MCL-Motion-Focused-Contrastive-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaofan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobilePhys: Personalized Mobile Camera-Based Contactless Physiological Sensing. (arXiv:2201.04039v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04039","description":"<p>Camera-based contactless photoplethysmography refers to a set of popular\ntechniques for contactless physiological measurement. The current\nstate-of-the-art neural models are typically trained in a supervised manner\nusing videos accompanied by gold standard physiological measurements. However,\nthey often generalize poorly out-of-domain examples (i.e., videos that are\nunlike those in the training set). Personalizing models can help improve model\ngeneralizability, but many personalization techniques still require some gold\nstandard data. To help alleviate this dependency, in this paper, we present a\nnovel mobile sensing system called MobilePhys, the first mobile personalized\nremote physiological sensing system, that leverages both front and rear cameras\non a smartphone to generate high-quality self-supervised labels for training\npersonalized contactless camera-based PPG models. To evaluate the robustness of\nMobilePhys, we conducted a user study with 39 participants who completed a set\nof tasks under different mobile devices, lighting conditions/intensities,\nmotion tasks, and skin types. Our results show that MobilePhys significantly\noutperforms the state-of-the-art on-device supervised training and few-shot\nadaptation methods. Through extensive user studies, we further examine how does\nMobilePhys perform in complex real-world settings. We envision that calibrated\nor personalized camera-based contactless PPG models generated from our proposed\ndual-camera mobile sensing system will open the door for numerous future\napplications such as smart mirrors, fitness and mobile health applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuntao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sinan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shwetak Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lightweight Neural Animation : Exploration of Neural Network Pruning in Mixture of Experts-based Animation Models. (arXiv:2201.04042v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04042","description":"<p>In the past few years, neural character animation has emerged and offered an\nautomatic method for animating virtual characters. Their motion is synthesized\nby a neural network. Controlling this movement in real time with a user-defined\ncontrol signal is also an important task in video games for example. Solutions\nbased on fully-connected layers (MLPs) and Mixture-of-Experts (MoE) have given\nimpressive results in generating and controlling various movements with\nclose-range interactions between the environment and the virtual character.\nHowever, a major shortcoming of fully-connected layers is their computational\nand memory cost which may lead to sub-optimized solution. In this work, we\napply pruning algorithms to compress an MLP- MoE neural network in the context\nof interactive character animation, which reduces its number of parameters and\naccelerates its computation time with a trade-off between this acceleration and\nthe synthesized motion quality. This work demonstrates that, with the same\nnumber of experts and parameters, the pruned model produces less motion\nartifacts than the dense model and the learned high-level motion features are\nsimilar for both\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_A/0/1/0/all/0/1\">Antoine Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubens_N/0/1/0/all/0/1\">Nathan Hubens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laraba_S/0/1/0/all/0/1\">Sohaib Laraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of chicken egg fertility using SVM classifier based on first-order statistical feature extraction. (arXiv:2201.04063v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04063","description":"<p>This study aims to identify chicken eggs fertility using the support vector\nmachine (SVM) classifier method. The classification basis used the first-order\nstatistical (FOS) parameters as feature extraction in the identification\nprocess. This research was developed based on the process's identification\nprocess, which is still manual (conventional). Although currently there are\nmany technologies in the identification process, they still need development.\nThus, this research is one of the developments in the field of image processing\ntechnology. The sample data uses datasets from previous studies with a total of\n100 egg images. The egg object in the image is a single object. From these\ndata, the classification of each fertile and infertile egg is 50 image data.\nChicken egg image data became input in image processing, with the initial\nprocess is segmentation. This initial segmentation aims to get the cropped\nimage according to the object. The cropped image is repaired using image\npreprocessing with grayscaling and image enhancement methods. This method\n(image enhancement) used two combination methods: contrast limited adaptive\nhistogram equalization (CLAHE) and histogram equalization (HE). The improved\nimage becomes the input for feature extraction using the FOS method. The FOS\nuses five parameters, namely mean, entropy, variance, skewness, and kurtosis.\nThe five parameters entered into the SVM classifier method to identify the\nfertility of chicken eggs. The results of these experiments, the method\nproposed in the identification process has a success percentage of 84.57%.\nThus, the implementation of this method can be used as a reference for future\nresearch improvements. In addition, it may be possible to use a second-order\nfeature extraction method to improve its accuracy and improve supervised\nlearning for classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saifullah_S/0/1/0/all/0/1\">Shoffan Saifullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suryotomo_A/0/1/0/all/0/1\">Andiko Putro Suryotomo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Denoise Raw Mobile UI Layouts for ImprovingDatasets at Scale. (arXiv:2201.04100v1 [cs.HC])","link":"http://arxiv.org/abs/2201.04100","description":"<p>The layout of a mobile screen is a critical data source for UI designresearch\nand semantic understanding of the screen. However, UIlayouts in existing\ndatasets are often noisy, have mismatches withtheir visual representation, or\nconsists of generic or app-specifictypes that are difficult to analyze and\nmodel. In this paper, wepropose the CLAY pipeline that uses a deep learning\napproach fordenoising UI layouts, allowing us to automatically improve\nexistingmobile UI layout datasets at scale. Our pipeline takes both\nthescreenshot and the raw UI layout, and annotates the raw layout byremoving\nincorrect nodes and assigning a semantically meaningfultype to each node. To\nexperiment with our data-cleaning pipeline,we create the CLAY dataset of 59,555\nhuman-annotated screenlayouts, based on screenshots and raw layouts from Rico,\na publicmobile UI corpus. Our deep models achieve high accuracy withF1 scores\nof 82.7% for detecting layout objects that do not have avalid visual\nrepresentation and 85.9% for recognizing object types,which significantly\noutperforms a heuristic baseline. Our work laysa foundation for creating\nlarge-scale high quality UI layout datasetsfor data-driven mobile UI research\nand reduces the need of manuallabeling efforts that are prohibitively\nexpensive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baechler_G/0/1/0/all/0/1\">Gilles Baechler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tragut_M/0/1/0/all/0/1\">Manuel Tragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DM-VIO: Delayed Marginalization Visual-Inertial Odometry. (arXiv:2201.04114v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04114","description":"<p>We present DM-VIO, a monocular visual-inertial odometry system based on two\nnovel techniques called delayed marginalization and pose graph bundle\nadjustment. DM-VIO performs photometric bundle adjustment with a dynamic weight\nfor visual residuals. We adopt marginalization, which is a popular strategy to\nkeep the update time constrained, but it cannot easily be reversed, and\nlinearization points of connected variables have to be fixed. To overcome this\nwe propose delayed marginalization: The idea is to maintain a second factor\ngraph, where marginalization is delayed. This allows us to later readvance this\ndelayed graph, yielding an updated marginalization prior with new and\nconsistent linearization points. In addition, delayed marginalization enables\nus to inject IMU information into already marginalized states. This is the\nfoundation of the proposed pose graph bundle adjustment, which we use for IMU\ninitialization. In contrast to prior works on IMU initialization, it is able to\ncapture the full photometric uncertainty, improving the scale estimation. In\norder to cope with initially unobservable scale, we continue to optimize scale\nand gravity direction in the main system after IMU initialization is complete.\nWe evaluate our system on the EuRoC, TUM-VI, and 4Seasons datasets, which\ncomprise flying drone, large-scale handheld, and automotive scenarios. Thanks\nto the proposed IMU initialization, our system exceeds the state of the art in\nvisual-inertial odometry, even outperforming stereo-inertial methods while\nusing only a single camera and IMU. The code will be published at\n<a href=\"http://vision.in.tum.de/dm-vio\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stumberg_L/0/1/0/all/0/1\">Lukas von Stumberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v1 [cs.LG])","link":"http://arxiv.org/abs/2201.04122","description":"<p>Recent multi-task learning research argues against unitary scalarization,\nwhere training simply minimizes the sum of the task losses. Several ad-hoc\nmulti-task optimization algorithms have instead been proposed, inspired by\nvarious hypotheses about what makes multi-task settings difficult. The majority\nof these optimizers require per-task gradients, and introduce significant\nmemory, runtime, and implementation overhead. We present a theoretical analysis\nsuggesting that many specialized multi-task optimizers can be interpreted as\nforms of regularization. Moreover, we show that, when coupled with standard\nregularization and stabilization techniques from single-task learning, unitary\nscalarization matches or improves upon the performance of complex multi-task\noptimizers in both supervised and reinforcement learning settings. We believe\nour results call for a critical reevaluation of recent research in the area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurin_V/0/1/0/all/0/1\">Vitaly Kurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palma_A/0/1/0/all/0/1\">Alessandro De Palma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostrikov_I/0/1/0/all/0/1\">Ilya Kostrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"gDNA: Towards Generative Detailed Neural Avatars. (arXiv:2201.04123v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04123","description":"<p>To make 3D human avatars widely available, we must be able to generate a\nvariety of 3D virtual humans with varied identities and shapes in arbitrary\nposes. This task is challenging due to the diversity of clothed body shapes,\ntheir complex articulations, and the resulting rich, yet stochastic geometric\ndetail in clothing. Hence, current methods to represent 3D people do not\nprovide a full generative model of people in clothing. In this paper, we\npropose a novel method that learns to generate detailed 3D shapes of people in\na variety of garments with corresponding skinning weights. Specifically, we\ndevise a multi-subject forward skinning module that is learned from only a few\nposed, un-rigged scans per subject. To capture the stochastic nature of\nhigh-frequency details in garments, we leverage an adversarial loss formulation\nthat encourages the model to capture the underlying statistics. We provide\nempirical evidence that this leads to realistic generation of local details\nsuch as wrinkles. We show that our model is able to generate natural human\navatars wearing diverse and detailed clothing. Furthermore, we show that our\nmethod can be used on the task of fitting human models to raw scans,\noutperforming the previous state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianjian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinlong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video. (arXiv:2201.04127v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04127","description":"<p>We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on\na given monocular video of a human performing complex body motions, e.g. a\nvideo from YouTube. Our method enables pausing the video at any frame and\nrendering the subject from arbitrary new camera viewpoints or even a full\n360-degree camera path for that particular frame and body pose. This task is\nparticularly challenging, as it requires synthesizing photorealistic details of\nthe body, as seen from various camera angles that may not exist in the input\nvideo, as well as synthesizing fine details such as cloth folds and facial\nappearance. Our method optimizes for a volumetric representation of the person\nin a canonical T-pose, in concert with a motion field that maps the estimated\ncanonical representation to every frame of the video via backward warps. The\nmotion field is decomposed into skeletal rigid and non-rigid motions, produced\nby deep networks. We show significant performance improvements over prior work,\nand compelling examples of free-viewpoint renderings from monocular video of\nmoving humans in challenging uncontrolled capture scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chung-Yi Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemelmacher_Shlizerman_I/0/1/0/all/0/1\">Ira Kemelmacher-Shlizerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent 3D Attentional Networks for End-to-End Active Object Recognition. (arXiv:1610.04308v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1610.04308","description":"<p>Active vision is inherently attention-driven: The agent actively selects\nviews to attend in order to fast achieve the vision task while improving its\ninternal representation of the scene being observed. Inspired by the recent\nsuccess of attention-based models in 2D vision tasks based on single RGB\nimages, we propose to address the multi-view depth-based active object\nrecognition using attention mechanism, through developing an end-to-end\nrecurrent 3D attentional network. The architecture takes advantage of a\nrecurrent neural network (RNN) to store and update an internal representation.\nOur model, trained with 3D shape datasets, is able to iteratively attend to the\nbest views targeting an object of interest for recognizing it. To realize 3D\nview selection, we derive a 3D spatial transformer network which is\ndifferentiable for training with backpropagation, achieving much faster\nconvergence than the reinforcement learning employed by most existing\nattention-based models. Experiments show that our method, with only depth\ninput, achieves state-of-the-art next-best-view performance in time efficiency\nand recognition accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lintao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate. (arXiv:1910.04858v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1910.04858","description":"<p>Uncertainty estimation is an essential step in the evaluation of the\nrobustness for deep learning models in computer vision, especially when applied\nin risk-sensitive areas. However, most state-of-the-art deep learning models\neither fail to obtain uncertainty estimation or need significant modification\n(e.g., formulating a proper Bayesian treatment) to obtain it. Most previous\nmethods are not able to take an arbitrary model off the shelf and generate\nuncertainty estimation without retraining or redesigning it. To address this\ngap, we perform a systematic exploration into training-free uncertainty\nestimation for dense regression, an unrecognized yet important problem, and\nprovide a theoretical construction justifying such estimations. We propose\nthree simple and scalable methods to analyze the variance of outputs from a\ntrained network under tolerable perturbations: infer-transformation,\ninfer-noise, and infer-dropout. They operate solely during the inference,\nwithout the need to re-train, re-design, or fine-tune the models, as typically\nrequired by state-of-the-art uncertainty estimation methods. Surprisingly, even\nwithout involving such perturbations in training, our methods produce\ncomparable or even better uncertainty estimation when compared to\ntraining-required state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1\">Lu Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1\">Nir Shavit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition. (arXiv:2004.12349v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.12349","description":"<p>Recognizing objects and scenes are two challenging but essential tasks in\nimage understanding. In particular, the use of RGB-D sensors in handling these\ntasks has emerged as an important area of focus for better visual\nunderstanding. Meanwhile, deep neural networks, specifically convolutional\nneural networks (CNNs), have become widespread and have been applied to many\nvisual tasks by replacing hand-crafted features with effective deep features.\nHowever, it is an open problem how to exploit deep features from a multi-layer\nCNN model effectively. In this paper, we propose a novel two-stage framework\nthat extracts discriminative feature representations from multi-modal RGB-D\nimages for object and scene recognition tasks. In the first stage, a pretrained\nCNN model has been employed as a backbone to extract visual features at\nmultiple levels. The second stage maps these features into high level\nrepresentations with a fully randomized structure of recursive neural networks\n(RNNs) efficiently. To cope with the high dimensionality of CNN activations, a\nrandom weighted pooling scheme has been proposed by extending the idea of\nrandomness in RNNs. Multi-modal fusion has been performed through a soft voting\napproach by computing weights based on individual recognition confidences (i.e.\nSVM scores) of RGB and depth streams separately. This produces consistent class\nlabel estimation in final RGB-D classification performance. Extensive\nexperiments verify that fully randomized structure in RNN stage encodes CNN\nactivations to discriminative solid features successfully. Comparative\nexperimental results on the popular Washington RGB-D Object and SUN RGB-D Scene\ndatasets show that the proposed approach achieves superior or on-par\nperformance compared to state-of-the-art methods both in object and scene\nrecognition tasks. Code is available at\nhttps://github.com/acaglayan/CNN_randRNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caglayan_A/0/1/0/all/0/1\">Ali Caglayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imamoglu_N/0/1/0/all/0/1\">Nevrez Imamoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_A/0/1/0/all/0/1\">Ahmet Burak Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_R/0/1/0/all/0/1\">Ryosuke Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MED-TEX: Transferring and Explaining Knowledge with Less Data from Pretrained Medical Imaging Models. (arXiv:2008.02593v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.02593","description":"<p>Deep learning methods usually require a large amount of training data and\nlack interpretability. In this paper, we propose a novel knowledge distillation\nand model interpretation framework for medical image classification that\njointly solves the above two issues. Specifically, to address the data-hungry\nissue, a small student model is learned with less data by distilling knowledge\nfrom a cumbersome pretrained teacher model. To interpret the teacher model and\nassist the learning of the student, an explainer module is introduced to\nhighlight the regions of an input that are important for the predictions of the\nteacher model. Furthermore, the joint framework is trained by a principled way\nderived from the information-theoretic perspective. Our framework outperforms\non the knowledge distillation and model interpretation tasks compared to\nstate-of-the-art methods on a fundus dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Duc_T/0/1/0/all/0/1\">Thanh Nguyen-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex-valued Iris Recognition Network. (arXiv:2011.11198v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11198","description":"<p>In this work, we design a fully complex-valued neural network for the task of\niris recognition. Unlike the problem of general object recognition, where\nreal-valued neural networks can be used to extract pertinent features, iris\nrecognition depends on the extraction of both phase and magnitude information\nfrom the input iris texture in order to better represent its biometric content.\nThis necessitates the extraction and processing of phase information that\ncannot be effectively handled by a real-valued neural network. In this regard,\nwe design a fully complex-valued neural network that can better capture the\nmulti-scale, multi-resolution, and multi-orientation phase and amplitude\nfeatures of the iris texture. We show a strong correspondence of the proposed\ncomplex-valued iris recognition network with Gabor wavelets that are used to\ngenerate the classical IrisCode; however, the proposed method enables a new\ncapability of automatic complex-valued feature learning that is tailored for\niris recognition. We conduct experiments on three benchmark datasets -\nND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit\nof the proposed network for the task of iris recognition. We exploit\nvisualization schemes to convey how the complex-valued network, when compared\nto standard real-valued networks, extracts fundamentally different features\nfrom the iris texture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie. (arXiv:2103.04715v5 [stat.ME] UPDATED)","link":"http://arxiv.org/abs/2103.04715","description":"<p>Since the seminal work of Venkatakrishnan et al. (2013), Plug &amp; Play (PnP)\nmethods have become ubiquitous in Bayesian imaging. These methods derive\nMinimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for\ninverse problems in imaging by combining an explicit likelihood function with a\nprior that is implicitly defined by an image denoising algorithm. The PnP\nalgorithms proposed in the literature mainly differ in the iterative schemes\nthey use for optimisation or for sampling. In the case of optimisation schemes,\nsome recent works guarantee the convergence to a fixed point, albeit not\nnecessarily a MAP estimate. In the case of sampling schemes, to the best of our\nknowledge, there is no known proof of convergence. There also remain important\nopen questions regarding whether the underlying Bayesian models and estimators\nare well defined, well-posed, and have the basic regularity properties required\nto support these numerical schemes. To address these limitations, this paper\ndevelops theory, methods, and provably convergent algorithms for performing\nBayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA\n(Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference;\nand 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent\nresults on the quantitative convergence of Markov chains, we establish detailed\nconvergence guarantees for these two algorithms under realistic assumptions on\nthe denoising operators used, with special attention to denoisers based on deep\nneural networks. We also show that these algorithms approximately target a\ndecision-theoretically optimal Bayesian model that is well-posed. The proposed\nalgorithms are demonstrated on several canonical problems such as image\ndeblurring, inpainting, and denoising, where they are used for point estimation\nas well as for uncertainty visualisation and quantification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Laumont_R/0/1/0/all/0/1\">R&#xe9;mi Laumont</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1\">Valentin de Bortoli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delon_J/0/1/0/all/0/1\">Julie Delon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pereyra_M/0/1/0/all/0/1\">Marcelo Pereyra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation. (arXiv:2103.14304v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14304","description":"<p>Despite the great progress in 3D human pose estimation from videos, it is\nstill an open problem to take full advantage of a redundant 2D pose sequence to\nlearn representative representations for generating one 3D pose. To this end,\nwe propose an improved Transformer-based architecture, called Strided\nTransformer, which simply and effectively lifts a long sequence of 2D joint\nlocations to a single 3D pose. Specifically, a Vanilla Transformer Encoder\n(VTE) is adopted to model long-range dependencies of 2D pose sequences. To\nreduce the redundancy of the sequence, fully-connected layers in the\nfeed-forward network of VTE are replaced with strided convolutions to\nprogressively shrink the sequence length and aggregate information from local\ncontexts. The modified VTE is termed as Strided Transformer Encoder (STE),\nwhich is built upon the outputs of VTE. STE not only effectively aggregates\nlong-range information to a single-vector representation in a hierarchical\nglobal and local fashion, but also significantly reduces the computation cost.\nFurthermore, a full-to-single supervision scheme is designed at both full\nsequence and single target frame scales applied to the outputs of VTE and STE,\nrespectively. This scheme imposes extra temporal smoothness constraints in\nconjunction with the single target frame supervision and hence helps produce\nsmoother and more accurate 3D poses. The proposed Strided Transformer is\nevaluated on two challenging benchmark datasets, Human3.6M and HumanEva-I, and\nachieves state-of-the-art results with fewer parameters. Code and models are\navailable at \\url{https://github.com/Vegetebird/StridedTransformer-Pose3D}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runwei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynO: Dynamic Onloading of Deep Neural Networks from Cloud to Device. (arXiv:2104.09949v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2104.09949","description":"<p>Recently, there has been an explosive growth of mobile and embedded\napplications using convolutional neural networks(CNNs). To alleviate their\nexcessive computational demands, developers have traditionally resorted to\ncloud offloading, inducing high infrastructure costs and a strong dependence on\nnetworking conditions. On the other end, the emergence of powerful SoCs is\ngradually enabling on-device execution. Nonetheless, low- and mid-tier\nplatforms still struggle to run state-of-the-art CNNs sufficiently. In this\npaper, we present DynO, a distributed inference framework that combines the\nbest of both worlds to address several challenges, such as device\nheterogeneity, varying bandwidth and multi-objective requirements. Key\ncomponents that enable this are its novel CNN-specific data packing method,\nwhich exploits the variability of precision needs in different parts of the CNN\nwhen onloading computation, and its novel scheduler that jointly tunes the\npartition point and transferred data precision at run time to adapt inference\nto its execution environment. Quantitative evaluation shows that DynO\noutperforms the current state-of-the-art, improving throughput by over an order\nof magnitude over device-only execution and up to 7.9x over competing CNN\noffloading systems, with up to 60x less data transferred.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_M/0/1/0/all/0/1\">Mario Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1\">Stefanos Laskaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1\">Stylianos I. Venieris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1\">Ilias Leontiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BADet: Boundary-Aware 3D Object Detection from Point Clouds. (arXiv:2104.10330v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10330","description":"<p>Currently, existing state-of-the-art 3D object detectors are in two-stage\nparadigm. These methods typically comprise two steps: 1) Utilize a region\nproposal network to propose a handful of high-quality proposals in a bottom-up\nfashion. 2) Resize and pool the semantic features from the proposed regions to\nsummarize RoI-wise representations for further refinement. Note that these\nRoI-wise representations in step 2) are considered individually as uncorrelated\nentries when fed to following detection headers. Nevertheless, we observe these\nproposals generated by step 1) offset from ground truth somehow, emerging in\nlocal neighborhood densely with an underlying probability. Challenges arise in\nthe case where a proposal largely forsakes its boundary information due to\ncoordinate offset while existing networks lack corresponding information\ncompensation mechanism. In this paper, we propose $BADet$ for 3D object\ndetection from point clouds. Specifically, instead of refining each proposal\nindependently as previous works do, we represent each proposal as a node for\ngraph construction within a given cut-off threshold, associating proposals in\nthe form of local neighborhood graph, with boundary correlations of an object\nbeing explicitly exploited. Besides, we devise a lightweight Region Feature\nAggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise\nfeatures with expanding receptive fields for more informative RoI-wise\nrepresentations. We validate BADet both on widely used KITTI Dataset and highly\nchallenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par\nperformance on KITTI 3D detection leaderboard and ranks $1^{st}$ on $Moderate$\ndifficulty of $Car$ category on KITTI BEV detection leaderboard. The source\ncode is available at https://github.com/rui-qian/BADet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nnDetection: A Self-configuring Method for Medical Object Detection. (arXiv:2106.00817v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.00817","description":"<p>Simultaneous localisation and categorization of objects in medical images,\nalso referred to as medical object detection, is of high clinical relevance\nbecause diagnostic decisions often depend on rating of objects rather than e.g.\npixels. For this task, the cumbersome and iterative process of method\nconfiguration constitutes a major research bottleneck. Recently, nnU-Net has\ntackled this challenge for the task of image segmentation with great success.\nFollowing nnU-Net's agenda, in this work we systematize and automate the\nconfiguration process for medical object detection. The resulting\nself-configuring method, nnDetection, adapts itself without any manual\nintervention to arbitrary medical detection problems while achieving results en\npar with or superior to the state-of-the-art. We demonstrate the effectiveness\nof nnDetection on two public benchmarks, ADAM and LUNA16, and propose 11\nfurther medical object detection tasks on public data sets for comprehensive\nmethod evaluation. Code is at https://github.com/MIC-DKFZ/nnDetection .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1\">Michael Baumgartner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaeger_P/0/1/0/all/0/1\">Paul F. Jaeger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus H. Maier-Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-like Relational Models for Activity Recognition in Video. (arXiv:2107.05319v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05319","description":"<p>Video activity recognition by deep neural networks is impressive for many\nclasses. However, it falls short of human performance, especially for\nchallenging to discriminate activities. Humans differentiate these complex\nactivities by recognising critical spatio-temporal relations among explicitly\nrecognised objects and parts, for example, an object entering the aperture of a\ncontainer. Deep neural networks can struggle to learn such critical\nrelationships effectively. Therefore we propose a more human-like approach to\nactivity recognition, which interprets a video in sequential temporal phases\nand extracts specific relationships among objects and hands in those phases.\nRandom forest classifiers are learnt from these extracted relationships. We\napply the method to a challenging subset of the something-something dataset and\nachieve a more robust performance against neural network baselines on\nchallenging activities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrol_Cannon_J/0/1/0/all/0/1\">Joseph Chrol-Cannon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazic_R/0/1/0/all/0/1\">Ranko Lazic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhusoodanan_A/0/1/0/all/0/1\">Adithya Madhusoodanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2Looking: A Satellite Side-Looking Dataset for Building Change Detection. (arXiv:2107.09244v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09244","description":"<p>Building-change detection underpins many important applications, especially\nin the military and crisis-management domains. Recent methods used for change\ndetection have shifted towards deep learning, which depends on the quality of\nits training data. The assembly of large-scale annotated satellite imagery\ndatasets is therefore essential for global building-change surveillance.\nExisting datasets almost exclusively offer near-nadir viewing angles. This\nlimits the range of changes that can be detected. By offering larger\nobservation ranges, the scroll imaging mode of optical satellites presents an\nopportunity to overcome this restriction. This paper therefore introduces\nS2Looking, a building-change-detection dataset that contains large-scale\nside-looking satellite images captured at various off-nadir angles. The dataset\nconsists of 5000 bitemporal image pairs of rural areas and more than 65,920\nannotated instances of changes throughout the world. The dataset can be used to\ntrain deep-learning-based change-detection algorithms. It expands upon existing\ndatasets by providing (1) larger viewing angles; (2) large illumination\nvariances; and (3) the added complexity of rural images. To facilitate {the}\nuse of the dataset, a benchmark task has been established, and preliminary\ntests suggest that deep-learning algorithms find the dataset significantly more\nchallenging than the closest-competing near-nadir dataset, LEVIR-CD+. S2Looking\nmay therefore promote important advances in existing building-change-detection\nalgorithms. The dataset is available at https://github.com/S2Looking/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Donghai Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jiabao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shouye Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bitao Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphFPN: Graph Feature Pyramid Network for Object Detection. (arXiv:2108.00580v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00580","description":"<p>Feature pyramids have been proven powerful in image understanding tasks that\nrequire multi-scale features. State-of-the-art methods for multi-scale feature\nlearning focus on performing feature interactions across space and scales using\nneural networks with a fixed topology. In this paper, we propose graph feature\npyramid networks that are capable of adapting their topological structures to\nvarying intrinsic image structures and supporting simultaneous feature\ninteractions across all scales. We first define an image-specific superpixel\nhierarchy for each input image to represent its intrinsic image structures. The\ngraph feature pyramid network inherits its structure from this superpixel\nhierarchy. Contextual and hierarchical layers are designed to achieve feature\ninteractions within the same scale and across different scales. To make these\nlayers more powerful, we introduce two types of local channel attention for\ngraph neural networks by generalizing global channel attention for\nconvolutional neural networks. The proposed graph feature pyramid network can\nenhance the multiscale features from a convolutional feature pyramid network.\nWe evaluate our graph feature pyramid network in the object detection task by\nintegrating it into the Faster R-CNN algorithm. The modified algorithm\noutperforms not only previous state-of-the-art feature pyramid-based methods\nwith a clear margin but also other popular detection methods on both MS-COCO\n2017 validation and test datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on IQA. (arXiv:2109.00347v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.00347","description":"<p>Image quality assessment(IQA) is of increasing importance for image-based\napplications. Its purpose is to establish a model that can replace humans for\naccurately evaluating image quality. According to whether the reference image\nis complete and available, image quality evaluation can be divided into three\ncategories: full-reference(FR), reduced-reference(RR), and non-reference(NR)\nimage quality assessment. Due to the vigorous development of deep learning and\nthe widespread attention of researchers, several non-reference image quality\nassessment methods based on deep learning have been proposed in recent years,\nand some have exceeded the performance of reduced -reference or even\nfull-reference image quality assessment models. This article will review the\nconcepts and metrics of image quality assessment and also video quality\nassessment, briefly introduce some methods of full-reference and semi-reference\nimage quality assessment, and focus on the non-reference image quality\nassessment methods based on deep learning. Then introduce the commonly used\nsynthetic database and real-world database. Finally, summarize and present\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lanjiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Wide-area, Low-latency, and Power-efficient 6-DoF Pose Tracking System for Rigid Objects. (arXiv:2109.07428v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.07428","description":"<p>Position sensitive detectors (PSDs) offer possibility to track single active\nmarker's two (or three) degrees of freedom (DoF) position with a high accuracy,\nwhile having a fast response time with high update frequency and low latency,\nall using a very simple signal processing circuit. However they are not\nparticularly suitable for 6-DoF object pose tracking system due to lack of\norientation measurement, limited tracking range, and sensitivity to\nenvironmental variation. We propose a novel 6-DoF pose tracking system for a\nrigid object tracking requiring a single active marker. The proposed system\nuses a stereo-based PSD pair and multiple Inertial Measurement Units (IMUs).\nThis is done based on a practical approach to identify and control the power of\nInfrared-Light Emitting Diode (IR-LED) active markers, with an aim to increase\nthe tracking work space and reduce the power consumption. Our proposed tracking\nsystem is validated with three different work space sizes and for static and\ndynamic positional accuracy using robotic arm manipulator with three different\ndynamic motion patterns. The results show that the static position\nroot-mean-square (RMS) error is 0.6mm. The dynamic position RMS error is\n0.7-0.9mm. The orientation RMS error is between 0.04 and 0.9 degree at varied\ndynamic motion. Overall, our proposed tracking system is capable of tracking a\nrigid object pose with sub-millimeter accuracy at the mid range of the work\nspace and sub-degree accuracy for all work space under a lab setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Ankur Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansi_T/0/1/0/all/0/1\">Tommaso Mansi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamen_A/0/1/0/all/0/1\">Ali Kamen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attacking Video Recognition Models with Bullet-Screen Comments. (arXiv:2110.15629v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.15629","description":"<p>Recent research has demonstrated that Deep Neural Networks (DNNs) are\nvulnerable to adversarial patches which introduce perceptible but localized\nchanges to the input. Nevertheless, existing approaches have focused on\ngenerating adversarial patches on images, their counterparts in videos have\nbeen less explored. Compared with images, attacking videos is much more\nchallenging as it needs to consider not only spatial cues but also temporal\ncues. To close this gap, we introduce a novel adversarial attack in this paper,\nthe bullet-screen comment (BSC) attack, which attacks video recognition models\nwith BSCs. Specifically, adversarial BSCs are generated with a Reinforcement\nLearning (RL) framework, where the environment is set as the target model and\nthe agent plays the role of selecting the position and transparency of each\nBSC. By continuously querying the target models and receiving feedback, the\nagent gradually adjusts its selection strategies in order to achieve a high\nfooling rate with non-overlapping BSCs. As BSCs can be regarded as a kind of\nmeaningful patch, adding it to a clean video will not affect people' s\nunderstanding of the video content, nor will arouse people' s suspicion. We\nconduct extensive experiments to verify the effectiveness of the proposed\nmethod. On both UCF-101 and HMDB-51 datasets, our BSC attack method can achieve\nabout 90\\% fooling rate when attacking three mainstream video recognition\nmodels, while only occluding \\textless 8\\% areas in the video. Our code is\navailable at https://github.com/kay-ck/BSC-attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhipeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning of Time-Frequency Attention Mechanism for Automatic Modulation Recognition. (arXiv:2111.03258v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2111.03258","description":"<p>Recent learning-based image classification and speech recognition approaches\nmake extensive use of attention mechanisms to achieve state-of-the-art\nrecognition power, which demonstrates the effectiveness of attention\nmechanisms. Motivated by the fact that the frequency and time information of\nmodulated radio signals are crucial for modulation mode recognition, this paper\nproposes a time-frequency attention mechanism for a convolutional neural\nnetwork (CNN)-based modulation recognition framework. The proposed\ntime-frequency attention module is designed to learn which channel, frequency\nand time information is more meaningful in CNN for modulation recognition. We\nanalyze the effectiveness of the proposed time-frequency attention mechanism\nand compare the proposed method with two existing learning-based methods.\nExperiments on an open-source modulation recognition dataset show that the\nrecognition performance of the proposed framework is better than those of the\nframework without time-frequency attention and existing learning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1\">Shangao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuan Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yi Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NarrationBot and InfoBot: A Hybrid System for Automated Video Description. (arXiv:2111.03994v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2111.03994","description":"<p>Video accessibility is crucial for blind and low vision users for equitable\nengagements in education, employment, and entertainment. Despite the\navailability of professional and amateur services and tools, most\nhuman-generated descriptions are expensive and time consuming. Moreover, the\nrate of human-generated descriptions cannot match the speed of video\nproduction. To overcome the increasing gaps in video accessibility, we\ndeveloped a hybrid system of two tools to 1) automatically generate\ndescriptions for videos and 2) provide answers or additional descriptions in\nresponse to user queries on a video. Results from a mixed-methods study with 26\nblind and low vision individuals show that our system significantly improved\nuser comprehension and enjoyment of selected videos when both tools were used\nin tandem. In addition, participants reported no significant difference in\ntheir ability to understand videos when presented with autogenerated\ndescriptions versus human-revised autogenerated descriptions. Our results\ndemonstrate user enthusiasm about the developed system and its promise for\nproviding customized access to videos. We discuss the limitations of the\ncurrent work and provide recommendations for the future development of\nautomated video description tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ihorn_S/0/1/0/all/0/1\">Shasta Ihorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siu_Y/0/1/0/all/0/1\">Yue-Ting Siu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodi_A/0/1/0/all/0/1\">Aditya Bodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narins_L/0/1/0/all/0/1\">Lothar Narins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castanon_J/0/1/0/all/0/1\">Jose M. Castanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kant_Y/0/1/0/all/0/1\">Yash Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Abhishek Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1\">Ilmi Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazli_P/0/1/0/all/0/1\">Pooyan Fazli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Diverse Model Selection for Accessible Transfer Learning. (arXiv:2111.06977v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.06977","description":"<p>With the preponderance of pretrained deep learning models available\noff-the-shelf from model banks today, finding the best weights to fine-tune to\nyour use-case can be a daunting task. Several methods have recently been\nproposed to find good models for transfer learning, but they either don't scale\nwell to large model banks or don't perform well on the diversity of\noff-the-shelf models. Ideally the question we want to answer is, \"given some\ndata and a source model, can you quickly predict the model's accuracy after\nfine-tuning?\" In this paper, we formalize this setting as \"Scalable Diverse\nModel Selection\" and propose several benchmarks for evaluating on this task. We\nfind that existing model selection and transferability estimation methods\nperform poorly here and analyze why this is the case. We then introduce simple\ntechniques to improve the performance and speed of these algorithms. Finally,\nwe iterate on existing methods to create PARC, which outperforms all other\nmethods on diverse model selection. We have released the benchmarks and method\ncode in hope to inspire future work in model selection for accessible transfer\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bolya_D/0/1/0/all/0/1\">Daniel Bolya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittapalli_R/0/1/0/all/0/1\">Rohit Mittapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Images Meet Crowdsourced Trajectories: A New Approach to Robust Road Extraction. (arXiv:2111.15119v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15119","description":"<p>Land remote sensing analysis is a crucial research in earth science. In this\nwork, we focus on a challenging task of land analysis, i.e., automatic\nextraction of traffic roads from remote sensing data, which has widespread\napplications in urban development and expansion estimation. Nevertheless,\nconventional methods either only utilized the limited information of aerial\nimages, or simply fused multimodal information (e.g., vehicle trajectories),\nthus cannot well recognize unconstrained roads. To facilitate this problem, we\nintroduce a novel neural network framework termed Cross-Modal Message\nPropagation Network (CMMPNet), which fully benefits the complementary different\nmodal data (i.e., aerial images and crowdsourced trajectories). Specifically,\nCMMPNet is composed of two deep Auto-Encoders for modality-specific\nrepresentation learning and a tailor-designed Dual Enhancement Module for\ncross-modal representation refinement. In particular, the complementary\ninformation of each modality is comprehensively extracted and dynamically\npropagated to enhance the representation of another modality. Extensive\nexperiments on three real-world benchmarks demonstrate the effectiveness of our\nCMMPNet for robust road extraction benefiting from blending different modal\ndata, either using image and trajectory data or image and Lidar data. From the\nexperimental results, we observe that the proposed approach outperforms current\nstate-of-the-art methods by large margins.Our source code is resealed on the\nproject page \\url{<a href=\"http://lingboliu.com/multimodal\">this http URL</a> road extraction.html}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zewei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIVE: Evaluating the Human Interpretability of Visual Explanations. (arXiv:2112.03184v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03184","description":"<p>As machine learning is increasingly applied to high-impact, high-risk\ndomains, there have been a number of new methods aimed at making AI models more\nhuman interpretable. Despite the recent growth of interpretability work, there\nis a lack of systematic evaluation of proposed techniques. In this work, we\npropose a novel human evaluation framework HIVE (Human Interpretability of\nVisual Explanations) for diverse interpretability methods in computer vision;\nto the best of our knowledge, this is the first work of its kind. We argue that\nhuman studies should be the gold standard in properly evaluating how\ninterpretable a method is to human users. While human studies are often avoided\ndue to challenges associated with cost, study design, and cross-method\ncomparison, we describe how our framework mitigates these issues and conduct\nIRB-approved studies of four methods that represent the diversity of\ninterpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our results\nsuggest that explanations (regardless of if they are actually correct) engender\nhuman trust, yet are not distinct enough for users to distinguish between\ncorrect and incorrect predictions. Lastly, we also open-source our framework to\nenable future studies and to encourage more human-centered approaches to\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunnie S. Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_N/0/1/0/all/0/1\">Nicole Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_V/0/1/0/all/0/1\">Vikram V. Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1\">Ruth Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SalFBNet: Learning Pseudo-Saliency Distribution via Feedback Convolutional Networks. (arXiv:2112.03731v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03731","description":"<p>Feed-forward only convolutional neural networks (CNNs) may ignore intrinsic\nrelationships and potential benefits of feedback connections in vision tasks\nsuch as saliency detection, despite their significant representation\ncapabilities. In this work, we propose a feedback-recursive convolutional\nframework (SalFBNet) for saliency detection. The proposed feedback model can\nlearn abundant contextual representations by bridging a recursive pathway from\nhigher-level feature blocks to low-level layer. Moreover, we create a\nlarge-scale Pseudo-Saliency dataset to alleviate the problem of data deficiency\nin saliency detection. We first use the proposed feedback model to learn\nsaliency distribution from pseudo-ground-truth. Afterwards, we fine-tune the\nfeedback model on existing eye-fixation datasets. Furthermore, we present a\nnovel Selective Fixation and Non-Fixation Error (sFNE) loss to make proposed\nfeedback model better learn distinguishable eye-fixation-based features.\nExtensive experimental results show that our SalFBNet with fewer parameters\nachieves competitive results on the public saliency detection benchmarks, which\ndemonstrate the effectiveness of proposed feedback model and Pseudo-Saliency\ndata. Source codes and Pseudo-Saliency dataset can be found at\nhttps://github.com/gqding/SalFBNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guanqun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imamoglu_N/0/1/0/all/0/1\">Nevrez Imamoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caglayan_A/0/1/0/all/0/1\">Ali Caglayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakawa_M/0/1/0/all/0/1\">Masahiro Murakawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_R/0/1/0/all/0/1\">Ryosuke Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPRAL: Collaborative Panoptic-Regional Active Learning for Semantic Segmentation. (arXiv:2112.05975v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05975","description":"<p>Acquiring the most representative examples via active learning (AL) can\nbenefit many data-dependent computer vision tasks by minimizing efforts of\nimage-level or pixel-wise annotations. In this paper, we propose a novel\nCollaborative Panoptic-Regional Active Learning framework (CPRAL) to address\nthe semantic segmentation task. For a small batch of images initially sampled\nwith pixel-wise annotations, we employ panoptic information to initially select\nunlabeled samples. Considering the class imbalance in the segmentation dataset,\nwe import a Regional Gaussian Attention module (RGA) to achieve\nsemantics-biased selection. The subset is highlighted by vote entropy and then\nattended by Gaussian kernels to maximize the biased regions. We also propose a\nContextual Labels Extension (CLE) to boost regional annotations with contextual\nattention guidance. With the collaboration of semantics-agnostic panoptic\nmatching and regionbiased selection and extension, our CPRAL can strike a\nbalance between labeling efforts and performance and compromise the semantics\ndistribution. We perform extensive experiments on Cityscapes and BDD10K\ndatasets and show that CPRAL outperforms the cutting-edge methods with\nimpressive results and less labeling proportion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jincheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhenjun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRIDA -- Generative Feature Replay for Incremental Domain Adaptation. (arXiv:2112.14316v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14316","description":"<p>We tackle the novel problem of incremental unsupervised domain adaptation\n(IDA) in this paper. We assume that a labeled source domain and different\nunlabeled target domains are incrementally observed with the constraint that\ndata corresponding to the current domain is only available at a time. The goal\nis to preserve the accuracies for all the past domains while generalizing well\nfor the current domain. The IDA setup suffers due to the abrupt differences\namong the domains and the unavailability of past data including the source\ndomain. Inspired by the notion of generative feature replay, we propose a novel\nframework called Feature Replay based Incremental Domain Adaptation (FRIDA)\nwhich leverages a new incremental generative adversarial network (GAN) called\ndomain-generic auxiliary classification GAN (DGAC-GAN) for producing\ndomain-specific feature representations seamlessly. For domain alignment, we\npropose a simple extension of the popular domain adversarial neural network\n(DANN) called DANN-IB which encourages discriminative domain-invariant and\ntask-relevant feature learning. Experimental results on Office-Home,\nOffice-CalTech, and DomainNet datasets confirm that FRIDA maintains superior\nstability-plasticity trade-off than the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakshit_S/0/1/0/all/0/1\">Sayan Rakshit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_A/0/1/0/all/0/1\">Anwesh Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavhan_R/0/1/0/all/0/1\">Ruchika Chavhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1\">Gemma Roig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01293","description":"<p>This paper presents a transformer-based Siamese network architecture\n(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of\nco-registered remote sensing images. Different from recent CD frameworks, which\nare based on fully convolutional networks (ConvNets), the proposed method\nunifies hierarchically structured transformer encoder with Multi-Layer\nPerception (MLP) decoder in a Siamese network architecture to efficiently\nrender multi-scale long-range details required for accurate CD. Experiments on\ntwo CD datasets show that the proposed end-to-end trainable ChangeFormer\narchitecture achieves better CD performance than previous counterparts. Our\ncode is available at https://github.com/wgcban/ChangeFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02428","description":"<p>Today, deep convolutional neural networks (CNNs) have demonstrated\nstate-of-the-art performance for medical image segmentation, on various imaging\nmodalities and tasks. Despite early success, segmentation networks may still\ngenerate anatomically aberrant segmentations, with holes or inaccuracies near\nthe object boundaries. To enforce anatomical plausibility, recent research\nstudies have focused on incorporating prior knowledge such as object shape or\nboundary, as constraints in the loss function. Prior integrated could be\nlow-level referring to reformulated representations extracted from the\nground-truth segmentations, or high-level representing external medical\ninformation such as the organ's shape or size. Over the past few years,\nprior-based losses exhibited a rising interest in the research field since they\nallow integration of expert knowledge while still being architecture-agnostic.\nHowever, given the diversity of prior-based losses on different medical imaging\nchallenges and tasks, it has become hard to identify what loss works best for\nwhich dataset. In this paper, we establish a benchmark of recent prior-based\nlosses for medical image segmentation. The main objective is to provide\nintuition onto which losses to choose given a particular task or dataset. To\nthis end, four low-level and high-level prior-based losses are selected. The\nconsidered losses are validated on 8 different datasets from a variety of\nmedical image segmentation challenges including the Decathlon, the ISLES and\nthe WMH challenge. Results show that whereas low-level prior-based losses can\nguarantee an increase in performance over the Dice loss baseline regardless of\nthe dataset characteristics, high-level prior-based losses can increase\nanatomical plausibility as per data characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jurdi_R/0/1/0/all/0/1\">Rosana El Jurdi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petitjean_C/0/1/0/all/0/1\">Caroline Petitjean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honeine_P/0/1/0/all/0/1\">Paul Honeine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdallah_F/0/1/0/all/0/1\">Fahed Abdallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy. (arXiv:2201.02867v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02867","description":"<p>Recent breakthroughs in high resolution imaging of biomolecules in solution\nwith cryo-electron microscopy (cryo-EM) have unlocked new doors for the\nreconstruction of molecular volumes, thereby promising further advances in\nbiology, chemistry, and pharmacological research amongst others. Despite\nsignificant headway, the immense challenges in cryo-EM data analysis remain\nlegion and intricately inter-disciplinary in nature, requiring insights from\nphysicists, structural biologists, computer scientists, statisticians, and\napplied mathematicians. Meanwhile, recent next-generation volume reconstruction\nalgorithms that combine generative modeling with end-to-end unsupervised deep\nlearning techniques have shown promising results on simulated data, but still\nface considerable hurdles when applied to experimental cryo-EM images. In light\nof the proliferation of such methods and given the interdisciplinary nature of\nthe task, we propose here a critical review of recent advances in the field of\ndeep generative modeling for high resolution cryo-EM volume reconstruction. The\npresent review aims to (i) compare and contrast these new methods, while (ii)\npresenting them from a perspective and using terminology familiar to scientists\nin each of the five aforementioned fields with no specific background in\ncryo-EM. The review begins with an introduction to the mathematical and\ncomputational challenges of deep generative models for cryo-EM volume\nreconstruction, along with an overview of the baseline methodology shared\nacross this class of algorithms. Having established the common thread weaving\nthrough these different models, we provide a practical comparison of these\nstate-of-the-art algorithms, highlighting their relative strengths and\nweaknesses, along with the assumptions that they rely on. This allows us to\nidentify bottlenecks in current methods and avenues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Donnat_C/0/1/0/all/0/1\">Claire Donnat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Levy_A/0/1/0/all/0/1\">Axel Levy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poitevin_F/0/1/0/all/0/1\">Frederic Poitevin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision. (arXiv:2201.02885v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02885","description":"<p>UAV-based image retrieval in modern agriculture enables gathering large\namounts of spatially referenced crop image data. In large-scale experiments,\nhowever, UAV images suffer from containing a multitudinous amount of crops in a\ncomplex canopy architecture. Especially for the observation of temporal\neffects, this complicates the recognition of individual plants over several\nimages and the extraction of relevant information tremendously. In this work,\nwe present a hands-on workflow for the automatized temporal and spatial\nidentification and individualization of crop images from UAVs abbreviated as\n\"cataloging\" based on comprehensible computer vision methods. We evaluate the\nworkflow on two real-world datasets. One dataset is recorded for observation of\nCercospora leaf spot - a fungal disease - in sugar beet over an entire growing\ncycle. The other one deals with harvest prediction of cauliflower plants. The\nplant catalog is utilized for the extraction of single plant images seen over\nmultiple time points. This gathers large-scale spatio-temporal image dataset\nthat in turn can be applied to train further machine learning models including\nvarious data layers. The presented approach improves analysis and\ninterpretation of UAV data in agriculture significantly. By validation with\nsome reference data, our method shows an accuracy that is similar to more\ncomplex deep learning-based recognition techniques. Our workflow is able to\nautomatize plant cataloging and training image extraction, especially for large\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunder_M/0/1/0/all/0/1\">Maurice G&#xfc;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamati_F/0/1/0/all/0/1\">Facundo R. Ispizua Yamati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1\">Jana Kierdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahlein_A/0/1/0/all/0/1\">Anne-Katrin Mahlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskMTL: Attribute prediction in masked facial images with deep multitask learning. (arXiv:2201.03002v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03002","description":"<p>Predicting attributes in the landmark free facial images is itself a\nchallenging task which gets further complicated when the face gets occluded due\nto the usage of masks. Smart access control gates which utilize identity\nverification or the secure login to personal electronic gadgets may utilize\nface as a biometric trait. Particularly, the Covid-19 pandemic increasingly\nvalidates the essentiality of hygienic and contactless identity verification.\nIn such cases, the usage of masks become more inevitable and performing\nattribute prediction helps in segregating the target vulnerable groups from\ncommunity spread or ensuring social distancing for them in a collaborative\nenvironment. We create a masked face dataset by efficiently overlaying masks of\ndifferent shape, size and textures to effectively model variability generated\nby wearing mask. This paper presents a deep Multi-Task Learning (MTL) approach\nto jointly estimate various heterogeneous attributes from a single masked\nfacial image. Experimental results on benchmark face attribute UTKFace dataset\ndemonstrate that the proposed approach supersedes in performance to other\ncompeting techniques. The source code is available at\nhttps://github.com/ritikajha/Attribute-prediction-in-masked-facial-images-with-deep-multitask-learning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1\">Prerana Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_V/0/1/0/all/0/1\">Vinay Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Ronak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_R/0/1/0/all/0/1\">Ritika Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanwadi_D/0/1/0/all/0/1\">Daneshwari Kankanwadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lall_B/0/1/0/all/0/1\">Brejesh Lall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification. (arXiv:2201.03194v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03194","description":"<p>Hierarchical multi-granularity classification (HMC) assigns hierarchical\nmulti-granularity labels to each object and focuses on encoding the label\nhierarchy, e.g., [\"Albatross\", \"Laysan Albatross\"] from coarse-to-fine levels.\nHowever, the definition of what is fine-grained is subjective, and the image\nquality may affect the identification. Thus, samples could be observed at any\nlevel of the hierarchy, e.g., [\"Albatross\"] or [\"Albatross\", \"Laysan\nAlbatross\"], and examples discerned at coarse categories are often neglected in\nthe conventional setting of HMC. In this paper, we study the HMC problem in\nwhich objects are labeled at any level of the hierarchy. The essential designs\nof the proposed method are derived from two motivations: (1) learning with\nobjects labeled at various levels should transfer hierarchical knowledge\nbetween levels; (2) lower-level classes should inherit attributes related to\nupper-level superclasses. The proposed combinatorial loss maximizes the\nmarginal probability of the observed ground truth label by aggregating\ninformation from related labels defined in the tree hierarchy. If the observed\nlabel is at the leaf level, the combinatorial loss further imposes the\nmulti-class cross-entropy loss to increase the weight of fine-grained\nclassification loss. Considering the hierarchical feature interaction, we\npropose a hierarchical residual network (HRN), in which granularity-specific\nfeatures from parent levels acting as residual connections are added to\nfeatures of children levels. Experiments on three commonly used datasets\ndemonstrate the effectiveness of our approach compared to the state-of-the-art\nHMC approaches and fine-grained visual classification (FGVC) methods exploiting\nthe label hierarchy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuntao Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}