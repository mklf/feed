{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"TamilEmo: Finegrained Emotion Detection Dataset for Tamil. (arXiv:2202.04725v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04725","description":"<p>Emotional Analysis from textual input has been considered both a challenging\nand interesting task in Natural Language Processing. However, due to the lack\nof datasets in low-resource languages (i.e. Tamil), it is difficult to conduct\nresearch of high standard in this area. Therefore we introduce this labelled\ndataset (a largest manually annotated dataset of more than 42k Tamil YouTube\ncomments, labelled for 31 emotions including neutral) for emotion recognition.\nThe goal of this dataset is to improve emotion detection in multiple downstream\ntasks in Tamil. We have also created three different groupings of our emotions\n(3-class, 7-class and 31-class) and evaluated the model's performance on each\ncategory of the grouping. Our MURIL-base model has achieved a 0.60 macro\naverage F1-score across our 3-class group dataset. With 7-class and 31-class\ngroups, the Random Forest model performed well with a macro average F1-scores\nof 0.42 and 0.29 respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasantharajan_C/0/1/0/all/0/1\">Charangan Vasantharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarasen_P/0/1/0/all/0/1\">Prasanna Kumar Kumarasen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponnusamy_R/0/1/0/all/0/1\">Rahul Ponnusamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangasamy_S/0/1/0/all/0/1\">Sathiyaraj Thangasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durairaj_T/0/1/0/all/0/1\">Thenmozhi Durairaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1\">John Phillip McCrae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Human Similarity Judgments Using Large Language Models. (arXiv:2202.04728v1 [cs.LG])","link":"http://arxiv.org/abs/2202.04728","description":"<p>Similarity judgments provide a well-established method for accessing mental\nrepresentations, with applications in psychology, neuroscience and machine\nlearning. However, collecting similarity judgments can be prohibitively\nexpensive for naturalistic datasets as the number of comparisons grows\nquadratically in the number of stimuli. One way to tackle this problem is to\nconstruct approximation procedures that rely on more accessible proxies for\npredicting similarity. Here we leverage recent advances in language models and\nonline recruitment, proposing an efficient domain-general procedure for\npredicting human similarity judgments based on text descriptions. Intuitively,\nsimilar stimuli are likely to evoke similar descriptions, allowing us to use\ndescription similarity to predict pairwise similarity judgments. Crucially, the\nnumber of descriptions required grows only linearly with the number of stimuli,\ndrastically reducing the amount of data required. We test this procedure on six\ndatasets of naturalistic images and show that our models outperform previous\napproaches based on visual information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1\">Theodore R. Sumers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedQAS: Privacy-aware machine reading comprehension with federated learning. (arXiv:2202.04742v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04742","description":"<p>Machine reading comprehension (MRC) of text data is one important task in\nNatural Language Understanding. It is a complex NLP problem with a lot of\nongoing research fueled by the release of the Stanford Question Answering\nDataset (SQuAD) and Conversational Question Answering (CoQA). It is considered\nto be an effort to teach computers how to \"understand\" a text, and then to be\nable to answer questions about it using deep learning. However, until now\nlarge-scale training on private text data and knowledge sharing has been\nmissing for this NLP task. Hence, we present FedQAS, a privacy-preserving\nmachine reading system capable of leveraging large-scale private data without\nthe need to pool those datasets in a central location. The proposed approach\ncombines transformer models and federated learning technologies. The system is\ndeveloped using the FEDn framework and deployed as a proof-of-concept alliance\ninitiative. FedQAS is flexible, language-agnostic, and allows intuitive\nparticipation and execution of local model training. In addition, we present\nthe architecture and implementation of the system, as well as provide a\nreference evaluation based on the SQUAD dataset, to showcase how it overcomes\ndata privacy issues and enables knowledge sharing between alliance members in a\nFederated learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ait_Mlouk_A/0/1/0/all/0/1\">Addi Ait-Mlouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alawadi_S/0/1/0/all/0/1\">Sadi Alawadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toor_S/0/1/0/all/0/1\">Salman Toor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellander_A/0/1/0/all/0/1\">Andreas Hellander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHAS: Approaching optimal Segmentation for End-to-End Speech Translation. (arXiv:2202.04774v1 [cs.SD])","link":"http://arxiv.org/abs/2202.04774","description":"<p>Speech translation models are unable to directly process long audios, like\nTED talks, which have to be split into shorter segments. Speech translation\ndatasets provide manual segmentations of the audios, which are not available in\nreal-world scenarios, and existing segmentation methods usually significantly\nreduce translation quality at inference time. To bridge the gap between the\nmanual segmentation of training and the automatic one at inference, we propose\nSupervised Hybrid Audio Segmentation (SHAS), a method that can effectively\nlearn the optimal segmentation from any manually segmented speech corpus.\nFirst, we train a classifier to identify the included frames in a segmentation,\nusing speech representations from a pre-trained wav2vec 2.0. The optimal\nsplitting points are then found by a probabilistic Divide-and-Conquer algorithm\nthat progressively splits at the frame of lowest probability until all segments\nare below a pre-specified length. Experiments on MuST-C and mTEDx show that the\ntranslation of the segments produced by our method approaches the quality of\nthe manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of\nthe manual segmentation's BLEU score, compared to the 87-93% of the best\nexisting methods. Our method is additionally generalizable to different domains\nand achieves high zero-shot performance in unseen languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04800","description":"<p>Humans have remarkable capacity to reason abductively and hypothesize about\nwhat lies beyond the literal content of an image. By identifying concrete\nvisual clues scattered throughout a scene, we almost can't help but draw\nprobable inferences beyond the literal scene based on our everyday experience\nand knowledge about the world. For example, if we see a \"20 mph\" sign alongside\na road, we might assume the street sits in a residential area (rather than on a\nhighway), even if no houses are pictured. Can machines perform similar visual\nreasoning?\n</p>\n<p>We present Sherlock, an annotated corpus of 103K images for testing machine\ncapacity for abductive reasoning beyond literal image contents. We adopt a\nfree-viewing paradigm: participants first observe and identify salient clues\nwithin images (e.g., objects, actions) and then provide a plausible inference\nabout the scene, given the clue. In total, we collect 363K (clue, inference)\npairs, which form a first-of-its-kind abductive visual reasoning dataset. Using\nour corpus, we test three complementary axes of abductive reasoning. We\nevaluate the capacity of models to: i) retrieve relevant inferences from a\nlarge candidate corpus; ii) localize evidence for inferences via bounding\nboxes, and iii) compare plausible inferences to match human judgments on a\nnewly-collected diagnostic corpus of 19K Likert-scale judgments. While we find\nthat fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong\nbaselines, significant headroom exists between model performance and human\nagreement. We provide analysis that points towards future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaPrompt: Adaptive Model Training for Prompt-based NLP. (arXiv:2202.04824v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04824","description":"<p>Prompt-based learning, with its capability to tackle zero-shot and few-shot\nNLP tasks, has gained much attention in community. The main idea is to bridge\nthe gap between NLP downstream tasks and language modeling (LM), by mapping\nthese tasks into natural language prompts, which are then filled by pre-trained\nlanguage models (PLMs). However, for prompt learning, there are still two\nsalient gaps between NLP tasks and pretraining. First, prompt information is\nnot necessarily sufficiently present during LM pretraining. Second,\ntask-specific data are not necessarily well represented during pretraining. We\naddress these two issues by proposing AdaPrompt, adaptively retrieving external\ndata for continual pretraining of PLMs by making use of both task and prompt\ncharacteristics. In addition, we make use of knowledge in Natural Language\nInference models for deriving adaptive verbalizers. Experimental results on\nfive NLP benchmarks show that AdaPrompt can improve over standard PLMs in\nfew-shot settings. In addition, in zero-shot settings, our method outperforms\nstandard prompt-based methods by up to 26.35\\% relative error reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Networks and Identity Drive Geographic Properties of the Diffusion of Linguistic Innovation. (arXiv:2202.04842v1 [cs.SI])","link":"http://arxiv.org/abs/2202.04842","description":"<p>Adoption of cultural innovation (e.g., music, beliefs, language) is often\ngeographically correlated, with adopters largely residing within the boundaries\nof relatively few well-studied, socially significant areas. These cultural\nregions are often hypothesized to be the result of either (i) identity\nperformance driving the adoption of cultural innovation, or (ii) homophily in\nthe networks underlying diffusion. In this study, we show that demographic\nidentity and network topology are both required to model the diffusion of\ninnovation, as they play complementary roles in producing its spatial\nproperties. We develop an agent-based model of cultural adoption, and validate\ngeographic patterns of transmission in our model against a novel dataset of\ninnovative words that we identify from a 10% sample of Twitter. Using our\nmodel, we are able to directly compare a combined network + identity model of\ndiffusion to simulated network-only and identity-only counterfactuals --\nallowing us to test the separate and combined roles of network and identity.\nWhile social scientists often treat either network or identity as the core\nsocial structure in modeling culture change, we show that key geographic\nproperties of diffusion actually depend on both factors as each one influences\ndifferent mechanisms of diffusion. Specifically, the network principally drives\nspread among urban counties via weak-tie diffusion, while identity plays a\ndisproportionate role in transmission among rural counties via strong-tie\ndiffusion. Diffusion between urban and rural areas, a key component in\ninnovation diffusing nationally, requires both network and identity. Our work\nsuggests that models must integrate both factors in order to understand and\nreproduce the adoption of innovation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ananthasubramaniam_A/0/1/0/all/0/1\">Aparna Ananthasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1\">Daniel M. Romero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective. (arXiv:2202.04847v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04847","description":"<p>In this survey paper, we overview major deep learning methods used in Natural\nLanguage Processing (NLP) and source code over the last 35 years. Next, we\npresent a survey of the applications of Artificial Intelligence (AI) for source\ncode, also known as Code Intelligence (CI) and Programming Language Processing\n(PLP). We survey over 287 publications and present a software-engineering\ncentered taxonomy for CI placing each of the works into one category describing\nhow it best assists the software development cycle. Then, we overview the field\nof conversational assistants and their applications in software engineering and\neducation. Lastly, we highlight research opportunities at the intersection of\nAI for code and conversational assistants and provide future directions for\nresearching conversational assistants with CI capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Hossami_E/0/1/0/all/0/1\">Erfan Al-Hossami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1\">Samira Shaikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The USTC-Ximalaya system for the ICASSP 2022 multi-channel multi-party meeting transcription (M2MeT) challenge. (arXiv:2202.04855v1 [eess.AS])","link":"http://arxiv.org/abs/2202.04855","description":"<p>We propose two improvements to target-speaker voice activity detection\n(TS-VAD), the core component in our proposed speaker diarization system that\nwas submitted to the 2022 Multi-Channel Multi-Party Meeting Transcription\n(M2MeT) challenge. These techniques are designed to handle multi-speaker\nconversations in real-world meeting scenarios with high speaker-overlap ratios\nand under heavy reverberant and noisy condition. First, for data preparation\nand augmentation in training TS-VAD models, speech data containing both real\nmeetings and simulated indoor conversations are used. Second, in refining\nresults obtained after TS-VAD based decoding, we perform a series of\npost-processing steps to improve the VAD results needed to reduce diarization\nerror rates (DERs). Tested on the ALIMEETING corpus, the newly released\nMandarin meeting dataset used in M2MeT, we demonstrate that our proposed system\ncan decrease the DER by up to 66.55/60.59% relatively when compared with\nclassical clustering based diarization on the Eval/Test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_M/0/1/0/all/0/1\">Maokui He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_X/0/1/0/all/0/1\">Xiang Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weilin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_J/0/1/0/all/0/1\">JingJing Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_S/0/1/0/all/0/1\">Shutong Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1\">Yuhang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1\">Heng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Hypernymy Relations from Language Models: On the Effectiveness of Zero-Shot Taxonomy Induction. (arXiv:2202.04876v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04876","description":"<p>In this paper, we analyze zero-shot taxonomy learning methods which are based\non distilling knowledge from language models via prompting and sentence\nscoring. We show that, despite their simplicity, these methods outperform some\nsupervised strategies and are competitive with the current state-of-the-art\nunder adequate conditions. We also show that statistical and linguistic\nproperties of prompts dictate downstream performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Devansh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anke_L/0/1/0/all/0/1\">Luis Espinosa Anke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaxoEnrich: Self-Supervised Taxonomy Completion via Structure-Semantic Representations. (arXiv:2202.04887v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04887","description":"<p>Taxonomies are fundamental to many real-world applications in various\ndomains, serving as structural representations of knowledge. To deal with the\nincreasing volume of new concepts needed to be organized as taxonomies,\nresearchers turn to automatically completion of an existing taxonomy with new\nconcepts. In this paper, we propose TaxoEnrich, a new taxonomy completion\nframework, which effectively leverages both semantic features and structural\ninformation in the existing taxonomy and offers a better representation of\ncandidate position to boost the performance of taxonomy completion.\nSpecifically, TaxoEnrich consists of four components: (1)\ntaxonomy-contextualized embedding which incorporates both semantic meanings of\nconcept and taxonomic relations based on powerful pretrained language models;\n(2) a taxonomy-aware sequential encoder which learns candidate position\nrepresentations by encoding the structural information of taxonomy; (3) a\nquery-aware sibling encoder which adaptively aggregates candidate siblings to\naugment candidate position representations based on their importance to the\nquery-position matching; (4) a query-position matching model which extends\nexisting work with our new candidate position representations. Extensive\nexperiments on four large real-world datasets from different domains show that\n\\TaxoEnrich achieves the best performance among all evaluation metrics and\noutperforms previous state-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiangchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterHT: Knowledge Graph Embeddings by Interaction between Head and Tail Entities. (arXiv:2202.04897v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04897","description":"<p>Knowledge graph embedding (KGE) models learn the representation of entities\nand relations in knowledge graphs. Distance-based methods show promising\nperformance on link prediction task, which predicts the result by the distance\nbetween two entity representations. However, most of these methods represent\nthe head entity and tail entity separately, which limits the model capacity. We\npropose a novel distance-based method named InterHT that allows the head and\ntail entities to interact better and get better entity representation.\nExperimental results show that our proposed method achieves the best results on\nogbl-wikikg2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qingye Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dayong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slovene SuperGLUE Benchmark: Translation and Evaluation. (arXiv:2202.04994v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04994","description":"<p>We present a Slovene combined machine-human translated SuperGLUE benchmark.\nWe describe the translation process and problems arising due to differences in\nmorphology and grammar. We evaluate the translated datasets in several modes:\nmonolingual, cross-lingual, and multilingual, taking into account differences\nbetween machine and human translated training sets. The results show that the\nmonolingual Slovene SloBERTa model is superior to massively multilingual and\ntrilingual BERT models, but these also show a good cross-lingual performance on\ncertain tasks. The performance of Slovene models still lags behind the best\nEnglish models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zagar_A/0/1/0/all/0/1\">Ale&#x161; &#x17d;agar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language in Requirements Engineering for Structure Inference -- An Integrative Review. (arXiv:2202.05065v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05065","description":"<p>The automatic extraction of structure from text can be difficult for\nmachines. Yet, the elicitation of this information can provide many benefits\nand opportunities for various applications. Benefits have also been identified\nfor the area of Requirements Engineering. To evaluate what work has been done\nand is currently available, the paper at hand provides an integrative review\nregarding Natural Language Processing (NLP) tools for Requirements Engineering.\nThis assessment was conducted to provide a foundation for future work as well\nas deduce insights from the stats quo. To conduct the review, the history of\nRequirements Engineering and NLP are described as well as an evaluation of over\n136 NLP tools. To assess these tools, a set of criteria was defined. The\nresults are that currently no open source approach exists that allows for the\ndirect/primary extraction of information structure and even closed source\nsolutions show limitations such as supervision or input limitations, which\neliminates the possibility for fully automatic and universal application. As a\nresults, the authors deduce that the current approaches are not applicable and\na different methodology is necessary. An approach that allows for individual\nmanagement of the algorithm, knowledge base, and text corpus is a possibility\nbeing pursued.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vierlboeck_M/0/1/0/all/0/1\">Maximilian Vierlboeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipizzi_C/0/1/0/all/0/1\">Carlo Lipizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilchiani_R/0/1/0/all/0/1\">Roshanak Nilchiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-speaker style transfer for text-to-speech using data augmentation. (arXiv:2202.05083v1 [eess.AS])","link":"http://arxiv.org/abs/2202.05083","description":"<p>We address the problem of cross-speaker style transfer for text-to-speech\n(TTS) using data augmentation via voice conversion. We assume to have a corpus\nof neutral non-expressive data from a target speaker and supporting\nconversational expressive data from different speakers. Our goal is to build a\nTTS system that is expressive, while retaining the target speaker's identity.\nThe proposed approach relies on voice conversion to first generate high-quality\ndata from the set of supporting expressive speakers. The voice converted data\nis then pooled with natural data from the target speaker and used to train a\nsingle-speaker multi-style TTS system. We provide evidence that this approach\nis efficient, flexible, and scalable. The method is evaluated using one or more\nsupporting speakers, as well as a variable amount of supporting data. We\nfurther provide evidence that this approach allows some controllability of\nspeaking style, when using multiple supporting speakers. We conclude by scaling\nour proposed technology to a set of 14 speakers across 7 languages. Results\nindicate that our technology consistently improves synthetic samples in terms\nof style similarity, while retaining the target speaker's identity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ribeiro_M/0/1/0/all/0/1\">Manuel Sam Ribeiro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_J/0/1/0/all/0/1\">Julian Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Comini_G/0/1/0/all/0/1\">Giulia Comini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huybrechts_G/0/1/0/all/0/1\">Goeric Huybrechts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabrys_A/0/1/0/all/0/1\">Adam Gabrys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1\">Jaime Lorenzo-Trueba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InPars: Data Augmentation for Information Retrieval using Large Language Models. (arXiv:2202.05144v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05144","description":"<p>The information retrieval community has recently witnessed a revolution due\nto large pretrained transformer models. Another key ingredient for this\nrevolution was the MS MARCO dataset, whose scale and diversity has enabled\nzero-shot transfer learning to various tasks. However, not all IR tasks and\ndomains can benefit from one single dataset equally. Extensive research in\nvarious NLP tasks has shown that using domain-specific training data, as\nopposed to a general-purpose one, improves the performance of neural models. In\nthis work, we harness the few-shot capabilities of large pretrained language\nmodels as synthetic data generators for IR tasks. We show that models finetuned\nsolely on our unsupervised dataset outperform strong baselines such as BM25 as\nwell as recently proposed self-supervised dense retrieval methods. Furthermore,\nretrievers finetuned on both supervised and our synthetic data achieve better\nzero-shot transfer than models finetuned only on supervised data. Code, models,\nand data are available at https://github.com/zetaalphavector/inpars .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1\">Hugo Abonizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1\">Marzieh Fadaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET. (arXiv:2202.05148v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05148","description":"<p>Neural metrics have achieved impressive correlation with human judgements in\nthe evaluation of machine translation systems, but before we can safely\noptimise towards such metrics, we should be aware of (and ideally eliminate)\nbiases towards bad translations that receive high scores. Our experiments show\nthat sample-based Minimum Bayes Risk decoding can be used to explore and\nquantify such weaknesses. When applying this strategy to COMET for en-de and\nde-en, we find that COMET models are not sensitive enough to discrepancies in\nnumbers and named entities. We further show that these biases cannot be fully\nremoved by simply training on additional synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amrhein_C/0/1/0/all/0/1\">Chantal Amrhein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding. (arXiv:2202.05209v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05209","description":"<p>ASR systems designed for native English (L1) usually underperform on\nnon-native English (L2). To address this performance gap, \\textbf{(i)} we\nextend our previous work to investigate fine-tuning of a pre-trained wav2vec\n2.0 model \\cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2\ntraining conditions. We further \\textbf{(ii)} incorporate language model\ndecoding in the ASR system, along with the fine-tuning method. Quantifying\ngains acquired from each of these two approaches separately and an error\nanalysis allows us to identify different sources of improvement within our\nmodels. We find that while the large self-trained wav2vec 2.0 may be\ninternalizing sufficient decoding knowledge for clean L1 speech\n\\cite{xu2021self}, this does not hold for L2 speech and accounts for the\nutility of employing language model decoding on L2 data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_P/0/1/0/all/0/1\">Peter Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibano_T/0/1/0/all/0/1\">Toshiko Shibano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locating and Editing Factual Knowledge in GPT. (arXiv:2202.05262v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05262","description":"<p>We investigate the mechanisms underlying factual knowledge recall in\nautoregressive transformer language models. First, we develop a causal\nintervention for identifying neuron activations capable of altering a model's\nfactual predictions. Within large GPT-style models, this reveals two distinct\nsets of neurons that we hypothesize correspond to knowing an abstract fact and\nsaying a concrete word, respectively. This insight inspires the development of\nROME, a novel method for editing facts stored in model weights. For evaluation,\nwe assemble CounterFact, a dataset of over twenty thousand counterfactuals and\ntools to facilitate sensitive measurements of knowledge editing. Using\nCounterFact, we confirm the distinction between saying and knowing neurons, and\nwe find that ROME achieves state-of-the-art performance in knowledge editing\ncompared to other methods. An interactive demo notebook, full code\nimplementation, and the dataset are available at https://rome.baulab.info/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_K/0/1/0/all/0/1\">Kevin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness-aware Summarization for Justified Decision-Making. (arXiv:2107.06243v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2107.06243","description":"<p>In consequential domains such as recidivism prediction, facility inspection,\nand benefit assignment, it's important for individuals to know the\ndecision-relevant information for the model's prediction. In addition,\npredictions should be fair both in terms of the outcome and the justification\nof the outcome. In other words, decision-relevant features should provide\nsufficient information for the predicted outcome and should be independent of\nthe membership of individuals in protected groups such as race and gender. In\nthis work, we focus on the problem of (un)fairness in the justification of the\ntext-based neural models. We tie the explanatory power of the model to fairness\nin the outcome and propose a fairness-aware summarization mechanism to detect\nand counteract the bias in such models. Given a potentially biased natural\nlanguage explanation for a decision, we use a multi-task neural model and an\nattribution mechanism based on integrated gradients to extract high-utility and\nlow-bias justifications in form of a summary. The extracted summary is then\nused for training a model to make decisions for individuals. Results on several\nreal world datasets suggest that our method drastically limits the demographic\nleakage in the input (fairness in justification) while moderately enhancing the\nfairness in the outcome. Our model is also effective in detecting and\ncounteracting several types of data poisoning attacks that synthesize\nrace-coded reasoning or irrelevant justifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keymanesh_M/0/1/0/all/0/1\">Moniba Keymanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1\">Tanya Berger-Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsner_M/0/1/0/all/0/1\">Micha Elsner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining. (arXiv:2109.01411v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01411","description":"<p>The Linked Open Data practice has led to a significant growth of structured\ndata on the Web in the last decade. Such structured data describe real-world\nentities in a machine-readable way, and have created an unprecedented\nopportunity for research in the field of Natural Language Processing. However,\nthere is a lack of studies on how such data can be used, for what kind of\ntasks, and to what extent they can be useful for these tasks. This work focuses\non the e-commerce domain to explore methods of utilising such structured data\nto create language resources that may be used for product classification and\nlinking. We process billions of structured data points in the form of RDF\nn-quads, to create multi-million words of product-related corpora that are\nlater used in three different ways for creating of language resources: training\nword embedding models, continued pre-training of BERT-like language models, and\ntraining Machine Translation models that are used as a proxy to generate\nproduct-related keywords. Our evaluation on an extensive set of benchmarks\nshows word embeddings to be the most reliable and consistent method to improve\nthe accuracy on both tasks (with up to 6.9 percentage points in macro-average\nF1 on some datasets). The other two methods however, are not as useful. Our\nanalysis shows that this could be due to a number of reasons, including the\nbiased domain representation in the structured data and lack of vocabulary\ncoverage. We share our datasets and discuss how our lessons learned could be\ntaken forward to inform future research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tamizhi-Net OCR: Creating A Quality Large Scale Tamil-Sinhala-English Parallel Corpus Using Deep Learning Based Printed Character Recognition (PCR). (arXiv:2109.05952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05952","description":"<p>Most of the low resource languages do not have the necessary resources to\ncreate even a substantial monolingual corpus. These languages may often be\nfound in government proceedings but mainly in Portable Document Formats (PDFs)\nthat contain legacy fonts. Extracting text from these documents to create a\nmonolingual corpus is challenging due to legacy font usage and printer-friendly\nencoding, which are not optimised for text extraction. Therefore, we propose a\nsimple, automatic, and novel idea that can scale for Tamil, Sinhala, English\nlanguages and many documents. For this purpose, we enhanced the performance of\nTesseract 4.1.1 by employing LSTM-based training on many legacy fonts to\nrecognise printed characters in the above languages. Especially, our model\ndetects code-mix text, numbers, and special characters from the printed\ndocument. It is shown that this approach can boost the character-level accuracy\nof Tesseract 4.1.1 from 85.5 to 98.2 for Tamil (+12.9% relative change) and\n91.8 to 94.8 for Sinhala (+3.26% relative change) on a dataset that is\nconsidered as challenging by its authors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasantharajan_C/0/1/0/all/0/1\">Charangan Vasantharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayasivam_U/0/1/0/all/0/1\">Uthayasanker Thayasivam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-turn RNN-T for streaming recognition of multi-party speech. (arXiv:2112.10200v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2112.10200","description":"<p>Automatic speech recognition (ASR) of single channel far-field recordings\nwith an unknown number of speakers is traditionally tackled by cascaded\nmodules. Recent research shows that end-to-end (E2E) multi-speaker ASR models\ncan achieve superior recognition accuracy compared to modular systems. However,\nthese models do not ensure real-time applicability due to their dependency on\nfull audio context. This work takes real-time applicability as the first\npriority in model design and addresses a few challenges in previous work on\nmulti-speaker recurrent neural network transducer (MS-RNN-T). First, we\nintroduce on-the-fly overlapping speech simulation during training, yielding\n14% relative word error rate (WER) improvement on LibriSpeechMix test set.\nSecond, we propose a novel multi-turn RNN-T (MT-RNN-T) model with an\noverlap-based target arrangement strategy that generalizes to an arbitrary\nnumber of speakers without changes in the model architecture. We investigate\nthe impact of the maximum number of speakers seen during training on MT-RNN-T\nperformance on LibriCSS test set, and report 28% relative WER improvement over\nthe two-speaker MS-RNN-T. Third, we experiment with a rich transcription\nstrategy for joint recognition and segmentation of multi-party speech. Through\nan in-depth analysis, we discuss potential pitfalls of the proposed system as\nwell as promising future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sklyar_I/0/1/0/all/0/1\">Ilya Sklyar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Piunova_A/0/1/0/all/0/1\">Anna Piunova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_X/0/1/0/all/0/1\">Xianrui Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yulan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dark Side of the Language: Pre-trained Transformers in the DarkNet. (arXiv:2201.05613v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05613","description":"<p>Pre-trained Transformers are challenging human performances in many natural\nlanguage processing tasks. The gigantic datasets used for pre-training seem to\nbe the key for their success on existing tasks. In this paper, we explore how a\nrange of pre-trained natural language understanding models perform on truly\nnovel and unexplored data, provided by classification tasks over a DarkNet\ncorpus. Surprisingly, results show that syntactic and lexical neural networks\nlargely outperform pre-trained Transformers. This seems to suggest that\npre-trained Transformers have serious difficulties in adapting to radically\nnovel texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1\">Leonardo Ranaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nourbakhsh_A/0/1/0/all/0/1\">Aria Nourbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patrizi_A/0/1/0/all/0/1\">Arianna Patrizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzzetti_E/0/1/0/all/0/1\">Elena Sofia Ruzzetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onorati_D/0/1/0/all/0/1\">Dario Onorati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallucchi_F/0/1/0/all/0/1\">Francesca Fallucchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1\">Fabio Massimo Zanzotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaMDA: Language Models for Dialog Applications. (arXiv:2201.08239v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08239","description":"<p>We present LaMDA: Language Models for Dialog Applications. LaMDA is a family\nof Transformer-based neural language models specialized for dialog, which have\nup to 137B parameters and are pre-trained on 1.56T words of public dialog data\nand web text. While model scaling alone can improve quality, it shows less\nimprovements on safety and factual grounding. We demonstrate that fine-tuning\nwith annotated data and enabling the model to consult external knowledge\nsources can lead to significant improvements towards the two key challenges of\nsafety and factual grounding. The first challenge, safety, involves ensuring\nthat the model's responses are consistent with a set of human values, such as\npreventing harmful suggestions and unfair bias. We quantify safety using a\nmetric based on an illustrative set of human values, and we find that filtering\ncandidate responses using a LaMDA classifier fine-tuned with a small amount of\ncrowdworker-annotated data offers a promising approach to improving model\nsafety. The second challenge, factual grounding, involves enabling the model to\nconsult external knowledge sources, such as an information retrieval system, a\nlanguage translator, and a calculator. We quantify factuality using a\ngroundedness metric, and we find that our approach enables the model to\ngenerate responses grounded in known sources, rather than responses that merely\nsound plausible. Finally, we explore the use of LaMDA in the domains of\neducation and content recommendations, and analyze their helpfulness and role\nconsistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thoppilan_R/0/1/0/all/0/1\">Romal Thoppilan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_D/0/1/0/all/0/1\">Daniel De Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jamie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_A/0/1/0/all/0/1\">Apoorv Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1\">Alicia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_T/0/1/0/all/0/1\">Taylor Bos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_L/0/1/0/all/0/1\">Leslie Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">YaGuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hongrae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghafouri_A/0/1/0/all/0/1\">Amin Ghafouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menegali_M/0/1/0/all/0/1\">Marcelo Menegali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepikhin_D/0/1/0/all/0/1\">Dmitry Lepikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dehao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokon_I/0/1/0/all/0/1\">Igor Krivokon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusch_W/0/1/0/all/0/1\">Will Rusch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pickett_M/0/1/0/all/0/1\">Marc Pickett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pranesh Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Man_L/0/1/0/all/0/1\">Laichee Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_Hellstern_K/0/1/0/all/0/1\">Kathleen Meier-Hellstern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Meredith Ringel Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_T/0/1/0/all/0/1\">Tulsee Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1\">Renelito Delos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_T/0/1/0/all/0/1\">Toju Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soraker_J/0/1/0/all/0/1\">Johnny Soraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zevenbergen_B/0/1/0/all/0/1\">Ben Zevenbergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1\">Ben Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olson_K/0/1/0/all/0/1\">Kristen Olson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1\">Alejandra Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_John_E/0/1/0/all/0/1\">Erin Hoffman-John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Josh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1\">Lora Aroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajakumar_R/0/1/0/all/0/1\">Ravi Rajakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butryna_A/0/1/0/all/0/1\">Alena Butryna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamm_M/0/1/0/all/0/1\">Matthew Lamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmina_V/0/1/0/all/0/1\">Viktoriya Kuzmina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenton_J/0/1/0/all/0/1\">Joe Fenton</a>, et al. (8 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Typical Decoding for Natural Language Generation. (arXiv:2202.00666v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00666","description":"<p>Despite achieving incredibly low perplexities on myriad natural language\ncorpora, today's language models still often underperform when used to generate\ntext. This dichotomy has puzzled the language generation community for the last\nfew years. In this work, we posit that the abstraction of natural language as a\ncommunication channel (\\`a la Shannon, 1948) can provide new insights into the\nbehaviors of probabilistic language generators, e.g., why high-probability\ntexts can be dull or repetitive. Humans use language as a means of\ncommunicating information, and do so in an efficient yet error-minimizing\nmanner, choosing each word in a string with this (perhaps subconscious) goal in\nmind. We propose that generation from probabilistic models should mimic this\nbehavior. Rather than always choosing words from the high-probability region of\nthe distribution--which have a low Shannon information content--we sample from\nthe set of words with an information content close to its expected value, i.e.,\nclose to the conditional entropy of our model. This decision criterion can be\nrealized through a simple and efficient implementation, which we call typical\nsampling. Automatic and human evaluations show that, in comparison to nucleus\nand top-k sampling, typical sampling offers competitive performance in terms of\nquality while consistently reducing the number of degenerate repetitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiher_G/0/1/0/all/0/1\">Gian Wiher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to pronounce as measuring cross-lingual joint orthography-phonology complexity. (arXiv:2202.00794v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00794","description":"<p>Machine learning models allow us to compare languages by showing how hard a\ntask in each language might be to learn and perform well on. Following this\nline of investigation, we explore what makes a language \"hard to pronounce\" by\nmodelling the task of grapheme-to-phoneme (g2p) transliteration. By training a\ncharacter-level transformer model on this task across 22 languages and\nmeasuring the model's proficiency against its grapheme and phoneme inventories,\nwe show that certain characteristics emerge that separate easier and harder\nlanguages with respect to learning to pronounce. Namely the complexity of a\nlanguage's pronunciation from its orthography is due to the expressive or\nsimplicity of its grapheme-to-phoneme mapping. Further discussion illustrates\nhow future studies should consider relative data sparsity per language to\ndesign fairer cross-lingual comparison tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Platform Difference in Facebook and Text Messages Language Use: Illustrated by Depression Diagnosis. (arXiv:2202.01802v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01802","description":"<p>How does language differ across one's Facebook status updates vs. one's text\nmessages (SMS)? In this study, we show how Facebook and SMS use differs in\npsycho-linguistic characteristics and how these differences drive downstream\nanalyses with an illustration of depression diagnosis. We use a sample of\nconsenting participants who shared Facebook status updates, SMS data, and\nanswered a standard psychological depression screener. We quantify domain\ndifferences using psychologically driven lexical methods and find that language\non Facebook involves more personal concerns, experiences, and content features\nwhile the language in SMS contains more informal and style features. Next, we\nestimate depression from both text domains, using a depression model trained on\nFacebook data, and find a drop in accuracy when predicting self-reported\ndepression assessments from the SMS-based depression estimates. Finally, we\nevaluate a simple domain adaption correction based on words driving the\ncross-platform differences and applied it to the SMS-derived depression\nestimates, resulting in significant improvement in prediction. Our work shows\nthe Facebook vs. SMS difference in language use and suggests the necessity of\ncross-domain adaption for text-based predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xiangyu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellew_D/0/1/0/all/0/1\">Douglas Bellew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_B/0/1/0/all/0/1\">Brenda Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What are the best systems? New perspectives on NLP Benchmarking. (arXiv:2202.03799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03799","description":"<p>In Machine Learning, a benchmark refers to an ensemble of datasets associated\nwith one or multiple metrics together with a way to aggregate different systems\nperformances. They are instrumental in (i) assessing the progress of new\nmethods along different axes and (ii) selecting the best systems for practical\nuse. This is particularly the case for NLP with the development of large\npre-trained models (e.g. GPT, BERT) that are expected to generalize well on a\nvariety of tasks. While the community mainly focused on developing new datasets\nand metrics, there has been little interest in the aggregation procedure, which\nis often reduced to a simple average over various performance measures.\nHowever, this procedure can be problematic when the metrics are on a different\nscale, which may lead to spurious conclusions. This paper proposes a new\nprocedure to rank systems based on their performance across different tasks.\nMotivated by the social choice theory, the final system ordering is obtained\nthrough aggregating the rankings induced by each task and is theoretically\ngrounded. We conduct extensive numerical experiments (on over 270k scores) to\nassess the soundness of our approach both on synthetic and real scores (e.g.\nGLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method\nyields different conclusions on state-of-the-art systems than the\nmean-aggregation procedure while being both more reliable and robust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noiry_N/0/1/0/all/0/1\">Nathan Noiry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irurozki_E/0/1/0/all/0/1\">Ekhine Irurozki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Clemencon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04003","description":"<p>ROUGE is a standard automatic evaluation metric based on n-grams for\nsequence-to-sequence tasks, while cross-entropy loss is an essential objective\nof neural network language model that optimizes at a unigram level. We present\ndifferentiable n-gram objectives, attempting to alleviate the discrepancy\nbetween training criterion and evaluating criterion. The objective maximizes\nthe probabilistic weight of matched sub-sequences, and the novelty of our work\nis the objective weights the matched sub-sequences equally and does not ceil\nthe number of matched sub-sequences by the ground truth count of n-grams in\nreference sequence. We jointly optimize cross-entropy loss and the proposed\nobjective, providing decent ROUGE score enhancement over abstractive\nsummarization dataset CNN/DM and XSum, outperforming alternative n-gram\nobjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"FCM-DNN: diagnosing coronary artery disease by deep accuracy Fuzzy C-Means clustering model. (arXiv:2202.04645v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04645","description":"<p>Cardiovascular disease is one of the most challenging diseases in middle-aged\nand older people, which causes high mortality. Coronary artery disease (CAD) is\nknown as a common cardiovascular disease. A standard clinical tool for\ndiagnosing CAD is angiography. The main challenges are dangerous side effects\nand high angiography costs. Today, the development of artificial\nintelligence-based methods is a valuable achievement for diagnosing disease.\nHence, in this paper, artificial intelligence methods such as neural network\n(NN), deep neural network (DNN), and Fuzzy C-Means clustering combined with\ndeep neural network (FCM-DNN) are developed for diagnosing CAD on a cardiac\nmagnetic resonance imaging (CMRI) dataset. The original dataset is used in two\ndifferent approaches. First, the labeled dataset is applied to the NN and DNN\nto create the NN and DNN models. Second, the labels are removed, and the\nunlabeled dataset is clustered via the FCM method, and then, the clustered\ndataset is fed to the DNN to create the FCM-DNN model. By utilizing the second\nclustering and modeling, the training process is improved, and consequently,\nthe accuracy is increased. As a result, the proposed FCM-DNN model achieves the\nbest performance with a 99.91% accuracy specifying 10 clusters, i.e., 5\nclusters for healthy subjects and 5 clusters for sick subjects, through the\n10-fold cross-validation technique compared to the NN and DNN models reaching\nthe accuracies of 92.18% and 99.63%, respectively. To the best of our\nknowledge, no study has been conducted for CAD diagnosis on the CMRI dataset\nusing artificial intelligence methods. The results confirm that the proposed\nFCM-DNN model can be helpful for scientific and research centers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saadatfar_H/0/1/0/all/0/1\">Hamid Saadatfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+GhasemiGol_M/0/1/0/all/0/1\">Mohammad GhasemiGol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sani_Z/0/1/0/all/0/1\">Zahra Alizadeh Sani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasanzadeh_F/0/1/0/all/0/1\">Fereshteh Hasanzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassannataj_E/0/1/0/all/0/1\">Edris Hassannataj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansor_Z/0/1/0/all/0/1\">Zulkefli Mansor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal unsupervised brain image registration using edge maps. (arXiv:2202.04647v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04647","description":"<p>Diffeomorphic deformable multi-modal image registration is a challenging task\nwhich aims to bring images acquired by different modalities to the same\ncoordinate space and at the same time to preserve the topology and the\ninvertibility of the transformation. Recent research has focused on leveraging\ndeep learning approaches for this task as these have been shown to achieve\ncompetitive registration accuracy while being computationally more efficient\nthan traditional iterative registration methods. In this work, we propose a\nsimple yet effective unsupervised deep learning-based {\\em multi-modal} image\nregistration approach that benefits from auxiliary information coming from the\ngradient magnitude of the image, i.e. the image edges, during the training. The\nintuition behind this is that image locations with a strong gradient are\nassumed to denote a transition of tissues, which are locations of high\ninformation value able to act as a geometry constraint. The task is similar to\nusing segmentation maps to drive the training, but the edge maps are easier and\nfaster to acquire and do not require annotations. We evaluate our approach in\nthe context of registering multi-modal (T1w to T2w) magnetic resonance (MR)\nbrain images of different subjects using three different loss functions that\nare said to assist multi-modal registration, showing that in all cases the\nauxiliary information leads to better results without compromising the runtime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sideri_Lampretsa_V/0/1/0/all/0/1\">Vasiliki Sideri-Lampretsa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation of Anaemic RBCs Using Multilevel Deep Convolutional Encoder-Decoder Network. (arXiv:2202.04650v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04650","description":"<p>Pixel-level analysis of blood images plays a pivotal role in diagnosing\nblood-related diseases, especially Anaemia. These analyses mainly rely on an\naccurate diagnosis of morphological deformities like shape, size, and precise\npixel counting. In traditional segmentation approaches, instance or\nobject-based approaches have been adopted that are not feasible for pixel-level\nanalysis. The convolutional neural network (CNN) model required a large dataset\nwith detailed pixel-level information for the semantic segmentation of red\nblood cells in the deep learning domain. In current research work, we address\nthese problems by proposing a multi-level deep convolutional encoder-decoder\nnetwork along with two state-of-the-art healthy and Anaemic-RBC datasets. The\nproposed multi-level CNN model preserved pixel-level semantic information\nextracted in one layer and then passed to the next layer to choose relevant\nfeatures. This phenomenon helps to precise pixel-level counting of healthy and\nanaemic-RBC elements along with morphological analysis. For experimental\npurposes, we proposed two state-of-the-art RBC datasets, i.e., Healthy-RBCs and\nAnaemic-RBCs dataset. Each dataset contains 1000 images, ground truth masks,\nrelevant, complete blood count (CBC), and morphology reports for performance\nevaluation. The proposed model results were evaluated using crossmatch analysis\nwith ground truth mask by finding IoU, individual training, validation, testing\naccuracies, and global accuracies using a 05-fold training procedure. This\nmodel got training, validation, and testing accuracies as 0.9856, 0.9760, and\n0.9720 on the Healthy-RBC dataset and 0.9736, 0.9696, and 0.9591 on an\nAnaemic-RBC dataset. The IoU and BFScore of the proposed model were 0.9311,\n0.9138, and 0.9032, 0.8978 on healthy and anaemic datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shahzad_M/0/1/0/all/0/1\">Muhammad Shahzad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umar_A/0/1/0/all/0/1\">Arif Iqbal Umar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirazi_S/0/1/0/all/0/1\">Syed Hamad Shirazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaikh_I/0/1/0/all/0/1\">Israr Ahmed Shaikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Humans Do Less-Than-One-Shot Learning?. (arXiv:2202.04670v1 [cs.LG])","link":"http://arxiv.org/abs/2202.04670","description":"<p>Being able to learn from small amounts of data is a key characteristic of\nhuman intelligence, but exactly {\\em how} small? In this paper, we introduce a\nnovel experimental paradigm that allows us to examine classification in an\nextremely data-scarce setting, asking whether humans can learn more categories\nthan they have exemplars (i.e., can humans do \"less-than-one shot\" learning?).\nAn experiment conducted using this paradigm reveals that people are capable of\nlearning in such settings, and provides several insights into underlying\nmechanisms. First, people can accurately infer and represent high-dimensional\nfeature spaces from very little data. Second, having inferred the relevant\nspaces, people use a form of prototype-based categorization (as opposed to\nexemplar-based) to make categorical inferences. Finally, systematic,\nmachine-learnable patterns in responses indicate that people may have efficient\ninductive biases for dealing with this class of data-scarce problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malaviya_M/0/1/0/all/0/1\">Maya Malaviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktar_K/0/1/0/all/0/1\">Kerem Oktar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Joint Variational Multichannel Multiphase Segmentation Framework. (arXiv:2202.04680v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04680","description":"<p>In this paper, we propose a variational image segmentation framework for\nmultichannel multiphase image segmentation based on the Chan-Vese active\ncontour model. The core of our method lies in finding a variable u encoding the\nsegmentation, by minimizing a multichannel energy functional that combines the\ninformation of multiple images. We create a decomposition of the input, either\nby multichannel filtering, or simply by using plain natural RGB, or medical\nimages, which already consist of several channels. Subsequently we minimize the\nproposed functional for each of the channels simultaneously. Our model meets\nthe necessary assumptions such that it can be solved efficiently by\noptimization techniques like the Chambolle-Pock method. We prove that the\nproposed energy functional has global minimizers, and show its stability and\nconvergence with respect to noisy inputs. Experimental results show that the\nproposed method performs well in single- and multichannel segmentation tasks,\nand can be employed to the segmentation of various types of images, such as\nnatural and texture images as well as medical images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gruber_N/0/1/0/all/0/1\">Nadja Gruber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_J/0/1/0/all/0/1\">Johannes Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Court_S/0/1/0/all/0/1\">Sebastien Court</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gizewski_E/0/1/0/all/0/1\">Elke Gizewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PINs: Progressive Implicit Networks for Multi-Scale Neural Representations. (arXiv:2202.04713v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04713","description":"<p>Multi-layer perceptrons (MLP) have proven to be effective scene encoders when\ncombined with higher-dimensional projections of the input, commonly referred to\nas \\textit{positional encoding}. However, scenes with a wide frequency spectrum\nremain a challenge: choosing high frequencies for positional encoding\nintroduces noise in low structure areas, while low frequencies result in poor\nfitting of detailed regions. To address this, we propose a progressive\npositional encoding, exposing a hierarchical MLP structure to incremental sets\nof frequency encodings. Our model accurately reconstructs scenes with wide\nfrequency bands and learns a scene representation at progressive level of\ndetail \\textit{without explicit per-level supervision}. The architecture is\nmodular: each level encodes a continuous implicit representation that can be\nleveraged separately for its respective resolution, meaning a smaller network\nfor coarser reconstructions. Experiments on several 2D and 3D datasets show\nimprovements in reconstruction accuracy, representational capacity and training\nspeed compared to baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landgraf_Z/0/1/0/all/0/1\">Zoe Landgraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hornung_A/0/1/0/all/0/1\">Alexander Sorkine Hornung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabral_R/0/1/0/all/0/1\">Ricardo Silveira Cabral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Network for Cell Tracking in Microscopy Videos. (arXiv:2202.04731v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04731","description":"<p>We present a novel graph neural network (GNN) approach for cell tracking in\nhigh-throughput microscopy videos. By modeling the entire time-lapse sequence\nas a direct graph where cell instances are represented by its nodes and their\nassociations by its edges, we extract the entire set of cell trajectories by\nlooking for the maximal paths in the graph. This is accomplished by several key\ncontributions incorporated into an end-to-end deep learning framework. We\nexploit a deep metric learning algorithm to extract cell feature vectors that\ndistinguish between instances of different biological cells and assemble same\ncell instances. We introduce a new GNN block type which enables a mutual update\nof node and edge feature vectors, thus facilitating the underlying message\npassing process. The message passing concept, whose extent is determined by the\nnumber of GNN blocks, is of fundamental importance as it enables the `flow' of\ninformation between nodes and edges much behind their neighbors in consecutive\nframes. Finally, we solve an edge classification problem and use the identified\nactive edges to construct the cells' tracks and lineage trees. We demonstrate\nthe strengths of the proposed cell tracking approach by applying it to 2D and\n3D datasets of different cell types, imaging setups, and experimental\nconditions. We show that our framework outperforms most of the current\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Haim_T/0/1/0/all/0/1\">Tal Ben-Haim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riklin_Raviv_T/0/1/0/all/0/1\">Tammy Riklin-Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimation of Clinical Workload and Patient Activity using Deep Learning and Optical Flow. (arXiv:2202.04748v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04748","description":"<p>Contactless monitoring using thermal imaging has become increasingly proposed\nto monitor patient deterioration in hospital, most recently to detect fevers\nand infections during the COVID-19 pandemic. In this letter, we propose a novel\nmethod to estimate patient motion and observe clinical workload using a similar\ntechnical setup but combined with open source object detection algorithms\n(YOLOv4) and optical flow. Patient motion estimation was used to approximate\npatient agitation and sedation, while worker motion was used as a surrogate for\ncaregiver workload. Performance was illustrated by comparing over 32000 frames\nfrom videos of patients recorded in an Intensive Care Unit, to clinical\nagitation scores recorded by clinical workers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Duc_T/0/1/0/all/0/1\">Thanh Nguyen-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_P/0/1/0/all/0/1\">Peter Y Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_A/0/1/0/all/0/1\">Andrew Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">David Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1\">John Tan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyall_J/0/1/0/all/0/1\">Jessica Lyall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_M/0/1/0/all/0/1\">Maria De Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Concepts in Learned Representations using Statistical Inference and Interactive Visualization. (arXiv:2202.04753v1 [cs.LG])","link":"http://arxiv.org/abs/2202.04753","description":"<p>Concept discovery is one of the open problems in the interpretability\nliterature that is important for bridging the gap between non-deep learning\nexperts and model end-users. Among current formulations, concepts defines them\nby as a direction in a learned representation space. This definition makes it\npossible to evaluate whether a particular concept significantly influences\nclassification decisions for classes of interest. However, finding relevant\nconcepts is tedious, as representation spaces are high-dimensional and hard to\nnavigate. Current approaches include hand-crafting concept datasets and then\nconverting them to latent space directions; alternatively, the process can be\nautomated by clustering the latent space. In this study, we offer another two\napproaches to guide user discovery of meaningful concepts, one based on\nmultiple hypothesis testing, and another on interactive visualization. We\nexplore the potential value and limitations of these approaches through\nsimulation experiments and an demo visual interface to real data. Overall, we\nfind that these techniques offer a promising strategy for discovering relevant\nconcepts in settings where users do not have predefined descriptions of them,\nbut without completely automating the process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janik_A/0/1/0/all/0/1\">Adrianna Janik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaran_K/0/1/0/all/0/1\">Kris Sankaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wireless Transmission of Images With The Assistance of Multi-level Semantic Information. (arXiv:2202.04754v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04754","description":"<p>Semantic-oriented communication has been considered as a promising to boost\nthe bandwidth efficiency by only transmitting the semantics of the data. In\nthis paper, we propose a multi-level semantic aware communication system for\nwireless image transmission, named MLSC-image, which is based on the deep\nlearning techniques and trained in an end to end manner. In particular, the\nproposed model includes a multilevel semantic feature extractor, that extracts\nboth the highlevel semantic information, such as the text semantics and the\nsegmentation semantics, and the low-level semantic information, such as local\nspatial details of the images. We employ a pretrained image caption to capture\nthe text semantics and a pretrained image segmentation model to obtain the\nsegmentation semantics. These high-level and low-level semantic features are\nthen combined and encoded by a joint semantic and channel encoder into symbols\nto transmit over the physical channel. The numerical results validate the\neffectiveness and efficiency of the proposed semantic communication system,\nespecially under the limited bandwidth condition, which indicates the\nadvantages of the high-level semantics in the compression of images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenguo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qianqian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_S/0/1/0/all/0/1\">Shibo He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1\">Mingyang Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSSN: a deep convolutional neural network to assess spatial scene similarity. (arXiv:2202.04755v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04755","description":"<p>Spatial-query-by-sketch is an intuitive tool to explore human spatial\nknowledge about geographic environments and to support communication with scene\ndatabase queries. However, traditional sketch-based spatial search methods\nperform insufficiently due to their inability to find hidden multi-scale map\nfeatures from mental sketches. In this research, we propose a deep\nconvolutional neural network, namely Deep Spatial Scene Network (DeepSSN), to\nbetter assess the spatial scene similarity. In DeepSSN, a triplet loss function\nis designed as a comprehensive distance metric to support the similarity\nassessment. A positive and negative example mining strategy using qualitative\nconstraint networks in spatial reasoning is designed to ensure a consistently\nincreasing distinction of triplets during the training process. Moreover, we\ndevelop a prototype spatial scene search system using the proposed DeepSSN, in\nwhich the users input spatial query via sketch maps and the system can\nautomatically augment the sketch training data. The proposed model is validated\nusing multi-source conflated map data including 131,300 labeled scene samples\nafter data augmentation. The empirical results demonstrate that the DeepSSN\noutperforms baseline methods including k-nearest-neighbors, multilayer\nperceptron, AlexNet, DenseNet, and ResNet using mean reciprocal rank and\nprecision metrics. This research advances geographic information retrieval\nstudies by introducing a novel deep learning method tailored to spatial scene\nqueries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Danhuai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiyin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Song Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Encoder-Decoder Network with Guided Transmission Map for Single Image Dehazing. (arXiv:2202.04757v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04757","description":"<p>A novel Encoder-Decoder Network with Guided Transmission Map (EDN-GTM) for\nsingle image dehazing scheme is proposed in this paper. The proposed EDN-GTM\ntakes conventional RGB hazy image in conjunction with its transmission map\nestimated by adopting dark channel prior as the inputs of the network. The\nproposed EDN-GTM utilizes U-Net for image segmentation as the core network and\nutilizes various modifications including spatial pyramid pooling module and\nSwish activation to achieve state-of-the-art dehazing performance. Experiments\non benchmark datasets show that the proposed EDN-GTM outperforms most of\ntraditional and deep learning-based image dehazing schemes in terms of PSNR and\nSSIM metrics. The proposed EDN-GTM furthermore proves its applicability to\nobject detection problems. Specifically, when applied to an image preprocessing\ntool for driving object detection, the proposed EDN-GTM can efficiently remove\nhaze and significantly improve detection accuracy by 4.73% in terms of mAP\nmeasure. The code is available at: https://github.com/tranleanh/edn-gtm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1\">Le-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seokyong Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dong-Chul Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling Strategy for Fine-Tuning Segmentation Models to Crisis Area under Scarcity of Data. (arXiv:2202.04766v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04766","description":"<p>The use of remote sensing in humanitarian crisis response missions is\nwell-established and has proven relevant repeatedly. One of the problems is\nobtaining gold annotations as it is costly and time consuming which makes it\nalmost impossible to fine-tune models to new regions affected by the crisis.\nWhere time is critical, resources are limited and environment is constantly\nchanging, models has to evolve and provide flexible ways to adapt to a new\nsituation. The question that we want to answer is if prioritization of samples\nprovide better results in fine-tuning vs other classical sampling methods under\nannotated data scarcity? We propose a method to guide data collection during\nfine-tuning, based on estimated model and sample properties, like predicted IOU\nscore. We propose two formulas for calculating sample priority. Our approach\nblends techniques from interpretability, representation learning and active\nlearning. We have applied our method to a deep learning model for semantic\nsegmentation, U-Net, in a remote sensing application of building detection -\none of the core use cases of remote sensing in humanitarian applications.\nPreliminary results shows utility in prioritization of samples for tuning\nsemantic segmentation models under scarcity of data condition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janik_A/0/1/0/all/0/1\">Adrianna Janik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaran_K/0/1/0/all/0/1\">Kris Sankaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attack and Defense of YOLO Detectors in Autonomous Driving Scenarios. (arXiv:2202.04781v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04781","description":"<p>Visual detection is a key task in autonomous driving, and it serves as one\nfoundation for self-driving planning and control. Deep neural networks have\nachieved promising results in various computer vision tasks, but they are known\nto be vulnerable to adversarial attacks. A comprehensive understanding of deep\nvisual detectors' vulnerability is required before people can improve their\nrobustness. However, only a few adversarial attack/defense works have focused\non object detection, and most of them employed only classification and/or\nlocalization losses, ignoring the objectness aspect. In this paper, we identify\na serious objectness-related adversarial vulnerability in YOLO detectors and\npresent an effective attack strategy aiming the objectness aspect of visual\ndetection in autonomous vehicles. Furthermore, to address such vulnerability,\nwe propose a new objectness-aware adversarial training approach for visual\ndetection. Experiments show that the proposed attack targeting the objectness\naspect is 45.17% and 43.50% more effective than those generated from\nclassification and/or localization losses on the KITTI and COCO_traffic\ndatasets, respectively. Also, the proposed adversarial defense approach can\nimprove the detectors' robustness against objectness-oriented attacks by up to\n21% and 12% mAP on KITTI and COCO_traffic, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jung Im Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qing Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiclass histogram-based thresholding using kernel density estimation and scale-space representations. (arXiv:2202.04785v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04785","description":"<p>We present a new method for multiclass thresholding of a histogram which is\nbased on the nonparametric Kernel Density (KD) estimation, where the unknown\nparameters of the KD estimate are defined using the Expectation-Maximization\n(EM) iterations. The method compares the number of extracted minima of the KD\nestimate with the number of the requested clusters minus one. If these numbers\nmatch, the algorithm returns positions of the minima as the threshold values,\notherwise, the method gradually decreases/increases the kernel bandwidth until\nthe numbers match. We verify the method using synthetic histograms with known\nthreshold values and using the histogram of real X-ray computed tomography\nimages. After thresholding of the real histogram, we estimated the porosity of\nthe sample and compare it with the direct experimental measurements. The\ncomparison shows the meaningfulness of the thresholding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Korneev_S/0/1/0/all/0/1\">S. Korneev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gilles_J/0/1/0/all/0/1\">J. Gilles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Battiato_I/0/1/0/all/0/1\">I. Battiato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04800","description":"<p>Humans have remarkable capacity to reason abductively and hypothesize about\nwhat lies beyond the literal content of an image. By identifying concrete\nvisual clues scattered throughout a scene, we almost can't help but draw\nprobable inferences beyond the literal scene based on our everyday experience\nand knowledge about the world. For example, if we see a \"20 mph\" sign alongside\na road, we might assume the street sits in a residential area (rather than on a\nhighway), even if no houses are pictured. Can machines perform similar visual\nreasoning?\n</p>\n<p>We present Sherlock, an annotated corpus of 103K images for testing machine\ncapacity for abductive reasoning beyond literal image contents. We adopt a\nfree-viewing paradigm: participants first observe and identify salient clues\nwithin images (e.g., objects, actions) and then provide a plausible inference\nabout the scene, given the clue. In total, we collect 363K (clue, inference)\npairs, which form a first-of-its-kind abductive visual reasoning dataset. Using\nour corpus, we test three complementary axes of abductive reasoning. We\nevaluate the capacity of models to: i) retrieve relevant inferences from a\nlarge candidate corpus; ii) localize evidence for inferences via bounding\nboxes, and iii) compare plausible inferences to match human judgments on a\nnewly-collected diagnostic corpus of 19K Likert-scale judgments. While we find\nthat fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong\nbaselines, significant headroom exists between model performance and human\nagreement. We provide analysis that points towards future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Semantic Segmentation with Visual Words Learning and Hybrid Pooling. (arXiv:2202.04812v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04812","description":"<p>Weakly-Supervised Semantic Segmentation (WSSS) methods with image-level\nlabels generally train a classification network to generate the Class\nActivation Maps (CAMs) as the initial coarse segmentation labels. However,\ncurrent WSSS methods still perform far from satisfactorily because their\nadopted CAMs 1) typically focus on partial discriminative object regions and 2)\nusually contain useless background regions. These two problems are attributed\nto the sole image-level supervision and aggregation of global information when\ntraining the classification networks. In this work, we propose the visual words\nlearning module and hybrid pooling approach, and incorporate them in the\nclassification network to mitigate the above problems. In the visual words\nlearning module, we counter the first problem by enforcing the classification\nnetwork to learn fine-grained visual word labels so that more object extents\ncould be discovered. Specifically, the visual words are learned with a\ncodebook, which could be updated via two proposed strategies, i.e.\nlearning-based strategy and memory-bank strategy. The second drawback of CAMs\nis alleviated with the proposed hybrid pooling, which incorporates the global\naverage and local discriminative information to simultaneously ensure object\ncompleteness and reduce background regions. We evaluated our methods on PASCAL\nVOC 2012 and MS COCO 2014 datasets. Without any extra saliency prior, our\nmethod achieved 70.6% and 70.7% mIoU on the $val$ and $test$ set of PASCAL VOC\ndataset, respectively, and 36.2% mIoU on the $val$ set of MS COCO dataset,\nwhich significantly surpassed the performance of state-of-the-art WSSS methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1\">Lixiang Ru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decreasing Annotation Burden of Pairwise Comparisons with Human-in-the-Loop Sorting: Application in Medical Image Artifact Rating. (arXiv:2202.04823v1 [q-bio.QM])","link":"http://arxiv.org/abs/2202.04823","description":"<p>Ranking by pairwise comparisons has shown improved reliability over ordinal\nclassification. However, as the annotations of pairwise comparisons scale\nquadratically, this becomes less practical when the dataset is large. We\npropose a method for reducing the number of pairwise comparisons required to\nrank by a quantitative metric, demonstrating the effectiveness of the approach\nin ranking medical images by image quality in this proof of concept study.\nUsing the medical image annotation software that we developed, we actively\nsubsample pairwise comparisons using a sorting algorithm with a human rater in\nthe loop. We find that this method substantially reduces the number of\ncomparisons required for a full ordinal ranking without compromising\ninter-rater reliability when compared to pairwise comparisons without sorting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Jang_I/0/1/0/all/0/1\">Ikbeom Jang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Danley_G/0/1/0/all/0/1\">Garrison Danley</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias-Eliminated Semantic Refinement for Any-Shot Learning. (arXiv:2202.04827v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04827","description":"<p>When training samples are scarce, the semantic embedding technique, ie,\ndescribing class labels with attributes, provides a condition to generate\nvisual features for unseen objects by transferring the knowledge from seen\nobjects. However, semantic descriptions are usually obtained in an external\nparadigm, such as manual annotation, resulting in weak consistency between\ndescriptions and visual features. In this paper, we refine the coarse-grained\nsemantic description for any-shot learning tasks, ie, zero-shot learning (ZSL),\ngeneralized zero-shot learning (GZSL), and few-shot learning (FSL). A new\nmodel, namely, the semantic refinement Wasserstein generative adversarial\nnetwork (SRWGAN) model, is designed with the proposed multihead representation\nand hierarchical alignment techniques. Unlike conventional methods, semantic\nrefinement is performed with the aim of identifying a bias-eliminated condition\nfor disjoint-class feature generation and is applicable in both inductive and\ntransductive settings. We extensively evaluate model performance on six\nbenchmark datasets and observe state-of-the-art results for any-shot learning;\neg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset\nand 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in the\nstandard GZSL setting. Various visualizations are also provided to show the\nbias-eliminated generation of SRWGAN. Our code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Liangjun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunhui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Digital Twinning of Industrial Facilities: Retrieval of Industrial Shapes. (arXiv:2202.04834v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04834","description":"<p>This paper devises, implements and benchmarks a novel shape retrieval method\nthat can accurately match individual labelled point clusters (instances) of\nexisting industrial facilities with their respective CAD models. It employs a\ncombination of image and point cloud deep learning networks to classify and\nmatch instances to their geometrically similar CAD model. It extends our\nprevious research on geometric digital twin generation from point cloud data,\nwhich currently is a tedious, manual process. Experiments with our joint\nnetwork reveal that it can reliably retrieve CAD models at 85.2\\% accuracy. The\nproposed research is a fundamental framework to enable the geometric Digital\nTwin (gDT) pipeline and incorporate the real geometric configuration into the\nDigital Twin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agapaki_E/0/1/0/all/0/1\">Eva Agapaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brilakis_I/0/1/0/all/0/1\">Ioannis Brilakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency and Diversity induced Human Motion Segmentation. (arXiv:2202.04861v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04861","description":"<p>Subspace clustering is a classical technique that has been widely used for\nhuman motion segmentation and other related tasks. However, existing\nsegmentation methods often cluster data without guidance from prior knowledge,\nresulting in unsatisfactory segmentation results. To this end, we propose a\nnovel Consistency and Diversity induced human Motion Segmentation (CDMS)\nalgorithm. Specifically, our model factorizes the source and target data into\ndistinct multi-layer feature spaces, in which transfer subspace learning is\nconducted on different layers to capture multi-level information. A\nmulti-mutual consistency learning strategy is carried out to reduce the domain\ngap between the source and target data. In this way, the domain-specific\nknowledge and domain-invariant properties can be explored simultaneously.\nBesides, a novel constraint based on the Hilbert Schmidt Independence Criterion\n(HSIC) is introduced to ensure the diversity of multi-level subspace\nrepresentations, which enables the complementarity of multi-level\nrepresentations to be explored to boost the transfer learning performance.\nMoreover, to preserve the temporal correlations, an enhanced graph regularizer\nis imposed on the learned representation coefficients and the multi-level\nrepresentations of the source data. The proposed model can be efficiently\nsolved using the Alternating Direction Method of Multipliers (ADMM) algorithm.\nExtensive experimental results on public human motion datasets demonstrate the\neffectiveness of our method against several state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-based gaze prediction in deep imitation learning for robot manipulation. (arXiv:2202.04877v1 [cs.RO])","link":"http://arxiv.org/abs/2202.04877","description":"<p>Deep imitation learning is a promising approach that does not require\nhard-coded control rules in autonomous robot manipulation. The current\napplications of deep imitation learning to robot manipulation have been limited\nto reactive control based on the states at the current time step. However,\nfuture robots will also be required to solve tasks utilizing their memory\nobtained by experience in complicated environments (e.g., when the robot is\nasked to find a previously used object on a shelf). In such a situation, simple\ndeep imitation learning may fail because of distractions caused by complicated\nenvironments. We propose that gaze prediction from sequential visual input\nenables the robot to perform a manipulation task that requires memory. The\nproposed algorithm uses a Transformer-based self-attention architecture for the\ngaze estimation based on sequential data to implement memory. The proposed\nmethod was evaluated with a real robot multi-object manipulation task that\nrequires memory of the previous states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heecheol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohmura_Y/0/1/0/all/0/1\">Yoshiyuki Ohmura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1\">Yasuo Kuniyoshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVSeRF: Joint Pixel-, Voxel- and Surface-Aligned Radiance Field for Single-Image Novel View Synthesis. (arXiv:2202.04879v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04879","description":"<p>We present PVSeRF, a learning framework that reconstructs neural radiance\nfields from single-view RGB images, for novel view synthesis. Previous\nsolutions, such as pixelNeRF, rely only on pixel-aligned features and suffer\nfrom feature ambiguity issues. As a result, they struggle with the\ndisentanglement of geometry and appearance, leading to implausible geometries\nand blurry results. To address this challenge, we propose to incorporate\nexplicit geometry reasoning and combine it with pixel-aligned features for\nradiance field prediction. Specifically, in addition to pixel-aligned features,\nwe further constrain the radiance field learning to be conditioned on i)\nvoxel-aligned features learned from a coarse volumetric grid and ii) fine\nsurface-aligned features extracted from a regressed point cloud. We show that\nthe introduction of such geometry-aware features helps to achieve a better\ndisentanglement between appearance and geometry, i.e. recovering more accurate\ngeometries and synthesizing higher quality images of novel views. Extensive\nexperiments against state-of-the-art methods on ShapeNet benchmarks demonstrate\nthe superiority of our approach for single-image novel view synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xianggang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiapeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the automated large-scale reconstruction of past road networks from historical maps. (arXiv:2202.04883v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04883","description":"<p>Transportation infrastructure, such as road or railroad networks, represent a\nfundamental component of our civilization. For sustainable planning and\ninformed decision making, a thorough understanding of the long-term evolution\nof transportation infrastructure such as road networks is crucial. However,\nspatially explicit, multi-temporal road network data covering large spatial\nextents are scarce and rarely available prior to the 2000s. Herein, we propose\na framework that employs increasingly available scanned and georeferenced\nhistorical map series to reconstruct past road networks, by integrating\nabundant, contemporary road network data and color information extracted from\nhistorical maps. Specifically, our method uses contemporary road segments as\nanalytical units and extracts historical roads by inferring their existence in\nhistorical map series based on image processing and clustering techniques. We\ntested our method on over 300,000 road segments representing more than 50,000\nkm of the road network in the United States, extending across three study areas\nthat cover 53 historical topographic map sheets dated between 1890 and 1950. We\nevaluated our approach by comparison to other historical datasets and against\nmanually created reference data, achieving F-1 scores of up to 0.95, and showed\nthat the extracted road network statistics are highly plausible over time,\ni.e., following general growth patterns. We demonstrated that contemporary\ngeospatial data integrated with information extracted from historical map\nseries open up new avenues for the quantitative analysis of long-term\nurbanization processes and landscape changes far beyond the era of operational\nremote sensing and digital cartography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uhl_J/0/1/0/all/0/1\">Johannes H. Uhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyk_S/0/1/0/all/0/1\">Stefan Leyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1\">Yao-Yi Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoblock_C/0/1/0/all/0/1\">Craig A. Knoblock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving performance of aircraft detection in satellite imagery while limiting the labelling effort: Hybrid active learning. (arXiv:2202.04890v1 [cs.AI])","link":"http://arxiv.org/abs/2202.04890","description":"<p>The earth observation industry provides satellite imagery with high spatial\nresolution and short revisit time. To allow efficient operational employment of\nthese images, automating certain tasks has become necessary. In the defense\ndomain, aircraft detection on satellite imagery is a valuable tool for\nanalysts. Obtaining high performance detectors on such a task can only be\nachieved by leveraging deep learning and thus us-ing a large amount of labeled\ndata. To obtain labels of a high enough quality, the knowledge of military\nexperts is needed.We propose a hybrid clustering active learning method to\nselect the most relevant data to label, thus limiting the amount of data\nrequired and further improving the performances. It combines diversity- and\nuncertainty-based active learning selection methods. For aircraft detection by\nsegmentation, we show that this method can provide better or competitive\nresults compared to other active learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imbert_J/0/1/0/all/0/1\">Julie Imbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dashyan_G/0/1/0/all/0/1\">Gohar Dashyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goupilleau_A/0/1/0/all/0/1\">Alex Goupilleau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceillier_T/0/1/0/all/0/1\">Tugdual Ceillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corbineau_M/0/1/0/all/0/1\">Marie-Caroline Corbineau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04901","description":"<p>We present a frame interpolation algorithm that synthesizes multiple\nintermediate frames from two input images with large in-between motion. Recent\nmethods use multiple networks to estimate optical flow or depth and a separate\nnetwork dedicated to frame synthesis. This is often complex and requires scarce\noptical flow or depth ground-truth. In this work, we present a single unified\nnetwork, distinguished by a multi-scale feature extractor that shares weights\nat all scales, and is trainable from frames alone. To synthesize crisp and\npleasing frames, we propose to optimize our network with the Gram matrix loss\nthat measures the correlation difference between feature maps. Our approach\noutperforms state-of-the-art methods on the Xiph large motion benchmark. We\nalso achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing\nto methods that use perceptual losses. We study the effect of weight sharing\nand of training with datasets of increasing motion range. Finally, we\ndemonstrate our model's effectiveness in synthesizing high quality and\ntemporally coherent videos on a challenging near-duplicate photos dataset.\nCodes and pre-trained models are available at\nhttps://github.com/google-research/frame-interpolation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reda_F/0/1/0/all/0/1\">Fitsum Reda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontkanen_J/0/1/0/all/0/1\">Janne Kontkanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabellion_E/0/1/0/all/0/1\">Eric Tabellion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantofaru_C/0/1/0/all/0/1\">Caroline Pantofaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spherical Transformer. (arXiv:2202.04942v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04942","description":"<p>Using convolutional neural networks for 360images can induce sub-optimal\nperformance due to distortions entailed by a planar projection. The distortion\ngets deteriorated when a rotation is applied to the 360image. Thus, many\nresearches based on convolutions attempt to reduce the distortions to learn\naccurate representation. In contrast, we leverage the transformer architecture\nto solve image classification problems for 360images. Using the proposed\ntransformer for 360images has two advantages. First, our method does not\nrequire the erroneous planar projection process by sampling pixels from the\nsphere surface. Second, our sampling method based on regular polyhedrons makes\nlow rotation equivariance errors, because specific rotations can be reduced to\npermutations of faces. In experiments, we validate our network on two aspects,\nas follows. First, we show that using a transformer with highly uniform\nsampling methods can help reduce the distortion. Second, we demonstrate that\nthe transformer architecture can achieve rotation equivariance on specific\nrotations. We compare our method to other state-of-the-art algorithms using the\nSPH-MNIST, SPH-CIFAR, and SUN360 datasets and show that our method is\ncompetitive with other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sungmin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_R/0/1/0/all/0/1\">Raehyuk Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Junseok Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OWL (Observe, Watch, Listen): Localizing Actions in Egocentric Video via Audiovisual Temporal Context. (arXiv:2202.04947v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04947","description":"<p>Temporal action localization (TAL) is an important task extensively explored\nand improved for third-person videos in recent years. Recent efforts have been\nmade to perform fine-grained temporal localization on first-person videos.\nHowever, current TAL methods only use visual signals, neglecting the audio\nmodality that exists in most videos and that shows meaningful action\ninformation in egocentric videos. In this work, we take a deep look into the\neffectiveness of audio in detecting actions in egocentric videos and introduce\na simple-yet-effective approach via Observing, Watching, and Listening (OWL) to\nleverage audio-visual information and context for egocentric TAL. For doing\nthat, we: 1) compare and study different strategies for where and how to fuse\nthe two modalities; 2) propose a transformer-based model to incorporate\ntemporal audio-visual context. Our experiments show that our approach achieves\nstate-of-the-art performance on EPIC-KITCHENS-100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramazanova_M/0/1/0/all/0/1\">Merey Ramazanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escorcia_V/0/1/0/all/0/1\">Victor Escorcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monotonically Convergent Regularization by Denoising. (arXiv:2202.04961v1 [eess.IV])","link":"http://arxiv.org/abs/2202.04961","description":"<p>Regularization by denoising (RED) is a widely-used framework for solving\ninverse problems by leveraging image denoisers as image priors. Recent work has\nreported the state-of-the-art performance of RED in a number of imaging\napplications using pre-trained deep neural nets as denoisers. Despite the\nrecent progress, the stable convergence of RED algorithms remains an open\nproblem. The existing RED theory only guarantees stability for convex\ndata-fidelity terms and nonexpansive denoisers. This work addresses this issue\nby developing a new monotone RED (MRED) algorithm, whose convergence does not\nrequire nonexpansiveness of the deep denoising prior. Simulations on image\ndeblurring and compressive sensing recovery from random matrices show the\nstability of MRED even when the traditional RED algorithm diverges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yuyang Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaojian Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamilov_U/0/1/0/all/0/1\">Ulugbek S. Kamilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Siamese Multiple Object Tracker with Enhanced Proposals. (arXiv:2202.04966v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04966","description":"<p>Maintaining the identity of multiple objects in real-time video is a\nchallenging task, as it is not always possible to run a detector on every\nframe. Thus, motion estimation systems are often employed, which either do not\nscale well with the number of targets or produce features with limited semantic\ninformation. To solve the aforementioned problems and allow the tracking of\ndozens of arbitrary objects in real-time, we propose SiamMOTION. SiamMOTION\nincludes a novel proposal engine that produces quality features through an\nattention mechanism and a region-of-interest extractor fed by an inertia module\nand powered by a feature pyramid network. Finally, the extracted tensors enter\na comparison head that efficiently matches pairs of exemplars and search areas,\ngenerating quality predictions via a pairwise depthwise region proposal network\nand a multi-object penalization module. SiamMOTION has been validated on five\npublic benchmarks, achieving leading performance against current\nstate-of-the-art trackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaquero_L/0/1/0/all/0/1\">Lorenzo Vaquero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brea_V/0/1/0/all/0/1\">V&#xed;ctor M. Brea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mucientes_M/0/1/0/all/0/1\">Manuel Mucientes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Assessing and Characterizing the Semantic Robustness of Face Recognition. (arXiv:2202.04978v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04978","description":"<p>Deep Neural Networks (DNNs) lack robustness against imperceptible\nperturbations to their input. Face Recognition Models (FRMs) based on DNNs\ninherit this vulnerability. We propose a methodology for assessing and\ncharacterizing the robustness of FRMs against semantic perturbations to their\ninput. Our methodology causes FRMs to malfunction by designing adversarial\nattacks that search for identity-preserving modifications to faces. In\nparticular, given a face, our attacks find identity-preserving variants of the\nface such that an FRM fails to recognize the images belonging to the same\nidentity. We model these identity-preserving semantic modifications via\ndirection- and magnitude-constrained perturbations in the latent space of\nStyleGAN. We further propose to characterize the semantic robustness of an FRM\nby statistically describing the perturbations that induce the FRM to\nmalfunction. Finally, we combine our methodology with a certification\ntechnique, thus providing (i) theoretical guarantees on the performance of an\nFRM, and (ii) a formal description of how an FRM may model the notion of face\nidentity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N\\\"UWA-LIP: Language Guided Image Inpainting with Defect-free VQGAN. (arXiv:2202.05009v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05009","description":"<p>Language guided image inpainting aims to fill in the defective regions of an\nimage under the guidance of text while keeping non-defective regions unchanged.\nHowever, the encoding process of existing models suffers from either receptive\nspreading of defective regions or information loss of non-defective regions,\ngiving rise to visually unappealing inpainting results. To address the above\nissues, this paper proposes N\\\"UWA-LIP by incorporating defect-free VQGAN\n(DF-VQGAN) with multi-perspective sequence to sequence (MP-S2S). In particular,\nDF-VQGAN introduces relative estimation to control receptive spreading and\nadopts symmetrical connections to protect information. MP-S2S further enhances\nvisual information from complementary perspectives, including both low-level\npixels and high-level tokens. Experiments show that DF-VQGAN performs more\nrobustness than VQGAN. To evaluate the inpainting performance of our model, we\nbuilt up 3 open-domain benchmarks, where N\\\"UWA-LIP is also superior to recent\nstrong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_M/0/1/0/all/0/1\">Minheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Spatial Sparsity for Event Cameras with Visual Transformers. (arXiv:2202.05054v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05054","description":"<p>Event cameras report local changes of brightness through an asynchronous\nstream of output events. Events are spatially sparse at pixel locations with\nlittle brightness variation. We propose using a visual transformer (ViT)\narchitecture to leverage its ability to process a variable-length input. The\ninput to the ViT consists of events that are accumulated into time bins and\nspatially separated into non-overlapping sub-regions called patches. Patches\nare selected when the number of nonzero pixel locations within a sub-region is\nabove a threshold. We show that by fine-tuning a ViT model on the selected\nactive patches, we can reduce the average number of patches fed into the\nbackbone during the inference by at least 50% with only a minor drop (0.34%) of\nthe classification accuracy on the N-Caltech101 dataset. This reduction\ntranslates into a decrease of 51% in Multiply-Accumulate (MAC) operations and\nan increase of 46% in the inference speed using a server CPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zuowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuhuang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariance Regularization for Image Reconstruction. (arXiv:2202.05062v1 [math.OC])","link":"http://arxiv.org/abs/2202.05062","description":"<p>In this work, we propose Regularization-by-Equivariance (REV), a novel\nstructure-adaptive regularization scheme for solving imaging inverse problems\nunder incomplete measurements. Our regularization scheme utilizes the\nequivariant structure in the physics of the measurements -- which is prevalent\nin many inverse problems such as tomographic image reconstruction -- to\nmitigate the ill-poseness of the inverse problem. Our proposed scheme can be\napplied in a plug-and-play manner alongside with any classic first-order\noptimization algorithm such as the accelerated gradient descent/FISTA for\nsimplicity and fast convergence. Our numerical experiments in sparse-view X-ray\nCT image reconstruction tasks demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Tang_J/0/1/0/all/0/1\">Junqi Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Computational Cytology: A Survey. (arXiv:2202.05126v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05126","description":"<p>Computational cytology is a critical, rapid-developing, yet challenging topic\nin the field of medical image computing which analyzes the digitized cytology\nimage by computer-aided technologies for cancer screening. Recently, an\nincreasing number of deep learning (DL) algorithms have made significant\nprogress in medical image analysis, leading to the boosting publications of\ncytological studies. To investigate the advanced methods and comprehensive\napplications, we survey more than 120 publications of DL-based cytology image\nanalysis in this article. We first introduce various deep learning methods,\nincluding fully supervised, weakly supervised, unsupervised, and transfer\nlearning. Then, we systematically summarize the public datasets, evaluation\nmetrics, versatile cytology image analysis applications including\nclassification, detection, segmentation, and other related tasks. Finally, we\ndiscuss current challenges and potential research directions of computational\ncytology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanning Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_R/0/1/0/all/0/1\">Ronald CK Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-level augmentation to improve robustness of deep neural networks to affine transformations. (arXiv:2202.05152v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05152","description":"<p>Recent studies revealed that convolutional neural networks do not generalize\nwell to small image transformations, e.g. rotations by a few degrees or\ntranslations of a few pixels. To improve the robustness to such\ntransformations, we propose to introduce data augmentation at intermediate\nlayers of the neural architecture, in addition to the common data augmentation\napplied on the input images. By introducing small perturbations to activation\nmaps (features) at various levels, we develop the capacity of the neural\nnetwork to cope with such transformations. We conduct experiments on three\nimage classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101),\nconsidering two different convolutional architectures (ResNet-18 and\nDenseNet-121). When compared with two state-of-the-art methods, the empirical\nresults show that our approach consistently attains the best trade-off between\naccuracy and mean flip rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class Distance Weighted Cross-Entropy Loss for Ulcerative Colitis Severity Estimation. (arXiv:2202.05167v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05167","description":"<p>Endoscopic Mayo score and Ulcerative Colitis Endoscopic Index of Severity are\ncommonly used scoring systems for the assessment of endoscopic severity of\nulcerative colitis. They are based on assigning a score in relation to the\ndisease activity, which creates a rank among the levels, making it an ordinal\nregression problem. On the other hand, most studies use categorical\ncross-entropy loss function, which is not optimal for the ordinal regression\nproblem, to train the deep learning models. In this study, we propose a novel\nloss function called class distance weighted cross-entropy (CDW-CE) that\nrespects the order of the classes and takes the distance of the classes into\naccount in calculation of cost. Experimental evaluations show that CDW-CE\noutperforms the conventional categorical cross-entropy and CORN framework,\nwhich is designed for the ordinal regression problems. In addition, CDW-CE does\nnot require any modifications at the output layer and is compatible with the\nclass activation map visualization techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ergenc_I/0/1/0/all/0/1\">Ilkay Ergenc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kani_H/0/1/0/all/0/1\">Haluk Tarik Kani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alahdab_Y/0/1/0/all/0/1\">Yesim Ozen Alahdab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atug_O/0/1/0/all/0/1\">Ozlen Atug</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adults as Augmentations for Children in Facial Emotion Recognition with Contrastive Learning. (arXiv:2202.05187v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05187","description":"<p>Emotion recognition in children can help the early identification of, and\nintervention on, psychological complications that arise in stressful situations\nsuch as cancer treatment. Though deep learning models are increasingly being\nadopted, data scarcity is often an issue in pediatric medicine, including for\nfacial emotion recognition in children. In this paper, we study the application\nof data augmentation-based contrastive learning to overcome data scarcity in\nfacial emotion recognition for children. We explore the idea of ignoring\ngenerational gaps, by adding abundantly available adult data to pediatric data,\nto learn better representations. We investigate different ways by which adult\nfacial expression images can be used alongside those of children. In\nparticular, we propose to explicitly incorporate within each mini-batch adult\nimages as augmentations for children's. Out of $84$ combinations of learning\napproaches and training set sizes, we find that supervised contrastive learning\nwith the proposed training scheme performs best, reaching a test accuracy that\ntypically surpasses the one of the second-best approach by 2% to 3%. Our\nresults indicate that adult data can be considered to be a meaningful\naugmentation of pediatric data for the recognition of emotional facial\nexpression in children, and open up the possibility for other applications of\ncontrastive learning to improve pediatric care by complementing data of\nchildren with that of adults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Virgolin_M/0/1/0/all/0/1\">Marco Virgolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzo_A/0/1/0/all/0/1\">Andrea De Lorenzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Human-Centered Machine-Learning Approach for Muscle-Tendon Junction Tracking in Ultrasound Images. (arXiv:2202.05199v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05199","description":"<p>Biomechanical and clinical gait research observes muscles and tendons in\nlimbs to study their functions and behaviour. Therefore, movements of distinct\nanatomical landmarks, such as muscle-tendon junctions, are frequently measured.\nWe propose a reliable and time efficient machine-learning approach to track\nthese junctions in ultrasound videos and support clinical biomechanists in gait\nanalysis. In order to facilitate this process, a method based on deep-learning\nwas introduced. We gathered an extensive dataset, covering 3 functional\nmovements, 2 muscles, collected on 123 healthy and 38 impaired subjects with 3\ndifferent ultrasound systems, and providing a total of 66864 annotated\nultrasound images in our network training. Furthermore, we used data collected\nacross independent laboratories and curated by researchers with varying levels\nof experience. For the evaluation of our method a diverse test-set was selected\nthat is independently verified by four specialists. We show that our model\nachieves similar performance scores to the four human specialists in\nidentifying the muscle-tendon junction position. Our method provides\ntime-efficient tracking of muscle-tendon junctions, with prediction times of up\nto 0.078 seconds per frame (approx. 100 times faster than manual labeling). All\nour codes, trained models and test-set were made publicly available and our\nmodel is provided as a free-to-use online service on https://deepmtj.org/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leitner_C/0/1/0/all/0/1\">Christoph Leitner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarolim_R/0/1/0/all/0/1\">Robert Jarolim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Englmair_B/0/1/0/all/0/1\">Bernhard Englmair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruse_A/0/1/0/all/0/1\">Annika Kruse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_K/0/1/0/all/0/1\">Karen Andrea Lara Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konrad_A/0/1/0/all/0/1\">Andreas Konrad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_E/0/1/0/all/0/1\">Eric Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schrottner_J/0/1/0/all/0/1\">J&#xf6;rg Schr&#xf6;ttner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_L/0/1/0/all/0/1\">Luke A. Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lichtwark_G/0/1/0/all/0/1\">Glen A. Lichtwark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tilp_M/0/1/0/all/0/1\">Markus Tilp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_C/0/1/0/all/0/1\">Christian Baumgartner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Servoing for Pose Control of Soft Continuum Arm in a Structured Environment. (arXiv:2202.05200v1 [cs.RO])","link":"http://arxiv.org/abs/2202.05200","description":"<p>For soft continuum arms, visual servoing is a popular control strategy that\nrelies on visual feedback to close the control loop. However, robust visual\nservoing is challenging as it requires reliable feature extraction from the\nimage, accurate control models and sensors to perceive the shape of the arm,\nboth of which can be hard to implement in a soft robot. This letter circumvents\nthese challenges by presenting a deep neural network-based method to perform\nsmooth and robust 3D positioning tasks on a soft arm by visual servoing using a\ncamera mounted at the distal end of the arm. A convolutional neural network is\ntrained to predict the actuations required to achieve the desired pose in a\nstructured environment. Integrated and modular approaches for estimating the\nactuations from the image are proposed and are experimentally compared. A\nproportional control law is implemented to reduce the error between the desired\nand current image as seen by the camera. The model together with the\nproportional feedback control makes the described approach robust to several\nvariations such as new targets, lighting, loads, and diminution of the soft\narm. Furthermore, the model lends itself to be transferred to a new environment\nwith minimal effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamtikar_S/0/1/0/all/0/1\">Shivani Kamtikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marri_S/0/1/0/all/0/1\">Samhita Marri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walt_B/0/1/0/all/0/1\">Benjamin Walt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uppalapati_N/0/1/0/all/0/1\">Naveen Kumar Uppalapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1\">Girish Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Predicting Fine Finger Motions from Ultrasound Images via Kinematic Representation. (arXiv:2202.05204v1 [cs.RO])","link":"http://arxiv.org/abs/2202.05204","description":"<p>A central challenge in building robotic prostheses is the creation of a\nsensor-based system able to read physiological signals from the lower limb and\ninstruct a robotic hand to perform various tasks. Existing systems typically\nperform discrete gestures such as pointing or grasping, by employing\nelectromyography (EMG) or ultrasound (US) technologies to analyze the state of\nthe muscles. In this work, we study the inference problem of identifying the\nactivation of specific fingers from a sequence of US images when performing\ndexterous tasks such as keyboard typing or playing the piano. While estimating\nfinger gestures has been done in the past by detecting prominent gestures, we\nare interested in classification done in the context of fine motions that\nevolve over time. We consider this task as an important step towards higher\nadoption rates of robotic prostheses among arm amputees, as it has the\npotential to dramatically increase functionality in performing daily tasks. Our\nkey observation, motivating this work, is that modeling the hand as a robotic\nmanipulator allows to encode an intermediate representation wherein US images\nare mapped to said configurations. Given a sequence of such learned\nconfigurations, coupled with a neural-network architecture that exploits\ntemporal coherence, we are able to infer fine finger motions. We evaluated our\nmethod by collecting data from a group of subjects and demonstrating how our\nframework can be used to replay music played or text typed. To the best of our\nknowledge, this is the first study demonstrating these downstream tasks within\nan end-to-end system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zadok_D/0/1/0/all/0/1\">Dean Zadok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzman_O/0/1/0/all/0/1\">Oren Salzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_A/0/1/0/all/0/1\">Alon Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1\">Alex M. Bronstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization. (arXiv:2202.05239v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05239","description":"<p>Neural network quantization is a promising compression technique to reduce\nmemory footprint and save energy consumption, potentially leading to real-time\ninference. However, there is a performance gap between quantized and\nfull-precision models. To reduce it, existing quantization approaches require\nhigh-precision INT32 or full-precision multiplication during inference for\nscaling or dequantization. This introduces a noticeable cost in terms of\nmemory, speed, and required energy. To tackle these issues, we present F8Net, a\nnovel quantization framework consisting of only fixed-point 8-bit\nmultiplication. To derive our method, we first discuss the advantages of\nfixed-point multiplication with different formats of fixed-point numbers and\nstudy the statistical behavior of the associated fixed-point numbers. Second,\nbased on the statistical and algorithmic analysis, we apply different\nfixed-point formats for weights and activations of different layers. We\nintroduce a novel algorithm to automatically determine the right format for\neach layer during training. Third, we analyze a previous quantization algorithm\n-- parameterized clipping activation (PACT) -- and reformulate it using\nfixed-point arithmetic. Finally, we unify the recently proposed method for\nquantization fine-tuning and our fixed-point approach to show the potential of\nour method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50.\nOur approach achieves comparable and better performance, when compared not only\nto existing quantization techniques with INT32 multiplication or floating-point\narithmetic, but also to the full-precision counterparts, achieving\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_R/0/1/0/all/0/1\">Richard Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanumante_S/0/1/0/all/0/1\">Sumant Hanumante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block-NeRF: Scalable Large Scene Neural View Synthesis. (arXiv:2202.05263v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05263","description":"<p>We present Block-NeRF, a variant of Neural Radiance Fields that can represent\nlarge-scale environments. Specifically, we demonstrate that when scaling NeRF\nto render city-scale scenes spanning multiple blocks, it is vital to decompose\nthe scene into individually trained NeRFs. This decomposition decouples\nrendering time from scene size, enables rendering to scale to arbitrarily large\nenvironments, and allows per-block updates of the environment. We adopt several\narchitectural changes to make NeRF robust to data captured over months under\ndifferent environmental conditions. We add appearance embeddings, learned pose\nrefinement, and controllable exposure to each individual NeRF, and introduce a\nprocedure for aligning appearance between adjacent NeRFs so that they can be\nseamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to\ncreate the largest neural scene representation to date, capable of rendering an\nentire neighborhood of San Francisco.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tancik_M/0/1/0/all/0/1\">Matthew Tancik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casser_V/0/1/0/all/0/1\">Vincent Casser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xinchen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1\">Sabeek Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kretzschmar_H/0/1/0/all/0/1\">Henrik Kretzschmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-to-Image Regression with Distribution-Free Uncertainty Quantification and Applications in Imaging. (arXiv:2202.05265v1 [cs.LG])","link":"http://arxiv.org/abs/2202.05265","description":"<p>Image-to-image regression is an important learning task, used frequently in\nbiological imaging. Current algorithms, however, do not generally offer\nstatistical guarantees that protect against a model's mistakes and\nhallucinations. To address this, we develop uncertainty quantification\ntechniques with rigorous statistical guarantees for image-to-image regression\nproblems. In particular, we show how to derive uncertainty intervals around\neach pixel that are guaranteed to contain the true value with a user-specified\nconfidence probability. Our methods work in conjunction with any base machine\nlearning model, such as a neural network, and endow it with formal mathematical\nguarantees -- regardless of the true unknown data distribution or choice of\nmodel. Furthermore, they are simple to implement and computationally\ninexpensive. We evaluate our procedure on three image-to-image regression\ntasks: quantitative phase microscopy, accelerated magnetic resonance imaging,\nand super-resolution transmission electron microscopy of a Drosophila\nmelanogaster brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios N Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohli_A/0/1/0/all/0/1\">Amit P Kohli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyayula_S/0/1/0/all/0/1\">Srigokul Upadhyayula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_Y/0/1/0/all/0/1\">Yaniv Romano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Image Generation via Self-Conditioned GANs. (arXiv:2006.10728v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.10728","description":"<p>We introduce a simple but effective unsupervised method for generating\nrealistic and diverse images. We train a class-conditional GAN model without\nusing manually annotated class labels. Instead, our model is conditional on\nlabels automatically derived from clustering in the discriminator's feature\nspace. Our clustering step automatically discovers diverse modes, and\nexplicitly requires the generator to cover them. Experiments on standard mode\ncollapse benchmarks show that our method outperforms several competing methods\nwhen addressing mode collapse. Our method also performs well on large-scale\ndatasets such as ImageNet and Places365, improving both image diversity and\nstandard quality metrics, compared to previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Steven Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tongzhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-Time Predictive Pedestrian Collision Warning Service for Cooperative Intelligent Transportation Systems Using 3D Pose Estimation. (arXiv:2009.10868v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.10868","description":"<p>Minimizing traffic accidents between vehicles and pedestrians is one of the\nprimary research goals in intelligent transportation systems. To achieve the\ngoal, pedestrian orientation recognition and prediction of pedestrian's\ncrossing or not-crossing intention play a central role. Contemporary approaches\ndo not guarantee satisfactory performance due to limited field-of-view, lack of\ngeneralization, and high computational complexity. To overcome these\nlimitations, we propose a real-time predictive pedestrian collision warning\nservice (P2CWS) for two tasks: pedestrian orientation recognition (100.53 FPS)\nand intention prediction (35.76 FPS). Our framework obtains satisfying\ngeneralization over multiple sites because of the proposed site-independent\nfeatures. At the center of the feature extraction lies 3D pose estimation. The\n3D pose analysis enables robust and accurate recognition of pedestrian\norientations and prediction of intentions over multiple sites. The proposed\nvision framework realizes 89.3% accuracy in the behavior recognition task on\nthe TUD dataset without any training process and 91.28% accuracy in intention\nprediction on our dataset achieving new state-of-the-art performance. To\ncontribute to the corresponding research community, we make our source codes\npublic which are available at https://github.com/Uehwan/VisionForPedestrian\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_U/0/1/0/all/0/1\">Ue-Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ka_D/0/1/0/all/0/1\">Dongho Ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Hwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex-valued Iris Recognition Network. (arXiv:2011.11198v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11198","description":"<p>In this work, we design a fully complex-valued neural network for the task of\niris recognition. Unlike the problem of general object recognition, where\nreal-valued neural networks can be used to extract pertinent features, iris\nrecognition depends on the extraction of both phase and magnitude information\nfrom the input iris texture in order to better represent its biometric content.\nThis necessitates the extraction and processing of phase information that\ncannot be effectively handled by a real-valued neural network. In this regard,\nwe design a fully complex-valued neural network that can better capture the\nmulti-scale, multi-resolution, and multi-orientation phase and amplitude\nfeatures of the iris texture. We show a strong correspondence of the proposed\ncomplex-valued iris recognition network with Gabor wavelets that are used to\ngenerate the classical IrisCode; however, the proposed method enables a new\ncapability of automatic complex-valued feature learning that is tailored for\niris recognition. We conduct experiments on three benchmark datasets -\nND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit\nof the proposed network for the task of iris recognition. We exploit\nvisualization schemes to convey how the complex-valued network, when compared\nto standard real-valued networks, extracts fundamentally different features\nfrom the iris texture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing and Improving Adversarial Training for Generative Modeling. (arXiv:2012.06568v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.06568","description":"<p>We study a new generative modeling technique based on adversarial training\n(AT). We show that in a setting where the model is trained to discriminate\nin-distribution data from adversarial examples perturbed from out-distribution\nsamples, the model learns the support of the in-distribution data. The learning\nprocess is also closely related to MCMC-based maximum likelihood learning of\nenergy-based models (EBMs), and can be considered as an approximate maximum\nlikelihood learning method. We show that this AT generative model achieves\ncompetitive image generation performance to state-of-the-art EBMs, and at the\nsame time is stable to train and has better sampling efficiency. We demonstrate\nthat the AT generative model is well-suited for the task of image translation\nand worst-case out-of-distribution detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1\">Gustavo K. Rohde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Body Schema Adaptation through Cost-Sensitive Active Learning. (arXiv:2101.10892v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2101.10892","description":"<p>Humanoid robots have complex bodies and kinematic chains with several\nDegrees-of-Freedom (DoF) which are difficult to model. Learning the parameters\nof a kinematic model can be achieved by observing the position of the robot\nlinks during prospective motions and minimising the prediction errors. This\nwork proposes a movement efficient approach for estimating online the\nbody-schema of a humanoid robot arm in the form of Denavit-Hartenberg (DH)\nparameters. A cost-sensitive active learning approach based on the A-Optimality\ncriterion is used to select optimal joint configurations. The chosen joint\nconfigurations simultaneously minimise the error in the estimation of the body\nschema and minimise the movement between samples. This reduces energy\nconsumption, along with mechanical fatigue and wear, while not compromising the\nlearning accuracy. The work was implemented in a simulation environment, using\nthe 7DoF arm of the iCub robot simulator. The hand pose is measured with a\nsingle camera via markers placed in the palm and back of the robot's hand. A\nnon-parametric occlusion model is proposed to avoid choosing joint\nconfigurations where the markers are not visible, thus preventing worthless\nattempts. The results show cost-sensitive active learning has similar accuracy\nto the standard active learning approach, while reducing in about half the\nexecuted movement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cunha_G/0/1/0/all/0/1\">Gon&#xe7;alo Cunha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_P/0/1/0/all/0/1\">Pedro Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardino_A/0/1/0/all/0/1\">Alexandre Bernardino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_R/0/1/0/all/0/1\">Ricardo Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pl&#xed;nio Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Image Classifiers in Object Detectors. (arXiv:2106.05209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05209","description":"<p>Knowledge distillation constitutes a simple yet effective way to improve the\nperformance of a compact student network by exploiting the knowledge of a more\npowerful teacher. Nevertheless, the knowledge distillation literature remains\nlimited to the scenario where the student and the teacher tackle the same task.\nHere, we investigate the problem of transferring knowledge not only across\narchitectures but also across tasks. To this end, we study the case of object\ndetection and, instead of following the standard detector-to-detector\ndistillation approach, introduce a classifier-to-detector knowledge transfer\nframework. In particular, we propose strategies to exploit the classification\nteacher to improve both the detector's recognition accuracy and localization\nperformance. Our experiments on several detectors with different backbones\ndemonstrate the effectiveness of our approach, allowing us to outperform the\nstate-of-the-art detector-to-detector distillation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuxuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14350","description":"<p>Powerful deep learning algorithms open an opportunity for solving non-image\nMachine Learning (ML) problems by transforming these problems to into the image\nrecognition problems. The CPC-R algorithm presented in this chapter converts\nnon-image data into images by visualizing non-image data. Then deep learning\nCNN algorithms solve the learning problems on these images. The design of the\nCPC-R algorithm allows preserving all high-dimensional information in 2-D\nimages. The use of pair values mapping instead of single value mapping used in\nthe alternative approaches allows encoding each n-D point with 2 times fewer\nvisual elements. The attributes of an n-D point are divided into pairs of its\nvalues and each pair is visualized as 2-D points in the same 2-D Cartesian\ncoordinates. Next, grey scale or color intensity values are assigned to each\npair to encode the order of pairs. This is resulted in the heatmap image. The\ncomputational experiments with CPC-R are conducted for different CNN\narchitectures, and methods to optimize the CPC-R images showing that the\ncombined CPC-R and deep learning CNN algorithms are able to solve non-image ML\nproblems reaching high accuracy on the benchmark datasets. This chapter expands\nour prior work by adding more experiments to test accuracy of classification,\nexploring saliency and informativeness of discovered features to test their\ninterpretability, and generalizing the approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1\">Divya Chandrika Kalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1\">Bedant Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Bandit for Visual-aware Recommendation. (arXiv:2107.07438v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.07438","description":"<p>Online recommendation/advertising is ubiquitous in web business. Image\ndisplaying is considered as one of the most commonly used formats to interact\nwith customers. Contextual multi-armed bandit has shown success in the\napplication of advertising to solve the exploration-exploitation dilemma\nexisting in the recommendation procedure. Inspired by the visual-aware\nrecommendation, in this paper, we propose a contextual bandit algorithm, where\nthe convolutional neural network (CNN) is utilized to learn the reward function\nalong with an upper confidence bound (UCB) for exploration. We also prove a\nnear-optimal regret bound $\\tilde{\\mathcal{O}}(\\sqrt{T})$ when the network is\nover-parameterized, and establish strong connections with convolutional neural\ntangent kernel (CNTK). Finally, we evaluate the empirical performance of the\nproposed algorithm and show that it outperforms other state-of-the-art\nUCB-based bandit algorithms on real-world image data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rail Component Detection Based on AttnConv-Net. (arXiv:2108.02423v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02423","description":"<p>The automatic detection of major rail components using railway images is\nbeneficial to ensure the rail transport safety. In this paper, we propose an\nattention-powered deep convolutional network (AttnConv-net) to detect multiple\nrail components including the rail, clips, and bolts. The proposed method\nconsists of a deep convolutional neural network (DCNN) as the backbone,\ncascading attention blocks (CAB), and two feed forward networks (FFN). Two\ntypes of positional embedding are applied to enrich information in latent\nfeatures extracted from the backbone. Based on processed latent features, the\nCAB aims to learn the local context of rail components including their\ncategories and component boundaries. Final categories and bounding boxes are\ngenerated via two FFN implemented in parallel. To enhance the detection of\nsmall components, various data augmentation methods are employed in the\ntraining process. The effectiveness of the proposed AttnConv-net is validated\nwith one real dataset and another synthesized dataset. Compared with classic\nconvolutional neural network based methods, our proposed method simplifies the\ndetection pipeline by eliminating the need of prior- and post-processing, which\noffers a new speed-quality solution to enable faster and more accurate\nimage-based rail component detections\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangfang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Single-Image Defocus Deblurring: How Dual-Pixel Images Help Through Multi-Task Learning. (arXiv:2108.05251v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05251","description":"<p>Many camera sensors use a dual-pixel (DP) design that operates as a\nrudimentary light field providing two sub-aperture views of a scene in a single\ncapture. The DP sensor was developed to improve how cameras perform autofocus.\nSince the DP sensor's introduction, researchers have found additional uses for\nthe DP data, such as depth estimation, reflection removal, and defocus\ndeblurring. We are interested in the latter task of defocus deblurring. In\nparticular, we propose a single-image deblurring network that incorporates the\ntwo sub-aperture views into a multi-task framework. Specifically, we show that\njointly learning to predict the two DP views from a single blurry input image\nimproves the network's ability to learn to deblur the image. Our experiments\nshow this multi-task strategy achieves +1dB PSNR improvement over\nstate-of-the-art defocus deblurring methods. In addition, our multi-task\nframework allows accurate DP-view synthesis (e.g., ~39dB PSNR) from the single\ninput image. These high-quality DP views can be used for other DP-based\napplications, such as reflection removal. As part of this effort, we have\ncaptured a new dataset of 7,059 high-quality images to support our training for\nthe DP-view synthesis task. Our dataset, code, and trained models are publicly\navailable at\nhttps://github.com/Abdullah-Abuolaim/multi-task-defocus-deblurring-dual-pixel-nimat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EKTVQA: Generalized use of External Knowledge to empower Scene Text in Text-VQA. (arXiv:2108.09717v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09717","description":"<p>The open-ended question answering task of Text-VQA often requires reading and\nreasoning about rarely seen or completely unseen scene-text content of an\nimage. We address this zero-shot nature of the problem by proposing the\ngeneralized use of external knowledge to augment our understanding of the scene\ntext. We design a framework to extract, validate, and reason with knowledge\nusing a standard multimodal transformer for vision language understanding\ntasks. Through empirical evidence and qualitative results, we demonstrate how\nexternal knowledge can highlight instance-only cues and thus help deal with\ntraining data bias, improve answer entity type correctness, and detect\nmultiword named entities. We generate results comparable to the\nstate-of-the-art on three publicly available datasets, under the constraints of\nsimilar upstream OCR systems and training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arka Ujjal Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1\">Gaurav Harit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active label cleaning for improved dataset quality under resource constraints. (arXiv:2109.00574v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00574","description":"<p>Imperfections in data annotation, known as label noise, are detrimental to\nthe training of machine learning models and have an often-overlooked\nconfounding effect on the assessment of model performance. Nevertheless,\nemploying experts to remove label noise by fully re-annotating large datasets\nis infeasible in resource-constrained settings, such as healthcare. This work\nadvocates for a data-driven approach to prioritising samples for re-annotation\n- which we term \"active label cleaning\". We propose to rank instances according\nto estimated label correctness and labelling difficulty of each sample, and\nintroduce a simulation framework to evaluate relabelling efficacy. Our\nexperiments on natural images and on a new medical imaging benchmark show that\ncleaning noisy labels mitigates their negative impact on model training,\nevaluation, and selection. Crucially, the proposed active label cleaning\nenables correcting labels up to 4 times more effectively than typical random\nselection in realistic conditions, making better use of experts' valuable time\nfor improving dataset quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1\">Ryutaro Tanno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tezcan_K/0/1/0/all/0/1\">Kerem C. Tezcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1\">Miguel Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments. (arXiv:2109.06452v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06452","description":"<p>Spiking neural networks (SNNs) offer both compelling potential advantages,\nincluding energy efficiency and low latencies and challenges including the\nnon-differentiable nature of event spikes. Much of the initial research in this\narea has converted deep neural networks to equivalent SNNs, but this conversion\napproach potentially negates some of the advantages of SNN-based approaches\ndeveloped from scratch. One promising area for high-performance SNNs is\ntemplate matching and image recognition. This research introduces the first\nhigh-performance SNN for the Visual Place Recognition (VPR) task: given a query\nimage, the SNN has to find the closest match out of a list of reference images.\nAt the core of this new system is a novel assignment scheme that implements a\nform of ambiguity-informed salience, by up-weighting single-place-encoding\nneurons and down-weighting \"ambiguous\" neurons that respond to multiple\ndifferent reference places. In a range of experiments on the challenging\nNordland, Oxford RobotCar, SPEDTest, Synthia, and St Lucia datasets, we show\nthat our SNN achieves comparable VPR performance to state-of-the-art and\nclassical techniques, and degrades gracefully in performance with an increasing\nnumber of reference places. Our results provide a significant milestone towards\nSNNs that can provide robust, energy-efficient, and low latency robot\nlocalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussaini_S/0/1/0/all/0/1\">Somayeh Hussaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective. (arXiv:2110.03095v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03095","description":"<p>Deep neural networks (DNNs) often rely on easy-to-learn discriminatory\nfeatures, or cues, that are not necessarily essential to the problem at hand.\nFor example, ducks in an image may be recognized based on their typical\nbackground scenery, such as lakes or streams. This phenomenon, also known as\nshortcut learning, is emerging as a key limitation of the current generation of\nmachine learning models. In this work, we introduce a set of experiments to\ndeepen our understanding of shortcut learning and its implications. We design a\ntraining setup with several shortcut cues, named WCST-ML, where each cue is\nequally conducive to the visual recognition problem at hand. Even under equal\nopportunities, we observe that (1) certain cues are preferred to others, (2)\nsolutions biased to the easy-to-learn cues tend to converge to relatively flat\nminima on the loss surface, and (3) the solutions focusing on those preferred\ncues are far more abundant in the parameter space. We explain the abundance of\ncertain cues via their Kolmogorov (descriptional) complexity: solutions\ncorresponding to Kolmogorov-simple cues are abundant in the parameter space and\nare thus preferred by DNNs. Our studies are based on the synthetic dataset\nDSprites and the face dataset UTKFace. In our WCST-ML, we observe that the\ninborn bias of models leans toward simple cues, such as color and ethnicity.\nOur findings emphasize the importance of active human intervention to remove\nthe inborn model biases that may cause negative societal impacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1\">Luca Scimeca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Color To Identify Insider Threats. (arXiv:2111.13176v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13176","description":"<p>Insider threats are costly, hard to detect, and unfortunately rising in\noccurrence. Seeking to improve detection of such threats, we develop novel\ntechniques to enable us to extract powerful features and augment attack vectors\nfor greater classification power. Most importantly, we generate high quality\ncolor image encodings of user behavior that do not have the downsides of\ntraditional greyscale image encodings. Combined, they form Computer Vision User\nand Entity Behavior Analytics, a detection system designed from the ground up\nto improve upon advancements in academia and mitigate the issues that prevent\nthe usage of advanced models in industry. The proposed system beats\nstate-of-art methods used in academia and as well as in industry on a gold\nstandard benchmarking dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1\">Sameer Khanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Vision Transformers for Incremental Learning. (arXiv:2112.06103v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06103","description":"<p>This paper studies using Vision Transformers (ViT) in class incremental\nlearning. Surprisingly, naive application of ViT to replace convolutional\nneural networks (CNNs) results in performance degradation. Our analysis reveals\nthree issues of naively using ViT: (a) ViT has very slow convergence when class\nnumber is small, (b) more bias towards new classes is observed in ViT than\nCNN-based models, and (c) the proper learning rate of ViT is too low to learn a\ngood classifier. Base on this analysis, we show these issues can be simply\naddressed by using existing techniques: using convolutional stem, balanced\nfinetuning to correct bias, and higher learning rate for the classifier. Our\nsimple solution, named ViTIL (ViT for Incremental Learning), achieves the new\nstate-of-the-art for all three class incremental learning setups by a clear\nmargin, providing a strong baseline for the research community. For instance,\non ImageNet-1000, our ViTIL achieves 69.20% top-1 accuracy for the protocol of\n500 initial classes with 5 incremental steps (100 new classes for each),\noutperforming LUCIR+DDE by 1.69%. For more challenging protocol of 10\nincremental steps (100 new classes), our method outperforms PODNet by 7.27%\n(65.13% vs. 57.86%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Birds Eye View Social Distancing Analysis System. (arXiv:2112.07159v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07159","description":"<p>Social distancing can reduce the infection rates in respiratory pandemics\nsuch as COVID-19. Traffic intersections are particularly suitable for\nmonitoring and evaluation of social distancing behavior in metropolises. We\npropose and evaluate a privacy-preserving social distancing analysis system\n(B-SDA), which uses bird's-eye view video recordings of pedestrians who cross\ntraffic intersections. We devise algorithms for video pre-processing, object\ndetection and tracking which are rooted in the known computer-vision and deep\nlearning techniques, but modified to address the problem of detecting very\nsmall objects/pedestrians captured by a highly elevated camera. We propose a\nmethod for incorporating pedestrian grouping for detection of social distancing\nviolations. B-SDA is used to compare pedestrian behavior based on pre-pandemic\nand pandemic videos in a major metropolitan area. The accomplished pedestrian\ndetection performance is $63.0\\%$ $AP_{50}$ and the tracking performance is\n$47.6\\%$ MOTA. The social distancing violation rate of $15.6\\%$ during the\npandemic is notably lower than $31.4\\%$ pre-pandemic baseline, indicating that\npedestrians followed CDC-prescribed social distancing recommendations. The\nproposed system is suitable for deployment in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingfei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongzhe Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zihao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zussman_G/0/1/0/all/0/1\">Gil Zussman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zoran Kostic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (arXiv:2201.11528v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11528","description":"<p>Adversarial examples have posed a severe threat to deep neural networks due\nto their transferable nature. Currently, various works have paid great efforts\nto enhance the cross-model transferability, which mostly assume the substitute\nmodel is trained in the same domain as the target model. However, in reality,\nthe relevant information of the deployed model is unlikely to leak. Hence, it\nis vital to build a more practical black-box threat model to overcome this\nlimitation and evaluate the vulnerability of deployed models. In this paper,\nwith only the knowledge of the ImageNet domain, we propose a Beyond ImageNet\nAttack (BIA) to investigate the transferability towards black-box domains\n(unknown classification tasks). Specifically, we leverage a generative model to\nlearn the adversarial function for disrupting low-level features of input\nimages. Based on this framework, we further propose two variants to narrow the\ngap between the source and target domains from the data and model perspectives,\nrespectively. Extensive experiments on coarse-grained and fine-grained domains\ndemonstrate the effectiveness of our proposed methods. Notably, our methods\noutperform state-of-the-art approaches by up to 7.71\\% (towards coarse-grained\ndomains) and 25.91\\% (towards fine-grained domains) on average. Our code is\navailable at \\url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory Forecasting from Detection with Uncertainty-Aware Motion Encoding. (arXiv:2202.01478v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01478","description":"<p>Trajectory forecasting is critical for autonomous platforms to make safe\nplanning and actions. Currently, most trajectory forecasting methods assume\nthat object trajectories have been extracted and directly develop trajectory\npredictors based on the ground truth trajectories. However, this assumption\ndoes not hold in practical situations. Trajectories obtained from object\ndetection and tracking are inevitably noisy, which could cause serious\nforecasting errors to predictors built on ground truth trajectories. In this\npaper, we propose a trajectory predictor directly based on detection results\nwithout relying on explicitly formed trajectories. Different from the\ntraditional methods which encode the motion cue of an agent based on its\nclearly defined trajectory, we extract the motion information only based on the\naffinity cues among detection results, in which an affinity-aware state update\nmechanism is designed to take the uncertainty of association into account. In\naddition, considering that there could be multiple plausible matching\ncandidates, we aggregate the states of them. This design relaxes the\nundesirable effect of noisy trajectory obtained from data association.\nExtensive ablation experiments validate the effectiveness of our method and its\ngeneralization ability on different detectors. Cross-comparison to other\nforecasting schemes further proves the superiority of our method. Code will be\nreleased upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jianru Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jianwu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework. (arXiv:2202.02626v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02626","description":"<p>Deep neural network models are used today in various applications of\nartificial intelligence, the strengthening of which, in the face of adversarial\nattacks is of particular importance. An appropriate solution to adversarial\nattacks is adversarial training, which reaches a trade-off between robustness\nand generalization. This paper introduces a novel framework (Layer\nSustainability Analysis (LSA)) for the analysis of layer vulnerability in a\ngiven neural network in the scenario of adversarial attacks. LSA can be a\nhelpful toolkit to assess deep neural networks and to extend adversarial\ntraining approaches towards improving the sustainability of model layers via\nlayer monitoring and analysis. The LSA framework identifies a list of Most\nVulnerable Layers (MVL list) of a given network. The relative error, as a\ncomparison measure, is used to evaluate the representation sustainability of\neach layer against adversarial attack inputs. The proposed approach for\nobtaining robust neural networks to fend off adversarial attacks is based on a\nlayer-wise regularization (LR) over LSA proposal(s) for adversarial training\n(AT); i.e. the AT-LR procedure. AT-LR could be used with any benchmark\nadversarial attack to reduce the vulnerability of network layers and to improve\nconventional adversarial training approaches. The proposed idea performs well\ntheoretically and experimentally for state-of-the-art multilayer perceptron and\nconvolutional neural network architectures. Compared with the AT-LR and its\ncorresponding base adversarial training, the classification accuracy of more\nsignificant perturbations increased by 16.35%, 21.79%, and 10.730% on Moon,\nMNIST, and CIFAR-10 benchmark datasets in comparison with the AT-LR and its\ncorresponding base adversarial training, respectively. The LSA framework is\navailable and published at https://github.com/khalooei/LSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalooei_M/0/1/0/all/0/1\">Mohammad Khalooei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayounpour_M/0/1/0/all/0/1\">Mohammad Mehdi Homayounpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirmazlaghani_M/0/1/0/all/0/1\">Maryam Amirmazlaghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Contrastive Quantization for Efficient Cross-View Video Retrieval. (arXiv:2202.03384v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2202.03384","description":"<p>With the recent boom of video-based social platforms (e.g., YouTube and\nTikTok), video retrieval using sentence queries has become an important demand\nand attracts increasing research attention. Despite the decent performance,\nexisting text-video retrieval models in vision and language communities are\nimpractical for large-scale Web search because they adopt brute-force search\nbased on high-dimensional embeddings. To improve efficiency, Web search engines\nwidely apply vector compression libraries (e.g., FAISS) to post-process the\nlearned embeddings. Unfortunately, separate compression from feature encoding\ndegrades the robustness of representations and incurs performance decay. To\npursue a better balance between performance and efficiency, we propose the\nfirst quantized representation learning method for cross-view video retrieval,\nnamely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both\ncoarse-grained and fine-grained quantizations with transformers, which provide\ncomplementary understandings for texts and videos and preserve comprehensive\nsemantic information. By performing Asymmetric-Quantized Contrastive Learning\n(AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and\nmultiple fine-grained levels. This hybrid-grained learning strategy serves as\nstrong supervision on the cross-view video quantization model, where\ncontrastive learning at different levels can be mutually promoted. Extensive\nexperiments on three Web video benchmark datasets demonstrate that HCQ achieves\ncompetitive performance with state-of-the-art non-compressed retrieval methods\nwhile showing high efficiency in storage and computation. Code and\nconfigurations are available at https://github.com/gimpong/WWW22-HCQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1\">Dongliang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongfu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-Aware Transformer For Occluded Person Re-identification. (arXiv:2202.04243v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04243","description":"<p>Recently, occluded person re-identification(Re-ID) remains a challenging task\nthat people are frequently obscured by other people or obstacles, especially in\na crowd massing situation. In this paper, we propose a self-supervised deep\nlearning method to improve the location performance for human parts through\noccluded person Re-ID. Unlike previous works, we find that motion information\nderived from the photos of various human postures can help identify major human\nbody components. Firstly, a motion-aware transformer encoder-decoder\narchitecture is designed to obtain keypoints heatmaps and part-segmentation\nmaps. Secondly, an affine transformation module is utilized to acquire motion\ninformation from the keypoint detection branch. Then the motion information\nwill support the segmentation branch to achieve refined human part segmentation\nmaps, and effectively divide the human body into reasonable groups. Finally,\nseveral cases demonstrate the efficiency of the proposed model in\ndistinguishing different representative parts of the human body, which can\navoid the background and occlusion disturbs. Our method consistently achieves\nstate-of-the-art results on several popular datasets, including occluded,\npartial, and holistic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhekun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRAT-Pred: Vehicle Trajectory Prediction with Crystal Graph Convolutional Neural Networks and Multi-Head Self-Attention. (arXiv:2202.04488v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04488","description":"<p>Predicting the motion of surrounding vehicles is essential for autonomous\nvehicles, as it governs their own motion plan. Current state-of-the-art vehicle\nprediction models heavily rely on map information. In reality, however, this\ninformation is not always available. We therefore propose CRAT-Pred, a\nmulti-modal and non-rasterization-based trajectory prediction model,\nspecifically designed to effectively model social interactions between\nvehicles, without relying on map information. CRAT-Pred applies a graph\nconvolution method originating from the field of material science to vehicle\nprediction, allowing to efficiently leverage edge features, and combines it\nwith multi-head self-attention. Compared to other map-free approaches, the\nmodel achieves state-of-the-art performance with a significantly lower number\nof model parameters. In addition to that, we quantitatively show that the\nself-attention mechanism is able to learn social interactions between vehicles,\nwith the weights representing a measurable interaction score. The source code\nis publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1\">Julian Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_J/0/1/0/all/0/1\">Julian Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritschneder_F/0/1/0/all/0/1\">Franz Gritschneder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04595","description":"<p>Neural image compression have reached or out-performed traditional methods\n(such as JPEG, BPG, WebP). However,their sophisticated network structures with\ncascaded convolution layers bring heavy computational burden for practical\ndeployment. In this paper, we explore the structural sparsity in neural image\ncompression network to obtain real-time acceleration without any specialized\nhardware design or algorithm. We propose a simple plug-in adaptive binary\nchannel masking(ABCM) to judge the importance of each convolution channel and\nintroduce sparsity during training. During inference, the unimportant channels\nare pruned to obtain slimmer network and less computation. We implement our\nmethod into three neural image compression networks with different entropy\nmodels to verify its effectiveness and generalization, the experiment results\nshow that up to 7x computation reduction and 3x acceleration can be achieved\nwith negligible performance drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yin_S/0/1/0/all/0/1\">Shanzhi Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1\">Fanyang Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Wen Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Youneng Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yongsheng Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}