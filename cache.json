{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-12-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies. (arXiv:2112.11471v1 [cs.AI])","link":"http://arxiv.org/abs/2112.11471","description":"<p>As AI systems demonstrate increasingly strong predictive performance, their\nadoption has grown in numerous domains. However, in high-stakes domains such as\ncriminal justice and healthcare, full automation is often not desirable due to\nsafety, ethical, and legal concerns, yet fully manual approaches can be\ninaccurate and time consuming. As a result, there is growing interest in the\nresearch community to augment human decision making with AI assistance. Besides\ndeveloping AI technologies for this purpose, the emerging field of human-AI\ndecision making must embrace empirical approaches to form a foundational\nunderstanding of how humans interact and work with AI to make decisions. To\ninvite and help structure research efforts towards a science of understanding\nand improving human-AI decision making, we survey recent literature of\nempirical human-subject studies on this topic. We summarize the study design\nchoices made in over 100 papers in three important aspects: (1) decision tasks,\n(2) AI models and AI assistance elements, and (3) evaluation metrics. For each\naspect, we summarize current trends, discuss gaps in current practices of the\nfield, and make a list of recommendations for future research. Our survey\nhighlights the need to develop common frameworks to account for the design and\nresearch spaces of human-AI decision making, so that researchers can make\nrigorous choices in study design, and the research community can build on each\nother's work and produce generalizable scientific knowledge. We also hope this\nsurvey will serve as a bridge for HCI and AI communities to work together to\nmutually shape the empirical science and computational technologies for\nhuman-AI decision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Vivian Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chacha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Q. Vera Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_Renner_A/0/1/0/all/0/1\">Alison Smith-Renner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSH methods for data deduplication in a Wikipedia artificial dataset. (arXiv:2112.11478v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11478","description":"<p>This paper illustrates locality sensitive hasing (LSH) models for the\nidentification and removal of nearly redundant data in a text dataset. To\nevaluate the different models, we create an artificial dataset for data\ndeduplication using English Wikipedia articles. Area-Under-Curve (AUC) over 0.9\nwere observed for most models, with the best model reaching 0.96. Deduplication\nenables more effective model training by preventing the model from learning a\ndistribution that differs from the real one as a result of the repeated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciro_J/0/1/0/all/0/1\">Juan Ciro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galvez_D/0/1/0/all/0/1\">Daniel Galvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlippe_T/0/1/0/all/0/1\">Tim Schlippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanter_D/0/1/0/all/0/1\">David Kanter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AtteSTNet -- An attention and subword tokenization based approach for code-switched Hindi-English hate speech detection. (arXiv:2112.11479v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11479","description":"<p>Recent advancements in technology have led to a boost in social media usage\nwhich has ultimately led to large amounts of user-generated data which also\nincludes hateful and offensive speech. The language used in social media is\noften a combination of English and the native language in the region. In India,\nHindi is used predominantly and is often code-switched with English, giving\nrise to the Hinglish (Hindi+English) language. Various approaches have been\nmade in the past to classify the code-mixed Hinglish hate speech using\ndifferent machine learning and deep learning-based techniques. However, these\ntechniques make use of recurrence on convolution mechanisms which are\ncomputationally expensive and have high memory requirements. Past techniques\nalso make use of complex data processing making the existing techniques very\ncomplex and non-sustainable to change in data. We propose a much simpler\napproach which is not only at par with these complex networks but also exceeds\nperformance with the use of subword tokenization algorithms like BPE and\nUnigram along with multi-head attention-based technique giving an accuracy of\n87.41% and F1 score of 0.851 on standard datasets. Efficient use of BPE and\nUnigram algorithms help handle the non-conventional Hinglish vocabulary making\nour technique simple, efficient and sustainable to use in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagh_V/0/1/0/all/0/1\">Vedangi Wagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shingi_G/0/1/0/all/0/1\">Geet Shingi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Compression of Natural Language Models. (arXiv:2112.11480v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11480","description":"<p>Deep neural networks are effective feature extractors but they are\nprohibitively large for deployment scenarios. Due to the huge number of\nparameters, interpretability of parameters in different layers is not\nstraight-forward. This is why neural networks are sometimes considered black\nboxes. Although simpler models are easier to explain, finding them is not easy.\nIf found, a sparse network that can fit to a data from scratch would help to\ninterpret parameters of a neural network. To this end, lottery ticket\nhypothesis states that typical dense neural networks contain a small sparse\nsub-network that can be trained to a reach similar test accuracy in an equal\nnumber of steps. The goal of this work is to assess whether such a trainable\nsubnetwork exists for natural language models (NLM)s. To achieve this goal we\nwill review state-of-the-art compression techniques such as quantization,\nknowledge distillation, and pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damadi_S/0/1/0/all/0/1\">Saeed Damadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Human Mobility Forecasting through Natural Language Generation. (arXiv:2112.11481v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11481","description":"<p>Existing human mobility forecasting models follow the standard design of the\ntime-series prediction model which takes a series of numerical values as input\nto generate a numerical value as a prediction. Although treating this as a\nregression problem seems straightforward, incorporating various contextual\ninformation such as the semantic category information of each Place-of-Interest\n(POI) is a necessary step, and often the bottleneck, in designing an effective\nmobility prediction model. As opposed to the typical approach, we treat\nforecasting as a translation problem and propose a novel forecasting through a\nlanguage generation pipeline. The paper aims to address the human mobility\nforecasting problem as a language translation task in a sequence-to-sequence\nmanner. A mobility-to-language template is first introduced to describe the\nnumerical mobility data as natural language sentences. The core intuition of\nthe human mobility forecasting translation task is to convert the input\nmobility description sentences into a future mobility description from which\nthe prediction target can be obtained. Under this pipeline, a two-branch\nnetwork, SHIFT (Translating Human Mobility Forecasting), is designed.\nSpecifically, it consists of one main branch for language generation and one\nauxiliary branch to directly learn mobility patterns. During the training, we\ndevelop a momentum mode for better connecting and training the two branches.\nExtensive experiments on three real-world datasets demonstrate that the\nproposed SHIFT is effective and presents a new revolutionary approach to\nforecasting human mobility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yongli Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarke_C/0/1/0/all/0/1\">Charles L. A. Clarke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"English2Gbe: A multilingual machine translation model for {Fon/Ewe}Gbe. (arXiv:2112.11482v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11482","description":"<p>Language is an essential factor of emancipation. Unfortunately, most of the\nmore than 2,000 African languages are low-resourced. The community has recently\nused machine translation to revive and strengthen several African languages.\nHowever, the trained models are often bilingual, resulting in a potentially\nexponential number of models to train and maintain to cover all possible\ntranslation directions. Additionally, bilingual models do not leverage the\nsimilarity between some of the languages. Consequently, multilingual neural\nmachine translation (NMT) is gaining considerable interest, especially for\nlow-resourced languages. Nevertheless, its adoption by the community is still\nlimited. This paper introduces English2Gbe, a multilingual NMT model capable of\ntranslating from English to Ewe or Fon. Using the BLEU, CHRF, and TER scores\ncomputed with the Sacrebleu (Post, 2018) package for reproducibility, we show\nthat English2Gbe outperforms bilingual models (English to Ewe and English to\nFon) and gives state-of-the-art results on the JW300 benchmark for Fon\nestablished by Nekoto et al. (2020). We hope this work will contribute to the\nmassive adoption of Multilingual models inside the community. Our code is made\naccessible from Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1\">Gilles Hacheme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BACON: Deep-Learning Powered AI for Poetry Generation with Author Linguistic Style Transfer. (arXiv:2112.11483v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11483","description":"<p>This paper describes BACON, a basic prototype of an automatic poetry\ngenerator with author linguistic style transfer. It combines concepts and\ntechniques from finite state machinery, probabilistic models, artificial neural\nnetworks and deep learning, to write original poetry with rich\naesthetic-qualities in the style of any given author. Extrinsic evaluation of\nthe output generated by BACON shows that participants were unable to tell the\ndifference between human and AI-generated poems in any statistically\nsignificant way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pascual_A/0/1/0/all/0/1\">Alejandro Rodriguez Pascual</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Embeddings and High-speed Similarity Search for Fast Computer Assisted Annotation of Legal Documents. (arXiv:2112.11494v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11494","description":"<p>Human-performed annotation of sentences in legal documents is an important\nprerequisite to many machine learning based systems supporting legal tasks.\nTypically, the annotation is done sequentially, sentence by sentence, which is\noften time consuming and, hence, expensive. In this paper, we introduce a\nproof-of-concept system for annotating sentences \"laterally.\" The approach is\nbased on the observation that sentences that are similar in meaning often have\nthe same label in terms of a particular type system. We use this observation in\nallowing annotators to quickly view and annotate sentences that are\nsemantically similar to a given sentence, across an entire corpus of documents.\nHere, we present the interface of the system and empirically evaluate the\napproach. The experiments show that lateral annotation has the potential to\nmake the annotation process quicker and more consistent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Westermann_H/0/1/0/all/0/1\">Hannes Westermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_V/0/1/0/all/0/1\">Vern R. Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin D. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benyekhlef_K/0/1/0/all/0/1\">Karim Benyekhlef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Precision of Quantization of Transformer Language Models for Speech Recognition. (arXiv:2112.11540v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11540","description":"<p>State-of-the-art neural language models represented by Transformers are\nbecoming increasingly complex and expensive for practical applications. Low-bit\ndeep neural network quantization techniques provides a powerful solution to\ndramatically reduce their model size. Current low-bit quantization methods are\nbased on uniform precision and fail to account for the varying performance\nsensitivity at different parts of the system to quantization errors. To this\nend, novel mixed precision DNN quantization methods are proposed in this paper.\nThe optimal local precision settings are automatically learned using two\ntechniques. The first is based on a quantization sensitivity metric in the form\nof Hessian trace weighted quantization perturbation. The second is based on\nmixed precision Transformer architecture search. Alternating direction methods\nof multipliers (ADMM) are used to efficiently train mixed precision quantized\nDNN systems. Experiments conducted on Penn Treebank (PTB) and a Switchboard\ncorpus trained LF-MMI TDNN system suggest the proposed mixed precision\nTransformer quantization techniques achieved model size compression ratios of\nup to 16 times over the full precision baseline with no recognition performance\ndegradation. When being used to compress a larger full precision Transformer LM\nwith more layers, overall word error rate (WER) reductions up to 1.7% absolute\n(18% relative) were obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junhao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shoukang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diformer: Directional Transformer for Neural Machine Translation. (arXiv:2112.11632v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11632","description":"<p>Autoregressive (AR) and Non-autoregressive (NAR) models have their own\nsuperiority on the performance and latency, combining them into one model may\ntake advantage of both. Current combination frameworks focus more on the\nintegration of multiple decoding paradigms with a unified generative model,\ne.g. Masked Language Model. However, the generalization can be harmful to the\nperformance due to the gap between training objective and inference. In this\npaper, we aim to close the gap by preserving the original objective of AR and\nNAR under a unified framework. Specifically, we propose the Directional\nTransformer (Diformer) by jointly modelling AR and NAR into three generation\ndirections (left-to-right, right-to-left and straight) with a newly introduced\ndirection variable, which works by controlling the prediction of each token to\nhave specific dependencies under that direction. The unification achieved by\ndirection successfully preserves the original dependency assumption used in AR\nand NAR, retaining both generalization and performance. Experiments on 4 WMT\nbenchmarks demonstrate that Diformer outperforms current united-modelling works\nwith more than 1.5 BLEU points for both AR and NAR decoding, and is also\ncompetitive to the state-of-the-art independent AR and NAR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hengchao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinglu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency and Coherence from Points of Contextual Similarity. (arXiv:2112.11638v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11638","description":"<p>Factual consistency is one of important summary evaluation dimensions,\nespecially as summary generation becomes more fluent and coherent. The ESTIME\nmeasure, recently proposed specifically for factual consistency, achieves high\ncorrelations with human expert scores both for consistency and fluency, while\nin principle being restricted to evaluating such text-summary pairs that have\nhigh dictionary overlap. This is not a problem for current styles of\nsummarization, but it may become an obstacle for future summarization systems,\nor for evaluating arbitrary claims against the text. In this work we generalize\nthe method, making it applicable to any text-summary pairs. As ESTIME uses\npoints of contextual similarity, it provides insights into usefulness of\ninformation taken from different BERT layers. We observe that useful\ninformation exists in almost all of the layers except the several lowest ones.\nFor consistency and fluency - qualities focused on local text details - the\nmost useful layers are close to the top (but not at the top); for coherence and\nrelevance we found a more complicated and interesting picture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasilyev_O/0/1/0/all/0/1\">Oleg Vasilyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_J/0/1/0/all/0/1\">John Bohannon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Distillation Mixup Training for Non-autoregressive Neural Machine Translation. (arXiv:2112.11640v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11640","description":"<p>Recently, non-autoregressive (NAT) models predict outputs in parallel,\nachieving substantial improvements in generation speed compared to\nautoregressive (AT) models. While performing worse on raw data, most NAT models\nare trained as student models on distilled data generated by AT teacher models,\nwhich is known as sequence-level Knowledge Distillation. An effective training\nstrategy to improve the performance of AT models is Self-Distillation Mixup\n(SDM) Training, which pre-trains a model on raw data, generates distilled data\nby the pre-trained model itself and finally re-trains a model on the\ncombination of raw data and distilled data. In this work, we aim to view SDM\nfor NAT models, but find directly adopting SDM to NAT models gains no\nimprovements in terms of translation quality. Through careful analysis, we\nobserve the invalidation is correlated to Modeling Diversity and Confirmation\nBias between the AT teacher model and the NAT student models. Based on these\nfindings, we propose an enhanced strategy named SDMRT by adding two stages to\nclassic SDM: one is Pre-Rerank on self-distilled data, the other is Fine-Tune\non Filtered teacher-distilled data. Our results outperform baselines by 0.6 to\n1.2 BLEU on multiple NAT models. As another bonus, for Iterative Refinement NAT\nmodels, our methods can outperform baselines within half iteration number,\nwhich means 2X acceleration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hengchao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengzhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhanglin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lizhi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+tao_s/0/1/0/all/0/1\">shimin tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models. (arXiv:2112.11642v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11642","description":"<p>Deep encoders have been proven to be effective in improving neural machine\ntranslation (NMT) systems, but it reaches the upper bound of translation\nquality when the number of encoder layers exceeds 18. Worse still, deeper\nnetworks consume a lot of memory, making it impossible to train efficiently. In\nthis paper, we present Symbiosis Networks, which include a full network as the\nSymbiosis Main Network (M-Net) and another shared sub-network with the same\nstructure but less layers as the Symbiotic Sub Network (S-Net). We adopt\nSymbiosis Networks on Transformer-deep (m-n) architecture and define a\nparticular regularization loss $\\mathcal{L}_{\\tau}$ between the M-Net and S-Net\nin NMT. We apply joint-training on the Symbiosis Networks and aim to improve\nthe M-Net performance. Our proposed training strategy improves Transformer-deep\n(12-6) by 0.61, 0.49 and 0.69 BLEU over the baselines under classic training on\nWMT'14 EN-&gt;DE, DE-&gt;EN and EN-&gt;FR tasks. Furthermore, our Transformer-deep\n(12-6) even outperforms classic Transformer-deep (18-6).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengzhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hengchao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhanglin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lizhi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+tao_s/0/1/0/all/0/1\">shimin tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?. (arXiv:2112.11668v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11668","description":"<p>The fine-tuning of pre-trained language models has a great success in many\nNLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g.,\nword substitution attacks using only synonyms can easily fool a BERT-based\nsentiment analysis model. In this paper, we demonstrate that adversarial\ntraining, the prevalent defense technique, does not directly fit a conventional\nfine-tuning scenario, because it suffers severely from catastrophic forgetting:\nfailing to retain the generic and robust linguistic features that have already\nbeen captured by the pre-trained model. In this light, we propose Robust\nInformative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an\ninformation-theoretical perspective. In particular, RIFT encourages an\nobjective model to retain the features learned from the pre-trained model\nthroughout the entire fine-tuning process, whereas a conventional one only uses\nthe pre-trained weights for initialization. Experimental results show that RIFT\nconsistently outperforms the state-of-the-arts on two popular NLP tasks:\nsentiment analysis and natural language inference, under different attacks\nacross various pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinhsuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1\">Luu Anh Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Min Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization. (arXiv:2112.11670v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11670","description":"<p>The Query Focused Text Summarization (QFTS) task aims at building systems\nthat generate the summary of the text document(s) based on the given query. A\nkey challenge in addressing this task is the lack of large labeled data for\ntraining the summarization model. In this paper, we address this challenge by\nexploring a series of domain adaptation techniques. Given the recent success of\npre-trained transformer models in a wide range of natural language processing\ntasks, we utilize such models to generate abstractive summaries for the QFTS\ntask for both single-document and multi-document scenarios. For domain\nadaptation, we apply a variety of techniques using pre-trained\ntransformer-based summarization models including transfer learning, weakly\nsupervised learning, and distant supervision. Extensive experiments on six\ndatasets show that our proposed approach is very effective in generating\nabstractive summaries for the QFTS task while setting a new state-of-the-art\nresult in several datasets across a set of automatic and human evaluation\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Xiangji Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Curriculum Learning for Emotion Recognition in Conversation. (arXiv:2112.11718v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11718","description":"<p>Emotion recognition in conversation (ERC) aims to detect the emotion label\nfor each utterance. Motivated by recent studies which have proven that feeding\ntraining examples in a meaningful order rather than considering them randomly\ncan boost the performance of models, we propose an ERC-oriented hybrid\ncurriculum learning framework. Our framework consists of two curricula: (1)\nconversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In\nCC, we construct a difficulty measurer based on \"emotion shift\" frequency\nwithin a conversation, then the conversations are scheduled in an \"easy to\nhard\" schema according to the difficulty score returned by the difficulty\nmeasurer. For UC, it is implemented from an emotion-similarity perspective,\nwhich progressively strengthens the model's ability in identifying the\nconfusing emotions. With the proposed model-agnostic hybrid curriculum learning\nstrategy, we observe significant performance boosts over a wide range of\nexisting ERC models and we are able to achieve new state-of-the-art results on\nfour public ERC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yue Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Longjun Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Natural Language Generation. (arXiv:2112.11739v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11739","description":"<p>This paper offers a comprehensive review of the research on Natural Language\nGeneration (NLG) over the past two decades, especially in relation to\ndata-to-text generation and text-to-text generation deep learning methods, as\nwell as new applications of NLG technology. This survey aims to (a) give the\nlatest synthesis of deep learning research on the NLG core tasks, as well as\nthe architectures adopted in the field; (b) detail meticulously and\ncomprehensively various NLG tasks and datasets, and draw attention to the\nchallenges in NLG evaluation, focusing on different evaluation methods and\ntheir relationships; (c) highlight some future emphasis and relatively recent\nresearch issues that arise due to the increasing synergy between NLG and other\nartificial intelligence areas, such as computer vision, text and computational\ncreativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Haifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Miaoxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Label Dependence-aware Sequence Generation Model for Multi-level Implicit Discourse Relation Recognition. (arXiv:2112.11740v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11740","description":"<p>Implicit discourse relation recognition (IDRR) is a challenging but crucial\ntask in discourse analysis. Most existing methods train multiple models to\npredict multi-level labels independently, while ignoring the dependence between\nhierarchically structured labels. In this paper, we consider multi-level IDRR\nas a conditional label sequence generation task and propose a Label\nDependence-aware Sequence Generation Model (LDSGM) for it. Specifically, we\nfirst design a label attentive encoder to learn the global representation of an\ninput instance and its level-specific contexts, where the label dependence is\nintegrated to obtain better label embeddings. Then, we employ a label sequence\ndecoder to output the predicted labels in a top-down manner, where the\npredicted higher-level labels are directly used to guide the label prediction\nat the current level. We further develop a mutual learning enhanced training\nmethod to exploit the label dependence in a bottomup direction, which is\ncaptured by an auxiliary decoder introduced during training. Experimental\nresults on the PDTB dataset show that our model achieves the state-of-the-art\nperformance on multi-level IDRR. We will release our code at\nhttps://github.com/nlpersECJTU/LDSGM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Changxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liuwen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Theoretical Complexity and Boolean Satisfiability. (arXiv:2112.11769v1 [cs.CC])","link":"http://arxiv.org/abs/2112.11769","description":"<p>Theoretical complexity is a vital subfield of computer science that enables\nus to mathematically investigate computation and answer many interesting\nqueries about the nature of computational problems. It provides theoretical\ntools to assess time and space requirements of computations along with\nassessing the difficultly of problems - classifying them accordingly. It also\ngarners at its core one of the most important problems in mathematics, namely,\nthe $\\textbf{P vs. NP}$ millennium problem. In essence, this problem asks\nwhether solution and verification reside on two different levels of difficulty.\nIn this thesis, we introduce some of the most central concepts in the Theory of\nComputing, giving an overview of how computation can be abstracted using Turing\nmachines. Further, we introduce the two most famous problem complexity classes\n$\\textbf{P}$ and $\\textbf{NP}$ along with the relationship between them. In\naddition, we explicate the concept of problem reduction and how it is an\nessential tool for making hardness comparisons between different problems.\nLater, we present the problem of Boolean Satisfiability (SAT) which lies at the\ncenter of NP-complete problems. We then explore some of its tractable as well\nas intractable variants such as Horn-SAT and 3-SAT, respectively. Last but not\nleast, we establish polynomial-time reductions from 3-SAT to some of the famous\nNP-complete graph problems, namely, Clique Finding, Hamiltonian Cycle Finding,\nand 3-Coloring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_M/0/1/0/all/0/1\">Mohamed Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniora_D/0/1/0/all/0/1\">Dauod Siniora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Importance of the Current Input in Sequence Modeling. (arXiv:2112.11776v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11776","description":"<p>The last advances in sequence modeling are mainly based on deep learning\napproaches. The current state of the art involves the use of variations of the\nstandard LSTM architecture, combined with several tricks that improve the final\nprediction rates of the trained neural networks. However, in some cases, these\nadaptations might be too much tuned to the particular problems being addressed.\nIn this article, we show that a very simple idea, to add a direct connection\nbetween the input and the output, skipping the recurrent module, leads to an\nincrease of the prediction accuracy in sequence modeling problems related to\nnatural language processing. Experiments carried out on different problems show\nthat the addition of this kind of connection to a recurrent network always\nimproves the results, regardless of the architecture and training-specific\ndetails. When this idea is introduced into the models that lead the field, the\nresulting networks achieve a new state-of-the-art perplexity in language\nmodeling problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oliva_C/0/1/0/all/0/1\">Christian Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lago_Fernandez_L/0/1/0/all/0/1\">Luis F. Lago-Fern&#xe1;ndez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEREO: Scientific Text Reuse in Open Access Publications. (arXiv:2112.11800v1 [cs.DL])","link":"http://arxiv.org/abs/2112.11800","description":"<p>We present the Webis-STEREO-21 dataset, a massive collection of Scientific\nText Reuse in Open-access publications. It contains more than 91 million cases\nof reused text passages found in 4.2 million unique open-access publications.\nFeaturing a high coverage of scientific disciplines and varieties of reuse, as\nwell as comprehensive metadata to contextualize each case, our dataset\naddresses the most salient shortcomings of previous ones on scientific writing.\nWebis-STEREO-21 allows for tackling a wide range of research questions from\ndifferent scientific backgrounds, facilitating both qualitative and\nquantitative analysis of the phenomenon as well as a first-time grounding on\nthe base rate of text reuse in scientific publications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gienapp_L/0/1/0/all/0/1\">Lukas Gienapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kircheis_W/0/1/0/all/0/1\">Wolfgang Kircheis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sievers_B/0/1/0/all/0/1\">Bjarne Sievers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Analysis of memes for sentiment extraction. (arXiv:2112.11850v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11850","description":"<p>Memes are one of the most ubiquitous forms of social media communication. The\nstudy and processing of memes, which are intrinsically multimedia, is a popular\ntopic right now. The study presented in this research is based on the Memotion\ndataset, which involves categorising memes based on irony, comedy, motivation,\nand overall-sentiment. Three separate innovative transformer-based techniques\nhave been developed, and their outcomes have been thoroughly reviewed.The best\nalgorithm achieved a macro F1 score of 0.633 for humour classification, 0.55\nfor motivation classification, 0.61 for sarcasm classification, and 0.575 for\noverall sentiment of the meme out of all our techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alluri_N/0/1/0/all/0/1\">Nayan Varma Alluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_N/0/1/0/all/0/1\">Neeli Dheeraj Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Multi-hop Question Answering over Knowledge Base. (arXiv:2112.11909v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11909","description":"<p>Previous work on Chinese Knowledge Base Question Answering has been\nrestricted due to the lack of complex Chinese semantic parsing dataset and the\nexponentially growth of searching space with the length of relation paths. This\npaper proposes an efficient pipeline method equipped with a pre-trained\nlanguage model and a strategy to construct artificial training samples, which\nonly needs small amount of data but performs well on open-domain complex\nChinese Question Answering task. Besides, By adopting a Beam Search algorithm\nbased on a language model marking scores for candidate query tuples, we\ndecelerate the growing relation paths when generating multi-hop query paths.\nFinally, we evaluate our model on CCKS2019 Complex Question Answering via\nKnowledge Base task and achieves F1-score of 62.55\\% on the test dataset.\nMoreover when training with only 10\\% data, our model can still achieves\nF1-score of 58.54\\%. The result shows the capability of our model to process\nKBQA task and the advantage in few-shot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meihao_F/0/1/0/all/0/1\">Fan Meihao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Interactive Language Modeling. (arXiv:2112.11911v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11911","description":"<p>Interaction between caregivers and children plays a critical role in human\nlanguage acquisition and development. Given this observation, it is remarkable\nthat explicit interaction plays little to no role in artificial language\nmodeling -- which also targets the acquisition of human language, yet by\nartificial models. Moreover, an interactive approach to language modeling has\nthe potential to make language models substantially more versatile and to\nconsiderably impact downstream applications. Motivated by these considerations,\nwe pioneer the space of interactive language modeling. As a first contribution\nwe present a road map in which we detail the steps that need to be taken\ntowards interactive language modeling. We then lead by example and take the\nfirst steps on this road map, showing the initial feasibility of our approach.\nAs such, this work aims to be the start of a larger research agenda on\ninteractive language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Evgeny Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trees in transformers: a theoretical analysis of the Transformer's ability to represent trees. (arXiv:2112.11913v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11913","description":"<p>Transformer networks are the de facto standard architecture in natural\nlanguage processing. To date, there are no theoretical analyses of the\nTransformer's ability to capture tree structures. We focus on the ability of\nTransformer networks to learn tree structures that are important for tree\ntransduction problems. We first analyze the theoretical capability of the\nstandard Transformer architecture to learn tree structures given enumeration of\nall possible tree backbones, which we define as trees without labels. We then\nprove that two linear layers with ReLU activation function can recover any tree\nbackbone from any two nonzero, linearly independent starting backbones. This\nimplies that a Transformer can learn tree structures well in theory. We conduct\nexperiments with synthetic data and find that the standard Transformer achieves\nsimilar accuracy compared to a Transformer where tree position information is\nexplicitly encoded, albeit with slower convergence. This confirms empirically\nthat Transformers can learn tree structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodu_J/0/1/0/all/0/1\">Jordan Rodu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assisted Text Annotation Using Active Learning to Achieve High Quality with Little Effort. (arXiv:2112.11914v1 [cs.DL])","link":"http://arxiv.org/abs/2112.11914","description":"<p>Large amounts of annotated data have become more important than ever,\nespecially since the rise of deep learning techniques. However, manual\nannotations are costly. We propose a tool that enables researchers to create\nlarge, high-quality, annotated datasets with only a few manual annotations,\nthus strongly reducing annotation cost and effort. For this purpose, we combine\nan active learning (AL) approach with a pre-trained language model to\nsemi-automatically identify annotation categories in the given text documents.\nTo highlight our research direction's potential, we evaluate the approach on\nthe task of identifying frames in news articles. Our preliminary results show\nthat employing AL strongly reduces the number of annotations for correct\nclassification of even these complex and subtle frames. On the framing dataset,\nthe AL approach needs only 16.3\\% of the annotations to reach the same\nperformance as a model trained on the full dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weeber_F/0/1/0/all/0/1\">Franziska Weeber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Product Copywriting for E-Commerce. (arXiv:2112.11915v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11915","description":"<p>Product copywriting is a critical component of e-commerce recommendation\nplatforms. It aims to attract users' interest and improve user experience by\nhighlighting product characteristics with textual descriptions. In this paper,\nwe report our experience deploying the proposed Automatic Product Copywriting\nGeneration (APCG) system into the JD.com e-commerce product recommendation\nplatform. It consists of two main components: 1) natural language generation,\nwhich is built from a transformer-pointer network and a pre-trained\nsequence-to-sequence model based on millions of training data from our in-house\nplatform; and 2) copywriting quality control, which is based on both automatic\nevaluation and human screening. For selected domains, the models are trained\nand updated daily with the updated training data. In addition, the model is\nalso used as a real-time writing assistant tool on our live broadcast platform.\nThe APCG system has been deployed in JD.com since Feb 2021. By Sep 2021, it has\ngenerated 2.53 million product descriptions, and improved the overall averaged\nclick-through rate (CTR) and the Conversion Rate (CVR) by 4.22% and 3.61%,\ncompared to baselines, respectively on a year-on-year basis. The accumulated\nGross Merchandise Volume (GMV) made by our system is improved by 213.42%,\ncompared to the number in Feb 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shiliang Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xueqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALP: Data Augmentation using Lexicalized PCFGs for Few-Shot Text Classification. (arXiv:2112.11916v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11916","description":"<p>Data augmentation has been an important ingredient for boosting performances\nof learned models. Prior data augmentation methods for few-shot text\nclassification have led to great performance boosts. However, they have not\nbeen designed to capture the intricate compositional structure of natural\nlanguage. As a result, they fail to generate samples with plausible and diverse\nsentence structures. Motivated by this, we present the data Augmentation using\nLexicalized Probabilistic context-free grammars (ALP) that generates augmented\nsamples with diverse syntactic structures with plausible grammar. The\nlexicalized PCFG parse trees consider both the constituents and dependencies to\nproduce a syntactic frame that maximizes a variety of word choices in a\nsyntactically preservable manner without specific domain experts. Experiments\non few-shot text classification tasks demonstrate that ALP enhances many\nstate-of-the-art classification methods. As a second contribution, we delve\ninto the train-val splitting methodologies when a data augmentation method\ncomes into play. We argue empirically that the traditional splitting of\ntraining and validation sets is sub-optimal compared to our novel\naugmentation-based splitting strategies that further expand the training split\nwith the same number of labeled data. Taken together, our contributions on the\ndata augmentation strategies yield a strong training recipe for few-shot text\nclassification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hazel Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_D/0/1/0/all/0/1\">Daecheol Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1\">Jeong-Won Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yo-Sub Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models. (arXiv:2112.11941v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11941","description":"<p>We introduce the CRASS (counterfactual reasoning assessment) data set and\nbenchmark utilizing questionized counterfactual conditionals as a novel and\npowerful tool to evaluate large language models. We present the data set design\nand benchmark as well as the accompanying API that supports scoring against a\ncrowd-validated human baseline. We test six state-of-the-art models against our\nbenchmark. Our results show that it poses a valid challenge for these models\nand opens up considerable room for their improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frohberg_J/0/1/0/all/0/1\">J&#xf6;rg Frohberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1\">Frank Binder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text is no more Enough! A Benchmark for Profile-based Spoken Language Understanding. (arXiv:2112.11953v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11953","description":"<p>Current researches on spoken language understanding (SLU) heavily are limited\nto a simple setting: the plain text-based SLU that takes the user utterance as\ninput and generates its corresponding semantic frames (e.g., intent and slots).\nUnfortunately, such a simple setting may fail to work in complex real-world\nscenarios when an utterance is semantically ambiguous, which cannot be achieved\nby the text-based SLU models. In this paper, we first introduce a new and\nimportant task, Profile-based Spoken Language Understanding (ProSLU), which\nrequires the model that not only relies on the plain text but also the\nsupporting profile information to predict the correct intents and slots. To\nthis end, we further introduce a large-scale human-annotated Chinese dataset\nwith over 5K utterances and their corresponding supporting profile information\n(Knowledge Graph (KG), User Profile (UP), Context Awareness (CA)). In addition,\nwe evaluate several state-of-the-art baseline models and explore a multi-level\nknowledge adapter to effectively incorporate profile information. Experimental\nresults reveal that all existing text-based SLU models fail to work when the\nutterances are semantically ambiguous and our proposed framework can\neffectively fuse the supporting information for sentence-level intent detection\nand token-level slot filling. Finally, we summarize key challenges and provide\nnew points for future directions, which hopes to facilitate the research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiji Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Educator-focused Automated Scoring Systems for Reading and Writing. (arXiv:2112.11973v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11973","description":"<p>This paper presents methods for improving automated essay scoring with\ntechniques that address the computational trade-offs of self-attention and\ndocument length. To make Automated Essay Scoring (AES) more useful to\npractitioners, researchers must overcome the challenges of data and label\navailability, authentic and extended writing, domain scoring, prompt and source\nvariety, and transfer learning. This paper addresses these challenges using\nneural network models by employing techniques that preserve essay length as an\nimportant feature without increasing model training costs. It introduces\ntechniques for minimizing classification loss on ordinal labels using\nmulti-objective learning, capturing semantic information across the entire\nessay using sentence embeddings to use transformer architecture across\narbitrarily long documents, the use of such models for transfer learning,\nautomated hyperparameter generation based on prompt-corpus metadata, and, most\nimportantly, the use of semantic information to provide meaningful insights\ninto student reading through analysis of passage-dependent writing resulting in\nstate-of-the-art results for various essay tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardy_M/0/1/0/all/0/1\">Mike Hardy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Gender Biases Towards Politicians on Reddit. (arXiv:2112.12014v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12014","description":"<p>Despite attempts to increase gender parity in politics, global efforts have\nstruggled to ensure equal female representation. This is likely tied to\nimplicit gender biases against women in authority. In this work, we present a\ncomprehensive study of gender biases that appear in online political\ndiscussion. To this end, we collect 10 million comments on Reddit in\nconversations about male and female politicians, which enables an exhaustive\nstudy of automatic gender bias detection. We address not only misogynistic\nlanguage, but also benevolent sexism in the form of seemingly positive\nattitudes examining both sentiment and dominance attributed to female\npoliticians. Finally, we conduct a multi-faceted study of gender bias towards\npoliticians investigating both linguistic and extra-linguistic cues. We assess\n5 different types of gender bias, evaluating coverage, combinatorial, nominal,\nsentimental and lexical biases extant in social media language and discourse.\nOverall, we find that, contrary to previous research, coverage and sentiment\nbiases suggest equal public interest in female politicians. However, the\nresults of the nominal and lexical analyses suggest this interest is not as\nprofessional or respectful as that expressed about male politicians. Female\npoliticians are often named by their first names and are described in relation\nto their body, clothing, or family; this is a treatment that is not similarly\nextended to men. On the now banned far-right subreddits, this disparity is\ngreatest, though differences in gender biases still appear in the right and\nleft-leaning subreddits. We release the curated dataset to the public for\nfuture studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjanovic_S/0/1/0/all/0/1\">Sara Marjanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoiceMoji: A Novel On-Device Pipeline for Seamless Emoji Insertion in Dictation. (arXiv:2112.12028v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12028","description":"<p>Most of the speech recognition systems recover only words in the speech and\nfail to capture emotions. Users have to manually add emoji(s) in text for\nadding tone and making communication fun. Though there is much work done on\npunctuation addition on transcribed speech, the area of emotion addition is\nuntouched. In this paper, we propose a novel on-device pipeline to enrich the\nvoice input experience. It involves, given a blob of transcribed text,\nintelligently processing and identifying structure where emoji insertion makes\nsense. Moreover, it includes semantic text analysis to predict emoji for each\nof the sub-parts for which we propose a novel architecture Attention-based Char\nAware (ACA) LSTM which handles Out-Of-Vocabulary (OOV) words as well. All these\ntasks are executed completely on-device and hence can aid on-device dictation\nsystems. To the best of our knowledge, this is the first work that shows how to\nadd emoji(s) in the transcribed text. We demonstrate that our components\nachieve comparable results to previous neural approaches for punctuation\naddition and emoji prediction with 80% fewer parameters. Overall, our proposed\nmodel has a very small memory footprint of a mere 4MB to suit on-device\ndeployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1\">Harichandana B S S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization. (arXiv:2112.12072v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12072","description":"<p>Multimodal summarization with multimodal output (MSMO) generates a summary\nwith both textual and visual content. Multimodal news report contains\nheterogeneous contents, which makes MSMO nontrivial. Moreover, it is observed\nthat different modalities of data in the news report correlate hierarchically.\nTraditional MSMO methods indistinguishably handle different modalities of data\nby learning a representation for the whole data, which is not directly\nadaptable to the heterogeneous contents and hierarchical correlation. In this\npaper, we propose a hierarchical cross-modality semantic correlation learning\nmodel (HCSCL) to learn the intra- and inter-modal correlation existing in the\nmultimodal data. HCSCL adopts a graph network to encode the intra-modal\ncorrelation. Then, a hierarchical fusion framework is proposed to learn the\nhierarchical correlation between text and images. Furthermore, we construct a\nnew dataset with relevant image annotation and image object label information\nto provide the supervision information for the learning procedure. Extensive\nexperiments on the dataset show that HCSCL significantly outperforms the\nbaseline methods in automatic summarization metrics and fine-grained diversity\ntests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Litian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junshu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiran Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reevaluating Adversarial Examples in Natural Language. (arXiv:2004.14174v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14174","description":"<p>State-of-the-art attacks on NLP models lack a shared definition of a what\nconstitutes a successful attack. We distill ideas from past work into a unified\nframework: a successful natural language adversarial example is a perturbation\nthat fools the model and follows some linguistic constraints. We then analyze\nthe outputs of two state-of-the-art synonym substitution attacks. We find that\ntheir perturbations often do not preserve semantics, and 38% introduce\ngrammatical errors. Human surveys reveal that to successfully preserve\nsemantics, we need to significantly increase the minimum cosine similarities\nbetween the embeddings of swapped words and between the sentence encodings of\noriginal and perturbed sentences.With constraints adjusted to better preserve\nsemantics and grammaticality, the attack success rate drops by over 70\npercentage points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">John X. Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lifland_E/0/1/0/all/0/1\">Eli Lifland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanchantin_J/0/1/0/all/0/1\">Jack Lanchantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Text Classification: From Shallow to Deep Learning. (arXiv:2008.00364v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.00364","description":"<p>Text classification is the most fundamental and essential task in natural\nlanguage processing. The last decade has seen a surge of research in this area\ndue to the unprecedented success of deep learning. Numerous methods, datasets,\nand evaluation metrics have been proposed in the literature, raising the need\nfor a comprehensive and updated survey. This paper fills the gap by reviewing\nthe state-of-the-art approaches from 1961 to 2021, focusing on models from\ntraditional models to deep learning. We create a taxonomy for text\nclassification according to the text involved and the models used for feature\nextraction and classification. We then discuss each of these categories in\ndetail, dealing with both the technical developments and benchmark datasets\nthat support tests of predictions. A comprehensive comparison between different\ntechniques, as well as identifying the pros and cons of various evaluation\nmetrics are also provided in this survey. Finally, we conclude by summarizing\nkey implications, future research directions, and the challenges facing the\nresearch area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Renyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Understanding for Field and Service Robots in a Priori Unknown Environments. (arXiv:2105.10396v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.10396","description":"<p>Contemporary approaches to perception, planning, estimation, and control have\nallowed robots to operate robustly as our remote surrogates in uncertain,\nunstructured environments. This progress now creates an opportunity for robots\nto operate not only in isolation, but also with and alongside humans in our\ncomplex environments. Realizing this opportunity requires an efficient and\nflexible medium through which humans can communicate with collaborative robots.\nNatural language provides one such medium, and through significant progress in\nstatistical methods for natural-language understanding, robots are now able to\ninterpret a diverse array of free-form commands. However, most contemporary\napproaches require a detailed, prior spatial-semantic map of the robot's\nenvironment that models the space of possible referents of an utterance.\nConsequently, these methods fail when robots are deployed in new, previously\nunknown, or partially-observed environments, particularly when mental models of\nthe environment differ between the human operator and the robot. This paper\nprovides a comprehensive description of a novel learning framework that allows\nfield and service robots to interpret and correctly execute natural-language\ninstructions in a priori unknown, unstructured environments. Integral to our\napproach is its use of language as a \"sensor\" -- inferring spatial,\ntopological, and semantic information implicit in the utterance and then\nexploiting this information to learn a distribution over a latent environment\nmodel. We incorporate this distribution in a probabilistic, language grounding\nmodel and infer a distribution over a symbolic representation of the robot's\naction space. We use imitation learning to identify a belief-space policy that\nreasons over the environment and behavior distributions. We evaluate our\nframework through a variety navigation and mobile-manipulation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1\">Matthew R. Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patki_S/0/1/0/all/0/1\">Siddharth Patki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniele_A/0/1/0/all/0/1\">Andrea F. Daniele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahnestock_E/0/1/0/all/0/1\">Ethan Fahnestock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duvallet_F/0/1/0/all/0/1\">Felix Duvallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1\">Sachithra Hemachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stentz_A/0/1/0/all/0/1\">Anthony Stentz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1\">Nicholas Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_T/0/1/0/all/0/1\">Thomas M. Howard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Curricula to Implicitly Train Non-Expert Annotators. (arXiv:2106.02382v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.02382","description":"<p>Annotation studies often require annotators to familiarize themselves with\nthe task, its annotation scheme, and the data domain. This can be overwhelming\nin the beginning, mentally taxing, and induce errors into the resulting\nannotations; especially in citizen science or crowd sourcing scenarios where\ndomain expertise is not required and only annotation guidelines are provided.\nTo alleviate these issues, we propose annotation curricula, a novel approach to\nimplicitly train annotators. Our goal is to gradually introduce annotators into\nthe task by ordering instances that are annotated according to a learning\ncurriculum. To do so, we first formalize annotation curricula for sentence- and\nparagraph-level annotation tasks, define an ordering strategy, and identify\nwell-performing heuristics and interactively trained models on three existing\nEnglish datasets. We then conduct a user study with 40 voluntary participants\nwho are asked to identify the most fitting misconception for English tweets\nabout the Covid-19 pandemic. Our results show that using a simple heuristic to\norder instances can already significantly reduce the total annotation time\nwhile preserving a high annotation quality. Annotation curricula thus can\nprovide a novel way to improve data collection. To facilitate future research,\nwe further share our code and data consisting of 2,400 annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klie_J/0/1/0/all/0/1\">Jan-Christoph Klie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEOVER: Sentence-level Emotion Orientation Vector based Conversation Emotion Recognition Model. (arXiv:2106.08785v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08785","description":"<p>For the task of conversation emotion recognition, recent works focus on\nspeaker relationship modeling but ignore the role of utterance's emotional\ntendency.In this paper, we propose a new expression paradigm of sentence-level\nemotion orientation vector to model the potential correlation of emotions\nbetween sentence vectors. Based on it, we design an emotion recognition model,\nwhich extracts the sentence-level emotion orientation vectors from the language\nmodel and jointly learns from the dialogue sentiment analysis model and\nextracted sentence-level emotion orientation vectors to identify the speaker's\nemotional orientation during the conversation. We conduct experiments on two\nbenchmark datasets and compare them with the five baseline models.The\nexperimental results show that our model has better performance on all data\nsets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaijing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fengxiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tieyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yusen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning-based Dialogue Guided Event Extraction to Exploit Argument Relations. (arXiv:2106.12384v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.12384","description":"<p>Event extraction is a fundamental task for natural language processing.\nFinding the roles of event arguments like event participants is essential for\nevent extraction. However, doing so for real-life event descriptions is\nchallenging because an argument's role often varies in different contexts.\nWhile the relationship and interactions between multiple arguments are useful\nfor settling the argument roles, such information is largely ignored by\nexisting approaches. This paper presents a better approach for event extraction\nby explicitly utilizing the relationships of event arguments. We achieve this\nthrough a carefully designed task-oriented dialogue system. To model the\nargument relation, we employ reinforcement learning and incremental learning to\nextract multiple arguments via a multi-turned, iterative process. Our approach\nleverages knowledge of the already extracted arguments of the same sentence to\ndetermine the role of arguments that would be difficult to decide individually.\nIt then uses the newly obtained information to improve the decisions of\npreviously extracted arguments. This two-way feedback process allows us to\nexploit the argument relations to effectively settle argument roles, leading to\nbetter sentence understanding and event extraction. Experimental results show\nthat our approach consistently outperforms seven state-of-the-art event\nextraction methods for the classification of events and argument role and\nargument identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yuanxing Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Compact Survey on Event Extraction: Approaches and Applications. (arXiv:2107.02126v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02126","description":"<p>Event extraction is a critical technique to apprehend the essential content\nof events promptly. With the rapid development of deep learning technology,\nevent extraction technology based on deep learning has become a research\nhotspot. Numerous methods, datasets, and evaluation metrics have been proposed\nin the literature, raising the need for a comprehensive and updated survey.\nThis paper fills the gap by reviewing the state-of-the-art approaches, focusing\non deep learning-based models. We summarize the task definition, paradigm, and\nmodels of event extraction and then discuss each of these in detail. We\nintroduce benchmark datasets that support tests of predictions and evaluation\nmetrics. A comprehensive comparison between different techniques is also\nprovided in this survey. Finally, we conclude by summarizing future research\ndirections facing the research area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shiyao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1\">Amin Beheshti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brazilian Portuguese Speech Recognition Using Wav2vec 2.0. (arXiv:2107.11414v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.11414","description":"<p>Deep learning techniques have been shown to be efficient in various tasks,\nespecially in the development of speech recognition systems, that is, systems\nthat aim to transcribe an audio sentence in a sequence of written words.\nDespite the progress in the area, speech recognition can still be considered\ndifficult, especially for languages lacking available data, such as Brazilian\nPortuguese (BP). In this sense, this work presents the development of an public\nAutomatic Speech Recognition (ASR) system using only open available audio data,\nfrom the fine-tuning of the Wav2vec 2.0 XLSR-53 model pre-trained in many\nlanguages, over BP data. The final model presents an average word error rate of\n12.4% over 7 different datasets (10.5% when applying a language model).\nAccording to our knowledge, the obtained error is the lowest among open\nend-to-end (E2E) ASR models for BP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Anderson da Silva Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v6 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08597","description":"<p>Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique or doing this is to apply named entity\ndisambiguation (NED) systems to the question, and retrieve KB facts for the\ndisambiguated entities. This work presents CLOCQ, an efficient method that\nprunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses\na top-k query processor over score-ordered lists of KB items that combine\nsignals about lexical matching, relevance to the question, coherence among\ncandidate items, and connectivity in the KB graph. Experiments with two recent\nQA benchmarks for complex questions demonstrate the superiority of CLOCQ over\nstate-of-the-art baselines with respect to answer presence, size of the search\nspace, and runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Unified View of Parameter-Efficient Transfer Learning. (arXiv:2110.04366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04366","description":"<p>Fine-tuning large pre-trained language models on downstream tasks has become\nthe de-facto learning paradigm in NLP. However, conventional approaches\nfine-tune all the parameters of the pre-trained model, which becomes\nprohibitive as the model size and the number of tasks grow. Recent work has\nproposed a variety of parameter-efficient transfer learning methods that only\nfine-tune a small number of (extra) parameters to attain strong performance.\nWhile effective, the critical ingredients for success and the connections among\nthe various methods are poorly understood. In this paper, we break down the\ndesign of state-of-the-art parameter-efficient transfer learning methods and\npresent a unified framework that establishes connections between them.\nSpecifically, we re-frame them as modifications to specific hidden states in\npre-trained models, and define a set of design dimensions along which different\nmethods vary, such as the function to compute the modification and the position\nto apply the modification. Through comprehensive empirical studies across\nmachine translation, text summarization, language understanding, and text\nclassification benchmarks, we utilize the unified view to identify important\ndesign choices in previous methods. Furthermore, our unified framework enables\nthe transfer of design elements across different approaches, and as a result we\nare able to instantiate new parameter-efficient fine-tuning methods that tune\nless parameters than previous methods while being more effective, achieving\ncomparable results to fine-tuning all parameters on all four tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs. (arXiv:2110.13715v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2110.13715","description":"<p>Query embedding (QE) -- which aims to embed entities and first-order logical\n(FOL) queries in low-dimensional spaces -- has shown great power in multi-hop\nreasoning over knowledge graphs. Recently, embedding entities and queries with\ngeometric shapes becomes a promising direction, as geometric shapes can\nnaturally represent answer sets of queries and logical relationships among\nthem. However, existing geometry-based models have difficulty in modeling\nqueries with negation, which significantly limits their applicability. To\naddress this challenge, we propose a novel query embedding model, namely Cone\nEmbeddings (ConE), which is the first geometry-based QE model that can handle\nall the FOL operations, including conjunction, disjunction, and negation.\nSpecifically, ConE represents entities and queries as Cartesian products of\ntwo-dimensional cones, where the intersection and union of cones naturally\nmodel the conjunction and disjunction operations. By further noticing that the\nclosure of complement of cones remains cones, we design geometric complement\noperators in the embedding space for the negation operations. Experiments\ndemonstrate that ConE significantly outperforms existing state-of-the-art\nmethods on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhanqiu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. (arXiv:2110.13900v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.13900","description":"<p>Self-supervised learning (SSL) achieves great success in speech recognition,\nwhile limited exploration has been attempted for other speech processing tasks.\nAs speech signal contains multi-faceted information including speaker identity,\nparalinguistics, spoken content, etc., learning universal representations for\nall speech tasks is challenging. In this paper, we propose a new pre-trained\nmodel, WavLM, to solve full-stack downstream speech tasks. WavLM extends HuBERT\nframework to denoising masked speech modeling, where the target is to predict\npseudo-labels of simulated noisy speech on masked regions. The simulated speech\nis created by adding additional noise or speech from other utterances on the\noriginal speech. The denoising masked speech modeling tasks aim to improve the\nmodel robustness to complex acoustic environments and the preservation of\nspeaker identity. We scale up the training dataset from 60k hours to 94k hours.\nWavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and\nbrings significant improvements for various speech processing tasks on their\nrepresentative benchmarks. The code and pretrained models are available at\nhttps://aka.ms/wavlm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building ASR Systems for the Next Billion Users. (arXiv:2111.03945v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03945","description":"<p>Recent methods in speech and language technology pretrain very LARGE models\nwhich are fine-tuned for specific tasks. However, the benefits of such LARGE\nmodels are often limited to a few resource rich languages of the world. In this\nwork, we make multiple contributions towards building ASR systems for low\nresource languages from the Indian subcontinent. First, we curate 17,000 hours\nof raw speech data for 40 Indian languages from a wide variety of domains\nincluding education, news, technology, and finance. Second, using this raw\nspeech data we pretrain several variants of wav2vec style models for 40 Indian\nlanguages. Third, we analyze the pretrained models to find key features:\ncodebook vectors of similar sounding phonemes are shared across languages,\nrepresentations across layers are discriminative of the language family, and\nattention heads often pay attention within small local windows. Fourth, we\nfine-tune this model for downstream ASR for 9 languages and obtain\nstate-of-the-art results on 3 public datasets, including on very low-resource\nlanguages such as Sinhala and Nepali. Our work establishes that multilingual\npretraining is an effective strategy for building ASR systems for the\nlinguistically diverse speakers of the Indian subcontinent. Our code, data and\nmodels are available publicly at https://indicnlp.ai4bharat.org/indicwav2vec/\nand we hope they will help advance research in ASR for Indic languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javed_T/0/1/0/all/0/1\">Tahir Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_A/0/1/0/all/0/1\">Abhigyan Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhogale_K/0/1/0/all/0/1\">Kaushal Santosh Bhogale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1\">Gowtham Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling. (arXiv:2112.08914v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.08914","description":"<p>Neural autoregressive sequence models smear the probability among many\npossible sequences including degenerate ones, such as empty or repetitive\nsequences. In this work, we tackle one specific case where the model assigns a\nhigh probability to unreasonably short sequences. We define the oversmoothing\nrate to quantify this issue. After confirming the high degree of oversmoothing\nin neural machine translation, we propose to explicitly minimize the\noversmoothing rate during training. We conduct a set of experiments to study\nthe effect of the proposed regularization on both model distribution and\ndecoding performance. We use a neural machine translation task as the testbed\nand consider three different datasets of varying size. Our experiments reveal\nthree major findings. First, we can control the oversmoothing rate of the model\nby tuning the strength of the regularization. Second, by enhancing the\noversmoothing loss contribution, the probability and the rank of &lt;eos&gt; token\ndecrease heavily at positions where it is not supposed to be. Third, the\nproposed regularization impacts the outcome of beam search especially when a\nlarge beam is used. The degradation of translation quality (measured in BLEU)\nwith a large beam significantly lessens with lower oversmoothing rate, but the\ndegradation compared to smaller beam sizes remains to exist. From these\nobservations, we conclude that the high degree of oversmoothing is the main\nreason behind the degenerate case of overly probable short sequences in a\nneural autoregressive model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eremeev_M/0/1/0/all/0/1\">Maksim Eremeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Teacher-Student Architecture for Mixed Supervised Lung Tumor Segmentation. (arXiv:2112.11541v1 [eess.IV])","link":"http://arxiv.org/abs/2112.11541","description":"<p>Purpose: Automating tasks such as lung tumor localization and segmentation in\nradiological images can free valuable time for radiologists and other clinical\npersonnel. Convolutional neural networks may be suited for such tasks, but\nrequire substantial amounts of labeled data to train. Obtaining labeled data is\na challenge, especially in the medical domain. Methods: This paper investigates\nthe use of a teacher-student design to utilize datasets with different types of\nsupervision to train an automatic model performing pulmonary tumor segmentation\non computed tomography images. The framework consists of two models: the\nstudent that performs end-to-end automatic tumor segmentation and the teacher\nthat supplies the student additional pseudo-annotated data during training.\nResults: Using only a small proportion of semantically labeled data and a large\nnumber of bounding box annotated data, we achieved competitive performance\nusing a teacher-student design. Models trained on larger amounts of semantic\nannotations did not perform better than those trained on teacher-annotated\ndata. Conclusions: Our results demonstrate the potential of utilizing\nteacher-student designs to reduce the annotation load, as less supervised\nannotation schemes may be performed, without any real degradation in\nsegmentation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fredriksen_V/0/1/0/all/0/1\">Vemund Fredriksen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Svele_S/0/1/0/all/0/1\">Svein Ole M. Svele</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersen_A/0/1/0/all/0/1\">Andr&#xe9; Pedersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lango_T/0/1/0/all/0/1\">Thomas Lang&#xf8;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiss_G/0/1/0/all/0/1\">Gabriel Kiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindseth_F/0/1/0/all/0/1\">Frank Lindseth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation. (arXiv:2112.11542v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11542","description":"<p>ViTs are often too computationally expensive to be fitted onto real-world\nresource-constrained devices, due to (1) their quadratically increased\ncomplexity with the number of input tokens and (2) their overparameterized\nself-attention heads and model depth. In parallel, different images are of\nvaried complexity and their different regions can contain various levels of\nvisual information, indicating that treating all regions/tokens equally in\nterms of model complexity is unnecessary while such opportunities for trimming\ndown ViTs' complexity have not been fully explored. To this end, we propose a\nMulti-grained Input-adaptive Vision Transformer framework dubbed MIA-Former\nthat can input-adaptively adjust the structure of ViTs at three\ncoarse-to-fine-grained granularities (i.e., model depth and the number of model\nheads/tokens). In particular, our MIA-Former adopts a low-cost network trained\nwith a hybrid supervised and reinforcement training method to skip unnecessary\nlayers, heads, and tokens in an input adaptive manner, reducing the overall\ncomputational cost. Furthermore, an interesting side effect of our MIA-Former\nis that its resulting ViTs are naturally equipped with improved robustness\nagainst adversarial attacks over their static counterparts, because\nMIA-Former's multi-grained dynamic control improves the model diversity similar\nto the effect of ensemble and thus increases the difficulty of adversarial\nattacks against all its sub-models. Extensive experiments and ablation studies\nvalidate that the proposed MIA-Former framework can effectively allocate\ncomputation budgets adaptive to the difficulty of input images meanwhile\nincrease robustness, achieving state-of-the-art (SOTA) accuracy-efficiency\ntrade-offs, e.g., 20% computation savings with the same or even a higher\naccuracy compared with SOTA dynamic transformer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaojian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Street Human Motion Capture. (arXiv:2112.11543v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11543","description":"<p>In recent years, motion capture technology using computers has developed\nrapidly. Because of its high efficiency and excellent performance, it replaces\nmany traditional methods and is being widely used in many fields. Our project\nis about street scene video human motion capturing and analysis. The primary\ngoal of the project is to capture the human motion in a video and use the\nmotion information for 3D animation (human) in real-time. We applied a neural\nnetwork for motion capture and implement it in the unity under a street view\nscene. By analyzing the motion data, we will have a better estimation of the\nstreet condition, which is useful for other high-tech applications such as\nself-driving cars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_T/0/1/0/all/0/1\">Tianyu Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanfang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Anup Basu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decompose the Sounds and Pixels, Recompose the Events. (arXiv:2112.11547v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11547","description":"<p>In this paper, we propose a framework centering around a novel architecture\ncalled the Event Decomposition Recomposition Network (EDRNet) to tackle the\nAudio-Visual Event (AVE) localization problem in the supervised and weakly\nsupervised settings. AVEs in the real world exhibit common unravelling patterns\n(termed as Event Progress Checkpoints (EPC)), which humans can perceive through\nthe cooperation of their auditory and visual senses. Unlike earlier methods\nwhich attempt to recognize entire event sequences, the EDRNet models EPCs and\ninter-EPC relationships using stacked temporal convolutions. Based on the\npostulation that EPC representations are theoretically consistent for an event\ncategory, we introduce the State Machine Based Video Fusion, a novel\naugmentation technique that blends source videos using different EPC template\nsequences. Additionally, we design a new loss function called the\nLand-Shore-Sea loss to compactify continuous foreground and background\nrepresentations. Lastly, to alleviate the issue of confusing events during weak\nsupervision, we propose a prediction stabilization method called Bag to\nInstance Label Correction. Experiments on the AVE dataset show that our\ncollective framework outperforms the state-of-the-art by a sizable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1\">Varshanth R. Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_M/0/1/0/all/0/1\">Md Ibrahim Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Juwei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution-aware Margin Calibration for Semantic Segmentation in Images. (arXiv:2112.11554v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11554","description":"<p>The Jaccard index, also known as Intersection-over-Union (IoU), is one of the\nmost critical evaluation metrics in image semantic segmentation. However,\ndirect optimization of IoU score is very difficult because the learning\nobjective is neither differentiable nor decomposable. Although some algorithms\nhave been proposed to optimize its surrogates, there is no guarantee provided\nfor the generalization ability. In this paper, we propose a margin calibration\nmethod, which can be directly used as a learning objective, for an improved\ngeneralization of IoU over the data-distribution, underpinned by a rigid lower\nbound. This scheme theoretically ensures a better segmentation performance in\nterms of IoU score. We evaluated the effectiveness of the proposed margin\ncalibration method on seven image datasets, showing substantial improvements in\nIoU score over other learning objectives using deep segmentation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Litao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhibin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Clustering: Grouping Images into Coherent Clusters of Anomaly Types. (arXiv:2112.11573v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11573","description":"<p>We introduce anomaly clustering, whose goal is to group data into\nsemantically coherent clusters of anomaly types. This is different from anomaly\ndetection, whose goal is to divide anomalies from normal data. Unlike\nobject-centered image clustering applications, anomaly clustering is\nparticularly challenging as anomalous patterns are subtle and local. We present\na simple yet effective clustering framework using a patch-based pretrained deep\nembeddings and off-the-shelf clustering methods. We define a distance function\nbetween images, each of which is represented as a bag of embeddings, by the\nEuclidean distance between weighted averaged embeddings. The weight defines the\nimportance of instances (i.e., patch embeddings) in the bag, which may\nhighlight defective regions. We compute weights in an unsupervised way or in a\nsemi-supervised way if labeled normal data is available. Extensive experimental\nstudies show the effectiveness of the proposed clustering framework along with\na novel distance function upon existing multiple instance or deep clustering\nframeworks. Overall, our framework achieves 0.451 and 0.674 normalized mutual\ninformation scores on MVTec object and texture categories and further improve\nwith a few labeled normal data (0.577, 0.669), far exceeding the baselines\n(0.244, 0.273) or state-of-the-art deep clustering methods (0.176, 0.277).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by Learnable Motion Generation. (arXiv:2112.11593v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11593","description":"<p>This paper addresses the problem of cross-dataset generalization of 3D human\npose estimation models. Testing a pre-trained 3D pose estimator on a new\ndataset results in a major performance drop. Previous methods have mainly\naddressed this problem by improving the diversity of the training data. We\nargue that diversity alone is not sufficient and that the characteristics of\nthe training data need to be adapted to those of the new dataset such as camera\nviewpoint, position, human actions, and body size. To this end, we propose\nAdaptPose, an end-to-end framework that generates synthetic 3D human motions\nfrom a source dataset and uses them to fine-tune a 3D pose estimator. AdaptPose\nfollows an adversarial training scheme. From a source 3D pose the generator\ngenerates a sequence of 3D poses and a camera orientation that is used to\nproject the generated poses to a novel view. Without any 3D labels or camera\ninformation AdaptPose successfully learns to create synthetic 3D poses from the\ntarget dataset while only being trained on 2D poses. In experiments on the\nHuman3.6M, MPI-INF-3DHP, 3DPW, and Ski-Pose datasets our method outperforms\nprevious work in cross-dataset evaluations by 14% and previous semi-supervised\nlearning methods that use partial 3D annotations by 16%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gholami_M/0/1/0/all/0/1\">Mohsen Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1\">Rabab Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EyePAD++: A Distillation-based approach for joint Eye Authentication and Presentation Attack Detection using Periocular Images. (arXiv:2112.11610v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11610","description":"<p>A practical eye authentication (EA) system targeted for edge devices needs to\nperform authentication and be robust to presentation attacks, all while\nremaining compute and latency efficient. However, existing eye-based frameworks\na) perform authentication and Presentation Attack Detection (PAD) independently\nand b) involve significant pre-processing steps to extract the iris region.\nHere, we introduce a joint framework for EA and PAD using periocular images.\nWhile a deep Multitask Learning (MTL) network can perform both the tasks, MTL\nsuffers from the forgetting effect since the training datasets for EA and PAD\nare disjoint. To overcome this, we propose Eye Authentication with PAD\n(EyePAD), a distillation-based method that trains a single network for EA and\nPAD while reducing the effect of forgetting. To further improve the EA\nperformance, we introduce a novel approach called EyePAD++ that includes\ntraining an MTL network on both EA and PAD data, while distilling the\n`versatility' of the EyePAD network through an additional distillation step.\nOur proposed methods outperform the SOTA in PAD and obtain near-SOTA\nperformance in eye-to-eye verification, without any pre-processing. We also\ndemonstrate the efficacy of EyePAD and EyePAD++ in user-to-user verification\nwith PAD across network backbones and image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_P/0/1/0/all/0/1\">Prithviraj Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_K/0/1/0/all/0/1\">Kirsten Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Khushi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1\">Rakesh Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOSAIC: Mobile Segmentation via decoding Aggregated Information and encoded Context. (arXiv:2112.11623v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11623","description":"<p>We present a next-generation neural network architecture, MOSAIC, for\nefficient and accurate semantic image segmentation on mobile devices. MOSAIC is\ndesigned using commonly supported neural operations by diverse mobile hardware\nplatforms for flexible deployment across various mobile platforms. With a\nsimple asymmetric encoder-decoder structure which consists of an efficient\nmulti-scale context encoder and a light-weight hybrid decoder to recover\nspatial details from aggregated information, MOSAIC achieves new\nstate-of-the-art performance while balancing accuracy and computational cost.\nDeployed on top of a tailored feature extraction backbone based on a searched\nclassification network, MOSAIC achieves a 5% absolute accuracy gain surpassing\nthe current industry standard MLPerf models and state-of-the-art architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1\">Andrew Howard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional neural network based on transfer learning for breast cancer screening. (arXiv:2112.11629v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11629","description":"<p>Breast cancer is the most common cancer in the world and the most prevalent\ncause of death among women worldwide. Nevertheless, it is also one of the most\ntreatable malignancies if detected early. In this paper, a deep convolutional\nneural network-based algorithm is proposed to aid in accurately identifying\nbreast cancer from ultrasonic images. In this algorithm, several neural\nnetworks are fused in a parallel architecture to perform the classification\nprocess and the voting criteria are applied in the final classification\ndecision between the candidate object classes where the output of each neural\nnetwork is representing a single vote. Several experiments were conducted on\nthe breast ultrasound dataset consisting of 537 Benign, 360 malignant, and 133\nnormal images. These experiments show an optimistic result and a capability of\nthe proposed model to outperform many state-of-the-art algorithms on several\nmeasures. Using k-fold cross-validation and a bagging classifier ensemble, we\nachieved an accuracy of 99.5% and a sensitivity of 99.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ragb_H/0/1/0/all/0/1\">Hussin Ragb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1\">Redha Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jera_E/0/1/0/all/0/1\">Elforjani Jera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buaossa_N/0/1/0/all/0/1\">Nagi Buaossa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JoJoGAN: One Shot Face Stylization. (arXiv:2112.11641v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11641","description":"<p>While there have been recent advances in few-shot image stylization, these\nmethods fail to capture stylistic details that are obvious to humans. Details\nsuch as the shape of the eyes, the boldness of the lines, are especially\ndifficult for a model to learn, especially so under a limited data setting. In\nthis work, we aim to perform one-shot image stylization that gets the details\nright. Given a reference style image, we approximate paired real data using GAN\ninversion and finetune a pretrained StyleGAN using that approximate paired\ndata. We then encourage the StyleGAN to generalize so that the learned style\ncan be applied to all other images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Credibility Scoring Metrics of Perception Systems for Autonomous Driving. (arXiv:2112.11643v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11643","description":"<p>Autonomous and semi-autonomous vehicles' perception algorithms can encounter\nsituations with erroneous object detection, such as misclassification of\nobjects on the road, which can lead to safety violations and potentially fatal\nconsequences. While there has been substantial work in the robustness of object\ndetection algorithms and online metric learning, there is little research on\nbenchmarking scoring metrics to determine any possible indicators of potential\nmisclassification. An emphasis is put on exploring the potential of taking\nthese scoring metrics online in order to allow the AV to make perception-based\ndecisions given real-time constraints. In this work, we explore which, if any,\nmetrics act as online indicators of when perception algorithms and object\ndetectors are failing. Our work provides insight on better design principles\nand characteristics of online metrics to accurately evaluate the credibility of\nobject detectors. Our approach employs non-adversarial and realistic\nperturbations to images, on which we evaluate various quantitative metrics. We\nfound that offline metrics can be designed to account for real-world\ncorruptions such as poor weather conditions and that the analysis of such\nmetrics can provide a segue into designing online metrics. This is a clear next\nstep as it can allow for error-free autonomous vehicle perception and safer\ntime-critical and safety-critical decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khandal_V/0/1/0/all/0/1\">Viren Khandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidyarthi_A/0/1/0/all/0/1\">Arth Vidyarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN Based Boundary Aware Classifier for Detecting Out-of-distribution Samples. (arXiv:2112.11648v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11648","description":"<p>This paper focuses on the problem of detecting out-of-distribution (ood)\nsamples with neural nets. In image recognition tasks, the trained classifier\noften gives high confidence score for input images which are remote from the\nin-distribution (id) data, and this has greatly limited its application in real\nworld. For alleviating this problem, we propose a GAN based boundary aware\nclassifier (GBAC) for generating a closed hyperspace which only contains most\nid data. Our method is based on the fact that the traditional neural net\nseperates the feature space as several unclosed regions which are not suitable\nfor ood detection. With GBAC as an auxiliary module, the ood data distributed\noutside the closed hyperspace will be assigned with much lower score, allowing\nmore effective ood detection while maintaining the classification performance.\nMoreover, we present a fast sampling method for generating hard ood\nrepresentations which lie on the boundary of pre-mentioned closed hyperspace.\nExperiments taken on several datasets and neural net architectures promise the\neffectiveness of GBAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard YiDa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ghost-dil-NetVLAD: A Lightweight Neural Network for Visual Place Recognition. (arXiv:2112.11679v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11679","description":"<p>Visual place recognition (VPR) is a challenging task with the unbalance\nbetween enormous computational cost and high recognition performance. Thanks to\nthe practical feature extraction ability of the lightweight convolution neural\nnetworks (CNNs) and the train-ability of the vector of locally aggregated\ndescriptors (VLAD) layer, we propose a lightweight weakly supervised end-to-end\nneural network consisting of a front-ended perception model called GhostCNN and\na learnable VLAD layer as a back-end. GhostCNN is based on Ghost modules that\nare lightweight CNN-based architectures. They can generate redundant feature\nmaps using linear operations instead of the traditional convolution process,\nmaking a good trade-off between computation resources and recognition accuracy.\nTo enhance our proposed lightweight model further, we add dilated convolutions\nto the Ghost module to get features containing more spatial semantic\ninformation, improving accuracy. Finally, rich experiments conducted on a\ncommonly used public benchmark and our private dataset validate that the\nproposed neural network reduces the FLOPs and parameters of VGG16-NetVLAD by\n99.04% and 80.16%, respectively. Besides, both models achieve similar accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Q/0/1/0/all/0/1\">Qingyuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Renhe Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost Aggregation Is All You Need for Few-Shot Segmentation. (arXiv:2112.11685v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11685","description":"<p>We introduce a novel cost aggregation network, dubbed Volumetric Aggregation\nwith Transformers (VAT), to tackle the few-shot segmentation task by using both\nconvolutions and transformers to efficiently handle high dimensional\ncorrelation maps between query and support. In specific, we propose our encoder\nconsisting of volume embedding module to not only transform the correlation\nmaps into more tractable size but also inject some convolutional inductive bias\nand volumetric transformer module for the cost aggregation. Our encoder has a\npyramidal structure to let the coarser level aggregation to guide the finer\nlevel and enforce to learn complementary matching scores. We then feed the\noutput into our affinity-aware decoder along with the projected feature maps\nfor guiding the segmentation process. Combining these components, we conduct\nexperiments to demonstrate the effectiveness of the proposed method, and our\nmethod sets a new state-of-the-art for all the standard benchmarks in few-shot\nsegmentation task. Furthermore, we find that the proposed method attains\nstate-of-the-art performance even for the standard benchmarks in semantic\ncorrespondence task although not specifically designed for this task. We also\nprovide an extensive ablation study to validate our architectural choices. The\ntrained weights and codes are available at: https://seokju-cho.github.io/VAT/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seokju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Jisu Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Centroid Representation Network for Domain Adaptive Person Re-ID. (arXiv:2112.11689v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11689","description":"<p>Recently, many approaches tackle the Unsupervised Domain Adaptive person\nre-identification (UDA re-ID) problem through pseudo-label-based contrastive\nlearning. During training, a uni-centroid representation is obtained by simply\naveraging all the instance features from a cluster with the same pseudo label.\nHowever, a cluster may contain images with different identities (label noises)\ndue to the imperfect clustering results, which makes the uni-centroid\nrepresentation inappropriate. In this paper, we present a novel Multi-Centroid\nMemory (MCM) to adaptively capture different identity information within the\ncluster. MCM can effectively alleviate the issue of label noises by selecting\nproper positive/negative centroids for the query image. Moreover, we further\npropose two strategies to improve the contrastive learning process. First, we\npresent a Domain-Specific Contrastive Learning (DSCL) mechanism to fully\nexplore intradomain information by comparing samples only from the same domain.\nSecond, we propose Second-Order Nearest Interpolation (SONI) to obtain abundant\nand informative negative samples. We integrate MCM, DSCL, and SONI into a\nunified framework named Multi-Centroid Representation Network (MCRN). Extensive\nexperiments demonstrate the superiority of MCRN over state-of-the-art\napproaches on multiple UDA re-ID tasks and fully unsupervised re-ID tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tengteng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Haotian Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuanjie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chuchu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEVR3D: Compositional Language and Elementary Visual Reasoning for Question Answering in 3D Real-World Scenes. (arXiv:2112.11691v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11691","description":"<p>3D scene understanding is a relatively emerging research field. In this\npaper, we introduce the Visual Question Answering task in 3D real-world scenes\n(VQA-3D), which aims to answer all possible questions given a 3D scene. To\ntackle this problem, the first VQA-3D dataset, namely CLEVR3D, is proposed,\nwhich contains 60K questions in 1,129 real-world scenes. Specifically, we\ndevelop a question engine leveraging 3D scene graph structures to generate\ndiverse reasoning questions, covering the questions of objects' attributes\n(i.e., size, color, and material) and their spatial relationships. Built upon\nthis dataset, we further design the first VQA-3D baseline model, TransVQA3D.\nThe TransVQA3D model adopts well-designed Transformer architectures to achieve\nsuperior VQA-3D performance, compared with the pure language baseline and\nprevious 3D reasoning methods directly applied to 3D scenarios. Experimental\nresults verify that taking VQA-3D as an auxiliary task can boost the\nperformance of 3D scene understanding, including scene graph analysis for the\nnode-wise classification and whole-graph recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Object Detection: A Survey. (arXiv:2112.11699v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11699","description":"<p>Humans are able to learn to recognize new objects even from a few examples.\nIn contrast, training deep-learning-based object detectors requires huge\namounts of annotated data. To avoid the need to acquire and annotate these huge\namounts of data, few-shot object detection aims to learn from few object\ninstances of new categories in the target domain. In this survey, we provide an\noverview of the state of the art in few-shot object detection. We categorize\napproaches according to their training scheme and architectural layout. For\neach type of approaches, we describe the general realization as well as\nconcepts to improve the performance on novel categories. Whenever appropriate,\nwe give short takeaways regarding these concepts in order to highlight the best\nideas. Eventually, we introduce commonly used datasets and their evaluation\nprotocols and analyze reported benchmark results. As a result, we emphasize\ncommon challenges in evaluation and identify the most promising current trends\nin this emerging field of few-shot object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kohler_M/0/1/0/all/0/1\">Mona K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenbach_M/0/1/0/all/0/1\">Markus Eisenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gross_H/0/1/0/all/0/1\">Horst-Michael Gross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Contrast for Image Regression in Computer-Aided Disease Assessment. (arXiv:2112.11700v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11700","description":"<p>Image regression tasks for medical applications, such as bone mineral density\n(BMD) estimation and left-ventricular ejection fraction (LVEF) prediction, play\nan important role in computer-aided disease assessment. Most deep regression\nmethods train the neural network with a single regression loss function like\nMSE or L1 loss. In this paper, we propose the first contrastive learning\nframework for deep image regression, namely AdaCon, which consists of a feature\nlearning branch via a novel adaptive-margin contrastive loss and a regression\nprediction branch. Our method incorporates label distance relationships as part\nof the learned feature representations, which allows for better performance in\ndownstream regression tasks. Moreover, it can be used as a plug-and-play module\nto improve performance of existing regression methods. We demonstrate the\neffectiveness of AdaCon on two medical image regression tasks, ie, bone mineral\ndensity estimation from X-ray images and left-ventricular ejection fraction\nprediction from echocardiogram videos. AdaCon leads to relative improvements of\n3.3% and 5.9% in MAE over state-of-the-art BMD estimation and LVEF prediction\nmethods, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Weihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wan Hang Keith Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_M/0/1/0/all/0/1\">Michael D. Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy Regularized Iterative Weighted Shrinkage-Thresholding Algorithm (ERIWSTA): An Application to CT Image Restoration. (arXiv:2112.11706v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11706","description":"<p>The iterative weighted shrinkage-thresholding algorithm (IWSTA) has shown\nsuperiority to the classic unweighted iterative shrinkage-thresholding\nalgorithm (ISTA) for solving linear inverse problems, which address the\nattributes differently. This paper proposes a new entropy regularized IWSTA\n(ERIWSTA) that adds an entropy regularizer to the cost function to measure the\nuncertainty of the weights to stimulate attributes to participate in problem\nsolving. Then, the weights are solved with a Lagrange multiplier method to\nobtain a simple iterative update. The weights can be explained as the\nprobability of the contribution of an attribute to the problem solution.\nExperimental results on CT image restoration show that the proposed method has\nbetter performance in terms of convergence speed and restoration accuracy than\nthe existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingxue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yueyang Teng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusion of medical imaging and electronic health records with attention and multi-head machanisms. (arXiv:2112.11710v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11710","description":"<p>Doctors often make diagonostic decisions based on patient's image scans, such\nas magnetic resonance imaging (MRI), and patient's electronic health records\n(EHR) such as age, gender, blood pressure and so on. Despite a lot of automatic\nmethods have been proposed for either image or text analysis in computer vision\nor natural language research areas, much fewer studies have been developed for\nthe fusion of medical image and EHR data for medical problems. Among existing\nearly or intermediate fusion methods, concatenation of features from both\nmodalities is still a mainstream. For a better exploiting of image and EHR\ndata, we propose a multi-modal attention module which use EHR data to help the\nselection of important regions during image feature extraction process\nconducted by traditional CNN. Moreover, we propose to incorporate multi-head\nmachnism to gated multimodal unit (GMU) to make it able to parallelly fuse\nimage and EHR features in different subspaces. With the help of the two\nmodules, existing CNN architecture can be enhanced using both modalities.\nExperiments on predicting Glasgow outcome scale (GOS) of intracerebral\nhemorrhage patients and classifying Alzheimer's Disease showed the proposed\nmethod can automatically focus on task-related areas and achieve better results\nby making better use of image and EHR features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianbo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Ming Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jianhua Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Accuracy RGB-D Face Recognition via Segmentation-Aware Face Depth Estimation and Mask-Guided Attention Network. (arXiv:2112.11713v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11713","description":"<p>Deep learning approaches have achieved highly accurate face recognition by\ntraining the models with very large face image datasets. Unlike the\navailability of large 2D face image datasets, there is a lack of large 3D face\ndatasets available to the public. Existing public 3D face datasets were usually\ncollected with few subjects, leading to the over-fitting problem. This paper\nproposes two CNN models to improve the RGB-D face recognition task. The first\nis a segmentation-aware depth estimation network, called DepthNet, which\nestimates depth maps from RGB face images by including semantic segmentation\ninformation for more accurate face region localization. The other is a novel\nmask-guided RGB-D face recognition model that contains an RGB recognition\nbranch, a depth map recognition branch, and an auxiliary segmentation mask\nbranch with a spatial attention module. Our DepthNet is used to augment a large\n2D face image dataset to a large RGB-D face dataset, which is used for training\nan accurate RGB-D face recognition model. Furthermore, the proposed mask-guided\nRGB-D face recognition model can fully exploit the depth map and segmentation\nmask information and is more robust against pose variation than previous\nmethods. Our experimental results show that DepthNet can produce more reliable\ndepth maps from face images with the segmentation mask. Our mask-guided face\nrecognition model outperforms state-of-the-art methods on several public 3D\nface datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_M/0/1/0/all/0/1\">Meng-Tzu Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hsun-Ying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chien-Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shang-Hong Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing radiologists' gaze and saliency maps generated by interpretability methods for chest x-rays. (arXiv:2112.11716v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11716","description":"<p>The interpretability of medical image analysis models is considered a key\nresearch field. We use a dataset of eye-tracking data from five radiologists to\ncompare the outputs of interpretability methods against the heatmaps\nrepresenting where radiologists looked. We conduct a class-independent analysis\nof the saliency maps generated by two methods selected from the literature:\nGrad-CAM and attention maps from an attention-gated model. For the comparison,\nwe use shuffled metrics, which avoid biases from fixation locations. We achieve\nscores comparable to an interobserver baseline in one shuffled metric,\nhighlighting the potential of saliency maps from Grad-CAM to mimic a\nradiologist's attention over an image. We also divide the dataset into subsets\nto evaluate in which cases similarities are higher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lanfredi_R/0/1/0/all/0/1\">Ricardo Bigolin Lanfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Ambuj Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drew_T/0/1/0/all/0/1\">Trafton Drew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Local Optimality for Video Steganalysis in Motion Vector Domain. (arXiv:2112.11729v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11729","description":"<p>The local optimality of motion vectors (MVs) is an intrinsic property in\nvideo coding, and any modifications to the MVs will inevitably destroy this\noptimality, making it a sensitive indicator of steganography in the MV domain.\nThus the local optimality is commonly used to design steganalytic features, and\nthe estimation for local optimality has become a top priority in video\nsteganalysis. However, the local optimality in existing works is often\nestimated inaccurately or using an unreasonable assumption, limiting its\ncapability in steganalysis. In this paper, we propose to estimate the local\noptimality in a more reasonable and comprehensive fashion, and generalize the\nconcept of local optimality in two aspects. First, the local optimality\nmeasured in a rate-distortion sense is jointly determined by MV and predicted\nmotion vector (PMV), and the variability of PMV will affect the estimation for\nlocal optimality. Hence we generalize the local optimality from a static\nestimation to a dynamic one. Second, the PMV is a special case of MV, and can\nalso reflect the embedding traces in MVs. So we generalize the local optimality\nfrom the MV domain to the PMV domain. Based on the two generalizations of local\noptimality, we construct new types of steganalytic features and also propose\nfeature symmetrization rules to reduce feature dimension. Extensive experiments\nperformed on three databases demonstrate the effectiveness of the proposed\nfeatures, which achieve state-of-the-art in both accuracy and robustness in\nvarious conditions, including cover source mismatch, video prediction methods,\nvideo codecs, and video resolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_L/0/1/0/all/0/1\">Liming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yanzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Balance of Contrastive Losses. (arXiv:2112.11743v1 [cs.LG])","link":"http://arxiv.org/abs/2112.11743","description":"<p>Contrastive losses have long been a key ingredient of deep metric learning\nand are now becoming more popular due to the success of self-supervised\nlearning. Recent research has shown the benefit of decomposing such losses into\ntwo sub-losses which act in a complementary way when learning the\nrepresentation network: a positive term and an entropy term. Although the\noverall loss is thus defined as a combination of two terms, the balance of\nthese two terms is often hidden behind implementation details and is largely\nignored and sub-optimal in practice. In this work, we approach the balance of\ncontrastive losses as a hyper-parameter optimization problem, and propose a\ncoordinate descent-based search method that efficiently find the\nhyper-parameters that optimize evaluation performance. In the process, we\nextend existing balance analyses to the contrastive margin loss, include batch\nsize in the balance, and explain how to aggregate loss elements from the batch\nto maintain near-optimal performance over a larger range of batch sizes.\nExtensive experiments with benchmarks from deep metric learning and\nself-supervised learning show that optimal hyper-parameters are found faster\nwith our method than with other common search methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sors_A/0/1/0/all/0/1\">Arnaud Sors</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezende_R/0/1/0/all/0/1\">Rafael Sampaio de Rezende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimi_S/0/1/0/all/0/1\">Sarah Ibrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreoli_J/0/1/0/all/0/1\">Jean-Marc Andreoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-aware Sounding Objects Localization via Audiovisual Correspondence. (arXiv:2112.11749v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11749","description":"<p>Audiovisual scenes are pervasive in our daily life. It is commonplace for\nhumans to discriminatively localize different sounding objects but quite\nchallenging for machines to achieve class-aware sounding objects localization\nwithout category annotations, i.e., localizing the sounding object and\nrecognizing its category. To address this problem, we propose a two-stage\nstep-by-step learning framework to localize and recognize sounding objects in\ncomplex audiovisual scenarios using only the correspondence between audio and\nvision. First, we propose to determine the sounding area via coarse-grained\naudiovisual correspondence in the single source cases. Then visual features in\nthe sounding area are leveraged as candidate object representations to\nestablish a category-representation object dictionary for expressive visual\ncharacter extraction. We generate class-aware object localization maps in\ncocktail-party scenarios and use audiovisual correspondence to suppress silent\nareas by referring to this dictionary. Finally, we employ category-level\naudiovisual consistency as the supervision to achieve fine-grained audio and\nsounding object distribution alignment. Experiments on both realistic and\nsynthesized videos show that our model is superior in localizing and\nrecognizing objects as well as filtering out silent ones. We also transfer the\nlearned audiovisual network into the unsupervised object detection task,\nobtaining reasonable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yake Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Inter-frequency Guidance of Image for Lightweight Gaussian Denoising. (arXiv:2112.11779v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11779","description":"<p>Image denoising is of vital importance in many imaging or computer vision\nrelated areas. With the convolutional neural networks showing strong capability\nin computer vision tasks, the performance of image denoising has also been\nbrought up by CNN based methods. Though CNN based image denoisers show\npromising results on this task, most of the current CNN based methods try to\nlearn the mapping from noisy image to clean image directly, which lacks the\nexplicit exploration of prior knowledge of images and noises. Natural images\nare observed to obey the reciprocal power law, implying the low-frequency band\nof image tend to occupy most of the energy. Thus in the condition of AGWN\n(additive gaussian white noise) deterioration, low-frequency band tend to\npreserve a higher PSNR than high-frequency band. Considering the spatial\nmorphological consistency of different frequency bands, low-frequency band with\nmore fidelity can be used as a guidance to refine the more contaminated\nhigh-frequency bands. Based on this thought, we proposed a novel network\narchitecture denoted as IGNet, in order to refine the frequency bands from low\nto high in a progressive manner. Firstly, it decomposes the feature maps into\nhigh- and low-frequency subbands using DWT (discrete wavelet transform)\niteratively, and then each low band features are used to refine the high band\nfeatures. Finally, the refined feature maps are processed by a decoder to\nrecover the clean result. With this design, more inter-frequency prior and\ninformation are utilized, thus the model size can be lightened while still\nperserves competitive results. Experiments on several public datasets show that\nour model obtains competitive performance comparing with other state-of-the-art\nmethods yet with a lightweight structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhuang Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View. (arXiv:2112.11790v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11790","description":"<p>Autonomous driving perceives the surrounding environment for decision making,\nwhich is one of the most complicated scenes for visual perception. The great\npower of paradigm innovation in solving the 2D object detection task inspires\nus to seek an elegant, feasible, and scalable paradigm for pushing the\nperformance boundary in this area. To this end, we contribute the BEVDet\nparadigm in this paper. BEVDet is developed by following the principle of\ndetecting the 3D objects in Bird-Eye-View (BEV), where route planning can be\nhandily performed. In this paradigm, four kinds of modules are conducted in\nsuccession with different roles: an image-view encoder for encoding feature in\nimage view, a view transformer for feature transformation from image view to\nBEV, a BEV encoder for further encoding feature in BEV, and a task-specific\nhead for predicting the targets in BEV. We merely reuse the existing modules\nfor constructing BEVDet and make it feasible for multi-camera 3D object\ndetection by constructing an exclusive data augmentation strategy. The proposed\nparadigm works well in multi-camera 3D object detection and offers a good\ntrade-off between computing budget and performance. BEVDet with 704x256 (1/8 of\nthe competitors) image size scores 29.4% mAP and 38.4% NDS on the nuScenes val\nset, which is comparable with FCOS3D (i.e., 2008.2 GFLOPs, 1.7 FPS, 29.5% mAP\nand 37.2% NDS), while requires merely 12% computing budget of 239.4 GFLOPs and\nruns 4.3 times faster. Scaling up the input size to 1408x512, BEVDet scores\n34.9% mAP, and 41.7% NDS, which requires just 601.4 GFLOPs and significantly\nsuppresses FCOS3D by 5.4% mAP and 4.5% NDS. The superior performance of BEVDet\ntells the magic of paradigm innovation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dalong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles. (arXiv:2112.11798v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11798","description":"<p>As autonomous vehicles and autonomous racing rise in popularity, so does the\nneed for faster and more accurate detectors. While our naked eyes are able to\nextract contextual information almost instantly, even from far away, image\nresolution and computational resources limitations make detecting smaller\nobjects (that is, objects that occupy a small pixel area in the input image) a\ngenuinely challenging task for machines and a wide-open research field. This\nstudy explores how the popular YOLOv5 object detector can be modified to\nimprove its performance in detecting smaller objects, with a particular\napplication in autonomous racing. To achieve this, we investigate how replacing\ncertain structural elements of the model (as well as their connections and\nother parameters) can affect performance and inference time. In doing so, we\npropose a series of models at different scales, which we name `YOLO-Z', and\nwhich display an improvement of up to 6.9% in mAP when detecting smaller\nobjects at 50% IOU, at the cost of just a 3ms increase in inference time\ncompared to the original YOLOv5. Our objective is to inform future research on\nthe potential of adjusting a popular detector such as YOLOv5 to address\nspecific tasks and provide insights on how specific changes can impact small\nobject detection. Such findings, applied to the broader context of autonomous\nvehicles, could increase the amount of contextual information available to such\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benjumea_A/0/1/0/all/0/1\">Aduen Benjumea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1\">Izzedin Teeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1\">Fabio Cuzzolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1\">Andrew Bradley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary Image Skeletonization Using 2-Stage U-Net. (arXiv:2112.11824v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11824","description":"<p>Object Skeletonization is the process of extracting skeletal, line-like\nrepresentations of shapes. It provides a very useful tool for geometric shape\nunderstanding and minimal shape representation. It also has a wide variety of\napplications, most notably in anatomical research and activity detection.\nSeveral mathematical algorithmic approaches have been developed to solve this\nproblem, and some of them have been proven quite robust. However, a lesser\namount of attention has been invested into deep learning solutions for it. In\nthis paper, we use a 2-stage variant of the famous U-Net architecture to split\nthe problem space into two sub-problems: shape minimization and corrective\nskeleton thinning. Our model produces results that are visually much better\nthan the baseline SkelNetOn model. We propose a new metric, M-CCORR, based on\nnormalized correlation coefficients as an alternative to F1 for this challenge\nas it solves the problem of class imbalance, managing to recognize skeleton\nsimilarity without suffering from F1's over-sensitivity to pixel-shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_M/0/1/0/all/0/1\">Mohamed A. Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anani_A/0/1/0/all/0/1\">Alaa A. Anani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for brain metastasis detection and segmentation in longitudinal MRI data. (arXiv:2112.11833v1 [eess.IV])","link":"http://arxiv.org/abs/2112.11833","description":"<p>Brain metastases occur frequently in patients with metastatic cancer. Early\nand accurate detection of brain metastases is very essential for treatment\nplanning and prognosis in radiation therapy. To improve brain metastasis\ndetection performance with deep learning, a custom detection loss called\nvolume-level sensitivity-specificity (VSS) is proposed, which rates individual\nmetastasis detection sensitivity and specificity in (sub-)volume levels. As\nsensitivity and precision are always a trade-off in a metastasis level, either\na high sensitivity or a high precision can be achieved by adjusting the weights\nin the VSS loss without decline in dice score coefficient for segmented\nmetastases. To reduce metastasis-like structures being detected as false\npositive metastases, a temporal prior volume is proposed as an additional input\nof the neural network. Our proposed VSS loss improves the sensitivity of brain\nmetastasis detection, increasing the sensitivity from 86.7% to 95.5%.\nAlternatively, it improves the precision from 68.8% to 97.8%. With the\nadditional temporal prior volume, about 45% of the false positive metastases\nare reduced in the high sensitivity model and the precision reaches 99.6% for\nthe high specificity model. The mean dice coefficient for all metastases is\nabout 0.81. With the ensemble of the high sensitivity and high specificity\nmodels, on average only 1.5 false positive metastases per patient needs further\ncheck, while the majority of true positive metastases are confirmed. The\nensemble learning is able to distinguish high confidence true positive\nmetastases from metastases candidates that require special expert review or\nfurther follow-up, being particularly well-fit to the requirements of expert\nsupport in real clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bert_C/0/1/0/all/0/1\">Christoph Bert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sommer_P/0/1/0/all/0/1\">Philipp Sommer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frey_B/0/1/0/all/0/1\">Benjamin Frey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaipl_U/0/1/0/all/0/1\">Udo Gaipl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Distel_L/0/1/0/all/0/1\">Luitpold V. Distel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weissmann_T/0/1/0/all/0/1\">Thomas Weissmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uder_M/0/1/0/all/0/1\">Michael Uder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_M/0/1/0/all/0/1\">Manuel A. Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorfler_A/0/1/0/all/0/1\">Arnd D&#xf6;rfler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andrreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fietkau_R/0/1/0/all/0/1\">Rainer Fietkau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putz_F/0/1/0/all/0/1\">Florian Putz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bottom-up approaches for multi-person pose estimation and it's applications: A brief review. (arXiv:2112.11834v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11834","description":"<p>Human Pose Estimation (HPE) is one of the fundamental problems in computer\nvision. It has applications ranging from virtual reality, human behavior\nanalysis, video surveillance, anomaly detection, self-driving to medical\nassistance. The main objective of HPE is to obtain the person's posture from\nthe given input. Among different paradigms for HPE, one paradigm is called\nbottom-up multi-person pose estimation. In the bottom-up approach, initially,\nall the key points of the targets are detected, and later in the optimization\nstage, the detected key points are associated with the corresponding targets.\nThis review paper discussed the recent advancements in bottom-up approaches for\nthe HPE and listed the possible high-quality datasets used to train the models.\nAdditionally, a discussion of the prominent bottom-up approaches and their\nquantitative results on the standard performance matrices are given. Finally,\nthe limitations of the existing methods are highlighted, and guidelines of the\nfuture research directions are given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kresovic_M/0/1/0/all/0/1\">Milan Kresovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Duy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Discriminative Single-Shot Segmentation Network for Visual Object Tracking. (arXiv:2112.11846v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11846","description":"<p>Template-based discriminative trackers are currently the dominant tracking\nparadigm due to their robustness, but are restricted to bounding box tracking\nand a limited range of transformation models, which reduces their localization\naccuracy. We propose a discriminative single-shot segmentation tracker -- D3S2,\nwhich narrows the gap between visual object tracking and video object\nsegmentation. A single-shot network applies two target models with\ncomplementary geometric properties, one invariant to a broad range of\ntransformations, including non-rigid deformations, the other assuming a rigid\nobject to simultaneously achieve robust online target segmentation. The overall\ntracking reliability is further increased by decoupling the object and feature\nscale estimation. Without per-dataset finetuning, and trained only for\nsegmentation as the primary output, D3S2 outperforms all published trackers on\nthe recent short-term tracking benchmark VOT2020 and performs very close to the\nstate-of-the-art trackers on the GOT-10k, TrackingNet, OTB100 and LaSoT. D3S2\noutperforms the leading segmentation tracker SiamMask on video object\nsegmentation benchmarks and performs on par with top video object segmentation\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lukezic_A/0/1/0/all/0/1\">Alan Luke&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Analysis of memes for sentiment extraction. (arXiv:2112.11850v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11850","description":"<p>Memes are one of the most ubiquitous forms of social media communication. The\nstudy and processing of memes, which are intrinsically multimedia, is a popular\ntopic right now. The study presented in this research is based on the Memotion\ndataset, which involves categorising memes based on irony, comedy, motivation,\nand overall-sentiment. Three separate innovative transformer-based techniques\nhave been developed, and their outcomes have been thoroughly reviewed.The best\nalgorithm achieved a macro F1 score of 0.633 for humour classification, 0.55\nfor motivation classification, 0.61 for sarcasm classification, and 0.575 for\noverall sentiment of the meme out of all our techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alluri_N/0/1/0/all/0/1\">Nayan Varma Alluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_N/0/1/0/all/0/1\">Neeli Dheeraj Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geodesic squared exponential kernel for non-rigid shape registration. (arXiv:2112.11853v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11853","description":"<p>This work addresses the problem of non-rigid registration of 3D scans, which\nis at the core of shape modeling techniques. Firstly, we propose a new kernel\nbased on geodesic distances for the Gaussian Process Morphable Models (GPMMs)\nframework. The use of geodesic distances into the kernel makes it more adapted\nto the topological and geometric characteristics of the surface and leads to\nmore realistic deformations around holes and curved areas. Since the kernel\npossesses hyperparameters we have optimized them for the task of face\nregistration on the FaceWarehouse dataset. We show that the Geodesic squared\nexponential kernel performs significantly better than state of the art kernels\nfor the task of face registration on all the 20 expressions of the\nFaceWarehouse dataset. Secondly, we propose a modification of the loss function\nused in the non-rigid ICP registration algorithm, that allows to weight the\ncorrespondences according to the confidence given to them. As a use case, we\nshow that we can make the registration more robust to outliers in the 3D scans,\nsuch as non-skin parts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jousse_F/0/1/0/all/0/1\">Florent Jousse</a> (UCA, Qc, EPIONE), <a href=\"http://arxiv.org/find/cs/1/au:+Pennec_X/0/1/0/all/0/1\">Xavier Pennec</a> (UCA, EPIONE), <a href=\"http://arxiv.org/find/cs/1/au:+Delingette_H/0/1/0/all/0/1\">Herv&#xe9; Delingette</a> (UCA, EPIONE), <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Matilde Gonzalez</a> (Qc)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Font Generation with Weakly Supervised Localized Representations. (arXiv:2112.11895v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11895","description":"<p>Automatic few-shot font generation aims to solve a well-defined, real-world\nproblem because manual font designs are expensive and sensitive to the\nexpertise of designers. Existing methods learn to disentangle style and content\nelements by developing a universal style representation for each font style.\nHowever, this approach limits the model in representing diverse local styles,\nbecause it is unsuitable for complicated letter systems, for example, Chinese,\nwhose characters consist of a varying number of components (often called\n\"radical\") -- with a highly complex structure. In this paper, we propose a\nnovel font generation method that learns localized styles, namely\ncomponent-wise style representations, instead of universal styles. The proposed\nstyle representations enable the synthesis of complex local details in text\ndesigns. However, learning component-wise styles solely from a few reference\nglyphs is infeasible when a target script has a large number of components, for\nexample, over 200 for Chinese. To reduce the number of required reference\nglyphs, we represent component-wise styles by a product of component and style\nfactors, inspired by low-rank matrix factorization. Owing to the combination of\nstrong representation and a compact factorization strategy, our method shows\nremarkably better few-shot font generation results (with only eight reference\nglyphs) than other state-of-the-art methods. Moreover, strong locality\nsupervision, for example, location of each component, skeleton, or strokes, was\nnot utilized. The source code is available at https://github.com/clovaai/lffont\nand https://github.com/clovaai/fewshot-font-generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Song Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1\">Junbum Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bado Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1\">Hyunjung Shim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning and Self-Supervised Pretraining for Real World Image Translation. (arXiv:2112.11929v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11929","description":"<p>Recent advances in deep learning, in particular enabled by hardware advances\nand big data, have provided impressive results across a wide range of\ncomputational problems such as computer vision, natural language, or\nreinforcement learning. Many of these improvements are however constrained to\nproblems with large-scale curated data-sets which require a lot of human labor\nto gather. Additionally, these models tend to generalize poorly under both\nslight distributional shifts and low-data regimes. In recent years, emerging\nfields such as meta-learning or self-supervised learning have been closing the\ngap between proof-of-concept results and real-life applications of machine\nlearning by extending deep-learning to the semi-supervised and few-shot\ndomains. We follow this line of work and explore spatio-temporal structure in a\nrecently introduced image-to-image translation problem in order to: i)\nformulate a novel multi-task few-shot image generation benchmark and ii)\nexplore data augmentations in contrastive pre-training for image translation\ndownstream tasks. We present several baselines for the few-shot problem and\ndiscuss trade-offs between different approaches. Our code is available at\nhttps://github.com/irugina/meta-image-translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rugina_I/0/1/0/all/0/1\">Ileana Rugina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dangovski_R/0/1/0/all/0/1\">Rumen Dangovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veillette_M/0/1/0/all/0/1\">Mark Veillette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorrami_P/0/1/0/all/0/1\">Pooya Khorrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simek_O/0/1/0/all/0/1\">Olga Simek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1\">Marin Solja&#x10d;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Page Segmentation using Visual Adjacency Analysis. (arXiv:2112.11975v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11975","description":"<p>Page segmentation is a web page analysis process that divides a page into\ncohesive segments, such as sidebars, headers, and footers. Current page\nsegmentation approaches use either the DOM, textual content, or rendering style\ninformation of the page. However, these approaches have a number of drawbacks,\nsuch as a large number of parameters and rigid assumptions about the page,\nwhich negatively impact their segmentation accuracy. We propose a novel page\nsegmentation approach based on visual analysis of localized adjacency regions.\nIt combines DOM attributes and visual analysis to build features of a given\npage and guide an unsupervised clustering. We evaluate our approach on 35\nreal-world web pages, and examine the effectiveness and efficiency of\nsegmentation. The results show that, compared with state-of-the-art, our\napproach achieves an average of 156% increase in precision and 249% improvement\nin F-measure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bajammal_M/0/1/0/all/0/1\">Mohammad Bajammal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesbah_A/0/1/0/all/0/1\">Ali Mesbah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Estimation of Anthropometric Human Body Measurements. (arXiv:2112.11992v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11992","description":"<p>Research tasks related to human body analysis have been drawing a lot of\nattention in computer vision area over the last few decades, considering its\npotential benefits on our day-to-day life. Anthropometry is a field defining\nphysical measures of a human body size, form, and functional capacities.\nSpecifically, the accurate estimation of anthropometric body measurements from\nvisual human body data is one of the challenging problems, where the solution\nwould ease many different areas of applications, including ergonomics, garment\nmanufacturing, etc. This paper formulates a research in the field of deep\nlearning and neural networks, to tackle the challenge of body measurements\nestimation from various types of visual input data (such as 2D images or 3D\npoint clouds). Also, we deal with the lack of real human data annotated with\nground truth body measurements required for training and evaluation, by\ngenerating a synthetic dataset of various human body shapes and performing a\nskeleton-driven annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skorvankova_D/0/1/0/all/0/1\">Dana &#x160;korv&#xe1;nkov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riecicky_A/0/1/0/all/0/1\">Adam Rie&#x10d;ick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaras_M/0/1/0/all/0/1\">Martin Madaras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DA-FDFtNet: Dual Attention Fake Detection Fine-tuning Network to Detect Various AI-Generated Fake Images. (arXiv:2112.12001v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12001","description":"<p>Due to the advancement of Generative Adversarial Networks (GAN),\nAutoencoders, and other AI technologies, it has been much easier to create fake\nimages such as \"Deepfakes\". More recent research has introduced few-shot\nlearning, which uses a small amount of training data to produce fake images and\nvideos more effectively. Therefore, the ease of generating manipulated images\nand the difficulty of distinguishing those images can cause a serious threat to\nour society, such as propagating fake information. However, detecting realistic\nfake images generated by the latest AI technology is challenging due to the\nreasons mentioned above. In this work, we propose Dual Attention Fake Detection\nFine-tuning Network (DA-FDFtNet) to detect the manipulated fake face images\nfrom the real face data. Our DA-FDFtNet integrates the pre-trained model with\nFine-Tune Transformer, MBblockV3, and a channel attention module to improve the\nperformance and robustness across different types of fake images. In\nparticular, Fine-Tune Transformer consists of multiple numbers of an\nimage-based self-attention module and a down-sampling layer. The channel\nattention module is also connected with the pre-trained model to capture the\nfake images feature space. We experiment with our DA-FDFtNet with the\nFaceForensics++ dataset and various GAN-generated datasets, and we show that\nour approach outperforms the previous baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Young Oh Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Beyond Corners: Contrastive Learning of Visual Representations for Keypoint Detection and Description Extraction. (arXiv:2112.12002v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12002","description":"<p>Learnable keypoint detectors and descriptors are beginning to outperform\nclassical hand-crafted feature extraction methods. Recent studies on\nself-supervised learning of visual representations have driven the increasing\nperformance of learnable models based on deep networks. By leveraging\ntraditional data augmentations and homography transformations, these networks\nlearn to detect corners under adverse conditions such as extreme illumination\nchanges. However, their generalization capabilities are limited to corner-like\nfeatures detected a priori by classical methods or synthetically generated\ndata.\n</p>\n<p>In this paper, we propose the Correspondence Network (CorrNet) that learns to\ndetect repeatable keypoints and to extract discriminative descriptions via\nunsupervised contrastive learning under spatial constraints. Our experiments\nshow that CorrNet is not only able to detect low-level features such as\ncorners, but also high-level features that represent similar objects present in\na pair of input images through our proposed joint guided backpropagation of\ntheir latent space. Our approach obtains competitive results under viewpoint\nchanges and achieves state-of-the-art performance under illumination changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_H/0/1/0/all/0/1\">Henrique Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhkamp_P/0/1/0/all/0/1\">Patrick Ruhkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halfaoui_I/0/1/0/all/0/1\">Ibrahim Halfaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmann_M/0/1/0/all/0/1\">Markus Karmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urfalioglu_O/0/1/0/all/0/1\">Onay Urfalioglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Barely-Supervised Learning: Semi-Supervised Learning with very few labeled images. (arXiv:2112.12004v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12004","description":"<p>This paper tackles the problem of semi-supervised learning when the set of\nlabeled samples is limited to a small number of images per class, typically\nless than 10, problem that we refer to as barely-supervised learning. We\nanalyze in depth the behavior of a state-of-the-art semi-supervised method,\nFixMatch, which relies on a weakly-augmented version of an image to obtain\nsupervision signal for a more strongly-augmented version. We show that it\nfrequently fails in barely-supervised scenarios, due to a lack of training\nsignal when no pseudo-label can be predicted with high confidence. We propose a\nmethod to leverage self-supervised methods that provides training signal in the\nabsence of confident pseudo-labels. We then propose two methods to refine the\npseudo-label selection process which lead to further improvements. The first\none relies on a per-sample history of the model predictions, akin to a voting\nscheme. The second iteratively updates class-dependent confidence thresholds to\nbetter explore classes that are under-represented in the pseudo-labels. Our\nexperiments show that our approach performs significantly better on STL-10 in\nthe barely-supervised regime, e.g. with 4 or 8 labeled images per class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucas_T/0/1/0/all/0/1\">Thomas Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinzaepfel_P/0/1/0/all/0/1\">Philippe Weinzaepfel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogez_G/0/1/0/all/0/1\">Gregory Rogez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Community Detection in Medical Image Datasets: Using Wavelets and Spectral Methods. (arXiv:2112.12021v1 [eess.IV])","link":"http://arxiv.org/abs/2112.12021","description":"<p>Medical image datasets can have large number of images representing patients\nwith different health conditions and various disease severity. When dealing\nwith raw unlabeled image datasets, the large number of samples often makes it\nhard for experts and non-experts to understand the variety of images present in\na dataset. Supervised learning methods rely on labeled images which requires a\nconsiderable effort by medical experts to first understand the communities of\nimages present in the data and then labeling the images. Here, we propose an\nalgorithm to facilitate the automatic identification of communities in medical\nimage datasets. We further explain that such analysis can also be insightful in\na supervised setting, when the images are already labeled. Such insights are\nuseful because in reality, health and disease severity can be considered a\ncontinuous spectrum, and within each class, there usually are finer communities\nworthy of investigation, especially when they have similarities to communities\nin other classes. In our approach, we use wavelet decomposition of images in\ntandem with spectral methods. We show that the eigenvalues of a graph Laplacian\ncan reveal the number of notable communities in an image dataset. In our\nexperiments, we use a dataset of images labeled with different conditions for\nCOVID patients. We detect 25 communities in the dataset and then observe that\nonly 6 of those communities contain patients with pneumonia. We also\ninvestigate the contents of a colorectal cancer histopathology dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yousefzadeh_R/0/1/0/all/0/1\">Roozbeh Yousefzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning and Crafting for the Wide Multiple Baseline Stereo. (arXiv:2112.12027v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12027","description":"<p>This thesis introduces the wide multiple baseline stereo (WxBS) problem.\nWxBS, a generalization of the standard wide baseline stereo problem, considers\nthe matching of images that simultaneously differ in more than one image\nacquisition factor such as viewpoint, illumination, sensor type, or where\nobject appearance changes significantly, e.g., over time. A new dataset with\nthe ground truth, evaluation metric and baselines has been introduced.\n</p>\n<p>The thesis presents the following improvements of the WxBS pipeline. (i) A\nloss function, called HardNeg, for learning a local image descriptor that\nrelies on hard negative mining within a mini-batch and on the maximization of\nthe distance between the closest positive and the closest negative patches.\n(ii) The descriptor trained with the HardNeg loss, called HardNet, is compact\nand shows state-of-the-art performance in standard matching, patch verification\nand retrieval benchmarks. (iii) A method for learning the affine shape,\norientation, and potentially other parameters related to geometric and\nappearance properties of local features. (iv) A tentative correspondences\ngeneration strategy which generalizes the standard first to second closest\ndistance ratio is presented. The selection strategy, which shows performance\nsuperior to the standard method, is applicable to either hard-engineered\ndescriptors like SIFT, LIOP, and MROGH or deeply learned like HardNet. (v) A\nfeedback loop is introduced for the two-view matching problem, resulting in\nMODS -- matching with on-demand view synthesis -- algorithm. MODS is an\nalgorithm that handles a viewing angle difference even larger than the previous\nstate-of-the-art ASIFT algorithm, without a significant increase of\ncomputational cost over \"standard\" wide and narrow baseline approaches. Last,\nbut not least, a comprehensive benchmark for local features and robust\nestimation algorithms is introduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1\">Dmytro Mishkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Partial (MVP) Point Cloud Challenge 2021 on Completion and Registration: Methods and Results. (arXiv:2112.12053v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12053","description":"<p>As real-scanned point clouds are mostly partial due to occlusions and\nviewpoints, reconstructing complete 3D shapes based on incomplete observations\nbecomes a fundamental problem for computer vision. With a single incomplete\npoint cloud, it becomes the partial point cloud completion problem. Given\nmultiple different observations, 3D reconstruction can be addressed by\nperforming partial-to-partial point cloud registration. Recently, a large-scale\nMulti-View Partial (MVP) point cloud dataset has been released, which consists\nof over 100,000 high-quality virtual-scanned partial point clouds. Based on the\nMVP dataset, this paper reports methods and results in the Multi-View Partial\nPoint Cloud Challenge 2021 on Completion and Registration. In total, 128\nparticipants registered for the competition, and 31 teams made valid\nsubmissions. The top-ranked solutions will be analyzed, and then we will\ndiscuss future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingye Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kexue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junsheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuanjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Junyi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lifa Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Changwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Fernandez_F/0/1/0/all/0/1\">Francisco G&#xf3;mez-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Models for Visual Sentiment Analysis of Disaster-related Multimedia Content. (arXiv:2112.12060v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12060","description":"<p>This paper presents a solutions for the MediaEval 2021 task namely \"Visual\nSentiment Analysis: A Natural Disaster Use-case\". The task aims to extract and\nclassify sentiments perceived by viewers and the emotional message conveyed by\nnatural disaster-related images shared on social media. The task is composed of\nthree sub-tasks including, one single label multi-class image classification\ntask, and, two multi-label multi-class image classification tasks, with\ndifferent sets of labels. In our proposed solutions, we rely mainly on two\ndifferent state-of-the-art models namely, Inception-v3 and VggNet-19,\npre-trained on ImageNet, which are fine-tuned for each of the three task using\ndifferent strategies. Overall encouraging results are obtained on all the three\ntasks. On the single-label classification task (i.e. Task 1), we obtained the\nweighted average F1-scores of 0.540 and 0.526 for the Inception-v3 and\nVggNet-19 based solutions, respectively. On the multi-label classification\ni.e., Task 2 and Task 3, the weighted F1-score of our Inception-v3 based\nsolutions was 0.572 and 0.516, respectively. Similarly, the weighted F1-score\nof our VggNet-19 based solution on Task 2 and Task 3 was 0.584 and 0.495,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Khubaib Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayub_M/0/1/0/all/0/1\">Muhammad Asif Ayub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Single-Target License Plate Detection with Attention. (arXiv:2112.12070v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12070","description":"<p>With the development of deep learning, Neural Network is commonly adopted to\nthe License Plate Detection (LPD) task and achieves much better performance and\nprecision, especially CNN-based networks can achieve state of the art\nRetinaNet[1]. For a single object detection task such as LPD, modified general\nobject detection would be time-consuming, unable to cope with complex scenarios\nand a cumbersome weights file that is too hard to deploy on the embedded\ndevice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization. (arXiv:2112.12072v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12072","description":"<p>Multimodal summarization with multimodal output (MSMO) generates a summary\nwith both textual and visual content. Multimodal news report contains\nheterogeneous contents, which makes MSMO nontrivial. Moreover, it is observed\nthat different modalities of data in the news report correlate hierarchically.\nTraditional MSMO methods indistinguishably handle different modalities of data\nby learning a representation for the whole data, which is not directly\nadaptable to the heterogeneous contents and hierarchical correlation. In this\npaper, we propose a hierarchical cross-modality semantic correlation learning\nmodel (HCSCL) to learn the intra- and inter-modal correlation existing in the\nmultimodal data. HCSCL adopts a graph network to encode the intra-modal\ncorrelation. Then, a hierarchical fusion framework is proposed to learn the\nhierarchical correlation between text and images. Furthermore, we construct a\nnew dataset with relevant image annotation and image object label information\nto provide the supervision information for the learning procedure. Extensive\nexperiments on the dataset show that HCSCL significantly outperforms the\nbaseline methods in automatic summarization metrics and fine-grained diversity\ntests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Litian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junshu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiran Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Stream Network for Stroke Detection in Table Tennis. (arXiv:2112.12073v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12073","description":"<p>This paper presents a table tennis stroke detection method from videos. The\nmethod relies on a two-stream Convolutional Neural Network processing in\nparallel the RGB Stream and its computed optical flow. The method has been\ndeveloped as part of the MediaEval 2021 benchmark for the Sport task. Our\ncontribution did not outperform the provided baseline on the test set but has\nperformed the best among the other participants with regard to the mAP metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zahra_A/0/1/0/all/0/1\">Anam Zahra</a> (MPI-EVA), <a href=\"http://arxiv.org/find/cs/1/au:+Martin_P/0/1/0/all/0/1\">Pierre-Etienne Martin</a> (LaBRI, MPI-EVA, UB)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal CNN baseline method for the Sports Video Task of MediaEval 2021 benchmark. (arXiv:2112.12074v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12074","description":"<p>This paper presents the baseline method proposed for the Sports Video task\npart of the MediaEval 2021 benchmark. This task proposes a stroke detection and\na stroke classification subtasks. This baseline addresses both subtasks. The\nspatio-temporal CNN architecture and the training process of the model are\ntailored according to the addressed subtask. The method has the purpose of\nhelping the participants to solve the task and is not meant to reach\nstateof-the-art performance. Still, for the detection task, the baseline is\nperforming better than the other participants, which stresses the difficulty of\nsuch a task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_P/0/1/0/all/0/1\">Pierre-Etienne Martin</a> (LaBRI, MPI-EVA, UB)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeper Learning with CoLU Activation. (arXiv:2112.12078v1 [cs.LG])","link":"http://arxiv.org/abs/2112.12078","description":"<p>In neural networks, non-linearity is introduced by activation functions. One\ncommonly used activation function is Rectified Linear Unit (ReLU). ReLU has\nbeen a popular choice as an activation but has flaws. State-of-the-art\nfunctions like Swish and Mish are now gaining attention as a better choice as\nthey combat many flaws presented by other activation functions. CoLU is an\nactivation function similar to Swish and Mish in properties. It is defined as\nf(x)=x/(1-xe^-(x+e^x)). It is smooth, continuously differentiable, unbounded\nabove, bounded below, non-saturating, and non-monotonic. Based on experiments\ndone with CoLU with different activation functions, it is observed that CoLU\nusually performs better than other functions on deeper neural networks. While\ntraining different neural networks on MNIST on an incrementally increasing\nnumber of convolutional layers, CoLU retained the highest accuracy for more\nlayers. On a smaller network with 8 convolutional layers, CoLU had the highest\nmean accuracy, closely followed by ReLU. On VGG-13 trained on Fashion-MNIST,\nCoLU had a 4.20% higher accuracy than Mish and 3.31% higher accuracy than ReLU.\nOn ResNet-9 trained on Cifar-10, CoLU had 0.05% higher accuracy than Swish,\n0.09% higher accuracy than Mish, and 0.29% higher accuracy than ReLU. It is\nobserved that activation functions may behave better than other activation\nfunctions based on different factors including the number of layers, types of\nlayers, number of parameters, learning rate, optimizer, etc. Further research\ncan be done on these factors and activation functions for more optimal\nactivation functions and more knowledge on their behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vagerwal_A/0/1/0/all/0/1\">Advait Vagerwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Adaptive Noise Covariance Matrices Estimation and Filtering Method: Application to Multi-Object Tracking. (arXiv:2112.12082v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12082","description":"<p>Kalman filters are widely used for object tracking, where process and\nmeasurement noise are usually considered accurately known and constant.\nHowever, the exact known and constant assumptions do not always hold in\npractice. For example, when lidar is used to track noncooperative targets, the\nmeasurement noise is different under different distances and weather\nconditions. In addition, the process noise changes with the object's motion\nstate, especially when the tracking object is a pedestrian, and the process\nnoise changes more frequently. This paper proposes a new\nestimation-calibration-correction closed-loop estimation method to estimate the\nKalman filter process and measurement noise covariance matrices online. First,\nwe decompose the noise covariance matrix into an element distribution matrix\nand noise intensity and improve the Sage filter to estimate the element\ndistribution matrix. Second, we propose a calibration method to accurately\ndiagnose the noise intensity deviation. We then propose a correct method to\nadaptively correct the noise intensity online. Third, under the assumption that\nthe system is detectable, the unbiased and convergence of the proposed method\nis mathematically proven. Simulation results prove the effectiveness and\nreliability of the proposed method. Finally, we apply the proposed method to\nmultiobject tracking of lidar and evaluate it on the official KITTI server. The\nproposed method on the KITTI pedestrian multiobject tracking leaderboard\n(<a href=\"http://www.cvlibs.net/datasets\">this http URL</a> /kitti/eval_tracking.php) surpasses all\nexisting methods using lidar, proving the feasibility of the method in\npractical applications. This work provides a new way to improve the performance\nof the Kalman filter and multiobject tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shuhang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huawei Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input-Specific Robustness Certification for Randomized Smoothing. (arXiv:2112.12084v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12084","description":"<p>Although randomized smoothing has demonstrated high certified robustness and\nsuperior scalability to other certified defenses, the high computational\noverhead of the robustness certification bottlenecks the practical\napplicability, as it depends heavily on the large sample approximation for\nestimating the confidence interval. In existing works, the sample size for the\nconfidence interval is universally set and agnostic to the input for\nprediction. This Input-Agnostic Sampling (IAS) scheme may yield a poor Average\nCertified Radius (ACR)-runtime trade-off which calls for improvement. In this\npaper, we propose Input-Specific Sampling (ISS) acceleration to achieve the\ncost-effectiveness for robustness certification, in an adaptive way of reducing\nthe sampling size based on the input characteristic. Furthermore, our method\nuniversally controls the certified radius decline from the ISS sample size\nreduction. The empirical results on CIFAR-10 and ImageNet show that ISS can\nspeed up the certification by more than three times at a limited cost of 0.05\ncertified radius. Meanwhile, ISS surpasses IAS on the average certified radius\nacross the extensive hyperparameter settings. Specifically, ISS achieves\nACR=0.958 on ImageNet ($\\sigma=1.0$) in 250 minutes, compared to ACR=0.917 by\nIAS under the same condition. We release our code in\n\\url{https://github.com/roy-ch/Input-Specific-Certification}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruoxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_B/0/1/0/all/0/1\">Bin Sheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved skin lesion recognition by a Self-Supervised Curricular Deep Learning approach. (arXiv:2112.12086v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12086","description":"<p>State-of-the-art deep learning approaches for skin lesion recognition often\nrequire pretraining on larger and more varied datasets, to overcome the\ngeneralization limitations derived from the reduced size of the skin lesion\nimaging datasets. ImageNet is often used as the pretraining dataset, but its\ntransferring potential is hindered by the domain gap between the source dataset\nand the target dermatoscopic scenario. In this work, we introduce a novel\npretraining approach that sequentially trains a series of Self-Supervised\nLearning pretext tasks and only requires the unlabeled skin lesion imaging\ndata. We present a simple methodology to establish an ordering that defines a\npretext task curriculum. For the multi-class skin lesion classification\nproblem, and ISIC-2019 dataset, we provide experimental evidence showing that:\ni) a model pretrained by a curriculum of pretext tasks outperforms models\npretrained by individual pretext tasks, and ii) a model pretrained by the\noptimal pretext task curriculum outperforms a model pretrained on ImageNet. We\ndemonstrate that this performance gain is related to the fact that the\ncurriculum of pretext tasks better focuses the attention of the final model on\nthe skin lesion. Beyond performance improvement, this strategy allows for a\nlarge reduction in the training time with respect to ImageNet pretraining,\nwhich is especially advantageous for network architectures tailored for a\nspecific problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirotkin_K/0/1/0/all/0/1\">Kirill Sirotkin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vinolo_M/0/1/0/all/0/1\">Marcos Escudero Vi&#xf1;olo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Carballeira_P/0/1/0/all/0/1\">Pablo Carballeira</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+SanMiguel_J/0/1/0/all/0/1\">Juan Carlos SanMiguel</a> (1) ((1) Universidad Aut&#xf3;noma de Madrid, Escuela Polit&#xe9;cnica Superior, Spain)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflash Dropout in Image Super-Resolution. (arXiv:2112.12089v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12089","description":"<p>Dropout is designed to relieve the overfitting problem in high-level vision\ntasks but is rarely applied in low-level vision tasks, like image\nsuper-resolution (SR). As a classic regression problem, SR exhibits a different\nbehaviour as high-level tasks and is sensitive to the dropout operation.\nHowever, in this paper, we show that appropriate usage of dropout benefits SR\nnetworks and improves the generalization ability. Specifically, dropout is\nbetter embedded at the end of the network and is significantly helpful for the\nmulti-degradation settings. This discovery breaks our common sense and inspires\nus to explore its working mechanism. We further use two analysis tools -- one\nis from recent network interpretation works, and the other is specially\ndesigned for this task. The analysis results provide side proofs to our\nexperimental findings and show us a new perspective to understand SR networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangtao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xina Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NICE-SLAM: Neural Implicit Scalable Encoding for SLAM. (arXiv:2112.12130v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12130","description":"<p>Neural implicit representations have recently shown encouraging results in\nvarious domains, including promising progress in simultaneous localization and\nmapping (SLAM). Nevertheless, existing methods produce over-smoothed scene\nreconstructions and have difficulty scaling up to large scenes. These\nlimitations are mainly due to their simple fully-connected network architecture\nthat does not incorporate local information in the observations. In this paper,\nwe present NICE-SLAM, a dense SLAM system that incorporates multi-level local\ninformation by introducing a hierarchical scene representation. Optimizing this\nrepresentation with pre-trained geometric priors enables detailed\nreconstruction on large indoor scenes. Compared to recent neural implicit SLAM\nsystems, our approach is more scalable, efficient, and robust. Experiments on\nfive challenging datasets demonstrate competitive results of NICE-SLAM in both\nmapping and tracking quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Songyou Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_V/0/1/0/all/0/1\">Viktor Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Deep Neural Networks be Converted to Ultra Low-Latency Spiking Neural Networks?. (arXiv:2112.12133v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12133","description":"<p>Spiking neural networks (SNNs), that operate via binary spikes distributed\nover time, have emerged as a promising energy efficient ML paradigm for\nresource-constrained devices. However, the current state-of-the-art (SOTA) SNNs\nrequire multiple time steps for acceptable inference accuracy, increasing\nspiking activity and, consequently, energy consumption. SOTA training\nstrategies for SNNs involve conversion from a non-spiking deep neural network\n(DNN). In this paper, we determine that SOTA conversion strategies cannot yield\nultra low latency because they incorrectly assume that the DNN and SNN\npre-activation values are uniformly distributed. We propose a new training\nalgorithm that accurately captures these distributions, minimizing the error\nbetween the DNN and converted SNN. The resulting SNNs have ultra low latency\nand high activation sparsity, yielding significant improvements in compute\nefficiency. In particular, we evaluate our framework on image recognition tasks\nfrom CIFAR-10 and CIFAR-100 datasets on several VGG and ResNet architectures.\nWe obtain top-1 accuracy of 64.19% with only 2 time steps on the CIFAR-100\ndataset with ~159.2x lower compute energy compared to an iso-architecture\nstandard DNN. Compared to other SOTA SNN models, our models perform inference\n2.5-8x faster (i.e., with fewer time steps).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1\">Gourav Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving. (arXiv:2112.12141v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12141","description":"<p>3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other\nuse cases in many factors, including the 3D resolution and range of data,\nabsence of dense depth maps, failure modes for LiDAR, relative location between\nthe camera and LiDAR, and a high bar for estimation accuracy. Data collected\nfor other use cases (such as virtual reality, gaming, and animation) may\ntherefore not be usable for AV applications. This necessitates the collection\nand annotation of a large amount of 3D data for HPE in AV, which is\ntime-consuming and expensive. In this paper, we propose one of the first\napproaches to alleviate this problem in the AV setting. Specifically, we\npropose a multi-modal approach which uses 2D labels on RGB images as weak\nsupervision to perform 3D HPE. The proposed multi-modal architecture\nincorporates LiDAR and camera inputs with an auxiliary segmentation branch. On\nthe Waymo Open Dataset, our approach achieves a 22% relative improvement over\ncamera-only 2D HPE baseline, and 6% improvement over LiDAR-only model. Finally,\ncareful ablation studies and parts based analysis illustrate the advantages of\neach of our contributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jingxiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xinwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1\">Alexander Gorban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Junhua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Charles R. Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chari_V/0/1/0/all/0/1\">Visesh Chari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornman_A/0/1/0/all/0/1\">Andre Cornman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Congcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Vocabulary Image Segmentation. (arXiv:2112.12143v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12143","description":"<p>We design an open-vocabulary image segmentation model to organize an image\ninto meaningful regions indicated by arbitrary texts. We identify that recent\nopen-vocabulary models can not localize visual concepts well despite\nrecognizing what are in an image. We argue that these models miss an important\nstep of visual grouping, which organizes pixels into groups before learning\nvisual-semantic alignments. We propose OpenSeg to address the above issue.\nFirst, it learns to propose segmentation masks for possible organizations. Then\nit learns visual-semantic alignments by aligning each word in a caption to one\nor a few predicted masks. We find the mask representations are the key to\nsupport learning from captions, making it possible to scale up the dataset and\nvocabulary sizes. Our work is the first to perform zero-shot transfer on\nholdout segmentation datasets. We set up two strong baselines by applying class\nactivation maps or fine-tuning with pixel-wise labels on a pre-trained ALIGN\nmodel. OpenSeg outperforms these baselines by 3.4 mIoU on PASCAL-Context (459\nclasses) and 2.7 mIoU on ADE-20k (847 classes).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiuye Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HAD-GAN: A Human-perception Auxiliary Defense GAN to Defend Adversarial Examples. (arXiv:1909.07558v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.07558","description":"<p>Adversarial examples reveal the vulnerability and unexplained nature of\nneural networks. Studying the defense of adversarial examples is of\nconsiderable practical importance. Most adversarial examples that misclassify\nnetworks are often undetectable by humans. In this paper, we propose a defense\nmodel to train the classifier into a human-perception classification model with\nshape preference. The proposed model comprising a texture transfer network\n(TTN) and an auxiliary defense generative adversarial networks (GAN) is called\nHuman-perception Auxiliary Defense GAN (HAD-GAN). The TTN is used to extend the\ntexture samples of a clean image and helps classifiers focus on its shape. GAN\nis utilized to form a training framework for the model and generate the\nnecessary images. A series of experiments conducted on MNIST, Fashion-MNIST and\nCIFAR10 show that the proposed model outperforms the state-of-the-art defense\nmethods for network robustness. The model also demonstrates a significant\nimprovement on defense capability of adversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wanting Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lingyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Images Processing for Car Detection using Convolutional Neural Networks: Comparison between Faster R-CNN and YoloV3. (arXiv:1910.07234v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1910.07234","description":"<p>In this paper, we address the problem of car detection from aerial images\nusing Convolutional Neural Networks (CNN). This problem presents additional\nchallenges as compared to car (or any object) detection from ground images\nbecause features of vehicles from aerial images are more difficult to discern.\nTo investigate this issue, we assess the performance of two state-of-the-art\nCNN algorithms, namely Faster R-CNN, which is the most popular region-based\nalgorithm, and YOLOv3, which is known to be the fastest detection algorithm. We\nanalyze two datasets with different characteristics to check the impact of\nvarious factors, such as UAV's altitude, camera resolution, and object size. A\ntotal of 39 training experiments were conducted to account for the effect of\ndifferent hyperparameter values. The objective of this work is to conduct the\nmost robust and exhaustive comparison between these two cutting-edge algorithms\non the specific domain of aerial images. By using a variety of metrics, we show\nthat YOLOv3 yields better performance in most configurations, except that it\nexhibits a lower recall and less confident detections when object sizes and\nscales in the testing dataset differ largely from those in the training\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ammar_A/0/1/0/all/0/1\">Adel Ammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koubaa_A/0/1/0/all/0/1\">Anis Koubaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Mohanned Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_A/0/1/0/all/0/1\">Abdulrahman Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benjdira_B/0/1/0/all/0/1\">Bilel Benjdira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ocular Recognition Databases and Competitions: A Survey. (arXiv:1911.09646v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.09646","description":"<p>The use of the iris and periocular region as biometric traits has been\nextensively investigated, mainly due to the singularity of the iris features\nand the use of the periocular region when the image resolution is not\nsufficient to extract iris information. In addition to providing information\nabout an individual's identity, features extracted from these traits can also\nbe explored to obtain other information such as the individual's gender, the\ninfluence of drug use, the use of contact lenses, spoofing, among others. This\nwork presents a survey of the databases created for ocular recognition,\ndetailing their protocols and how their images were acquired. We also describe\nand discuss the most popular ocular recognition competitions (contests),\nhighlighting the submitted algorithms that achieved the best results using only\niris trait and also fusing iris and periocular region information. Finally, we\ndescribe some relevant works applying deep learning techniques to ocular\nrecognition and point out new challenges and future directions. Considering\nthat there are a large number of ocular databases, and each one is usually\ndesigned for a specific problem, we believe this survey can provide a broad\noverview of the challenges in ocular biometrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zanlorensi_L/0/1/0/all/0/1\">Luiz A. Zanlorensi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luz_E/0/1/0/all/0/1\">Eduardo Luz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1\">Alceu S. Britto Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Luiz S. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Little Bit More: Bitplane-Wise Bit-Depth Recovery. (arXiv:2005.01091v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2005.01091","description":"<p>Imaging sensors digitize incoming scene light at a dynamic range of 10--12\nbits (i.e., 1024--4096 tonal values). The sensor image is then processed\nonboard the camera and finally quantized to only 8 bits (i.e., 256 tonal\nvalues) to conform to prevailing encoding standards. There are a number of\nimportant applications, such as high-bit-depth displays and photo editing,\nwhere it is beneficial to recover the lost bit depth. Deep neural networks are\neffective at this bit-depth reconstruction task. Given the quantized\nlow-bit-depth image as input, existing deep learning methods employ a\nsingle-shot approach that attempts to either (1) directly estimate the\nhigh-bit-depth image, or (2) directly estimate the residual between the high-\nand low-bit-depth images. In contrast, we propose a training and inference\nstrategy that recovers the residual image bitplane-by-bitplane. Our\nbitplane-wise learning framework has the advantage of allowing for multiple\nlevels of supervision during training and is able to obtain state-of-the-art\nresults using a simple network architecture. We test our proposed method\nextensively on several image datasets and demonstrate an improvement from 0.5dB\nto 2.3dB PSNR over prior methods depending on the quantization level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Punnappurath_A/0/1/0/all/0/1\">Abhijith Punnappurath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Image Segmentation Using Deep Learning: A Survey. (arXiv:2009.13120v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2009.13120","description":"<p>Deep learning has been widely used for medical image segmentation and a large\nnumber of papers has been presented recording the success of deep learning in\nthe field. In this paper, we present a comprehensive thematic survey on medical\nimage segmentation using deep learning techniques. This paper makes two\noriginal contributions. Firstly, compared to traditional surveys that directly\ndivide literatures of deep learning on medical image segmentation into many\ngroups and introduce literatures in detail for each group, we classify\ncurrently popular literatures according to a multi-level structure from coarse\nto fine. Secondly, this paper focuses on supervised and weakly supervised\nlearning approaches, without including unsupervised approaches since they have\nbeen introduced in many old surveys and they are not popular currently. For\nsupervised learning approaches, we analyze literatures in three aspects: the\nselection of backbone networks, the design of network blocks, and the\nimprovement of loss functions. For weakly supervised learning approaches, we\ninvestigate literature according to data augmentation, transfer learning, and\ninteractive segmentation, separately. Compared to existing surveys, this survey\nclassifies the literatures very differently from before and is more convenient\nfor readers to understand the relevant rationale and will guide them to think\nof appropriate improvements in medical image segmentation based on deep\nlearning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Risheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_R/0/1/0/all/0/1\">Ruixia Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_B/0/1/0/all/0/1\">Bingtao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Hongying Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nandi_A/0/1/0/all/0/1\">Asoke K. Nandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling possible reconstructions of undersampled acquisitions in MR imaging. (arXiv:2010.00042v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.00042","description":"<p>Undersampling the k-space during MR acquisitions saves time, however results\nin an ill-posed inversion problem, leading to an infinite set of images as\npossible solutions. Traditionally, this is tackled as a reconstruction problem\nby searching for a single \"best\" image out of this solution set according to\nsome chosen regularization or prior. This approach, however, misses the\npossibility of other solutions and hence ignores the uncertainty in the\ninversion process. In this paper, we propose a method that instead returns\nmultiple images which are possible under the acquisition model and the chosen\nprior to capture the uncertainty in the inversion process. To this end, we\nintroduce a low dimensional latent space and model the posterior distribution\nof the latent vectors given the acquisition data in k-space, from which we can\nsample in the latent space and obtain the corresponding images. We use a\nvariational autoencoder for the latent model and the Metropolis adjusted\nLangevin algorithm for the sampling. We evaluate our method on two datasets;\nwith images from the Human Connectome Project and in-house measured multi-coil\nimages. We compare to five alternative methods. Results indicate that the\nproposed method produces images that match the measured k-space data better\nthan the alternatives, while showing realistic structural variability.\nFurthermore, in contrast to the compared methods, the proposed method yields\nhigher uncertainty in the undersampled phase encoding direction, as expected.\n</p>\n<p>Keywords: Magnetic Resonance image reconstruction, uncertainty estimation,\ninverse problems, sampling, MCMC, deep learning, unsupervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tezcan_K/0/1/0/all/0/1\">Kerem C. Tezcan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karani_N/0/1/0/all/0/1\">Neerav Karani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumgartner_C/0/1/0/all/0/1\">Christian F. Baumgartner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble and Random Collaborative Representation-Based Anomaly Detector for Hyperspectral Imagery. (arXiv:2101.01976v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.01976","description":"<p>In recent years, hyperspectral anomaly detection (HAD) has become an active\ntopic and plays a significant role in military and civilian fields. As a\nclassic HAD method, the collaboration representation-based detector (CRD) has\nattracted extensive attention and in-depth research. Despite the good\nperformance of the CRD method, its computational cost mainly arising from the\nsliding dual window strategy is too high for wide applications. Moreover, it\ntakes multiple repeated tests to determine the size of the dual window, which\nneeds to be reset once the dataset changes and cannot be identified in advance\nwith prior knowledge. To alleviate this problem, we proposed a novel ensemble\nand random collaborative representation-based detector (ERCRD) for HAD, which\ncomprises two closely related stages. Firstly, we process the random\nsub-sampling on CRD (RCRD) to gain several detection results instead of the\nsliding dual window strategy, which significantly reduces the computational\ncomplexity and makes it more feasible in practical applications. Secondly,\nensemble learning is employed to refine the multiple results of RCRD, which act\nas various \"experts\" providing abundant complementary information to better\ntarget different anomalies. Such two stages form an organic and theoretical\ndetector, which can not only improve the accuracy and stability of HAD methods\nbut also enhance its generalization ability. Experiments on four real\nhyperspectral datasets exhibit the accuracy and efficiency of this proposed\nERCRD method compared with ten state-of-the-art HAD methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yihang Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianrong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nie_F/0/1/0/all/0/1\">Feiping Nie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Open Domain Adaptation for Sketch-to-Photo Synthesis. (arXiv:2104.05703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05703","description":"<p>In this paper, we explore open-domain sketch-to-photo translation, which aims\nto synthesize a realistic photo from a freehand sketch with its class label,\neven if the sketches of that class are missing in the training data. It is\nchallenging due to the lack of training supervision and the large geometric\ndistortion between the freehand sketch and photo domains. To synthesize the\nabsent freehand sketches from photos, we propose a framework that jointly\nlearns sketch-to-photo and photo-to-sketch generation. However, the generator\ntrained from fake sketches might lead to unsatisfying results when dealing with\nsketches of missing classes, due to the domain gap between synthesized sketches\nand real ones. To alleviate this issue, we further propose a simple yet\neffective open-domain sampling and optimization strategy to \"fool\" the\ngenerator into treating fake sketches as real ones. Our method takes advantage\nof the learned sketch-to-photo and photo-to-sketch mapping of in-domain data\nand generalizes it to the open-domain classes. We validate our method on the\nScribble and SketchyCOCO datasets. Compared with the recent competing methods,\nour approach shows impressive results in synthesizing realistic color, texture,\nand maintaining the geometric composition for various categories of open-domain\nsketches. Our code is available at https://github.com/Mukosame/AODA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiaoyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Ding Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaohui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allebach_J/0/1/0/all/0/1\">Jan P. Allebach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Log-Determinant Divergences for Positive Definite Matrices. (arXiv:2104.06461v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.06461","description":"<p>Representations in the form of Symmetric Positive Definite (SPD) matrices\nhave been popularized in a variety of visual learning applications due to their\ndemonstrated ability to capture rich second-order statistics of visual data.\nThere exist several similarity measures for comparing SPD matrices with\ndocumented benefits. However, selecting an appropriate measure for a given\nproblem remains a challenge and in most cases, is the result of a\ntrial-and-error process. In this paper, we propose to learn similarity measures\nin a data-driven manner. To this end, we capitalize on the \\alpha\\beta-log-det\ndivergence, which is a meta-divergence parametrized by scalars \\alpha and\n\\beta, subsuming a wide family of popular information divergences on SPD\nmatrices for distinct and discrete values of these parameters. Our key idea is\nto cast these parameters in a continuum and learn them from data. We\nsystematically extend this idea to learn vector-valued parameters, thereby\nincreasing the expressiveness of the underlying non-linear measure. We conjoin\nthe divergence learning problem with several standard tasks in machine\nlearning, including supervised discriminative dictionary learning and\nunsupervised SPD matrix clustering. We present Riemannian gradient descent\nschemes for optimizing our formulations efficiently, and show the usefulness of\nour method on eight standard computer vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanitsas_P/0/1/0/all/0/1\">Panagiotis Stanitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morellas_V/0/1/0/all/0/1\">Vassilios Morellas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papanikolopoulos_N/0/1/0/all/0/1\">Nikolaos Papanikolopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming the Distance Estimation Bottleneck in Estimating Animal Abundance with Camera Traps. (arXiv:2105.04244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04244","description":"<p>The biodiversity crisis is still accelerating, despite increasing efforts by\nthe international community. Estimating animal abundance is of critical\nimportance to assess, for example, the consequences of land-use change and\ninvasive species on community composition, or the effectiveness of conservation\ninterventions. Various approaches have been developed to estimate abundance of\nunmarked animal populations. Whereas these approaches differ in methodological\ndetails, they all require the estimation of the effective area surveyed in\nfront of a camera trap. Until now camera-to-animal distance measurements are\nderived by laborious, manual and subjective estimation methods. To overcome\nthis distance estimation bottleneck, this study proposes an automatized\npipeline utilizing monocular depth estimation and depth image calibration\nmethods. We are able to reduce the manual effort required by a factor greater\nthan 21 and provide our system at\nhttps://timm.haucke.xyz/publications/distance-estimation-animal-abundance\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haucke_T/0/1/0/all/0/1\">Timm Haucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_H/0/1/0/all/0/1\">Hjalmar S. K&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_J/0/1/0/all/0/1\">Jacqueline Hoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhage_V/0/1/0/all/0/1\">Volker Steinhage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHD360: A Benchmark Dataset for Salient Human Detection in 360{\\deg} Videos. (arXiv:2105.11578v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11578","description":"<p>Salient human detection (SHD) in dynamic 360{\\deg} immersive videos is of\ngreat importance for various applications such as robotics, inter-human and\nhuman-object interaction in augmented reality. However, 360{\\deg} video SHD has\nbeen seldom discussed in the computer vision community due to a lack of\ndatasets with large-scale omnidirectional videos and rich annotations. To this\nend, we propose SHD360, the first 360{\\deg} video SHD dataset which contains\nvarious real-life daily scenes. Since so far there is no method proposed for\n360{\\deg} image/video SHD, we systematically benchmark 11 representative\nstate-of-the-art salient object detection (SOD) approaches on our SHD360, and\nexplore key issues derived from extensive experimenting results. We hope our\nproposed dataset and benchmark could serve as a good starting point for\nadvancing human-centric researches towards 360{\\deg} panoramic data. The\ndataset is available at https://github.com/PanoAsh/SHD360.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination. (arXiv:2106.01970v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01970","description":"<p>We address the problem of recovering the shape and spatially-varying\nreflectance of an object from multi-view images (and their camera poses) of an\nobject illuminated by one unknown lighting condition. This enables the\nrendering of novel views of the object under arbitrary environment lighting and\nediting of the object's material properties. The key to our approach, which we\ncall Neural Radiance Factorization (NeRFactor), is to distill the volumetric\ngeometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020]\nrepresentation of the object into a surface representation and then jointly\nrefine the geometry while solving for the spatially-varying reflectance and\nenvironment lighting. Specifically, NeRFactor recovers 3D neural fields of\nsurface normals, light visibility, albedo, and Bidirectional Reflectance\nDistribution Functions (BRDFs) without any supervision, using only a\nre-rendering loss, simple smoothness priors, and a data-driven BRDF prior\nlearned from real-world BRDF measurements. By explicitly modeling light\nvisibility, NeRFactor is able to separate shadows from albedo and synthesize\nrealistic soft or hard shadows under arbitrary lighting conditions. NeRFactor\nis able to recover convincing 3D models for free-viewpoint relighting in this\nchallenging and underconstrained capture setup for both synthetic and real\nscenes. Qualitative and quantitative experiments show that NeRFactor\noutperforms classic and deep learning-based state of the art across various\ntasks. Our videos, code, and data are available at\npeople.csail.mit.edu/xiuming/projects/nerfactor/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Boyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debevec_P/0/1/0/all/0/1\">Paul Debevec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Short-Term Transformer for Online Action Detection. (arXiv:2107.03377v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03377","description":"<p>We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm\nfor online action detection, which employs a long- and short-term memory\nmechanism to model prolonged sequence data. It consists of an LSTR encoder that\ndynamically leverages coarse-scale historical information from an extended\ntemporal window (e.g., 2048 frames spanning of up to 8 minutes), together with\nan LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8\nseconds) to model the fine-scale characteristics of the data. Compared to prior\nwork, LSTR provides an effective and efficient method to model long videos with\nfewer heuristics, which is validated by extensive empirical analysis. LSTR\nachieves state-of-the-art performance on three standard online action detection\nbenchmarks, THUMOS'14, TVSeries, and HACS Segment. Code has been made available\nat: https://xumingze0308.github.io/projects/lstr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v2 [math.NA] UPDATED)","link":"http://arxiv.org/abs/2108.04327","description":"<p>Natural numerical networks are introduced as a new classification algorithm\nbased on the numerical solution of nonlinear partial differential equations of\nforward-backward diffusion type on complete graphs. The proposed natural\nnumerical network is applied to open important environmental and nature\nconservation task, the automated identification of protected habitats by using\nsatellite images. In the natural numerical network, the forward diffusion\ncauses the movement of points in a feature space toward each other. The\nopposite effect, keeping the points away from each other, is caused by backward\ndiffusion. This yields the desired classification. The natural numerical\nnetwork contains a few parameters that are optimized in the learning phase of\nthe method. After learning parameters and optimizing the topology of the\nnetwork graph, classification necessary for habitat identification is\nperformed. A relevancy map for each habitat is introduced as a tool for\nvalidating the classification and finding new Natura 2000 habitat appearances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1\">Karol Mikula</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1\">Michal Kollar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1\">Aneta A. Ozvat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1\">Martin Ambroz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1\">Lucia Cahojova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1\">Ivan Jarolimek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1\">Jozef Sibik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1\">Maria Sibikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.05265","description":"<p>Stereoscopy exposits a natural perception of distance in a scene, and its\nmanifestation in 3D world understanding is an intuitive phenomenon. However, an\ninnate rigid calibration of binocular vision sensors is crucial for accurate\ndepth estimation. Alternatively, a monocular camera alleviates the limitation\nat the expense of accuracy in estimating depth, and the challenge exacerbates\nin harsh environmental conditions. Moreover, an optical sensor often fails to\nacquire vital signals in harsh environments, and radar is used instead, which\ngives coarse but more accurate signals. This work explores the utility of\ncoarse signals from radar when fused with fine-grained data from a monocular\ncamera for depth estimation in harsh environmental conditions. A variant of\nfeature pyramid network (FPN) extensively operates on fine-grained image\nfeatures at multiple scales with a fewer number of parameters. FPN feature maps\nare fused with sparse radar features extracted with a Convolutional neural\nnetwork. The concatenated hierarchical features are used to predict the depth\nwith ordinal regression. We performed experiments on the nuScenes dataset, and\nthe proposed architecture stays on top in quantitative evaluations with reduced\nparameters and faster inference. The depth estimation results suggest that the\nproposed techniques can be used as an alternative to stereo depth estimation in\ncritical applications in robotics and self-driving cars. The source code will\nbe available in the following: \\url{https://github.com/MI-Hussain/RVMDE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1\">Muhamamd Ishfaq Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_M/0/1/0/all/0/1\">Muhammad Aasim Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis. (arXiv:2110.06803v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06803","description":"<p>Limited availability of large image datasets is a major issue in the\ndevelopment of accurate and generalizable machine learning methods in medicine.\nThe limitations in the amount of data are mainly due to the use of different\nacquisition protocols, different hardware, and data privacy. At the same time,\ntraining a classification model on a small dataset leads to a poor\ngeneralization quality of the model. To overcome this issue, a combination of\nvarious image datasets of different provenance is often used, e.g., multi-site\nstudies. However, if an additional dataset does not include all classes of the\ntask, the learning of the classification model can be biased to the device or\nplace of acquisition.\n</p>\n<p>This is especially the case for Magnetic Resonance (MR) images, where\ndifferent MR scanners introduce a bias that limits the performance of the\nmodel. In this paper, we present a novel method that learns to ignore the\nscanner-related features present in the images, while learning features\nrelevant for the classification task. We focus on a real-world scenario, where\nonly a small dataset provides images of all classes. We exploit this\ncircumstance by introducing specific additional constraints on the latent\nspace, which lead the focus on disease-related rather than scanner-specific\nfeatures. Our method Learn to Ignore outperforms state-of-the-art domain\nadaptation methods on a multi-site MRI dataset on a classification task between\nMultiple Sclerosis patients and healthy subjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolleb_J/0/1/0/all/0/1\">Julia Wolleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandkuhler_R/0/1/0/all/0/1\">Robin Sandk&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Florentin Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barakovic_M/0/1/0/all/0/1\">Muhamed Barakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulou_A/0/1/0/all/0/1\">Athina Papadopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadjikhani_N/0/1/0/all/0/1\">Nouchine Hadjikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaldizli_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;r Yaldizli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhle_J/0/1/0/all/0/1\">Jens Kuhle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granziera_C/0/1/0/all/0/1\">Cristina Granziera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattin_P/0/1/0/all/0/1\">Philippe C. Cattin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithmic encoding of protected characteristics and its implications on performance disparities. (arXiv:2110.14755v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.14755","description":"<p>It has been rightfully emphasized that the use of AI for clinical decision\nmaking could amplify health disparities. A machine learning model may pick up\nundesirable correlations, for example, between a patient's racial identity and\nclinical outcome. Such correlations are often present in (historical) data used\nfor model development. There has been an increase in studies reporting biases\nin disease detection models. Besides the scarcity of data from underserved\npopulations, very little is known about how these biases are encoded and how\none may reduce or even remove disparate performance. There are concerns that an\nalgorithm may recognize patient characteristics such as biological sex or\nracial identity, and then directly or indirectly use this information when\nmaking predictions. But it remains unclear how we can establish whether such\ninformation is actually used. This article aims to shed some light on these\nissues by exploring methodology allowing intuitive inspections of the inner\nworking of machine learning models for image-based detection of disease. We\nalso investigate how to address performance disparities and find automatic\nthreshold selection to be an effective yet questionable technique, resulting in\nmodels with comparable true and false positive rates across subgroups. Our\nfindings call for further research to better understand the underlying causes\nof performance disparities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Charles Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing a Novel Approach for Periapical Dental Radiographs Segmentation. (arXiv:2111.07156v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07156","description":"<p>Image processing techniques has been widely used in dental researches such as\nhuman identification and forensic dentistry, teeth numbering, dental carries\ndetection and periodontal disease analysis. One of the most challenging parts\nin dental imaging is teeth segmentation and how to separate them from each\nother. In this paper, an automated method for teeth segmentation of Periapical\ndental x-ray images which contain at least one root-canalled tooth is proposed.\nThe result of this approach can be used as an initial step in bone lesion\ndetection. The proposed algorithm is made of two stages. The first stage is\npre-processing. The second and main part of this algorithm calculated rotation\ndegree and uses the integral projection method for tooth isolation.\nExperimental results show that this algorithm is robust and achieves high\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hatamimajoumerd_E/0/1/0/all/0/1\">Elaheh Hatamimajoumerd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajeripour_F/0/1/0/all/0/1\">Farshad Tajeripour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representing Prior Knowledge Using Randomly, Weighted Feature Networks for Visual Relationship Detection. (arXiv:2111.10686v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10686","description":"<p>The single-hidden-layer Randomly Weighted Feature Network (RWFN) introduced\nby Hong and Pavlic (2021) was developed as an alternative to neural tensor\nnetwork approaches for relational learning tasks. Its relatively small\nfootprint combined with the use of two randomized input projections -- an\ninsect-brain-inspired input representation and random Fourier features -- allow\nit to achieve rich expressiveness for relational learning with relatively low\ntraining cost. In particular, when Hong and Pavlic compared RWFN to Logic\nTensor Networks (LTNs) for Semantic Image Interpretation (SII) tasks to extract\nstructured semantic descriptions from images, they showed that the RWFN\nintegration of the two hidden, randomized representations better captures\nrelationships among inputs with a faster training process even though it uses\nfar fewer learnable parameters. In this paper, we use RWFNs to perform Visual\nRelationship Detection (VRD) tasks, which are more challenging SII tasks. A\nzero-shot learning approach is used with RWFN that can exploit similarities\nwith other seen relationships and background knowledge -- expressed with\nlogical constraints between subjects, relations, and objects -- to achieve the\nability to predict triples that do not appear in the training set. The\nexperiments on the Visual Relationship Dataset to compare the performance\nbetween RWFNs and LTNs, one of the leading Statistical Relational Learning\nframeworks, show that RWFNs outperform LTNs for the predicate-detection task\nwhile using fewer number of adaptable parameters (1:56 ratio). Furthermore,\nbackground knowledge represented by RWFNs can be used to alleviate the\nincompleteness of training sets even though the space complexity of RWFNs is\nmuch smaller than LTNs (1:27 ratio).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jinyung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Networks with Conditional Neural Movement Primitives for An Interactive Generative Drawing Tool. (arXiv:2111.14934v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2111.14934","description":"<p>Sketches are abstract representations of visual perception and visuospatial\nconstruction. In this work, we proposed a new framework, Generative Adversarial\nNetworks with Conditional Neural Movement Primitives (GAN-CNMP), that\nincorporates a novel adversarial loss on CNMP to increase sketch smoothness and\nconsistency. Through the experiments, we show that our model can be trained\nwith few unlabeled samples, can construct distributions automatically in the\nlatent space, and produces better results than the base model in terms of shape\nconsistency and smoothness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ada_S/0/1/0/all/0/1\">Suzan Ece Ada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1\">M. Yunus Seker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiPath++: Efficient Information Fusion and Trajectory Aggregation for Behavior Prediction. (arXiv:2111.14973v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14973","description":"<p>Predicting the future behavior of road users is one of the most challenging\nand important problems in autonomous driving. Applying deep learning to this\nproblem requires fusing heterogeneous world state in the form of rich\nperception signals and map information, and inferring highly multi-modal\ndistributions over possible futures. In this paper, we present MultiPath++, a\nfuture prediction model that achieves state-of-the-art performance on popular\nbenchmarks. MultiPath++ improves the MultiPath architecture by revisiting many\ndesign choices. The first key design difference is a departure from dense\nimage-based encoding of the input world state in favor of a sparse encoding of\nheterogeneous scene elements: MultiPath++ consumes compact and efficient\npolylines to describe road features, and raw agent state information directly\n(e.g., position, velocity, acceleration). We propose a context-aware fusion of\nthese elements and develop a reusable multi-context gating fusion component.\nSecond, we reconsider the choice of pre-defined, static anchors, and develop a\nway to learn latent anchor embeddings end-to-end in the model. Lastly, we\nexplore ensembling and output aggregation techniques -- common in other ML\ndomains -- and find effective variants for our probabilistic multimodal output\nrepresentation. We perform an extensive ablation on these design choices, and\nshow that our proposed model achieves state-of-the-art performance on the\nArgoverse Motion Forecasting Competition and the Waymo Open Dataset Motion\nPrediction Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varadarajan_B/0/1/0/all/0/1\">Balakrishnan Varadarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hefny_A/0/1/0/all/0/1\">Ahmed Hefny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Avikalp Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Refaat_K/0/1/0/all/0/1\">Khaled S. Refaat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayakanti_N/0/1/0/all/0/1\">Nigamaa Nayakanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornman_A/0/1/0/all/0/1\">Andre Cornman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douillard_B/0/1/0/all/0/1\">Bertrand Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_C/0/1/0/all/0/1\">Chi Pang Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Benjamin Sapp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superpixel-Based Building Damage Detection from Post-earthquake Very High Resolution Imagery Using Deep Neural Networks. (arXiv:2112.04744v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04744","description":"<p>Building damage detection after natural disasters like earthquakes is crucial\nfor initiating effective emergency response actions. Remotely sensed very high\nspatial resolution (VHR) imagery can provide vital information due to their\nability to map the affected buildings with high geometric precision. Many\napproaches have been developed to detect damaged buildings due to earthquakes.\nHowever, little attention has been paid to exploiting rich features represented\nin VHR images using Deep Neural Networks (DNN). This paper presents a novel\nsuperpixel based approach combining DNN and a modified segmentation method, to\ndetect damaged buildings from VHR imagery. Firstly, a modified Fast Scanning\nand Adaptive Merging method is extended to create initial over-segmentation.\nSecondly, the segments are merged based on the Region Adjacent Graph (RAG),\nconsidered an improved semantic similarity criterion composed of Local Binary\nPatterns (LBP) texture, spectral, and shape features. Thirdly, a pre-trained\nDNN using Stacked Denoising Auto-Encoders called SDAE-DNN is presented, to\nexploit the rich semantic features for building damage detection. Deep-layer\nfeature abstraction of SDAE-DNN could boost detection accuracy through learning\nmore intrinsic and discriminative features, which outperformed other methods\nusing state-of-the-art alternative classifiers. We demonstrate the feasibility\nand effectiveness of our method using a subset of WorldView-2 imagery, in the\ncomplex urban areas of Bhaktapur, Nepal, which was affected by the Nepal\nEarthquake of April 25, 2015.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yixuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Q/0/1/0/all/0/1\">Qiming Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Shifted Attention With Early Global Integration. (arXiv:2112.05080v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05080","description":"<p>Recent work has shown the potential of transformers for computer vision\napplications. An image is first partitioned into patches, which are then used\nas input tokens for the attention mechanism. Due to the expensive quadratic\ncost of the attention mechanism, either a large patch size is used, resulting\nin coarse-grained global interactions, or alternatively, attention is applied\nonly on a local region of the image, at the expense of long-range interactions.\nIn this work, we propose an approach that allows for both coarse global\ninteractions and fine-grained local interactions already at early layers of a\nvision transformer.\n</p>\n<p>At the core of our method is the application of local and global attention\nlayers. In the local attention layer, we apply attention to each patch and its\nlocal shifts, resulting in virtually located local patches, which are not bound\nto a single, specific location. These virtually located patches are then used\nin a global attention layer. The separation of the attention layer into local\nand global counterparts allows for a low computational cost in the number of\npatches, while still supporting data-dependent localization already at the\nfirst layer, as opposed to the static positioning in other visual transformers.\nOur method is shown to be superior to both convolutional and transformer-based\nmethods for image classification on CIFAR10, CIFAR100, and ImageNet. Code is\navailable at: https://github.com/shellysheynin/Locally-SAG-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheynin_S/0/1/0/all/0/1\">Shelly Sheynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Minimal Misalignment at Minimal Cost in One-Stage and Anchor-Free Object Detection. (arXiv:2112.08902v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08902","description":"<p>Common object detection models consist of classification and regression\nbranches, due to different task drivers, these two branches have different\nsensibility to the features from the same scale level and the same spatial\nlocation. The point-based prediction method, which is based on the assumption\nthat the high classification confidence point has the high regression quality,\nleads to the misalignment problem. Our analysis shows, the problem is further\ncomposed of scale misalignment and spatial misalignment specifically. We aim to\nresolve the phenomenon at minimal cost: a minor adjustment of the head network\nand a new label assignment method replacing the rigid one. Our experiments show\nthat, compared to the baseline FCOS, a one-stage and anchor-free object\ndetection model, our model consistently get around 3 AP improvement with\ndifferent backbones, demonstrating both simplicity and efficiency of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shuaizheng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongzhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ningwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Animation with Keypoint Mask. (arXiv:2112.10457v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10457","description":"<p>Motion transfer is the task of synthesizing future video frames of a single\nsource image according to the motion from a given driving video. In order to\nsolve it, we face the challenging complexity of motion representation and the\nunknown relations between the driving video and the source image. Despite its\ndifficulty, this problem attracted great interests from researches at the\nrecent years, with gradual improvements. The goal is often thought as the\ndecoupling of motion and appearance, which is may be solved by extracting the\nmotion from keypoint movement. We chose to tackle the generic, unsupervised\nsetting, where we need to apply animation to any arbitrary object, without any\ndomain specific model for the structure of the input. In this work, we extract\nthe structure from a keypoint heatmap, without an explicit motion\nrepresentation. Then, the structures from the image and the video are extracted\nto warp the image according to the video, by a deep generator. We suggest two\nvariants of the structure from different steps in the keypoint module, and show\nsuperior qualitative pose and quantitative scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toledano_O/0/1/0/all/0/1\">Or Toledano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marmor_Y/0/1/0/all/0/1\">Yanir Marmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_D/0/1/0/all/0/1\">Dov Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-user Oriented Live Free-viewpoint Video Streaming System Based On View Interpolation. (arXiv:2112.10603v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2112.10603","description":"<p>As an important application form of immersive multimedia services,\nfree-viewpoint video(FVV) enables users with great immersive experience by\nstrong interaction. However, the computational complexity of virtual view\nsynthesis algorithms poses a significant challenge to the real-time performance\nof an FVV system. Furthermore, the individuality of user interaction makes it\ndifficult to serve multiple users simultaneously for a system with conventional\narchitecture. In this paper, we novelly introduce a CNN-based view\ninterpolation algorithm to synthesis dense virtual views in real time. Based on\nthis, we also build an end-to-end live free-viewpoint system with a multi-user\noriented streaming strategy. Our system can utilize a single edge server to\nserve multiple users at the same time without having to bring a large view\nsynthesis load on the client side. We analyze the whole system and show that\nour approaches give the user a pleasant immersive experience, in terms of both\nvisual quality and latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingchuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Li Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. (arXiv:2112.10741v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10741","description":"<p>Diffusion models have recently been shown to generate high-quality synthetic\nimages, especially when paired with a guidance technique to trade off diversity\nfor fidelity. We explore diffusion models for the problem of text-conditional\nimage synthesis and compare two different guidance strategies: CLIP guidance\nand classifier-free guidance. We find that the latter is preferred by human\nevaluators for both photorealism and caption similarity, and often produces\nphotorealistic samples. Samples from a 3.5 billion parameter text-conditional\ndiffusion model using classifier-free guidance are favored by human evaluators\nto those from DALL-E, even when the latter uses expensive CLIP reranking.\nAdditionally, we find that our models can be fine-tuned to perform image\ninpainting, enabling powerful text-driven image editing. We train a smaller\nmodel on a filtered dataset and release the code and weights at\nhttps://github.com/openai/glide-text2im.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1\">Alex Nichol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhariwal_P/0/1/0/all/0/1\">Prafulla Dhariwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_A/0/1/0/all/0/1\">Aditya Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shyam_P/0/1/0/all/0/1\">Pranav Shyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1\">Pamela Mishkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGrew_B/0/1/0/all/0/1\">Bob McGrew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutskever_I/0/1/0/all/0/1\">Ilya Sutskever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mark Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Semantic Transfer for Multi-Label Recognition with Partial Labels. (arXiv:2112.10941v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10941","description":"<p>Multi-label image recognition is a fundamental yet practical task because\nreal-world images inherently possess multiple semantic labels. However, it is\ndifficult to collect large-scale multi-label annotations due to the complexity\nof both the input images and output label spaces. To reduce the annotation\ncost, we propose a structured semantic transfer (SST) framework that enables\ntraining multi-label recognition models with partial labels, i.e., merely some\nlabels are known while other labels are missing (also called unknown labels)\nper image. The framework consists of two complementary transfer modules that\nexplore within-image and cross-image semantic correlations to transfer\nknowledge of known labels to generate pseudo labels for unknown labels.\nSpecifically, an intra-image semantic transfer module learns image-specific\nlabel co-occurrence matrix and maps the known labels to complement unknown\nlabels based on this matrix. Meanwhile, a cross-image transfer module learns\ncategory-specific feature similarities and helps complement unknown labels with\nhigh similarities. Finally, both known and generated labels are used to train\nthe multi-label recognition models. Extensive experiments on the Microsoft\nCOCO, Visual Genome and Pascal VOC datasets show that the proposed SST\nframework obtains superior performance over current state-of-the-art\nalgorithms. Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1\">Tao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hefeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Interactive Backpropagating Refinement for Dense Prediction. (arXiv:2112.10969v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10969","description":"<p>As deep neural networks become the state-of-the-art approach in the field of\ncomputer vision for dense prediction tasks, many methods have been developed\nfor automatic estimation of the target outputs given the visual inputs.\nAlthough the estimation accuracy of the proposed automatic methods continues to\nimprove, interactive refinement is oftentimes necessary for further correction.\nRecently, feature backpropagating refinement scheme (f-BRS) has been proposed\nfor the task of interactive segmentation, which enables efficient optimization\nof a small set of auxiliary variables inserted into the pretrained network to\nproduce object segmentation that better aligns with user inputs. However, the\nproposed auxiliary variables only contain channel-wise scale and bias, limiting\nthe optimization to global refinement only. In this work, in order to\ngeneralize backpropagating refinement for a wide range of dense prediction\ntasks, we introduce a set of G-BRS (Generalized Backpropagating Refinement\nScheme) layers that enable both global and localized refinement for the\nfollowing tasks: interactive segmentation, semantic segmentation, image matting\nand monocular depth estimation. Experiments on SBD, Cityscapes, Mapillary\nVista, Composition-1k and NYU-Depth-V2 show that our method can successfully\ngeneralize and significantly improve performance of existing pretrained\nstate-of-the-art models with only a few clicks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fanqing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_T/0/1/0/all/0/1\">Tony Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projected Sliced Wasserstein Autoencoder-based Hyperspectral Images Anomaly Detection. (arXiv:2112.11243v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11243","description":"<p>Anomaly detection (AD) has been an active research area in various domains.\nYet, the increasing data scale, complexity, and dimension turn the traditional\nmethods into challenging. Recently, the deep generative model, such as the\nvariational autoencoder (VAE), has sparked a renewed interest in the AD\nproblem. However, the probability distribution divergence used as the\nregularization is too strong, which causes the model cannot capture the\nmanifold of the true data. In this paper, we propose the Projected Sliced\nWasserstein (PSW) autoencoder-based anomaly detection method. Rooted in the\noptimal transportation, the PSW distance is a weaker distribution measure\ncompared with $f$-divergence. In particular, the computation-friendly\neigen-decomposition method is leveraged to find the principal component for\nslicing the high-dimensional data. In this case, the Wasserstein distance can\nbe calculated with the closed-form, even the prior distribution is not\nGaussian. Comprehensive experiments conducted on various real-world\nhyperspectral anomaly detection benchmarks demonstrate the superior performance\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Q. M. Jonathan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yimin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Fidelity Point Cloud Completion with Low-Resolution Recovery and Noise-Aware Upsampling. (arXiv:2112.11271v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11271","description":"<p>Completing an unordered partial point cloud is a challenging task. Existing\napproaches that rely on decoding a latent feature to recover the complete\nshape, often lead to the completed point cloud being over-smoothing, losing\ndetails, and noisy. Instead of decoding a whole shape, we propose to decode and\nrefine a low-resolution (low-res) point cloud first, and then performs a\npatch-wise noise-aware upsampling rather than interpolating the whole sparse\npoint cloud at once, which tends to lose details. Regarding the possibility of\nlacking details of the initially decoded low-res point cloud, we propose an\niterative refinement to recover the geometric details and a symmetrization\nprocess to preserve the trustworthy information from the input partial point\ncloud. After obtaining a sparse and complete point cloud, we propose a\npatch-wise upsampling strategy. Patch-based upsampling allows to better recover\nfine details unlike decoding a whole shape, however, the existing upsampling\nmethods are not applicable to completion task due to the data discrepancy\n(i.e., input sparse data here is not from ground-truth). Therefore, we propose\na patch extraction approach to generate training patch pairs between the sparse\nand ground-truth point clouds, and an outlier removal step to suppress the\nnoisy points from the sparse point cloud. Together with the low-res recovery,\nour whole method is able to achieve high-fidelity point cloud completion.\nComprehensive evaluations are provided to demonstrate the effectiveness of the\nproposed method and its individual components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren-Wu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Ling-Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrimSeq: a deep learning-based pipeline to quantitate rehabilitation training. (arXiv:2112.11330v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.11330","description":"<p>Stroke rehabilitation seeks to increase neuroplasticity through the repeated\npractice of functional motions, but may have minimal impact on recovery because\nof insufficient repetitions. The optimal training content and quantity are\ncurrently unknown because no practical tools exist to measure them. Here, we\npresent PrimSeq, a pipeline to classify and count functional motions trained in\nstroke rehabilitation. Our approach integrates wearable sensors to capture\nupper-body motion, a deep learning model to predict motion sequences, and an\nalgorithm to tally motions. The trained model accurately decomposes\nrehabilitation activities into component functional motions, outperforming\ncompetitive machine learning methods. PrimSeq furthermore quantifies these\nmotions at a fraction of the time and labor costs of human experts. We\ndemonstrate the capabilities of PrimSeq in previously unseen stroke patients\nwith a range of upper extremity motor impairment. We expect that these advances\nwill support the rigorous measurement required for quantitative dosing trials\nin stroke rehabilitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parnandi_A/0/1/0/all/0/1\">Avinash Parnandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaku_A/0/1/0/all/0/1\">Aakash Kaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesan_A/0/1/0/all/0/1\">Anita Venkatesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandit_N/0/1/0/all/0/1\">Natasha Pandit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirtanen_A/0/1/0/all/0/1\">Audre Wirtanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajamohan_H/0/1/0/all/0/1\">Haresh Rajamohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_K/0/1/0/all/0/1\">Kannan Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilsen_D/0/1/0/all/0/1\">Dawn Nilsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schambra_H/0/1/0/all/0/1\">Heidi Schambra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Based 3D Point Cloud Regression for Estimating Forest Biomass. (arXiv:2112.11335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11335","description":"<p>Knowledge of forest biomass stocks and their development is important for\nimplementing effective climate change mitigation measures. It is needed for\nstudying the processes driving af-, re-, and deforestation and is a\nprerequisite for carbon-accounting. Remote sensing using airborne LiDAR can be\nused to measure vegetation biomass at large scale. We present deep learning\nsystems for predicting wood volume, above-ground biomass (AGB), and\nsubsequently carbon directly from 3D LiDAR point cloud data. We devise\ndifferent neural network architectures for point cloud regression and evaluate\nthem on remote sensing data of areas for which AGB estimates have been obtained\nfrom field measurements in a national forest inventory. Our adaptation of\nMinkowski convolutional neural networks for regression gave the best results.\nThe deep neural networks produced significantly more accurate wood volume, AGB,\nand carbon estimates compared to state-of-the-art approaches operating on basic\nstatistics of the point clouds, and we expect this finding to have a strong\nimpact on LiDAR-based analyses of terrestrial ecosystem dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oehmcke_S/0/1/0/all/0/1\">Stefan Oehmcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revenga_J/0/1/0/all/0/1\">Jaime Revenga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nord_Larsen_T/0/1/0/all/0/1\">Thomas Nord-Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trepekli_K/0/1/0/all/0/1\">Katerina Trepekli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gieseke_F/0/1/0/all/0/1\">Fabian Gieseke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. (arXiv:2112.11427v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2112.11427","description":"<p>We introduce a high resolution, 3D-consistent image and shape generation\ntechnique which we call StyleSDF. Our method is trained on single-view RGB data\nonly, and stands on the shoulders of StyleGAN2 for image generation, while\nsolving two main challenges in 3D-aware GANs: 1) high-resolution,\nview-consistent generation of the RGB images, and 2) detailed 3D shape. We\nachieve this by merging a SDF-based 3D representation with a style-based 2D\ngenerator. Our 3D implicit network renders low-resolution feature maps, from\nwhich the style-based network generates view-consistent, 1024x1024 images.\nNotably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to\nconsistent volume rendering. Our method shows higher quality results compared\nto state of the art in terms of visual and geometric quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Or_El_R/0/1/0/all/0/1\">Roy Or-El</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1\">Mengyi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeong Joon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemelmacher_Shlizerman_I/0/1/0/all/0/1\">Ira Kemelmacher-Shlizerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}