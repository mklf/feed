{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling. (arXiv:2205.12986v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12986","description":"<p>Sentence scoring aims at measuring the likelihood score of a sentence and is\nwidely used in many natural language processing scenarios, like reranking,\nwhich is to select the best sentence from multiple candidates. Previous works\non sentence scoring mainly adopted either causal language modeling (CLM) like\nGPT or masked language modeling (MLM) like BERT, which have some limitations:\n1) CLM only utilizes unidirectional information for the probability estimation\nof a sentence without considering bidirectional context, which affects the\nscoring quality; 2) MLM can only estimate the probability of partial tokens at\na time and thus requires multiple forward passes to estimate the probability of\nthe whole sentence, which incurs large computation and time cost. In this\npaper, we propose \\textit{Transcormer} -- a Transformer model with a novel\n\\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,\nour SLM adopts a triple-stream self-attention mechanism to estimate the\nprobability of all tokens in a sentence with bidirectional context and only\nrequires a single forward pass. SLM can avoid the limitations of CLM (only\nunidirectional context) and MLM (multiple forward passes) and inherit their\nadvantages, and thus achieve high effectiveness and efficiency in scoring.\nExperimental results on multiple tasks demonstrate that our method achieves\nbetter performance than other language modelings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiT: Robustly Binarized Multi-distilled Transformer. (arXiv:2205.13016v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13016","description":"<p>Modern pre-trained transformers have rapidly advanced the state-of-the-art in\nmachine learning, but have also grown in parameters and computational\ncomplexity, making them increasingly difficult to deploy in\nresource-constrained environments. Binarization of the weights and activations\nof the network can significantly alleviate these issues, however is technically\nchallenging from an optimization perspective. In this work, we identify a\nseries of improvements which enables binary transformers at a much higher\naccuracy than what was possible previously. These include a two-set\nbinarization scheme, a novel elastic binary activation function with learned\nparameters, and a method to quantize a network to its limit by successively\ndistilling higher precision models into lower precision students. These\napproaches allow for the first time, fully binarized transformer models that\nare at a practical level of accuracy, approaching a full-precision BERT\nbaseline on the GLUE language understanding benchmark within as little as 5.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappu_A/0/1/0/all/0/1\">Aasish Pappu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1\">Scott Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1\">Raghuraman Krishnamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV Conversion. (arXiv:2205.13108v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13108","description":"<p>We advance the state-of-the-art in unsupervised abstractive dialogue\nsummarization by utilizing multi-sentence compression graphs. Starting from\nwell-founded assumptions about word graphs, we present simple but reliable\npath-reranking and topic segmentation schemes. Robustness of our method is\ndemonstrated on datasets across multiple domains, including meetings,\ninterviews, movie scripts, and day-to-day conversations. We also identify\npossible avenues to augment our heuristic-based system with deep learning. We\nopen-source our code, to provide a strong, reproducible baseline for future\nresearch into unsupervised dialogue summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihwa Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13115","description":"<p>Modern image captioning models are usually trained with text similarity\nobjectives. However, since reference captions in public datasets often describe\nthe most salient common objects, models trained with text similarity objectives\ntend to ignore specific and detailed aspects of an image that distinguish it\nfrom others. Toward more descriptive and distinctive caption generation, we\npropose using CLIP, a multimodal encoder trained on huge image-text pairs from\nweb, to calculate multimodal similarity and use it as a reward function. We\nalso propose a simple finetuning strategy of the CLIP text encoder to improve\ngrammar that does not require extra text annotation. This completely eliminates\nthe need for reference captions during the reward computation. To\ncomprehensively evaluate descriptive captions, we introduce FineCapEval, a new\ndataset for caption evaluation with fine-grained criteria: overall, background,\nobject, relations. In our experiments on text-to-image retrieval and\nFineCapEval, the proposed CLIP-guided model generates more distinctive captions\nthan the CIDEr-optimized model. We also show that our unsupervised grammar\nfinetuning of the CLIP text encoder alleviates the degeneration problem of the\nnaive CLIP reward. Lastly, we show human analysis where the annotators strongly\nprefer the CLIP reward to the CIDEr and MLE objectives according to various\ncriteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_A/0/1/0/all/0/1\">Ajinkya Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Metrics for Paraphrasing. (arXiv:2205.13119v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13119","description":"<p>Paraphrase generation is a difficult problem. This is not only because of the\nlimitations in text generation capabilities but also due that to the lack of a\nproper definition of what qualifies as a paraphrase and corresponding metrics\nto measure how good it is. Metrics for evaluation of paraphrasing quality is an\non going research problem. Most of the existing metrics in use having been\nborrowed from other tasks do not capture the complete essence of a good\nparaphrase, and often fail at borderline-cases. In this work, we propose a\nnovel metric $ROUGE_P$ to measure the quality of paraphrases along the\ndimensions of adequacy, novelty and fluency. We also provide empirical evidence\nto show that the current natural language generation metrics are insufficient\nto measure these desired properties of a good paraphrase. We look at paraphrase\nmodel fine-tuning and generation from the lens of metrics to gain a deeper\nunderstanding of what it takes to generate and evaluate a good paraphrase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_O/0/1/0/all/0/1\">Omkar Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">Tarun Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Reinforcement Adaptation for Class-Imbalanced Text Classification. (arXiv:2205.13139v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13139","description":"<p>Class imbalance naturally exists when train and test models in different\ndomains. Unsupervised domain adaptation (UDA) augments model performance with\nonly accessible annotations from the source domain and unlabeled data from the\ntarget domain. However, existing state-of-the-art UDA models learn\ndomain-invariant representations and evaluate primarily on class-balanced data\nacross domains. In this work, we propose an unsupervised domain adaptation\napproach via reinforcement learning that jointly leverages feature variants and\nimbalanced labels across domains. We experiment with the text classification\ntask for its easily accessible datasets and compare the proposed method with\nfive baselines. Experiments on three datasets prove that our proposed method\ncan effectively learn robust domain-invariant representations and successfully\nadapt text classifiers on imbalanced classes over domains. The code is\navailable at https://github.com/woqingdoua/ImbalanceClass.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar Detection for Sentiment Analysis through Improved Viterbi Algorithm. (arXiv:2205.13148v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13148","description":"<p>Grammar Detection, also referred to as Parts of Speech Tagging of raw text,\nis considered an underlying building block of the various Natural Language\nProcessing pipelines like named entity recognition, question answering, and\nsentiment analysis. In short, forgiven a sentence, Parts of Speech tagging is\nthe task of specifying and tagging each word of a sentence with nouns, verbs,\nadjectives, adverbs, and more. Sentiment Analysis may well be a procedure\naccustomed to determining if a given sentence's emotional tone is neutral,\npositive or negative. To assign polarity scores to the thesis or entities\nwithin phrase, in-text analysis and analytics, machine learning and natural\nlanguage processing, approaches are incorporated. This Sentiment Analysis using\nPOS tagger helps us urge a summary of the broader public over a specific topic.\nFor this, we are using the Viterbi algorithm, Hidden Markov Model, Constraint\nbased Viterbi algorithm for POS tagging. By comparing the accuracies, we select\nthe foremost accurate result of the model for Sentiment Analysis for\ndetermining the character of the sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavali_S/0/1/0/all/0/1\">Surya Teja Chavali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandavalli_C/0/1/0/all/0/1\">Charan Tej Kandavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_S/0/1/0/all/0/1\">Sugash T M</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks. (arXiv:2205.13164v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13164","description":"<p>The last few years have witnessed an exponential rise in the propagation of\noffensive text on social media. Identification of this text with high precision\nis crucial for the well-being of society. Most of the existing approaches tend\nto give high toxicity scores to innocuous statements (e.g., \"I am a gay man\").\nThese false positives result from over-generalization on the training data\nwhere specific terms in the statement may have been used in a pejorative sense\n(e.g., \"gay\"). Emphasis on such words alone can lead to discrimination against\nthe classes these systems are designed to protect. In this paper, we address\nthe problem of offensive language detection on Twitter, while also detecting\nthe type and the target of the offence. We propose a novel approach called\nSyLSTM, which integrates syntactic features in the form of the dependency parse\ntree of a sentence and semantic features in the form of word embeddings into a\ndeep learning architecture using a Graph Convolutional Network. Results show\nthat the proposed approach significantly outperforms the state-of-the-art BERT\nmodel with orders of magnitude fewer number of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_D/0/1/0/all/0/1\">Divyam Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Raksha Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach. (arXiv:2205.13183v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13183","description":"<p>Pre-trained models (PTMs) have lead to great improvements in natural language\ngeneration (NLG). However, it is still unclear how much commonsense knowledge\nthey possess. With the goal of evaluating commonsense knowledge of NLG models,\nrecent work has proposed the problem of generative commonsense reasoning, e.g.,\nto compose a logical sentence given a set of unordered concepts. Existing\napproaches to this problem hypothesize that PTMs lack sufficient parametric\nknowledge for this task, which can be overcome by introducing external\nknowledge or task-specific pre-training objectives. Different from this trend,\nwe argue that PTM's inherent ability for generative commonsense reasoning is\nunderestimated due to the order-agnostic property of its input. In particular,\nwe hypothesize that the order of the input concepts can affect the PTM's\nability to utilize its commonsense knowledge. To this end, we propose a\npre-ordering approach to elaborately manipulate the order of the given concepts\nbefore generation. Experiments show that our approach can outperform the more\nsophisticated models that have access to a lot of external data and resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tenghao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions. (arXiv:2205.13190v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13190","description":"<p>Role-oriented dialogue summarization is to generate summaries for different\nroles in the dialogue, e.g., merchants and consumers. Existing methods handle\nthis task by summarizing each role's content separately and thus are prone to\nignore the information from other roles. However, we believe that other roles'\ncontent could benefit the quality of summaries, such as the omitted information\nmentioned by other roles. Therefore, we propose a novel role interaction\nenhanced method for role-oriented dialogue summarization. It adopts cross\nattention and decoder self-attention interactions to interactively acquire\nother roles' critical information. The cross attention interaction aims to\nselect other roles' critical dialogue utterances, while the decoder\nself-attention interaction aims to obtain key information from other roles'\nsummaries. Experimental results have shown that our proposed method\nsignificantly outperforms strong baselines on two public role-oriented dialogue\nsummarization datasets. Extensive analyses have demonstrated that other roles'\ncontent could help generate summaries with more complete semantics and correct\ntopic structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junnan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbiotic Child Emotional Support with Social Robots and Temporal Knowledge Graphs. (arXiv:2205.13229v1 [cs.RO])","link":"http://arxiv.org/abs/2205.13229","description":"<p>In current youth-care programs, children with needs (mental health, family\nissues, learning disabilities, and autism) receive support from youth and\nfamily experts as one-to-one assistance at schools or hospitals. Occasionally,\nsocial robots have featured in such settings as support roles in a one-to-one\ninteraction with the child. In this paper, we suggest the development of a\nsymbiotic framework for real-time Emotional Support (ES) with social robots\nKnowledge Graphs (KG). By augmenting a domain-specific corpus from the\nliterature on ES for children (between the age of 8 and 12) and providing\nscenario-driven context including the history of events, we suggest developing\nan experimental knowledge-aware ES framework. The framework both guides the\nsocial robot in providing ES statements to the child and assists the expert in\ntracking and interpreting the child's emotional state and related events over\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saccardi_I/0/1/0/all/0/1\">Isabella Saccardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islakoglu_D/0/1/0/all/0/1\">Duygu Sezen Islakoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neerincx_A/0/1/0/all/0/1\">Anouk Neerincx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinella_F/0/1/0/all/0/1\">Federica Lucia Vinella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Split BERT for Heterogeneous Text Classification. (arXiv:2205.13299v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13299","description":"<p>Pre-trained BERT models have achieved impressive performance in many natural\nlanguage processing (NLP) tasks. However, in many real-world situations,\ntextual data are usually decentralized over many clients and unable to be\nuploaded to a central server due to privacy protection and regulations.\nFederated learning (FL) enables multiple clients collaboratively to train a\nglobal model while keeping the local data privacy. A few researches have\ninvestigated BERT in federated learning setting, but the problem of performance\nloss caused by heterogeneous (e.g., non-IID) data over clients remain\nunder-explored. To address this issue, we propose a framework, FedSplitBERT,\nwhich handles heterogeneous data and decreases the communication cost by\nsplitting the BERT encoder layers into local part and global part. The local\npart parameters are trained by the local client only while the global part\nparameters are trained by aggregating gradients of multiple clients. Due to the\nsheer size of BERT, we explore a quantization method to further reduce the\ncommunication cost with minimal performance loss. Our framework is ready-to-use\nand compatible to many existing federated learning algorithms, including\nFedAvg, FedProx and FedAdam. Our experiments verify the effectiveness of the\nproposed framework, which outperforms baseline methods by a significant margin,\nwhile FedSplitBERT with quantization can reduce the communication cost by\n$11.9\\times$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Non-negative Matrix Factorization for Short Texts Topic Modeling with Mutual Information. (arXiv:2205.13300v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13300","description":"<p>Non-negative matrix factorization (NMF) based topic modeling is widely used\nin natural language processing (NLP) to uncover hidden topics of short text\ndocuments. Usually, training a high-quality topic model requires large amount\nof textual data. In many real-world scenarios, customer textual data should be\nprivate and sensitive, precluding uploading to data centers. This paper\nproposes a Federated NMF (FedNMF) framework, which allows multiple clients to\ncollaboratively train a high-quality NMF based topic model with locally stored\ndata. However, standard federated learning will significantly undermine the\nperformance of topic models in downstream tasks (e.g., text classification)\nwhen the data distribution over clients is heterogeneous. To alleviate this\nissue, we further propose FedNMF+MI, which simultaneously maximizes the mutual\ninformation (MI) between the count features of local texts and their topic\nweight vectors to mitigate the performance degradation. Experimental results\nshow that our FedNMF+MI methods outperform Federated Latent Dirichlet\nAllocation (FedLDA) and the FedNMF without MI methods for short texts by a\nsignificant margin on both coherence score and classification F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qinliang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target-aware Abstractive Related Work Generation with Contrastive Learning. (arXiv:2205.13339v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13339","description":"<p>The related work section is an important component of a scientific paper,\nwhich highlights the contribution of the target paper in the context of the\nreference papers. Authors can save their time and effort by using the\nautomatically generated related work section as a draft to complete the final\nrelated work. Most of the existing related work section generation methods rely\non extracting off-the-shelf sentences to make a comparative discussion about\nthe target work and the reference papers. However, such sentences need to be\nwritten in advance and are hard to obtain in practice. Hence, in this paper, we\npropose an abstractive target-aware related work generator (TAG), which can\ngenerate related work sections consisting of new sentences. Concretely, we\nfirst propose a target-aware graph encoder, which models the relationships\nbetween reference papers and the target paper with target-centered attention\nmechanisms. In the decoding process, we propose a hierarchical decoder that\nattends to the nodes of different levels in the graph with keyphrases as\nsemantic indicators. Finally, to generate a more informative related work, we\npropose multi-level contrastive optimization objectives, which aim to maximize\nthe mutual information between the generated related work with the references\nand minimize that with non-references. Extensive experiments on two public\nscholar datasets show that the proposed model brings substantial improvements\nover several strong baselines in terms of automatic and tailored human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamro_H/0/1/0/all/0/1\">Hind Alamro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Causal Inference for Explainable Automatic Program Repair. (arXiv:2205.13342v1 [cs.SE])","link":"http://arxiv.org/abs/2205.13342","description":"<p>Deep learning models have made significant progress in automatic program\nrepair. However, the black-box nature of these methods has restricted their\npractical applications. To address this challenge, this paper presents an\ninterpretable approach for program repair based on sequence-to-sequence models\nwith causal inference and our method is called CPR, short for causal program\nrepair. Our CPR can generate explanations in the process of decision making,\nwhich consists of groups of causally related input-output tokens. Firstly, our\nmethod infers these relations by querying the model with inputs disturbed by\ndata augmentation. Secondly, it generates a graph over tokens from the\nresponses and solves a partitioning problem to select the most relevant\ncomponents. The experiments on four programming languages (Java, C, Python, and\nJavaScript) show that CPR can generate causal graphs for reasonable\ninterpretations and boost the performance of bug fixing in automatic program\nrepair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhitao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoyang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhenhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation. (arXiv:2205.13346v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13346","description":"<p>Contrastive learning has achieved impressive success in generation tasks to\nmilitate the \"exposure bias\" problem and discriminatively exploit the different\nquality of references. Existing works mostly focus on contrastive learning on\nthe instance-level without discriminating the contribution of each word, while\nkeywords are the gist of the text and dominant the constrained mapping\nrelationships. Hence, in this work, we propose a hierarchical contrastive\nlearning mechanism, which can unify hybrid granularities semantic meaning in\nthe input text. Concretely, we first propose a keyword graph via contrastive\ncorrelations of positive-negative pairs to iteratively polish the keyword\nrepresentations. Then, we construct intra-contrasts within instance-level and\nkeyword-level, where we assume words are sampled nodes from a sentence\ndistribution. Finally, to bridge the gap between independent contrast levels\nand tackle the common contrast vanishing problem, we propose an inter-contrast\nmechanism that measures the discrepancy between contrastive keyword nodes\nrespectively to the instance distribution. Experiments demonstrate that our\nmodel outperforms competitive baselines on paraphrasing, dialogue generation,\nand storytelling tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">XieXiong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jinxiong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qishen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Taifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Document Vectors Using Cosine Similarity Revisited. (arXiv:2205.13357v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13357","description":"<p>The current state-of-the-art test accuracy (97.42\\%) on the IMDB movie\nreviews dataset was reported by \\citet{thongtan-phienthrakul-2019-sentiment}\nand achieved by the logistic regression classifier trained on the Document\nVectors using Cosine Similarity (DV-ngrams-cosine) proposed in their paper and\nthe Bag-of-N-grams (BON) vectors scaled by Naive Bayesian weights. While large\npre-trained Transformer-based models have shown SOTA results across many\ndatasets and tasks, the aforementioned model has not been surpassed by them,\ndespite being much simpler and pre-trained on the IMDB dataset only.\n</p>\n<p>In this paper, we describe an error in the evaluation procedure of this\nmodel, which was found when we were trying to analyze its excellent performance\non the IMDB dataset. We further show that the previously reported test accuracy\nof 97.42\\% is invalid and should be corrected to 93.68\\%. We also analyze the\nmodel performance with different amounts of training data (subsets of the IMDB\ndataset) and compare it to the Transformer-based RoBERTa model. The results\nshow that while RoBERTa has a clear advantage for larger training sets, the\nDV-ngrams-cosine performs better than RoBERTa when the labelled training set is\nvery small (10 or 20 documents). Finally, we introduce a sub-sampling scheme\nbased on Naive Bayesian weights for the training process of the\nDV-ngrams-cosine, which leads to faster training and better quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bingyu_Z/0/1/0/all/0/1\">Zhang Bingyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefyev_N/0/1/0/all/0/1\">Nikolay Arefyev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Your Transformer May Not be as Powerful as You Expect. (arXiv:2205.13401v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13401","description":"<p>Relative Positional Encoding (RPE), which encodes the relative distance\nbetween any pair of tokens, is one of the most successful modifications to the\noriginal Transformer. As far as we know, theoretical understanding of the\nRPE-based Transformers is largely unexplored. In this work, we mathematically\nanalyze the power of RPE-based Transformers regarding whether the model is\ncapable of approximating any continuous sequence-to-sequence functions. One may\nnaturally assume the answer is in the affirmative -- RPE-based Transformers are\nuniversal function approximators. However, we present a negative result by\nshowing there exist continuous sequence-to-sequence functions that RPE-based\nTransformers cannot approximate no matter how deep and wide the neural network\nis. One key reason lies in that most RPEs are placed in the softmax attention\nthat always generates a right stochastic matrix. This restricts the network\nfrom capturing positional information in the RPEs and limits its capacity. To\novercome the problem and make the model more powerful, we first present\nsufficient conditions for RPE-based Transformers to achieve universal function\napproximation. With the theoretical guidance, we develop a novel attention\nmodule, called Universal RPE-based (URPE) Attention, which satisfies the\nconditions. Therefore, the corresponding URPE-based Transformers become\nuniversal function approximators. Extensive experiments covering typical\narchitectures and tasks demonstrate that our model is parameter-efficient and\ncan achieve superior performance to strong baselines in a wide range of\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Learning Span Extraction and Sequence Labeling for Information Extraction from Business Documents. (arXiv:2205.13434v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13434","description":"<p>This paper introduces a new information extraction model for business\ndocuments. Different from prior studies which only base on span extraction or\nsequence labeling, the model takes into account advantage of both span\nextraction and sequence labeling. The combination allows the model to deal with\nlong documents with sparse information (the small amount of extracted\ninformation). The model is trained end-to-end to jointly optimize the two tasks\nin a unified manner. Experimental results on four business datasets in English\nand Japanese show that the model achieves promising results and is\nsignificantly faster than the normal span-based extraction method. The code is\nalso available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_N/0/1/0/all/0/1\">Nguyen Hong Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_H/0/1/0/all/0/1\">Hieu M. Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Anh D. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. (arXiv:2205.13445v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13445","description":"<p>Text-to-image generation and image captioning are recently emerged as a new\nexperimental paradigm to assess machine intelligence. They predict continuous\nquantity accompanied by their sampling techniques in the generation, making\nevaluation complicated and intractable to get marginal distributions. Based on\na recent trend that multimodal generative evaluations exploit a\nvison-and-language pre-trained model, we propose the negative Gaussian\ncross-mutual information using the CLIP features as a unified metric, coined by\nMutual Information Divergence (MID). To validate, we extensively compare it\nwith competing metrics using carefully-generated or human-annotated judgments\nin text-to-image generation and image captioning tasks. The proposed MID\nsignificantly outperforms the competitive methods by having consistency across\nbenchmarks, sample parsimony, and robustness toward the exploited CLIP model.\nWe look forward to seeing the underrepresented implications of the Gaussian\ncross-mutual information in multimodal representation learning and the future\nworks based on this novel proposition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Parsing of Interpage Relations. (arXiv:2205.13530v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13530","description":"<p>Page-level analysis of documents has been a topic of interest in digitization\nefforts, and multimodal approaches have been applied to both classification and\npage stream segmentation. In this work, we focus on capturing finer semantic\nrelations between pages of a multi-page document. To this end, we formalize the\ntask as semantic parsing of interpage relations and we propose an end-to-end\napproach for interpage dependency extraction, inspired by the dependency\nparsing literature. We further design a multi-task training approach to jointly\noptimize for page embeddings to be used in segmentation, classification, and\nparsing of the page dependencies using textual and visual features extracted\nfrom the pages. Moreover, we also combine the features from two modalities to\nobtain multimodal page embeddings. To the best of our knowledge, this is the\nfirst study to extract rich semantic interpage relations from multi-page\ndocuments. Our experimental results show that the proposed method increased LAS\nby 41 percentage points for semantic parsing, increased accuracy by 33\npercentage points for page stream segmentation, and 45 percentage points for\npage classification over a naive baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demirtas_M/0/1/0/all/0/1\">Mehmet Arif Demirta&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oral_B/0/1/0/all/0/1\">Berke Oral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akpinar_M/0/1/0/all/0/1\">Mehmet Yasin Akp&#x131;nar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deniz_O/0/1/0/all/0/1\">Onur Deniz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCoLM: COmplex COmmonsense Enhanced Language Model with Discourse Relations. (arXiv:2012.15643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15643","description":"<p>Large-scale pre-trained language models have demonstrated strong knowledge\nrepresentation ability. However, recent studies suggest that even though these\ngiant models contains rich simple commonsense knowledge (e.g., bird can fly and\nfish can swim.), they often struggle with the complex commonsense knowledge\nthat involves multiple eventualities (verb-centric phrases, e.g., identifying\nthe relationship between ``Jim yells at Bob'' and ``Bob is upset'').To address\nthis problem, in this paper, we propose to help pre-trained language models\nbetter incorporate complex commonsense knowledge. Different from existing\nfine-tuning approaches, we do not focus on a specific task and propose a\ngeneral language model named CoCoLM. Through the careful training over a\nlarge-scale eventuality knowledge graphs ASER, we successfully teach\npre-trained language models (i.e., BERT and RoBERTa) rich complex commonsense\nknowledge among eventualities. Experiments on multiple downstream commonsense\ntasks that requires the correct understanding of eventualities demonstrate the\neffectiveness of CoCoLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_W/0/1/0/all/0/1\">Wilfred Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Resource Multi-Dialectal Arabic Natural Language Understanding. (arXiv:2104.06591v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06591","description":"<p>A reasonable amount of annotated data is required for fine-tuning pre-trained\nlanguage models (PLM) on downstream tasks. However, obtaining labeled examples\nfor different language varieties can be costly. In this paper, we investigate\nthe zero-shot performance on Dialectal Arabic (DA) when fine-tuning a PLM on\nmodern standard Arabic (MSA) data only -- identifying a significant performance\ndrop when evaluating such models on DA. To remedy such performance drop, we\npropose self-training with unlabeled DA data and apply it in the context of\nnamed entity recognition (NER), part-of-speech (POS) tagging, and sarcasm\ndetection (SRD) on several DA varieties. Our results demonstrate the\neffectiveness of self-training with unlabeled DA data: improving zero-shot\nMSA-to-DA transfer by as large as $\\sim$10\\% F$_1$ (NER), 2\\% accuracy (POS\ntagging), and 4.5\\% F$_1$ (SRD). We conduct an ablation experiment and show\nthat the performance boost observed directly results from the unlabeled DA\nexamples used for self-training. Our work opens up opportunities for leveraging\nthe relatively abundant labeled MSA datasets to develop DA models for zero and\nlow-resource dialects. We also report new state-of-the-art performance on all\nthree tasks and open-source our fine-tuned models for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hesham Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahmy_A/0/1/0/all/0/1\">Aly Fahmy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems. (arXiv:2104.08570v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08570","description":"<p>In task-oriented dialogue (ToD), a user holds a conversation with an\nartificial agent to complete a concrete task. Although this technology\nrepresents one of the central objectives of AI and has been the focus of ever\nmore intense research and development efforts, it is currently limited to a few\nnarrow domains (e.g., food ordering, ticket booking) and a handful of languages\n(e.g., English, Chinese). This work provides an extensive overview of existing\nmethods and resources in multilingual ToD as an entry point to this exciting\nand emerging field. We find that the most critical factor preventing the\ncreation of truly multilingual ToD systems is the lack of datasets in most\nlanguages for both training and evaluation. In fact, acquiring annotations or\nhuman feedback for each component of modular systems or for data-hungry\nend-to-end systems is expensive and tedious. Hence, state-of-the-art approaches\nto multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer\nfrom resource-rich languages (almost exclusively English), either by means of\nmachine translation or multilingual representations. These approaches are\ncurrently viable only for typologically similar languages and languages with\nparallel / monolingual corpora available. On the other hand, their\neffectiveness beyond these boundaries is doubtful or hard to assess due to the\nlack of linguistically diverse benchmarks (especially for natural language\ngeneration and end-to-end evaluation). To overcome this limitation, we draw\nparallels between components of the ToD pipeline and other NLP tasks, which can\ninspire solutions for learning in low-resource scenarios. Finally, we list\nadditional challenges that multilinguality poses for related areas (such as\nspeech and human-centred evaluation), and indicate future directions that hold\npromise to further expand language coverage and dialogue capabilities of\ncurrent ToD systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razumovskaia_E/0/1/0/all/0/1\">Evgeniia Razumovskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majewska_O/0/1/0/all/0/1\">Olga Majewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarization, Simplification, and Generation: The Case of Patents. (arXiv:2104.14860v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.14860","description":"<p>We survey Natural Language Processing (NLP) approaches to summarizing,\nsimplifying, and generating patents' text. While solving these tasks has\nimportant practical applications - given patents' centrality in the R&amp;D process\n- patents' idiosyncrasies open peculiar challenges to the current NLP state of\nthe art. This survey aims at a) describing patents' characteristics and the\nquestions they raise to the current NLP systems, b) critically presenting\nprevious work and its evolution, and c) drawing attention to directions of\nresearch in which further work is needed. To the best of our knowledge, this is\nthe first survey of generative approaches in the patent domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casola_S/0/1/0/all/0/1\">Silvia Casola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavelli_A/0/1/0/all/0/1\">Alberto Lavelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dense Information Retrieval with Contrastive Learning. (arXiv:2112.09118v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2112.09118","description":"<p>Recently, information retrieval has seen the emergence of dense retrievers,\nbased on neural networks, as an alternative to classical sparse methods based\non term-frequency. These models have obtained state-of-the-art results on\ndatasets and tasks where large training sets are available. However, they do\nnot transfer well to new applications with no training data, and are\noutperformed by unsupervised term-frequency methods such as BM25. In this work,\nwe explore the limits of contrastive learning as a way to train unsupervised\ndense retrievers and show that it leads to strong performance in various\nretrieval settings. On the BEIR benchmark our unsupervised model outperforms\nBM25 on 11 out of 15 datasets for the Recall@100 metric. When used as\npre-training before fine-tuning, either on a few thousands in-domain examples\nor on the large MS MARCO dataset, our contrastive model leads to improvements\non the BEIR benchmark. Finally, we evaluate our approach for multi-lingual\nretrieval, where training data is even scarcer than for English, and show that\nour approach leads to strong unsupervised performance. Our model also exhibits\nstrong cross-lingual transfer when fine-tuned on supervised English data only\nand evaluated on low resources language such as Swahili. We show that our\nunsupervised models can perform cross-lingual retrieval between different\nscripts, such as retrieving English documents from Arabic queries, which would\nnot be possible with term matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1\">Gautier Izacard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_L/0/1/0/all/0/1\">Lucas Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1\">Edouard Grave</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation. (arXiv:2201.08054v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08054","description":"<p>Existing multimodal machine translation (MMT) datasets consist of images and\nvideo captions or general subtitles, which rarely contain linguistic ambiguity,\nmaking visual information not so effective to generate appropriate\ntranslations. We introduce VISA, a new dataset that consists of 40k\nJapanese-English parallel sentence pairs and corresponding video clips with the\nfollowing key features: (1) the parallel sentences are subtitles from movies\nand TV episodes; (2) the source subtitles are ambiguous, which means they have\nmultiple possible translations with different meanings; (3) we divide the\ndataset into Polysemy and Omission according to the cause of ambiguity. We show\nthat VISA is challenging for the latest MMT system, and we hope that the\ndataset can facilitate MMT research. The VISA dataset is available at:\nhttps://github.com/ku-nlp/VISA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1\">Shuichiro Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation. (arXiv:2205.06457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06457","description":"<p>We present ViT5, a pretrained Transformer-based encoder-decoder model for the\nVietnamese language. With T5-style self-supervised pretraining, ViT5 is trained\non a large corpus of high-quality and diverse Vietnamese texts. We benchmark\nViT5 on two downstream text generation tasks, Abstractive Text Summarization\nand Named Entity Recognition. Although Abstractive Text Summarization has been\nwidely studied for the English language thanks to its rich and large source of\ndata, there has been minimal research into the same task in Vietnamese, a much\nlower resource language. In this work, we perform exhaustive experiments on\nboth Vietnamese Abstractive Summarization and Named Entity Recognition,\nvalidating the performance of ViT5 against many other pretrained\nTransformer-based encoder-decoder models. Our experiments show that ViT5\nsignificantly outperforms existing models and achieves state-of-the-art results\non Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5\nis competitive against previous best results from pretrained encoder-based\nTransformer models. Further analysis shows the importance of context length\nduring the self-supervised pretraining on downstream performance across\ndifferent settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinh_T/0/1/0/all/0/1\">Trieu H. Trinh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Approach for Automatic Construction of an Algorithmic Knowledge Graph from Textual Resources. (arXiv:2205.06854v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.06854","description":"<p>There is enormous growth in various fields of research. This development is\naccompanied by new problems. To solve these problems efficiently and in an\noptimized manner, algorithms are created and described by researchers in the\nscientific literature. Scientific algorithms are vital for understanding and\nreusing existing work in numerous domains. However, algorithms are generally\nchallenging to find. Also, the comparison among similar algorithms is difficult\nbecause of the disconnected documentation. Information about algorithms is\nmostly present in websites, code comments, and so on. There is an absence of\nstructured metadata to portray algorithms. As a result, sometimes redundant or\nsimilar algorithms are published, and the researchers build them from scratch\ninstead of reusing or expanding upon the already existing algorithm. In this\npaper, we introduce an approach for automatically developing a knowledge graph\n(KG) for algorithmic problems from unstructured data. Because it captures\ninformation more clearly and extensively, an algorithm KG will give additional\ncontext and explainability to the algorithm metadata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1\">Jyotima Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_B/0/1/0/all/0/1\">Biswanath Dutta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization. (arXiv:2205.07208v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07208","description":"<p>It is challenging to train a good intent classifier for a task-oriented\ndialogue system with only a few annotations. Recent studies have shown that\nfine-tuning pre-trained language models with a small amount of labeled\nutterances from public benchmarks in a supervised manner is extremely helpful.\nHowever, we find that supervised pre-training yields an anisotropic feature\nspace, which may suppress the expressive power of the semantic representations.\nInspired by recent research in isotropization, we propose to improve supervised\npre-training by regularizing the feature space towards isotropy. We propose two\nregularizers based on contrastive learning and correlation matrix respectively,\nand demonstrate their effectiveness through extensive experiments. Our main\nfinding is that it is promising to regularize supervised pre-training with\nisotropization to further improve the performance of few-shot intent detection.\nThe source code can be found at https://github.com/fanolabs/isoIntentBert-main.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haode Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haowen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liming Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaolei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1\">Albert Y.S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese. (arXiv:2205.10517v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10517","description":"<p>Multilingual language models such as mBERT have seen impressive cross-lingual\ntransfer to a variety of languages, but many languages remain excluded from\nthese models. In this paper, we analyse the effect of pre-training with\nmonolingual data for a low-resource language that is not included in mBERT --\nMaltese -- with a range of pre-training set ups. We conduct evaluations with\nthe newly pre-trained models on three morphosyntactic tasks -- dependency\nparsing, part-of-speech tagging, and named-entity recognition -- and one\nsemantic classification task -- sentiment analysis. We also present a newly\ncreated corpus for Maltese, and determine the effect that the pre-training data\nsize and domain have on the downstream performance. Our results show that using\na mixture of pre-training domains is often superior to using Wikipedia text\nonly. We also find that a fraction of this corpus is enough to make significant\nleaps in performance over Wikipedia-trained models. We pre-train and compare\ntwo models on the new corpus: a monolingual BERT model trained from scratch\n(BERTu), and a further pre-trained multilingual BERT (mBERTu). The models\nachieve state-of-the-art performance on these tasks, despite the new corpus\nbeing considerably smaller than typically used corpora for high-resourced\nlanguages. On average, BERTu outperforms or performs competitively with mBERTu,\nand the largest gains are observed for higher-level tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Micallef_K/0/1/0/all/0/1\">Kurt Micallef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanti_M/0/1/0/all/0/1\">Marc Tanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plas_L/0/1/0/all/0/1\">Lonneke van der Plas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borg_C/0/1/0/all/0/1\">Claudia Borg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certified Robustness Against Natural Language Attacks by Causal Intervention. (arXiv:2205.12331v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12331","description":"<p>Deep learning models have achieved great success in many fields, yet they are\nvulnerable to adversarial examples. This paper follows a causal perspective to\nlook into the adversarial vulnerability and proposes Causal Intervention by\nSemantic Smoothing (CISS), a novel framework towards robustness against natural\nlanguage attacks. Instead of merely fitting observational data, CISS learns\ncausal effects p(y|do(x)) by smoothing in the latent semantic space to make\nrobust predictions, which scales to deep architectures and avoids tedious\nconstruction of noise customized for specific attacks. CISS is provably robust\nagainst word substitution attacks, as well as empirically robust even when\nperturbations are strengthened by unknown attack algorithms. For example, on\nYELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness\nagainst word substitutions, and achieves 79.4% empirical robustness when\nsyntactic attacks are integrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiteng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma%2A_C/0/1/0/all/0/1\">Chang Ma*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAMPARI: : An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. (arXiv:2205.12665v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12665","description":"<p>Existing benchmarks for open-domain question answering (ODQA) typically focus\non questions whose answers can be extracted from a single paragraph. By\ncontrast, many natural questions, such as \"What players were drafted by the\nBrooklyn Nets?\" have a list of answers. Answering such questions requires\nretrieving and reading from many passages, in a large corpus. We introduce\nQAMPARI, an ODQA benchmark, where question answers are lists of entities,\nspread across many paragraphs. We created QAMPARI by (a) generating questions\nwith multiple answers from Wikipedia's knowledge graph and tables, (b)\nautomatically pairing answers with supporting evidence in Wikipedia paragraphs,\nand (c) manually paraphrasing questions and validating each answer. We train\nODQA models from the retrieve-and-read family and find that QAMPARI is\nchallenging in terms of both passage retrieval and answer generation, reaching\nan F1 score of 26.6 at best. Our results highlight the need for developing ODQA\nmodels that handle a broad range of question types, including single and\nmulti-answer questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amouyal_S/0/1/0/all/0/1\">Samuel Joseph Amouyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_O/0/1/0/all/0/1\">Ohad Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards Diverse and Natural Scene-aware 3D Human Motion Synthesis. (arXiv:2205.13001v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13001","description":"<p>The ability to synthesize long-term human motion sequences in real-world\nscenes can facilitate numerous applications. Previous approaches for\nscene-aware motion synthesis are constrained by pre-defined target objects or\npositions and thus limit the diversity of human-scene interactions for\nsynthesized motions. In this paper, we focus on the problem of synthesizing\ndiverse scene-aware human motions under the guidance of target action\nsequences. To achieve this, we first decompose the diversity of scene-aware\nhuman motions into three aspects, namely interaction diversity (e.g. sitting on\ndifferent objects with different poses in the given scenes), path diversity\n(e.g. moving to the target locations following different paths), and the motion\ndiversity (e.g. having various body movements during moving). Based on this\nfactorized scheme, a hierarchical framework is proposed, with each sub-module\nresponsible for modeling one aspect. We assess the effectiveness of our\nframework on two challenging datasets for scene-aware human motion synthesis.\nThe experiment results show that the proposed framework remarkably outperforms\nprevious methods in terms of diversity and naturalness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Sijie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"People counting system for retail analytics using edge AI. (arXiv:2205.13020v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13020","description":"<p>Developments in IoT applications are playing an important role in our\nday-to-day life, starting from business predictions to self driving cars. One\nof the area, most influenced by the field of AI and IoT is retail analytics. In\nRetail Analytics, Conversion Rates - a metric which is most often used by\nretail stores to measure how many people have visited the store and how many\npurchases has happened. This retail conversion rate assess the marketing\noperations, increasing stock, store outlet and running promotions ..etc. Our\nproject intends to build a cost-effective people counting system with AI at\nEdge, where it calculates Conversion rates using total number of people counted\nby the system and number of transactions for the day, which helps in providing\nanalytical insights for retail store optimization with a very minimum hardware\nrequirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanjula_K/0/1/0/all/0/1\">Karthik Reddy Kanjula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_V/0/1/0/all/0/1\">Vishnu Vardhan Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P_J/0/1/0/all/0/1\">Jnanesh K P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_J/0/1/0/all/0/1\">Jeffy S Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_T/0/1/0/all/0/1\">Tanuja K</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How explainable are adversarially-robust CNNs?. (arXiv:2205.13042v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13042","description":"<p>Three important criteria of existing convolutional neural networks (CNNs) are\n(1) test-set accuracy; (2) out-of-distribution accuracy; and (3)\nexplainability. While these criteria have been studied independently, their\nrelationship is unknown. For example, do CNNs that have a stronger\nout-of-distribution performance have also stronger explainability? Furthermore,\nmost prior feature-importance studies only evaluate methods on 2-3 common\nvanilla ImageNet-trained CNNs, leaving it unknown how these methods generalize\nto CNNs of other architectures and training algorithms. Here, we perform the\nfirst, large-scale evaluation of the relations of the three criteria using 9\nfeature-importance methods and 12 ImageNet-trained CNNs that are of 3 training\nalgorithms and 5 CNN architectures. We find several important insights and\nrecommendations for ML practitioners. First, adversarially robust CNNs have a\nhigher explainability score on gradient-based attribution methods (but not\nCAM-based or perturbation-based methods). Second, AdvProp models, despite being\nhighly accurate more than both vanilla and robust models alone, are not\nsuperior in explainability. Third, among 9 feature attribution methods tested,\nGradCAM and RISE are consistently the best methods. Fourth, Insertion and\nDeletion are biased towards vanilla and robust models respectively, due to\ntheir strong correlation with the confidence score distributions of a CNN.\nFifth, we did not find a single CNN to be the best in all three criteria, which\ninterestingly suggests that CNNs are harder to interpret as they become more\naccurate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nourelahi_M/0/1/0/all/0/1\">Mehdi Nourelahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthoff_L/0/1/0/all/0/1\">Lars Kotthoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Deep Equilibrium Learning for Regularization by Denoising. (arXiv:2205.13051v1 [eess.IV])","link":"http://arxiv.org/abs/2205.13051","description":"<p>Plug-and-Play Priors (PnP) and Regularization by Denoising (RED) are\nwidely-used frameworks for solving imaging inverse problems by computing\nfixed-points of operators combining physical measurement models and learned\nimage priors. While traditional PnP/RED formulations have focused on priors\nspecified using image denoisers, there is a growing interest in learning\nPnP/RED priors that are end-to-end optimal. The recent Deep Equilibrium Models\n(DEQ) framework has enabled memory-efficient end-to-end learning of PnP/RED\npriors by implicitly differentiating through the fixed-point equations without\nstoring intermediate activation values. However, the dependence of the\ncomputational/memory complexity of the measurement models in PnP/RED on the\ntotal number of measurements leaves DEQ impractical for many imaging\napplications. We propose ODER as a new strategy for improving the efficiency of\nDEQ through stochastic approximations of the measurement models. We\ntheoretically analyze ODER giving insights into its convergence and ability to\napproximate the traditional DEQ approach. Our numerical results suggest the\npotential improvements in training/testing complexity due to ODER on three\ndistinct imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaojian Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Weijie Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shoushtari_S/0/1/0/all/0/1\">Shirin Shoushtari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamilov_U/0/1/0/all/0/1\">Ulugbek S. Kamilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing an Efficient End-to-end Machine Learning Pipeline for Real-time Empty-shelf Detection. (arXiv:2205.13060v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13060","description":"<p>On-Shelf Availability (OSA) of products in retail stores is a critical\nbusiness criterion in the fast moving consumer goods and retails sector. When a\nproduct is out-of-stock (OOS) and a customer cannot find it on its designed\nshelf, this causes a negative impact on the customer's behaviors and future\ndemands. Several methods are being adopted by retailers today to detect empty\nshelves and ensure high OSA of products; however, such methods are generally\nineffective and infeasible since they are either manual, expensive or less\naccurate. Recently machine learning based solutions have been proposed, but\nthey suffer from high computation cost and low accuracy problem due to lack of\nlarge annotated datasets of on-shelf products. Here, we present an elegant\napproach for designing an end-to-end machine learning (ML) pipeline for\nreal-time empty shelf detection. Considering the strong dependency between the\nquality of ML models and the quality of data, we focus on the importance of\nproper data collection, cleaning and correct data annotation before delving\ninto modeling. Since an empty-shelf detection solution should be\ncomputationally-efficient for real-time predictions, we explore different\nrun-time optimizations to improve the model performance. Our dataset contains\n1000 images, collected and annotated by following well-defined guidelines. Our\nlow-latency model achieves a mean average F1-score of 68.5%, and can process up\nto 67 images/s on Intel Xeon Gold and up to 860 images/s on an A100 GPU. Our\nannotated dataset is publicly available along with our optimized models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Dipendra Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjoubfar_A/0/1/0/all/0/1\">Ata Mahjoubfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Anupama Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Map-based Features for Efficient Attention-based Vehicle Motion Prediction. (arXiv:2205.13071v1 [cs.RO])","link":"http://arxiv.org/abs/2205.13071","description":"<p>Motion prediction (MP) of multiple agents is a crucial task in arbitrarily\ncomplex environments, from social robots to self-driving cars. Current\napproaches tackle this problem using end-to-end networks, where the input data\nis usually a rendered top-view of the scene and the past trajectories of all\nthe agents; leveraging this information is a must to obtain optimal\nperformance. In that sense, a reliable Autonomous Driving (AD) system must\nproduce reasonable predictions on time, however, despite many of these\napproaches use simple ConvNets and LSTMs, models might not be efficient enough\nfor real-time applications when using both sources of information (map and\ntrajectory history). Moreover, the performance of these models highly depends\non the amount of training data, which can be expensive (particularly the\nannotated HD maps). In this work, we explore how to achieve competitive\nperformance on the Argoverse 1.0 Benchmark using efficient attention-based\nmodels, which take as input the past trajectories and map-based features from\nminimal map information to ensure efficient and reliable MP. These features\nrepresent interpretable information as the driveable area and plausible goal\npoints, in opposition to black-box CNN-based methods for map processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Huelamo_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Hu&#xe9;lamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_M/0/1/0/all/0/1\">Miguel Ortiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels. (arXiv:2205.13092v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13092","description":"<p>Despite achieving impressive progress, current multi-label image recognition\n(MLR) algorithms heavily depend on large-scale datasets with complete labels,\nmaking collecting large-scale datasets extremely time-consuming and\nlabor-intensive. Training the multi-label image recognition models with partial\nlabels (MLR-PL) is an alternative way to address this issue, in which merely\nsome labels are known while others are unknown for each image (see Figure 1).\nHowever, current MLP-PL algorithms mainly rely on the pre-trained image\nclassification or similarity models to generate pseudo labels for the unknown\nlabels. Thus, they depend on a certain amount of data annotations and\ninevitably suffer from obvious performance drops, especially when the known\nlabel proportion is low. To address this dilemma, we propose a unified\nsemantic-aware representation blending (SARB) that consists of two crucial\nmodules to blend multi-granularity category-specific semantic representation\nacross different images to transfer information of known labels to complement\nunknown labels. Extensive experiments on the MS-COCO, Visual Genome, and Pascal\nVOC 2007 datasets show that the proposed SARB consistently outperforms current\nstate-of-the-art algorithms on all known label proportion settings. Concretely,\nit obtain the average mAP improvement of 1.9%, 4.5%, 1.0% on the three\nbenchmark datasets compared with the second-best algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1\">Tao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hefeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yongyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VizInspect Pro -- Automated Optical Inspection (AOI) solution. (arXiv:2205.13095v1 [cs.AI])","link":"http://arxiv.org/abs/2205.13095","description":"<p>Traditional vision based Automated Optical Inspection (referred to as AOI in\npaper) systems present multiple challenges in factory settings including\ninability to scale across multiple product lines, requirement of vendor\nprogramming expertise, little tolerance to variations and lack of cloud\nconnectivity for aggregated insights. The lack of flexibility in these systems\npresents a unique opportunity for a deep learning based AOI system specifically\nfor factory automation. The proposed solution, VizInspect pro is a generic\ncomputer vision based AOI solution built on top of Leo - An edge AI platform.\nInnovative features that overcome challenges of traditional vision systems\ninclude deep learning based image analysis which combines the power of\nself-learning with high speed and accuracy, an intuitive user interface to\nconfigure inspection profiles in minutes without ML or vision expertise and the\nability to solve complex inspection challenges while being tolerant to\ndeviations and unpredictable defects. This solution has been validated by\nmultiple external enterprise customers with confirmed value propositions. In\nthis paper we show you how this solution and platform solved problems around\nmodel development, deployment, scaling multiple inferences and visualizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waseem_F/0/1/0/all/0/1\">Faraz Waseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_S/0/1/0/all/0/1\">Sanjit Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haotian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1\">Debashis Mondal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to segment with limited annotations: Self-supervised pretraining with regression and contrastive loss in MRI. (arXiv:2205.13109v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13109","description":"<p>Obtaining manual annotations for large datasets for supervised training of\ndeep learning (DL) models is challenging. The availability of large unlabeled\ndatasets compared to labeled ones motivate the use of self-supervised\npretraining to initialize DL models for subsequent segmentation tasks. In this\nwork, we consider two pre-training approaches for driving a DL model to learn\ndifferent representations using: a) regression loss that exploits spatial\ndependencies within an image and b) contrastive loss that exploits semantic\nsimilarity between pairs of images. The effect of pretraining techniques is\nevaluated in two downstream segmentation applications using Magnetic Resonance\n(MR) images: a) liver segmentation in abdominal T2-weighted MR images and b)\nprostate segmentation in T2-weighted MR images of the prostate. We observed\nthat DL models pretrained using self-supervision can be finetuned for\ncomparable performance with fewer labeled datasets. Additionally, we also\nobserved that initializing the DL model using contrastive loss based\npretraining performed better than the regression loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Umapathy_L/0/1/0/all/0/1\">Lavanya Umapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhiyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_R/0/1/0/all/0/1\">Rohit Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_D/0/1/0/all/0/1\">Diego Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altbach_M/0/1/0/all/0/1\">Maria Altbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilgin_A/0/1/0/all/0/1\">Ali Bilgin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13115","description":"<p>Modern image captioning models are usually trained with text similarity\nobjectives. However, since reference captions in public datasets often describe\nthe most salient common objects, models trained with text similarity objectives\ntend to ignore specific and detailed aspects of an image that distinguish it\nfrom others. Toward more descriptive and distinctive caption generation, we\npropose using CLIP, a multimodal encoder trained on huge image-text pairs from\nweb, to calculate multimodal similarity and use it as a reward function. We\nalso propose a simple finetuning strategy of the CLIP text encoder to improve\ngrammar that does not require extra text annotation. This completely eliminates\nthe need for reference captions during the reward computation. To\ncomprehensively evaluate descriptive captions, we introduce FineCapEval, a new\ndataset for caption evaluation with fine-grained criteria: overall, background,\nobject, relations. In our experiments on text-to-image retrieval and\nFineCapEval, the proposed CLIP-guided model generates more distinctive captions\nthan the CIDEr-optimized model. We also show that our unsupervised grammar\nfinetuning of the CLIP text encoder alleviates the degeneration problem of the\nnaive CLIP reward. Lastly, we show human analysis where the annotators strongly\nprefer the CLIP reward to the CIDEr and MLE objectives according to various\ncriteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_A/0/1/0/all/0/1\">Ajinkya Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Cluster Faces via Pairwise Classification. (arXiv:2205.13117v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13117","description":"<p>Face clustering plays an essential role in exploiting massive unlabeled face\ndata. Recently, graph-based face clustering methods are getting popular for\ntheir satisfying performances. However, they usually suffer from excessive\nmemory consumption especially on large-scale graphs, and rely on empirical\nthresholds to determine the connectivities between samples in inference, which\nrestricts their applications in various real-world scenes. To address such\nproblems, in this paper, we explore face clustering from the pairwise angle.\nSpecifically, we formulate the face clustering task as a pairwise relationship\nclassification task, avoiding the memory-consuming learning on large-scale\ngraphs. The classifier can directly determine the relationship between samples\nand is enhanced by taking advantage of the contextual information. Moreover, to\nfurther facilitate the efficiency of our method, we propose a rank-weighted\ndensity to guide the selection of pairs sent to the classifier. Experimental\nresults demonstrate that our method achieves state-of-the-art performances on\nseveral public clustering benchmarks at the fastest speed and shows a great\nadvantage in comparison with graph-based clustering methods on memory\nconsumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1\">Di Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Pengfei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Learned Source-Channel Coding for High-Fidelity Image Semantic Transmission. (arXiv:2205.13120v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13120","description":"<p>As one novel approach to realize end-to-end wireless image semantic\ntransmission, deep learning-based joint source-channel coding (deep JSCC)\nmethod is emerging in both deep learning and communication communities.\nHowever, current deep JSCC image transmission systems are typically optimized\nfor traditional distortion metrics such as peak signal-to-noise ratio (PSNR) or\nmulti-scale structural similarity (MS-SSIM). But for low transmission rates,\ndue to the imperfect wireless channel, these distortion metrics lose\nsignificance as they favor pixel-wise preservation. To account for human visual\nperception in semantic communications, it is of great importance to develop new\ndeep JSCC systems optimized beyond traditional PSNR and MS-SSIM metrics. In\nthis paper, we introduce adversarial losses to optimize deep JSCC, which tends\nto preserve global semantic information and local texture. Our new deep JSCC\narchitecture combines encoder, wireless channel, decoder/generator, and\ndiscriminator, which are jointly learned under both perceptual and adversarial\nlosses. Our method yields human visually much more pleasing results than\nstate-of-the-art engineered image coded transmission systems and traditional\ndeep JSCC systems. A user study confirms that achieving the perceptually\nsimilar end-to-end image transmission quality, the proposed method can save\nabout 50\\% wireless channel bandwidth cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sixian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jincheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_Z/0/1/0/all/0/1\">Zhongwei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dekun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kai Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To image, or not to image: Class-specific diffractive cameras with all-optical erasure of undesired objects. (arXiv:2205.13122v1 [physics.optics])","link":"http://arxiv.org/abs/2205.13122","description":"<p>Privacy protection is a growing concern in the digital era, with machine\nvision techniques widely used throughout public and private settings. Existing\nmethods address this growing problem by, e.g., encrypting camera images or\nobscuring/blurring the imaged information through digital algorithms. Here, we\ndemonstrate a camera design that performs class-specific imaging of target\nobjects with instantaneous all-optical erasure of other classes of objects.\nThis diffractive camera consists of transmissive surfaces structured using deep\nlearning to perform selective imaging of target classes of objects positioned\nat its input field-of-view. After their fabrication, the thin diffractive\nlayers collectively perform optical mode filtering to accurately form images of\nthe objects that belong to a target data class or group of classes, while\ninstantaneously erasing objects of the other data classes at the output\nfield-of-view. Using the same framework, we also demonstrate the design of\nclass-specific permutation cameras, where the objects of a target data class\nare pixel-wise permuted for all-optical class-specific encryption, while the\nother objects are irreversibly erased from the output image. The success of\nclass-specific diffractive cameras was experimentally demonstrated using\nterahertz (THz) waves and 3D-printed diffractive layers that selectively imaged\nonly one class of the MNIST handwritten digit dataset, all-optically erasing\nthe other handwritten digits. This diffractive camera design can be scaled to\ndifferent parts of the electromagnetic spectrum, including, e.g., the visible\nand infrared wavelengths, to provide transformative opportunities for\nprivacy-preserving digital cameras and task-specific data-efficient imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Bai_B/0/1/0/all/0/1\">Bijie Bai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1\">Yi Luo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gan_T/0/1/0/all/0/1\">Tianyi Gan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hu_J/0/1/0/all/0/1\">Jingtian Hu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mengu_D/0/1/0/all/0/1\">Deniz Mengu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jarrahi_M/0/1/0/all/0/1\">Mona Jarrahi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PixelGame: Infrared small target segmentation as a Nash equilibrium. (arXiv:2205.13124v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13124","description":"<p>A key challenge of infrared small target segmentation (ISTS) is to balance\nfalse negative pixels (FNs) and false positive pixels (FPs). Traditional\nmethods combine FNs and FPs into a single objective by weighted sum, and the\noptimization process is decided by one actor. Minimizing FNs and FPs with the\nsame strategy leads to antagonistic decisions. To address this problem, we\npropose a competitive game framework (pixelGame) from a novel perspective for\nISTS. In pixelGame, FNs and FPs are controlled by different player whose goal\nis to minimize their own utility function. FNs-player and FPs-player are\ndesigned with different strategies: One is to minimize FNs and the other is to\nminimize FPs. The utility function drives the evolution of the two participants\nin competition. We consider the Nash equilibrium of pixelGame as the optimal\nsolution. In addition, we propose maximum information modulation (MIM) to\nhighlight the tar-get information. MIM effectively focuses on the salient\nregion including small targets. Extensive experiments on two standard public\ndatasets prove the effectiveness of our method. Compared with other\nstate-of-the-art methods, our method achieves better performance in terms of\nF1-measure (F1) and the intersection of union (IoU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Heng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chunna Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenxi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yongqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongbo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Learning for Unpaired Image Captioning. (arXiv:2205.13125v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13125","description":"<p>Unpaired Image Captioning (UIC) has been developed to learn image\ndescriptions from unaligned vision-language sample pairs. Existing schemes\nusually adopt the visual concept reward of reinforcement learning to obtain the\nalignment between visual concepts and images. However, the cross-domain\nalignment is usually weak that severely constrains the overall performance of\nthese existing schemes. Recent successes of Vision-Language Pre-Trained Models\n(VL-PTMs) have triggered the development of prompt-based learning from VL-PTMs.\nWe present in this paper a novel scheme based on prompt to train the UIC model,\nmaking best use of the powerful generalization ability and abundant\nvision-language prior knowledge learned under VL-PTMs. We adopt the CLIP model\nfor this research in unpaired image captioning. Specifically, the visual images\nare taken as input to the prompt generation module, which contains the\npre-trained model as well as one feed-forward layer for prompt extraction.\nThen, the input images and generated prompts are aggregated for unpaired\nadversarial captioning learning. To further enhance the potential performance\nof the captioning, we designed a high-quality pseudo caption filter guided by\nthe CLIP logits to measure correlations between predicted captions and the\ncorresponding images. This allows us to improve the captioning model in a\nsupervised learning manner. Extensive experiments on the COCO and Flickr30K\ndatasets have been carried out to validate the superiority of the proposed\nmodel. We have achieved the state-of-the-art performance on the COCO dataset,\nwhich outperforms the best UIC model by 1.9% on the BLEU-4 metric. We expect\nthat the proposed prompt-based UIC model will inspire a new line of research\nfor the VL-PTMs based captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peipei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenglong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weishi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changwen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wireless Deep Video Semantic Transmission. (arXiv:2205.13129v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13129","description":"<p>In this paper, we design a new class of high-efficiency deep joint\nsource-channel coding methods to achieve end-to-end video transmission over\nwireless channels. The proposed methods exploit nonlinear transform and\nconditional coding architecture to adaptively extract semantic features across\nvideo frames, and transmit semantic feature domain representations over\nwireless channels via deep joint source-channel coding. Our framework is\ncollected under the name deep video semantic transmission (DVST). In\nparticular, benefiting from the strong temporal prior provided by the feature\ndomain context, the learned nonlinear transform function becomes temporally\nadaptive, resulting in a richer and more accurate entropy model guiding the\ntransmission of current frame. Accordingly, a novel rate adaptive transmission\nmechanism is developed to customize deep joint source-channel coding for video\nsources. It learns to allocate the limited channel bandwidth within and among\nvideo frames to maximize the overall transmission performance. The whole DVST\ndesign is formulated as an optimization problem whose goal is to minimize the\nend-to-end transmission rate-distortion performance under perceptual quality\nmetrics or machine vision task performance metrics. Across standard video\nsource test sequences and various communication scenarios, experiments show\nthat our DVST can generally surpass traditional wireless video coded\ntransmission schemes. The proposed DVST framework can well support future\nsemantic communications due to its video content-aware and machine vision task\nintegration abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sixian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jincheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zijian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kai Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_Z/0/1/0/all/0/1\">Zhongwei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaoqi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Ping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning. (arXiv:2205.13137v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13137","description":"<p>In this study, we propose Mixed and Masked Image Modeling (MixMIM), a simple\nbut efficient MIM method that is applicable to various hierarchical Vision\nTransformers. Existing MIM methods replace a random subset of input tokens with\na special MASK symbol and aim at reconstructing original image tokens from the\ncorrupted image. However, we find that using the MASK symbol greatly slows down\nthe training and causes training-finetuning inconsistency, due to the large\nmasking ratio (e.g., 40% in BEiT). In contrast, we replace the masked tokens of\none image with visible tokens of another image, i.e., creating a mixed image.\nWe then conduct dual reconstruction to reconstruct the original two images from\nthe mixed input, which significantly improves efficiency. While MixMIM can be\napplied to various architectures, this paper explores a simpler but stronger\nhierarchical Transformer, and scales with MixMIM-B, -L, and -H. Empirical\nresults demonstrate that MixMIM can learn high-quality visual representations\nefficiently. Notably, MixMIM-B with 88M parameters achieves 85.1% top-1\naccuracy on ImageNet-1K by pretraining for 600 epochs, setting a new record for\nneural networks with comparable model sizes (e.g., ViT-B) among MIM methods.\nBesides, its transferring performances on the other 6 datasets show MixMIM has\nbetter FLOPs / performance tradeoff than previous MIM methods. Code is\navailable at https://github.com/Sense-X/MixMIM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matryoshka Representations for Adaptive Deployment. (arXiv:2205.13147v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13147","description":"<p>Learned representations are a central component in modern ML systems, serving\na multitude of downstream tasks. When training such representations, it is\noften the case that computational and statistical constraints for each\ndownstream task are unknown. In this context rigid, fixed capacity\nrepresentations can be either over or under-accommodating to the task at hand.\nThis leads us to ask: can we design a flexible representation that can adapt to\nmultiple downstream tasks with varying computational resources? Our main\ncontribution is Matryoshka Representation Learning (MRL) which encodes\ninformation at different granularities and allows a single embedding to adapt\nto the computational constraints of downstream tasks. MRL minimally modifies\nexisting representation learning pipelines and imposes no additional cost\nduring inference and deployment. MRL learns coarse-to-fine representations that\nare at least as accurate and rich as independently trained low-dimensional\nrepresentations. The flexibility within the learned Matryoshka Representations\noffer: (a) up to 14x smaller embedding size for ImageNet-1K classification at\nthe same level of accuracy; (b) up to 14x real-world speed-ups for large-scale\nretrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for\nlong-tail few-shot classification, all while being as robust as the original\nrepresentations. Finally, we show that MRL extends seamlessly to web-scale\ndatasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),\nvision + language (ALIGN) and language (BERT). MRL code and pretrained models\nare open-sourced at https://github.com/RAIVNLab/MRL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1\">Gantavya Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rege_A/0/1/0/all/0/1\">Aniket Rege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1\">Matthew Wallingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Aditya Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_Snyder_W/0/1/0/all/0/1\">William Howard-Snyder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferable Adversarial Attack based on Integrated Gradients. (arXiv:2205.13152v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13152","description":"<p>The vulnerability of deep neural networks to adversarial examples has drawn\ntremendous attention from the community. Three approaches, optimizing standard\nobjective functions, exploiting attention maps, and smoothing decision\nsurfaces, are commonly used to craft adversarial examples. By tightly\nintegrating the three approaches, we propose a new and simple algorithm named\nTransferable Attack based on Integrated Gradients (TAIG) in this paper, which\ncan find highly transferable adversarial examples for black-box attacks. Unlike\nprevious methods using multiple computational terms or combining with other\nmethods, TAIG integrates the three approaches into one single term. Two\nversions of TAIG that compute their integrated gradients on a straight-line\npath and a random piecewise linear path are studied. Both versions offer strong\ntransferability and can seamlessly work together with the previous methods.\nExperimental results demonstrate that TAIG outperforms the state-of-the-art\nmethods. The code will available at https://github.com/yihuang2016/TAIG\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_A/0/1/0/all/0/1\">Adams Wai-Kin Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwinVRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation. (arXiv:2205.13158v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13158","description":"<p>Data-driven approaches for medium-range weather forecasting are recently\nshown extraordinarily promising for ensemble forecasting for their fast\ninference speed compared to traditional numerical weather prediction (NWP)\nmodels, but their forecast accuracy can hardly match the state-of-the-art\noperational ECMWF Integrated Forecasting System (IFS) model. Previous\ndata-driven attempts achieve ensemble forecast using some simple perturbation\nmethods, like initial condition perturbation and Monte Carlo dropout. However,\nthey mostly suffer unsatisfactory ensemble performance, which is arguably\nattributed to the sub-optimal ways of applying perturbation. We propose a Swin\nTransformer-based Variational Recurrent Neural Network (SwinVRNN), which is a\nstochastic weather forecasting model combining a SwinRNN predictor with a\nperturbation module. SwinRNN is designed as a Swin Transformer-based recurrent\nneural network, which predicts future states deterministically. Furthermore, to\nmodel the stochasticity in prediction, we design a perturbation module\nfollowing the Variational Auto-Encoder paradigm to learn multivariate Gaussian\ndistributions of a time-variant stochastic latent variable from data. Ensemble\nforecasting can be easily achieved by perturbing the model features leveraging\nnoise sampled from the learned distribution. We also compare four categories of\nperturbation methods for ensemble forecasting, i.e. fixed distribution\nperturbation, learned distribution perturbation, MC dropout, and multi model\nensemble. Comparisons on WeatherBench dataset show the learned distribution\nperturbation method using our SwinVRNN model achieves superior forecast\naccuracy and reasonable ensemble spread due to joint optimization of the two\ntargets. More notably, SwinVRNN surpasses operational IFS on surface variables\nof 2-m temperature and 6-hourly total precipitation at all lead times up to\nfive days.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIRL: A General Framework for Hierarchical Image Representation Learning. (arXiv:2205.13159v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13159","description":"<p>Learning self-supervised image representations has been broadly studied to\nboost various visual understanding tasks. Existing methods typically learn a\nsingle level of image semantics like pairwise semantic similarity or image\nclustering patterns. However, these methods can hardly capture multiple levels\nof semantic information that naturally exists in an image dataset, e.g., the\nsemantic hierarchy of \"Persian cat to cat to mammal\" encoded in an image\ndatabase for species. It is thus unknown whether an arbitrary image\nself-supervised learning (SSL) approach can benefit from learning such\nhierarchical semantics. To answer this question, we propose a general framework\nfor Hierarchical Image Representation Learning (HIRL). This framework aims to\nlearn multiple semantic representations for each image, and these\nrepresentations are structured to encode image semantics from fine-grained to\ncoarse-grained. Based on a probabilistic factorization, HIRL learns the most\nfine-grained semantics by an off-the-shelf image SSL approach and learns\nmultiple coarse-grained semantics by a novel semantic path discrimination\nscheme. We adopt six representative image SSL methods as baselines and study\nhow they perform under HIRL. By rigorous fair comparison, performance gain is\nobserved on all the six methods for diverse downstream tasks, which, for the\nfirst time, verifies the general effectiveness of learning hierarchical image\nsemantics. All source code and model weights are available at\nhttps://github.com/hirl-team/HIRL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xuanyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenbang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Field Raindrop Removal via 4D Re-sampling. (arXiv:2205.13165v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13165","description":"<p>The Light Field Raindrop Removal (LFRR) aims to restore the background areas\nobscured by raindrops in the Light Field (LF). Compared with single image, the\nLF provides more abundant information by regularly and densely sampling the\nscene. Since raindrops have larger disparities than the background in the LF,\nthe majority of texture details occluded by raindrops are visible in other\nviews. In this paper, we propose a novel LFRR network by directly utilizing the\ncomplementary pixel information of raindrop-free areas in the input raindrop\nLF, which consists of the re-sampling module and the refinement module.\nSpecifically, the re-sampling module generates a new LF which is less polluted\nby raindrops through re-sampling position predictions and the proposed 4D\ninterpolation. The refinement module improves the restoration of the completely\noccluded background areas and corrects the pixel error caused by 4D\ninterpolation. Furthermore, we carefully build the first real scene LFRR\ndataset for model training and validation. Experiments demonstrate that the\nproposed method can effectively remove raindrops and achieves state-of-the-art\nperformance in both background restoration and view consistency maintenance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_D/0/1/0/all/0/1\">Dong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Song Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Latent Space of GAN through Local Dimension Estimation. (arXiv:2205.13182v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13182","description":"<p>The impressive success of style-based GANs (StyleGANs) in high-fidelity image\nsynthesis has motivated research to understand the semantic properties of their\nlatent spaces. Recently, a close relationship was observed between the\nsemantically disentangled local perturbations and the local PCA components in\nthe learned latent space $\\mathcal{W}$. However, understanding the number of\ndisentangled perturbations remains challenging. Building upon this observation,\nwe propose a local dimension estimation algorithm for an arbitrary intermediate\nlayer in a pre-trained GAN model. The estimated intrinsic dimension corresponds\nto the number of disentangled local perturbations. In this perspective, we\nanalyze the intermediate layers of the mapping network in StyleGANs. Our\nanalysis clarifies the success of $\\mathcal{W}$-space in StyleGAN and suggests\nan alternative. Moreover, the intrinsic dimension estimation opens the\npossibility of unsupervised evaluation of global-basis-compatibility and\ndisentanglement for a latent space. Our proposed metric, called Distortion,\nmeasures an inconsistency of intrinsic tangent space on the learned latent\nspace. The metric is purely geometric and does not require any additional\nattribute information. Nevertheless, the metric shows a high correlation with\nthe global-basis-compatibility and supervised disentanglement score. Our\nfindings pave the way towards an unsupervised selection of globally\ndisentangled latent space among the intermediate latent spaces in a GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaewoong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Geonho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI for Porosity and Permeability Prediction from Geologic Core X-Ray Micro-Tomography. (arXiv:2205.13189v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13189","description":"<p>Geologic cores are rock samples that are extracted from deep under the ground\nduring the well drilling process. They are used for petroleum reservoirs'\nperformance characterization. Traditionally, physical studies of cores are\ncarried out by the means of manual time-consuming experiments. With the\ndevelopment of deep learning, scientists actively started working on developing\nmachine-learning-based approaches to identify physical properties without any\nmanual experiments. Several previous works used machine learning to determine\nthe porosity and permeability of the rocks, but either method was inaccurate or\ncomputationally expensive. We are proposing to use self-supervised pretraining\nof the very small CNN-transformer-based model to predict the physical\nproperties of the rocks with high accuracy in a time-efficient manner. We show\nthat this technique prevents overfitting even for extremely small datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iklassov_Z/0/1/0/all/0/1\">Zangir Iklassov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medvedev_D/0/1/0/all/0/1\">Dmitrii Medvedev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazarov_O/0/1/0/all/0/1\">Otabek Nazarov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree Reconstruction using Topology Optimisation. (arXiv:2205.13192v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13192","description":"<p>Generating accurate digital tree models from scanned environments is\ninvaluable for forestry, agriculture, and other outdoor industries in tasks\nsuch as identifying biomass, fall hazards and traversability, as well as\ndigital applications such as animation and gaming. Existing methods for tree\nreconstruction rely on feature identification (trunk, crown, etc) to\nheuristically segment a forest into individual trees and generate a branch\nstructure graph, limiting their application to sparse trees and uniform\nforests. However, the natural world is a messy place in which trees present\nwith significant heterogeneity and are frequently encroached upon by the\nsurrounding environment. We present a general method for extracting the branch\nstructure of trees from point cloud data, which estimates the structure of\ntrees by adapting the methods of structural topology optimisation to find the\noptimal material distribution to support wind-loading. We present the results\nof this optimisation over a wide variety of scans, and discuss the benefits and\ndrawbacks of this novel approach to tree structure reconstruction. Despite the\nhigh variability of datasets containing trees, and the high rate of occlusions,\nour method generates detailed and accurate tree structures in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_T/0/1/0/all/0/1\">Thomas Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinskier_J/0/1/0/all/0/1\">Joshua Pinskier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Pyramid Correlation Network for Liver Tumor Segmentation from CT images. (arXiv:2205.13199v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13199","description":"<p>Purpose: Automated liver tumor segmentation from Computed Tomography (CT)\nimages is a necessary prerequisite in the interventions of hepatic\nabnormalities and surgery planning. However, accurate liver tumor segmentation\nremains challenging due to the large variability of tumor sizes and\ninhomogeneous texture. Recent advances based on Fully Convolutional Network\n(FCN) for medical image segmentation drew on the success of learning\ndiscriminative pyramid features. In this paper, we propose a Decoupled Pyramid\nCorrelation Network (DPC-Net) that exploits attention mechanisms to fully\nleverage both low- and high-level features embedded in FCN to segment liver\ntumor. Methods: We first design a powerful Pyramid Feature Encoder (PFE) to\nextract multi-level features from input images. Then we decouple the\ncharacteristics of features concerning spatial dimension (i.e., height, width,\ndepth) and semantic dimension (i.e., channel). On top of that, we present two\ntypes of attention modules, Spatial Correlation (SpaCor) and Semantic\nCorrelation (SemCor) modules, to recursively measure the correlation of\nmulti-level features. The former selectively emphasizes global semantic\ninformation in low-level features with the guidance of high-level ones. The\nlatter adaptively enhance spatial details in high-level features with the\nguidance of low-level ones. Results: We evaluate the DPC-Net on MICCAI 2017\nLiTS Liver Tumor Segmentation (LiTS) challenge dataset. Dice Similarity\nCoefficient (DSC) and Average Symmetric Surface Distance (ASSD) are employed\nfor evaluation. The proposed method obtains a DSC of 76.4% and an ASSD of 0.838\nmm for liver tumor segmentation, outperforming the state-of-the-art methods. It\nalso achieves a competitive results with a DSC of 96.0% and an ASSD of 1.636 mm\nfor liver segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiawei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13213","description":"<p>Vision Transformers (ViTs) have triggered the most recent and significant\nbreakthroughs in computer vision. Their efficient designs are mostly guided by\nthe indirect metric of computational complexity, i.e., FLOPs, which however has\na clear gap with the direct metric such as throughput. Thus, we propose to use\nthe direct speed evaluation on the target platform as the design principle for\nefficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT\nwhich performs favourably against the existing state-of-the-art methods across\na spectrum of different model sizes with faster speed. At the core of LITv2 is\na novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the\ninsight that high frequencies in an image capture local fine details and low\nfrequencies focus on global structures, whereas a multi-head self-attention\nlayer neglects the characteristic of different frequencies. Therefore, we\npropose to disentangle the high/low frequency patterns in an attention layer by\nseparating the heads into two groups, where one group encodes high frequencies\nvia self-attention within each local window, and another group performs the\nattention to model the global relationship between the average-pooled\nlow-frequency keys from each window and each query position in the input\nfeature map. Benefit from the efficient design for both groups, we show that\nHiLo is superior to the existing attention mechanisms by comprehensively\nbenchmarking on FLOPs, speed and memory consumption on GPUs. Powered by HiLo,\nLITv2 serves as a strong backbone for mainstream vision tasks including image\nclassification, dense detection and segmentation. Code is available at\nhttps://github.com/zip-group/LITv2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning. (arXiv:2205.13218v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13218","description":"<p>Real-world applications require the classification model to adapt to new\nclasses without forgetting old ones. Correspondingly, Class-Incremental\nLearning (CIL) aims to train a model with limited memory size to meet this\nrequirement. Typical CIL methods tend to save representative exemplars from\nformer classes to resist forgetting, while recent works find that storing\nmodels from history can substantially boost the performance. However, the\nstored models are not counted into the memory budget, which implicitly results\nin unfair comparisons. We find that when counting the model size into the total\nbudget and comparing methods with aligned memory size, saving models do not\nconsistently work, especially for the case with limited memory budgets. As a\nresult, we need to holistically evaluate different CIL methods at different\nmemory scales and simultaneously consider accuracy and memory size for\nmeasurement. On the other hand, we dive deeply into the construction of the\nmemory buffer for memory efficiency. By analyzing the effect of different\nlayers in the network, we find that shallow and deep layers have different\ncharacteristics in CIL. Motivated by this, we propose a simple yet effective\nbaseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends\nspecialized layers based on the shared generalized representations, efficiently\nextracting diverse representations with modest cost and maintaining\nrepresentative exemplars. Extensive experiments on benchmark datasets validate\nMEMO's competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Penalizing Proposals using Classifiers for Semi-Supervised Object Detection. (arXiv:2205.13219v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13219","description":"<p>Obtaining gold standard annotated data for object detection is often costly,\ninvolving human-level effort. Semi-supervised object detection algorithms solve\nthe problem with a small amount of gold-standard labels and a large unlabelled\ndataset used to generate silver-standard labels. But training on the silver\nstandard labels does not produce good results, because they are\nmachine-generated annotations. In this work, we design a modified loss function\nto train on large silver standard annotated sets generated by a weak annotator.\nWe include a confidence metric associated with the annotation as an additional\nterm in the loss function, signifying the quality of the annotation. We test\nthe effectiveness of our approach on various test sets and use numerous\nvariations to compare the results with some of the current approaches to object\ndetection. In comparison with the baseline where no confidence metric is used,\nwe achieved a 4\\% gain in mAP with 25\\% labeled data and 10\\% gain in mAP with\n50\\% labeled data by using the proposed confidence metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazra_S/0/1/0/all/0/1\">Somnath Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1\">Pallab Dasgupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DGSVis: Visual Analysis of Hierarchical Snapshots in Dynamic Graph. (arXiv:2205.13220v1 [cs.HC])","link":"http://arxiv.org/abs/2205.13220","description":"<p>Dynamic graph visualization attracts researchers' concentration as it\nrepresents time-varying relationships between entities in multiple domains\n(e.g., social media analysis, academic cooperation analysis, team sports\nanalysis). Integrating visual analytic methods is consequential in presenting,\ncomparing, and reviewing dynamic graphs. Even though dynamic graph\nvisualization is developed for many years, how to effectively visualize\nlarge-scale and time-intensive dynamic graph data with subtle changes is still\nchallenging for researchers. To provide an effective analysis method for this\ntype of dynamic graph data, we propose a snapshot generation algorithm\ninvolving Human-In-Loop to help users divide the dynamic graphs into\nmulti-granularity and hierarchical snapshots for further analysis. In addition,\nwe design a visual analysis prototype system (DGSVis) to assist users in\naccessing the dynamic graph insights effectively. DGSVis integrates a graphical\noperation interface to help users generate snapshots visually and\ninteractively. It is equipped with the overview and details for visualizing\nhierarchical snapshots of the dynamic graph data. To illustrate the usability\nand efficiency of our proposed methods for this type of dynamic graph data, we\nintroduce two case studies based on basketball player networks in a\ncompetition. In addition, we conduct an evaluation and receive exciting\nfeedback from experienced visualization experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baofeng Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Censor-aware Semi-supervised Learning for Survival Time Prediction from Medical Images. (arXiv:2205.13226v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13226","description":"<p>Survival time prediction from medical images is important for treatment\nplanning, where accurate estimations can improve healthcare quality. One issue\naffecting the training of survival models is censored data. Most of the current\nsurvival prediction approaches are based on Cox models that can deal with\ncensored data, but their application scope is limited because they output a\nhazard function instead of a survival time. On the other hand, methods that\npredict survival time usually ignore censored data, resulting in an\nunder-utilization of the training set. In this work, we propose a new training\nmethod that predicts survival time using all censored and uncensored data. We\npropose to treat censored data as samples with a lower-bound time to death and\nestimate pseudo labels to semi-supervise a censor-aware survival time\nregressor. We evaluate our method on pathology and x-ray images from the\nTCGA-GM and NLST datasets. Our results establish the state-of-the-art survival\nprediction accuracy on both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hermoza_R/0/1/0/all/0/1\">Renato Hermoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maicas_G/0/1/0/all/0/1\">Gabriel Maicas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_J/0/1/0/all/0/1\">Jacinto C. Nascimento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denial-of-Service Attacks on Learned Image Compression. (arXiv:2205.13253v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13253","description":"<p>Deep learning techniques have shown promising results in image compression,\nwith competitive bitrate and image reconstruction quality from compressed\nlatent. However, while image compression has progressed towards higher peak\nsignal-to-noise ratio (PSNR) and fewer bits per pixel (bpp), their robustness\nto corner-case images has never received deliberation. In this work, we, for\nthe first time, investigate the robustness of image compression systems where\nimperceptible perturbation of input images can precipitate a significant\nincrease in the bitrate of their compressed latent. To characterize the\nrobustness of state-of-the-art learned image compression, we mount white and\nblack-box attacks. Our results on several image compression models with various\nbitrate qualities show that they are surprisingly fragile, where the white-box\nattack achieves up to 56.326x and black-box 1.947x bpp change. To improve\nrobustness, we propose a novel model which incorporates attention modules and a\nbasic factorized entropy model, resulting in a promising trade-off between the\nPSNR/bpp ratio and robustness to adversarial attacks that surpasses existing\nlearned image compressors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Benjamin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddharth Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Customized Self-Supervised Pre-training with Scalable Dynamic Routing. (arXiv:2205.13267v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13267","description":"<p>Self-supervised learning (SSL), especially contrastive methods, has raised\nattraction recently as it learns effective transferable representations without\nsemantic annotations. A common practice for self-supervised pre-training is to\nuse as much data as possible. For a specific downstream task, however,\ninvolving irrelevant data in pre-training may degenerate the downstream\nperformance, observed from our extensive experiments. On the other hand, for\nexisting SSL methods, it is burdensome and infeasible to use different\ndownstream-task-customized datasets in pre-training for different tasks. To\naddress this issue, we propose a novel SSL paradigm called Scalable Dynamic\nRouting (SDR), which can be trained once and deployed efficiently to different\ndownstream tasks with task-customized pre-trained models. Specifically, we\nconstruct the SDRnet with various sub-nets and train each sub-net with only one\nsubset of the data by data-aware progressive training. When a downstream task\narrives, we route among all the pre-trained sub-nets to get the best along with\nits corresponding weights. Experiment results show that our SDR can train 256\nsub-nets on ImageNet simultaneously, which provides better transfer performance\nthan a unified model trained on the full ImageNet, achieving state-of-the-art\n(SOTA) averaged accuracy over 11 downstream classification tasks and AP on\nPASCAL VOC detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhili Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemeTector: Enforcing deep focus for meme detection. (arXiv:2205.13268v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13268","description":"<p>Image memes and specifically their widely-known variation image macros, is a\nspecial new media type that combines text with images and is used in social\nmedia to playfully or subtly express humour, irony, sarcasm and even hate. It\nis important to accurately retrieve image memes from social media to better\ncapture the cultural and social aspects of online phenomena and detect\npotential issues (hate-speech, disinformation). Essentially, the background\nimage of an image macro is a regular image easily recognized as such by humans\nbut cumbersome for the machine to do so due to feature map similarity with the\ncomplete image macro. Hence, accumulating suitable feature maps in such cases\ncan lead to deep understanding of the notion of image memes. To this end, we\npropose a methodology that utilizes the visual part of image memes as instances\nof the regular image class and the initial image memes as instances of the\nimage meme class to force the model to concentrate on the critical parts that\ncharacterize an image meme. Additionally, we employ a trainable attention\nmechanism on top of a standard ViT architecture to enhance the model's ability\nto focus on these critical parts and make the predictions interpretable.\nSeveral training and test scenarios involving web-scraped regular images of\ncontrolled text presence are considered in terms of model robustness and\naccuracy. The findings indicate that light visual part utilization combined\nwith sufficient text presence during training provides the best and most robust\nmodel, surpassing state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1\">Christos Koutlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schinas_M/0/1/0/all/0/1\">Manos Schinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multi-object Segmentation Using Attention and Soft-argmax. (arXiv:2205.13271v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13271","description":"<p>We introduce a new architecture for unsupervised object-centric\nrepresentation learning and multi-object detection and segmentation, which uses\nan attention mechanism to associate a feature vector to each object present in\nthe scene and to predict the coordinates of these objects using soft-argmax. A\ntransformer encoder handles occlusions and redundant detections, and a separate\npre-trained background model is in charge of background reconstruction. We show\nthat this architecture significantly outperforms the state of the art on\ncomplex synthetic benchmarks and provide examples of applications to real-world\ntraffic videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sauvalle_B/0/1/0/all/0/1\">Bruno Sauvalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortelle_A/0/1/0/all/0/1\">Arnaud de La Fortelle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices. (arXiv:2205.13272v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13272","description":"<p>IoT devices suffer from resource limitations, such as processor, RAM, and\ndisc storage. These limitations become more evident when handling demanding\napplications, such as deep learning, well-known for their heavy computational\nrequirements. A case in point is robot pose estimation, an application that\npredicts the critical points of the desired image object. One way to mitigate\nprocessing and storage problems is compressing that deep learning application.\nThis paper proposes a new CNN for the pose estimation while applying the\ncompression techniques of pruning and quantization to reduce his demands and\nimprove the response time. While the pruning process reduces the total number\nof parameters required for inference, quantization decreases the precision of\nthe floating-point. We run the approach using a pose estimation task for a\nrobotic arm and compare the results in a high-end device and a constrained\ndevice. As metrics, we consider the number of Floating-point Operations Per\nSecond(FLOPS), the total of mathematical computations, the calculation of\nparameters, the inference time, and the number of video frames processed per\nsecond. In addition, we undertake a qualitative evaluation where we compare the\noutput image predicted for each pruned network with the corresponding original\none. We reduce the originally proposed network to a 70% pruning rate, implying\nan 88.86% reduction in parameters, 94.45% reduction in FLOPS, and for the disc\nstorage, we reduced the requirement in 70% while increasing error by a mere\n$1\\%$. With regard input image processing, this metric increases from 11.71 FPS\nto 41.9 FPS for the Desktop case. When using the constrained device, image\nprocessing augmented from 2.86 FPS to 10.04 FPS. The higher processing rate of\nimage frames achieved by the proposed approach allows a much shorter response\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dantas_M/0/1/0/all/0/1\">Marrone Silv&#xe9;rio Melo Dantas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_I/0/1/0/all/0/1\">Iago Richard Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filho_A/0/1/0/all/0/1\">Assis Tiago Oliveira Filho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_G/0/1/0/all/0/1\">Gibson Barbosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezerra_D/0/1/0/all/0/1\">Daniel Bezerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadok_D/0/1/0/all/0/1\">Djamel F. H. Sadok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelner_J/0/1/0/all/0/1\">Judith Kelner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquezini_M/0/1/0/all/0/1\">Maria Marquezini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Ricardo Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued Convolutional Neural Networks. (arXiv:2205.13273v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13273","description":"<p>This paper features convolutional neural networks defined on hypercomplex\nalgebras applied to classify lymphocytes in blood smear digital microscopic\nimages. Such classification is helpful for the diagnosis of acute lymphoblast\nleukemia (ALL), a type of blood cancer. We perform the classification task\nusing eight hypercomplex-valued convolutional neural networks (HvCNNs) along\nwith real-valued convolutional networks. Our results show that HvCNNs perform\nbetter than the real-valued model, showcasing higher accuracy with a much\nsmaller number of parameters. Moreover, we found that HvCNNs based on Clifford\nalgebras processing HSV-encoded images attained the highest observed\naccuracies. Precisely, our HvCNN yielded an average accuracy rate of 96.6%\nusing the ALL-IDB2 dataset with a 50% train-test split, a value extremely close\nto the state-of-the-art models but using a much simpler architecture with\nsignificantly fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vieira_G/0/1/0/all/0/1\">Guilherme Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1\">Marcos Eduardo Valle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIDI: A Video Dataset of Incidents. (arXiv:2205.13277v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13277","description":"<p>Automatic detection of natural disasters and incidents has become more\nimportant as a tool for fast response. There have been many studies to detect\nincidents using still images and text. However, the number of approaches that\nexploit temporal information is rather limited. One of the main reasons for\nthis is that a diverse video dataset with various incident types does not\nexist. To address this need, in this paper we present a video dataset, Video\nDataset of Incidents, VIDI, that contains 4,534 video clips corresponding to 43\nincident categories. Each incident class has around 100 videos with a duration\nof ten seconds on average. To increase diversity, the videos have been searched\nin several languages. To assess the performance of the recent state-of-the-art\napproaches, Vision Transformer and TimeSformer, as well as to explore the\ncontribution of video-based information for incident classification, we\nperformed benchmark experiments on the VIDI and Incidents Dataset. We have\nshown that the recent methods improve the incident classification accuracy. We\nhave found that employing video data is very beneficial for the task. By using\nthe video data, the top-1 accuracy is increased to 76.56% from 67.37%, which\nwas obtained using a single frame. VIDI will be made publicly available.\nAdditional materials can be found at the following link:\nhttps://github.com/vididataset/VIDI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sesver_D/0/1/0/all/0/1\">Duygu Sesver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencoglu_A/0/1/0/all/0/1\">Alp Eren Gen&#xe7;o&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildiz_C/0/1/0/all/0/1\">&#xc7;a&#x11f;r&#x131; Emre Y&#x131;ld&#x131;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunindi_Z/0/1/0/all/0/1\">Zehra G&#xfc;nindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habibi_F/0/1/0/all/0/1\">Faeze Habibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazici_Z/0/1/0/all/0/1\">Ziya Ata Yaz&#x131;c&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation for Thermal Images: A Comparative Survey. (arXiv:2205.13278v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13278","description":"<p>Semantic segmentation is a challenging task since it requires excessively\nmore low-level spatial information of the image compared to other computer\nvision problems. The accuracy of pixel-level classification can be affected by\nmany factors, such as imaging limitations and the ambiguity of object\nboundaries in an image. Conventional methods exploit three-channel RGB images\ncaptured in the visible spectrum with deep neural networks (DNN). Thermal\nimages can significantly contribute during the segmentation since thermal\nimaging cameras are capable of capturing details despite the weather and\nillumination conditions. Using infrared spectrum in semantic segmentation has\nmany real-world use cases, such as autonomous driving, medical imaging,\nagriculture, defense industry, etc. Due to this wide range of use cases,\ndesigning accurate semantic segmentation algorithms with the help of infrared\nspectrum is an important challenge. One approach is to use both visible and\ninfrared spectrum images as inputs. These methods can accomplish higher\naccuracy due to enriched input information, with the cost of extra effort for\nthe alignment and processing of multiple inputs. Another approach is to use\nonly thermal images, enabling less hardware cost for smaller use cases. Even\nthough there are multiple surveys on semantic segmentation methods, the\nliterature lacks a comprehensive survey centered explicitly around semantic\nsegmentation using infrared spectrum. This work aims to fill this gap by\npresenting algorithms in the literature and categorizing them by their input\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kutuk_Z/0/1/0/all/0/1\">Z&#xfc;lfiye K&#xfc;t&#xfc;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Algan_G/0/1/0/all/0/1\">G&#xf6;rkem Algan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Objects Matter: Learning Object Relation Graph for Robust Camera Relocalization. (arXiv:2205.13280v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13280","description":"<p>Visual relocalization aims to estimate the pose of a camera from one or more\nimages. In recent years deep learning based pose regression methods have\nattracted many attentions. They feature predicting the absolute poses without\nrelying on any prior built maps or stored images, making the relocalization\nvery efficient. However, robust relocalization under environments with complex\nappearance changes and real dynamics remains very challenging. In this paper,\nwe propose to enhance the distinctiveness of the image features by extracting\nthe deep relationship among objects. In particular, we extract objects in the\nimage and construct a deep object relation graph (ORG) to incorporate the\nsemantic connections and relative spatial clues of the objects. We integrate\nour ORG module into several popular pose regression models. Extensive\nexperiments on various public indoor and outdoor datasets demonstrate that our\nmethod improves the performance significantly and outperforms the previous\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chengyu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinglu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surround-view Fisheye Camera Perception for Automated Driving: Overview, Survey and Challenges. (arXiv:2205.13281v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13281","description":"<p>Surround-view fisheye cameras are commonly used for near-field sensing in\nautomated driving. Four fisheye cameras on four sides of the vehicle are\nsufficient to cover 360{\\deg} around the vehicle capturing the entire\nnear-field region. Some primary use cases are automated parking, traffic jam\nassist, and urban driving. There are limited datasets and very little work on\nnear-field perception tasks as the main focus in automotive perception is on\nfar-field perception. In contrast to far-field, surround-view perception poses\nadditional challenges due to high precision object detection requirements of\n10cm and partial visibility of objects. Due to the large radial distortion of\nfisheye cameras, standard algorithms can not be extended easily to the\nsurround-view use case. Thus we are motivated to provide a self-contained\nreference for automotive fisheye camera perception for researchers and\npractitioners. Firstly, we provide a unified and taxonomic treatment of\ncommonly used fisheye camera models. Secondly, we discuss various perception\ntasks and existing literature. Finally, we discuss the challenges and future\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1\">Ciaran Eising</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1\">Christian Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Eigenvalues of Global Covariance Pooling for Fine-grained Visual Recognition. (arXiv:2205.13282v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13282","description":"<p>The Fine-Grained Visual Categorization (FGVC) is challenging because the\nsubtle inter-class variations are difficult to be captured. One notable\nresearch line uses the Global Covariance Pooling (GCP) layer to learn powerful\nrepresentations with second-order statistics, which can effectively model\ninter-class differences. In our previous conference paper, we show that\ntruncating small eigenvalues of the GCP covariance can attain smoother gradient\nand improve the performance on large-scale benchmarks. However, on fine-grained\ndatasets, truncating the small eigenvalues would make the model fail to\nconverge. This observation contradicts the common assumption that the small\neigenvalues merely correspond to the noisy and unimportant information.\nConsequently, ignoring them should have little influence on the performance. To\ndiagnose this peculiar behavior, we propose two attribution methods whose\nvisualizations demonstrate that the seemingly unimportant small eigenvalues are\ncrucial as they are in charge of extracting the discriminative class-specific\nfeatures. Inspired by this observation, we propose a network branch dedicated\nto magnifying the importance of small eigenvalues. Without introducing any\nadditional parameters, this branch simply amplifies the small eigenvalues and\nachieves state-of-the-art performances of GCP methods on three fine-grained\nbenchmarks. Furthermore, the performance is also competitive against other FGVC\napproaches on larger datasets. Code is available at\n\\href{https://github.com/KingJamesSong/DifferentiableSVD}{https://github.com/KingJamesSong/DifferentiableSVD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analytical Interpretation of Latent Codes in InfoGAN with SAR Images. (arXiv:2205.13294v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13294","description":"<p>Generative Adversarial Networks (GANs) can synthesize abundant\nphoto-realistic synthetic aperture radar (SAR) images. Some recent GANs (e.g.,\nInfoGAN), are even able to edit specific properties of the synthesized images\nby introducing latent codes. It is crucial for SAR image synthesis since the\ntargets in real SAR images are with different properties due to the imaging\nmechanism. Despite the success of InfoGAN in manipulating properties, there\nstill lacks a clear explanation of how these latent codes affect synthesized\nproperties, thus editing specific properties usually relies on empirical\ntrials, unreliable and time-consuming. In this paper, we show that latent codes\nare disentangled to affect the properties of SAR images in a non-linear manner.\nBy introducing some property estimators for latent codes, we are able to\nprovide a completely analytical nonlinear model to decompose the entangled\ncausality between latent codes and different properties. The qualitative and\nquantitative experimental results further reveal that the properties can be\ncalculated by latent codes, inversely, the satisfying latent codes can be\nestimated given desired properties. In this case, properties can be manipulated\nby latent codes as we expect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhenpeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dakovic_M/0/1/0/all/0/1\">Milos Dakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Hongbing Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingzhe Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankovic_L/0/1/0/all/0/1\">Ljubisa Stankovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Interpretable Tree for Pedestrian Trajectory Prediction. (arXiv:2205.13296v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13296","description":"<p>Understanding the multiple socially-acceptable future behaviors is an\nessential task for many vision applications. In this paper, we propose a\ntree-based method, termed as Social Interpretable Tree (SIT), to address this\nmulti-modal prediction task, where a hand-crafted tree is built depending on\nthe prior information of observed trajectory to model multiple future\ntrajectories. Specifically, a path in the tree from the root to leaf represents\nan individual possible future trajectory. SIT employs a coarse-to-fine\noptimization strategy, in which the tree is first built by high-order velocity\nto balance the complexity and coverage of the tree and then optimized greedily\nto encourage multimodality. Finally, a teacher-forcing refining operation is\nused to predict the final fine trajectory. Compared with prior methods which\nleverage implicit latent variables to represent possible future trajectories,\nthe path in the tree can explicitly explain the rough moving behaviors (e.g.,\ngo straight and then turn right), and thus provides better interpretability.\nDespite the hand-crafted tree, the experimental results on ETH-UCY and Stanford\nDrone datasets demonstrate that our method is capable of matching or exceeding\nthe performance of state-of-the-art methods. Interestingly, the experiments\nshow that the raw built tree without training outperforms many prior deep\nneural network based approaches. Meanwhile, our method presents sufficient\nflexibility in long-term prediction and different best-of-$K$ predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Liushuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepTechnome: Mitigating Unknown Bias in Deep Learning Based Assessment of CT Images. (arXiv:2205.13297v1 [eess.IV])","link":"http://arxiv.org/abs/2205.13297","description":"<p>Reliably detecting diseases using relevant biological information is crucial\nfor real-world applicability of deep learning techniques in medical imaging. We\ndebias deep learning models during training against unknown bias - without\npreprocessing/filtering the input beforehand or assuming specific knowledge\nabout its distribution or precise nature in the dataset. We use control regions\nas surrogates that carry information regarding the bias, employ the classifier\nmodel to extract features, and suppress biased intermediate features with our\ncustom, modular DecorreLayer. We evaluate our method on a dataset of 952 lung\ncomputed tomography scans by introducing simulated biases w.r.t. reconstruction\nkernel and noise level and propose including an adversarial test set in\nevaluations of bias reduction techniques. In a moderately sized model\narchitecture, applying the proposed method to learn from data exhibiting a\nstrong bias, it near-perfectly recovers the classification performance observed\nwhen training with corresponding unbiased data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Langer_S/0/1/0/all/0/1\">Simon Langer</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Taubmann_O/0/1/0/all/0/1\">Oliver Taubmann</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1\">Felix Denzinger</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Muhlberg_A/0/1/0/all/0/1\">Alexander M&#xfc;hlberg</a> (2) ((1) Pattern Recognition Lab, Friedrich-Alexander-Universit&#xe4;t Erlangen-N&#xfc;rnberg, Germany, (2) Siemens Healthcare GmbH, Forchheim, Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SARS-CoV-2 Result Interpretation based on Image Analysis of Lateral Flow Devices. (arXiv:2205.13311v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13311","description":"<p>The widely used gene quantisation technique, Lateral Flow Device (LFD), is\nnow commonly used to detect the presence of SARS-CoV-2. It is enabling the\ncontrol and prevention of the spread of the virus. Depending on the viral load,\nLFD have different sensitivity and self-test for normal user present additional\nchallenge to interpret the result. With the evolution of machine learning\nalgorithms, image processing and analysis has seen unprecedented growth. In\nthis interdisciplinary study, we employ novel image analysis methods of\ncomputer vision and machine learning field to study visual features of the\ncontrol region of LFD. Here, we automatically derive results for any image\ncontaining LFD into positive, negative or inconclusive. This will reduce the\nburden of human involvement of health workers and perception bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vashistha_N/0/1/0/all/0/1\">Neeraj Vashistha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Architecture Self-supervised Video Representation Learning. (arXiv:2205.13313v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13313","description":"<p>In this paper, we present a new cross-architecture contrastive learning\n(CACL) framework for self-supervised video representation learning. CACL\nconsists of a 3D CNN and a video transformer which are used in parallel to\ngenerate diverse positive pairs for contrastive learning. This allows the model\nto learn strong representations from such diverse yet meaningful pairs.\nFurthermore, we introduce a temporal self-supervised learning module able to\npredict an Edit distance explicitly between two video sequences in the temporal\norder. This enables the model to learn a rich temporal representation that\ncompensates strongly to the video-level representation learned by the CACL. We\nevaluate our method on the tasks of video retrieval and action recognition on\nUCF101 and HMDB51 datasets, where our method achieves excellent performance,\nsurpassing the state-of-the-art methods such as VideoMoCo and MoCo+BE by a\nlarge margin. The code is made available at https://github.com/guoshengcv/CACL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zihua Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaobo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13326","description":"<p>This paper describes the methods submitted for evaluation to the SHREC 2022\ntrack on pothole and crack detection in the road pavement. A total of 7\ndifferent runs for the semantic segmentation of the road surface are compared,\n6 from the participants plus a baseline method. All methods exploit Deep\nLearning techniques and their performance is tested using the same environment\n(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic\nsegmentation image/mask pairs and 797 RGB-D video clips collected with the\nlatest depth cameras was made available to the participants. The methods are\nthen evaluated on the 496 image/mask pairs in the validation set, on the 504\npairs in the test set and finally on 8 video clips. The analysis of the results\nis based on quantitative metrics for image segmentation and qualitative\nanalysis of the video clips. The participation and the results show that the\nscenario is of great interest and that the use of RGB-D data is still\nchallenging in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_E/0/1/0/all/0/1\">Elia Moscoso Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranieri_A/0/1/0/all/0/1\">Andrea Ranieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biasotti_S/0/1/0/all/0/1\">Silvia Biasotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chicchon_M/0/1/0/all/0/1\">Miguel Chicchon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sipiran_I/0/1/0/all/0/1\">Ivan Sipiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh-Khoi Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Ho_T/0/1/0/all/0/1\">Thang-Long Nguyen-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai-Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13331","description":"<p>This paper deals with deep transductive learning, and proposes TransBoost as\na procedure for fine-tuning any deep neural model to improve its performance on\nany (unlabeled) test set provided at training time. TransBoost is inspired by a\nlarge margin principle and is efficient and simple to use. The ImageNet\nclassification performance is consistently and significantly improved with\nTransBoost on many architectures such as ResNets, MobileNetV3-L,\nEfficientNetB0, ViT-S, and ConvNext-T. Additionally we show that TransBoost is\neffective on a wide variety of image classification datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belhasin_O/0/1/0/all/0/1\">Omer Belhasin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Shalom_G/0/1/0/all/0/1\">Guy Bar-Shalom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Yaniv_R/0/1/0/all/0/1\">Ran El-Yaniv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning What and Where -- Unsupervised Disentangling Location and Identity Tracking. (arXiv:2205.13349v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13349","description":"<p>Our brain can almost effortlessly decompose visual data streams into\nbackground and salient objects. Moreover, it can track the objects and\nanticipate their motion and interactions. In contrast, recent object reasoning\ndatasets, such as CATER, have revealed fundamental shortcomings of current\nvision-based AI systems, particularly when targeting explicit object encodings,\nobject permanence, and object reasoning. We introduce an unsupervised\ndisentangled LOCation and Identity tracking system (Loci), which excels on the\nCATER tracking challenge. Inspired by the dorsal-ventral pathways in the brain,\nLoci tackles the what-and-where binding problem by means of a self-supervised\nsegregation mechanism. Our autoregressive neural network partitions and\ndistributes the visual input stream across separate, identically-parameterized\nand autonomously recruited neural network modules. Each module binds what with\nwhere, that is, compressed Gestalt encodings with locations. On the deep latent\nencoding levels interaction dynamics are processed. Besides exhibiting superior\nperformance in current benchmarks, we propose that Loci may set the stage for\ndeeper, explanation-oriented video processing -- akin to some deeper networked\nprocesses in the brain that appear to integrate individual entity and\nspatiotemporal interaction dynamics into event structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Traub_M/0/1/0/all/0/1\">Manuel Traub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otte_S/0/1/0/all/0/1\">Sebastian Otte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menge_T/0/1/0/all/0/1\">Tobias Menge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlbauer_M/0/1/0/all/0/1\">Matthias Karlbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thummel_J/0/1/0/all/0/1\">Jannik Th&#xfc;mmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butz_M/0/1/0/all/0/1\">Martin V. Butz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Shot Face Reenactment on Megapixels. (arXiv:2205.13368v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13368","description":"<p>The goal of face reenactment is to transfer a target expression and head pose\nto a source face while preserving the source identity. With the popularity of\nface-related applications, there has been much research on this topic. However,\nthe results of existing methods are still limited to low-resolution and lack\nphotorealism. In this work, we present a one-shot and high-resolution face\nreenactment method called MegaFR. To be precise, we leverage StyleGAN by using\n3DMM-based rendering images and overcome the lack of high-quality video\ndatasets by designing a loss function that works without high-quality videos.\nAlso, we apply iterative refinement to deal with extreme poses and/or\nexpressions. Since the proposed method controls source images through 3DMM\nparameters, we can explicitly manipulate source images. We apply MegaFR to\nvarious applications such as face frontalization, eye in-painting, and talking\nhead generation. Experimental results show that our method successfully\ndisentangles identity from expression and head pose, and outperforms\nconventional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wonjun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Geonsu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1\">Hyung Il Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning. (arXiv:2205.13383v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13383","description":"<p>Deep neural networks are vulnerable to Trojan attacks. Existing attacks use\nvisible patterns (e.g., a patch or image transformations) as triggers, which\nare vulnerable to human inspection. In this paper, we propose stealthy and\nefficient Trojan attacks, BppAttack. Based on existing biology literature on\nhuman visual systems, we propose to use image quantization and dithering as the\nTrojan trigger, making imperceptible changes. It is a stealthy and efficient\nattack without training auxiliary models. Due to the small changes made to\nimages, it is hard to inject such triggers during training. To alleviate this\nproblem, we propose a contrastive learning based approach that leverages\nadversarial attacks to generate negative sample pairs so that the learned\ntrigger is precise and accurate. The proposed method achieves high attack\nsuccess rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and\nCelebA. It also effectively bypasses existing Trojan defenses and human\ninspection. Our code can be found in\nhttps://github.com/RU-System-Software-and-Security/BppAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1\">Juan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Visual Search with Backward Consistent Feature Embedding. (arXiv:2205.13384v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13384","description":"<p>In visual search, the gallery set could be incrementally growing and added to\nthe database in practice. However, existing methods rely on the model trained\non the entire dataset, ignoring the continual updating of the model. Besides,\nas the model updates, the new model must re-extract features for the entire\ngallery set to maintain compatible feature space, imposing a high computational\ncost for a large gallery set. To address the issues of long-term visual search,\nwe introduce a continual learning (CL) approach that can handle the\nincrementally growing gallery set with backward embedding consistency. We\nenforce the losses of inter-session data coherence, neighbor-session model\ncoherence, and intra-session discrimination to conduct a continual learner. In\naddition to the disjoint setup, our CL solution also tackles the situation of\nincreasingly adding new classes for the blurry boundary without assuming all\ncategories known in the beginning and during model update. To our knowledge,\nthis is the first CL method both tackling the issue of backward-consistent\nfeature embedding and allowing novel classes to occur in the new sessions.\nExtensive experiments on various benchmarks show the efficacy of our approach\nunder a wide range of setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_T/0/1/0/all/0/1\">Timmy S. T. Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tzer-Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Physical-World Adversarial Attack Against 3D Face Recognition. (arXiv:2205.13412v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13412","description":"<p>3D face recognition systems have been widely employed in intelligent\nterminals, among which structured light imaging is a common method to measure\nthe 3D shape. However, this method could be easily attacked, leading to\ninaccurate 3D face recognition. In this paper, we propose a novel,\nphysically-achievable attack on the fringe structured light system, named\nstructured light attack. The attack utilizes a projector to project optical\nadversarial fringes on faces to generate point clouds with well-designed\nnoises. We firstly propose a 3D transform-invariant loss function to enhance\nthe robustness of 3D adversarial examples in the physical-world attack. Then we\nreverse the 3D adversarial examples to the projector's input to place noises on\nphase-shift images, which models the process of structured light imaging. A\nreal-world structured light system is constructed for the attack and several\nstate-of-the-art 3D face recognition neural networks are tested. Experiments\nshow that our method can attack the physical system successfully and only needs\nminor modifications of projected images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation. (arXiv:2205.13425v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13425","description":"<p>Action classification has made great progress, but segmenting and recognizing\nactions from long untrimmed videos remains a challenging problem. Most\nstate-of-the-art methods focus on designing temporal convolution-based models,\nbut the limitations on modeling long-term temporal dependencies and\ninflexibility of temporal convolutions limit the potential of these models.\nRecently, Transformer-based models with flexible and strong sequence modeling\nability have been applied in various tasks. However, the lack of inductive bias\nand the inefficiency of handling long video sequences limit the application of\nTransformer in action segmentation. In this paper, we design a pure\nTransformer-based model without temporal convolutions by incorporating the\nU-Net architecture. The U-Transformer architecture reduces complexity while\nintroducing an inductive bias that adjacent frames are more likely to belong to\nthe same class, but the introduction of coarse resolutions results in the\nmisclassification of boundaries. We observe that the similarity distribution\nbetween a boundary frame and its neighboring frames depends on whether the\nboundary frame is the start or end of an action segment. Therefore, we further\npropose a boundary-aware loss based on the distribution of similarity scores\nbetween frames from attention modules to enhance the ability to recognize\nboundaries. Extensive experiments show the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dazhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhongang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Lingyu Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. (arXiv:2205.13445v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13445","description":"<p>Text-to-image generation and image captioning are recently emerged as a new\nexperimental paradigm to assess machine intelligence. They predict continuous\nquantity accompanied by their sampling techniques in the generation, making\nevaluation complicated and intractable to get marginal distributions. Based on\na recent trend that multimodal generative evaluations exploit a\nvison-and-language pre-trained model, we propose the negative Gaussian\ncross-mutual information using the CLIP features as a unified metric, coined by\nMutual Information Divergence (MID). To validate, we extensively compare it\nwith competing metrics using carefully-generated or human-annotated judgments\nin text-to-image generation and image captioning tasks. The proposed MID\nsignificantly outperforms the competitive methods by having consistency across\nbenchmarks, sample parsimony, and robustness toward the exploited CLIP model.\nWe look forward to seeing the underrepresented implications of the Gaussian\ncross-mutual information in multimodal representation learning and the future\nworks based on this novel proposition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual evaluation for lifelong learning: Identifying the stability gap. (arXiv:2205.13452v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13452","description":"<p>Introducing a time dependency on the data generating distribution has proven\nto be difficult for gradient-based training of neural networks, as the greedy\nupdates result in catastrophic forgetting of previous timesteps. Continual\nlearning aims to overcome the greedy optimization to enable continuous\naccumulation of knowledge over time. The data stream is typically divided into\nlocally stationary distributions, called tasks, allowing task-based evaluation\non held-out data from the training tasks. Contemporary evaluation protocols and\nmetrics in continual learning are task-based and quantify the trade-off between\nstability and plasticity only at task transitions. However, our empirical\nevidence suggests that between task transitions significant, temporary\nforgetting can occur, remaining unidentified in task-based evaluation.\nTherefore, we propose a framework for continual evaluation that establishes\nper-iteration evaluation and define a new set of metrics that enables\nidentifying the worst-case performance of the learner over its lifetime.\nPerforming continual evaluation, we empirically identify that replay suffers\nfrom a stability gap: upon learning a new task, there is a substantial but\ntransient decrease in performance on past tasks. Further conceptual and\nempirical analysis suggests not only replay-based, but also\nregularization-based continual learning methods are prone to the stability gap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_M/0/1/0/all/0/1\">Matthias De Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1\">Gido van de Ven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2D versus 3D Convolutional Spiking Neural Networks Trained with Unsupervised STDP for Human Action Recognition. (arXiv:2205.13474v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13474","description":"<p>Current advances in technology have highlighted the importance of video\nanalysis in the domain of computer vision. However, video analysis has\nconsiderably high computational costs with traditional artificial neural\nnetworks (ANNs). Spiking neural networks (SNNs) are third generation\nbiologically plausible models that process the information in the form of\nspikes. Unsupervised learning with SNNs using the spike timing dependent\nplasticity (STDP) rule has the potential to overcome some bottlenecks of\nregular artificial neural networks, but STDP-based SNNs are still immature and\ntheir performance is far behind that of ANNs. In this work, we study the\nperformance of SNNs when challenged with the task of human action recognition,\nbecause this task has many real-time applications in computer vision, such as\nvideo surveillance. In this paper we introduce a multi-layered 3D convolutional\nSNN model trained with unsupervised STDP. We compare the performance of this\nmodel to those of a 2D STDP-based SNN when challenged with the KTH and Weizmann\ndatasets. We also compare single-layer and multi-layer versions of these models\nin order to get an accurate assessment of their performance. We show that\nSTDP-based convolutional SNNs can learn motion patterns using 3D kernels, thus\nenabling motion-based recognition from videos. Finally, we give evidence that\n3D convolution is superior to 2D convolution with STDP-based SNNs, especially\nwhen dealing with long video sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+El_Assal_M/0/1/0/all/0/1\">Mireille El-Assal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirilly_P/0/1/0/all/0/1\">Pierre Tirilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilasco_I/0/1/0/all/0/1\">Ioan Marius Bilasco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Perceptual Color Differences of Smartphone Photography. (arXiv:2205.13489v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13489","description":"<p>Measuring perceptual color differences (CDs) is of great importance in modern\nsmartphone photography. Despite the long history, most CD measures have been\nconstrained by psychophysical data of homogeneous color patches or a limited\nnumber of simplistic natural images. It is thus questionable whether existing\nCD measures generalize in the age of smartphone photography characterized by\ngreater content complexities and learning-based image signal processors. In\nthis paper, we put together so far the largest image dataset for perceptual CD\nassessment, in which the natural images are 1) captured by six flagship\nsmartphones, 2) altered by Photoshop, 3) post-processed by built-in filters of\nthe smartphones, and 4) reproduced with incorrect color profiles. We then\nconduct a large-scale psychophysical experiment to gather perceptual CDs of\n30,000 image pairs in a carefully controlled laboratory environment. Based on\nthe newly established dataset, we make one of the first attempts to construct\nan end-to-end learnable CD formula based on a lightweight neural network, as a\ngeneralization of several previous metrics. Extensive experiments demonstrate\nthat the optimized formula outperforms 28 existing CD measures by a large\nmargin, offers reasonable local CD maps without the use of dense supervision,\ngeneralizes well to color patch data, and empirically behaves as a proper\nmetric in the mathematical sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Keshuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianlei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lihao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation. (arXiv:2205.13490v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13490","description":"<p>Conventional point cloud semantic segmentation methods usually employ an\nencoder-decoder architecture, where mid-level features are locally aggregated\nto extract geometric information. However, the over-reliance on these\nclass-agnostic local geometric representations may raise confusion between\nlocal parts from different categories that are similar in appearance or\nspatially adjacent. To address this issue, we argue that mid-level features can\nbe further enhanced with semantic information, and propose semantic-affine\ntransformation that transforms features of mid-level points belonging to\ndifferent categories with class-specific affine parameters. Based on this\ntechnique, we propose SemAffiNet for point cloud semantic segmentation, which\nutilizes the attention mechanism in the Transformer module to implicitly and\nexplicitly capture global structural knowledge within local parts for overall\ncomprehension of each category. We conduct extensive experiments on the\nScanNetV2 and NYUv2 datasets, and evaluate semantic-affine transformation on\nvarious 3D point cloud and 2D image segmentation baselines, where both\nqualitative and quantitative results demonstrate the superiority and\ngeneralization ability of our proposed approach. Code is available at\nhttps://github.com/wangzy22/SemAffiNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13515","description":"<p>We present an efficient approach for Masked Image Modeling (MIM) with\nhierarchical Vision Transformers (ViTs), e.g., Swin Transformer, allowing the\nhierarchical ViTs to discard masked patches and operate only on the visible\nones. Our approach consists of two key components. First, for the window\nattention, we design a Group Window Attention scheme following the\nDivide-and-Conquer strategy. To mitigate the quadratic complexity of the\nself-attention w.r.t. the number of patches, group attention encourages a\nuniform partition that visible patches within each local window of arbitrary\nsize can be grouped with equal size, where masked self-attention is then\nperformed within each group. Second, we further improve the grouping strategy\nvia the Dynamic Programming algorithm to minimize the overall computation cost\nof the attention on the grouped patches. As a result, MIM now can work on\nhierarchical ViTs in a green and efficient way. For example, we can train the\nhierarchical ViTs about 2.7$\\times$ faster and reduce the GPU memory usage by\n70%, while still enjoying competitive performance on ImageNet classification\nand the superiority on downstream COCO object detection benchmarks. Code and\npre-trained models have been made publicly available at\nhttps://github.com/LayneH/GreenMIM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamasaki_T/0/1/0/all/0/1\">Toshihiko Yamasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PREF: Phasorial Embedding Fields for Compact Neural Representations. (arXiv:2205.13524v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13524","description":"<p>We present a phasorial embedding field \\emph{PREF} as a compact\nrepresentation to facilitate neural signal modeling and reconstruction tasks.\nPure multi-layer perceptron (MLP) based neural techniques are biased towards\nlow frequency signals and have relied on deep layers or Fourier encoding to\navoid losing details. PREF instead employs a compact and physically explainable\nencoding field based on the phasor formulation of the Fourier embedding space.\nWe conduct a comprehensive theoretical analysis to demonstrate the advantages\nof PREF over the latest spatial embedding techniques. We then develop a highly\nefficient frequency learning framework using an approximated inverse Fourier\ntransform scheme for PREF along with a novel Parseval regularizer. Extensive\nexperiments show our compact PREF-based neural signal processing technique is\non par with the state-of-the-art in 2D image completion, 3D SDF surface\nregression, and 5D radiance field reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Binbin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xinhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anpei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition. (arXiv:2205.13535v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13535","description":"<p>Although the pre-trained Vision Transformers (ViTs) achieved great success in\ncomputer vision, adapting a ViT to various image and video tasks is challenging\nbecause of its heavy computation and storage burdens, where each model needs to\nbe independently and comprehensively fine-tuned to different tasks, limiting\nits transferability in different domains. To address this challenge, we propose\nan effective adaptation approach for Transformer, namely AdaptFormer, which can\nadapt the pre-trained ViTs into many different image and video tasks\nefficiently. It possesses several benefits more appealing than prior arts.\nFirstly, AdaptFormer introduces lightweight modules that only add less than 2%\nextra parameters to a ViT, while it is able to increase the ViT's\ntransferability without updating its original pre-trained parameters,\nsignificantly outperforming the existing 100% fully fine-tuned models on action\nrecognition benchmarks. Secondly, it can be plug-and-play in different\nTransformers and scalable to many visual tasks. Thirdly, extensive experiments\non five image and video datasets show that AdaptFormer largely improves ViTs in\nthe target domains. For example, when updating just 1.5% extra parameters, it\nachieves about 10% and 19% relative improvement compared to the fully\nfine-tuned models on Something-Something~v2 and HMDB51, respectively. Project\npage: <a href=\"http://www.shoufachen.com/adaptformer-page.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shoufa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chongjian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiangliu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation. (arXiv:2205.13542v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13542","description":"<p>Multi-sensor fusion is essential for an accurate and reliable autonomous\ndriving system. Recent approaches are based on point-level fusion: augmenting\nthe LiDAR point cloud with camera features. However, the camera-to-LiDAR\nprojection throws away the semantic density of camera features, hindering the\neffectiveness of such methods, especially for semantic-oriented tasks (such as\n3D scene segmentation). In this paper, we break this deeply-rooted convention\nwith BEVFusion, an efficient and generic multi-task multi-sensor fusion\nframework. It unifies multi-modal features in the shared bird's-eye view (BEV)\nrepresentation space, which nicely preserves both geometric and semantic\ninformation. To achieve this, we diagnose and lift key efficiency bottlenecks\nin the view transformation with optimized BEV pooling, reducing latency by more\nthan 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports\ndifferent 3D perception tasks with almost no architectural changes. It\nestablishes the new state of the art on nuScenes, achieving 1.3% higher mAP and\nNDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with\n1.9x lower computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Huizi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing the Dark Secrets of Masked Image Modeling. (arXiv:2205.13543v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13543","description":"<p>Masked image modeling (MIM) as pre-training is shown to be effective for\nnumerous vision downstream tasks, but how and where MIM works remain unclear.\nIn this paper, we compare MIM with the long-dominant supervised pre-trained\nmodels from two perspectives, the visualizations and the experiments, to\nuncover their key representational differences. From the visualizations, we\nfind that MIM brings locality inductive bias to all layers of the trained\nmodels, but supervised models tend to focus locally at lower layers but more\nglobally at higher layers. That may be the reason why MIM helps Vision\nTransformers that have a very large receptive field to optimize. Using MIM, the\nmodel can maintain a large diversity on attention heads in all layers. But for\nsupervised models, the diversity on attention heads almost disappears from the\nlast three layers and less diversity harms the fine-tuning performance. From\nthe experiments, we find that MIM models can perform significantly better on\ngeometric and motion tasks with weak semantics or fine-grained classification\ntasks, than their supervised counterparts. Without bells and whistles, a\nstandard MIM pre-trained SwinV2-L could achieve state-of-the-art performance on\npose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth\nestimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object\ntracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the\ncategories are sufficiently covered by the supervised pre-training, MIM models\ncan still achieve highly competitive transfer performance. With a deeper\nunderstanding of MIM, we hope that our work can inspire new and solid research\nin this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zigang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingcheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural Networks Acceleration. (arXiv:2006.12963v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.12963","description":"<p>Deep learning has achieved impressive results in many areas, but the\ndeployment of edge intelligent devices is still very slow. To solve this\nproblem, we propose a novel compression and acceleration method based on data\ndistribution characteristics for deep neural networks, namely Pruning Filter\nvia Gaussian Distribution Feature (PFGDF). Compared with previous advanced\npruning methods, PFGDF compresses the model by filters with insignificance in\ndistribution, regardless of the contribution and sensitivity information of the\nconvolution filter. PFGDF is significantly different from weight sparsification\npruning because it does not require the special accelerated library to process\nthe sparse weight matrix and introduces no more extra parameters. The pruning\nprocess of PFGDF is automated. Furthermore, the model compressed by PFGDF can\nrestore the same performance as the uncompressed model. We evaluate PFGDF\nthrough extensive experiments, on CIFAR-10, PFGDF compresses the convolution\nfilter on VGG-16 by 66.62% with more than 90% parameter reduced, while the\ninference time is accelerated by 83.73% on Huawei MATE 10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianrong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_B/0/1/0/all/0/1\">Boyu Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bifeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polygon-free: Unconstrained Scene Text Detection with Box Annotations. (arXiv:2011.13307v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13307","description":"<p>Although a polygon is a more accurate representation than an upright bounding\nbox for text detection, the annotations of polygons are extremely expensive and\nchallenging. Unlike existing works that employ fully-supervised training with\npolygon annotations, this study proposes an unconstrained text detection system\ntermed Polygon-free (PF), in which most existing polygon-based text detectors\n(e.g., PSENet [33],DB [16]) are trained with only upright bounding box\nannotations. Our core idea is to transfer knowledge from synthetic data to real\ndata to enhance the supervision information of upright bounding boxes. This is\nmade possible with a simple segmentation network, namely Skeleton Attention\nSegmentation Network (SASN), that includes three vital components (i.e.,\nchannel attention, spatial attention and skeleton attention map) and one soft\ncross-entropy loss. Experiments demonstrate that the proposed Polygonfree\nsystem can combine general detectors (e.g., EAST, PSENet, DB) to yield\nsurprisingly high-quality pixel-level results with only upright bounding box\nannotations on a variety of datasets (e.g., ICDAR2019-Art, TotalText,\nICDAR2015). For example, without using polygon annotations, PSENet achieves an\n80.5% F-score on TotalText [3] (vs. 80.9% of fully supervised counterpart),\n31.1% better than training directly with upright bounding box annotations, and\nsaves 80%+ labeling costs. We hope that PF can provide a new perspective for\ntext detection to reduce the labeling costs. The code can be found at\nhttps://github.com/weijiawu/Unconstrained-Text-Detection-with-Box-Supervisionand-Dynamic-Self-Training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.15081","description":"<p>We propose Deep Estimators of Features (DEFs), a learning-based framework for\npredicting sharp geometric features in sampled 3D shapes. Differently from\nexisting data-driven methods, which reduce this problem to feature\nclassification, we propose to regress a scalar field representing the distance\nfrom point samples to the closest feature line on local patches. Our approach\nis the first that scales to massive point clouds by fusing distance-to-feature\nestimates obtained on individual patches. We extensively evaluate our approach\nagainst related state-of-the-art methods on newly proposed synthetic and\nreal-world 3D CAD model benchmarks. Our approach not only outperforms these\n(with improvements in Recall and False Positives Rates), but generalizes to\nreal-world scans after training our model on synthetic data and fine-tuning it\non a small dataset of scanned data. We demonstrate a downstream application,\nwhere we reconstruct an explicit representation of straight and curved sharp\nfeature lines from range scan data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matveev_A/0/1/0/all/0/1\">Albert Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhimov_R/0/1/0/all/0/1\">Ruslan Rakhimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobrovskikh_G/0/1/0/all/0/1\">Gleb Bobrovskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egiazarian_V/0/1/0/all/0/1\">Vage Egiazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1\">Emil Bogomolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panozzo_D/0/1/0/all/0/1\">Daniele Panozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Blind Image Quality Assessment. (arXiv:2102.09717v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.09717","description":"<p>The explosive growth of image data facilitates the fast development of image\nprocessing and computer vision methods for emerging visual applications,\nmeanwhile introducing novel distortions to the processed images. This poses a\ngrand challenge to existing blind image quality assessment (BIQA) models,\nfailing to continually adapt to such subpopulation shift. Recent work suggests\ntraining BIQA methods on the combination of all available human-rated IQA\ndatasets. However, this type of approach is not scalable to a large number of\ndatasets, and is cumbersome to incorporate a newly created dataset as well. In\nthis paper, we formulate continual learning for BIQA, where a model learns\ncontinually from a stream of IQA datasets, building on what was learned from\npreviously seen data. We first identify five desiderata in the new setting with\na measure to quantify the plasticity-stability trade-off. We then propose a\nsimple yet effective method for learning BIQA models continually. Specifically,\nbased on a shared backbone network, we add a prediction head for a new dataset,\nand enforce a regularizer to allow all prediction heads to evolve with new data\nwhile being resistant to catastrophic forgetting of old data. We compute the\nquality score by an adaptive weighted summation of estimates from all\nprediction heads. Extensive experiments demonstrate the promise of the proposed\ncontinual learning method in comparison to standard training techniques for\nBIQA. We made the code publicly available at\nhttps://github.com/zwx8981/BIQA_CL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Little Energy Goes a Long Way: Build an Energy-Efficient, Accurate Spiking Neural Network from Convolutional Neural Network. (arXiv:2103.00944v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00944","description":"<p>Spiking neural networks (SNNs) offer an inherent ability to process\nspatial-temporal data, or in other words, realworld sensory data, but suffer\nfrom the difficulty of training high accuracy models. A major thread of\nresearch on SNNs is on converting a pre-trained convolutional neural network\n(CNN) to an SNN of the same structure. State-of-the-art conversion methods are\napproaching the accuracy limit, i.e., the near-zero accuracy loss of SNN\nagainst the original CNN. However, we note that this is made possible only when\nsignificantly more energy is consumed to process an input. In this paper, we\nargue that this trend of \"energy for accuracy\" is not necessary -- a little\nenergy can go a long way to achieve the near-zero accuracy loss. Specifically,\nwe propose a novel CNN-to-SNN conversion method that is able to use a\nreasonably short spike train (e.g., 256 timesteps for CIFAR10 images) to\nachieve the near-zero accuracy loss. The new conversion method, named as\nexplicit current control (ECC), contains three techniques (current\nnormalisation, thresholding for residual elimination, and consistency\nmaintenance for batch-normalisation), in order to explicitly control the\ncurrents flowing through the SNN when processing inputs. We implement ECC into\na tool nicknamed SpKeras, which can conveniently import Keras CNN models and\nconvert them into SNNs. We conduct an extensive set of experiments with the\ntool -- working with VGG16 and various datasets such as CIFAR10 and CIFAR100 --\nand compare with state-of-the-art conversion methods. Results show that ECC is\na promising method that can optimise over energy consumption and accuracy loss\nsimultaneously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dengyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xinping Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steerable 3D Spherical Neurons. (arXiv:2106.13863v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13863","description":"<p>Emerging from low-level vision theory, steerable filters found their\ncounterpart in prior work on steerable convolutional neural networks\nequivariant to rigid transformations. In our work, we propose a steerable\nfeed-forward learning-based approach that consists of neurons with spherical\ndecision surfaces and operates on point clouds. Such spherical neurons are\nobtained by conformal embedding of Euclidean space and have recently been\nrevisited in the context of learning representations of point sets. Focusing on\n3D geometry, we exploit the isometry property of spherical neurons and derive a\n3D steerability constraint. After training spherical neurons to classify point\nclouds in a canonical orientation, we use a tetrahedron basis to quadruplicate\nthe neurons and construct rotation-equivariant spherical filter banks. We then\napply the derived constraint to interpolate the filter bank outputs and, thus,\nobtain a rotation-invariant network. Finally, we use a synthetic point set and\nreal-world 3D skeleton data to verify our theoretical findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.03996","description":"<p>We propose to address quadrupedal locomotion tasks using Reinforcement\nLearning (RL) with a Transformer-based model that learns to combine\nproprioceptive information and high-dimensional depth sensor inputs. While\nlearning-based locomotion has made great advances using RL, most methods still\nrely on domain randomization for training blind agents that generalize to\nchallenging terrains. Our key insight is that proprioceptive states only offer\ncontact measurements for immediate reaction, whereas an agent equipped with\nvisual sensory observations can learn to proactively maneuver environments with\nobstacles and uneven terrain by anticipating changes in the environment many\nsteps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL\nmethod that leverages both proprioceptive states and visual observations for\nlocomotion control. We evaluate our method in challenging simulated\nenvironments with different obstacles and uneven terrain. We transfer our\nlearned policy from simulation to a real robot by running it indoors and in the\nwild with unseen obstacles and terrain. Our method not only significantly\nimproves over baselines, but also achieves far better generalization\nperformance, especially when transferred to the real robot. Our project page\nwith videos is at https://rchalyang.github.io/LocoTransformer/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Perceptual Locomotion on Uneven Terrains using Sparse Visual Observations. (arXiv:2109.14026v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.14026","description":"<p>To proactively navigate and traverse various terrains, active use of visual\nperception becomes indispensable. We aim to investigate the feasibility and\nperformance of using sparse visual observations to achieve perceptual\nlocomotion over a range of common terrains (steps, ramps, gaps, and stairs) in\nhuman-centered environments. We formulate a selection of sparse visual inputs\nsuitable for locomotion over the terrains of interest, and propose a learning\nframework to integrate exteroceptive and proprioceptive states. We specifically\ndesign the state observations and a training curriculum to learn feedback\ncontrol policies effectively over a range of different terrains. We extensively\nvalidate and benchmark the learned policy in various tasks: omnidirectional\nwalking on flat ground, and forward locomotion over various obstacles, showing\nhigh success rate of traversability. Furthermore, we study exteroceptive\nablations and evaluate policy generalization by adding various levels of noise\nand testing on new unseen terrains. We demonstrate the capabilities of\nautonomous perceptual locomotion that can be achieved by only using sparse\nvisual observations from direct depth measurements, which are easily available\nfrom a Lidar or RGB-D sensor, showing robust ascent and descent over high\nstairs of 20 cm height, i.e., 50% leg length, and robustness against noise and\nunseen terrains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acero_F/0/1/0/all/0/1\">Fernando Acero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kai Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhibin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Memorization of Noisy Labels via Regularization between Representations. (arXiv:2110.09022v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09022","description":"<p>Designing robust loss functions is popular in learning with noisy labels\nwhile existing designs did not explicitly consider the overfitting property of\ndeep neural networks (DNNs). As a result, applying these losses may still\nsuffer from overfitting/memorizing noisy labels as training proceeds. In this\npaper, we first theoretically analyze the memorization effect and show that a\nlower-capacity model may perform better on noisy datasets. However, it is\nnon-trivial to design a neural network with the best capacity given an\narbitrary task. To circumvent this dilemma, instead of changing the model\narchitecture, we decouple DNNs into an encoder followed by a linear classifier\nand propose to restrict the function space of a DNN by a representation\nregularizer. Particularly, we require the distance between two self-supervised\nfeatures to be positively related to the distance between the corresponding two\nsupervised model outputs. Our proposed framework is easily extendable and can\nincorporate many other robust loss functions to further improve performance.\nExtensive experiments and theoretical analyses support our claims. Code is\navailable at github.com/UCSC-REAL/SelfSup_NoisyLabel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaowei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Generalization of Contrastive Self-Supervised Learning. (arXiv:2111.00743v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.00743","description":"<p>Recently, self-supervised learning has attracted great attention, since it\nonly requires unlabeled data for training. Contrastive learning is one popular\nmethod for self-supervised learning and has achieved promising empirical\nperformance. However, the theoretical understanding of its generalization\nability is still limited. To this end, we define a kind of\n$(\\sigma,\\delta)$-measure to mathematically quantify the data augmentation, and\nthen provide an upper bound of the downstream classification error based on the\nmeasure. We show that the generalization ability of contrastive self-supervised\nlearning depends on three key factors: alignment of positive samples,\ndivergence of class centers, and concentration of augmented data. The first two\nfactors can be optimized by contrastive algorithms, while the third one is\npriorly determined by pre-defined data augmentation. With the above theoretical\nfindings, we further study two canonical contrastive losses, InfoNCE and\ncross-correlation loss, and prove that both of them are indeed able to satisfy\nthe first two factors. Moreover, we empirically verify the third factor by\nconducting various experiments on the real-world dataset, and show that our\ntheoretical inferences on the relationship between the data augmentation and\nthe generalization of contrastive self-supervised learning agree with the\nempirical observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weiran Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Mingyang Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuyang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated pharyngeal phase detection and bolus localization in videofluoroscopic swallowing study: Killing two birds with one stone?. (arXiv:2111.04699v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.04699","description":"<p>The videofluoroscopic swallowing study (VFSS) is a gold-standard imaging\ntechnique for assessing swallowing, but analysis and rating of VFSS recordings\nis time consuming and requires specialized training and expertise. Researchers\nhave recently demonstrated that it is possible to automatically detect the\npharyngeal phase of swallowing and to localize the bolus in VFSS recordings via\ncomputer vision, fostering the development of novel techniques for automatic\nVFSS analysis. However, training of algorithms to perform these tasks requires\nlarge amounts of annotated data that are seldom available. We demonstrate that\nthe challenges of pharyngeal phase detection and bolus localization can be\nsolved together using a single approach. We propose a deep-learning framework\nthat jointly tackles pharyngeal phase detection and bolus localization in a\nweakly-supervised manner, requiring only the initial and final frames of the\npharyngeal phase as ground truth annotations for the training. Our approach\nstems from the observation that bolus presence in the pharynx is the most\nprominent visual feature upon which to infer whether individual VFSS frames\nbelong to the pharyngeal phase. We conducted extensive experiments with\nmultiple convolutional neural networks (CNNs) on a dataset of 1245 bolus-level\nclips from 59 healthy subjects. We demonstrated that the pharyngeal phase can\nbe detected with an F1-score higher than 0.9. Moreover, by processing the class\nactivation maps of the CNNs, we were able to localize the bolus with promising\nresults, obtaining correlations with ground truth trajectories higher than 0.9,\nwithout any manual annotations of bolus location used for training purposes.\nOnce validated on a larger sample of participants with swallowing disorders,\nour framework will pave the way for the development of intelligent tools for\nVFSS analysis to support clinicians in swallowing assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bandini_A/0/1/0/all/0/1\">Andrea Bandini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smaoui_S/0/1/0/all/0/1\">Sana Smaoui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Steele_C/0/1/0/all/0/1\">Catriona M. Steele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Fine-Tuning by Better Leveraging Pre-Training Data. (arXiv:2111.12292v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12292","description":"<p>As a dominant paradigm, fine-tuning a pre-trained model on the target data is\nwidely used in many deep learning applications, especially for small data sets.\nHowever, recent studies have empirically shown that training from scratch has\nthe final performance that is no worse than this pre-training strategy once the\nnumber of training samples is increased in some vision tasks. In this work, we\nrevisit this phenomenon from the perspective of generalization analysis by\nusing excess risk bound which is popular in learning theory. The result reveals\nthat the excess risk bound may have a weak dependency on the pre-trained model.\nThe observation inspires us to leverage pre-training data for fine-tuning,\nsince this data is also available for fine-tuning. The generalization result of\nusing pre-training data shows that the excess risk bound on a target task can\nbe improved when the appropriate pre-training data is included in fine-tuning.\nWith the theoretical motivation, we propose a novel selection strategy to\nselect a subset from pre-training data to help improve the generalization on\nthe target task. Extensive experimental results for image classification tasks\non 8 benchmark data sets verify the effectiveness of the proposed data\nselection based fine-tuning pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation. (arXiv:2112.02478v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.02478","description":"<p>Background and Objective: Artificial intelligence (AI) methods coupled with\nbiomedical analysis has a critical role during pandemics as it helps to release\nthe overwhelming pressure from healthcare systems and physicians. As the\nongoing COVID-19 crisis worsens in countries having dense populations and\ninadequate testing kits like Brazil and India, radiological imaging can act as\nan important diagnostic tool to accurately classify covid-19 patients and\nprescribe the necessary treatment in due time. With this motivation, we present\nour study based on deep learning architecture for detecting covid-19 infected\nlungs using chest X-rays. Dataset: We collected a total of 2470 images for\nthree different class labels, namely, healthy lungs, ordinary pneumonia, and\ncovid-19 infected pneumonia, out of which 470 X-ray images belong to the\ncovid-19 category. Methods: We first pre-process all the images using histogram\nequalization techniques and segment them using U-net architecture. VGG-16\nnetwork is then used for feature extraction from the pre-processed images which\nis further sampled by SMOTE oversampling technique to achieve a balanced\ndataset. Finally, the class-balanced features are classified using a support\nvector machine (SVM) classifier with 10-fold cross-validation and the accuracy\nis evaluated. Result and Conclusion: Our novel approach combining well-known\npre-processing techniques, feature extraction methods, and dataset balancing\nmethod, lead us to an outstanding rate of recognition of 98% for COVID-19\nimages over a dataset of 2470 X-ray images. Our model is therefore fit to be\nutilized in healthcare facilities for screening purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhadouria_H/0/1/0/all/0/1\">Hitendra Singh Bhadouria</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_K/0/1/0/all/0/1\">Krishan Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swaraj_A/0/1/0/all/0/1\">Aman Swaraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verma_K/0/1/0/all/0/1\">Karan Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory-Constrained Deep Latent Visual Attention for Improved Local Planning in Presence of Heterogeneous Terrain. (arXiv:2112.04684v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.04684","description":"<p>We present a reward-predictive, model-based deep learning method featuring\ntrajectory-constrained visual attention for local planning in visual navigation\ntasks. Our method learns to place visual attention at locations in latent image\nspace which follow trajectories caused by vehicle control actions to enhance\npredictive accuracy during planning. The attention model is jointly optimized\nby the task-specific loss and an additional trajectory-constraint loss,\nallowing adaptability yet encouraging a regularized structure for improved\ngeneralization and reliability. Importantly, visual attention is applied in\nlatent feature map space instead of raw image space to promote efficient\nplanning. We validated our model in visual navigation tasks of planning low\nturbulence, collision-free trajectories in off-road settings and hill climbing\nwith locking differentials in the presence of slippery terrain. Experiments\ninvolved randomized procedural generated simulation and real-world\nenvironments. We found our method improved generalization and learning\nefficiency when compared to no-attention and self-attention alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wapnick_S/0/1/0/all/0/1\">Stefan Wapnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manderson_T/0/1/0/all/0/1\">Travis Manderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1\">David Meger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Gregory Dudek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonlinear Transform Source-Channel Coding for Semantic Communications. (arXiv:2112.10961v2 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2112.10961","description":"<p>In this paper, we propose a class of high-efficiency deep joint\nsource-channel coding methods that can closely adapt to the source distribution\nunder the nonlinear transform, it can be collected under the name nonlinear\ntransform source-channel coding (NTSCC). In the considered model, the\ntransmitter first learns a nonlinear analysis transform to map the source data\ninto latent space, then transmits the latent representation to the receiver via\ndeep joint source-channel coding. Our model incorporates the nonlinear\ntransform as a strong prior to effectively extract the source semantic features\nand provide side information for source-channel coding. Unlike existing\nconventional deep joint source-channel coding methods, the proposed NTSCC\nessentially learns both the source latent representation and an entropy model\nas the prior on the latent representation. Accordingly, novel adaptive rate\ntransmission and hyperprior-aided codec refinement mechanisms are developed to\nupgrade deep joint source-channel coding. The whole system design is formulated\nas an optimization problem whose goal is to minimize the end-to-end\ntransmission rate-distortion performance under established perceptual quality\nmetrics. Across test image sources with various resolutions, we find that the\nproposed NTSCC transmission method generally outperforms both the analog\ntransmission using the standard deep joint source-channel coding and the\nclassical separation-based digital transmission. Notably, the proposed NTSCC\nmethod can potentially support future semantic communications due to its\ncontent-aware ability and perceptual optimization goal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jincheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sixian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kailin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_Z/0/1/0/all/0/1\">Zhongwei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaoqi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kai Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Ping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Extraction, Classification and Prediction for Hand Hygiene Gestures with KNN Algorithm. (arXiv:2112.15085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15085","description":"<p>There are six, well-structured hand gestures for washing hands as provided by\nWorld Health Organisation guidelines. In this paper, hand features such as\ncontours of the hands, the centroid of the hands, and extreme hand points along\nthe largest contour are extracted for specific hand-washing gestures with the\nuse of a computer vision library, OpenCV. For this project, a robust dataset of\nhand hygiene video recordings is built with the help of 30 research\nparticipants. In this work, a subset of the dataset was used as a pilot study\nto demonstrate the effectiveness of the KNN algorithm. Extracted hand features\nsaved in a CSV file are passed to a KNN model with a cross-fold validation\ntechnique for the classification and prediction of the unlabelled data. A mean\naccuracy score of &gt;95% is achieved and proves that the KNN algorithm with an\nappropriate input value of K=3 is efficient for hand hygiene gestures\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (arXiv:2201.01080v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01080","description":"<p>Deep neural networks (DNNs) are threatened by adversarial examples.\nAdversarial detection, which distinguishes adversarial images from benign\nimages, is fundamental for robust DNN-based services. Image transformation is\none of the most effective approaches to detect adversarial examples. During the\nlast few years, a variety of image transformations have been studied and\ndiscussed to design reliable adversarial detectors. In this paper, we\nsystematically synthesize the recent progress on adversarial detection via\nimage transformations with a novel classification method. Then, we conduct\nextensive experiments to test the detection performance of image\ntransformations against state-of-the-art adversarial attacks. Furthermore, we\nreveal that each individual transformation is not capable of detecting\nadversarial examples in a robust way, and propose a DNN-based approach referred\nto as \\emph{AdvJudge}, which combines scores of 9 image transformations.\nWithout knowing which individual scores are misleading or not misleading,\nAdvJudge can make the right judgment, and achieve a significant improvement in\ndetection rate. Finally, we utilize an explainable AI tool to show the\ncontribution of each image transformation to adversarial detection.\nExperimental results show that the contribution of image transformations to\nadversarial detection is significantly different, the combination of them can\nsignificantly improve the generic detection ability against state-of-the-art\nadversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy. (arXiv:2201.02867v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02867","description":"<p>Recent breakthroughs in high-resolution imaging of biomolecules in solution\nwith cryo-electron microscopy (cryo-EM) have unlocked new doors for the\nreconstruction of molecular volumes, thereby promising further advances in\nbiology, chemistry, and pharmacological research. Recent next-generation volume\nreconstruction algorithms that combine generative modeling with end-to-end\nunsupervised deep learning techniques have shown promising preliminary results,\nbut still face considerable technical and theoretical hurdles when applied to\nexperimental cryo-EM images. In light of the proliferation of such methods, we\npropose here a critical review of recent advances in the field of deep\ngenerative modeling for cryo-EM volume reconstruction. The present review aims\nto (i) unify and compare these new methods using a consistent statistical\nframework, (ii) present them using a terminology familiar to machine learning\nresearchers and computational biologists with no specific background in\ncryo-EM, and (iii) provide the necessary perspective on current advances to\nhighlight their relative strengths and weaknesses, along with outstanding\nbottlenecks and avenues for improvements in the field. This review might also\nraise the interest of computer vision practitioners, as it highlights\nsignificant limits of deep generative models in low signal-to-noise regimes --\ntherefore emphasizing a need for new theoretical and methodological\ndevelopments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Donnat_C/0/1/0/all/0/1\">Claire Donnat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Levy_A/0/1/0/all/0/1\">Axel Levy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poitevin_F/0/1/0/all/0/1\">Frederic Poitevin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_E/0/1/0/all/0/1\">Ellen Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RDP-Net: Region Detail Preserving Network for Change Detection. (arXiv:2202.09745v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.09745","description":"<p>Change detection (CD) is an essential earth observation technique. It\ncaptures the dynamic information of land objects. With the rise of deep\nlearning, neural networks (NN) have shown great potential in CD. However,\ncurrent NN models introduce backbone architectures that lose detailed\ninformation during learning. Moreover, current NN models are heavy in\nparameters, which prevents their deployment on edge devices such as UAVs. In\nthis work, we tackle this issue by proposing RDP-Net: a region detail\npreserving network for CD. We propose an efficient training strategy that\nconstructs the training tasks during the warmup period of NN training and lets\nthe NN learn from easy to hard. The training strategy enables NN to learn more\npowerful features with fewer FLOPs and achieve better performance. Next, we\npropose an effective edge loss that increases the penalty for errors on details\nand improves the network's attention to details such as boundary regions and\nsmall areas. Furthermore, we provide a NN model with a brand new backbone that\nachieves the state-of-the-art empirical performance in CD with only 1.70M\nparameters. We hope our RDP-Net would benefit the practical CD applications on\ncompact devices and could inspire more people to bring change detection to a\nnew level with the efficient training strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hongjia Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pu_F/0/1/0/all/0/1\">Fangling Pu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-driven Visual Tempo Learning for Video-based Action Recognition. (arXiv:2202.12116v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12116","description":"<p>Action visual tempo characterizes the dynamics and the temporal scale of an\naction, which is helpful to distinguish human actions that share high\nsimilarities in visual dynamics and appearance. Previous methods capture the\nvisual tempo either by sampling raw videos with multiple rates, which require a\ncostly multi-layer network to handle each rate, or by hierarchically sampling\nbackbone features, which rely heavily on high-level features that miss\nfine-grained temporal dynamics. In this work, we propose a Temporal Correlation\nModule (TCM), which can be easily embedded into the current action recognition\nbackbones in a plug-in-and-play manner, to extract action visual tempo from\nlow-level backbone features at single-layer remarkably. Specifically, our TCM\ncontains two main components: a Multi-scale Temporal Dynamics Module (MTDM) and\na Temporal Attention Module (TAM). MTDM applies a correlation operation to\nlearn pixel-wise fine-grained temporal dynamics for both fast-tempo and\nslow-tempo. TAM adaptively emphasizes expressive features and suppresses\ninessential ones via analyzing the global information across various tempos.\nExtensive experiments conducted on several action recognition benchmarks, e.g.\nSomething-Something V1 $\\&amp;$ V2, Kinetics-400, UCF-101, and HMDB-51, have\ndemonstrated that the proposed TCM is effective to promote the performance of\nthe existing video-based action recognition models for a large margin. The\nsource code is publicly released at https://github.com/yzfly/TCM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Creativity Characterization of Generative Models via Group-based Subset Scanning. (arXiv:2203.00523v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00523","description":"<p>Deep generative models, such as Variational Autoencoders (VAEs) and\nGenerative Adversarial Networks (GANs), have been employed widely in\ncomputational creativity research. However, such models discourage\nout-of-distribution generation to avoid spurious sample generation, thereby\nlimiting their creativity. Thus, incorporating research on human creativity\ninto generative deep learning techniques presents an opportunity to make their\noutputs more compelling and human-like. As we see the emergence of generative\nmodels directed toward creativity research, a need for machine learning-based\nsurrogate metrics to characterize creative output from these models is\nimperative. We propose group-based subset scanning to identify, quantify, and\ncharacterize creative processes by detecting a subset of anomalous\nnode-activations in the hidden layers of the generative models. Our experiments\non the standard image benchmarks, and their \"creatively generated\" variants,\nreveal that the proposed subset scores distribution is more useful for\ndetecting creative processes in the activation space rather than the pixel\nspace. Further, we found that creative samples generate larger subsets of\nanomalies than normal or non-creative samples across datasets. The node\nactivations highlighted during the creative decoding process are different from\nthose responsible for the normal sample generation. Lastly, we assess if the\nimages from the subsets selected by our method were also found creative by\nhuman evaluators, presenting a link between creativity perception in humans and\nnode activations within deep neural nets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1\">Celia Cintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quanz_B/0/1/0/all/0/1\">Brian Quanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1\">Girmaw Abebe Tadesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance. (arXiv:2203.06749v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06749","description":"<p>In May 2021, the site runnersworld.com published that participation in\nultra-distance races has increased by 1,676% in the last 23 years. Moreover,\nnearly 41% of those runners participate in more than one race per year. The\ndevelopment of wearable devices has undoubtedly contributed to motivating\nparticipants by providing performance measures in real-time. However, we\nbelieve there is room for improvement, particularly from the organizers point\nof view. This work aims to determine how the runners performance can be\nquantified and predicted by considering a non-invasive technique focusing on\nthe ultra-running scenario. In this sense, participants are captured when they\npass through a set of locations placed along the race track. Each footage is\nconsidered an input to an I3D ConvNet to extract the participant's running gait\nin our work. Furthermore, weather and illumination capture conditions or\nocclusions may affect these footages due to the race staff and other runners.\nTo address this challenging task, we have tracked and codified the\nparticipant's running gait at some RPs and removed the context intending to\nensure a runner-of-interest proper evaluation. The evaluation suggests that the\nfeatures extracted by an I3D ConvNet provide enough information to estimate the\nparticipant's performance along the different race tracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freire_Obregon_D/0/1/0/all/0/1\">David Freire-Obreg&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzo_Navarro_J/0/1/0/all/0/1\">Javier Lorenzo-Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castrillon_Santana_M/0/1/0/all/0/1\">Modesto Castrill&#xf3;n-Santana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16325","description":"<p>Existing deep learning based methods effectively prompt the performance of\naerial scene classification. However, due to the large amount of parameters and\ncomputational cost, it is rather difficult to apply these methods to multiple\nreal-time remote sensing applications such as on-board data preception on\ndrones and satellites. In this paper, we address this task by developing a\nlight-weight ConvNet named multi-stage duplex fusion network (MSDF-Net). The\nkey idea is to use parameters as little as possible while obtaining as strong\nas possible scene representation capability. To this end, a residual-dense\nduplex fusion strategy is developed to enhance the feature propagation while\nre-using parameters as much as possible, and is realized by our duplex fusion\nblock (DFblock). Specifically, our MSDF-Net consists of multi-stage structures\nwith DFblock. Moreover, duplex semantic aggregation (DSA) module is developed\nto mine the remote sensing scene information from extracted convolutional\nfeatures, which also contains two parallel branches for semantic description.\nExtensive experiments are conducted on three widely-used aerial scene\nclassification benchmarks, and reflect that our MSDF-Net can achieve a\ncompetitive performance against the recent state-of-art while reducing up to\n80% parameter numbers. Particularly, an accuracy of 92.96% is achieved on AID\nwith only 0.49M parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingjun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Beichen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions. (arXiv:2204.00746v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00746","description":"<p>We propose a novel one-stage Transformer-based semantic and spatial refined\ntransformer (SSRT) to solve the Human-Object Interaction detection task, which\nrequires to localize humans and objects, and predicts their interactions.\nDifferently from previous Transformer-based HOI approaches, which mostly focus\nat improving the design of the decoder outputs for the final detection, SSRT\nintroduces two new modules to help select the most relevant object-action pairs\nwithin an image and refine the queries' representation using rich semantic and\nspatial features. These enhancements lead to state-of-the-art results on the\ntwo most popular HOI benchmarks: V-COCO and HICO-DET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iftekhar_A/0/1/0/all/0/1\">A S M Iftekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_K/0/1/0/all/0/1\">Kaustav Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Structured State-Evolution for Vision-Language Navigation. (arXiv:2204.09280v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09280","description":"<p>Vision-and-language Navigation (VLN) task requires an embodied agent to\nnavigate to a remote location following a natural language instruction.\nPrevious methods usually adopt a sequence model (e.g., Transformer and LSTM) as\nthe navigator. In such a paradigm, the sequence model predicts action at each\nstep through a maintained navigation state, which is generally represented as a\none-dimensional vector. However, the crucial navigation clues (i.e.,\nobject-level environment layout) for embodied navigation task is discarded\nsince the maintained vector is essentially unstructured. In this paper, we\npropose a novel Structured state-Evolution (SEvol) model to effectively\nmaintain the environment layout clues for VLN. Specifically, we utilise the\ngraph-based feature to represent the navigation state instead of the\nvector-based state. Accordingly, we devise a Reinforced Layout clues Miner\n(RLM) to mine and detect the most crucial layout graph for long-term navigation\nvia a customised reinforcement learning strategy. Moreover, the Structured\nEvolving Module (SEM) is proposed to maintain the structured graph-based state\nduring navigation, where the state is gradually evolved to learn the\nobject-level spatial-temporal relationship. The experiments on the R2R and R4R\ndatasets show that the proposed SEvol model improves VLN models' performance by\nlarge margins, e.g., +3% absolute SPL accuracy for NvEM and +8% for EnvDrop on\nthe R2R test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_E/0/1/0/all/0/1\">Erli Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model. (arXiv:2205.00690v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.00690","description":"<p>Noisy labels are inevitable yet problematic in machine learning society. It\nruins the generalization of a classifier by making the classifier over-fitted\nto noisy labels. Existing methods on noisy label have focused on modifying the\nclassifier during the training procedure. It has two potential problems. First,\nthese methods are not applicable to a pre-trained classifier without further\naccess to training. Second, it is not easy to train a classifier and regularize\nall negative effects from noisy labels, simultaneously. We suggest a new branch\nof method, Noisy Prediction Calibration (NPC) in learning with noisy labels.\nThrough the introduction and estimation of a new type of transition matrix via\ngenerative model, NPC corrects the noisy prediction from the pre-trained\nclassifier to the true label as a post-processing scheme. We prove that NPC\ntheoretically aligns with the transition matrix based methods. Yet, NPC\nempirically provides more accurate pathway to estimate true label, even without\ninvolvement in classifier learning. Also, NPC is applicable to any classifier\ntrained with noisy label methods, if training instances and its predictions are\navailable. Our method, NPC, boosts the classification performances of all\nbaseline models on both synthetic and real-world datasets. The implemented code\nis available at https://github.com/BaeHeeSun/NPC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1\">HeeSun Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungjae Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1\">Byeonghu Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">JoonHo Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyungwoo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_I/0/1/0/all/0/1\">Il-Chul Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2205.04464","description":"<p>We propose a new microscopy simulation system that can depict atomistic\nmodels in a micrograph visual style, similar to results of physical electron\nmicroscopy imaging. This system is scalable, able to represent simulation of\nelectron microscopy of tens of viral particles and synthesizes the image faster\nthan previous methods. On top of that, the simulator is differentiable, both\nits deterministic as well as stochastic stages that form signal and noise\nrepresentations in the micrograph. This notable property has the capability for\nsolving inverse problems by means of optimization and thus allows for\ngeneration of microscopy simulations using the parameter settings estimated\nfrom real data. We demonstrate this learning capability through two\napplications: (1) estimating the parameters of the modulation transfer function\ndefining the detector properties of the simulated and real micrographs, and (2)\ndenoising the real data based on parameters trained from the simulated\nexamples. While current simulators do not support any parameter estimation due\nto their forward design, we show that the results obtained using estimated\nparameters are very similar to the results of real micrographs. Additionally,\nwe evaluate the denoising capabilities of our approach and show that the\nresults showed an improvement over state-of-the-art methods. Denoised\nmicrographs exhibit less noise in the tilt-series tomography reconstructions,\nultimately reducing the visual dominance of noise in direct volume rendering of\nmicroscopy tomograms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Nguyen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Engel_D/0/1/0/all/0/1\">Dominik Engel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bohak_C/0/1/0/all/0/1\">Ciril Bohak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Viola_I/0/1/0/all/0/1\">Ivan Viola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When does dough become a bagel? Analyzing the remaining mistakes on ImageNet. (arXiv:2205.04596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04596","description":"<p>Image classification accuracy on the ImageNet dataset has been a barometer\nfor progress in computer vision over the last decade. Several recent papers\nhave questioned the degree to which the benchmark remains useful to the\ncommunity, yet innovations continue to contribute gains to performance, with\ntoday's largest models achieving 90%+ top-1 accuracy. To help contextualize\nprogress on ImageNet and provide a more meaningful evaluation for today's\nstate-of-the-art models, we manually review and categorize every remaining\nmistake that a few top models make in order to provide insight into the\nlong-tail of errors on one of the most benchmarked datasets in computer vision.\nWe focus on the multi-label subset evaluation of ImageNet, where today's best\nmodels achieve upwards of 97% top-1 accuracy. Our analysis reveals that nearly\nhalf of the supposed mistakes are not mistakes at all, and we uncover new valid\nmulti-labels, demonstrating that, without careful review, we are significantly\nunderestimating the performance of these models. On the other hand, we also\nfind that today's best models still make a significant number of mistakes (40%)\nthat are obviously wrong to human reviewers. To calibrate future progress on\nImageNet, we provide an updated multi-label evaluation set, and we curate\nImageNet-Major: a 68-example \"major error\" slice of the obvious mistakes made\nby today's top models -- a slice where models should achieve near perfection,\nbut today are far from doing so.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Benjamin Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fridovich_Keil_S/0/1/0/all/0/1\">Sara Fridovich-Keil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Visual Words for Interpretable Image Recognition. (arXiv:2205.10724v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10724","description":"<p>To interpret deep models' predictions, attention-based visual cues are widely\nused in addressing \\textit{why} deep models make such predictions. Beyond that,\nthe current research community becomes more interested in reasoning\n\\textit{how} deep models make predictions, where some prototype-based methods\nemploy interpretable representations with their corresponding visual cues to\nreveal the black-box mechanism of deep model behaviors. However, these\npioneering attempts only either learn the category-specific prototypes and\ndeteriorate their generalizing capacities, or demonstrate several illustrative\nexamples without a quantitative evaluation of visual-based interpretability\nwith further limitations on their practical usages. In this paper, we revisit\nthe concept of visual words and propose the Learnable Visual Words (LVW) to\ninterpret the model prediction behaviors with two novel modules: semantic\nvisual words learning and dual fidelity preservation. The semantic visual words\nlearning relaxes the category-specific constraint, enabling the general visual\nwords shared across different categories. Beyond employing the visual words for\nprediction to align visual words with the base model, our dual fidelity\npreservation also includes the attention guided semantic alignment that\nencourages the learned visual words to focus on the same conceptual regions for\nprediction. Experiments on six visual benchmarks demonstrate the superior\neffectiveness of our proposed LVW in both accuracy and model interpretation\nover the state-of-the-art methods. Moreover, we elaborate on various in-depth\nanalyses to further explore the learned visual words and the generalizability\nof our method for unseen categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenxiao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNNs are Myopic. (arXiv:2205.10760v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10760","description":"<p>We claim that Convolutional Neural Networks (CNNs) learn to classify images\nusing only small seemingly unrecognizable tiles. We show experimentally that\nCNNs trained only using such tiles can match or even surpass the performance of\nCNNs trained on full images. Conversely, CNNs trained on full images show\nsimilar predictions on small tiles. We also propose the first a priori\ntheoretical model for convolutional data sets that seems to explain this\nbehavior. This gives additional support to the long standing suspicion that\nCNNs do not need to understand the global structure of images to achieve\nstate-of-the-art accuracies. Surprisingly it also suggests that over-fitting is\nnot needed either.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madala_V/0/1/0/all/0/1\">Vamshi C. Madala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Shivkumar Chandrasekaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super Vision Transformer. (arXiv:2205.11397v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11397","description":"<p>We attempt to reduce the computational costs in vision transformers (ViTs),\nwhich increase quadratically in the token number. We present a novel training\nparadigm that trains only one ViT model at a time, but is capable of providing\nimproved image recognition performance with various computational costs. Here,\nthe trained ViT model, termed super vision transformer (SuperViT), is empowered\nwith the versatile ability to solve incoming patches of multiple sizes as well\nas preserve informative tokens with multiple keeping rates (the ratio of\nkeeping tokens) to achieve good hardware efficiency for inference, given that\nthe available hardware resources often change from time to time. Experimental\nresults on ImageNet demonstrate that our SuperViT can considerably reduce the\ncomputational costs of ViT models with even performance increase. For example,\nwe reduce 2x FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2% and\n0.7% for 1.5x reduction. Also, our SuperViT significantly outperforms existing\nstudies on efficient vision transformers. For example, when consuming the same\namount of FLOPs, our SuperViT surpasses the recent state-of-the-art (SoTA) EViT\nby 1.1% when using DeiT-S as their backbones. The project of this work is made\npublicly available at https://github.com/lmbxmu/SuperViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.11508","description":"<p>Self-Supervised Learning (SSL) surmises that inputs and pairwise positive\nrelationships are enough to learn meaningful representations. Although SSL has\nrecently reached a milestone: outperforming supervised methods in many\nmodalities\\dots the theoretical foundations are limited, method-specific, and\nfail to provide principled design guidelines to practitioners. In this paper,\nwe propose a unifying framework under the helm of spectral manifold learning to\naddress those limitations. Through the course of this study, we will rigorously\ndemonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous\nspectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al.\n</p>\n<p>This unification will then allow us to obtain (i) the closed-form optimal\nrepresentation for each method, (ii) the closed-form optimal network parameters\nin the linear regime for each method, (iii) the impact of the pairwise\nrelations used during training on each of those quantities and on downstream\ntask performances, and most importantly, (iv) the first theoretical bridge\nbetween contrastive and non-contrastive methods towards global and local\nspectral embedding methods respectively, hinting at the benefits and\nlimitations of each. For example, (i) if the pairwise relation is aligned with\nthe downstream task, any SSL method can be employed successfully and will\nrecover the supervised method, but in the low data regime, VICReg's invariance\nhyper-parameter should be high; (ii) if the pairwise relation is misaligned\nwith the downstream task, VICReg with small invariance hyper-parameter should\nbe preferred over SimCLR or BarlowTwins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance. (arXiv:2205.12022v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12022","description":"<p>With the rapid development of the Metaverse, virtual humans have emerged, and\nhuman image synthesis and editing techniques, such as pose transfer, have\nrecently become popular. Most of the existing techniques rely on GANs, which\ncan generate good human images even with large variants and occlusions. But\nfrom our best knowledge, the existing state-of-the-art method still has the\nfollowing problems: the first is that the rendering effect of the synthetic\nimage is not realistic, such as poor rendering of some regions. And the second\nis that the training of GAN is unstable and slow to converge, such as model\ncollapse. Based on the above two problems, we propose several methods to solve\nthem. To improve the rendering effect, we use the Residual Fast Fourier\nTransform Block to replace the traditional Residual Block. Then, spectral\nnormalization and Wasserstein distance are used to improve the speed and\nstability of GAN training. Experiments demonstrate that the methods we offer\nare effective at solving the problems listed above, and we get state-of-the-art\nscores in LPIPS and PSNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCoViT: Mobile Convolutional Vision Transformer. (arXiv:2205.12635v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12635","description":"<p>Recently, Transformer networks have achieved impressive results on a variety\nof vision tasks. However, most of them are computationally expensive and not\nsuitable for real-world mobile applications. In this work, we present Mobile\nConvolutional Vision Transformer (MoCoViT), which improves in performance and\nefficiency by introducing transformer into mobile convolutional networks to\nleverage the benefits of both architectures. Different from recent works on\nvision transformer, the mobile transformer block in MoCoViT is carefully\ndesigned for mobile devices and is very lightweight, accomplished through two\nprimary modifications: the Mobile Self-Attention (MoSA) module and the Mobile\nFeed Forward Network (MoFFN). MoSA simplifies the calculation of the attention\nmap through Branch Sharing scheme while MoFFN serves as a mobile version of MLP\nin the transformer, further reducing the computation by a large margin.\nComprehensive experiments verify that our proposed MoCoViT family outperform\nstate-of-the-art portable CNNs and transformer neural architectures on various\nvision tasks. On ImageNet classification, it achieves 74.5% top-1 accuracy at\n147M FLOPs, gaining 1.2% over MobileNetV3 with less computations. And on the\nCOCO object detection task, MoCoViT outperforms GhostNet by 2.1 AP in RetinaNet\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiashi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniInst: Unique Representation for End-to-End Instance Segmentation. (arXiv:2205.12646v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12646","description":"<p>Existing instance segmentation methods have achieved impressive performance\nbut still suffer from a common dilemma: redundant representations (e.g.,\nmultiple boxes, grids, and anchor points) are inferred for one instance, which\nleads to multiple duplicated predictions. Thus, mainstream methods usually rely\non a hand-designed non-maximum suppression (NMS) post-processing step to select\nthe optimal prediction result, which hinders end-to-end training. To address\nthis issue, we propose a box-free and NMS-free end-to-end instance segmentation\nframework, termed UniInst, that yields only one unique representation for each\ninstance. Specifically, we design an instance-aware one-to-one assignment\nscheme, namely Only Yield One Representation (OYOR), which dynamically assigns\none unique representation to each instance according to the matching quality\nbetween predictions and ground truths. Then, a novel prediction re-ranking\nstrategy is elegantly integrated into the framework to address the misalignment\nbetween the classification score and the mask quality, enabling the learned\nrepresentation to be more discriminative. With these techniques, our UniInst,\nthe first FCN-based end-to-end instance segmentation framework, achieves\ncompetitive performance, e.g., 39.0 mask AP using ResNet-50-FPN and 40.2 mask\nAP using ResNet-101-FPN, against mainstream methods on COCO test-dev. Moreover,\nthe proposed instance-aware method is robust to occlusion scenes, outperforming\ncommon baselines by remarkable mask AP on the heavily-occluded OCHuman\nbenchmark. Our codes will be available upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yimin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lufan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure Unbiased Adversarial Model for Medical Image Segmentation. (arXiv:2205.12857v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.12857","description":"<p>Generative models have been widely proposed in image recognition to generate\nmore images where the distribution is similar to that of the real images. It\noften introduces a discriminator network to discriminate original real data and\ngenerated data.\n</p>\n<p>However, such discriminator often considers the distribution of the data and\ndid not pay enough attention to the intrinsic gap due to structure.\n</p>\n<p>In this paper, we reformulate a new image to image translation problem to\nreduce structural gap, in addition to the typical intensity distribution gap.\nWe further propose a simple yet important Structure Unbiased Adversarial Model\nfor Medical Image Segmentation (SUAM) with learnable inverse structural\ndeformation for medical image segmentation. It consists of a structure\nextractor, an attention diffeomorphic registration and a structure \\&amp; intensity\ndistribution rendering module. The structure extractor aims to extract the\ndominant structure of the input image. The attention diffeomorphic registration\nis proposed to reduce the structure gap with an inverse deformation field to\nwarp the prediction masks back to their original form. The structure rendering\nmodule is to render the deformed structure to an image with targeted intensity\ndistribution. We apply the proposed SUAM on both optical coherence tomography\n(OCT), magnetic resonance imaging (MRI) and computerized tomography (CT) data.\nExperimental results show that the proposed method has the capability to\ntransfer both intensity and structure distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Shaoming Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xi Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bartlett_J/0/1/0/all/0/1\">Joseph Bartlett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaowen Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1\">Jinming Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inception Transformer. (arXiv:2205.12956v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12956","description":"<p>Recent studies show that Transformer has strong capability of building\nlong-range dependencies, yet is incompetent in capturing high frequencies that\npredominantly convey local information. To tackle this issue, we present a\nnovel and general-purpose Inception Transformer, or iFormer for short, that\neffectively learns comprehensive features with both high- and low-frequency\ninformation in visual data. Specifically, we design an Inception mixer to\nexplicitly graft the advantages of convolution and max-pooling for capturing\nthe high-frequency information to Transformers. Different from recent hybrid\nframeworks, the Inception mixer brings greater efficiency through a channel\nsplitting mechanism to adopt parallel convolution/max-pooling path and\nself-attention path as high- and low-frequency mixers, while having the\nflexibility to model discriminative information scattered within a wide\nfrequency range. Considering that bottom layers play more roles in capturing\nhigh-frequency details while top layers more in modeling low-frequency global\ninformation, we further introduce a frequency ramp structure, i.e. gradually\ndecreasing the dimensions fed to the high-frequency mixer and increasing those\nto the low-frequency mixer, which can effectively trade-off high- and\nlow-frequency components across different layers. We benchmark the iFormer on a\nseries of vision tasks, and showcase that it achieves impressive performance on\nimage classification, COCO detection and ADE20K segmentation. For example, our\niFormer-S hits the top-1 accuracy of 83.4% on ImageNet-1K, much higher than\nDeiT-S by 3.6%, and even slightly better than much bigger model Swin-B (83.3%)\nwith only 1/4 parameters and 1/3 FLOPs. Code and models will be released at\nhttps://github.com/sail-sg/iFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenyang Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}