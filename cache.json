{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v1 [cs.SI])","link":"http://arxiv.org/abs/2206.08406","description":"<p>Tweets are the most concise form of communication in online social media,\nwherein a single tweet has the potential to make or break the discourse of the\nconversation. Online hate speech is more accessible than ever, and stifling its\npropagation is of utmost importance for social media companies and users for\ncongenial communication. Most of the research barring a recent few has focused\non classifying an individual tweet regardless of the tweet thread/context\nleading up to that point. One of the classical approaches to curb hate speech\nis to adopt a reactive strategy after the hate speech postage. The ex-post\nfacto strategy results in neglecting subtle posts that do not show the\npotential to instigate hate speech on their own but may portend in the\nsubsequent discussion ensuing in the post's replies. In this paper, we propose\nDRAGNET++, which aims to predict the intensity of hatred that a tweet can bring\nin through its reply chain in the future. It uses the semantic and propagating\nstructure of the tweet threads to maximize the contextual information leading\nup to and the fall of hate intensity at each subsequent tweet. We explore three\npublicly available Twitter datasets -- Anti-Racism contains the reply tweets of\na collection of social media discourse on racist remarks during US political\nand Covid-19 background; Anti-Social presents a dataset of 40 million tweets\namidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents\nTwitter datasets collated based on anti-Asian behaviours during COVID-19\npandemic. All the curated datasets consist of structural graph information of\nthe Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art\nbaselines significantly. It beats the best baseline by an 11\\% margin on the\nPerson correlation coefficient and a decrease of 25\\% on RMSE for the\nAnti-Racism dataset with a similar performance on the other two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qing Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_T/0/1/0/all/0/1\">Tharun Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multi-Task Models for Misogyny Identification and Categorization on Arabic Social Media. (arXiv:2206.08407v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08407","description":"<p>The prevalence of toxic content on social media platforms, such as hate\nspeech, offensive language, and misogyny, presents serious challenges to our\ninterconnected society. These challenging issues have attracted widespread\nattention in Natural Language Processing (NLP) community. In this paper, we\npresent the submitted systems to the first Arabic Misogyny Identification\nshared task. We investigate three multi-task learning models as well as their\nsingle-task counterparts. In order to encode the input text, our models rely on\nthe pre-trained MARBERT language model. The overall obtained results show that\nall our submitted models have achieved the best performances (top three ranked\nsubmissions) in both misogyny identification and categorization tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1\">Abdelkader El Mahdaouy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1\">Abdellah El Mekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oumar_A/0/1/0/all/0/1\">Ahmed Oumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousannif_H/0/1/0/all/0/1\">Hajar Mousannif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1\">Ismail Berrada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CS-UM6P at SemEval-2022 Task 6: Transformer-based Models for Intended Sarcasm Detection in English and Arabic. (arXiv:2206.08415v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08415","description":"<p>Sarcasm is a form of figurative language where the intended meaning of a\nsentence differs from its literal meaning. This poses a serious challenge to\nseveral Natural Language Processing (NLP) applications such as Sentiment\nAnalysis, Opinion Mining, and Author Profiling. In this paper, we present our\nparticipating system to the intended sarcasm detection task in English and\nArabic languages. Our system\\footnote{The source code of our system is\navailable at \\url{https://github.com/AbdelkaderMH/iSarcasmEval}} consists of\nthree deep learning-based models leveraging two existing pre-trained language\nmodels for Arabic and English. We have participated in all sub-tasks. Our\nofficial submissions achieve the best performance on sub-task A for Arabic\nlanguage and rank second in sub-task B. For sub-task C, our system is ranked\n7th and 11th on Arabic and English datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1\">Abdelkader El Mahdaouy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1\">Abdellah El Mekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1\">Kabil Essefar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skiredj_A/0/1/0/all/0/1\">Abderrahman Skiredj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1\">Ismail Berrada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogueScript: Using Dialogue Agents to Produce a Script. (arXiv:2206.08425v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08425","description":"<p>We present a novel approach to generating scripts by using agents with\ndifferent personality types. To manage character interaction in the script, we\nemploy simulated dramatic networks. Automatic and human evaluation on multiple\ncriteria shows that our approach outperforms a vanilla-GPT2-based baseline. We\nfurther introduce a new metric to evaluate dialogue consistency based on\nnatural language inference and demonstrate its validity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmidtova_P/0/1/0/all/0/1\">Patr&#xed;cia Schmidtov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javorsky_D/0/1/0/all/0/1\">D&#xe1;vid Javorsk&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miklas_C/0/1/0/all/0/1\">Christi&#xe1;n Mikl&#xe1;&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musil_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Musil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_R/0/1/0/all/0/1\">Rudolf Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAAMA 2.0: An Integrated System that Answers Boolean and Extractive Question. (arXiv:2206.08441v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08441","description":"<p>Recent machine reading comprehension datasets include extractive and boolean\nquestions but current approaches do not offer integrated support for answering\nboth question types. We present a multilingual machine reading comprehension\nsystem and front-end demo that handles boolean questions by providing both a\nYES/NO answer and highlighting supporting evidence, and handles extractive\nquestions by highlighting the answer in the passage. Our system, GAAMA 2.0, is\nranked first on the Tydi QA leaderboard at the time of this writing. We\ncontrast two different implementations of our approach. The first includes\nseveral independent stacks of transformers allowing easy deployment of each\ncomponent. The second is a single stack of transformers utilizing adapters to\nreduce GPU memory footprint in a resource-constrained environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCarley_S/0/1/0/all/0/1\">Scott McCarley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1\">Mihaela Bornea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenthal_S/0/1/0/all/0/1\">Sara Rosenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferritto_A/0/1/0/all/0/1\">Anthony Ferritto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching Abusive Language Detection with Community Context. (arXiv:2206.08445v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08445","description":"<p>Uses of pejorative expressions can be benign or actively empowering. When\nmodels for abuse detection misclassify these expressions as derogatory, they\ninadvertently censor productive conversations held by marginalized groups. One\nway to engage with non-dominant perspectives is to add context around\nconversations. Previous research has leveraged user- and thread-level features,\nbut it often neglects the spaces within which productive conversations take\nplace. Our paper highlights how community context can improve classification\noutcomes in abusive language detection. We make two main contributions to this\nend. First, we demonstrate that online communities cluster by the nature of\ntheir support towards victims of abuse. Second, we establish how community\ncontext improves accuracy and reduces the false positive rates of\nstate-of-the-art abusive language classifiers. These findings suggest a\npromising direction for context-aware models in abusive language research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurrek_J/0/1/0/all/0/1\">Jana Kurrek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_H/0/1/0/all/0/1\">Haji Mohammad Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruths_D/0/1/0/all/0/1\">Derek Ruths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Methods for Estimating and Improving Robustness of Language Models. (arXiv:2206.08446v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08446","description":"<p>Despite their outstanding performance, large language models (LLMs) suffer\nnotorious flaws related to their preference for simple, surface-level textual\nrelations over full semantic complexity of the problem. This proposal\ninvestigates a common denominator of this problem in their weak ability to\ngeneralise outside of the training domain. We survey diverse research\ndirections providing estimations of model generalisation ability and find that\nincorporating some of these measures in the training objectives leads to\nenhanced distributional robustness of neural models. Based on these findings,\nwe present future research directions towards enhancing the robustness of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering. (arXiv:2206.08486v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08486","description":"<p>Effective multi-hop question answering (QA) requires reasoning over multiple\nscattered paragraphs and providing explanations for answers. Most existing\napproaches cannot provide an interpretable reasoning process to illustrate how\nthese models arrive at an answer. In this paper, we propose a Question\nDecomposition method based on Abstract Meaning Representation (QDAMR) for\nmulti-hop QA, which achieves interpretable reasoning by decomposing a multi-hop\nquestion into simpler sub-questions and answering them in order. Since\nannotating the decomposition is expensive, we first delegate the complexity of\nunderstanding the multi-hop question to an AMR parser. We then achieve the\ndecomposition of a multi-hop question via segmentation of the corresponding AMR\ngraph based on the required reasoning type. Finally, we generate sub-questions\nusing an AMR-to-Text generation model and answer them with an off-the-shelf QA\nmodel. Experimental results on HotpotQA demonstrate that our approach is\ncompetitive for interpretable reasoning and that the sub-questions generated by\nQDAMR are well-formed, outperforming existing question-decomposition-based\nmulti-hop QA approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhenyun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yonghua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riddle_P/0/1/0/all/0/1\">Patricia Riddle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Numerical Reasoning Question Answering System with Fine-grained Retriever and the Ensemble of Multiple Generators for FinQA. (arXiv:2206.08506v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08506","description":"<p>The numerical reasoning in the financial domain -- performing quantitative\nanalysis and summarizing the information from financial reports -- can greatly\nincrease business efficiency and reduce costs of billions of dollars. Here, we\npropose a numerical reasoning question answering system to answer numerical\nreasoning questions among financial text and table data sources, consisting of\na retriever module, a generator module, and an ensemble module. Specifically,\nin the retriever module, in addition to retrieving the whole row data, we\ninnovatively design a cell retriever that retrieves the gold cells to avoid\nbringing unrelated and similar cells in the same row to the inputs of the\ngenerator module. In the generator module, we utilize multiple generators to\nproduce programs, which are operation steps to answer the question. Finally, in\nthe ensemble module, we integrate multiple programs to choose the best program\nas the output of our system. In the final private test set in FinQA\nCompetition, our system obtains 69.79 execution accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jiangzhou Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yunlin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xin-Yu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. (arXiv:2206.08514v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08514","description":"<p>Textual backdoor attacks are a kind of practical threat to NLP systems. By\ninjecting a backdoor in the training phase, the adversary could control model\npredictions via predefined triggers. As various attack and defense models have\nbeen proposed, it is of great significance to perform rigorous evaluations.\nHowever, we highlight two issues in previous backdoor learning evaluations: (1)\nThe differences between real-world scenarios (e.g. releasing poisoned datasets\nor models) are neglected, and we argue that each scenario has its own\nconstraints and concerns, thus requires specific evaluation protocols; (2) The\nevaluation metrics only consider whether the attacks could flip the models'\npredictions on poisoned samples and retain performances on benign samples, but\nignore that poisoned samples should also be stealthy and semantic-preserving.\nTo address these issues, we categorize existing works into three practical\nscenarios in which attackers release datasets, pre-trained models, and\nfine-tuned models respectively, then discuss their unique evaluation\nmethodologies. On metrics, to completely evaluate poisoned samples, we use\ngrammar error increase and perplexity difference for stealthiness, along with\ntext similarity for validity. After formalizing the frameworks, we develop an\nopen-source toolkit OpenBackdoor to foster the implementations and evaluations\nof textual backdoor learning. With this toolkit, we perform extensive\nexperiments to benchmark attack and defense models under the suggested\nparadigm. To facilitate the underexplored defenses against poisoned datasets,\nwe further propose CUBE, a simple yet strong clustering-based defense baseline.\nWe hope that our frameworks and benchmarks could serve as the cornerstones for\nfuture model development and evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation. (arXiv:2206.08522v1 [cs.RO])","link":"http://arxiv.org/abs/2206.08522","description":"<p>Benefiting from language flexibility and compositionality, humans naturally\nintend to use language to command an embodied agent for complex tasks such as\nnavigation and object manipulation. In this work, we aim to fill the blank of\nthe last mile of embodied agents -- object manipulation by following human\nguidance, e.g., \"move the red mug next to the box while keeping it upright.\" To\nthis end, we introduce an Automatic Manipulation Solver (AMSolver) simulator\nand build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,\ncontaining various language instructions on categorized robotic manipulation\ntasks. Specifically, modular rule-based task templates are created to\nautomatically generate robot demonstrations with language instructions,\nconsisting of diverse object shapes and appearances, action types, and motion\nconstraints. We also develop a keypoint-based model 6D-CLIPort to deal with\nmulti-view observations and language input and output a sequence of 6 degrees\nof freedom (DoF) actions. We hope the new simulator and benchmark will\nfacilitate future research on language-guided robotic manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaizhi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_O/0/1/0/all/0/1\">Odest Chadwicke Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Correction of Human Translations. (arXiv:2206.08593v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08593","description":"<p>We introduce translation error correction (TEC), the task of automatically\ncorrecting human-generated translations. Imperfections in machine translations\n(MT) have long motivated systems for improving translations post-hoc with\nautomatic post-editing. In contrast, little attention has been devoted to the\nproblem of automatically correcting human translations, despite the intuition\nthat humans make distinct errors that machines would be well-suited to assist\nwith, from typos to inconsistencies in translation conventions. To investigate\nthis, we build and release the Aced corpus with three TEC datasets. We show\nthat human errors in TEC exhibit a more diverse range of errors and far fewer\ntranslation fluency errors than the MT errors in automatic post-editing\ndatasets, suggesting the need for dedicated TEC models that are specialized to\ncorrect human errors. We show that pre-training instead on synthetic errors\nbased on human errors improves TEC F-score by as much as 5.1 points. We\nconducted a human-in-the-loop user study with nine professional translation\neditors and found that the assistance of our TEC system led them to produce\nsignificantly higher quality revised translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1\">Geza Kovacs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shastry_A/0/1/0/all/0/1\">Aditya Shastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuebker_J/0/1/0/all/0/1\">Joern Wuebker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeNero_J/0/1/0/all/0/1\">John DeNero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment. (arXiv:2206.08614v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08614","description":"<p>Computational inference of aesthetics is an ill-defined task due to its\nsubjective nature. Many datasets have been proposed to tackle the problem by\nproviding pairs of images and aesthetic scores based on human ratings. However,\nhumans are better at expressing their opinion, taste, and emotions by means of\nlanguage rather than summarizing them in a single number. In fact, photo\ncritiques provide much richer information as they reveal how and why users rate\nthe aesthetics of visual stimuli. In this regard, we propose the Reddit Photo\nCritique Dataset (RPCD), which contains tuples of image and photo critiques.\nRPCD consists of 74K images and 220K comments and is collected from a Reddit\ncommunity used by hobbyists and professional photographers to improve their\nphotography skills by leveraging constructive community feedback. The proposed\ndataset differs from previous aesthetics datasets mainly in three aspects,\nnamely (i) the large scale of the dataset and the extension of the comments\ncriticizing different aspects of the image, (ii) it contains mostly UltraHD\nimages, and (iii) it can easily be extended to new data as it is collected\nthrough an automatic pipeline. To the best of our knowledge, in this work, we\npropose the first attempt to estimate the aesthetic quality of visual stimuli\nfrom the critiques. To this end, we exploit the polarity of the sentiment of\ncriticism as an indicator of aesthetic judgment. We demonstrate how sentiment\npolarity correlates positively with the aesthetic judgment available for two\naesthetic assessment benchmarks. Finally, we experiment with several models by\nusing the sentiment scores as a target for ranking images. Dataset and\nbaselines are available (https://github.com/mediatechnologycenter/aestheval).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nieto_D/0/1/0/all/0/1\">Daniel Vera Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celona_L/0/1/0/all/0/1\">Luigi Celona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Labrador_C/0/1/0/all/0/1\">Clara Fernandez-Labrador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08657","description":"<p>Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a cross-modal encoder, or feed the last-layer\nuni-modal features directly into the top cross-modal encoder, ignoring the\nsemantic information at the different levels in the deep uni-modal encoders.\nBoth approaches possibly restrict vision-language representation learning and\nlimit model performance. In this paper, we introduce multiple bridge layers\nthat build a connection between the top layers of uni-modal encoders and each\nlayer of the cross-modal encoder. This enables comprehensive bottom-up\ninteractions between visual and textual representations at different semantic\nlevels, resulting in more effective cross-modal alignment and fusion. Our\nproposed Bridge-Tower, pre-trained with only $4$M images, achieves\nstate-of-the-art performance on various downstream vision-language tasks. On\nthe VQAv2 test-std set, Bridge-Tower achieves an accuracy of $78.73\\%$,\noutperforming the previous state-of-the-art METER model by $1.09\\%$ with the\nsame pre-training data and almost no additional parameters and computational\ncost. Notably, when further scaling the model, Bridge-Tower achieves an\naccuracy of $81.15\\%$, surpassing models that are pre-trained on\norders-of-magnitude larger datasets. Code is available at\nhttps://github.com/microsoft/BridgeTower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenman_S/0/1/0/all/0/1\">Shachar Rosenman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Quantitative and Qualitative Analysis of Suicide Ideation Detection using Deep Learning. (arXiv:2206.08673v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08673","description":"<p>For preventing youth suicide, social media platforms have received much\nattention from researchers. A few researches apply machine learning, or deep\nlearning-based text classification approaches to classify social media posts\ncontaining suicidality risk. This paper replicated competitive social\nmedia-based suicidality detection/prediction models. We evaluated the\nfeasibility of detecting suicidal ideation using multiple datasets and\ndifferent state-of-the-art deep learning models, RNN-, CNN-, and\nAttention-based models. Using two suicidality evaluation datasets, we evaluated\n28 combinations of 7 input embeddings with 4 commonly used deep learning models\nand 5 pretrained language models in quantitative and qualitative ways. Our\nreplication study confirms that deep learning works well for social media-based\nsuicidality detection in general, but it highly depends on the dataset's\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabral_R/0/1/0/all/0/1\">Rina Cabral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish Text Using Transformers. (arXiv:2206.08680v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08680","description":"<p>Code-Mixed text data consists of sentences having words or phrases from more\nthan one language. Most multi-lingual communities worldwide communicate using\nmultiple languages, with English usually one of them. Hinglish is a Code-Mixed\ntext composed of Hindi and English but written in Roman script. This paper aims\nto determine the factors influencing the quality of Code-Mixed text data\ngenerated by the system. For the HinglishEval task, the proposed model uses\nmulti-lingual BERT to find the similarity between synthetically generated and\nhuman-generated sentences to predict the quality of synthetically generated\nHinglish sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Furniturewala_S/0/1/0/all/0/1\">Shaz Furniturewala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_V/0/1/0/all/0/1\">Vijay Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_A/0/1/0/all/0/1\">Amulya Ratna Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kedia_H/0/1/0/all/0/1\">Hriday Kedia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yashvardhan Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical and Neural Methods for Cross-lingual Entity Label Mapping in Knowledge Graphs. (arXiv:2206.08709v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08709","description":"<p>Knowledge bases such as Wikidata amass vast amounts of named entity\ninformation, such as multilingual labels, which can be extremely useful for\nvarious multilingual and cross-lingual applications. However, such labels are\nnot guaranteed to match across languages from an information consistency\nstandpoint, greatly compromising their usefulness for fields such as machine\ntranslation. In this work, we investigate the application of word and sentence\nalignment techniques coupled with a matching algorithm to align cross-lingual\nentity labels extracted from Wikidata in 10 languages. Our results indicate\nthat mapping between Wikidata's main labels stands to be considerably improved\n(up to $20$ points in F1-score) by any of the employed methods. We show how\nmethods relying on sentence embeddings outperform all others, even across\ndifferent scripts. We believe the application of such techniques to measure the\nsimilarity of label pairs, coupled with a knowledge base rich in high-quality\nentity labels, to be an excellent asset to machine translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amaral_G/0/1/0/all/0/1\">Gabriel Amaral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinnis_M/0/1/0/all/0/1\">M&#x101;rcis Pinnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skadina_I/0/1/0/all/0/1\">Inguna Skadi&#x146;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_O/0/1/0/all/0/1\">Odinaldo Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1\">Elena Simperl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CookDial: A dataset for task-oriented dialogs grounded in procedural documents. (arXiv:2206.08723v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08723","description":"<p>This work presents a new dialog dataset, CookDial, that facilitates research\non task-oriented dialog systems with procedural knowledge understanding. The\ncorpus contains 260 human-to-human task-oriented dialogs in which an agent,\ngiven a recipe document, guides the user to cook a dish. Dialogs in CookDial\nexhibit two unique features: (i) procedural alignment between the dialog flow\nand supporting document; (ii) complex agent decision-making that involves\nsegmenting long sentences, paraphrasing hard instructions and resolving\ncoreference in the dialog context. In addition, we identify three challenging\n(sub)tasks in the assumed task-oriented dialog system: (1) User Question\nUnderstanding, (2) Agent Action Frame Prediction, and (3) Agent Response\nGeneration. For each of these tasks, we develop a neural baseline model, which\nwe evaluate on the CookDial dataset. We publicly release the CookDial dataset,\ncomprising rich annotations of both dialogs and recipe documents, to stimulate\nfurther research on domain-specific document-grounded dialog systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaporojets_K/0/1/0/all/0/1\">Klim Zaporojets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crowdsourcing Relative Rankings of Multi-Word Expressions: Experts versus Non-Experts. (arXiv:2206.08724v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08724","description":"<p>In this study we investigate to which degree experts and non-experts agree on\nquestions of difficulty in a crowdsourcing experiment. We ask non-experts\n(second language learners of Swedish) and two groups of experts (teachers of\nSwedish as a second/foreign language and CEFR experts) to rank multi-word\nexpressions in a crowdsourcing experiment. We find that the resulting rankings\nby all the three tested groups correlate to a very high degree, which suggests\nthat judgments produced in a comparative setting are not influenced by\nprofessional insights into Swedish as a second language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfter_D/0/1/0/all/0/1\">David Alfter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_T/0/1/0/all/0/1\">Therese Lindstr&#xf6;m Tiedemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volodina_E/0/1/0/all/0/1\">Elena Volodina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ITU Faroese Pairs Dataset. (arXiv:2206.08727v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08727","description":"<p>This article documents a dataset of sentence pairs between Faroese and\nDanish, produced at ITU Copenhagen. The data covers tranlsation from both\nsource languages, and is intended for use as training data for machine\ntranslation systems in this language pair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isfeldt_A/0/1/0/all/0/1\">Annika Solveig Hedegaard Isfeldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djurhuus_S/0/1/0/all/0/1\">Signhild Djurhuus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE. (arXiv:2206.08790v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08790","description":"<p>The human perception system is often assumed to recruit motor knowledge when\nprocessing auditory speech inputs. Using articulatory modeling and deep\nlearning, this study examines how this articulatory information can be used for\ndiscovering speech units in a self-supervised setting. We used vector-quantized\nvariational autoencoders (VQ-VAE) to learn discrete representations from\narticulatory and acoustic speech data. In line with the zero-resource paradigm,\nan ABX test was then used to investigate how the extracted representations\nencode phonetically relevant properties. Experiments were conducted on three\ndifferent corpora in English and French. We found that articulatory information\nrather organises the latent representations in terms of place of articulation\nwhereas the speech acoustics mainly structure the latent space in terms of\nmanner of articulation. We show that an optimal fusion of the two modalities\ncan lead to a joint representation of these phonetic dimensions more accurate\nthan each modality considered individually. Since articulatory information is\nusually not available in a practical situation, we finally investigate the\nbenefit it provides when inferred from the speech acoustics in a\nself-supervised manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georges_M/0/1/0/all/0/1\">Marc-Antoine Georges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_J/0/1/0/all/0/1\">Jean-Luc Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hueber_T/0/1/0/all/0/1\">Thomas Hueber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language with Vision: a Study on Grounded Word and Sentence Embeddings. (arXiv:2206.08823v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08823","description":"<p>Language grounding to vision is an active field of research aiming to enrich\ntext-based representations of word meanings by leveraging perceptual knowledge\nfrom vision. Despite many attempts at language grounding, it is still unclear\nhow to effectively inject visual knowledge into the word embeddings of a\nlanguage in such a way that a proper balance of textual and visual knowledge is\nmaintained. Some common concerns are the following. Is visual grounding\nbeneficial for abstract words or is its contribution only limited to concrete\nwords? What is the optimal way of bridging the gap between text and vision? How\nmuch do we gain by visually grounding textual embeddings? The present study\naddresses these questions by proposing a simple yet very effective grounding\napproach for pre-trained word embeddings. Our model aligns textual embeddings\nwith vision while largely preserving the distributional statistics that\ncharacterize word use in text corpora. By applying a learned alignment, we are\nable to generate visually grounded embeddings for unseen words, including\nabstract words. A series of evaluations on word similarity benchmarks shows\nthat visual grounding is beneficial not only for concrete words, but also for\nabstract words. We also show that our method for visual grounding offers\nadvantages for contextualized embeddings, but only when these are trained on\ncorpora of relatively modest size. Code and grounded embeddings for English are\navailable at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_H/0/1/0/all/0/1\">Hassan Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heitmeier_M/0/1/0/all/0/1\">Maria Heitmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_Bajestan_E/0/1/0/all/0/1\">Elnaz Shafaei-Bajestan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_H/0/1/0/all/0/1\">Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What can Speech and Language Tell us About the Working Alliance in Psychotherapy. (arXiv:2206.08835v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08835","description":"<p>We are interested in the problem of conversational analysis and its\napplication to the health domain. Cognitive Behavioral Therapy is a structured\napproach in psychotherapy, allowing the therapist to help the patient to\nidentify and modify the malicious thoughts, behavior, or actions. This\ncooperative effort can be evaluated using the Working Alliance Inventory\nObserver-rated Shortened - a 12 items inventory covering task, goal, and\nrelationship - which has a relevant influence on therapeutic outcomes. In this\nwork, we investigate the relation between this alliance inventory and the\nspoken conversations (sessions) between the patient and the psychotherapist. We\nhave delivered eight weeks of e-therapy, collected their audio and video call\nsessions, and manually transcribed them. The spoken conversations have been\nannotated and evaluated with WAI ratings by professional therapists. We have\ninvestigated speech and language features and their association with WAI items.\nThe feature types include turn dynamics, lexical entrainment, and\nconversational descriptors extracted from the speech and language signals. Our\nfindings provide strong evidence that a subset of these features are strong\nindicators of working alliance. To the best of our knowledge, this is the first\nand a novel study to exploit speech and language for characterising working\nalliance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roccabruna_G/0/1/0/all/0/1\">Gabriel Roccabruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciulli_T/0/1/0/all/0/1\">Tommaso Ciulli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danieli_M/0/1/0/all/0/1\">Morena Danieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08853","description":"<p>Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite and knowledge bases\n(https://minedojo.org) to promote research towards the goal of generally\ncapable embodied agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunfan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuncong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haoyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1\">Andrew Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"niksss at HinglishEval: Language-agnostic BERT-based Contextual Embeddings with Catboost for Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text. (arXiv:2206.08910v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08910","description":"<p>This paper describes the system description for the HinglishEval challenge at\nINLG 2022. The goal of this task was to investigate the factors influencing the\nquality of the code-mixed text generation system. The task was divided into two\nsubtasks, quality rating prediction and annotators disagreement prediction of\nthe synthetic Hinglish dataset. We attempted to solve these tasks using\nsentence-level embeddings, which are obtained from mean pooling the\ncontextualized word embeddings for all input tokens in our text. We\nexperimented with various classifiers on top of the embeddings produced for\nrespective tasks. Our best-performing system ranked 1st on subtask B and 3rd on\nsubtask A.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Model to Measure the Spread Power of Rumors. (arXiv:2002.07563v5 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2002.07563","description":"<p>With technologies that have democratized the production and reproduction of\ninformation, a significant portion of daily interacted posts in social media\nhas been infected by rumors. Despite the extensive research on rumor detection\nand verification, so far, the problem of calculating the spread power of rumors\nhas not been considered. To address this research gap, the present study seeks\na model to calculate the Spread Power of Rumor (SPR) as the function of\ncontent-based features in two categories: False Rumor (FR) and True Rumor (TR).\nFor this purpose, the theory of Allport and Postman will be adopted, which it\nclaims that importance and ambiguity are the key variables in rumor-mongering\nand the power of rumor. Totally 42 content features in two categories\n\"importance\" (28 features) and \"ambiguity\" (14 features) are introduced to\ncompute SPR. The proposed model is evaluated on two datasets, Twitter and\nTelegram. The results showed that (i) the spread power of False Rumor documents\nis rarely more than True Rumors. (ii) there is a significant difference between\nthe SPR means of two groups False Rumor and True Rumor. (iii) SPR as a\ncriterion can have a positive impact on distinguishing False Rumors and True\nRumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Nagadeh_Z/0/1/0/all/0/1\">Zoleikha Jahanbakhsh-Nagadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akan_T/0/1/0/all/0/1\">Taymaz Akan</a> (Rahkar-Farshi), <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1\">Meysam Asgari-Chenaghlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1\">Narjes Nikzad-Khasmakhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1\">Ali-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1\">Mehrdad Ranjbar-Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafarani_Moattar_E/0/1/0/all/0/1\">Elnaz Zafarani-Moattar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohammad-Ali Balafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiSubs: A Large-scale Multimodal and Multilingual Dataset. (arXiv:2103.01910v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01910","description":"<p>This paper introduces a large-scale multimodal and multilingual dataset that\naims to facilitate research on grounding words to images in their contextual\nusage in language. The dataset consists of images selected to unambiguously\nillustrate concepts expressed in sentences from movie subtitles. The dataset is\na valuable resource as (i) the images are aligned to text fragments rather than\nwhole sentences; (ii) multiple images are possible for a text fragment and a\nsentence; (iii) the sentences are free-form and real-world like; (iv) the\nparallel texts are multilingual. We set up a fill-in-the-blank game for humans\nto evaluate the quality of the automatic image selection process of our\ndataset. We show the utility of the dataset on two automatic tasks: (i)\nfill-in-the-blank; (ii) lexical translation. Results of the human evaluation\nand automatic models demonstrate that images can be a useful complement to the\ntextual context. The dataset will benefit research on visual grounding of words\nespecially in the context of free-form sentences, and can be obtained from\nhttps://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Josiah Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1\">Josiel Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lala_C/0/1/0/all/0/1\">Chiraag Lala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GrASP: A Library for Extracting and Exploring Human-Interpretable Textual Patterns. (arXiv:2104.03958v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03958","description":"<p>Data exploration is an important step of every data science and machine\nlearning project, including those involving textual data. We provide a novel\nlanguage tool, in the form of a publicly available Python library for\nextracting patterns from textual data. The library integrates a first public\nimplementation of the existing GrASP algorithm. It allows users to extract\npatterns using a number of general-purpose built-in linguistic attributes (such\nas hypernyms, part-of-speech tags, and syntactic dependency tags), as envisaged\nfor the original algorithm, as well as domain-specific custom attributes which\ncan be incorporated into the library by implementing two functions. The library\nis equipped with a web-based interface empowering human users to conveniently\nexplore data via the extracted patterns, using complementary pattern-centric\nand example-centric views: the former includes a reading in natural language\nand statistics of each extracted pattern; the latter shows applications of each\nextracted pattern to training examples. We demonstrate the usefulness of the\nlibrary in classification (spam detection and argument mining), model analysis\n(machine translation), and artifact discovery in datasets (SNLI and\n20Newsgroups).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lertvittayakumjorn_P/0/1/0/all/0/1\">Piyawat Lertvittayakumjorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnarch_E/0/1/0/all/0/1\">Eyal Shnarch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), reducing inherent model hazards\n(\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR. (arXiv:2110.04484v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.04484","description":"<p>Self-supervised pre-training could effectively improve the performance of\nlow-resource automatic speech recognition (ASR). However, existing\nself-supervised pre-training are task-agnostic, i.e., could be applied to\nvarious downstream tasks. Although it enlarges the scope of its application,\nthe capacity of the pre-trained model is not fully utilized for the ASR task,\nand the learned representations may not be optimal for ASR. In this work, in\norder to build a better pre-trained model for low-resource ASR, we propose a\npre-training approach called wav2vec-S, where we use task-specific\nsemi-supervised pre-training to refine the self-supervised pre-trained model\nfor the ASR task thus more effectively utilize the capacity of the pre-trained\nmodel to generate task-specific representations for ASR. Experiments show that\ncompared to wav2vec 2.0, wav2vec-S only requires a marginal increment of\npre-training time but could significantly improve ASR performance on in-domain,\ncross-domain and cross-lingual datasets. Average relative WER reductions are\n24.5% and 6.6% for 1h and 10h fine-tuning, respectively. Furthermore, we show\nthat semi-supervised pre-training could close the representation gap between\nthe self-supervised pre-trained model and the corresponding fine-tuned model\nthrough canonical correlation analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Han Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_G/0/1/0/all/0/1\">Gaofeng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yonghong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Descriptive Patterns and Their Application to Characterizing Classification Errors. (arXiv:2110.09599v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09599","description":"<p>State-of-the-art deep learning methods achieve human-like performance on many\ntasks, but make errors nevertheless. Characterizing these errors in easily\ninterpretable terms gives insight into whether a classifier is prone to making\nsystematic errors, but also gives a way to act and improve the classifier. We\npropose to discover those feature-value combinations (i.e., patterns) that\nstrongly correlate with correct resp. erroneous predictions to obtain a global\nand interpretable description for arbitrary classifiers. We show this is an\ninstance of the more general label description problem, which we formulate in\nterms of the Minimum Description Length principle. To discover a good pattern\nset, we develop the efficient Premise algorithm. Through an extensive set of\nexperiments we show it performs very well in practice on both synthetic and\nreal-world data. Unlike existing solutions, it ably recovers ground truth\npatterns, even on highly imbalanced data over many features. Through two case\nstudies on Visual Question Answering and Named Entity Recognition, we confirm\nthat Premise gives clear and actionable insight into the systematic errors made\nby modern NLP classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1\">Jonas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1\">Jilles Vreeken</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. (arXiv:2110.13900v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.13900","description":"<p>Self-supervised learning (SSL) achieves great success in speech recognition,\nwhile limited exploration has been attempted for other speech processing tasks.\nAs speech signal contains multi-faceted information including speaker identity,\nparalinguistics, spoken content, etc., learning universal representations for\nall speech tasks is challenging. To tackle the problem, we propose a new\npre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM\njointly learns masked speech prediction and denoising in pre-training. By this\nmeans, WavLM does not only keep the speech content modeling capability by the\nmasked speech prediction, but also improves the potential to non-ASR tasks by\nthe speech denoising. In addition, WavLM employs gated relative position bias\nfor the Transformer structure to better capture the sequence ordering of input\nspeech. We also scale up the training dataset from 60k hours to 94k hours.\nWavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and\nbrings significant improvements for various speech processing tasks on their\nrepresentative benchmarks. The code and pre-trained models are available at\nhttps://aka.ms/wavlm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiangzhan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decision Attentive Regularization to Improve Simultaneous Speech Translation Systems. (arXiv:2110.15729v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.15729","description":"<p>Simultaneous translation systems start producing the output while processing\nthe partial source sentence in the incoming input stream. These systems need to\ndecide when to read more input and when to write the output. These decisions\ndepend on the structure of source/target language and the information contained\nin the partial input sequence. Hence, read/write decision policy remains the\nsame across different input modalities, i.e., speech and text. This motivates\nus to leverage the text transcripts corresponding to the speech input for\nimproving simultaneous speech-to-text translation (SimulST). We propose\nDecision Attentive Regularization (DAR) to improve the decision policy of\nSimulST systems by using the simultaneous text-to-text translation (SimulMT)\ntask. We also extend several techniques from the offline speech translation\ndomain to explore the role of SimulMT task in improving SimulST performance.\nOverall, we achieve 34.66% / 4.5 BLEU improvement over the baseline model\nacross different latency regimes for the MuST-C English-German (EnDe) SimulST\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_M/0/1/0/all/0/1\">Mohd Abbas Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Beomseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanwoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Normalized Importance Sampling for Neural Language Modeling. (arXiv:2111.06310v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06310","description":"<p>To mitigate the problem of having to traverse over the full vocabulary in the\nsoftmax normalization of a neural language model, sampling-based training\ncriteria are proposed and investigated in the context of large vocabulary\nword-based neural language models. These training criteria typically enjoy the\nbenefit of faster training and testing, at a cost of slightly degraded\nperformance in terms of perplexity and almost no visible drop in word error\nrate. While noise contrastive estimation is one of the most popular choices,\nrecently we show that other sampling-based criteria can also perform well, as\nlong as an extra correction step is done, where the intended class posterior\nprobability is recovered from the raw model outputs. In this work, we propose\nself-normalized importance sampling. Compared to our previous work, the\ncriteria considered in this work are self-normalized and there is no need to\nfurther conduct a correction step. Through self-normalized language model\ntraining as well as lattice rescoring experiments, we show that our proposed\nself-normalized importance sampling is competitive in both research-oriented\nand production-oriented automatic speech recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstenberger_A/0/1/0/all/0/1\">Alexander Gerstenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jintao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Neural Machine Translation with Dependency-Scaled Self-Attention Network. (arXiv:2111.11707v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.11707","description":"<p>Syntax knowledge contributes its powerful strength in Neural machine\ntranslation (NMT) tasks. The early NMT model supposed that syntax details can\nbe automatically learned from numerous texts via attention networks. However,\nsucceeding researches pointed out that limited by the uncontrolled nature of\nattention computation, the model requires an external syntax to capture the\ndeep syntactic awareness. Although recent syntax-aware NMT methods have bored\ngreat fruits in combining syntax, the additional workloads they introduced\nrender the model heavy and slow. Particularly, these efforts scarcely involve\nthe Transformer-based NMT and modify its core self-attention network (SAN). To\nthis end, we propose a parameter-free, dependency-scaled self-attention network\n(Deps-SAN) for syntax-aware Transformer-based NMT. It integrates a quantified\nmatrix of syntactic dependencies to impose explicit syntactic constraints into\nthe SAN to learn syntactic details and dispel the dispersion of attention\ndistributions. Two knowledge sparsing techniques are further proposed to avoid\nthe model overfitting the dependency noises. Extensive experiments and analyses\non the two benchmark NMT tasks verify the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Ru Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianyong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03857","description":"<p>This paper presents a grounded language-image pre-training (GLIP) model for\nlearning object-level, language-aware, and semantic-rich visual\nrepresentations. GLIP unifies object detection and phrase grounding for\npre-training. The unification brings two benefits: 1) it allows GLIP to learn\nfrom both detection and grounding data to improve both tasks and bootstrap a\ngood grounding model; 2) GLIP can leverage massive image-text pairs by\ngenerating grounding boxes in a self-training fashion, making the learned\nrepresentation semantic-rich. In our experiments, we pre-train GLIP on 27M\ngrounding data, including 3M human-annotated and 24M web-crawled image-text\npairs. The learned representations demonstrate strong zero-shot and few-shot\ntransferability to various object-level recognition tasks. 1) When directly\nevaluated on COCO and LVIS (without seeing any images in COCO during\npre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many\nsupervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val\nand 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13\ndownstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised\nDynamic Head. Code is released at https://github.com/microsoft/GLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiwu Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Interpretation of Saliency-based Explanation Over Text. (arXiv:2201.11569v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11569","description":"<p>While a lot of research in explainable AI focuses on producing effective\nexplanations, less work is devoted to the question of how people understand and\ninterpret the explanation. In this work, we focus on this question through a\nstudy of saliency-based explanations over textual data. Feature-attribution\nexplanations of text models aim to communicate which parts of the input text\nwere more influential than others towards the model decision. Many current\nexplanation methods, such as gradient-based or Shapley value-based methods,\nprovide measures of importance which are well-understood mathematically. But\nhow does a person receiving the explanation (the explainee) comprehend it? And\ndoes their understanding match what the explanation attempted to communicate?\nWe empirically investigate the effect of various factors of the input, the\nfeature-attribution explanation, and visualization procedure, on laypeople's\ninterpretation of the explanation. We query crowdworkers for their\ninterpretation on tasks in English and German, and fit a GAMM model to their\nresponses considering the factors of interest. We find that people often\nmis-interpret the explanations: superficial and unrelated factors, such as word\nlength, influence the explainees' importance assignment despite the explanation\ncommunicating importance directly. We then show that some of this distortion\ncan be attenuated: we propose a method to adjust saliencies based on model\nestimates of over- and under-perception, and explore bar charts as an\nalternative to heatmap saliency visualization. We find that both approaches can\nattenuate the distorting effect of specific factors, leading to\nbetter-calibrated understanding of the explanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROCK: Causal Inference Principles for Reasoning about Commonsense Causality. (arXiv:2202.00436v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00436","description":"<p>Commonsense causality reasoning (CCR) aims at identifying plausible causes\nand effects in natural language descriptions that are deemed reasonable by an\naverage person. Although being of great academic and practical interest, this\nproblem is still shadowed by the lack of a well-posed theoretical framework;\nexisting work usually relies on deep language models wholeheartedly, and is\npotentially susceptible to confounding co-occurrences. Motivated by classical\ncausal principles, we articulate the central question of CCR and draw parallels\nbetween human subjects in observational studies and natural languages to adopt\nCCR to the potential-outcomes framework, which is the first such attempt for\ncommonsense tasks. We propose a novel framework, ROCK, to Reason O(A)bout\nCommonsense K(C)ausality, which utilizes temporal signals as incidental\nsupervision, and balances confounding effects using temporal propensities that\nare analogous to propensity scores. The ROCK implementation is modular and\nzero-shot, and demonstrates good CCR capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Structure with Undirected Neural Networks. (arXiv:2202.03760v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.03760","description":"<p>Neural networks are powerful function estimators, leading to their status as\na paradigm of choice for modeling structured data. However, unlike other\nstructured representations that emphasize the modularity of the problem --\ne.g., factor graphs -- neural networks are usually monolithic mappings from\ninputs to outputs, with a fixed computation order. This limitation prevents\nthem from capturing different directions of computation and interaction between\nthe modeled variables.\n</p>\n<p>In this paper, we combine the representational strengths of factor graphs and\nof neural networks, proposing undirected neural networks (UNNs): a flexible\nframework for specifying computations that can be performed in any order. For\nparticular choices, our proposed models subsume and extend many existing\narchitectures: feed-forward, recurrent, self-attention networks, auto-encoders,\nand networks with implicit layers. We demonstrate the effectiveness of\nundirected neural architectures, both unstructured and structured, on a range\nof tasks: tree-constrained dependency parsing, convolutional image\nclassification, and sequence completion with attention. By varying the\ncomputation order, we show how a single UNN can be used both as a classifier\nand a prototype generator, and how it can fill in missing parts of an input\nsequence, making them a promising field for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihaylova_T/0/1/0/all/0/1\">Tsvetomila Mihaylova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. (arXiv:2204.08582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08582","description":"<p>We present the MASSIVE dataset--Multilingual Amazon Slu resource package\n(SLURP) for Slot-filling, Intent classification, and Virtual assistant\nEvaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant\nutterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE\nwas created by tasking professional translators to localize the English-only\nSLURP dataset into 50 typologically diverse languages from 29 genera. We also\npresent modeling results on XLM-R and mT5, including exact match accuracy,\nintent classification accuracy, and slot-filling F1 score. We have released our\ndataset, modeling code, and models publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_J/0/1/0/all/0/1\">Jack FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hench_C/0/1/0/all/0/1\">Christopher Hench</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackie_S/0/1/0/all/0/1\">Scott Mackie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_K/0/1/0/all/0/1\">Kay Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_A/0/1/0/all/0/1\">Ana Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_A/0/1/0/all/0/1\">Aaron Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbach_L/0/1/0/all/0/1\">Liam Urbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakarala_V/0/1/0/all/0/1\">Vishesh Kakarala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Richa Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_S/0/1/0/all/0/1\">Swetha Ranganath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crist_L/0/1/0/all/0/1\">Laurie Crist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Britan_M/0/1/0/all/0/1\">Misha Britan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeuwis_W/0/1/0/all/0/1\">Wouter Leeuwis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSe-Co: Text Conditioned Generative CommonSense Contextualizer. (arXiv:2206.05706v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05706","description":"<p>Pre-trained Language Models (PTLMs) have been shown to perform well on\nnatural language tasks. Many prior works have leveraged structured commonsense\npresent in the form of entities linked through labeled relations in Knowledge\nGraphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static\nmodule which limits coverage since KGs contain finite knowledge. Generative\nmethods train PTLMs on KG triples to improve the scale at which knowledge can\nbe obtained. However, training on symbolic KG entities limits their\napplicability in tasks involving natural language text where they ignore\noverall context. To mitigate this, we propose a CommonSense Contextualizer\n(CoSe-Co) conditioned on sentences as input to make it generically usable in\ntasks for generating knowledge relevant to the overall context of input text.\nTo train CoSe-Co, we propose a novel dataset comprising of sentence and\ncommonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and\ncontain novel entities not present in the underlying KG. We augment generated\nknowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading\nto improvements over current best methods on CSQA, ARC, QASC and OBQA datasets.\nWe also demonstrate its applicability in improving performance of a baseline\nmodel for paraphrase generation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rachit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Milan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_J/0/1/0/all/0/1\">Jivat Neet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonwords Pronunciation Classification in Language Development Tests for Preschool Children. (arXiv:2206.08058v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.08058","description":"<p>This work aims to automatically evaluate whether the language development of\nchildren is age-appropriate. Validated speech and language tests are used for\nthis purpose to test the auditory memory. In this work, the task is to\ndetermine whether spoken nonwords have been uttered correctly. We compare\ndifferent approaches that are motivated to model specific language structures:\nLow-level features (FFT), speaker embeddings (ECAPA-TDNN), grapheme-motivated\nembeddings (wav2vec 2.0), and phonetic embeddings in form of senones (ASR\nacoustic model). Each of the approaches provides input for VGG-like 5-layer CNN\nclassifiers. We also examine the adaptation per nonword. The evaluation of the\nproposed systems was performed using recordings from different kindergartens of\nspoken nonwords. ECAPA-TDNN and low-level FFT features do not explicitly model\nphonetic information; wav2vec2.0 is trained on grapheme labels, our ASR\nacoustic model features contain (sub-)phonetic information. We found that the\nmore granular the phonetic modeling is, the higher are the achieved recognition\nrates. The best system trained on ASR acoustic model features with VTLN\nachieved an accuracy of 89.4% and an area under the ROC (Receiver Operating\nCharacteristic) curve (AUC) of 0.923. This corresponds to an improvement in\naccuracy of 20.2% and AUC of 0.309 relative compared to the FFT-baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baumann_I/0/1/0/all/0/1\">Ilja Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bocklet_T/0/1/0/all/0/1\">Tobias Bocklet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All the World's a (Hyper)Graph: A Data Drama. (arXiv:2206.08225v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08225","description":"<p>We introduce Hyperbard, a dataset of diverse relational data representations\nderived from Shakespeare's plays. Our representations range from simple graphs\ncapturing character co-occurrence in single scenes to hypergraphs encoding\ncomplex communication settings and character contributions as hyperedges with\nedge-specific node weights. By making multiple intuitive representations\nreadily available for experimentation, we facilitate rigorous representation\nrobustness checks in graph learning, graph mining, and network analysis,\nhighlighting the advantages and drawbacks of specific representations.\nLeveraging the data released in Hyperbard, we demonstrate that many solutions\nto popular graph mining problems are highly dependent on the representation\nchoice, thus calling current graph curation practices into question. As an\nhomage to our data source, and asserting that science can also be art, we\npresent all our points in the form of a play.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coupette_C/0/1/0/all/0/1\">Corinna Coupette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1\">Jilles Vreeken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1\">Bastian Rieck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature Extraction from Downstream Tasks. (arXiv:2206.08398v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08398","description":"<p>Contemporary artificial neural networks (ANN) are trained end-to-end, jointly\nlearning both features and classifiers for the task of interest. Though\nenormously effective, this paradigm imposes significant costs in assembling\nannotated task-specific datasets and training large-scale networks. We propose\nto decouple feature learning from downstream lung ultrasound tasks by\nintroducing an auxiliary pre-task of visual biomarker classification. We\ndemonstrate that one can learn an informative, concise, and interpretable\nfeature space from ultrasound videos by training models for predicting\nbiomarker labels. Notably, biomarker feature extractors can be trained from\ndata annotated with weak video-scale supervision. These features can be used by\na variety of downstream Expert models targeted for diverse clinical tasks\n(Diagnosis, lung severity, S/F ratio). Crucially, task-specific expert models\nare comparable in accuracy to end-to-end models directly trained for such\ntarget tasks, while being significantly lower cost to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gare_G/0/1/0/all/0/1\">Gautam Rajendrakumar Gare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fox_T/0/1/0/all/0/1\">Tom Fox</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lowery_P/0/1/0/all/0/1\">Pete Lowery</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamora_K/0/1/0/all/0/1\">Kevin Zamora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_H/0/1/0/all/0/1\">Hai V. Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hutchins_L/0/1/0/all/0/1\">Laura Hutchins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montgomery_D/0/1/0/all/0/1\">David Montgomery</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishnan_A/0/1/0/all/0/1\">Amita Krishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Kannan Ramanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_R/0/1/0/all/0/1\">Ricardo Luis Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+deBoisblanc_B/0/1/0/all/0/1\">Bennett P deBoisblanc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1\">John Michael Galeotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going Deeper than Tracking: a Survey of Computer-Vision Based Recognition of Animal Pain and Affective States. (arXiv:2206.08405v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08405","description":"<p>Advances in animal motion tracking and pose recognition have been a game\nchanger in the study of animal behavior. Recently, an increasing number of\nworks go 'deeper' than tracking, and address automated recognition of animals'\ninternal states such as emotions and pain with the aim of improving animal\nwelfare, making this a timely moment for a systematization of the field. This\npaper provides a comprehensive survey of computer vision-based research on\nrecognition of affective states and pain in animals, addressing both facial and\nbodily behavior analysis. We summarize the efforts that have been presented so\nfar within this topic -- classifying them across different dimensions,\nhighlight challenges and research gaps, and provide best practice\nrecommendations for advancing the field, and some future directions for\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feighelstein_M/0/1/0/all/0/1\">Marcelo Feighelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamansky_A/0/1/0/all/0/1\">Anna Zamansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lencioni_G/0/1/0/all/0/1\">Gabriel Carreira Lencioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1\">Pia Haubro Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pessanha_F/0/1/0/all/0/1\">Francisca Pessanha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmoud_M/0/1/0/all/0/1\">Marwa Mahmoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salah_A/0/1/0/all/0/1\">Albert Ali Salah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time motion amplification on mobile devices. (arXiv:2206.08422v1 [cs.GR])","link":"http://arxiv.org/abs/2206.08422","description":"<p>A simple motion amplification algorithm suitable for real-time applications\non mobile devices is presented. It is based on motion enhancement by moving\naverage differencing (MEMAD), a temporal high-pass filter for video streams.\nMEMAD can amplify small moving objects or subtle motion in larger objects. It\nis computationally sufficiently simple to be implemented in real time on\nsmartphones. In the specific implementation as an Android phone app, MEMAD is\ndemonstrated on examples chosen such as to motivate applications in the\nengineering, biological, and medical sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voss_H/0/1/0/all/0/1\">Henning U. Voss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes. (arXiv:2206.08423v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08423","description":"<p>Indoor scenes exhibit significant appearance variations due to myriad\ninteractions between arbitrarily diverse object shapes, spatially-changing\nmaterials, and complex lighting. Shadows, highlights, and inter-reflections\ncaused by visible and invisible light sources require reasoning about\nlong-range interactions for inverse rendering, which seeks to recover the\ncomponents of image formation, namely, shape, material, and lighting. In this\nwork, our intuition is that the long-range attention learned by transformer\narchitectures is ideally suited to solve longstanding challenges in\nsingle-image inverse rendering. We demonstrate with a specific instantiation of\na dense vision transformer, IRISformer, that excels at both single-task and\nmulti-task reasoning required for inverse rendering. Specifically, we propose a\ntransformer architecture to simultaneously estimate depths, normals,\nspatially-varying albedo, roughness and lighting from a single image of an\nindoor scene. Our extensive evaluations on benchmark datasets demonstrate\nstate-of-the-art results on each of the above tasks, enabling applications like\nobject insertion and material editing in a single unconstrained real image,\nwith greater photorealism than prior works. Code and data are publicly released\nat https://github.com/ViLab-UCSD/IRISformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matai_J/0/1/0/all/0/1\">Janarbek Matai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SATBench: Benchmarking the speed-accuracy tradeoff in object recognition by humans and dynamic neural networks. (arXiv:2206.08427v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08427","description":"<p>The core of everyday tasks like reading and driving is active object\nrecognition. Attempts to model such tasks are currently stymied by the\ninability to incorporate time. People show a flexible tradeoff between speed\nand accuracy and this tradeoff is a crucial human skill. Deep neural networks\nhave emerged as promising candidates for predicting peak human object\nrecognition performance and neural activity. However, modeling the temporal\ndimension i.e., the speed-accuracy tradeoff (SAT), is essential for them to\nserve as useful computational models for how humans recognize objects. To this\nend, we here present the first large-scale (148 observers, 4 neural networks, 8\ntasks) dataset of the speed-accuracy tradeoff (SAT) in recognizing ImageNet\nimages. In each human trial, a beep, indicating the desired reaction time,\nsounds at a fixed delay after the image is presented, and observer's response\ncounts only if it occurs near the time of the beep. In a series of blocks, we\ntest many beep latencies, i.e., reaction times. We observe that human accuracy\nincreases with reaction time and proceed to compare its characteristics with\nthe behavior of several dynamic neural networks that are capable of\ninference-time adaptive computation. Using FLOPs as an analog for reaction\ntime, we compare networks with humans on curve-fit error, category-wise\ncorrelation, and curve steepness, and conclude that cascaded dynamic neural\nnetworks are a promising model of human reaction time in object recognition\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Ajay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_S/0/1/0/all/0/1\">Sara Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumbhar_O/0/1/0/all/0/1\">Omkar Kumbhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sizikova_E/0/1/0/all/0/1\">Elena Sizikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majaj_N/0/1/0/all/0/1\">Najib J. Majaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelli_D/0/1/0/all/0/1\">Denis G. Pelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation and Relighting of Human Eyes. (arXiv:2206.08428v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08428","description":"<p>A unique challenge in creating high-quality animatable and relightable 3D\navatars of people is modeling human eyes. The challenge of synthesizing eyes is\nmultifold as it requires 1) appropriate representations for the various\ncomponents of the eye and the periocular region for coherent viewpoint\nsynthesis, capable of representing diffuse, refractive and highly reflective\nsurfaces, 2) disentangling skin and eye appearance from environmental\nillumination such that it may be rendered under novel lighting conditions, and\n3) capturing eyeball motion and the deformation of the surrounding skin to\nenable re-gazing. These challenges have traditionally necessitated the use of\nexpensive and cumbersome capture setups to obtain high-quality results, and\neven then, modeling of the eye region holistically has remained elusive. We\npresent a novel geometry and appearance representation that enables\nhigh-fidelity capture and photorealistic animation, view synthesis and\nrelighting of the eye region using only a sparse set of lights and cameras. Our\nhybrid representation combines an explicit parametric surface model for the\neyeball with implicit deformable volumetric representations for the periocular\nregion and the interior of the eye. This novel hybrid model has been designed\nto address the various parts of that challenging facial area - the explicit\neyeball surface allows modeling refraction and high-frequency specular\nreflection at the cornea, whereas the implicit representation is well suited to\nmodel lower-frequency skin reflection via spherical harmonics and can represent\nnon-surface structures such as hair or diffuse volumetric bodies, both of which\nare a challenge for explicit surface models. We show that for high-resolution\nclose-ups of the eye, our model can synthesize high-fidelity animated gaze from\nnovel views under unseen illumination conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Muller_F/0/1/0/all/0/1\">Franziska M&#xfc;ller</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (2) ((1) Google Inc., (2) ETH Z&#xfc;rich)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Temporal Localization of Sensitive Activities in Movies and TV Episodes. (arXiv:2206.08429v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08429","description":"<p>To help customers make better-informed viewing choices, video-streaming\nservices try to moderate their content and provide more visibility into which\nportions of their movies and TV episodes contain age-appropriate material\n(e.g., nudity, sex, violence, or drug-use). Supervised models to localize these\nsensitive activities require large amounts of clip-level labeled data which is\nhard to obtain, while weakly-supervised models to this end usually do not offer\ncompetitive accuracy. To address this challenge, we propose a novel Coarse2Fine\nnetwork designed to make use of readily obtainable video-level weak labels in\nconjunction with sparse clip-level labels of age-appropriate activities. Our\nmodel aggregates frame-level predictions to make video-level classifications\nand is therefore able to leverage sparse clip-level labels along with\nvideo-level labels. Furthermore, by performing frame-level predictions in a\nhierarchical manner, our approach is able to overcome the label-imbalance\nproblem caused due to the rare-occurrence nature of age-appropriate content. We\npresent comparative results of our approach using 41,234 movies and TV episodes\n(~3 years of video-content) from 521 sub-genres and 250 countries making it by\nfar the largest-scale empirical analysis of age-appropriate activity\nlocalization in long-form videos ever published. Our approach offers 107.2%\nrelative mAP improvement (from 5.5% to 11.4%) over existing state-of-the-art\nactivity-localization approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shixing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_A/0/1/0/all/0/1\">Ahmed Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamid_R/0/1/0/all/0/1\">Raffay Hamid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology. (arXiv:2206.08439v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08439","description":"<p>Accurate intraoperative diagnosis is essential for providing safe and\neffective care during brain tumor surgery. Our standard-of-care diagnostic\nmethods are time, resource, and labor intensive, which restricts access to\noptimal surgical treatments. To address these limitations, we propose an\nalternative workflow that combines stimulated Raman histology (SRH), a rapid\noptical imaging method, with deep learning-based automated interpretation of\nSRH images for intraoperative brain tumor diagnosis and real-time surgical\ndecision support. Here, we present OpenSRH, the first public dataset of\nclinical SRH images from 300+ brain tumors patients and 1300+ unique whole\nslide optical images. OpenSRH contains data from the most common brain tumors\ndiagnoses, full pathologic annotations, whole slide tumor segmentations, raw\nand processed optical imaging data for end-to-end model development and\nvalidation. We provide a framework for patch-based whole slide SRH\nclassification and inference using weak (i.e. patient-level) diagnostic labels.\nFinally, we benchmark two computer vision tasks: multiclass histologic brain\ntumor classification and patch-based contrastive representation learning. We\nhope OpenSRH will facilitate the clinical translation of rapid optical imaging\nand real-time ML-based surgical decision support in order to improve the\naccess, safety, and efficacy of cancer surgery in the era of precision\nmedicine. Dataset access, code, and benchmarks are available at\nopensrh.mlins.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdury_A/0/1/0/all/0/1\">Asadur Chowdury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_X/0/1/0/all/0/1\">Xinhai Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kondepudi_A/0/1/0/all/0/1\">Akhil Kondepudi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Freudiger_C/0/1/0/all/0/1\">Christian W. Freudiger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Conway_K/0/1/0/all/0/1\">Kyle Conway</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Camelo_Piragua_S/0/1/0/all/0/1\">Sandra Camelo-Piragua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orringer_D/0/1/0/all/0/1\">Daniel A. Orringer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hollon_T/0/1/0/all/0/1\">Todd C. Hollon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TUSK: Task-Agnostic Unsupervised Keypoints. (arXiv:2206.08460v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08460","description":"<p>Existing unsupervised methods for keypoint learning rely heavily on the\nassumption that a specific keypoint type (e.g. elbow, digit, abstract geometric\nshape) appears only once in an image. This greatly limits their applicability,\nas each instance must be isolated before applying the method-an issue that is\nnever discussed or evaluated. We thus propose a novel method to learn\nTask-agnostic, UnSupervised Keypoints (TUSK) which can deal with multiple\ninstances. To achieve this, instead of the commonly-used strategy of detecting\nmultiple heatmaps, each dedicated to a specific keypoint type, we use a single\nheatmap for detection, and enable unsupervised learning of keypoint types\nthrough clustering. Specifically, we encode semantics into the keypoints by\nteaching them to reconstruct images from a sparse set of keypoints and their\ndescriptors, where the descriptors are forced to form distinct clusters in\nfeature space around learned prototypes. This makes our approach amenable to a\nwider range of tasks than any previous unsupervised keypoint method: we show\nexperiments on multiple-instance detection and classification, object\ndiscovery, and landmark detection-all unsupervised-with performance on par with\nthe state of the art, while also being able to deal with multiple instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuhe Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosang_J/0/1/0/all/0/1\">Jan Hosang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trulls_E/0/1/0/all/0/1\">Eduard Trulls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08462","description":"<p>Human vision involves parsing and representing objects and scenes using\nstructured representations based on part-whole hierarchies. Computer vision and\nmachine learning researchers have recently sought to emulate this capability\nusing capsule networks, reference frames and active predictive coding, but a\ngenerative model formulation has been lacking. We introduce Recursive Neural\nPrograms (RNPs), which, to our knowledge, is the first neural generative model\nto address the part-whole hierarchy learning problem. RNPs model images as\nhierarchical trees of probabilistic sensory-motor programs that recursively\nreuse learned sensory-motor primitives to model an image within different\nreference frames, forming recursive image grammars. We express RNPs as\nstructured variational autoencoders (sVAEs) for inference and sampling, and\ndemonstrate parts-based parsing, sampling and one-shot transfer learning for\nMNIST, Omniglot and Fashion-MNIST datasets, demonstrating the model's\nexpressive power. Our results show that RNPs provide an intuitive and\nexplainable way of composing objects and scenes, allowing rich compositionality\nand intuitive interpretations of objects in terms of part-whole hierarchies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fisher_A/0/1/0/all/0/1\">Ares Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajesh P.N. Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot AutoML with Pretrained Models. (arXiv:2206.08476v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08476","description":"<p>Given a new dataset D and a low compute budget, how should we choose a\npre-trained model to fine-tune to D, and set the fine-tuning hyperparameters\nwithout risking overfitting, particularly if D is small? Here, we extend\nautomated machine learning (AutoML) to best make these choices. Our\ndomain-independent meta-learning approach learns a zero-shot surrogate model\nwhich, at test time, allows to select the right deep learning (DL) pipeline\n(including the pre-trained model and fine-tuning hyperparameters) for a new\ndataset D given only trivial meta-features describing D such as image\nresolution or the number of classes. To train this zero-shot model, we collect\nperformance data for many DL pipelines on a large collection of datasets and\nmeta-train on this data to minimize a pairwise ranking objective. We evaluate\nour approach under the strict time limit of the vision track of the ChaLearn\nAutoDL challenge benchmark, clearly outperforming all challenge contenders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozturk_E/0/1/0/all/0/1\">Ekrem &#xd6;zt&#xfc;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1\">Fabio Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jomaa_H/0/1/0/all/0/1\">Hadi S. Jomaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabocka_J/0/1/0/all/0/1\">Josif Grabocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks on Vision Transformers. (arXiv:2206.08477v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08477","description":"<p>Vision Transformers (ViT) have recently demonstrated exemplary performance on\na variety of vision tasks and are being used as an alternative to CNNs. Their\ndesign is based on a self-attention mechanism that processes images as a\nsequence of patches, which is quite different compared to CNNs. Hence it is\ninteresting to study if ViTs are vulnerable to backdoor attacks. Backdoor\nattacks happen when an attacker poisons a small part of the training data for\nmalicious purposes. The model performance is good on clean test images, but the\nattacker can manipulate the decision of the model by showing the trigger at\ntest time. To the best of our knowledge, we are the first to show that ViTs are\nvulnerable to backdoor attacks. We also find an intriguing difference between\nViTs and CNNs - interpretation algorithms effectively highlight the trigger on\ntest images for ViTs but not for CNNs. Based on this observation, we propose a\ntest-time image blocking defense for ViTs which reduces the attack success rate\nby a large margin. Code is available here:\nhttps://github.com/UCDvision/backdoor_transformer.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanya_A/0/1/0/all/0/1\">Akshayvarun Subramanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aniruddha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orientation-guided Graph Convolutional Network for Bone Surface Segmentation. (arXiv:2206.08481v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08481","description":"<p>Due to imaging artifacts and low signal-to-noise ratio in ultrasound images,\nautomatic bone surface segmentation networks often produce fragmented\npredictions that can hinder the success of ultrasound-guided computer-assisted\nsurgical procedures. Existing pixel-wise predictions often fail to capture the\naccurate topology of bone tissues due to a lack of supervision to enforce\nconnectivity. In this work, we propose an orientation-guided graph\nconvolutional network to improve connectivity while segmenting the bone\nsurface. We also propose an additional supervision on the orientation of the\nbone surface to further impose connectivity. We validated our approach on 1042\nvivo US scans of femur, knee, spine, and distal radius. Our approach improves\nover the state-of-the-art methods by 5.01% in connectivity metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rahman_A/0/1/0/all/0/1\">Aimon Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacihaliloglu_I/0/1/0/all/0/1\">Ilker Hacihaliloglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Image Enhancement. (arXiv:2206.08488v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08488","description":"<p>Editing flat-looking images into stunning photographs requires skill and\ntime. Automated image enhancement algorithms have attracted increased interest\nby generating high-quality images without user interaction. However, the\nquality assessment of a photograph is subjective. Even in tone and color\nadjustments, a single photograph of auto-enhancement is challenging to fit user\npreferences which are subtle and even changeable. To address this problem, we\npresent a semiautomatic image enhancement algorithm that can generate\nhigh-quality images with multiple styles by controlling a few parameters. We\nfirst disentangle photo retouching skills from high-quality images and build an\nefficient enhancement system for each skill. Specifically, an encoder-decoder\nframework encodes the retouching skills into latent codes and decodes them into\nthe parameters of image signal processing (ISP) functions. The ISP functions\nare computationally efficient and consist of only 19 parameters. Despite our\napproach requiring multiple inferences to obtain the desired result,\nexperimental results present that the proposed method achieves state-of-the-art\nperformances on the benchmark dataset for image quality and model efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape Collections. (arXiv:2206.08497v1 [cs.GR])","link":"http://arxiv.org/abs/2206.08497","description":"<p>3D models of manufactured objects are important for populating virtual worlds\nand for synthetic data generation for vision and robotics. To be most useful,\nsuch objects should be articulated: their parts should move when interacted\nwith. While articulated object datasets exist, creating them is\nlabor-intensive. Learning-based prediction of part motions can help, but all\nexisting methods require annotated training data. In this paper, we present an\nunsupervised approach for discovering articulated motions in a part-segmented\n3D shape collection. Our approach is based on a concept we call category\nclosure: any valid articulation of an object's parts should keep the object in\nthe same semantic category (e.g. a chair stays a chair). We operationalize this\nconcept with an algorithm that optimizes a shape's part motion parameters such\nthat it can transform into other shapes in the collection. We evaluate our\napproach by using it to re-discover part motions from the PartNet-Mobility\ndataset. For almost all shape categories, our method's predicted motion\nparameters have low error with respect to ground truth annotations,\noutperforming two supervised motion prediction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xianghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yifan Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do navigation agents learn about their environment?. (arXiv:2206.08500v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08500","description":"<p>Today's state of the art visual navigation agents typically consist of large\ndeep learning models trained end to end. Such models offer little to no\ninterpretability about the learned skills or the actions of the agent taken in\nresponse to its environment. While past works have explored interpreting deep\nlearning models, little attention has been devoted to interpreting embodied AI\nsystems, which often involve reasoning about the structure of the environment,\ntarget characteristics and the outcome of one's actions. In this paper, we\nintroduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal\nand Object Goal navigation agents. We use iSEE to probe the dynamic\nrepresentations produced by these agents for the presence of information about\nthe agent as well as the environment. We demonstrate interesting insights about\nnavigation agents using iSEE, including the ability to encode reachable\nlocations (to avoid obstacles), visibility of the target, progress from the\ninitial spawn location as well as the dramatic effect on the behaviors of\nagents when we mask out critical individual neurons. The code is available at:\nhttps://github.com/allenai/iSEE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_K/0/1/0/all/0/1\">Kshitij Dwivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1\">Gemma Roig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Adaptation for Object Detection by Searching Channel Dimensions and Mapping Pre-trained Parameters. (arXiv:2206.08509v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08509","description":"<p>Most object detection frameworks use backbone architectures originally\ndesigned for image classification, conventionally with pre-trained parameters\non ImageNet. However, image classification and object detection are essentially\ndifferent tasks and there is no guarantee that the optimal backbone for\nclassification is also optimal for object detection. Recent neural architecture\nsearch (NAS) research has demonstrated that automatically designing a backbone\nspecifically for object detection helps improve the overall accuracy. In this\npaper, we introduce a neural architecture adaptation method that can optimize\nthe given backbone for detection purposes, while still allowing the use of\npre-trained parameters. We propose to adapt both the micro- and\nmacro-architecture by searching for specific operations and the number of\nlayers, in addition to the output channel dimensions of each block. It is\nimportant to find the optimal channel depth, as it greatly affects the feature\nrepresentation capability and computation cost. We conduct experiments with our\nsearched backbone for object detection and demonstrate that our backbone\noutperforms both manually designed and searched state-of-the-art backbones on\nthe COCO dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Harim Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Myeong-Seok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheoljong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Solid State LiDAR Odometry Using Continuous-time Filter Registration. (arXiv:2206.08517v1 [cs.RO])","link":"http://arxiv.org/abs/2206.08517","description":"<p>Solid-state LiDARs are more compact and cheaper than the conventional\nmechanical multi-line spinning LiDARs, which have become increasingly popular\nin autonomous driving recently. However, there are several challenges for these\nnew LiDAR sensors, including severe motion distortions, small field of view and\nsparse point cloud, which hinder them from being widely used in LiDAR odometry.\nTo tackle these problems, we present an effective continuous-time LiDAR\nodometry (ECTLO) method for the Risley prism-based LiDARs with non-repetitive\nscanning patterns. To account for the noisy data, a filter-based point-to-plane\nGaussian Mixture Model is used for robust registration. Moreover, a LiDAR-only\ncontinuous-time motion model is employed to relieve the inevitable distortions.\nTo facilitate the implicit data association in parallel, we maintain all map\npoints within a single range image. Extensive experiments have been conducted\non various testbeds using the solid-state LiDARs with different scanning\npatterns, whose promising results demonstrate the efficacy of our proposed\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation. (arXiv:2206.08522v1 [cs.RO])","link":"http://arxiv.org/abs/2206.08522","description":"<p>Benefiting from language flexibility and compositionality, humans naturally\nintend to use language to command an embodied agent for complex tasks such as\nnavigation and object manipulation. In this work, we aim to fill the blank of\nthe last mile of embodied agents -- object manipulation by following human\nguidance, e.g., \"move the red mug next to the box while keeping it upright.\" To\nthis end, we introduce an Automatic Manipulation Solver (AMSolver) simulator\nand build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,\ncontaining various language instructions on categorized robotic manipulation\ntasks. Specifically, modular rule-based task templates are created to\nautomatically generate robot demonstrations with language instructions,\nconsisting of diverse object shapes and appearances, action types, and motion\nconstraints. We also develop a keypoint-based model 6D-CLIPort to deal with\nmulti-view observations and language input and output a sequence of 6 degrees\nof freedom (DoF) actions. We hope the new simulator and benchmark will\nfacilitate future research on language-guided robotic manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaizhi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_O/0/1/0/all/0/1\">Odest Chadwicke Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDNet: Contrastive Disentangled Network for Fine-Grained Image Categorization of Ocular B-Scan Ultrasound. (arXiv:2206.08524v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08524","description":"<p>Precise and rapid categorization of images in the B-scan ultrasound modality\nis vital for diagnosing ocular diseases. Nevertheless, distinguishing various\ndiseases in ultrasound still challenges experienced ophthalmologists. Thus a\nnovel contrastive disentangled network (CDNet) is developed in this work,\naiming to tackle the fine-grained image categorization (FGIC) challenges of\nocular abnormalities in ultrasound images, including intraocular tumor (IOT),\nretinal detachment (RD), posterior scleral staphyloma (PSS), and vitreous\nhemorrhage (VH). Three essential components of CDNet are the weakly-supervised\nlesion localization module (WSLL), contrastive multi-zoom (CMZ) strategy, and\nhyperspherical contrastive disentangled loss (HCD-Loss), respectively. These\ncomponents facilitate feature disentanglement for fine-grained recognition in\nboth the input and output aspects. The proposed CDNet is validated on our ZJU\nOcular Ultrasound Dataset (ZJUOUSD), consisting of 5213 samples. Furthermore,\nthe generalization ability of CDNet is validated on two public and widely-used\nchest X-ray FGIC benchmarks. Quantitative and qualitative results demonstrate\nthe efficacy of our proposed CDNet, which achieves state-of-the-art performance\nin the FGIC task. Code is available at:\nhttps://github.com/ZeroOneGame/CDNet-for-OUS-FGIC .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dan_R/0/1/0/all/0/1\">Ruilong Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Gangyong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Ruiquan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Juan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qun Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Margin Representation Learning for Texture Classification. (arXiv:2206.08537v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08537","description":"<p>This paper presents a novel approach combining convolutional layers (CLs) and\nlarge-margin metric learning for training supervised models on small datasets\nfor texture classification. The core of such an approach is a loss function\nthat computes the distances between instances of interest and support vectors.\nThe objective is to update the weights of CLs iteratively to learn a\nrepresentation with a large margin between classes. Each iteration results in a\nlarge-margin discriminant model represented by support vectors based on such a\nrepresentation. The advantage of the proposed approach w.r.t. convolutional\nneural networks (CNNs) is two-fold. First, it allows representation learning\nwith a small amount of data due to the reduced number of parameters compared to\nan equivalent CNN. Second, it has a low training cost since the backpropagation\nconsiders only support vectors. The experimental results on texture and\nhistopathologic image datasets have shown that the proposed approach achieves\ncompetitive accuracy with lower computational cost and faster convergence when\ncompared to equivalent CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matos_J/0/1/0/all/0/1\">Jonathan de Matos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Luiz Eduardo Soares de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Alceu de Souza Britto Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro Lameiras Koerich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Classification of Brain Tumor Images Using Transfer Learning Based Deep Neural Network. (arXiv:2206.08543v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08543","description":"<p>In recent advancement towards computer based diagnostics system, the\nclassification of brain tumor images is a challenging task. This paper mainly\nfocuses on elevating the classification accuracy of brain tumor images with\ntransfer learning based deep neural network. The classification approach is\nstarted with the image augmentation operation including rotation, zoom,\nhori-zontal flip, width shift, height shift, and shear to increase the\ndiversity in image datasets. Then the general features of the input brain tumor\nimages are extracted based on a pre-trained transfer learning method comprised\nof Inception-v3. Fi-nally, the deep neural network with 4 customized layers is\nemployed for classi-fying the brain tumors in most frequent brain tumor types\nas meningioma, glioma, and pituitary. The proposed model acquires an effective\nperformance with an overall accuracy of 96.25% which is much improved than some\nexisting multi-classification methods. Whereas, the fine-tuning of\nhyper-parameters and inclusion of customized DNN with the Inception-v3 model\nresults in an im-provement of the classification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dutta_P/0/1/0/all/0/1\">Pramit Dutta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sathi_K/0/1/0/all/0/1\">Khaleda Akhter Sathi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1\">Md. Saiful Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Texture Generation Using Graph Generative Adversarial Network And Differentiable Rendering. (arXiv:2206.08547v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08547","description":"<p>Novel texture synthesis for existing 3D mesh models is an important step\ntowards photo realistic asset generation for existing simulators. But existing\nmethods inherently work in the 2D image space which is the projection of the 3D\nspace from a given camera perspective. These methods take camera angle, 3D\nmodel information, lighting information and generate photorealistic 2D image.\nTo generate a photorealistic image from another perspective or lighting, we\nneed to make a computationally expensive forward pass each time we change the\nparameters. Also, it is hard to generate such images for a simulator that can\nsatisfy the temporal constraints the sequences of images should be similar but\nonly need to change the viewpoint of lighting as desired. The solution can not\nbe directly integrated with existing tools like Blender and Unreal Engine.\nManual solution is expensive and time consuming. We thus present a new system\ncalled a graph generative adversarial network (GGAN) that can generate textures\nwhich can be directly integrated into a given 3D mesh models with tools like\nBlender and Unreal Engine and can be simulated from any perspective and\nlighting condition easily.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+KC_D/0/1/0/all/0/1\">Dharma KC</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_C/0/1/0/all/0/1\">Clayton T. Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walls_B/0/1/0/all/0/1\">Bradley Walls</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images. (arXiv:2206.08549v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08549","description":"<p>Evaluation metrics in image synthesis play a key role to measure performances\nof generative models. However, most metrics mainly focus on image fidelity.\nExisting diversity metrics are derived by comparing distributions, and thus\nthey cannot quantify the diversity or rarity degree of each generated image. In\nthis work, we propose a new evaluation metric, called `rarity score', to\nmeasure the individual rarity of each image synthesized by generative models.\nWe first show empirical observation that common samples are close to each other\nand rare samples are far from each other in nearest-neighbor distances of\nfeature space. We then use our metric to demonstrate that the extent to which\ndifferent generative models produce rare images can be effectively compared. We\nalso propose a method to compare rarities between datasets that share the same\nconcept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in\ndifferent designs of feature spaces to better understand the relationship\nbetween feature spaces and resulting sparse images. Code will be publicly\navailable online for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyeon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hwanil Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaesik Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Detection using Transfer Learning with Convolutional Neural Network. (arXiv:2206.08557v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08557","description":"<p>The Novel Coronavirus disease 2019 (COVID-19) is a fatal infectious disease,\nfirst recognized in December 2019 in Wuhan, Hubei, China, and has gone on an\nepidemic situation. Under these circumstances, it became more important to\ndetect COVID-19 in infected people. Nowadays, the testing kits are gradually\nlessening in number compared to the number of infected population. Under recent\nprevailing conditions, the diagnosis of lung disease by analyzing chest CT\n(Computed Tomography) images has become an important tool for both diagnosis\nand prophecy of COVID-19 patients. In this study, a Transfer learning strategy\n(CNN) for detecting COVID-19 infection from CT images has been proposed. In the\nproposed model, a multilayer Convolutional neural network (CNN) with Transfer\nlearning model Inception V3 has been designed. Similar to CNN, it uses\nconvolution and pooling to extract features, but this transfer learning model\ncontains weights of dataset Imagenet. Thus it can detect features very\neffectively which gives it an upper hand for achieving better accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dutta_P/0/1/0/all/0/1\">Pramit Dutta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_T/0/1/0/all/0/1\">Tanny Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anjum_N/0/1/0/all/0/1\">Nafisa Anjum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Data Discovery: Mining Unknown Data using Submodular Information Measures. (arXiv:2206.08566v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08566","description":"<p>Active Learning is a very common yet powerful framework for iteratively and\nadaptively sampling subsets of the unlabeled sets with a human in the loop with\nthe goal of achieving labeling efficiency. Most real world datasets have\nimbalance either in classes and slices, and correspondingly, parts of the\ndataset are rare. As a result, there has been a lot of work in designing active\nlearning approaches for mining these rare data instances. Most approaches\nassume access to a seed set of instances which contain these rare data\ninstances. However, in the event of more extreme rareness, it is reasonable to\nassume that these rare data instances (either classes or slices) may not even\nbe present in the seed labeled set, and a critical need for the active learning\nparadigm is to efficiently discover these rare data instances. In this work, we\nprovide an active data discovery framework which can mine unknown data slices\nand classes efficiently using the submodular conditional gain and submodular\nconditional mutual information functions. We provide a general algorithmic\nframework which works in a number of scenarios including image classification\nand object detection and works with both rare classes and rare slices present\nin the unlabeled set. We show significant accuracy and labeling efficiency\ngains with our approach compared to existing state-of-the-art active learning\napproaches for actively discovering these rare classes and slices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chopra_S/0/1/0/all/0/1\">Shivang Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saikat Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rectify ViT Shortcut Learning by Visual Saliency. (arXiv:2206.08567v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08567","description":"<p>Shortcut learning is common but harmful to deep learning models, leading to\ndegenerated feature representations and consequently jeopardizing the model's\ngeneralizability and interpretability. However, shortcut learning in the widely\nused Vision Transformer framework is largely unknown. Meanwhile, introducing\ndomain-specific knowledge is a major approach to rectifying the shortcuts,\nwhich are predominated by background related factors. For example, in the\nmedical imaging field, eye-gaze data from radiologists is an effective human\nvisual prior knowledge that has the great potential to guide the deep learning\nmodels to focus on meaningful foreground regions of interest. However,\nobtaining eye-gaze data is time-consuming, labor-intensive and sometimes even\nnot practical. In this work, we propose a novel and effective saliency-guided\nvision transformer (SGT) model to rectify shortcut learning in ViT with the\nabsence of eye-gaze data. Specifically, a computational visual saliency model\nis adopted to predict saliency maps for input image samples. Then, the saliency\nmaps are used to distil the most informative image patches. In the proposed\nSGT, the self-attention among image patches focus only on the distilled\ninformative ones. Considering this distill operation may lead to global\ninformation lost, we further introduce, in the last encoder layer, a residual\nconnection that captures the self-attention across all the image patches. The\nexperiment results on four independent public datasets show that our SGT\nframework can effectively learn and leverage human prior knowledge without eye\ngaze data and achieves much better performance than baselines. Meanwhile, it\nsuccessfully rectifies the harmful shortcut learning and significantly improves\nthe interpretability of the ViT model, demonstrating the promise of\ntransferring human prior knowledge derived visual saliency in rectifying\nshortcut learning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">David Weizhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xintao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Contextual Predictions with Vision Transformer for Video Anomaly Detection. (arXiv:2206.08568v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08568","description":"<p>Video Anomaly Detection(VAD) has been traditionally tackled in two main\nmethodologies: the reconstruction-based approach and the prediction-based one.\nAs the reconstruction-based methods learn to generalize the input image, the\nmodel merely learns an identity function and strongly causes the problem called\ngeneralizing issue. On the other hand, since the prediction-based ones learn to\npredict a future frame given several previous frames, they are less sensitive\nto the generalizing issue. However, it is still uncertain if the model can\nlearn the spatio-temporal context of a video. Our intuition is that the\nunderstanding of the spatio-temporal context of a video plays a vital role in\nVAD as it provides precise information on how the appearance of an event in a\nvideo clip changes. Hence, to fully exploit the context information for anomaly\ndetection in video circumstances, we designed the transformer model with three\ndifferent contextual prediction streams: masked, whole and partial. By learning\nto predict the missing frames of consecutive normal frames, our model can\neffectively learn various normality patterns in the video, which leads to a\nhigh reconstruction error at the abnormal cases that are unsuitable to the\nlearned context. To verify the effectiveness of our approach, we assess our\nmodel on the public benchmark datasets: USCD Pedestrian 2, CUHK Avenue and\nShanghaiTech and evaluate the performance with the anomaly score metric of\nreconstruction error. The results demonstrate that our proposed approach\nachieves a competitive performance compared to the existing video anomaly\ndetection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joo-Yeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_W/0/1/0/all/0/1\">Woo-Jeoung Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Bi-directional Motion Estimation for Video Frame Interpolation. (arXiv:2206.08572v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08572","description":"<p>We present a novel simple yet effective algorithm for motion-based video\nframe interpolation. Existing motion-based interpolation methods typically rely\non a pre-trained optical flow model or a U-Net based pyramid network for motion\nestimation, which either suffer from large model size or limited capacity in\nhandling complex and large motion cases. In this work, by carefully integrating\nintermediateoriented forward-warping, lightweight feature encoder, and\ncorrelation volume into a pyramid recurrent framework, we derive a compact\nmodel to simultaneously estimate the bidirectional motion between input frames.\nIt is 15 times smaller in size than PWC-Net, yet enables more reliable and\nflexible handling of challenging motion cases. Based on estimated\nbi-directional motion, we forward-warp input frames and their context features\nto intermediate frame, and employ a synthesis network to estimate the\nintermediate frame from warped representations. Our method achieves excellent\nperformance on a broad range of video frame interpolation benchmarks. Code will\nbe available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longhai_W/0/1/0/all/0/1\">Wu Longhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guotao_S/0/1/0/all/0/1\">Shen Guotao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youxin_C/0/1/0/all/0/1\">Chen Youxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Chen Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayoon_K/0/1/0/all/0/1\">Koo Jayoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheul_hee_H/0/1/0/all/0/1\">Hahm Cheul-hee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignment and Semantic-Region-Aware Inpainting. (arXiv:2206.08585v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08585","description":"<p>Hairstyle transfer is the task of modifying a source hairstyle to a target\none. Although recent hairstyle transfer models can reflect the delicate\nfeatures of hairstyles, they still have two major limitations. First, the\nexisting methods fail to transfer hairstyles when a source and a target image\nhave different poses (e.g., viewing direction or face size), which is prevalent\nin the real world. Also, the previous models generate unrealistic images when\nthere is a non-trivial amount of regions in the source image occluded by its\noriginal hair. When modifying long hair to short hair, shoulders or backgrounds\noccluded by the long hair need to be inpainted. To address these issues, we\npropose a novel framework for pose-invariant hairstyle transfer, HairFIT. Our\nmodel consists of two stages: 1) flow-based hair alignment and 2) hair\nsynthesis. In the hair alignment stage, we leverage a keypoint-based optical\nflow estimator to align a target hairstyle with a source pose. Then, we\ngenerate a final hairstyle-transferred image in the hair synthesis stage based\non Semantic-region-aware Inpainting Mask (SIM) estimator. Our SIM estimator\ndivides the occluded regions in the source image into different semantic\nregions to reflect their distinct features during the inpainting. To\ndemonstrate the effectiveness of our model, we conduct quantitative and\nqualitative evaluations using multi-view datasets, K-hairstyle and VoxCeleb.\nThe results indicate that HairFIT achieves a state-of-the-art performance by\nsuccessfully transferring hairstyles between images of different poses, which\nhas never been achieved before.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Chaeyeon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1\">Hyelin Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seunghwan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Gyojung Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Efficient Real-Time Semantic Segmentation: A Survey. (arXiv:2206.08605v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08605","description":"<p>Semantic segmentation is the problem of assigning a class label to every\npixel in an image, and is an important component of an autonomous vehicle\nvision stack for facilitating scene understanding and object detection.\nHowever, many of the top performing semantic segmentation models are extremely\ncomplex and cumbersome, and as such are not suited to deployment onboard\nautonomous vehicle platforms where computational resources are limited and\nlow-latency operation is a vital requirement. In this survey, we take a\nthorough look at the works that aim to address this misalignment with more\ncompact and efficient models capable of deployment on low-memory embedded\nsystems while meeting the constraint of real-time inference. We discuss several\nof the most prominent works in the field, placing them within a taxonomy based\non their major contributions, and finally we evaluate the inference speed of\nthe discussed models under consistent hardware and software setups that\nrepresent a typical research environment with high-end GPU and a realistic\ndeployed scenario using low-memory embedded GPU hardware. Our experimental\nresults demonstrate that many works are capable of real-time performance on\nresource-constrained hardware, while illustrating the consistent trade-off\nbetween latency and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holder_C/0/1/0/all/0/1\">Christopher J. Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Autoencoders for Generic Event Boundary Detection CVPR'2022 Kinetics-GEBD Challenge. (arXiv:2206.08610v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08610","description":"<p>Generic Event Boundary Detection (GEBD) tasks aim at detecting generic,\ntaxonomy-free event boundaries that segment a whole video into chunks. In this\npaper, we apply Masked Autoencoders to improve algorithm performance on the\nGEBD tasks. Our approach mainly adopted the ensemble of Masked Autoencoders\nfine-tuned on the GEBD task as a self-supervised learner with other base\nmodels. Moreover, we also use a semi-supervised pseudo-label method to take\nfull advantage of the abundant unlabeled Kinetics-400 data while training. In\naddition, we propose a soft-label method to partially balance the positive and\nnegative samples and alleviate the problem of ambiguous labeling in this task.\nLastly, a tricky segmentation alignment policy is implemented to refine\nboundaries predicted by our models to more accurate locations. With our\napproach, we achieved 85.94% on the F1-score on the Kinetics-GEBD test set,\nwhich improved the F1-score by 2.31% compared to the winner of the 2021\nKinetics-GEBD Challenge. Our code is available at\nhttps://github.com/ContentAndMaterialPortrait/MAE-GEBD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Rui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuanxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youzeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zuwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Feng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing. (arXiv:2206.08612v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08612","description":"<p>Optoacoustic (OA) imaging is based on excitation of biological tissues with\nnanosecond-duration laser pulses followed by subsequent detection of ultrasound\nwaves generated via light-absorption-mediated thermoelastic expansion. OA\nimaging features a powerful combination between rich optical contrast and high\nresolution in deep tissues. This enabled the exploration of a number of\nattractive new applications both in clinical and laboratory settings. However,\nno standardized datasets generated with different types of experimental set-up\nand associated processing methods are available to facilitate advances in\nbroader applications of OA in clinical settings. This complicates an objective\ncomparison between new and established data processing methods, often leading\nto qualitative results and arbitrary interpretations of the data. In this\npaper, we provide both experimental and synthetic OA raw signals and\nreconstructed image domain datasets rendered with different experimental\nparameters and tomographic acquisition geometries. We further provide trained\nneural networks to tackle three important challenges related to OA image\nprocessing, namely accurate reconstruction under limited view tomographic\nconditions, removal of spatial undersampling artifacts and anatomical\nsegmentation for improved image reconstruction. Specifically, we define 18\nexperiments corresponding to the aforementioned challenges as benchmarks to be\nused as a reference for the development of more advanced processing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lafci_B/0/1/0/all/0/1\">Berkan Lafci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozdemir_F/0/1/0/all/0/1\">Firat Ozdemir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dean_Ben_X/0/1/0/all/0/1\">Xos&#xe9; Lu&#xed;s De&#xe1;n-Ben</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razansky_D/0/1/0/all/0/1\">Daniel Razansky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perez_Cruz_F/0/1/0/all/0/1\">Fernando Perez-Cruz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment. (arXiv:2206.08614v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08614","description":"<p>Computational inference of aesthetics is an ill-defined task due to its\nsubjective nature. Many datasets have been proposed to tackle the problem by\nproviding pairs of images and aesthetic scores based on human ratings. However,\nhumans are better at expressing their opinion, taste, and emotions by means of\nlanguage rather than summarizing them in a single number. In fact, photo\ncritiques provide much richer information as they reveal how and why users rate\nthe aesthetics of visual stimuli. In this regard, we propose the Reddit Photo\nCritique Dataset (RPCD), which contains tuples of image and photo critiques.\nRPCD consists of 74K images and 220K comments and is collected from a Reddit\ncommunity used by hobbyists and professional photographers to improve their\nphotography skills by leveraging constructive community feedback. The proposed\ndataset differs from previous aesthetics datasets mainly in three aspects,\nnamely (i) the large scale of the dataset and the extension of the comments\ncriticizing different aspects of the image, (ii) it contains mostly UltraHD\nimages, and (iii) it can easily be extended to new data as it is collected\nthrough an automatic pipeline. To the best of our knowledge, in this work, we\npropose the first attempt to estimate the aesthetic quality of visual stimuli\nfrom the critiques. To this end, we exploit the polarity of the sentiment of\ncriticism as an indicator of aesthetic judgment. We demonstrate how sentiment\npolarity correlates positively with the aesthetic judgment available for two\naesthetic assessment benchmarks. Finally, we experiment with several models by\nusing the sentiment scores as a target for ranking images. Dataset and\nbaselines are available (https://github.com/mediatechnologycenter/aestheval).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nieto_D/0/1/0/all/0/1\">Daniel Vera Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celona_L/0/1/0/all/0/1\">Luigi Celona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Labrador_C/0/1/0/all/0/1\">Clara Fernandez-Labrador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Using Privileged Information for Zero-Shot Action Recognition. (arXiv:2206.08632v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08632","description":"<p>Zero-Shot Action Recognition (ZSAR) aims to recognize video actions that have\nnever been seen during training. Most existing methods assume a shared semantic\nspace between seen and unseen actions and intend to directly learn a mapping\nfrom a visual space to the semantic space. This approach has been challenged by\nthe semantic gap between the visual space and semantic space. This paper\npresents a novel method that uses object semantics as privileged information to\nnarrow the semantic gap and, hence, effectively, assist the learning. In\nparticular, a simple hallucination network is proposed to implicitly extract\nobject semantics during testing without explicitly extracting objects and a\ncross-attention module is developed to augment visual feature with the object\nsemantics. Experiments on the Olympic Sports, HMDB51 and UCF101 datasets have\nshown that the proposed method outperforms the state-of-the-art methods by a\nlarge margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zihui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yonghong Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum Noticeable Difference based Adversarial Privacy Preserving Image Generation. (arXiv:2206.08638v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08638","description":"<p>Deep learning models are found to be vulnerable to adversarial examples, as\nwrong predictions can be caused by small perturbation in input for deep\nlearning models. Most of the existing works of adversarial image generation try\nto achieve attacks for most models, while few of them make efforts on\nguaranteeing the perceptual quality of the adversarial examples. High quality\nadversarial examples matter for many applications, especially for the privacy\npreserving. In this work, we develop a framework based on the Minimum\nNoticeable Difference (MND) concept to generate adversarial privacy preserving\nimages that have minimum perceptual difference from the clean ones but are able\nto attack deep learning models. To achieve this, an adversarial loss is firstly\nproposed to make the deep learning models attacked by the adversarial images\nsuccessfully. Then, a perceptual quality-preserving loss is developed by taking\nthe magnitude of perturbation and perturbation-caused structural and gradient\nchanges into account, which aims to preserve high perceptual quality for\nadversarial image generation. To the best of our knowledge, this is the first\nwork on exploring quality-preserving adversarial image generation based on the\nMND concept for privacy preserving. To evaluate its performance in terms of\nperceptual quality, the deep models on image classification and face\nrecognition are tested with the proposed method and several anchor methods in\nthis work. Extensive experimental results demonstrate that the proposed MND\nframework is capable of generating adversarial images with remarkably improved\nperformance metrics (e.g., PSNR, SSIM, and MOS) than that generated with the\nanchor methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware Evaluation of Time-Series Classification for Online Handwriting Recognition with Domain Shift. (arXiv:2206.08640v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08640","description":"<p>For many applications, analyzing the uncertainty of a machine learning model\nis indispensable. While research of uncertainty quantification (UQ) techniques\nis very advanced for computer vision applications, UQ methods for\nspatio-temporal data are less studied. In this paper, we focus on models for\nonline handwriting recognition, one particular type of spatio-temporal data.\nThe data is observed from a sensor-enhanced pen with the goal to classify\nwritten characters. We conduct a broad evaluation of aleatoric (data) and\nepistemic (model) UQ based on two prominent techniques for Bayesian inference,\nStochastic Weight Averaging-Gaussian (SWAG) and Deep Ensembles. Next to a\nbetter understanding of the model, UQ techniques can detect out-of-distribution\ndata and domain shifts when combining right-handed and left-handed writers (an\nunderrepresented group).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klass_A/0/1/0/all/0/1\">Andreas Kla&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_S/0/1/0/all/0/1\">Sven M. Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauer_Schmaltz_M/0/1/0/all/0/1\">Martin W. Lauer-Schmaltz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1\">Christopher Mutschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1\">Felix Ott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Diversity of Multiple Trajectory Prediction based on Map-adaptive Lane Loss. (arXiv:2206.08641v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08641","description":"<p>Prior arts in the field of motion predictions for autonomous driving tend to\nfocus on finding a trajectory that is close to the ground truth trajectory.\nSuch problem formulations and approaches, however, frequently lead to loss of\ndiversity and biased trajectory predictions. Therefore, they are unsuitable for\nreal-world autonomous driving where diverse and road-dependent multimodal\ntrajectory predictions are critical for safety. To this end, this study\nproposes a novel loss function, \\textit{Lane Loss}, that ensures map-adaptive\ndiversity and accommodates geometric constraints. A two-stage trajectory\nprediction architecture with a novel trajectory candidate proposal module,\n\\textit{Trajectory Prediction Attention (TPA)}, is trained with Lane Loss\nencourages multiple trajectories to be diversely distributed, covering feasible\nmaneuvers in a map-aware manner. Furthermore, considering that the existing\ntrajectory performance metrics are focusing on evaluating the accuracy based on\nthe ground truth future trajectory, a quantitative evaluation metric is also\nsuggested to evaluate the diversity of predicted multiple trajectories. The\nexperiments performed on the Argoverse dataset show that the proposed method\nsignificantly improves the diversity of the predicted trajectories without\nsacrificing the prediction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sanmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyeongseok Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junwon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kum_D/0/1/0/all/0/1\">Dongsuk Kum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Slot Attention for Vision-and-Language Navigation. (arXiv:2206.08645v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08645","description":"<p>Vision-and-language navigation (VLN), a frontier study aiming to pave the way\nfor general-purpose robots, has been a hot topic in the computer vision and\nnatural language processing community. The VLN task requires an agent to\nnavigate to a goal location following natural language instructions in\nunfamiliar environments.\n</p>\n<p>Recently, transformer-based models have gained significant improvements on\nthe VLN task. Since the attention mechanism in the transformer architecture can\nbetter integrate inter- and intra-modal information of vision and language.\n</p>\n<p>However, there exist two problems in current transformer-based models.\n</p>\n<p>1) The models process each view independently without taking the integrity of\nthe objects into account.\n</p>\n<p>2) During the self-attention operation in the visual modality, the views that\nare spatially distant can be inter-weaved with each other without explicit\nrestriction. This kind of mixing may introduce extra noise instead of useful\ninformation.\n</p>\n<p>To address these issues, we propose 1) A slot-attention based module to\nincorporate information from segmentation of the same object. 2) A local\nattention mask mechanism to limit the visual attention span. The proposed\nmodules can be easily plugged into any VLN architecture and we use the\nRecurrent VLN-Bert as our base model. Experiments on the R2R dataset show that\nour model has achieved the state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yifeng Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sue_X/0/1/0/all/0/1\">Xiangyang Sue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP). (arXiv:2206.08653v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08653","description":"<p>This paper considers the problem of Hierarchical Multi-Label Classification\n(HMC), where (i) several labels can be present for each example, and (ii)\nlabels are related via a domain-specific hierarchy tree. Guided by the\nintuition that all mistakes are not equal, we present Comprehensive Hierarchy\nAware Multi-label Predictions (CHAMP), a framework that penalizes a\nmisprediction depending on its severity as per the hierarchy tree. While there\nhave been works that apply such an idea to single-label classification, to the\nbest of our knowledge, there are limited such works for multilabel\nclassification focusing on the severity of mistakes. The key reason is that\nthere is no clear way of quantifying the severity of a misprediction a priori\nin the multilabel setting. In this work, we propose a simple but effective\nmetric to quantify the severity of a mistake in HMC, naturally leading to\nCHAMP. Extensive experiments on six public HMC datasets across modalities\n(image, audio, and text) demonstrate that incorporating hierarchical\ninformation leads to substantial gains as CHAMP improves both AUPRC (2.6%\nmedian percentage improvement) and hierarchical metrics (2.85% median\npercentage improvement), over stand-alone hierarchical or multilabel\nclassification methods. Compared to standard multilabel baselines, CHAMP\nprovides improved AUPRC in both robustness (8.87% mean percentage improvement )\nand less data regimes. Further, our method provides a framework to enhance\nexisting multilabel classification algorithms with better mistakes (18.1% mean\npercentage increment).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashwin Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gaurav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1\">Praneeth Netrapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_N/0/1/0/all/0/1\">Narayan G Hegde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Implicit Feature Alignment Function for Semantic Segmentation. (arXiv:2206.08655v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08655","description":"<p>Integrating high-level context information with low-level details is of\ncentral importance in semantic segmentation. Towards this end, most existing\nsegmentation models apply bilinear up-sampling and convolutions to feature maps\nof different scales, and then align them at the same resolution. However,\nbilinear up-sampling blurs the precise information learned in these feature\nmaps and convolutions incur extra computation costs. To address these issues,\nwe propose the Implicit Feature Alignment function (IFA). Our method is\ninspired by the rapidly expanding topic of implicit neural representations,\nwhere coordinate-based neural networks are used to designate fields of signals.\nIn IFA, feature vectors are viewed as representing a 2D field of information.\nGiven a query coordinate, nearby feature vectors with their relative\ncoordinates are taken from the multi-level feature maps and then fed into an\nMLP to generate the corresponding output. As such, IFA implicitly aligns the\nfeature maps at different levels and is capable of producing segmentation maps\nin arbitrary resolutions. We demonstrate the efficacy of IFA on multiple\ndatasets, including Cityscapes, PASCAL Context, and ADE20K. Our method can be\ncombined with improvement on various architectures, and it achieves\nstate-of-the-art computation-accuracy trade-off on common benchmarks. Code will\nbe made available at https://github.com/hzhupku/IFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanzhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borse_S/0/1/0/all/0/1\">Shubhankar Borse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08657","description":"<p>Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a cross-modal encoder, or feed the last-layer\nuni-modal features directly into the top cross-modal encoder, ignoring the\nsemantic information at the different levels in the deep uni-modal encoders.\nBoth approaches possibly restrict vision-language representation learning and\nlimit model performance. In this paper, we introduce multiple bridge layers\nthat build a connection between the top layers of uni-modal encoders and each\nlayer of the cross-modal encoder. This enables comprehensive bottom-up\ninteractions between visual and textual representations at different semantic\nlevels, resulting in more effective cross-modal alignment and fusion. Our\nproposed Bridge-Tower, pre-trained with only $4$M images, achieves\nstate-of-the-art performance on various downstream vision-language tasks. On\nthe VQAv2 test-std set, Bridge-Tower achieves an accuracy of $78.73\\%$,\noutperforming the previous state-of-the-art METER model by $1.09\\%$ with the\nsame pre-training data and almost no additional parameters and computational\ncost. Notably, when further scaling the model, Bridge-Tower achieves an\naccuracy of $81.15\\%$, surpassing models that are pre-trained on\norders-of-magnitude larger datasets. Code is available at\nhttps://github.com/microsoft/BridgeTower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenman_S/0/1/0/all/0/1\">Shachar Rosenman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification. (arXiv:2206.08671v1 [stat.ML])","link":"http://arxiv.org/abs/2206.08671","description":"<p>Modern deep learning systems are increasingly deployed in situations such as\npersonalization and federated learning where it is necessary to support i)\nlearning on small amounts of data, and ii) communication efficient distributed\ntraining protocols. In this work we develop FiLM Transfer (FiT) which fulfills\nthese requirements in the image classification setting. FiT uses an\nautomatically configured Naive Bayes classifier on top of a fixed backbone that\nhas been pretrained on large image datasets. Parameter efficient FiLM layers\nare used to modulate the backbone, shaping the representation for the\ndownstream task. The network is trained via an episodic fine-tuning protocol.\nThe approach is parameter efficient which is key for enabling few-shot\nlearning, inexpensive model updates for personalization, and communication\nefficient federated learning. We experiment with FiT on a wide range of\ndownstream datasets and show that it achieves better classification accuracy\nthan the state-of-the-art Big Transfer (BiT) algorithm at low-shot and on the\nchallenging VTAB-1k benchmark, with fewer than 1% of the updateable parameters.\nFinally, we demonstrate the parameter efficiency of FiT in distributed low-shot\napplications including model personalization and federated learning where model\nupdate size is an important performance metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Shysheya_A/0/1/0/all/0/1\">Aliaksandra Shysheya</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bronskill_J/0/1/0/all/0/1\">John Bronskill</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Patacchiola_M/0/1/0/all/0/1\">Massimiliano Patacchiola</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nowozin_S/0/1/0/all/0/1\">Sebastian Nowozin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1\">Richard E Turner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AggNet: Learning to Aggregate Faces for Group Membership Verification. (arXiv:2206.08683v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08683","description":"<p>In some face recognition applications, we are interested to verify whether an\nindividual is a member of a group, without revealing their identity. Some\nexisting methods, propose a mechanism for quantizing precomputed face\ndescriptors into discrete embeddings and aggregating them into one group\nrepresentation. However, this mechanism is only optimized for a given closed\nset of individuals and needs to learn the group representations from scratch\nevery time the groups are changed. In this paper, we propose a deep\narchitecture that jointly learns face descriptors and the aggregation mechanism\nfor better end-to-end performances. The system can be applied to new groups\nwith individuals never seen before and the scheme easily manages new\nmemberships or membership endings. We show through experiments on multiple\nlarge-scale wild-face datasets, that the proposed method leads to higher\nverification performance compared to other baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gheisari_M/0/1/0/all/0/1\">Marzieh Gheisari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirian_J/0/1/0/all/0/1\">Javad Amirian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1\">Teddy Furon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1\">Laurent Amsaleg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Double Descent: Where Network Pruning Aggravates Overfitting. (arXiv:2206.08684v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08684","description":"<p>People usually believe that network pruning not only reduces the\ncomputational cost of deep networks, but also prevents overfitting by\ndecreasing model capacity. However, our work surprisingly discovers that\nnetwork pruning sometimes even aggravates overfitting. We report an unexpected\nsparse double descent phenomenon that, as we increase model sparsity via\nnetwork pruning, test performance first gets worse (due to overfitting), then\ngets better (due to relieved overfitting), and gets worse at last (due to\nforgetting useful information). While recent studies focused on the deep double\ndescent with respect to model overparameterization, they failed to recognize\nthat sparsity may also cause double descent. In this paper, we have three main\ncontributions. First, we report the novel sparse double descent phenomenon\nthrough extensive experiments. Second, for this phenomenon, we propose a novel\nlearning distance interpretation that the curve of $\\ell_{2}$ learning distance\nof sparse models (from initialized parameters to final parameters) may\ncorrelate with the sparse double descent curve well and reflect generalization\nbetter than minima flatness. Third, in the context of sparse double descent, a\nwinning ticket in the lottery ticket hypothesis surprisingly may not always\nwin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zeke Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Quanzhi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zengchang Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Real-Time Visual Tracking with Graded Color-names Features. (arXiv:2206.08701v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08701","description":"<p>MeanShift algorithm has been widely used in tracking tasks because of its\nsimplicity and efficiency. However, the traditional MeanShift algorithm needs\nto label the initial region of the target, which reduces the applicability of\nthe algorithm. Furthermore, it is only applicable to the scene with a large\noverlap rate between the target area and the candidate area. Therefore, when\nthe target speed is fast, the target scale change, shape deformation or the\ntarget occlusion occurs, the tracking performance will be deteriorated. In this\npaper, we address the challenges above-mentioned by developing a tracking\nmethod that combines the background models and the graded features of\ncolor-names under the MeanShift framework. This method significantly improve\nperformance in the above scenarios. In addition, it facilitates the balance\nbetween detection accuracy and detection speed. Experimental results\ndemonstrate the validation of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xuemei Guo</a>,"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximum Class Separation as Inductive Bias in One Matrix. (arXiv:2206.08704v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08704","description":"<p>Maximizing the separation between classes constitutes a well-known inductive\nbias in machine learning and a pillar of many traditional algorithms. By\ndefault, deep networks are not equipped with this inductive bias and therefore\nmany alternative solutions have been proposed through differential\noptimization. Current approaches tend to optimize classification and separation\njointly: aligning inputs with class vectors and separating class vectors\nangularly. This paper proposes a simple alternative: encoding maximum\nseparation as an inductive bias in the network by adding one fixed matrix\nmultiplication before computing the softmax activations. The main observation\nbehind our approach is that separation does not require optimization but can be\nsolved in closed-form prior to training and plugged into a network. We outline\na recursive approach to obtain the matrix consisting of maximally separable\nvectors for any number of classes, which can be added with negligible\nengineering effort and computational overhead. Despite its simple nature, this\none matrix multiplication provides real impact. We show that our proposal\ndirectly boosts classification, long-tailed recognition, out-of-distribution\ndetection, and open-set recognition, from CIFAR to ImageNet. We find\nempirically that maximum separation works best as a fixed bias; making the\nmatrix learnable adds nothing to the performance. The closed-form\nimplementation and code to reproduce the experiments are on github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasarla_T/0/1/0/all/0/1\">Tejaswi Kasarla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1\">Gertjan J. Burghouts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spengler_M/0/1/0/all/0/1\">Max van Spengler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pol_E/0/1/0/all/0/1\">Elise van der Pol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Algorithm for the SE(3)-Transformation on Neural Implicit Maps for Remapping Functions. (arXiv:2206.08712v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08712","description":"<p>Implicit representations are widely used for object reconstruction due to\ntheir efficiency and flexibility. In 2021, a novel structure named neural\nimplicit map has been invented for incremental reconstruction. A neural\nimplicit map alleviates the problem of inefficient memory cost of previous\nonline 3D dense reconstruction while producing better quality. % However, the\nneural implicit map suffers the limitation that it does not support remapping\nas the frames of scans are encoded into a deep prior after generating the\nneural implicit map. This means, that neither this generation process is\ninvertible, nor a deep prior is transformable. The non-remappable property\nmakes it not possible to apply loop-closure techniques. % We present a neural\nimplicit map based transformation algorithm to fill this gap. As our neural\nimplicit map is transformable, our model supports remapping for this special\nmap of latent features. % Experiments show that our remapping module is capable\nto well-transform neural implicit maps to new poses. Embedded into a SLAM\nframework, our mapping model is able to tackle the remapping of loop closures\nand demonstrates high-quality surface reconstruction. % Our implementation is\navailable at github\\footnote{\\url{https://github.com/Jarrome/IMT_Mapping}} for\nthe research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yijun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuechter_A/0/1/0/all/0/1\">Andreas Nuechter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReViSe: Remote Vital Signs Measurement Using Smartphone Camera. (arXiv:2206.08748v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08748","description":"<p>Remote Photoplethysmography (rPPG) is a fast, effective, inexpensive and\nconvenient method for collecting biometric data as it enables vital signs\nestimation using face videos. Remote contactless medical service provisioning\nhas proven to be a dire necessity during the COVID-19 pandemic. We propose an\nend-to-end framework to measure people's vital signs including Heart Rate (HR),\nHeart Rate Variability (HRV), Oxygen Saturation (SpO2) and Blood Pressure (BP)\nbased on the rPPG methodology from the video of a user's face captured with a\nsmartphone camera. We extract face landmarks with a deep learning-based neural\nnetwork model in real-time. Multiple face patches also called\nRegion-of-Interests (RoIs) are extracted by using the predicted face landmarks.\nSeveral filters are applied to reduce the noise from the RoIs in the extracted\ncardiac signals called Blood Volume Pulse (BVP) signal. We trained and\nvalidated machine learning models using two public rPPG datasets namely the\nTokyoTech rPPG and the Pulse Rate Detection (PURE) datasets, on which our\nmodels achieved the following Mean Absolute Errors (MAE): a) for HR, 1.73 and\n3.95 Beats-Per-Minute (bpm) respectively, b) for HRV, 18.55 and 25.03 ms\nrespectively, and c) for SpO2, a MAE of 1.64 on the PURE dataset. We validated\nour end-to-end rPPG framework, ReViSe, in real life environment, and thereby\ncreated the Video-HR dataset. Our HR estimation model achieved a MAE of 2.49\nbpm on this dataset. Since no publicly available rPPG datasets existed for BP\nmeasurement with face videos, we used a dataset with signals from fingertip\nsensor to train our model and also created our own video dataset, Video-BP. On\nour Video-BP dataset, our BP estimation model achieved a MAE of 6.7 mmHg for\nSystolic Blood Pressure (SBP), and a MAE of 9.6 mmHg for Diastolic Blood\nPressure (DBP).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_D/0/1/0/all/0/1\">Donghao Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayesha_A/0/1/0/all/0/1\">Amtul Haq Ayesha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zulkernine_F/0/1/0/all/0/1\">Farhana Zulkernine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masroor_R/0/1/0/all/0/1\">Raihan Masroor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaffar_N/0/1/0/all/0/1\">Nauman Jaffar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From a few Accurate 2D Correspondences to 3D Point Clouds. (arXiv:2206.08749v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08749","description":"<p>Key points, correspondences, projection matrices, point clouds and dense\nclouds are the skeletons in image-based 3D reconstruction, of which point\nclouds have the important role in generating a realistic and natural model for\na 3D reconstructed object. To achieve a good 3D reconstruction, the point\nclouds must be almost everywhere in the surface of the object. In this article,\nwith a main purpose to build the point clouds covering the entire surface of\nthe object, we propose a new feature named a geodesic feature or geo-feature.\nBased on the new geo-feature, if there are several (given) initial world points\non the object's surface along with all accurately estimated projection\nmatrices, some new world points on the geodesics connecting any two of these\ngiven world points will be reconstructed. Then the regions on the surface\nbordering by these initial world points will be covered by the point clouds.\nThus, if the initial world points are around the surface, the point clouds will\ncover the entire surface.\n</p>\n<p>This article proposes a new method to estimate the world points and\nprojection matrices from their correspondences. This method derives the\nclosed-form and iterative solutions for the world points and projection\nmatrices and proves that when the number of world points is less than seven and\nthe number of images is at least five, the proposed solutions are global\noptimal. We propose an algorithm named World points from their Correspondences\n(WPfC) to estimate the world points and projection matrices from their\ncorrespondences, and another algorithm named Creating Point Clouds (CrPC) to\ncreate the point clouds from the world points and projection matrices given by\nthe first algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Kien Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Database for Perceived Quality Assessment of User-Generated VR Videos. (arXiv:2206.08751v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08751","description":"<p>Virtual reality (VR) videos (typically in the form of 360$^\\circ$ videos)\nhave gained increasing attention due to the fast development of VR technologies\nand the remarkable popularization of consumer-grade 360$^\\circ$ cameras and\ndisplays. Thus it is pivotal to understand how people perceive user-generated\nVR videos, which may suffer from commingled authentic distortions, often\nlocalized in space and time. In this paper, we establish one of the largest\n360$^\\circ$ video databases, containing 502 user-generated videos with rich\ncontent and distortion diversities. We capture viewing behaviors (i.e.,\nscanpaths) of 139 users, and collect their opinion scores of perceived quality\nunder four different viewing conditions (two starting points $\\times$ two\nexploration times). We provide a thorough statistical analysis of recorded\ndata, resulting in several interesting observations, such as the significant\nimpact of viewing conditions on viewing behaviors and perceived quality.\nBesides, we explore other usage of our data and analysis, including evaluation\nof computational models for quality assessment and saliency detection of\n360$^\\circ$ videos. We have made the dataset and code available at\nhttps://github.com/Yao-Yiru/VR-Video-Database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yiru Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1\">Xiangjie Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume Segmentation on Cone Beam Computed Tomography Images. (arXiv:2206.08778v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08778","description":"<p>3D tooth segmentation is a prerequisite for computer-aided dental diagnosis\nand treatment. However, segmenting all tooth regions manually is subjective and\ntime-consuming. Recently, deep learning-based segmentation methods produce\nconvincing results and reduce manual annotation efforts, but it requires a\nlarge quantity of ground truth for training. To our knowledge, there are few\ntooth data available for the 3D segmentation study. In this paper, we establish\na fully annotated cone beam computed tomography dataset CTooth with tooth gold\nstandard. This dataset contains 22 volumes (7363 slices) with fine tooth labels\nannotated by experienced radiographic interpreters. To ensure a relative even\ndata sampling distribution, data variance is included in the CTooth including\nmissing teeth and dental restoration. Several state-of-the-art segmentation\nmethods are evaluated on this dataset. Afterwards, we further summarise and\napply a series of 3D attention-based Unet variants for segmenting tooth\nvolumes. This work provides a new benchmark for the tooth volume segmentation\ntask. Experimental evidence proves that attention modules of the 3D UNet\nstructure boost responses in tooth areas and inhibit the influence of\nbackground and noise. The best performance is achieved by 3D Unet with SKNet\nattention module, of 88.04 \\% Dice and 78.71 \\% IOU, respectively. The\nattention-based Unet framework outperforms other state-of-the-art methods on\nthe CTooth dataset. The codebase and dataset are released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Weiwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xingyong Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Gangyong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1\">Liaoyuan Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma Grading. (arXiv:2206.08787v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08787","description":"<p>Pancreatic cancers have one of the worst prognoses compared to other cancers,\nas they are diagnosed when cancer has progressed towards its latter stages. The\ncurrent manual histological grading for diagnosing pancreatic adenocarcinomas\nis time-consuming and often results in misdiagnosis. In digital pathology,\nAI-based cancer grading must be extremely accurate in prediction and\nuncertainty quantification to improve reliability and explainability and are\nessential for gaining clinicians trust in the technology. We present Bayesian\nConvolutional Neural Networks for automated pancreatic cancer grading from MGG\nand HE stained images to estimate uncertainty in model prediction. We show that\nthe estimated uncertainty correlates with prediction error. Specifically, it is\nuseful in setting the acceptance threshold using a metric that weighs\nclassification accuracy-reject trade-off and misclassification cost controlled\nby hyperparameters and can be employed in clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghoshal_B/0/1/0/all/0/1\">Biraja Ghoshal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghoshal_B/0/1/0/all/0/1\">Bhargab Ghoshal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tucker_A/0/1/0/all/0/1\">Allan Tucker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing vehicles from orthographic drawings using deep neural networks. (arXiv:2206.08789v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08789","description":"<p>This paper explores the current state-of-the-art of object reconstruction\nfrom multiple orthographic drawings using deep neural networks. It proposes two\nalgorithms to extract multiple views from a single image. The paper proposes a\nsystem based on pixel-aligned implicit functions (PIFu) and develops an\nadvanced sampling strategy to generate signed distance samples. It also\ncompares this approach to depth map regression from multiple views.\nAdditionally, the paper uses a novel dataset for vehicle reconstruction from\nthe racing game Assetto Corsa, which features higher quality models than the\ncommonly used ShapeNET dataset. The trained neural network generalizes well to\nreal-world inputs and creates plausible and detailed reconstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klippert_R/0/1/0/all/0/1\">Robin Klippert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DU-Net based Unsupervised Contrastive Learning for Cancer Segmentation in Histology Images. (arXiv:2206.08791v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08791","description":"<p>In this paper, we introduce an unsupervised cancer segmentation framework for\nhistology images. The framework involves an effective contrastive learning\nscheme for extracting distinctive visual representations for segmentation. The\nencoder is a Deep U-Net (DU-Net) structure that contains an extra fully\nconvolution layer compared to the normal U-Net. A contrastive learning scheme\nis developed to solve the problem of lacking training sets with high-quality\nannotations on tumour boundaries. A specific set of data augmentation\ntechniques are employed to improve the discriminability of the learned colour\nfeatures from contrastive learning. Smoothing and noise elimination are\nconducted using convolutional Conditional Random Fields. The experiments\ndemonstrate competitive performance in segmentation even better than some\npopular supervised networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaqiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Gangyong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FD-CAM: Improving Faithfulness and Discriminability of Visual Explanation for CNNs. (arXiv:2206.08792v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08792","description":"<p>Class activation map (CAM) has been widely studied for visual explanation of\nthe internal working mechanism of convolutional neural networks. The key of\nexisting CAM-based methods is to compute effective weights to combine\nactivation maps in the target convolution layer. Existing gradient and score\nbased weighting schemes have shown superiority in ensuring either the\ndiscriminability or faithfulness of the CAM, but they normally cannot excel in\nboth properties. In this paper, we propose a novel CAM weighting scheme, named\nFD-CAM, to improve both the faithfulness and discriminability of the CAM-based\nCNN visual explanation. First, we improve the faithfulness and discriminability\nof the score-based weights by performing a grouped channel switching operation.\nSpecifically, for each channel, we compute its similarity group and switch the\ngroup of channels on or off simultaneously to compute changes in the class\nprediction score as the weights. Then, we combine the improved score-based\nweights with the conventional gradient-based weights so that the\ndiscriminability of the final CAM can be further improved. We perform extensive\ncomparisons with the state-of-the-art CAM algorithms. The quantitative and\nqualitative results show our FD-CAM can produce more faithful and more\ndiscriminative visual explanations of the CNNs. We also conduct experiments to\nverify the effectiveness of the proposed grouped channel switching and weight\ncombination scheme on improving the results. Our code is available at\nhttps://github.com/crishhh1998/FD-CAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tieru Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Importance of Background Information for Out of Distribution Generalization. (arXiv:2206.08794v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08794","description":"<p>Domain generalization in medical image classification is an important problem\nfor trustworthy machine learning to be deployed in healthcare. We find that\nexisting approaches for domain generalization which utilize ground-truth\nabnormality segmentations to control feature attributions have poor\nout-of-distribution (OOD) performance relative to the standard baseline of\nempirical risk minimization (ERM). We investigate what regions of an image are\nimportant for medical image classification and show that parts of the\nbackground, that which is not contained in the abnormality segmentation,\nprovides helpful signal. We then develop a new task-specific mask which covers\nall relevant regions. Utilizing this new segmentation mask significantly\nimproves the performance of the existing methods on the OOD test sets. To\nobtain better generalization results than ERM, we find it necessary to scale up\nthe training data size in addition to the usage of these task-specific masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_J/0/1/0/all/0/1\">Jupinder Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1\">Khaled Saab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pogatchnik_B/0/1/0/all/0/1\">Brian Pogatchnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Shadow Detection via Spatio-Temporal Interpolation Consistency Training. (arXiv:2206.08801v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08801","description":"<p>It is challenging to annotate large-scale datasets for supervised video\nshadow detection methods. Using a model trained on labeled images to the video\nframes directly may lead to high generalization error and temporal inconsistent\nresults. In this paper, we address these challenges by proposing a\nSpatio-Temporal Interpolation Consistency Training (STICT) framework to\nrationally feed the unlabeled video frames together with the labeled images\ninto an image shadow detection network training. Specifically, we propose the\nSpatial and Temporal ICT, in which we define two new interpolation schemes,\n\\textit{i.e.}, the spatial interpolation and the temporal interpolation. We\nthen derive the spatial and temporal interpolation consistency constraints\naccordingly for enhancing generalization in the pixel-wise classification task\nand for encouraging temporal consistent predictions, respectively. In addition,\nwe design a Scale-Aware Network for multi-scale shadow knowledge learning in\nimages, and propose a scale-consistency constraint to minimize the discrepancy\namong the predictions at different scales. Our proposed approach is extensively\nvalidated on the ViSha dataset and a self-annotated dataset. Experimental\nresults show that, even without video labels, our approach is better than most\nstate of the art supervised, semi-supervised or unsupervised image/video shadow\ndetection methods and other methods in related tasks. Code and dataset are\navailable at \\url{https://github.com/yihong-97/STICT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zipei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuanyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yimin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets. (arXiv:2206.08802v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08802","description":"<p>Deep neural networks usually perform poorly when the training dataset suffers\nfrom extreme class imbalance. Recent studies found that directly training with\nout-of-distribution data (i.e., open-set samples) in a semi-supervised manner\nwould harm the generalization performance. In this work, we theoretically show\nthat out-of-distribution data can still be leveraged to augment the minority\nclasses from a Bayesian perspective. Based on this motivation, we propose a\nnovel method called Open-sampling, which utilizes open-set noisy labels to\nre-balance the class priors of the training dataset. For each open-set\ninstance, the label is sampled from our pre-defined distribution that is\ncomplementary to the distribution of original class priors. We empirically show\nthat Open-sampling not only re-balances the class priors but also encourages\nthe neural network to learn separable representations. Extensive experiments\ndemonstrate that our proposed method significantly outperforms existing data\nre-balancing methods and can boost the performance of existing state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongxin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Lue Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Renchunzi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bo An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis. (arXiv:2206.08826v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08826","description":"<p>Alzheimer's Disease (AD) is the most common neurodegenerative disorder with\none of the most complex pathogeneses, making effective and clinically\nactionable decision support difficult. The objective of this study was to\ndevelop a novel multimodal deep learning framework to aid medical professionals\nin AD diagnosis. We present a Multimodal Alzheimer's Disease Diagnosis\nframework (MADDi) to accurately detect the presence of AD and mild cognitive\nimpairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in\nthat we use cross-modal attention, which captures interactions between\nmodalities - a method not previously explored in this domain. We perform\nmulti-class classification, a challenging task considering the strong\nsimilarities between MCI and AD. We compare with previous state-of-the-art\nmodels, evaluate the importance of attention, and examine the contribution of\neach modality to the model's performance. MADDi classifies MCI, AD, and\ncontrols with 96.88% accuracy on a held-out test set. When examining the\ncontribution of different attention schemes, we found that the combination of\ncross-modal attention with self-attention performed the best, and no attention\nlayers in the model performed the worst, with a 7.9% difference in F1-Scores.\nOur experiments underlined the importance of structured clinical data to help\nmachine learning models contextualize and interpret the remaining modalities.\nExtensive ablation studies showed that any multimodal mixture of input features\nwithout access to structured clinical information suffered marked performance\nlosses. This study demonstrates the merit of combining multiple input\nmodalities via cross-modal attention to deliver highly accurate AD diagnostic\ndecision support.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golovanevsky_M/0/1/0/all/0/1\">Michal Golovanevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Ritambhara Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Confidence Calibration in Deep Learning: From Computer Vision to Medical Imaging. (arXiv:2206.08833v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08833","description":"<p>Although deep learning prediction models have been successful in the\ndiscrimination of different classes, they can often suffer from poor\ncalibration across challenging domains including healthcare. Moreover, the\nlong-tail distribution poses great challenges in deep learning classification\nproblems including clinical disease prediction. There are approaches proposed\nrecently to calibrate deep prediction in computer vision, but there are no\nstudies found to demonstrate how the representative models work in different\nchallenging contexts. In this paper, we bridge the confidence calibration from\ncomputer vision to medical imaging with a comparative study of four high-impact\ncalibration models. Our studies are conducted in different contexts (natural\nimage classification and lung cancer risk estimation) including in balanced vs.\nimbalanced training sets and in computer vision vs. medical imaging. Our\nresults support key findings: (1) We achieve new conclusions which are not\nstudied under different learning contexts, e.g., combining two calibration\nmodels that both mitigate the overconfident prediction can lead to\nunder-confident prediction, and simpler calibration models from the computer\nvision domain tend to be more generalizable to medical imaging. (2) We\nhighlight the gap between general computer vision tasks and medical imaging\nprediction, e.g., calibration methods ideal for general computer vision tasks\nmay in fact damage the calibration of medical imaging prediction. (3) We also\nreinforce previous conclusions in natural image classification settings. We\nbelieve that this study has merits to guide readers to choose calibration\nmodels and understand gaps between general computer vision and medical imaging\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Thomas Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kammer_M/0/1/0/all/0/1\">Michael Kammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antic_S/0/1/0/all/0/1\">Sanja L. Antic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_K/0/1/0/all/0/1\">Kim Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moldonado_F/0/1/0/all/0/1\">Fabien Moldonado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasko_T/0/1/0/all/0/1\">Thomas A. Lasko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett Landman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Graph Enhanced Cross-Modal Pretraining for Instance-level Product Retrieval. (arXiv:2206.08842v1 [cs.MM])","link":"http://arxiv.org/abs/2206.08842","description":"<p>Our goal in this research is to study a more realistic environment in which\nwe can conduct weakly-supervised multi-modal instance-level product retrieval\nfor fine-grained product categories. We first contribute the Product1M\ndatasets, and define two real practical instance-level retrieval tasks to\nenable the evaluations on the price comparison and personalized\nrecommendations. For both instance-level tasks, how to accurately pinpoint the\nproduct target mentioned in the visual-linguistic data and effectively decrease\nthe influence of irrelevant contents is quite challenging. To address this, we\nexploit to train a more effective cross-modal pertaining model which is\nadaptively capable of incorporating key concept information from the\nmulti-modal data, by using an entity graph whose node and edge respectively\ndenote the entity and the similarity relation between entities. Specifically, a\nnovel Entity-Graph Enhanced Cross-Modal Pretraining (EGE-CMP) model is proposed\nfor instance-level commodity retrieval, that explicitly injects entity\nknowledge in both node-based and subgraph-based ways into the multi-modal\nnetworks via a self-supervised hybrid-stream transformer, which could reduce\nthe confusion between different object contents, thereby effectively guiding\nthe network to focus on entities with real semantic. Experimental results well\nverify the efficacy and generalizability of our EGE-CMP, outperforming several\nSOTA cross-modal baselines like CLIP, UNITER and CAPTURE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08853","description":"<p>Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite and knowledge bases\n(https://minedojo.org) to promote research towards the goal of generally\ncapable embodied agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunfan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuncong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haoyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1\">Andrew Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2206.08861v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08861","description":"<p>Multiple Instance Learning (MIL) is widely used in analyzing\nhistopathological Whole Slide Images (WSIs). However, existing MIL methods do\nnot explicitly model the data distribution, and instead they only learn a\nbag-level or instance-level decision boundary discriminatively by training a\nclassifier. In this paper, we propose DGMIL: a feature distribution guided deep\nMIL framework for WSI classification and positive patch localization. Instead\nof designing complex discriminative network architectures, we reveal that the\ninherent feature distribution of histopathological image data can serve as a\nvery effective guide for instance classification. We propose a\ncluster-conditioned feature distribution modeling method and a pseudo\nlabel-based iterative feature space refinement strategy so that in the final\nfeature space the positive and negative instances can be easily separated.\nExperiments on the CAMELYON16 dataset and the TCGA Lung Cancer dataset show\nthat our method achieves new SOTA for both global classification and positive\npatch localization tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Linhao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaolei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhijian Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Lossless Neural Compression with Integer-Only Discrete Flows. (arXiv:2206.08869v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08869","description":"<p>By applying entropy codecs with learned data distributions, neural\ncompressors have significantly outperformed traditional codecs in terms of\ncompression ratio. However, the high inference latency of neural networks\nhinders the deployment of neural compressors in practical applications. In this\nwork, we propose Integer-only Discrete Flows (IODF), an efficient neural\ncompressor with integer-only arithmetic. Our work is built upon integer\ndiscrete flows, which consists of invertible transformations between discrete\nrandom variables. We propose efficient invertible transformations with\ninteger-only arithmetic based on 8-bit quantization. Our invertible\ntransformation is equipped with learnable binary gates to remove redundant\nfilters during inference. We deploy IODF with TensorRT on GPUs, achieving 10x\ninference speedup compared to the fastest existing neural compressors, while\nretaining the high compression rates on ImageNet32 and ImageNet64.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization of Metric Learning via Listwise Self-distillation. (arXiv:2206.08880v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08880","description":"<p>Most deep metric learning (DML) methods employ a strategy that forces all\npositive samples to be close in the embedding space while keeping them away\nfrom negative ones. However, such a strategy ignores the internal relationships\nof positive (negative) samples and often leads to overfitting, especially in\nthe presence of hard samples and mislabeled samples. In this work, we propose a\nsimple yet effective regularization, namely Listwise Self-Distillation (LSD),\nwhich progressively distills a model's own knowledge to adaptively assign a\nmore appropriate distance target to each sample pair in a batch. LSD encourages\nsmoother embeddings and information mining within positive (negative) samples\nas a way to mitigate overfitting and thus improve generalization. Our LSD can\nbe directly integrated into general DML frameworks. Extensive experiments show\nthat LSD consistently boosts the performance of various metric learning methods\non multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zelong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_S/0/1/0/all/0/1\">Shin&#x27;ichi Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-Aided Sensor Data Sharing in Vehicular Communication Networks. (arXiv:2206.08882v1 [cs.MA])","link":"http://arxiv.org/abs/2206.08882","description":"<p>Sensor data sharing in vehicular networks can significantly improve the range\nand accuracy of environmental perception for connected automated vehicles.\nDifferent concepts and schemes for dissemination and fusion of sensor data have\nbeen developed. It is common to these schemes that measurement errors of the\nsensors impair the perception quality and can result in road traffic accidents.\nSpecifically, when the measurement error from the sensors (also referred as\nmeasurement noise) is unknown and time varying, the performance of the data\nfusion process is restricted, which represents a major challenge in the\ncalibration of sensors. In this paper, we consider sensor data sharing and\nfusion in a vehicular network with both, vehicle-to-infrastructure and\nvehicle-to-vehicle communication. We propose a method, named Bidirectional\nFeedback Noise Estimation (BiFNoE), in which an edge server collects and caches\nsensor measurement data from vehicles. The edge estimates the noise and the\ntargets alternately in double dynamic sliding time windows and enhances the\ndistributed cooperative environment sensing at each vehicle with low\ncommunication costs. We evaluate the proposed algorithm and data dissemination\nstrategy in an application scenario by simulation and show that the perception\naccuracy is on average improved by around 80 % with only 12 kbps uplink and 28\nkbps downlink bandwidth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_A/0/1/0/all/0/1\">Anupama Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senel_N/0/1/0/all/0/1\">Numan Senel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Festag_A/0/1/0/all/0/1\">Andreas Festag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer. (arXiv:2206.08883v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08883","description":"<p>Transformer has achieved great successes in learning vision and language\nrepresentation, which is general across various downstream tasks. In visual\ncontrol, learning transferable state representation that can transfer between\ndifferent control tasks is important to reduce the training sample size.\nHowever, porting Transformer to sample-efficient visual control remains a\nchallenging and unsolved problem. To this end, we propose a novel Control\nTransformer (CtrlFormer), possessing many appealing benefits that prior arts do\nnot have. Firstly, CtrlFormer jointly learns self-attention mechanisms between\nvisual tokens and policy tokens among different control tasks, where multitask\nrepresentation can be learned and transferred without catastrophic forgetting.\nSecondly, we carefully design a contrastive reinforcement learning paradigm to\ntrain CtrlFormer, enabling it to achieve high sample efficiency, which is\nimportant in control problems. For example, in the DMControl benchmark, unlike\nrecent advanced methods that failed by producing a zero score in the \"Cartpole\"\ntask after transfer learning with 100k samples, CtrlFormer can achieve a\nstate-of-the-art score with only 100k samples while maintaining the performance\nof previous tasks. The code and models are released in our project homepage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yao Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shoufa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling. (arXiv:2206.08885v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08885","description":"<p>Supervised learning tasks such as cancer survival prediction from gigapixel\nwhole slide images (WSIs) are a critical challenge in computational pathology\nthat requires modeling complex features of the tumor microenvironment. These\nlearning tasks are often solved with deep multi-instance learning (MIL) models\nthat do not explicitly capture intratumoral heterogeneity. We develop a novel\nvariance pooling architecture that enables a MIL model to incorporate\nintratumoral heterogeneity into its predictions. Two interpretability tools\nbased on representative patches are illustrated to probe the biological signals\ncaptured by these models. An empirical study with 4,479 gigapixel WSIs from the\nCancer Genome Atlas shows that adding variance pooling onto MIL frameworks\nimproves survival prediction performance for five cancer types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Carmichael_I/0/1/0/all/0/1\">Iain Carmichael</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1\">Andrew H. Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F.K. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representational Multiplicity Should Be Exposed, Not Eliminated. (arXiv:2206.08890v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08890","description":"<p>It is prevalent and well-observed, but poorly understood, that two machine\nlearning models with similar performance during training can have very\ndifferent real-world performance characteristics. This implies elusive\ndifferences in the internals of the models, manifesting as representational\nmultiplicity (RM). We introduce a conceptual and experimental setup for\nanalyzing RM and show that certain training methods systematically result in\ngreater RM than others, measured by activation similarity via singular vector\ncanonical correlation analysis (SVCCA). We further correlate it with predictive\nmultiplicity measured by the variance in i.i.d. and out-of-distribution test\nset predictions, in four common image data sets. We call for systematic\nmeasurement and maximal exposure, not elimination, of RM in models. Qualitative\ntools such as our confabulator analysis can facilitate understanding and\ncommunication of RM effects to stakeholders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heljakka_A/0/1/0/all/0/1\">Ari Heljakka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Martin Trapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimA: Simple Softmax-free Attention for Vision Transformers. (arXiv:2206.08898v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08898","description":"<p>Recently, vision transformers have become very popular. However, deploying\nthem in many applications is computationally expensive partly due to the\nSoftmax layer in the attention block. We introduce a simple but effective,\nSoftmax-free attention block, SimA, which normalizes query and key matrices\nwith simple $\\ell_1$-norm instead of using Softmax layer. Then, the attention\nblock in SimA is a simple multiplication of three matrices, so SimA can\ndynamically change the ordering of the computation at the test time to achieve\nlinear computation on the number of tokens or the number of channels. We\nempirically show that SimA applied to three SOTA variations of transformers,\nDeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models,\nwithout any need for Softmax layer. Interestingly, changing SimA from\nmulti-head to single-head has only a small effect on the accuracy, which\nsimplifies the attention block further. The code is available here:\n$\\href{https://github.com/UCDvision/sima}{\\text{This https URL}}$\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colonoscopy 3D Video Dataset with Paired Depth from 2D-3D Registration. (arXiv:2206.08903v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08903","description":"<p>Screening colonoscopy is an important clinical application for several 3D\ncomputer vision techniques, including depth estimation, surface reconstruction,\nand missing region detection. However, the development, evaluation, and\ncomparison of these techniques in real colonoscopy videos remain largely\nqualitative due to the difficulty of acquiring ground truth data. In this work,\nwe present a Colonoscopy 3D Video Dataset (C3VD) acquired with a high\ndefinition clinical colonoscope and high-fidelity colon models for benchmarking\ncomputer vision methods in colonoscopy. We introduce a novel multimodal 2D-3D\nregistration technique to register optical video sequences with ground truth\nrendered views of a known 3D model. The different modalities are registered by\ntransforming optical images to depth maps with a Generative Adversarial Network\nand aligning edge features with an evolutionary optimizer. This registration\nmethod achieves an average translation error of 0.321 millimeters and an\naverage rotation error of 0.159 degrees in simulation experiments where\nerror-free ground truth is available. The method also leverages video\ninformation, improving registration accuracy by 55.6% for translation and 60.4%\nfor rotation compared to single frame registration. 22 short video sequences\nwere registered to generate 10,015 total frames with paired ground truth depth,\nsurface normals, optical flow, occlusion, six degree-of-freedom pose, coverage\nmaps, and 3D models. The dataset also includes screening videos acquired by a\ngastroenterologist with paired ground truth pose and 3D surface models. The\ndataset and registration source code are available at durr.jhu.edu/C3VD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bobrow_T/0/1/0/all/0/1\">Taylor L. Bobrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golhar_M/0/1/0/all/0/1\">Mayank Golhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayan_R/0/1/0/all/0/1\">Rohan Vijayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akshintala_V/0/1/0/all/0/1\">Venkata S. Akshintala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1\">Juan R. Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durr_N/0/1/0/all/0/1\">Nicholas J. Durr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks. (arXiv:2206.08916v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08916","description":"<p>We propose Unified-IO, a model that performs a large variety of AI tasks\nspanning classical computer vision tasks, including pose estimation, object\ndetection, depth estimation and image generation, vision-and-language tasks\nsuch as region captioning and referring expression comprehension, to natural\nlanguage processing tasks such as question answering and paraphrasing.\nDeveloping a single unified model for such a large variety of tasks poses\nunique challenges due to the heterogeneous inputs and outputs pertaining to\neach task, including RGB images, per-pixel maps, binary masks, bounding boxes,\nand language. We achieve this unification by homogenizing every supported input\nand output into a sequence of discrete vocabulary tokens. This common\nrepresentation across all tasks allows us to train a single transformer-based\narchitecture, jointly on over 80 diverse datasets in the vision and language\nfields. Unified-IO is the first model capable of performing all 7 tasks on the\nGRIT benchmark and produces strong results across 16 diverse benchmarks like\nNYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail,\nwith no task or benchmark specific fine-tuning. Demos for Unified-IO are\navailable at https://unified-io.allenai.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix. (arXiv:2206.08919v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08919","description":"<p>Existing vision-language pre-training (VLP) methods primarily rely on paired\nimage-text datasets, which are either annotated by enormous human labors, or\ncrawled from the internet followed by elaborate data cleaning techniques. To\nreduce the dependency on well-aligned image-text pairs, it is promising to\ndirectly leverage the large-scale text-only and image-only corpora. This paper\nproposes a data augmentation method, namely cross-modal CutMix (CMC), for\nimplicit cross-modal alignment learning in unpaired VLP. Specifically, CMC\ntransforms natural sentences from the textual view into a multi-modal view,\nwhere visually-grounded words in a sentence are randomly replaced by diverse\nimage patches with similar semantics. There are several appealing proprieties\nof the proposed CMC. First, it enhances the data diversity while keeping the\nsemantic meaning intact for tackling problems where the aligned data are\nscarce; Second, by attaching cross-modal noise on uni-modal data, it guides\nmodels to learn token-level interactions across modalities for better\ndenoising. Furthermore, we present a new unpaired VLP method, dubbed as\nVLMixer, that integrates CMC with contrastive learning to pull together the\nuni-modal and multi-modal views for better instance-level alignments among\ndifferent modalities. Extensive experiments on five downstream tasks show that\nVLMixer could surpass previous state-of-the-art unpaired VLP methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chengguo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VectorMapNet: End-to-end Vectorized HD Map Learning. (arXiv:2206.08920v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08920","description":"<p>Autonomous driving systems require a good understanding of surrounding\nenvironments, including moving obstacles and static High-Definition (HD)\nsemantic maps. Existing methods approach the semantic map problem by offline\nmanual annotations, which suffer from serious scalability issues. More recent\nlearning-based methods produce dense rasterized segmentation predictions which\ndo not include instance information of individual map elements and require\nheuristic post-processing that involves many hand-designed components, to\nobtain vectorized maps. To that end, we introduce an end-to-end vectorized HD\nmap learning pipeline, termed VectorMapNet. VectorMapNet takes onboard sensor\nobservations and predicts a sparse set of polylines primitives in the\nbird's-eye view to model the geometry of HD maps. Based on this pipeline, our\nmethod can explicitly model the spatial relation between map elements and\ngenerate vectorized maps that are friendly for downstream autonomous driving\ntasks without the need for post-processing. In our experiments, VectorMapNet\nachieves strong HD map learning performance on nuScenes dataset, surpassing\nprevious state-of-the-art methods by 14.2 mAP. Qualitatively, we also show that\nVectorMapNet is capable of generating comprehensive maps and capturing more\nfine-grained details of road geometry. To the best of our knowledge,\nVectorMapNet is the first work designed toward end-to-end vectorized HD map\nlearning problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-task Attention Mechanism for Dense Multi-task Learning. (arXiv:2206.08927v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08927","description":"<p>Multi-task learning has recently become a promising solution for a\ncomprehensive understanding of complex scenes. Not only being memory-efficient,\nmulti-task models with an appropriate design can favor exchange of\ncomplementary signals across tasks. In this work, we jointly address 2D\nsemantic segmentation, and two geometry-related tasks, namely dense depth,\nsurface normal estimation as well as edge estimation showing their benefit on\nindoor and outdoor datasets. We propose a novel multi-task learning\narchitecture that exploits pair-wise cross-task exchange through\ncorrelation-guided attention and self-attention to enhance the average\nrepresentation learning for all tasks. We conduct extensive experiments\nconsidering three multi-task setups, showing the benefit of our proposal in\ncomparison to competitive baselines in both synthetic and real benchmarks. We\nalso extend our method to the novel multi-task unsupervised domain adaptation\nsetting. Our code is available at https://github.com/cv-rits/DenseMTL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopes_I/0/1/0/all/0/1\">Ivan Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tuan-Hung Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAVA: Template-free Animatable Volumetric Actors. (arXiv:2206.08929v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08929","description":"<p>Coordinate-based volumetric representations have the potential to generate\nphoto-realistic virtual avatars from images. However, virtual avatars also need\nto be controllable even to a novel pose that may not have been observed.\nTraditional techniques, such as LBS, provide such a function; yet it usually\nrequires a hand-designed body template, 3D scan data, and limited appearance\nmodels. On the other hand, neural representation has been shown to be powerful\nin representing visual details, but are under explored on deforming dynamic\narticulated actors. In this paper, we propose TAVA, a method to create T\nemplate-free Animatable Volumetric Actors, based on neural representations. We\nrely solely on multi-view data and a tracked skeleton to create a volumetric\nmodel of an actor, which can be animated at the test time given novel pose.\nSince TAVA does not require a body template, it is applicable to humans as well\nas other creatures such as animals. Furthermore, TAVA is designed such that it\ncan recover accurate dense correspondences, making it amenable to\ncontent-creation and editing tasks. Through extensive experiments, we\ndemonstrate that the proposed method generalizes well to novel poses as well as\nunseen views and showcase basic editing capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanke_J/0/1/0/all/0/1\">Julian Tanke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1\">Michael Zollhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Jurgen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Nested Dropout. (arXiv:2101.11353v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.11353","description":"<p>Nested dropout is a variant of dropout operation that is able to order\nnetwork parameters or features based on the pre-defined importance during\ntraining. It has been explored for: I. Constructing nested nets: the nested\nnets are neural networks whose architectures can be adjusted instantly during\ntesting time, e.g., based on computational constraints. The nested dropout\nimplicitly ranks the network parameters, generating a set of sub-networks such\nthat any smaller sub-network forms the basis of a larger one. II. Learning\nordered representation: the nested dropout applied to the latent representation\nof a generative model (e.g., auto-encoder) ranks the features, enforcing\nexplicit order of the dense representation over dimensions.\n</p>\n<p>However, the dropout rate is fixed as a hyper-parameter during the whole\ntraining process. For nested nets, when network parameters are removed, the\nperformance decays in a human-specified trajectory rather than in a trajectory\nlearned from data. For generative models, the importance of features is\nspecified as a constant vector, restraining the flexibility of representation\nlearning. To address the problem, we focus on the probabilistic counterpart of\nthe nested dropout. We propose a variational nested dropout (VND) operation\nthat draws samples of multi-dimensional ordered masks at a low cost, providing\nuseful gradients to the parameters of nested dropout. Based on this approach,\nwe design a Bayesian nested neural network that learns the order knowledge of\nthe parameter distributions. We further exploit the VND under different\ngenerative models for learning ordered latent distributions. In experiments, we\nshow that the proposed approach outperforms the nested network in terms of\naccuracy, calibration, and out-of-domain detection in classification tasks. It\nalso outperforms the related generative models on data generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yufei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1\">Tei-Wei Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chun Jason Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification. (arXiv:2103.04053v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04053","description":"<p>Real-world large-scale medical image analysis (MIA) datasets have three\nchallenges: 1) they contain noisy-labelled samples that affect training\nconvergence and generalisation, 2) they usually have an imbalanced distribution\nof samples per class, and 3) they normally comprise a multi-label problem,\nwhere samples can have multiple diagnoses. Current approaches are commonly\ntrained to solve a subset of those problems, but we are unaware of methods that\naddress the three problems simultaneously. In this paper, we propose a new\ntraining module called Non-Volatile Unbiased Memory (NVUM), which\nnon-volatility stores running average of model logits for a new regularization\nloss on noisy multi-label problem. We further unbias the classification\nprediction in NVUM update for imbalanced learning problem. We run extensive\nexperiments to evaluate NVUM on new benchmarks proposed by this paper, where\ntraining is performed on noisy multi-label imbalanced chest X-ray (CXR)\ntraining sets, formed by Chest-Xray14 and CheXpert, and the testing is\nperformed on the clean multi-label CXR datasets OpenI and PadChest. Our method\noutperforms previous state-of-the-art CXR classifiers and previous methods that\ncan deal with noisy labels on all evaluations. Our code is available at\nhttps://github.com/FBLADL/NVUM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional GANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10060","description":"<p>Conditional generative models aim to learn the underlying joint distribution\nof data and labels to achieve conditional data generation. Among them, the\nauxiliary classifier generative adversarial network (AC-GAN) has been widely\nused, but suffers from the problem of low intra-class diversity of the\ngenerated samples. The fundamental reason pointed out in this paper is that the\nclassifier of AC-GAN is generator-agnostic, which therefore cannot provide\ninformative guidance for the generator to approach the joint distribution,\nresulting in a minimization of the conditional entropy that decreases the\nintra-class diversity. Motivated by this understanding, we propose a novel\nconditional GAN with an auxiliary discriminative classifier (ADC-GAN) to\nresolve the above problem. Specifically, the proposed auxiliary discriminative\nclassifier becomes generator-aware by recognizing the class-labels of the real\ndata and the generated data discriminatively. Our theoretical analysis reveals\nthat the generator can faithfully learn the joint distribution even without the\noriginal discriminator, making the proposed ADC-GAN robust to the value of the\ncoefficient hyperparameter and the selection of the GAN loss, and stable during\ntraining. Extensive experimental results on synthetic and real-world datasets\ndemonstrate the superiority of ADC-GAN in conditional generative modeling\ncompared to state-of-the-art classifier-based and projection-based conditional\nGANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Siyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoshuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Foot Ulcer Segmentation Using an Ensemble of Convolutional Neural Networks. (arXiv:2109.01408v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.01408","description":"<p>Foot ulcer is a common complication of diabetes mellitus and, associated with\nsubstantial morbidity and mortality, remains a major risk factor for lower leg\namputations. Extracting accurate morphological features from foot wounds is\ncrucial for appropriate treatment. Although visual inspection by a medical\nprofessional is the common approach for diagnosis, this is subjective and\nerror-prone, and computer-aided approaches thus provide an interesting\nalternative. Deep learning-based methods, and in particular convolutional\nneural networks (CNNs), have shown excellent performance for various tasks in\nmedical image analysis including medical image segmentation.\n</p>\n<p>In this paper, we propose an ensemble approach based on two\nencoder-decoder-based CNN models, namely LinkNet and U-Net, to perform foot\nulcer segmentation. To deal with a limited number of available training\nsamples, we use pre-trained weights (EfficientNetB1 for the LinkNet model and\nEfficientNetB2 for the U-Net model) and perform further pre-training using the\nMedetec dataset while also applying a number of morphological-based and\ncolour-based augmentation techniques. To boost the segmentation performance, we\nincorporate five-fold cross-validation, test time augmentation and result\nfusion.\n</p>\n<p>Applied on the publicly available chronic wound dataset and the MICCAI 2021\nFoot Ulcer Segmentation (FUSeg) Challenge, our method achieves state-of-the-art\nperformance with data-based Dice scores of 92.07% and 88.80%, respectively, and\nis the top ranked method in the FUSeg challenge leaderboard. The Dockerised\nguidelines, inference codes and saved trained models are publicly available at\nhttps://github.com/masih4/Foot_Ulcer_Segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mahbod_A/0/1/0/all/0/1\">Amirreza Mahbod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaefer_G/0/1/0/all/0/1\">Gerald Schaefer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ecker_R/0/1/0/all/0/1\">Rupert Ecker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ellinger_I/0/1/0/all/0/1\">Isabella Ellinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09818","description":"<p>Convolutional Neural Networks have demonstrated dermatologist-level\nperformance in the classification of melanoma from skin lesion images, but\nprediction irregularities due to biases seen within the training data are an\nissue that should be addressed before widespread deployment is possible. In\nthis work, we robustly remove bias and spurious variation from an automated\nmelanoma classification pipeline using two leading bias unlearning techniques.\nWe show that the biases introduced by surgical markings and rulers presented in\nprevious studies can be reasonably mitigated using these bias removal methods.\nWe also demonstrate the generalisation benefits of unlearning spurious\nvariation relating to the imaging instrument used to capture lesion images. Our\nexperimental results provide evidence that the effects of each of the\naforementioned biases are notably reduced, with different debiasing techniques\nexcelling at different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), reducing inherent model hazards\n(\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Fast Adversarial Training with Learnable Adversarial Initialization. (arXiv:2110.05007v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05007","description":"<p>Adversarial training (AT) has been demonstrated to be effective in improving\nmodel robustness by leveraging adversarial examples for training. However, most\nAT methods are in face of expensive time and computational cost for calculating\ngradients at multiple steps in generating adversarial examples. To boost\ntraining efficiency, fast gradient sign method (FGSM) is adopted in fast AT\nmethods by calculating gradient only once. Unfortunately, the robustness is far\nfrom satisfactory. One reason may arise from the initialization fashion.\nExisting fast AT generally uses a random sample-agnostic initialization, which\nfacilitates the efficiency yet hinders a further robustness improvement. Up to\nnow, the initialization in fast AT is still not extensively explored. In this\npaper, we boost fast AT with a sample-dependent adversarial initialization,\ni.e., an output from a generative network conditioned on a benign image and its\ngradient information from the target network. As the generative network and the\ntarget network are optimized jointly in the training phase, the former can\nadaptively generate an effective initialization with respect to the latter,\nwhich motivates gradually improved robustness. Experimental evaluations on four\nbenchmark databases demonstrate the superiority of our proposed method over\nstate-of-the-art fast AT methods, as well as comparable robustness to advanced\nmulti-step AT methods. The code is released at\nhttps://github.com//jiaxiaojunQAQ//FGSM-SDI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaojun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"whu-nercms at trecvid2021:instance search task. (arXiv:2111.00228v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00228","description":"<p>We will make a brief introduction of the experimental methods and results of\nthe WHU-NERCMS in the TRECVID2021 in the paper. This year we participate in the\nautomatic and interactive tasks of Instance Search (INS). For the automatic\ntask, the retrieval target is divided into two parts, person retrieval, and\naction retrieval. We adopt a two-stage method including face detection and face\nrecognition for person retrieval and two kinds of action detection methods\nconsisting of three frame-based human-object interaction detection methods and\ntwo video-based general action detection methods for action retrieval. After\nthat, the person retrieval results and action retrieval results are fused to\ninitialize the result ranking lists. In addition, we make attempts to use\ncomplementary methods to further improve search performance. For interactive\ntasks, we test two different interaction strategies on the fusion results. We\nsubmit 4 runs for automatic and interactive tasks respectively. The\nintroduction of each run is shown in Table 1. The official evaluations show\nthat the proposed strategies rank 1st in both automatic and interactive tracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yanrui Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Ankang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baojin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Ji Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shishi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongshu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Triangular 3D Models, Materials, and Lighting From Images. (arXiv:2111.12503v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12503","description":"<p>We present an efficient method for joint optimization of topology, materials\nand lighting from multi-view image observations. Unlike recent multi-view\nreconstruction approaches, which typically produce entangled 3D representations\nencoded in neural networks, we output triangle meshes with spatially-varying\nmaterials and environment lighting that can be deployed in any traditional\ngraphics engine unmodified. We leverage recent work in differentiable\nrendering, coordinate-based networks to compactly represent volumetric\ntexturing, alongside differentiable marching tetrahedrons to enable\ngradient-based optimization directly on the surface mesh. Finally, we introduce\na differentiable formulation of the split sum approximation of environment\nlighting to efficiently recover all-frequency lighting. Experiments show our\nextracted models used in advanced scene editing, material decomposition, and\nhigh quality view interpolation, all running at interactive rates in\ntriangle-based renderers (rasterizers and path tracers). Project website:\nhttps://nvlabs.github.io/nvdiffrec/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munkberg_J/0/1/0/all/0/1\">Jacob Munkberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasselgren_J/0/1/0/all/0/1\">Jon Hasselgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianchang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_A/0/1/0/all/0/1\">Alex Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions. (arXiv:2111.14813v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14813","description":"<p>Removing adverse weather conditions like rain, fog, and snow from images is\nan important problem in many applications. Most methods proposed in the\nliterature have been designed to deal with just removing one type of\ndegradation. Recently, a CNN-based method using neural architecture search\n(All-in-One) was proposed to remove all the weather conditions at once.\nHowever, it has a large number of parameters as it uses multiple encoders to\ncater to each weather removal task and still has scope for improvement in its\nperformance. In this work, we focus on developing an efficient solution for the\nall adverse weather removal problem. To this end, we propose TransWeather, a\ntransformer-based end-to-end model with just a single encoder and a decoder\nthat can restore an image degraded by any weather condition. Specifically, we\nutilize a novel transformer encoder using intra-patch transformer blocks to\nenhance attention inside the patches to effectively remove smaller weather\ndegradations. We also introduce a transformer decoder with learnable weather\ntype embeddings to adjust to the weather degradation at hand. TransWeather\nachieves improvements across multiple test datasets over both All-in-One\nnetwork as well as methods fine-tuned for specific tasks. TransWeather is also\nvalidated on real world test images and found to be more effective than\nprevious methods. Implementation code can be accessed at\nhttps://github.com/jeya-maria-jose/TransWeather .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasarla_R/0/1/0/all/0/1\">Rajeev Yasarla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03857","description":"<p>This paper presents a grounded language-image pre-training (GLIP) model for\nlearning object-level, language-aware, and semantic-rich visual\nrepresentations. GLIP unifies object detection and phrase grounding for\npre-training. The unification brings two benefits: 1) it allows GLIP to learn\nfrom both detection and grounding data to improve both tasks and bootstrap a\ngood grounding model; 2) GLIP can leverage massive image-text pairs by\ngenerating grounding boxes in a self-training fashion, making the learned\nrepresentation semantic-rich. In our experiments, we pre-train GLIP on 27M\ngrounding data, including 3M human-annotated and 24M web-crawled image-text\npairs. The learned representations demonstrate strong zero-shot and few-shot\ntransferability to various object-level recognition tasks. 1) When directly\nevaluated on COCO and LVIS (without seeing any images in COCO during\npre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many\nsupervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val\nand 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13\ndownstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised\nDynamic Head. Code is released at https://github.com/microsoft/GLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiwu Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning. (arXiv:2201.10029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10029","description":"<p>State-of-the-art approaches to ObjectGoal navigation rely on reinforcement\nlearning and typically require significant computational resources and time for\nlearning. We propose Potential functions for ObjectGoal Navigation with\nInteraction-free learning (PONI), a modular approach that disentangles the\nskills of `where to look?' for an object and `how to navigate to (x, y)?'. Our\nkey insight is that `where to look?' can be treated purely as a perception\nproblem, and learned without environment interactions. To address this, we\npropose a network that predicts two complementary potential functions\nconditioned on a semantic map and uses them to decide where to look for an\nunseen object. We train the potential function network using supervised\nlearning on a passive dataset of top-down semantic maps, and integrate it into\na modular framework to perform ObjectGoal navigation. Experiments on Gibson and\nMatterport3D demonstrate that our method achieves the state-of-the-art for\nObjectGoal navigation while incurring up to 1,600x less computational cost for\ntraining. Code and pre-trained models are available:\nhttps://vision.cs.utexas.edu/projects/poni/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Kumar Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Halah_Z/0/1/0/all/0/1\">Ziad Al-Halah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning To Recognize Procedural Activities with Distant Supervision. (arXiv:2201.10990v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10990","description":"<p>In this paper we consider the problem of classifying fine-grained, multi-step\nactivities (e.g., cooking different recipes, making disparate home\nimprovements, creating various forms of arts and crafts) from long videos\nspanning up to several minutes. Accurately categorizing these activities\nrequires not only recognizing the individual steps that compose the task but\nalso capturing their temporal dependencies. This problem is dramatically\ndifferent from traditional action classification, where models are typically\noptimized on videos that span only a few seconds and that are manually trimmed\nto contain simple atomic actions. While step annotations could enable the\ntraining of models to recognize the individual steps of procedural activities,\nexisting large-scale datasets in this area do not include such segment labels\ndue to the prohibitive cost of manually annotating temporal boundaries in long\nvideos. To address this issue, we propose to automatically identify steps in\ninstructional videos by leveraging the distant supervision of a textual\nknowledge base (wikiHow) that includes detailed descriptions of the steps\nneeded for the execution of a wide variety of complex activities. Our method\nuses a language model to match noisy, automatically-transcribed speech from the\nvideo to step descriptions in the knowledge base. We demonstrate that video\nmodels trained to recognize these automatically-labeled steps (without manual\nsupervision) yield a representation that achieves superior generalization\nperformance on four downstream tasks: recognition of procedural activities,\nstep classification, step forecasting and egocentric video classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PINs: Progressive Implicit Networks for Multi-Scale Neural Representations. (arXiv:2202.04713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04713","description":"<p>Multi-layer perceptrons (MLP) have proven to be effective scene encoders when\ncombined with higher-dimensional projections of the input, commonly referred to\nas \\textit{positional encoding}. However, scenes with a wide frequency spectrum\nremain a challenge: choosing high frequencies for positional encoding\nintroduces noise in low structure areas, while low frequencies result in poor\nfitting of detailed regions. To address this, we propose a progressive\npositional encoding, exposing a hierarchical MLP structure to incremental sets\nof frequency encodings. Our model accurately reconstructs scenes with wide\nfrequency bands and learns a scene representation at progressive level of\ndetail \\textit{without explicit per-level supervision}. The architecture is\nmodular: each level encodes a continuous implicit representation that can be\nleveraged separately for its respective resolution, meaning a smaller network\nfor coarser reconstructions. Experiments on several 2D and 3D datasets show\nimprovements in reconstruction accuracy, representational capacity and training\nspeed compared to baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landgraf_Z/0/1/0/all/0/1\">Zoe Landgraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hornung_A/0/1/0/all/0/1\">Alexander Sorkine Hornung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabral_R/0/1/0/all/0/1\">Ricardo Silveira Cabral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artemis: Articulated Neural Pets with Appearance and Motion synthesis. (arXiv:2202.05628v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2202.05628","description":"<p>We, humans, are entering into a virtual era and indeed want to bring animals\nto the virtual world as well for companion. Yet, computer-generated (CGI) furry\nanimals are limited by tedious off-line rendering, let alone interactive motion\ncontrol. In this paper, we present ARTEMIS, a novel neural modeling and\nrendering pipeline for generating ARTiculated neural pets with appEarance and\nMotion synthesIS. Our ARTEMIS enables interactive motion control, real-time\nanimation, and photo-realistic rendering of furry animals. The core of our\nARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient\noctree-based representation for animal animation and fur rendering. The\nanimation then becomes equivalent to voxel-level deformation based on explicit\nskeletal warping. We further use a fast octree indexing and efficient\nvolumetric rendering scheme to generate appearance and density features maps.\nFinally, we propose a novel shading network to generate high-fidelity details\nof appearance and opacity under novel poses from appearance and density feature\nmaps. For the motion control module in ARTEMIS, we combine state-of-the-art\nanimal motion capture approach with recent neural character control scheme. We\nintroduce an effective optimization scheme to reconstruct the skeletal motion\nof real animals captured by a multi-view RGB and Vicon camera array. We feed\nall the captured motion into a neural character control scheme to generate\nabstract control signals with motion styles. We further integrate ARTEMIS into\nexisting engines that support VR headsets, providing an unprecedented immersive\nexperience where a user can intimately interact with a variety of virtual\nanimals with vivid movements and photo-realistic appearance. We make available\nour ARTEMIS model and dynamic furry animal dataset at\nhttps://haiminluo.github.io/publication/artemis/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haimin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1\">Qiwei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Borrowing from yourself: Faster future video segmentation with partial channel update. (arXiv:2202.05748v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05748","description":"<p>Semantic segmentation is a well-addressed topic in the computer vision\nliterature, but the design of fast and accurate video processing networks\nremains challenging. In addition, to run on embedded hardware, computer vision\nmodels often have to make compromises on accuracy to run at the required speed,\nso that a latency/accuracy trade-off is usually at the heart of these real-time\nsystems' design. For the specific case of videos, models have the additional\npossibility to make use of computations made for previous frames to mitigate\nthe accuracy loss while being real-time.\n</p>\n<p>In this work, we propose to tackle the task of fast future video segmentation\nprediction through the use of convolutional layers with time-dependent channel\nmasking. This technique only updates a chosen subset of the feature maps at\neach time-step, bringing simultaneously less computation and latency, and\nallowing the network to leverage previously computed features. We apply this\ntechnique to several fast architectures and experimentally confirm its benefits\nfor the future prediction subtask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Courdier_E/0/1/0/all/0/1\">Evann Courdier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Fran&#xe7;ois Fleuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06767","description":"<p>Vision-Language Pre-training (VLP) models have shown remarkable performance\non various downstream tasks. Their success heavily relies on the scale of\npre-trained cross-modal datasets. However, the lack of large-scale datasets and\nbenchmarks in Chinese hinders the development of Chinese VLP models and broader\nmultilingual applications. In this work, we release a large-scale Chinese\ncross-modal dataset named Wukong, which contains 100 million Chinese image-text\npairs collected from the web. Wukong aims to benchmark different multi-modal\npre-training methods to facilitate the VLP research and community development.\nFurthermore, we release a group of models pre-trained with various image\nencoders (ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques\ninto VLP such as locked-image text tuning, token-wise similarity in contrastive\nlearning, and reduced-token interaction. Extensive experiments and a\nbenchmarking of different downstream tasks including a new largest\nhuman-verified image-text test dataset are also provided. Experiments show that\nWukong can serve as a promising Chinese pre-training dataset and benchmark for\ndifferent cross-modal learning methods. For the zero-shot image classification\ntask on 10 datasets, $Wukong_{ViT-L}$ achieves an average accuracy of 73.03%.\nFor the image-text retrieval task, it achieves a mean recall of 71.6% on\nAIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are\nbenchmarked on downstream tasks with other variants on multiple datasets, e.g.,\nFlickr8K-CN, Flickr-30K-CN, COCO-CN, et~al. More information can be referred\nto: https://wukong-dataset.github.io/wukong-dataset/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaxi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaojun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guansong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Minzhe Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Runhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BED: A Real-Time Object Detection System for Edge Devices. (arXiv:2202.07503v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07503","description":"<p>Deploying deep neural networks~(DNNs) on edge devices provides efficient and\neffective solutions for the real-world tasks. Edge devices have been used for\ncollecting a large volume of data efficiently in different domains. DNNs have\nbeen an effective tool for data processing and analysis. However, designing\nDNNs on edge devices is challenging due to the limited computational resources\nand memory. To tackle this challenge, we demonstrate Object Detection System\nfor Edge Devices~(BED) on the MAX78000 DNN accelerator. It integrates on-device\nDNN inference with a camera and an LCD display for image acquisition and\ndetection exhibition, respectively. BED is a concise, effective and detailed\nsolution, including model training, quantization, synthesis and deployment.\nExperiment results indicate that BED can produce accurate detection with a\n300-KB tiny DNN model, which takes only 91.9 ms of inference time and 1.845 mJ\nof energy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanchu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhimeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_A/0/1/0/all/0/1\">Alfredo Costilla Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niktash_A/0/1/0/all/0/1\">Afshin Niktash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulkar_G/0/1/0/all/0/1\">Gorkem Ulkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okman_E/0/1/0/all/0/1\">Erman Okman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Vision Transformer. (arXiv:2203.03821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03821","description":"<p>Vision Transformers (ViT) have made many breakthroughs in computer vision\ntasks. However, considerable redundancy arises in the spatial dimension of an\ninput image, leading to massive computational costs. Therefore, We propose a\ncoarse-to-fine vision transformer (CF-ViT) to relieve computational burden\nwhile retaining performance in this paper. Our proposed CF-ViT is motivated by\ntwo important observations in modern ViT models: (1) The coarse-grained patch\nsplitting can locate informative regions of an input image. (2) Most images can\nbe well recognized by a ViT model in a small-length token sequence. Therefore,\nour CF-ViT implements network inference in a two-stage manner. At coarse\ninference stage, an input image is split into a small-length patch sequence for\na computationally economical classification. If not well recognized, the\ninformative patches are identified and further re-split in a fine-grained\ngranularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For\nexample, without any compromise on performance, CF-ViT reduces 53% FLOPs of\nLV-ViT, and also achieves 2.01x throughput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetAug: Contrastive Learning via Meta Feature Augmentation. (arXiv:2203.05119v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05119","description":"<p>What matters for contrastive learning? We argue that contrastive learning\nheavily relies on informative features, or \"hard\" (positive or negative)\nfeatures. Early works include more informative features by applying complex\ndata augmentations and large batch size or memory bank, and recent works design\nelaborate sampling approaches to explore informative features. The key\nchallenge toward exploring such features is that the source multi-view data is\ngenerated by applying random data augmentations, making it infeasible to always\nadd useful information in the augmented data. Consequently, the informativeness\nof features learned from such augmented data is limited. In response, we\npropose to directly augment the features in latent space, thereby learning\ndiscriminative representations without a large amount of input data. We perform\na meta learning technique to build the augmentation generator that updates its\nnetwork parameters by considering the performance of the encoder. However,\ninsufficient input data may lead the encoder to learn collapsed features and\ntherefore malfunction the augmentation generator. A new margin-injected\nregularization is further added in the objective function to avoid the encoder\nlearning a degenerate mapping. To contrast all features in one gradient\nback-propagation step, we adopt the proposed optimization-driven unified\ncontrastive loss instead of the conventional contrastive loss. Empirically, our\nmethod achieves state-of-the-art results on several benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers. (arXiv:2203.10726v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.10726","description":"<p>Combining information from multi-view images is crucial to improve the\nperformance and robustness of automated methods for disease diagnosis. However,\ndue to the non-alignment characteristics of multi-view images, building\ncorrelation and data fusion across views largely remain an open problem. In\nthis study, we present TransFusion, a Transformer-based architecture to merge\ndivergent multi-view imaging information using convolutional layers and\npowerful attention mechanisms. In particular, the Divergent Fusion Attention\n(DiFA) module is proposed for rich cross-view context modeling and semantic\ndependency mining, addressing the critical issue of capturing long-range\ncorrelations between unaligned data from different image views. We further\npropose the Multi-Scale Attention (MSA) to collect global correspondence of\nmulti-scale feature representations. We evaluate TransFusion on the\nMulti-Disease, Multi-View \\&amp; Multi-Center Right Ventricular Segmentation in\nCardiac MRI (M\\&amp;Ms-2) challenge cohort. TransFusion demonstrates leading\nperformance against the state-of-the-art methods and opens up new perspectives\nfor multi-view imaging integration towards robust medical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhangli_Q/0/1/0/all/0/1\">Qilong Zhangli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1\">Ligong Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xiaoxiao He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_Z/0/1/0/all/0/1\">Zhaoyang Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_S/0/1/0/all/0/1\">Song Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_Q/0/1/0/all/0/1\">Qi Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zhennan Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning with Action-Free Pre-Training from Videos. (arXiv:2203.13880v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13880","description":"<p>Recent unsupervised pre-training methods have shown to be effective on\nlanguage and vision domains by learning useful representations for multiple\ndownstream tasks. In this paper, we investigate if such unsupervised\npre-training methods can also be effective for vision-based reinforcement\nlearning (RL). To this end, we introduce a framework that learns\nrepresentations useful for understanding the dynamics via generative\npre-training on videos. Our framework consists of two phases: we pre-train an\naction-free latent video prediction model, and then utilize the pre-trained\nrepresentations for efficiently learning action-conditional world models on\nunseen environments. To incorporate additional action inputs during\nfine-tuning, we introduce a new architecture that stacks an action-conditional\nlatent prediction model on top of the pre-trained action-free prediction model.\nMoreover, for better exploration, we propose a video-based intrinsic bonus that\nleverages pre-trained representations. We demonstrate that our framework\nsignificantly improves both final performances and sample-efficiency of\nvision-based RL in a variety of manipulation and locomotion tasks. Code is\navailable at https://github.com/younggyoseo/apv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Younggyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UV Volumes for Real-time Rendering of Editable Free-view Human Performance. (arXiv:2203.14402v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14402","description":"<p>Neural volume rendering enables photo-realistic renderings of a human\nperformer in free-view, a critical task in immersive VR/AR applications. But\nthe practice is severely limited by high computational costs in the rendering\nprocess. To solve this problem, we propose the UV Volumes, a new approach that\ncan render an editable free-view video of a human performer in realtime. It\nseparates the high-frequency (i.e., non-smooth) human appearance from the 3D\nvolume, and encodes them into 2D neural texture stacks (NTS). The smooth UV\nvolumes allow much smaller and shallower neural networks to obtain densities\nand texture coordinates in 3D while capturing detailed appearance in 2D NTS.\nFor editability, the mapping between the parameterized human model and the\nsmooth texture coordinates allows us a better generalization on novel poses and\nshapes. Furthermore, the use of NTS enables interesting applications, e.g.,\nretexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M\ndatasets show that our model can render 960 * 540 images in 30FPS on average\nwith comparable photo-realism to state-of-the-art methods. The project and\nsupplementary materials are available at https://github.com/fanegg/UV-Volumes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI. (arXiv:2203.15163v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.15163","description":"<p>Prostate cancer is the second leading cause of cancer death among men in the\nUnited States. The diagnosis of prostate MRI often relies on the accurate\nprostate zonal segmentation. However, state-of-the-art automatic segmentation\nmethods often fail to produce well-contained volumetric segmentation of the\nprostate zones since certain slices of prostate MRI, such as base and apex\nslices, are harder to segment than other slices. This difficulty can be\novercome by accounting for the cross-slice relationship of adjacent slices, but\ncurrent methods do not fully learn and exploit such relationships. In this\npaper, we propose a novel cross-slice attention mechanism, which we use in a\nTransformer module to systematically learn the cross-slice relationship at\ndifferent scales. The module can be utilized in any existing learning-based\nsegmentation framework with skip connections. Experiments show that our\ncross-slice attention is able to capture the cross-slice information in\nprostate zonal segmentation and improve the performance of current\nstate-of-the-art methods. Our method improves segmentation accuracy in the\nperipheral zone, such that the segmentation results are consistent across all\nthe prostate slices (apex, mid-gland, and base).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1\">Alex Ling Yu Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Haoxin Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_Q/0/1/0/all/0/1\">Qi Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raman_S/0/1/0/all/0/1\">Steven S. Raman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Terzopoulos_D/0/1/0/all/0/1\">Demetri Terzopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sung_K/0/1/0/all/0/1\">Kyunghyun Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Weakly Supervised Object Detection by Sampling Pseudo Ground-Truth Boxes. (arXiv:2204.00147v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00147","description":"<p>Semi- and weakly-supervised learning have recently attracted considerable\nattention in the object detection literature since they can alleviate the cost\nof annotation needed to successfully train deep learning models. State-of-art\napproaches for semi-supervised learning rely on student-teacher models trained\nusing a multi-stage process, and considerable data augmentation. Custom\nnetworks have been developed for the weakly-supervised setting, making it\ndifficult to adapt to different detectors. In this paper, a weakly\nsemi-supervised training method is introduced that reduces these training\nchallenges, yet achieves state-of-the-art performance by leveraging only a\nsmall fraction of fully-labeled images with information in weakly-labeled\nimages. In particular, our generic sampling-based learning strategy produces\npseudo-ground-truth (GT) bounding box annotations in an online fashion,\neliminating the need for multi-stage training, and student-teacher network\nconfigurations. These pseudo GT boxes are sampled from weakly-labeled images\nbased on the categorical score of object proposals accumulated via a score\npropagation process. Empirical results on the Pascal VOC dataset, indicate that\nthe proposed approach improves performance by 5.0% when using VOC 2007 as\nfully-labeled, and VOC 2012 as weak-labeled data. Also, with 5-10% fully\nannotated images, we observed an improvement of more than 10% in mAP, showing\nthat a modest investment in image-level annotation, can substantially improve\ndetection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meethal_A/0/1/0/all/0/1\">Akhil Meethal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhongwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_F/0/1/0/all/0/1\">Francisco Perdigon Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-Distribution Detection with Deep Nearest Neighbors. (arXiv:2204.06507v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.06507","description":"<p>Out-of-distribution (OOD) detection is a critical task for deploying machine\nlearning models in the open world. Distance-based methods have demonstrated\npromise, where testing samples are detected as OOD if they are relatively far\naway from in-distribution (ID) data. However, prior methods impose a strong\ndistributional assumption of the underlying feature space, which may not always\nhold. In this paper, we explore the efficacy of non-parametric nearest-neighbor\ndistance for OOD detection, which has been largely overlooked in the\nliterature. Unlike prior works, our method does not impose any distributional\nassumption, hence providing stronger flexibility and generality. We demonstrate\nthe effectiveness of nearest-neighbor-based OOD detection on several benchmarks\nand establish superior performance. Under the same model trained on\nImageNet-1k, our method substantially reduces the false positive rate\n(FPR@TPR95) by 24.77% compared to a strong baseline SSD+, which uses a\nparametric approach Mahalanobis distance in detection. Code is available:\nhttps://github.com/deeplearning-wisc/knn-ood.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiyou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yifei Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaojin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognising Known Configurations of Garments For Dual-Arm Robotic Flattening. (arXiv:2205.00225v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2205.00225","description":"<p>Robotic deformable-object manipulation is a challenge in the robotic industry\nbecause deformable objects have complicated and various object states.\nPredicting those object states and updating manipulation planning is\ntime-consuming and computationally expensive. In this paper, we propose\nlearning known configurations of garments to allow a robot to recognise garment\nstates and choose a pre-designed manipulation plan for garment flattening.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Li Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argon_Camarasa_G/0/1/0/all/0/1\">Gerardo Argon-Camarasa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction. (arXiv:2205.06672v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06672","description":"<p>Classical multiple instance learning (MIL) methods are often based on the\nidentical and independent distributed assumption between instances, hence\nneglecting the potentially rich contextual information beyond individual\nentities. On the other hand, Transformers with global self-attention modules\nhave been proposed to model the interdependencies among all instances. However,\nin this paper we question: Is global relation modeling using self-attention\nnecessary, or can we appropriately restrict self-attention calculations to\nlocal regimes in large-scale whole slide images (WSIs)? We propose a\ngeneral-purpose local attention graph-based Transformer for MIL (LA-MIL),\nintroducing an inductive bias by explicitly contextualizing instances in\nadaptive local regimes of arbitrary size. Additionally, an efficiently adapted\nloss function enables our approach to learn expressive WSI embeddings for the\njoint analysis of multiple biomarkers. We demonstrate that LA-MIL achieves\nstate-of-the-art results in mutation prediction for gastrointestinal cancer,\noutperforming existing models on important biomarkers such as microsatellite\ninstability for colorectal cancer. Our findings suggest that local\nself-attention sufficiently models dependencies on par with global modules. Our\nLA-MIL implementation is available at https://github.com/agentdr1/LA_MIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reisenbuchler_D/0/1/0/all/0/1\">Daniel Reisenb&#xfc;chler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Sophia J. Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boxberg_M/0/1/0/all/0/1\">Melanie Boxberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tingying Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Split Computing for Efficient Deep Edge Intelligence. (arXiv:2205.11269v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11269","description":"<p>Deploying deep neural networks (DNNs) on IoT and mobile devices is a\nchallenging task due to their limited computational resources. Thus, demanding\ntasks are often entirely offloaded to edge servers which can accelerate\ninference, however, it also causes communication cost and evokes privacy\nconcerns. In addition, this approach leaves the computational capacity of end\ndevices unused. Split computing is a paradigm where a DNN is split into two\nsections; the first section is executed on the end device, and the output is\ntransmitted to the edge server where the final section is executed. Here, we\nintroduce dynamic split computing, where the optimal split location is\ndynamically selected based on the state of the communication channel. By using\nnatural bottlenecks that already exist in modern DNN architectures, dynamic\nsplit computing avoids retraining and hyperparameter optimization, and does not\nhave any negative impact on the final accuracy of DNNs. Through extensive\nexperiments, we show that dynamic split computing achieves faster inference in\nedge computing environments where the data rate and server load vary over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1\">Arian Bakhtiarnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nemanja Milo&#x161;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajovic_D/0/1/0/all/0/1\">Dragana Bajovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Interpretability via Polynomials. (arXiv:2205.14108v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14108","description":"<p>Generalized Additive Models (GAMs) have quickly become the leading choice for\nfully-interpretable machine learning. However, unlike uninterpretable methods\nsuch as DNNs, they lack expressive power and easy scalability, and are hence\nnot a feasible alternative for real-world tasks. We present a new class of GAMs\nthat use tensor rank decompositions of polynomials to learn powerful, {\\em\nfully-interpretable} models. Our approach, titled Scalable Polynomial Additive\nModels (SPAM) is effortlessly scalable and models {\\em all} higher-order\nfeature interactions without a combinatorial parameter explosion. SPAM\noutperforms all current interpretable approaches, and matches DNN/XGBoost\nperformance on a series of real-world benchmarks with up to hundreds of\nthousands of features. We demonstrate by human subject evaluations that SPAMs\nare demonstrably more interpretable in practice, and are hence an effortless\nreplacement for DNNs for creating interpretable and high-performance systems\nsuitable for large-scale machine learning. Source code is available at\nhttps://github.com/facebookresearch/nbm-spam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhimanyu Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Basis Models for Interpretability. (arXiv:2205.14120v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14120","description":"<p>Due to the widespread use of complex machine learning models in real-world\napplications, it is becoming critical to explain model predictions. However,\nthese models are typically black-box deep neural networks, explained post-hoc\nvia methods with known faithfulness limitations. Generalized Additive Models\n(GAMs) are an inherently interpretable class of models that address this\nlimitation by learning a non-linear shape function for each feature separately,\nfollowed by a linear model on top. However, these models are typically\ndifficult to train, require numerous parameters, and are difficult to scale.\n</p>\n<p>We propose an entirely new subfamily of GAMs that utilizes basis\ndecomposition of shape functions. A small number of basis functions are shared\namong all features, and are learned jointly for a given task, thus making our\nmodel scale much better to large-scale data with high-dimensional features,\nespecially when features are sparse. We propose an architecture denoted as the\nNeural Basis Model (NBM) which uses a single neural network to learn these\nbases. On a variety of tabular and image datasets, we demonstrate that for\ninterpretable machine learning, NBMs are the state-of-the-art in accuracy,\nmodel size, and, throughput and can easily model all higher-order feature\ninteractions.\n</p>\n<p>Source code is available at https://github.com/facebookresearch/nbm-spam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhimanyu Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. (arXiv:2206.00843v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.00843","description":"<p>Efficient deep neural network (DNN) models equipped with compact operators\n(e.g., depthwise convolutions) have shown great potential in reducing DNNs'\ntheoretical complexity (e.g., the total number of weights/operations) while\nmaintaining a decent model accuracy. However, existing efficient DNNs are still\nlimited in fulfilling their promise in boosting real-hardware efficiency, due\nto their commonly adopted compact operators' low hardware utilization. In this\nwork, we open up a new compression paradigm for developing real-hardware\nefficient DNNs, leading to boosted hardware efficiency while maintaining model\naccuracy. Interestingly, we observe that while some DNN layers' activation\nfunctions help DNNs' training optimization and achievable accuracy, they can be\nproperly removed after training without compromising the model accuracy.\nInspired by this observation, we propose a framework dubbed DepthShrinker,\nwhich develops hardware-friendly compact networks via shrinking the basic\nbuilding blocks of existing efficient DNNs that feature irregular computation\npatterns into dense ones with much improved hardware utilization and thus\nreal-hardware efficiency. Excitingly, our DepthShrinker framework delivers\nhardware-friendly compact networks that outperform both state-of-the-art\nefficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and\n1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method\nMetaPruning. Our codes are available at:\nhttps://github.com/facebookresearch/DepthShrinker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiayi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Cheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1\">Raghuraman Krishnamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supernet Training for Federated Image Classification under System Heterogeneity. (arXiv:2206.01366v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.01366","description":"<p>Efficient deployment of deep neural networks across many devices and resource\nconstraints, especially on edge devices, is one of the most challenging\nproblems in the presence of data-privacy preservation issues. Conventional\napproaches have evolved to either improve a single global model while keeping\neach local training data decentralized (i.e., data-heterogeneity) or to train a\nonce-for-all network that supports diverse architectural settings to address\nheterogeneous systems equipped with different computational capabilities (i.e.,\nmodel-heterogeneity). However, little research has considered both directions\nsimultaneously. In this work, we propose a novel framework to consider both\nscenarios, namely Federation of Supernet Training (FedSup), where clients send\nand receive a supernet whereby it contains all possible architectures sampled\nfrom itself. It is inspired by how averaging parameters in the model\naggregation stage of Federated Learning (FL) is similar to weight-sharing in\nsupernet training. Specifically, in the FedSup framework, a weight-sharing\napproach widely used in the training single shot model is combined with the\naveraging of Federated Learning (FedAvg). Under our framework, we present an\nefficient algorithm (E-FedSup) by sending the sub-model to clients in the\nbroadcast stage for reducing communication costs and training overhead. We\ndemonstrate several strategies to enhance supernet training in the FL\nenvironment and conduct extensive empirical evaluations. The resulting\nframework is shown to pave the way for the robustness of both data- and\nmodel-heterogeneity on several standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Video Restoration Transformer with Guided Deformable Attention. (arXiv:2206.02146v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02146","description":"<p>Video restoration aims at restoring multiple high-quality frames from\nmultiple low-quality frames. Existing video restoration methods generally fall\ninto two extreme cases, i.e., they either restore all frames in parallel or\nrestore the video frame by frame in a recurrent way, which would result in\ndifferent merits and drawbacks. Typically, the former has the advantage of\ntemporal information fusion. However, it suffers from large model size and\nintensive memory consumption; the latter has a relatively small model size as\nit shares parameters across frames; however, it lacks long-range dependency\nmodeling ability and parallelizability. In this paper, we attempt to integrate\nthe advantages of the two cases by proposing a recurrent video restoration\ntransformer, namely RVRT. RVRT processes local neighboring frames in parallel\nwithin a globally recurrent framework which can achieve a good trade-off\nbetween model size, effectiveness, and efficiency. Specifically, RVRT divides\nthe video into multiple clips and uses the previously inferred clip feature to\nestimate the subsequent clip feature. Within each clip, different frame\nfeatures are jointly updated with implicit feature aggregation. Across\ndifferent clips, the guided deformable attention is designed for clip-to-clip\nalignment, which predicts multiple relevant locations from the whole inferred\nclip and aggregates their features by the attention mechanism. Extensive\nexperiments on video super-resolution, deblurring, and denoising show that the\nproposed RVRT achieves state-of-the-art performance on benchmark datasets with\nbalanced model size, testing memory and runtime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yuchen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiaoyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1\">Rakesh Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilg_E/0/1/0/all/0/1\">Eddy Ilg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_S/0/1/0/all/0/1\">Simon Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.04459","description":"<p>In order to deploy deep models in a computationally efficient manner, model\nquantization approaches have been frequently used. In addition, as new hardware\nthat supports mixed bitwidth arithmetic operations, recent research on mixed\nprecision quantization (MPQ) begins to fully leverage the capacity of\nrepresentation by searching optimized bitwidths for different layers and\nmodules in a network. However, previous studies mainly search the MPQ strategy\nin a costly scheme using reinforcement learning, neural architecture search,\netc., or simply utilize partial prior knowledge for bitwidth assignment, which\nmight be biased and sub-optimal. In this work, we present a novel Stochastic\nDifferentiable Quantization (SDQ) method that can automatically learn the MPQ\nstrategy in a more flexible and globally-optimized space with smoother gradient\napproximation. Particularly, Differentiable Bitwidth Parameters (DBPs) are\nemployed as the probability factors in stochastic quantization between adjacent\nbitwidth choices. After the optimal MPQ strategy is acquired, we further train\nour network with entropy-aware bin regularization and knowledge distillation.\nWe extensively evaluate our method for several networks on different hardware\n(GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or\nsingle precision quantization with a lower bitwidth and is even better than the\nfull-precision counterparts across various ResNet and MobileNet families,\ndemonstrating the effectiveness and superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xianghong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicaksana_J/0/1/0/all/0/1\">Jeffry Wicaksana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometrically Guided Integrated Gradients. (arXiv:2206.05903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05903","description":"<p>Interpretability methods for deep neural networks mainly focus on the\nsensitivity of the class score with respect to the original or perturbed input,\nusually measured using actual or modified gradients. Some methods also use a\nmodel-agnostic approach to understanding the rationale behind every prediction.\nIn this paper, we argue and demonstrate that local geometry of the model\nparameter space relative to the input can also be beneficial for improved\npost-hoc explanations. To achieve this goal, we introduce an interpretability\nmethod called \"geometrically-guided integrated gradients\" that builds on top of\nthe gradient calculation along a linear path as traditionally used in\nintegrated gradient methods. However, instead of integrating gradient\ninformation, our method explores the model's dynamic behavior from multiple\nscaled versions of the input and captures the best possible attribution for\neach input. We demonstrate through extensive experiments that the proposed\napproach outperforms vanilla and integrated gradients in subjective and\nquantitative assessment. We also propose a \"model perturbation\" sanity check to\ncomplement the traditionally used \"model randomization\" test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mahfuzur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_N/0/1/0/all/0/1\">Noah Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1\">Sergey Plis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06829","description":"<p>Vision transformers (ViTs) are changing the landscape of object detection\napproaches. A natural usage of ViTs in detection is to replace the CNN-based\nbackbone with a transformer-based backbone, which is straightforward and\neffective, with the price of bringing considerable computation burden for\ninference. More subtle usage is the DETR family, which eliminates the need for\nmany hand-designed components in object detection but introduces a decoder\ndemanding an extra-long time to converge. As a result, transformer-based object\ndetection can not prevail in large-scale applications. To overcome these\nissues, we propose a novel decoder-free fully transformer-based (DFFT) object\ndetector, achieving high efficiency in both training and inference stages, for\nthe first time. We simplify objection detection into an encoder-only\nsingle-level anchor-based dense prediction problem by centering around two\nentry points: 1) Eliminate the training-inefficient decoder and leverage two\nstrong encoders to preserve the accuracy of single-level feature map\nprediction; 2) Explore low-level semantic features for the detection task with\nlimited computational resources. In particular, we design a novel lightweight\ndetection-oriented transformer backbone that efficiently captures low-level\nfeatures with rich semantics based on a well-conceived ablation study.\nExtensive experiments on the MS COCO benchmark demonstrate that DFFT_SMALL\noutperforms DETR by 2.5% AP with 28% computation cost reduction and more than\n$10$x fewer training epochs. Compared with the cutting-edge anchor-based\ndetector RetinaNet, DFFT_SMALL obtains over 5.5% AP gain while cutting down 70%\ncomputation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peixian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELUDE: Generating interpretable explanations via a decomposition into labelled and unlabelled features. (arXiv:2206.07690v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07690","description":"<p>Deep learning models have achieved remarkable success in different areas of\nmachine learning over the past decade; however, the size and complexity of\nthese models make them difficult to understand. In an effort to make them more\ninterpretable, several recent works focus on explaining parts of a deep neural\nnetwork through human-interpretable, semantic attributes. However, it may be\nimpossible to completely explain complex models using only semantic attributes.\nIn this work, we propose to augment these attributes with a small set of\nuninterpretable features. Specifically, we develop a novel explanation\nframework ELUDE (Explanation via Labelled and Unlabelled DEcomposition) that\ndecomposes a model's prediction into two parts: one that is explainable through\na linear combination of the semantic attributes, and another that is dependent\non the set of uninterpretable features. By identifying the latter, we are able\nto analyze the \"unexplained\" portion of the model, obtaining insights into the\ninformation used by the model. We show that the set of unlabelled features can\ngeneralize to multiple models trained with the same feature space and compare\nour work to two popular attribute-oriented methods, Interpretable Basis\nDecomposition and Concept Bottleneck, and discuss the additional insights ELUDE\nprovides.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_V/0/1/0/all/0/1\">Vikram V. Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunnie S. Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_N/0/1/0/all/0/1\">Nicole Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1\">Ruth Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids. (arXiv:2206.07695v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07695","description":"<p>State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to\nparameterize 3D radiance fields. While demonstrating impressive results,\nquerying an MLP for every sample along each ray leads to slow rendering.\nTherefore, existing approaches often render low-resolution feature maps and\nprocess them with an upsampling network to obtain the final image. Albeit\nefficient, neural rendering often entangles viewpoint and content such that\nchanging the camera pose results in unwanted changes of geometry or appearance.\nMotivated by recent results in voxel-based novel view synthesis, we investigate\nthe utility of sparse voxel grid representations for fast and 3D-consistent\ngenerative modeling in this paper. Our results demonstrate that monolithic MLPs\ncan indeed be replaced by 3D convolutions when combining sparse voxel grids\nwith progressive growing, free space pruning and appropriate regularization. To\nobtain a compact representation of the scene and allow for scaling to higher\nvoxel resolutions, our model disentangles the foreground object (modeled in 3D)\nfrom the background (modeled in 2D). In contrast to existing approaches, our\nmethod requires only a single forward pass to generate a full 3D scene. It\nhence allows for efficient rendering from arbitrary viewpoints while yielding\n3D consistent results with high visual fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_K/0/1/0/all/0/1\">Katja Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_A/0/1/0/all/0/1\">Axel Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niemeyer_M/0/1/0/all/0/1\">Michael Niemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos. (arXiv:2206.07981v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07981","description":"<p>Multimodal sentiment analysis in videos is a key task in many real-world\napplications, which usually requires integrating multimodal streams including\nvisual, verbal and acoustic behaviors. To improve the robustness of multimodal\nfusion, some of the existing methods let different modalities communicate with\neach other and modal the crossmodal interaction via transformers. However,\nthese methods only use the single-scale representations during the interaction\nbut forget to exploit multi-scale representations that contain different levels\nof semantic information. As a result, the representations learned by\ntransformers could be biased especially for unaligned multimodal data. In this\npaper, we propose a multi-scale cooperative multimodal transformer (MCMulT)\narchitecture for multimodal sentiment analysis. On the whole, the \"multi-scale\"\nmechanism is capable of exploiting the different levels of semantic information\nof each modality which are used for fine-grained crossmodal interactions.\nMeanwhile, each modality learns its feature hierarchies via integrating the\ncrossmodal interactions from multiple level features of its source modality. In\nthis way, each pair of modalities progressively builds feature hierarchies\nrespectively in a cooperative manner. The empirical results illustrate that our\nMCMulT model not only outperforms existing approaches on unaligned multimodal\nsequences but also has strong performance on aligned multimodal sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lianyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07990","description":"<p>Recent self-supervised learning (SSL) methods have shown impressive results\nin learning visual representations from unlabeled images. This paper aims to\nimprove their performance further by utilizing the architectural advantages of\nthe underlying neural network, as the current state-of-the-art visual pretext\ntasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.\nIn particular, we focus on Vision Transformers (ViTs), which have gained much\nattention recently as a better architectural choice, often outperforming\nconvolutional networks for various visual tasks. The unique characteristic of\nViT is that it takes a sequence of disjoint patches from an image and processes\npatch-level representations internally. Inspired by this, we design a simple\nyet effective visual pretext task, coined SelfPatch, for learning better\npatch-level representations. To be specific, we enforce invariance against each\npatch and its neighbors, i.e., each patch treats similar neighboring patches as\npositive samples. Consequently, training ViTs with SelfPatch learns more\nsemantically meaningful relations among patches (without using human-annotated\nlabels), which can be beneficial, in particular, to downstream tasks of a dense\nprediction type. Despite its simplicity, we demonstrate that it can\nsignificantly improve the performance of existing SSL methods for various\nvisual tasks, including object detection and semantic segmentation.\nSpecifically, SelfPatch significantly improves the recent self-supervised ViT,\nDINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance\nsegmentation, and +2.9 mIoU on ADE20K semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sukmin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hankook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}