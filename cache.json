{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Machine Learning Model Sizes and the Parameter Gap. (arXiv:2207.02852v1 [cs.LG])","link":"http://arxiv.org/abs/2207.02852","description":"<p>We study trends in model size of notable machine learning systems over time\nusing a curated dataset. From 1950 to 2018, model size in language models\nincreased steadily by seven orders of magnitude. The trend then accelerated,\nwith model size increasing by another five orders of magnitude in just 4 years\nfrom 2018 to 2022. Vision models grew at a more constant pace, totaling 7\norders of magnitude of growth between 1950 and 2022.\n</p>\n<p>We also identify that, since 2020, there have been many language models below\n20B parameters, many models above 70B parameters, but a scarcity of models in\nthe 20-70B parameter range. We refer to that scarcity as the parameter gap.\n</p>\n<p>We provide some stylized facts about the parameter gap and propose a few\nhypotheses to explain it. The explanations we favor are: (a) increasing model\nsize beyond 20B parameters requires adopting different parallelism techniques,\nwhich makes mid-sized models less cost-effective, (b) GPT-3 was one order of\nmagnitude larger than previous language models, and researchers afterwards\nprimarily experimented with bigger models to outperform it. While these\ndynamics likely exist, and we believe they play some role in generating the\ngap, we don't have high confidence that there are no other, more important\ndynamics at play.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villalobos_P/0/1/0/all/0/1\">Pablo Villalobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_J/0/1/0/all/0/1\">Jaime Sevilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besiroglu_T/0/1/0/all/0/1\">Tamay Besiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heim_L/0/1/0/all/0/1\">Lennart Heim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Anson Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1\">Marius Hobbhahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding. (arXiv:2207.02971v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02971","description":"<p>Conformer has proven to be effective in many speech processing tasks. It\ncombines the benefits of extracting local dependencies using convolutions and\nglobal dependencies using self-attention. Inspired by this, we propose a more\nflexible, interpretable and customizable encoder alternative, Branchformer,\nwith parallel branches for modeling various ranged dependencies in end-to-end\nspeech processing. In each encoder layer, one branch employs self-attention or\nits variant to capture long-range dependencies, while the other branch utilizes\nan MLP module with convolutional gating (cgMLP) to extract local relationships.\nWe conduct experiments on several speech recognition and spoken language\nunderstanding benchmarks. Results show that our model outperforms both\nTransformer and cgMLP. It also matches with or outperforms state-of-the-art\nresults achieved by Conformer. Furthermore, we show various strategies to\nreduce computation thanks to the two-branch architecture, including the ability\nto have variable inference complexity in a single trained model. The weights\nlearned for merging branches indicate how local and global dependencies are\nutilized in different layers, which benefits model designing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_I/0/1/0/all/0/1\">Ian Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling. (arXiv:2207.03030v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03030","description":"<p>This paper studies multi-task training of retrieval-augmented generation\nmodels for knowledge-intensive tasks. We propose to clean the training set by\nutilizing a distinct property of knowledge-intensive generation: The connection\nof query-answer pairs to items in the knowledge base. We filter training\nexamples via a threshold of confidence on the relevance labels, whether a pair\nis answerable by the knowledge base or not. We train a single Fusion-in-Decoder\n(FiD) generator on seven combined tasks of the KILT benchmark. The experimental\nresults suggest that our simple yet effective approach substantially improves\ncompetitive baselines on two strongly imbalanced tasks; and shows either\nsmaller improvements or no significant regression on the remaining tasks.\nFurthermore, we demonstrate our multi-task training with relevance label\nsampling scales well with increased model capacity and achieves\nstate-of-the-art results in five out of seven KILT tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiecao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1\">Karthik Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensitivity Analysis on Transferred Neural Architectures of BERT and GPT-2 for Financial Sentiment Analysis. (arXiv:2207.03037v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03037","description":"<p>The explosion in novel NLP word embedding and deep learning techniques has\ninduced significant endeavors into potential applications. One of these\ndirections is in the financial sector. Although there is a lot of work done in\nstate-of-the-art models like GPT and BERT, there are relatively few works on\nhow well these methods perform through fine-tuning after being pre-trained, as\nwell as info on how sensitive their parameters are. We investigate the\nperformance and sensitivity of transferred neural architectures from\npre-trained GPT-2 and BERT models. We test the fine-tuning performance based on\nfreezing transformer layers, batch size, and learning rate. We find the\nparameters of BERT are hypersensitive to stochasticity in fine-tuning and that\nGPT-2 is more stable in such practice. It is also clear that the earlier layers\nof GPT-2 and BERT contain essential word pattern information that should be\nmaintained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tracy Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1\">Andy Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruckmann_C/0/1/0/all/0/1\">Camille Bruckmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03038","description":"<p>This paper describes our champion solution for the CVPR2022 Generic Event\nBoundary Captioning (GEBC) competition. GEBC requires the captioning model to\nhave a comprehension of instantaneous status changes around the given video\nboundary, which makes it much more challenging than conventional video\ncaptioning task. In this paper, a Dual-Stream Transformer with improvements on\nboth video content encoding and captions generation is proposed: (1) We utilize\nthree pre-trained models to extract the video features from different\ngranularities. Moreover, we exploit the types of boundary as hints to help the\nmodel generate captions. (2) We particularly design an model, termed as\nDual-Stream Transformer, to learn discriminative representations for boundary\ncaptioning. (3) Towards generating content-relevant and human-like captions, we\nimprove the description quality by designing a word-level ensemble strategy.\nThe promising results on the GEBC test split demonstrate the efficacy of our\nproposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanhua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Few-Shot Image Classification Using Machine- and User-Generated Natural Language Descriptions. (arXiv:2207.03133v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03133","description":"<p>Humans can obtain the knowledge of novel visual concepts from language\ndescriptions, and we thus use the few-shot image classification task to\ninvestigate whether a machine learning model can have this capability. Our\nproposed model, LIDE (Learning from Image and DEscription), has a text decoder\nto generate the descriptions and a text encoder to obtain the text\nrepresentations of machine- or user-generated descriptions. We confirmed that\nLIDE with machine-generated descriptions outperformed baseline models.\nMoreover, the performance was improved further with high-quality user-generated\ndescriptions. The generated descriptions can be viewed as the explanations of\nthe model's predictions, and we observed that such explanations were consistent\nwith prediction results. We also investigated why the language description\nimproved the few-shot image classification performance by comparing the image\nrepresentations and the text representations in the feature spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kyosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishioka_S/0/1/0/all/0/1\">Shuichi Nishioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning and Multi-label Classification for Ellipsis and Coreference Detection in Conversational Question-Answering. (arXiv:2207.03145v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03145","description":"<p>In human conversations, ellipsis and coreference are commonly occurring\nlinguistic phenomena. Although these phenomena are a mean of making\nhuman-machine conversations more fluent and natural, only few dialogue corpora\ncontain explicit indications on which turns contain ellipses and/or\ncoreferences. In this paper we address the task of automatically detecting\nellipsis and coreferences in conversational question answering. We propose to\nuse a multi-label classifier based on DistilBERT. Multi-label classification\nand active learning are employed to compensate the limited amount of labeled\ndata. We show that these methods greatly enhance the performance of the\nclassifier for detecting these phenomena on a manually labeled dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brabant_Q/0/1/0/all/0/1\">Quentin Brabant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina Maria Rojas-Barahona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardent_C/0/1/0/all/0/1\">Claire Gardent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Speech-to-Punctuated-Text Recognition. (arXiv:2207.03169v1 [eess.AS])","link":"http://arxiv.org/abs/2207.03169","description":"<p>Conventional automatic speech recognition systems do not produce punctuation\nmarks which are important for the readability of the speech recognition\nresults. They are also needed for subsequent natural language processing tasks\nsuch as machine translation. There have been a lot of works on punctuation\nprediction models that insert punctuation marks into speech recognition results\nas post-processing. However, these studies do not utilize acoustic information\nfor punctuation prediction and are directly affected by speech recognition\nerrors. In this study, we propose an end-to-end model that takes speech as\ninput and outputs punctuated texts. This model is expected to predict\npunctuation robustly against speech recognition errors while using acoustic\ninformation. We also propose to incorporate an auxiliary loss to train the\nmodel using the output of the intermediate layer and unpunctuated texts.\nThrough experiments, we compare the performance of the proposed model to that\nof a cascaded system. The proposed model achieves higher punctuation prediction\naccuracy than the cascaded system without sacrificing the speech recognition\nerror rate. It is also demonstrated that the multi-task learning using the\nintermediate output against the unpunctuated text is effective. Moreover, the\nproposed model has only about 1/7th of the parameters compared to the cascaded\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nozaki_J/0/1/0/all/0/1\">Jumon Nozaki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ishizuka_K/0/1/0/all/0/1\">Kenkichi Ishizuka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_T/0/1/0/all/0/1\">Taiichi Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoQAR: Question Rewriting on CoQA. (arXiv:2207.03240v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03240","description":"<p>Questions asked by humans during a conversation often contain contextual\ndependencies, i.e., explicit or implicit references to previous dialogue turns.\nThese dependencies take the form of coreferences (e.g., via pronoun use) or\nellipses, and can make the understanding difficult for automated systems. One\nway to facilitate the understanding and subsequent treatments of a question is\nto rewrite it into an out-of-context form, i.e., a form that can be understood\nwithout the conversational context. We propose CoQAR, a corpus containing\n$4.5$K conversations from the Conversational Question-Answering dataset CoQA,\nfor a total of $53$K follow-up question-answer pairs. Each original question\nwas manually annotated with at least 2 at most 3 out-of-context rewritings.\nCoQAR can be used in the supervised learning of three tasks: question\nparaphrasing, question rewriting and conversational question answering. In\norder to assess the quality of CoQAR's rewritings, we conduct several\nexperiments consisting in training and evaluating models for these three tasks.\nOur results support the idea that question rewriting can be used as a\npreprocessing step for question answering models, thereby increasing their\nperformances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brabant_Q/0/1/0/all/0/1\">Quentin Brabant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecorve_G/0/1/0/all/0/1\">Gwenole Lecorve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina M. Rojas-Barahona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Part-of-Speech Tagging of Odia Language Using statistical and Deep Learning-Based Approaches. (arXiv:2207.03256v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03256","description":"<p>Automatic Part-of-speech (POS) tagging is a preprocessing step of many\nnatural language processing (NLP) tasks such as name entity recognition (NER),\nspeech processing, information extraction, word sense disambiguation, and\nmachine translation. It has already gained a promising result in English and\nEuropean languages, but in Indian languages, particularly in Odia language, it\nis not yet well explored because of the lack of supporting tools, resources,\nand morphological richness of language. Unfortunately, we were unable to locate\nan open source POS tagger for Odia, and only a handful of attempts have been\nmade to develop POS taggers for Odia language. The main contribution of this\nresearch work is to present a conditional random field (CRF) and deep\nlearning-based approaches (CNN and Bidirectional Long Short-Term Memory) to\ndevelop Odia part-of-speech tagger. We used a publicly accessible corpus and\nthe dataset is annotated with the Bureau of Indian Standards (BIS) tagset.\nHowever, most of the languages around the globe have used the dataset annotated\nwith Universal Dependencies (UD) tagset. Hence, to maintain uniformity Odia\ndataset should use the same tagset. So we have constructed a simple mapping\nfrom BIS tagset to UD tagset. We experimented with various feature set inputs\nto the CRF model, observed the impact of constructed feature set. The deep\nlearning-based model includes Bi-LSTM network, CNN network, CRF layer,\ncharacter sequence information, and pre-trained word vector. Character sequence\ninformation was extracted by using convolutional neural network (CNN) and\nBi-LSTM network. Six different combinations of neural sequence labelling models\nare implemented, and their performance measures are investigated. It has been\nobserved that Bi-LSTM model with character sequence feature and pre-trained\nword vector achieved a significant state-of-the-art result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalai_T/0/1/0/all/0/1\">Tusarkanta Dalai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1\">Tapas Kumar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_P/0/1/0/all/0/1\">Pankaj K Sa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity Recognition. (arXiv:2207.03300v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03300","description":"<p>For Named Entity Recognition (NER), sequence labeling-based and span-based\nparadigms are quite different. Previous research has demonstrated that the two\nparadigms have clear complementary advantages, but few models have attempted to\nleverage these advantages in a single NER model as far as we know. In our\nprevious work, we proposed a paradigm known as Bundling Learning (BL) to\naddress the above problem. The BL paradigm bundles the two NER paradigms,\nenabling NER models to jointly tune their parameters by weighted summing each\nparadigm's training loss. However, three critical issues remain unresolved:\nWhen does BL work? Why does BL work? Can BL enhance the existing\nstate-of-the-art (SOTA) NER models? To address the first two issues, we\nimplement three NER models, involving a sequence labeling-based model--SeqNER,\na span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER\ntogether. We draw two conclusions regarding the two issues based on the\nexperimental results on eleven NER datasets from five domains. We then apply BL\nto five existing SOTA NER models to investigate the third issue, consisting of\nthree sequence labeling-based models and two span-based models. Experimental\nresults indicate that BL consistently enhances their performance, suggesting\nthat it is possible to construct a new SOTA NER system by incorporating BL into\nthe current SOTA system. Moreover, we find that BL reduces both entity boundary\nand type prediction errors. In addition, we compare two commonly used labeling\ntagging methods as well as three types of span semantic representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huijun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation. (arXiv:2207.03334v1 [eess.AS])","link":"http://arxiv.org/abs/2207.03334","description":"<p>Estimating dimensional emotions, such as activation, valence and dominance,\nfrom acoustic speech signals has been widely explored over the past few years.\nWhile accurate estimation of activation and dominance from speech seem to be\npossible, the same for valence remains challenging. Previous research has shown\nthat the use of lexical information can improve valence estimation performance.\nLexical information can be obtained from pre-trained acoustic models, where the\nlearned representations can improve valence estimation from speech. We\ninvestigate the use of pre-trained model representations to improve valence\nestimation from acoustic speech signal. We also explore fusion of\nrepresentations to improve emotion estimation across all three emotion\ndimensions: activation, valence and dominance. Additionally, we investigate if\nrepresentations from pre-trained models can be distilled into models trained\nwith low-level features, resulting in models with a less number of parameters.\nWe show that fusion of pre-trained model embeddings result in a 79% relative\nimprovement in concordance correlation coefficient CCC on valence estimation\ncompared to standard acoustic feature baseline (mel-filterbank energies), while\ndistillation from pre-trained model embeddings to lower-dimensional\nrepresentations yielded a relative 12% improvement. Such performance gains were\nobserved over two evaluation sets, indicating that our proposed architecture\ngeneralizes across those evaluation sets. We report new state-of-the-art\n\"text-free\" acoustic-only dimensional emotion estimation $CCC$ values on two\nMSP-Podcast evaluation sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chien_H/0/1/0/all/0/1\">Hsiang-Yun Sherry Chien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kowtha_V/0/1/0/all/0/1\">Vasudha Kowtha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Joseph Yitan Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azemi_E/0/1/0/all/0/1\">Erdrin Azemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Language Models are not Born Equal to Fit Brain Data, but Training Helps. (arXiv:2207.03380v1 [cs.AI])","link":"http://arxiv.org/abs/2207.03380","description":"<p>Neural Language Models (NLMs) have made tremendous advances during the last\nyears, achieving impressive performance on various linguistic tasks.\nCapitalizing on this, studies in neuroscience have started to use NLMs to study\nneural activity in the human brain during language processing. However, many\nquestions remain unanswered regarding which factors determine the ability of a\nneural language model to capture brain activity (aka its 'brain score'). Here,\nwe make first steps in this direction and examine the impact of test loss,\ntraining corpus and model architecture (comparing GloVe, LSTM, GPT-2 and BERT),\non the prediction of functional Magnetic Resonance Imaging timecourses of\nparticipants listening to an audiobook. We find that (1) untrained versions of\neach model already explain significant amount of signal in the brain by\ncapturing similarity in brain responses across identical words, with the\nuntrained LSTM outperforming the transformerbased models, being less impacted\nby the effect of context; (2) that training NLP models improves brain scores in\nthe same brain regions irrespective of the model's architecture; (3) that\nPerplexity (test loss) is not a good predictor of brain score; (4) that\ntraining data have a strong influence on the outcome and, notably, that\noff-the-shelf models may lack statistical power to detect brain activations.\nOverall, we outline the impact of modeltraining choices, and suggest good\npractices for future studies aiming at explaining the human language system\nusing neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasquiou_A/0/1/0/all/0/1\">Alexandre Pasquiou</a> (PARIETAL, UNICOG-U992), <a href=\"http://arxiv.org/find/cs/1/au:+Lakretz_Y/0/1/0/all/0/1\">Yair Lakretz</a> (UNICOG-U992), <a href=\"http://arxiv.org/find/cs/1/au:+Hale_J/0/1/0/all/0/1\">John Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1\">Bertrand Thirion</a> (PARIETAL), <a href=\"http://arxiv.org/find/cs/1/au:+Pallier_C/0/1/0/all/0/1\">Christophe Pallier</a> (UNICOG-U992)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Impact of Cross-lingual Acoustic-Phonetic Similarities on Multilingual Speech Recognition. (arXiv:2207.03390v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03390","description":"<p>Multilingual automatic speech recognition (ASR) systems mostly benefit low\nresource languages but suffer degradation in performance across several\nlanguages relative to their monolingual counterparts. Limited studies have\nfocused on understanding the languages behaviour in the multilingual speech\nrecognition setups. In this paper, a novel data-driven approach is proposed to\ninvestigate the cross-lingual acoustic-phonetic similarities. This technique\nmeasures the similarities between posterior distributions from various\nmonolingual acoustic models against a target speech signal. Deep neural\nnetworks are trained as mapping networks to transform the distributions from\ndifferent acoustic models into a directly comparable form. The analysis\nobserves that the languages closeness can not be truly estimated by the volume\nof overlapping phonemes set. Entropy analysis of the proposed mapping networks\nexhibits that a language with lesser overlap can be more amenable to\ncross-lingual transfer, and hence more beneficial in the multilingual setup.\nFinally, the proposed posterior transformation approach is leveraged to fuse\nmonolingual models for a target language. A relative improvement of ~8% over\nmonolingual counterpart is achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Muhammad Umar Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion. (arXiv:2207.03391v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03391","description":"<p>Multilingual speech recognition has drawn significant attention as an\neffective way to compensate data scarcity for low-resource languages.\nEnd-to-end (e2e) modelling is preferred over conventional hybrid systems,\nmainly because of no lexicon requirement. However, hybrid DNN-HMMs still\noutperform e2e models in limited data scenarios. Furthermore, the problem of\nmanual lexicon creation has been alleviated by publicly available trained\nmodels of grapheme-to-phoneme (G2P) and text to IPA transliteration for a lot\nof languages. In this paper, a novel approach of hybrid DNN-HMM acoustic models\nfusion is proposed in a multilingual setup for the low-resource languages.\nPosterior distributions from different monolingual acoustic models, against a\ntarget language speech signal, are fused together. A separate regression neural\nnetwork is trained for each source-target language pair to transform posteriors\nfrom source acoustic model to the target language. These networks require very\nlimited data as compared to the ASR training. Posterior fusion yields a\nrelative gain of 14.65% and 6.5% when compared with multilingual and\nmonolingual baselines respectively. Cross-lingual model fusion shows that the\ncomparable results can be achieved without using posteriors from the language\ndependent ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Muhammad Umar Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayana_D/0/1/0/all/0/1\">Darshan Adiga Haniya Narayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Modeling of Language-Evoked Event-Related Potentials. (arXiv:2207.03392v1 [q-bio.QM])","link":"http://arxiv.org/abs/2207.03392","description":"<p>Bayesian hierarchical models are well-suited to analyzing the often noisy\ndata from electroencephalography experiments in cognitive neuroscience: these\nmodels provide an intuitive framework to account for structures and\ncorrelations in the data, and they allow a straightforward handling of\nuncertainty. In a typical neurolinguistic experiment, event-related potentials\nshow only very small effect sizes and frequentist approaches to data analysis\nfail to establish the significance of some of these effects. Here, we present a\nBayesian approach to analyzing event-related potentials using as an example\ndata from an experiment which relates word surprisal and neural response. Our\nmodel is able to estimate the effect of word surprisal on most components of\nthe event-related potential and provides a richer description of the data. The\nBayesian framework also allows easier comparison between estimates based on\nsurprisal values calculated using different language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Turco_D/0/1/0/all/0/1\">Davide Turco</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Houghton_C/0/1/0/all/0/1\">Conor Houghton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AsNER -- Annotated Dataset and Baseline for Assamese Named Entity recognition. (arXiv:2207.03422v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03422","description":"<p>We present the AsNER, a named entity annotation dataset for low resource\nAssamese language with a baseline Assamese NER model. The dataset contains\nabout 99k tokens comprised of text from the speech of the Prime Minister of\nIndia and Assamese play. It also contains person names, location names and\naddresses. The proposed NER dataset is likely to be a significant resource for\ndeep neural based Assamese language processing. We benchmark the dataset by\ntraining NER models and evaluating using state-of-the-art architectures for\nsupervised named entity recognition (NER) such as Fasttext, BERT, XLM-R, FLAIR,\nMuRIL etc. We implement several baseline approaches with state-of-the-art\nsequence tagging Bi-LSTM-CRF architecture. The highest F1-score among all\nbaselines achieves an accuracy of 80.69% when using MuRIL as a word embedding\nmethod. The annotated dataset and the top performing model are made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Dhrubajyoti Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1\">Sukumar Nandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarmah_P/0/1/0/all/0/1\">Priyankoo Sarmah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web. (arXiv:2207.03477v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03477","description":"<p>The DarkWeb represents a hotbed for illicit activity, where users communicate\non different market forums in order to exchange goods and services. Law\nenforcement agencies benefit from forensic tools that perform authorship\nanalysis, in order to identify and profile users based on their textual\ncontent. However, authorship analysis has been traditionally studied using\ncorpora featuring literary texts such as fragments from novels or fan fiction,\nwhich may not be suitable in a cybercrime context. Moreover, the few works that\nemploy authorship analysis tools for cybercrime prevention usually employ\nad-hoc experimental setups and datasets. To address these issues, we release\nVeriDark: a benchmark comprised of three large scale authorship verification\ndatasets and one authorship identification dataset obtained from user activity\nfrom either Dark Web related Reddit communities or popular illicit Dark Web\nmarket forums. We evaluate competitive NLP baselines on the three datasets and\nperform an analysis of the predictions to better understand the limitations of\nsuch approaches. We make the datasets and baselines publicly available at\nhttps://github.com/bit-ml/VeriDark\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manolache_A/0/1/0/all/0/1\">Andrei Manolache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brad_F/0/1/0/all/0/1\">Florin Brad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbalau_A/0/1/0/all/0/1\">Antonio Barbalau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adposition and Case Supersenses v2.6: Guidelines for English. (arXiv:1704.02134v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1704.02134","description":"<p>This document offers a detailed linguistic description of SNACS (Semantic\nNetwork of Adposition and Case Supersenses; Schneider et al., 2018), an\ninventory of 52 semantic labels (\"supersenses\") that characterize the use of\nadpositions and case markers at a somewhat coarse level of granularity, as\ndemonstrated in the STREUSLE corpus (https://github.com/nert-nlp/streusle/ ;\nversion 4.5 tracks guidelines version 2.6). Though the SNACS inventory aspires\nto be universal, this document is specific to English; documentation for other\nlanguages will be published separately.\n</p>\n<p>Version 2 is a revision of the supersense inventory proposed for English by\nSchneider et al. (2015, 2016) (henceforth \"v1\"), which in turn was based on\nprevious schemes. The present inventory was developed after extensive review of\nthe v1 corpus annotations for English, plus previously unanalyzed genitive case\npossessives (Blodgett and Schneider, 2018), as well as consideration of\nadposition and case phenomena in Hebrew, Hindi, Korean, and German. Hwang et\nal. (2017) present the theoretical underpinnings of the v2 scheme. Schneider et\nal. (2018) summarize the scheme, its application to English corpus data, and an\nautomatic disambiguation task. Liu et al. (2021) offer an English Lexical\nSemantic Recognition tagger that includes SNACS labels in its output.\n</p>\n<p>This documentation can also be browsed alongside corpus data on the Xposition\nwebsite (Gessler et al., 2022): <a href=\"http://www.xposition.org/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Archna Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_N/0/1/0/all/0/1\">Na-Rae Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_S/0/1/0/all/0/1\">Sarah R. Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_A/0/1/0/all/0/1\">Adi Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_A/0/1/0/all/0/1\">Austin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prange_J/0/1/0/all/0/1\">Jakob Prange</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for Enabling Hardware-agnostic Programming with True Performance Portability for Heterogeneous HPC. (arXiv:2011.10896v5 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2011.10896","description":"<p>This paper presents HALO 1.0, an open-ended extensible multi-agent software\nframework that implements a set of proposed hardware-agnostic accelerator\norchestration (HALO) principles. HALO implements a novel compute-centric\nmessage passing interface (C^2MPI) specification for enabling the performance\nportable execution of a hardware-agnostic host application across heterogeneous\naccelerators. The experiment results of evaluating eight widely used HPC\nsubroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs, and\nNVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control\nflow for host programs to run across all the computing devices with a\nconsistently top performance portability score, which is up to five orders of\nmagnitude higher than the OpenCL-based solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riera_M/0/1/0/all/0/1\">Michael Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_E/0/1/0/all/0/1\">Erfan Bank Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quraishi_M/0/1/0/all/0/1\">Masudul Hassan Quraishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Fengbo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence tagging for biomedical extractive question answering. (arXiv:2104.07535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07535","description":"<p>Current studies in extractive question answering (EQA) have modeled the\nsingle-span extraction setting, where a single answer span is a label to\npredict for a given question-passage pair. This setting is natural for general\ndomain EQA as the majority of the questions in the general domain can be\nanswered with a single span. Following general domain EQA models, current\nbiomedical EQA (BioEQA) models utilize the single-span extraction setting with\npost-processing steps. In this article, we investigate the question\ndistribution across the general and biomedical domains and discover biomedical\nquestions are more likely to require list-type answers (multiple answers) than\nfactoid-type answers (single answer). This necessitates the models capable of\nproducing multiple answers for a question. Based on this preliminary study, we\npropose a sequence tagging approach for BioEQA, which is a multi-span\nextraction setting. Our approach directly tackles questions with a variable\nnumber of phrases as their answer and can learn to decide the number of answers\nfor a question from training data. Our experimental results on the BioASQ 7b\nand 8b list-type questions outperformed the best-performing existing models\nwithout requiring post-processing steps. Source codes and resources are freely\navailable for download at https://github.com/dmis-lab/SeqTagQA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_W/0/1/0/all/0/1\">Wonjin Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_R/0/1/0/all/0/1\">Richard Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagerberg_A/0/1/0/all/0/1\">Aron Lagerberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning grammar with a divide-and-concur neural network. (arXiv:2201.07341v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07341","description":"<p>We implement a divide-and-concur iterative projection approach to\ncontext-free grammar inference. Unlike most state-of-the-art models of natural\nlanguage processing, our method requires a relatively small number of discrete\nparameters, making the inferred grammar directly interpretable -- one can read\noff from a solution how to construct grammatically valid sentences. Another\nadvantage of our approach is the ability to infer meaningful grammatical rules\nfrom just a few sentences, compared to the hundreds of gigabytes of training\ndata many other models employ. We demonstrate several ways of applying our\napproach: classifying words and inferring a grammar from scratch, taking an\nexisting grammar and refining its categories and rules, and taking an existing\ngrammar and expanding its lexicon as it encounters new words in new data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deyo_S/0/1/0/all/0/1\">Sean Deyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elser_V/0/1/0/all/0/1\">Veit Elser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resources for Turkish Natural Language Processing: A critical survey. (arXiv:2204.05042v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05042","description":"<p>This paper presents a comprehensive survey of corpora and lexical resources\navailable for Turkish. We review a broad range of resources, focusing on the\nones that are publicly available. In addition to providing information about\nthe available linguistic resources, we present a set of recommendations, and\nidentify gaps in the data available for conducting research and building\napplications in Turkish Linguistics and Natural Language Processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coltekin_C/0/1/0/all/0/1\">&#xc7;a&#x11f;r&#x131; &#xc7;&#xf6;ltekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A. Seza Do&#x11f;ru&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetinoglu_O/0/1/0/all/0/1\">&#xd6;zlem &#xc7;etino&#x11f;lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. (arXiv:2205.00001v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.00001","description":"<p>Having a rich multimodal inner language is an important component of human\nintelligence that enables several necessary core cognitive functions such as\nmultimodal prediction, translation, and generation. Building upon the Conscious\nTuring Machine (CTM), a machine model for consciousness proposed by Blum and\nBlum (2021), we describe the desiderata of a multimodal language called\nBrainish, comprising words, images, audio, and sensations combined in\nrepresentations that the CTM's processors use to communicate with each other.\nWe define the syntax and semantics of Brainish before operationalizing this\nlanguage through the lens of multimodal artificial intelligence, a vibrant\nresearch area studying the computational tools necessary for processing and\nrelating information from heterogeneous signals. Our general framework for\nlearning Brainish involves designing (1) unimodal encoders to segment and\nrepresent unimodal data, (2) a coordinated representation space that relates\nand composes unimodal features to derive holistic meaning across multimodal\ninputs, and (3) decoders to map multimodal representations into predictions\n(for fusion) or raw data (for translation or generation). Through discussing\nhow Brainish is crucial for communication and coordination in order to achieve\nconsciousness in the CTM, and by implementing a simple version of Brainish and\nevaluating its capability of demonstrating intelligence on multimodal\nprediction and retrieval tasks on several real-world image, text, and audio\ndatasets, we argue that such an inner language will be important for advances\nin machine models of intelligence and consciousness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paragraph-based Transformer Pre-training for Multi-Sentence Inference. (arXiv:2205.01228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01228","description":"<p>Inference tasks such as answer sentence selection (AS2) or fact verification\nare typically solved by fine-tuning transformer-based models as individual\nsentence-pair classifiers. Recent studies show that these tasks benefit from\nmodeling dependencies across multiple candidate sentences jointly. In this\npaper, we first show that popular pre-trained transformers perform poorly when\nused for fine-tuning on multi-candidate inference tasks. We then propose a new\npre-training objective that models the paragraph-level semantics across\nmultiple input sentences. Our evaluation on three AS2 and one fact verification\ndatasets demonstrates the superiority of our pre-training technique over the\ntraditional ones for transformers used as joint models for multi-candidate\ninference tasks, as well as when used as cross-encoders for sentence-pair\nformulations of these tasks. Our code and pre-trained models are released at\nhttps://github.com/amazon-research/wqa-multi-sentence-inference .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liello_L/0/1/0/all/0/1\">Luca Di Liello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Machine Translation Systems for the Next Thousand Languages. (arXiv:2205.03983v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03983","description":"<p>In this paper we share findings from our effort to build practical machine\ntranslation (MT) systems capable of translating across over one thousand\nlanguages. We describe results in three research domains: (i) Building clean,\nweb-mined datasets for 1500+ languages by leveraging semi-supervised\npre-training for language identification and developing data-driven filtering\ntechniques; (ii) Developing practical MT models for under-served languages by\nleveraging massively multilingual models trained with supervised parallel data\nfor over 100 high-resource languages and monolingual datasets for an additional\n1000+ languages; and (iii) Studying the limitations of evaluation metrics for\nthese languages and conducting qualitative analysis of the outputs from our MT\nmodels, highlighting several frequent error modes of these types of models. We\nhope that our work provides useful insights to practitioners working towards\nbuilding MT systems for currently understudied languages, and highlights\nresearch directions that can complement the weaknesses of massively\nmultilingual models in data-sparse settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esch_D/0/1/0/all/0/1\">Daan van Esch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Mengmeng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baljekar_P/0/1/0/all/0/1\">Pallavi Baljekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1\">Wolfgang Macherey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breiner_T/0/1/0/all/0/1\">Theresa Breiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mia Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_K/0/1/0/all/0/1\">Klaus Macherey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Apurva Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Macduff Hughes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Analysis of Negative Sampling in Knowledge Graph Representation Learning. (arXiv:2206.10140v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.10140","description":"<p>Negative sampling (NS) loss plays an important role in learning knowledge\ngraph embedding (KGE) to handle a huge number of entities. However, the\nperformance of KGE degrades without hyperparameters such as the margin term and\nnumber of negative samples in NS loss being appropriately selected. Currently,\nempirical hyperparameter tuning addresses this problem at the cost of\ncomputational time. To solve this problem, we theoretically analyzed NS loss to\nassist hyperparameter tuning and understand the better use of the NS loss in\nKGE learning. Our theoretical analysis showed that scoring methods with\nrestricted value ranges, such as TransE and RotatE, require appropriate\nadjustment of the margin term or the number of negative samples different from\nthose without restricted value ranges, such as RESCAL, ComplEx, and DistMult.\nWe also propose subsampling methods specialized for the NS loss in KGE studied\nfrom a theoretical aspect. Our empirical analysis on the FB15k-237, WN18RR, and\nYAGO3-10 datasets showed that the results of actually trained models agree with\nour theoretical findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyword Extraction in Scientific Documents. (arXiv:2207.01888v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01888","description":"<p>The scientific publication output grows exponentially. Therefore, it is\nincreasingly challenging to keep track of trends and changes. Understanding\nscientific documents is an important step in downstream tasks such as knowledge\ngraph building, text mining, and discipline classification. In this workshop,\nwe provide a better understanding of keyword and keyphrase extraction from the\nabstract of scientific publications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Susie Xi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piriyatamwong_P/0/1/0/all/0/1\">Piriyakorn Piriyatamwong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghoshal_P/0/1/0/all/0/1\">Parijat Ghoshal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasirian_S/0/1/0/all/0/1\">Sara Nasirian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salis_E/0/1/0/all/0/1\">Emmanuel de Salis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1\">Sandra Mitrovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wechner_M/0/1/0/all/0/1\">Michael Wechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brucker_V/0/1/0/all/0/1\">Vanya Brucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_P/0/1/0/all/0/1\">Peter Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa. (arXiv:2207.02424v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.02424","description":"<p>Text sentiment analysis, also known as opinion mining, is research on the\ncalculation of people's views, evaluations, attitude and emotions expressed by\nentities. Text sentiment analysis can be divided into text-level sentiment\nanalysis, sen-tence-level sentiment analysis and aspect-level sentiment\nanalysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the\nfield of sentiment analysis, which aims to predict the polarity of aspects. The\nresearch of pre-training neural model has significantly improved the\nperformance of many natural language processing tasks. In recent years, pre\ntraining model (PTM) has been applied in ABSA. Therefore, there has been a\nquestion, which is whether PTMs contain sufficient syntactic information for\nABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced\nBERT with disentangled attention) to solve Aspect-Based Sentiment Analysis\nproblem. DeBERTa is a kind of neural language model based on transformer, which\nuses self-supervised learning to pre-train on a large number of original text\ncorpora. Based on the Local Context Focus (LCF) mechanism, by integrating\nDeBERTa model, we purpose a multi-task learning model for aspect-based\nsentiment analysis. The experiments result on the most commonly used the laptop\nand restaurant datasets of SemEval-2014 and the ACL twitter dataset show that\nLCF mechanism with DeBERTa has significant improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Junping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhe Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zeli Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Efficient fine-grained road segmentation using superpixel-based CNN and CRF models. (arXiv:2207.02844v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02844","description":"<p>Towards a safe and comfortable driving, road scene segmentation is a\nrudimentary problem in camera-based advance driver assistance systems (ADAS).\nDespite of the great achievement of Convolutional Neural Networks (CNN) for\nsemantic segmentation task, the high computational efforts of CNN based methods\nis still a challenging area. In recent work, we proposed a novel approach to\nutilise the advantages of CNNs for the task of road segmentation at reasonable\ncomputational effort. The runtime benefits from using irregular super pixels as\nbasis for the input for the CNN rather than the image grid, which tremendously\nreduces the input size. Although, this method achieved remarkable low\ncomputational time in both training and testing phases, the lower resolution of\nthe super pixel domain yields naturally lower accuracy compared to high cost\nstate of the art methods. In this work, we focus on a refinement of the road\nsegmentation utilising a Conditional Random Field (CRF).The refinement\nprocedure is limited to the super pixels touching the predicted road boundary\nto keep the additional computational effort low. Reducing the input to the\nsuper pixel domain allows the CNNs structure to stay small and efficient to\ncompute while keeping the advantage of convolutional layers and makes them\neligible for ADAS. Applying CRF compensate the trade off between accuracy and\ncomputational efficiency. The proposed system obtained comparable performance\namong the top performing algorithms on the KITTI road benchmark and its fast\ninference makes it particularly suitable for realtime applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zohourian_F/0/1/0/all/0/1\">Farnoush Zohourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegemund_J/0/1/0/all/0/1\">Jan Siegemund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuter_M/0/1/0/all/0/1\">Mirko Meuter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauli_J/0/1/0/all/0/1\">Josef Pauli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perfusion imaging in deep prostate cancer detection from mp-MRI: can we take advantage of it?. (arXiv:2207.02854v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02854","description":"<p>To our knowledge, all deep computer-aided detection and diagnosis (CAD)\nsystems for prostate cancer (PCa) detection consider bi-parametric magnetic\nresonance imaging (bp-MRI) only, including T2w and ADC sequences while\nexcluding the 4D perfusion sequence,which is however part of standard clinical\nprotocols for this diagnostic task. In this paper, we question strategies to\nintegrate information from perfusion imaging in deep neural architectures. To\ndo so, we evaluate several ways to encode the perfusion information in a U-Net\nlike architecture, also considering early versus mid fusion strategies. We\ncompare performance of multiparametric MRI (mp-MRI) models with the baseline\nbp-MRI model based on a private dataset of 219 mp-MRI exams. Perfusion maps\nderived from dynamic contrast enhanced MR exams are shown to positively impact\nsegmentation and grading performance of PCa lesions, especially the 3D MR\nvolume corresponding to the maximum slope of the wash-in curve as well as Tmax\nperfusion maps. The latter mp-MRI models indeed outperform the bp-MRI one\nwhatever the fusion strategy, with Cohen's kappa score of 0.318$\\pm$0.019 for\nthe bp-MRI model and 0.378 $\\pm$ 0.033 for the model including the maximum\nslope with a mid fusion strategy, also achieving competitive Cohen's kappa\nscore compared to state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duran_A/0/1/0/all/0/1\">Audrey Duran</a> (MYRIAD), <a href=\"http://arxiv.org/find/eess/1/au:+Dussert_G/0/1/0/all/0/1\">Gaspard Dussert</a> (MYRIAD), <a href=\"http://arxiv.org/find/eess/1/au:+Lartizien_C/0/1/0/all/0/1\">Carole Lartizien</a> (MYRIAD)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Interaction and Manipulation of the Environment using Aerial Robots. (arXiv:2207.02856v1 [cs.RO])","link":"http://arxiv.org/abs/2207.02856","description":"<p>The physical interaction of aerial robots with their environment has\ncountless potential applications and is an emerging area with many open\nchallenges. Fully-actuated multirotors have been introduced to tackle some of\nthese challenges. They provide complete control over position and orientation\nand eliminate the need for attaching a multi-DoF manipulation arm to the robot.\nHowever, there are many open problems before they can be used in real-world\napplications. Researchers have introduced some methods for physical interaction\nin limited settings. Their experiments primarily use prototype-level software\nwithout an efficient path to integration with real-world applications. We\ndescribe a new cost-effective solution for integrating these robots with the\nexisting software and hardware flight systems for real-world applications and\nexpand it to physical interaction applications. On the other hand, the existing\ncontrol approaches for fully-actuated robots assume conservative limits for the\nthrusts and moments available to the robot. Using conservative assumptions for\nthese already-inefficient robots makes their interactions even less optimal and\nmay even result in many feasible physical interaction applications becoming\ninfeasible. This work proposes a real-time method for estimating the complete\nset of instantaneously available forces and moments that robots can use to\noptimize their physical interaction performance. Finally, many real-world\napplications where aerial robots can improve the existing manual solutions deal\nwith deformable objects. However, the perception and planning for their\nmanipulation is still challenging. This research explores how aerial physical\ninteraction can be extended to deformable objects. It provides a detection\nmethod suitable for manipulating deformable one-dimensional objects and\nintroduces a new perspective on planning the manipulation of these objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keipour_A/0/1/0/all/0/1\">Azarakhsh Keipour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Humans Social Relationship Classification during Accompaniment. (arXiv:2207.02890v1 [cs.LG])","link":"http://arxiv.org/abs/2207.02890","description":"<p>This paper presents the design of deep learning architectures which allow to\nclassify the social relationship existing between two people who are walking in\na side-by-side formation into four possible categories --colleagues, couple,\nfamily or friendship. The models are developed using Neural Networks or\nRecurrent Neural Networks to achieve the classification and are trained and\nevaluated using a database of readings obtained from humans performing an\naccompaniment process in an urban environment. The best achieved model\naccomplishes a relatively good accuracy in the classification problem and its\nresults enhance partially the outcomes from a previous study [1]. Furthermore,\nthe model proposed shows its future potential to improve its efficiency and to\nbe implemented in a real robot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castro_O/0/1/0/all/0/1\">Oscar Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repiso_E/0/1/0/all/0/1\">Ely Repiso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrell_A/0/1/0/all/0/1\">Anais Garrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanfeliu_A/0/1/0/all/0/1\">Alberto Sanfeliu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Transparency in Dermatology Image Datasets with Skin Tone Annotations by Experts, Crowds, and an Algorithm. (arXiv:2207.02942v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02942","description":"<p>While artificial intelligence (AI) holds promise for supporting healthcare\nproviders and improving the accuracy of medical diagnoses, a lack of\ntransparency in the composition of datasets exposes AI models to the\npossibility of unintentional and avoidable mistakes. In particular, public and\nprivate image datasets of dermatological conditions rarely include information\non skin color. As a start towards increasing transparency, AI researchers have\nappropriated the use of the Fitzpatrick skin type (FST) from a measure of\npatient photosensitivity to a measure for estimating skin tone in algorithmic\naudits of computer vision applications including facial recognition and\ndermatology diagnosis. In order to understand the variability of estimated FST\nannotations on images, we compare several FST annotation methods on a diverse\nset of 460 images of skin conditions from both textbooks and online dermatology\natlases. We find the inter-rater reliability between three board-certified\ndermatologists is comparable to the inter-rater reliability between the\nboard-certified dermatologists and two crowdsourcing methods. In contrast, we\nfind that the Individual Typology Angle converted to FST (ITA-FST) method\nproduces annotations that are significantly less correlated with the experts'\nannotations than the experts' annotations are correlated with each other. These\nresults demonstrate that algorithms based on ITA-FST are not reliable for\nannotating large-scale image datasets, but human-centered, crowd-based\nprotocols can reliably add skin type transparency to dermatology datasets.\nFurthermore, we introduce the concept of dynamic consensus protocols with\ntunable parameters including expert review that increase the visibility of\ncrowdwork and provide guidance for future crowdsourced annotations of large\nimage datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1\">Matthew Groh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_C/0/1/0/all/0/1\">Caleb Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1\">Roxana Daneshjou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badri_O/0/1/0/all/0/1\">Omar Badri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koochek_A/0/1/0/all/0/1\">Arash Koochek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual staining of defocused autofluorescence images of unlabeled tissue using deep neural networks. (arXiv:2207.02946v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02946","description":"<p>Deep learning-based virtual staining was developed to introduce image\ncontrast to label-free tissue sections, digitally matching the histological\nstaining, which is time-consuming, labor-intensive, and destructive to tissue.\nStandard virtual staining requires high autofocusing precision during the whole\nslide imaging of label-free tissue, which consumes a significant portion of the\ntotal imaging time and can lead to tissue photodamage. Here, we introduce a\nfast virtual staining framework that can stain defocused autofluorescence\nimages of unlabeled tissue, achieving equivalent performance to virtual\nstaining of in-focus label-free images, also saving significant imaging time by\nlowering the microscope's autofocusing precision. This framework incorporates a\nvirtual-autofocusing neural network to digitally refocus the defocused images\nand then transforms the refocused images into virtually stained images using a\nsuccessive network. These cascaded networks form a collaborative inference\nscheme: the virtual staining model regularizes the virtual-autofocusing network\nthrough a style loss during the training. To demonstrate the efficacy of this\nframework, we trained and blindly tested these networks using human lung\ntissue. Using 4x fewer focus points with 2x lower focusing precision, we\nsuccessfully transformed the coarsely-focused autofluorescence images into\nhigh-quality virtually stained H&amp;E images, matching the standard virtual\nstaining framework that used finely-focused autofluorescence input images.\nWithout sacrificing the staining quality, this framework decreases the total\nimage acquisition time needed for virtual staining of a label-free whole-slide\nimage (WSI) by ~32%, together with a ~89% decrease in the autofocusing time,\nand has the potential to eliminate the laborious and costly histochemical\nstaining process in pathology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Luzhe Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1\">Keyi Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haan_K/0/1/0/all/0/1\">Kevin de Haan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuzhu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_B/0/1/0/all/0/1\">Bijie Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-aware Self-supervised Learning for Medical Images Using Graph Neural Network. (arXiv:2207.02957v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02957","description":"<p>Although self-supervised learning enables us to bootstrap the training by\nexploiting unlabeled data, the generic self-supervised methods for natural\nimages do not sufficiently incorporate the context. For medical images, a\ndesirable method should be sensitive enough to detect deviation from\nnormal-appearing tissue of each anatomical region; here, anatomy is the\ncontext. We introduce a novel approach with two levels of self-supervised\nrepresentation learning objectives: one on the regional anatomical level and\nanother on the patient-level. We use graph neural networks to incorporate the\nrelationship between different anatomical regions. The structure of the graph\nis informed by anatomical correspondences between each patient and an\nanatomical atlas. In addition, the graph representation has the advantage of\nhandling any arbitrarily sized image in full resolution. Experiments on\nlarge-scale Computer Tomography (CT) datasets of lung images show that our\napproach compares favorably to baseline methods that do not account for the\ncontext. We use the learned embedding for staging lung tissue abnormalities\nrelated to COVID-19.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1\">Ke Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SphereVLAD++: Attention-based and Signal-enhanced Viewpoint Invariant Descriptor. (arXiv:2207.02958v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02958","description":"<p>LiDAR-based localization approach is a fundamental module for large-scale\nnavigation tasks, such as last-mile delivery and autonomous driving, and\nlocalization robustness highly relies on viewpoints and 3D feature extraction.\nOur previous work provides a viewpoint-invariant descriptor to deal with\nviewpoint differences; however, the global descriptor suffers from a low\nsignal-noise ratio in unsupervised clustering, reducing the distinguishable\nfeature extraction ability. We develop SphereVLAD++, an attention-enhanced\nviewpoint invariant place recognition method in this work. SphereVLAD++\nprojects the point cloud on the spherical perspective for each unique area and\ncaptures the contextual connections between local features and their\ndependencies with global 3D geometry distribution. In return, clustered\nelements within the global descriptor are conditioned on local and global\ngeometries and support the original viewpoint-invariant property of SphereVLAD.\nIn the experiments, we evaluated the localization performance of SphereVLAD++\non both public KITTI360 datasets and self-generated datasets from the city of\nPittsburgh. The experiment results show that SphereVLAD++ outperforms all\nrelative state-of-the-art 3D place recognition methods under small or even\ntotally reversed viewpoint differences and shows 0.69% and 15.81% successful\nretrieval rates with better than the second best. Low computation requirements\nand high time efficiency also help its application for low-cost robots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_G/0/1/0/all/0/1\">Ge Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Weaknesses of Adversarial Camouflage in Overhead Imagery. (arXiv:2207.02963v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02963","description":"<p>Machine learning is increasingly critical for analysis of the ever-growing\ncorpora of overhead imagery. Advanced computer vision object detection\ntechniques have demonstrated great success in identifying objects of interest\nsuch as ships, automobiles, and aircraft from satellite and drone imagery. Yet\nrelying on computer vision opens up significant vulnerabilities, namely, the\nsusceptibility of object detection algorithms to adversarial attacks. In this\npaper we explore the efficacy and drawbacks of adversarial camouflage in an\noverhead imagery context. While a number of recent papers have demonstrated the\nability to reliably fool deep learning classifiers and object detectors with\nadversarial patches, most of this work has been performed on relatively uniform\ndatasets and only a single class of objects. In this work we utilize the\nVisDrone dataset, which has a large range of perspectives and object sizes. We\nexplore four different object classes: bus, car, truck, van. We build a library\nof 24 adversarial patches to disguise these objects, and introduce a patch\ntranslucency variable to our patches. The translucency (or alpha value) of the\npatches is highly correlated to their efficacy. Further, we show that while\nadversarial patches may fool object detectors, the presence of such patches is\noften easily uncovered, with patches on average 24% more detectable than the\nobjects the patches were meant to hide. This raises the question of whether\nsuch patches truly constitute camouflage. Source code is available at\nhttps://github.com/IQTLabs/camolo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Etten_A/0/1/0/all/0/1\">Adam Van Etten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network Binarization via Contrastive Learning. (arXiv:2207.02970v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02970","description":"<p>Neural network binarization accelerates deep models by quantizing their\nweights and activations into 1-bit. However, there is still a huge performance\ngap between Binary Neural Networks (BNNs) and their full-precision (FP)\ncounterparts. As the quantization error caused by weights binarization has been\nreduced in earlier works, the activations binarization becomes the major\nobstacle for further improvement of the accuracy. BNN characterises a unique\nand interesting structure, where the binary and latent FP activations exist in\nthe same forward pass (\\textit{i.e.} $\\text{Binarize}(\\mathbf{a}_F) =\n\\mathbf{a}_B$). To mitigate the information degradation caused by the\nbinarization operation from FP to binary activations, we establish a novel\ncontrastive learning framework while training BNNs through the lens of Mutual\nInformation (MI) maximization. MI is introduced as the metric to measure the\ninformation shared between binary and FP activations, which assists\nbinarization with contrastive learning. Specifically, the representation\nability of the BNNs is greatly strengthened via pulling the positive pairs with\nbinary and FP activations from the same input samples, as well as pushing\nnegative pairs from different samples (the number of negative pairs can be\nexponentially large). This benefits the downstream tasks, not only\nclassification but also segmentation and depth estimation,~\\textit{etc}. The\nexperimental results show that our method can be implemented as a pile-up\nmodule on existing state-of-the-art binarization methods and can remarkably\nimprove the performance over them on CIFAR-10/100 and ImageNet, in addition to\nthe great generalization ability on NYUD-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Human Pose Estimation in Art-historical Images. (arXiv:2207.02976v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02976","description":"<p>Gesture as \\enquote*{language} of non-verbal communication has been\ntheoretically established since the 17th century. However, its relevance for\nthe visual arts has been expressed only sporadically. This may be primarily due\nto the sheer overwhelming amount of data that traditionally had to be processed\nby hand. With the steady progress of digitization, though, a growing number of\nhistorical artifacts have been indexed and made available to the public,\ncreating a need for automatic retrieval of art-historical motifs with similar\nbody constellations or poses. Since the domain of art differs significantly\nfrom existing real-world data sets for human pose estimation due to its style\nvariance, this presents new challenges. In this paper, we propose a novel\napproach to estimate human poses in art-historical images. In contrast to\nprevious work that attempts to bridge the domain gap with pre-trained models or\nthrough style transfer, we suggest semi-supervised learning for both object and\nkeypoint detection. Furthermore, we introduce a novel domain-specific art data\nset that includes both bounding box and keypoint annotations of human figures.\nOur approach achieves significantly better results than methods that use\npre-trained models or style transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Springstein_M/0/1/0/all/0/1\">Matthias Springstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Stefanie Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althaus_C/0/1/0/all/0/1\">Christian Althaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orthogonal Matrix Retrieval with Spatial Consensus for 3D Unknown-View Tomography. (arXiv:2207.02985v1 [math.OC])","link":"http://arxiv.org/abs/2207.02985","description":"<p>Unknown-view tomography (UVT) reconstructs a 3D density map from its 2D\nprojections at unknown, random orientations. A line of work starting with Kam\n(1980) employs the method of moments (MoM) with rotation-invariant Fourier\nfeatures to solve UVT in the frequency domain, assuming that the orientations\nare uniformly distributed. This line of work includes the recent orthogonal\nmatrix retrieval (OMR) approaches based on matrix factorization, which, while\nelegant, either require side information about the density that is not\navailable, or fail to be sufficiently robust. In order for OMR to break free\nfrom those restrictions, we propose to jointly recover the density map and the\northogonal matrices by requiring that they be mutually consistent. We\nregularize the resulting non-convex optimization problem by a denoised\nreference projection and a nonnegativity constraint. This is enabled by the new\nclosed-form expressions for spatial autocorrelation features. Further, we\ndesign an easy-to-compute initial density map which effectively mitigates the\nnon-convexity of the reconstruction problem. Experimental results show that the\nproposed OMR with spatial consensus is more robust and performs significantly\nbetter than the previous state-of-the-art OMR approach in the typical low-SNR\nscenario of 3D UVT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_S/0/1/0/all/0/1\">Shuai Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zehni_M/0/1/0/all/0/1\">Mona Zehni</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhizhen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaiT: Leverage Attention Masks for More Efficient Image Transformers. (arXiv:2207.03006v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03006","description":"<p>Though image transformers have shown competitive results with convolutional\nneural networks in computer vision tasks, lacking inductive biases such as\nlocality still poses problems in terms of model efficiency especially for\nembedded applications. In this work, we address this issue by introducing\nattention masks to incorporate spatial locality into self-attention heads.\nLocal dependencies are captured efficiently with masked attention heads along\nwith global dependencies captured by unmasked attention heads. With Masked\nattention image Transformer - MaiT, top-1 accuracy increases by up to 1.7%\ncompared to CaiT with fewer parameters and FLOPs, and the throughput improves\nby up to 1.5X compared to Swin. Encoding locality with attention masks is model\nagnostic, and thus it applies to monolithic, hierarchical, or other novel\ntransformer architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Ling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardestani_A/0/1/0/all/0/1\">Ali Shafiee Ardestani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_J/0/1/0/all/0/1\">Joseph Hassoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Shape Similarity via Alignment of Multi-Metric Hamiltonian Spectra. (arXiv:2207.03018v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03018","description":"<p>Evaluating the similarity of non-rigid shapes with significant partiality is\na fundamental task in numerous computer vision applications. Here, we propose a\nnovel axiomatic method to match similar regions across shapes. Matching similar\nregions is formulated as the alignment of the spectra of operators closely\nrelated to the Laplace-Beltrami operator (LBO). The main novelty of the\nproposed approach is the consideration of differential operators defined on a\nmanifold with multiple metrics. The choice of a metric relates to fundamental\nshape properties while considering the same manifold under different metrics\ncan thus be viewed as analyzing the underlying manifold from different\nperspectives. Specifically, we examine the scale-invariant metric and the\ncorresponding scale-invariant Laplace-Beltrami operator (SI-LBO) along with the\nregular metric and the regular LBO. We demonstrate that the scale-invariant\nmetric emphasizes the locations of important semantic features in articulated\nshapes. A truncated spectrum of the SI-LBO consequently better captures locally\ncurved regions and complements the global information encapsulated in the\ntruncated spectrum of the regular LBO. We show that matching these dual spectra\noutperforms competing axiomatic frameworks when tested on standard benchmarks.\nWe introduced a new dataset and compare the proposed method with the\nstate-of-the-art learning based approach in a cross-database configuration.\nSpecifically, we show that, when trained on one data set and tested on another,\nthe proposed axiomatic approach which does not involve training, outperforms\nthe deep learning alternative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bensaid_D/0/1/0/all/0/1\">David Bensa&#xef;d</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bracha_A/0/1/0/all/0/1\">Amit Bracha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1\">Ron Kimmel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03038","description":"<p>This paper describes our champion solution for the CVPR2022 Generic Event\nBoundary Captioning (GEBC) competition. GEBC requires the captioning model to\nhave a comprehension of instantaneous status changes around the given video\nboundary, which makes it much more challenging than conventional video\ncaptioning task. In this paper, a Dual-Stream Transformer with improvements on\nboth video content encoding and captions generation is proposed: (1) We utilize\nthree pre-trained models to extract the video features from different\ngranularities. Moreover, we exploit the types of boundary as hints to help the\nmodel generate captions. (2) We particularly design an model, termed as\nDual-Stream Transformer, to learn discriminative representations for boundary\ncaptioning. (3) Towards generating content-relevant and human-like captions, we\nimprove the description quality by designing a word-level ensemble strategy.\nThe promising results on the GEBC test split demonstrate the efficacy of our\nproposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanhua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers: State of the Art and Research Challenges. (arXiv:2207.03041v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03041","description":"<p>Transformers have achieved great success in natural language processing. Due\nto the powerful capability of self-attention mechanism in transformers,\nresearchers develop the vision transformers for a variety of computer vision\ntasks, such as image recognition, object detection, image segmentation, pose\nestimation, and 3D reconstruction. This paper presents a comprehensive overview\nof the literature on different architecture designs and training tricks\n(including self-supervised learning) for vision transformers. Our goal is to\nprovide a systematic review with the open research opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_B/0/1/0/all/0/1\">Bo-Kai Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen-Huang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised RF Signal Representation Learning for NextG Signal Classification with Deep Learning. (arXiv:2207.03046v1 [cs.NI])","link":"http://arxiv.org/abs/2207.03046","description":"<p>Deep learning (DL) finds rich applications in the wireless domain to improve\nspectrum awareness. Typically, the DL models are either randomly initialized\nfollowing a statistical distribution or pretrained on tasks from other data\ndomains such as computer vision (in the form of transfer learning) without\naccounting for the unique characteristics of wireless signals. Self-supervised\nlearning enables the learning of useful representations from Radio Frequency\n(RF) signals themselves even when only limited training data samples with\nlabels are available. We present the first self-supervised RF signal\nrepresentation learning model and apply it to the automatic modulation\nrecognition (AMR) task by specifically formulating a set of transformations to\ncapture the wireless signal characteristics. We show that the sample efficiency\n(the number of labeled samples required to achieve a certain accuracy\nperformance) of AMR can be significantly increased (almost an order of\nmagnitude) by learning signal representations with self-supervised learning.\nThis translates to substantial time and cost savings. Furthermore,\nself-supervised learning increases the model accuracy compared to the\nstate-of-the-art DL methods and maintains high accuracy even when a small set\nof training data samples is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davaslioglu_K/0/1/0/all/0/1\">Kemal Davaslioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boztas_S/0/1/0/all/0/1\">Serdar Boztas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertem_M/0/1/0/all/0/1\">Mehmet Can Ertem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1\">Yalin E. Sagduyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayanoglu_E/0/1/0/all/0/1\">Ender Ayanoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-image Defocus Deblurring by Integration of Defocus Map Prediction Tracing the Inverse Problem Computation. (arXiv:2207.03047v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03047","description":"<p>In this paper, we consider the problem in defocus image deblurring. Previous\nclassical methods follow two-steps approaches, i.e., first defocus map\nestimation and then the non-blind deblurring. In the era of deep learning, some\nresearchers have tried to address these two problems by CNN. However, the\nsimple concatenation of defocus map, which represents the blur level, leads to\nsuboptimal performance. Considering the spatial variant property of the defocus\nblur and the blur level indicated in the defocus map, we employ the defocus map\nas conditional guidance to adjust the features from the input blurring images\ninstead of simple concatenation. Then we propose a simple but effective network\nwith spatial modulation based on the defocus map. To achieve this, we design a\nnetwork consisting of three sub-networks, including the defocus map estimation\nnetwork, a condition network that encodes the defocus map into condition\nfeatures, and the defocus deblurring network that performs spatially dynamic\nmodulation based on the condition features. Moreover, the spatially dynamic\nmodulation is based on an affine transform function to adjust the features from\nthe input blurry images. Experimental results show that our method can achieve\nbetter quantitative and qualitative evaluation performance than the existing\nstate-of-the-art methods on the commonly used public test datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AV-Gaze: A Study on the Effectiveness of Audio Guided Visual Attention Estimation for Non-Profilic Faces. (arXiv:2207.03048v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03048","description":"<p>In challenging real-life conditions such as extreme head-pose, occlusions,\nand low-resolution images where the visual information fails to estimate visual\nattention/gaze direction, audio signals could provide important and\ncomplementary information. In this paper, we explore if audio-guided coarse\nhead-pose can further enhance visual attention estimation performance for\nnon-prolific faces. Since it is difficult to annotate audio signals for\nestimating the head-pose of the speaker, we use off-the-shelf state-of-the-art\nmodels to facilitate cross-modal weak-supervision. During the training phase,\nthe framework learns complementary information from synchronized audio-visual\nmodality. Our model can utilize any of the available modalities i.e. audio,\nvisual or audio-visual for task-specific inference. It is interesting to note\nthat, when AV-Gaze is tested on benchmark datasets with these specific\nmodalities, it achieves competitive results on multiple datasets, while being\nhighly adaptive towards challenging scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shreya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1\">Abhinav Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knibbe_J/0/1/0/all/0/1\">Jarrod Knibbe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network. (arXiv:2207.03050v1 [eess.IV])","link":"http://arxiv.org/abs/2207.03050","description":"<p>Lung nodules can be an alarming precursor to potential lung cancer. Missed\nnodule detections during chest radiograph analysis remains a common challenge\namong thoracic radiologists. In this work, we present a multi-task lung nodule\ndetection algorithm for chest radiograph analysis. Unlike past approaches, our\nalgorithm predicts a global-level label indicating nodule presence along with\nlocal-level labels predicting nodule locations using a Dual Head Network (DHN).\nWe demonstrate the favorable nodule detection performance that our multi-task\nformulation yields in comparison to conventional methods. In addition, we\nintroduce a novel Dual Head Augmentation (DHA) strategy tailored for DHN, and\nwe demonstrate its significance in further enhancing global and local nodule\npredictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tsai_C/0/1/0/all/0/1\">Chen-Han Tsai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Shao Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Rotation Correction without Angle Prior. (arXiv:2207.03054v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03054","description":"<p>Not everybody can be equipped with professional photography skills and\nsufficient shooting time, and there can be some tilts in the captured images\noccasionally. In this paper, we propose a new and practical task, named\nRotation Correction, to automatically correct the tilt with high content\nfidelity in the condition that the rotated angle is unknown. This task can be\neasily integrated into image editing applications, allowing users to correct\nthe rotated images without any manual operations. To this end, we leverage a\nneural network to predict the optical flows that can warp the tilted images to\nbe perceptually horizontal. Nevertheless, the pixel-wise optical flow\nestimation from a single image is severely unstable, especially in large-angle\ntilted images. To enhance its robustness, we propose a simple but effective\nprediction strategy to form a robust elastic warp. Particularly, we first\nregress the mesh deformation that can be transformed into robust initial\noptical flows. Then we estimate residual optical flows to facilitate our\nnetwork the flexibility of pixel-wise deformation, further correcting the\ndetails of the tilted images. To establish an evaluation benchmark and train\nthe learning framework, a comprehensive rotation correction dataset is\npresented with a large diversity in scenes and rotated angles. Extensive\nexperiments demonstrate that even in the absence of the angle prior, our\nalgorithm can outperform other state-of-the-art solutions requiring this prior.\nThe codes and dataset will be available at\nhttps://github.com/nie-lang/RotationCorrection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Basics: Revisiting Out-of-Distribution Detection Baselines. (arXiv:2207.03061v1 [cs.LG])","link":"http://arxiv.org/abs/2207.03061","description":"<p>We study simple methods for out-of-distribution (OOD) image detection that\nare compatible with any already trained classifier, relying on only its\npredictions or learned representations. Evaluating the OOD detection\nperformance of various methods when utilized with ResNet-50 and Swin\nTransformer models, we find methods that solely consider the model's\npredictions can be easily outperformed by also considering the learned\nrepresentations. Based on our analysis, we advocate for a dead-simple approach\nthat has been neglected in other studies: simply flag as OOD images whose\naverage distance to their K nearest neighbors is large (in the representation\nspace of an image classifier trained on the in-distribution data).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuan_J/0/1/0/all/0/1\">Johnson Kuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shadow-Background-Noise 3D Spatial Decomposition Using Sparse Low-Rank Gaussian Properties for Video-SAR Moving Target Shadow Enhancement. (arXiv:2207.03064v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03064","description":"<p>Moving target shadows among video synthetic aperture radar (Video-SAR) images\nare always interfered by low scattering backgrounds and cluttered noises,\ncausing poor moving target shadow detection-tracking performance. To solve this\nproblem, this letter proposes a shadow-background-noise 3D spatial\nde-composition method named SBN-3D-SD to boost shadow saliency for better\nVideo-SAR moving target shadow detection-tracking performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jinyu Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning from Spatio-Temporal Mixed Skeleton Sequences for Self-Supervised Skeleton-Based Action Recognition. (arXiv:2207.03065v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03065","description":"<p>Self-supervised skeleton-based action recognition with contrastive learning\nhas attracted much attention. Recent literature shows that data augmentation\nand large sets of contrastive pairs are crucial in learning such\nrepresentations. In this paper, we found that directly extending contrastive\npairs based on normal augmentations brings limited returns in terms of\nperformance, because the contribution of contrastive pairs from the normal data\naugmentation to the loss get smaller as training progresses. Therefore, we\ndelve into hard contrastive pairs for contrastive learning. Motivated by the\nsuccess of mixing augmentation strategy which improves the performance of many\ntasks by synthesizing novel samples, we propose SkeleMixCLR: a contrastive\nlearning framework with a spatio-temporal skeleton mixing augmentation\n(SkeleMix) to complement current contrastive learning approaches by providing\nhard contrastive samples. First, SkeleMix utilizes the topological information\nof skeleton data to mix two skeleton sequences by randomly combing the cropped\nskeleton fragments (the trimmed view) with the remaining skeleton sequences\n(the truncated view). Second, a spatio-temporal mask pooling is applied to\nseparate these two views at the feature level. Third, we extend contrastive\npairs with these two views. SkeleMixCLR leverages the trimmed and truncated\nviews to provide abundant hard contrastive pairs since they involve some\ncontext information from each other due to the graph convolution operations,\nwhich allows the model to learn better motion representations for action\nrecognition. Extensive experiments on NTU-RGB+D, NTU120-RGB+D, and PKU-MMD\ndatasets show that SkeleMixCLR achieves state-of-the-art performance. Codes are\navailable at https://github.com/czhaneva/SkeleMixCLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tianyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_P/0/1/0/all/0/1\">Pinhao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes for Automatic Reconstruction of Pulmonary Segments. (arXiv:2207.03078v1 [eess.IV])","link":"http://arxiv.org/abs/2207.03078","description":"<p>3D reconstruction of pulmonary segments plays an important role in surgical\ntreatment planning of lung cancer, which facilitates preservation of pulmonary\nfunction and helps ensure low recurrence rates. However, automatic\nreconstruction of pulmonary segments remains unexplored in the era of deep\nlearning. In this paper, we investigate what makes for automatic reconstruction\nof pulmonary segments. First and foremost, we formulate, clinically and\ngeometrically, the anatomical definitions of pulmonary segments, and propose\nevaluation metrics adhering to these definitions. Second, we propose ImPulSe\n(Implicit Pulmonary Segment), a deep implicit surface model designed for\npulmonary segment reconstruction. The automatic reconstruction of pulmonary\nsegments by ImPulSe is accurate in metrics and visually appealing. Compared\nwith canonical segmentation methods, ImPulSe outputs continuous predictions of\narbitrary resolutions with higher training efficiency and fewer parameters.\nLastly, we experiment with different network inputs to analyze what matters in\nthe task of pulmonary segment reconstruction. Our code is available at\nhttps://github.com/M3DV/ImPulSe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kuang_K/0/1/0/all/0/1\">Kaiming Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jingyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning. (arXiv:2207.03081v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03081","description":"<p>In this paper, we propose a multi-objective camera ISP framework that\nutilizes Deep Reinforcement Learning (DRL) and camera ISP toolbox that consist\nof network-based and conventional ISP tools. The proposed DRL-based camera ISP\nframework iteratively selects a proper tool from the toolbox and applies it to\nthe image to maximize a given vision task-specific reward function. For this\npurpose, we implement total 51 ISP tools that include exposure correction,\ncolor-and-tone correction, white balance, sharpening, denoising, and the\nothers. We also propose an efficient DRL network architecture that can extract\nthe various aspects of an image and make a rigid mapping relationship between\nimages and a large number of actions. Our proposed DRL-based ISP framework\neffectively improves the image quality according to each vision task such as\nRAW-to-RGB image restoration, 2D object detection, and monocular depth\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_U/0/1/0/all/0/1\">Ukcheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptation of Surgical Activity Recognition Models Across Operating Rooms. (arXiv:2207.03083v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03083","description":"<p>Automatic surgical activity recognition enables more intelligent surgical\ndevices and a more efficient workflow. Integration of such technology in new\noperating rooms has the potential to improve care delivery to patients and\ndecrease costs. Recent works have achieved a promising performance on surgical\nactivity recognition; however, the lack of generalizability of these models is\none of the critical barriers to the wide-scale adoption of this technology. In\nthis work, we study the generalizability of surgical activity recognition\nmodels across operating rooms. We propose a new domain adaptation method to\nimprove the performance of the surgical activity recognition model in a new\noperating room for which we only have unlabeled videos. Our approach generates\npseudo labels for unlabeled video clips that it is confident about and trains\nthe model on the augmented version of the clips. We extend our method to a\nsemi-supervised domain adaptation setting where a small portion of the target\ndomain is also labeled. In our experiments, our proposed method consistently\noutperforms the baselines on a dataset of more than 480 long surgical videos\ncollected from two operating rooms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_A/0/1/0/all/0/1\">Ali Mottaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharghi_A/0/1/0/all/0/1\">Aidean Sharghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Serena Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1\">Omid Mohareri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022: Team HNU-FPV Technical Report. (arXiv:2207.03095v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03095","description":"<p>In this report, we present the technical details of our submission to the\n2022 EPIC-Kitchens Unsupervised Domain Adaptation (UDA) Challenge. Existing UDA\nmethods align the global features extracted from the whole video clips across\nthe source and target domains but suffer from the spatial redundancy of feature\nmatching in video recognition. Motivated by the observation that in most cases\na small image region in each video frame can be informative enough for the\naction recognition task, we propose to exploit informative image regions to\nperform efficient domain alignment. Specifically, we first use lightweight CNNs\nto extract the global information of the input two-stream video frames and\nselect the informative image patches by a differentiable interpolation-based\nselection strategy. Then the global information from videos frames and local\ninformation from image patches are processed by an existing video adaptation\nmethod, i.e., TA3N, in order to perform feature alignment for the source domain\nand the target domain. Our method (without model ensemble) ranks 4th among this\nyear's teams on the test set of EPIC-KITCHENS-100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Minjie Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Self-supervised Neural Network for Liver $T_{1\\rho}$ Mapping with Relaxation Constraint. (arXiv:2207.03105v1 [q-bio.TO])","link":"http://arxiv.org/abs/2207.03105","description":"<p>$T_{1\\rho}$ mapping is a promising quantitative MRI technique for the\nnon-invasive assessment of tissue properties. Learning-based approaches can map\n$T_{1\\rho}$ from a reduced number of $T_{1\\rho}$ weighted images, but requires\nsignificant amounts of high quality training data. Moreover, existing methods\ndo not provide the confidence level of the $T_{1\\rho}$ estimation. To address\nthese problems, we proposed a self-supervised learning neural network that\nlearns a $T_{1\\rho}$ mapping using the relaxation constraint in the learning\nprocess. Epistemic uncertainty and aleatoric uncertainty are modelled for the\n$T_{1\\rho}$ quantification network to provide a Bayesian confidence estimation\nof the $T_{1\\rho}$ mapping. The uncertainty estimation can also regularize the\nmodel to prevent it from learning imperfect data. We conducted experiments on\n$T_{1\\rho}$ data collected from 52 patients with non-alcoholic fatty liver\ndisease. The results showed that our method outperformed the existing methods\nfor $T_{1\\rho}$ quantification of the liver using as few as two\n$T_{1\\rho}$-weighted images. Our uncertainty estimation provided a feasible way\nof modelling the confidence of the self-supervised learning based $T_{1\\rho}$\nestimation, which is consistent with the reality in liver $T_{1\\rho}$ imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_C/0/1/0/all/0/1\">Chaoxing Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Qian_Y/0/1/0/all/0/1\">Yurui Qian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yu_S/0/1/0/all/0/1\">Simon Chun Ho Yu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hou_J/0/1/0/all/0/1\">Jian Hou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jiang_B/0/1/0/all/0/1\">Baiyan Jiang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chan_Q/0/1/0/all/0/1\">Queenie Chan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wong_V/0/1/0/all/0/1\">Vincent Wai-Sun Wong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chu_W/0/1/0/all/0/1\">Winnie Chiu-Wing Chu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_W/0/1/0/all/0/1\">Weitian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Surfel Prediction for Self-Supervised Point Cloud Learning. (arXiv:2207.03111v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03111","description":"<p>Masked auto-encoding is a popular and effective self-supervised learning\napproach to point cloud learning. However, most of the existing methods\nreconstruct only the masked points and overlook the local geometry information,\nwhich is also important to understand the point cloud data. In this work, we\nmake the first attempt, to the best of our knowledge, to consider the local\ngeometry information explicitly into the masked auto-encoding, and propose a\nnovel Masked Surfel Prediction (MaskSurf) method. Specifically, given the input\npoint cloud masked at a high ratio, we learn a transformer-based\nencoder-decoder network to estimate the underlying masked surfels by\nsimultaneously predicting the surfel positions (i.e., points) and per-surfel\norientations (i.e., normals). The predictions of points and normals are\nsupervised by the Chamfer Distance and a newly introduced Position-Indexed\nNormal Distance in a set-to-set manner. Our MaskSurf is validated on six\ndownstream tasks under three fine-tuning strategies. In particular, MaskSurf\noutperforms its closest competitor, Point-MAE, by 1.2\\% on the real-world\ndataset of ScanObjectNN under the OBJ-BG setting, justifying the advantages of\nmasked surfel prediction over masked point cloud reconstruction. Codes will be\navailable at https://github.com/YBZh/MaskSurf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiehong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design of Human Machine Interface through vision-based low-cost Hand Gesture Recognition system based on deep CNN with transfer-learning approach. (arXiv:2207.03112v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03112","description":"<p>In this work, a real-time hand gesture recognition system-based\nhuman-computer interface (HCI) is presented. The system consists of six stages:\n(1) hand detection, (2) gesture segmentation, (3) use of six pre-trained CNN\nmodels by using the transfer-learning method, (4) building an interactive\nhuman-machine interface, (5) development of a gesture-controlled virtual mouse,\n(6) use of Kalman filter to estimate the hand position, based on that the\nsmoothness of the motion of pointer is improved. Six pre-trained convolutional\nneural network (CNN) models (VGG16, VGG19, ResNet50, ResNet101, Inception-V1,\nand MobileNet-V1) have been used to classify hand gesture images. Three\nmulti-class datasets (two publicly and one custom) have been used to evaluate\nthe model performances. Considering the models' performances, it has been\nobserved that Inception-V1 has significantly shown a better classification\nperformance compared to the other five pre-trained models in terms of accuracy,\nprecision, recall, and F-score values. The gesture recognition system is\nexpanded and used to control multimedia applications (like VLC player, audio\nplayer, file management, playing 2D Super-Mario-Bros game, etc.) with different\ncustomized gesture commands in real-time scenarios. The average speed of this\nsystem has reached 35 fps (frame per seconds), which meets the requirements for\nthe real-time scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1\">Abir Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1\">Tapas Kumar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_R/0/1/0/all/0/1\">Ratnakar Dash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Vision-to-Geometry Knowledge Transfer for 3D Point Cloud Shape Analysis. (arXiv:2207.03128v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03128","description":"<p>As two fundamental representation modalities of 3D objects, 2D multi-view\nimages and 3D point clouds reflect shape information from different aspects of\nvisual appearances and geometric structures. Unlike deep learning-based 2D\nmulti-view image modeling, which demonstrates leading performances in various\n3D shape analysis tasks, 3D point cloud-based geometric modeling still suffers\nfrom insufficient learning capacity. In this paper, we innovatively construct a\nunified cross-modal knowledge transfer framework, which distills discriminative\nvisual descriptors of 2D images into geometric descriptors of 3D point clouds.\nTechnically, under a classic teacher-student learning paradigm, we propose\nmulti-view vision-to-geometry distillation, consisting of a deep 2D image\nencoder as teacher and a deep 3D point cloud encoder as student. To achieve\nheterogeneous feature alignment, we further propose visibility-aware feature\nprojection, through which per-point embeddings can be aggregated into\nmulti-view geometric descriptors. Extensive experiments on 3D shape\nclassification, part segmentation, and unsupervised learning validate the\nsuperiority of our method. We will make the code and data publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yue Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Interleaved Learning for Generalizable Person Re-identification. (arXiv:2207.03132v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03132","description":"<p>Domain generalization (DG) for person re-identification (ReID) is a\nchallenging problem, as there is no access to target domain data permitted\nduring the training process. Most existing DG ReID methods employ the same\nfeatures for the updating of the feature extractor and classifier parameters.\nThis common practice causes the model to overfit to existing feature styles in\nthe source domain, resulting in sub-optimal generalization ability on target\ndomains even if meta-learning is used. To solve this problem, we propose a\nnovel style interleaved learning framework. Unlike conventional learning\nstrategies, interleaved learning incorporates two forward propagations and one\nbackward propagation for each iteration. We employ the features of interleaved\nstyles to update the feature extractor and classifiers using different forward\npropagations, which helps the model avoid overfitting to certain domain styles.\nIn order to fully explore the advantages of style interleaved learning, we\nfurther propose a novel feature stylization approach to diversify feature\nstyles. This approach not only mixes the feature styles of multiple training\nsamples, but also samples new and meaningful feature styles from batch-level\nstyle distribution. Extensive experimental results show that our model\nconsistently outperforms state-of-the-art methods on large-scale benchmarks for\nDG ReID, yielding clear advantages in computational efficiency. Code is\navailable at https://github.com/WentaoTan/Interleaved-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wentao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Few-Shot Image Classification Using Machine- and User-Generated Natural Language Descriptions. (arXiv:2207.03133v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03133","description":"<p>Humans can obtain the knowledge of novel visual concepts from language\ndescriptions, and we thus use the few-shot image classification task to\ninvestigate whether a machine learning model can have this capability. Our\nproposed model, LIDE (Learning from Image and DEscription), has a text decoder\nto generate the descriptions and a text encoder to obtain the text\nrepresentations of machine- or user-generated descriptions. We confirmed that\nLIDE with machine-generated descriptions outperformed baseline models.\nMoreover, the performance was improved further with high-quality user-generated\ndescriptions. The generated descriptions can be viewed as the explanations of\nthe model's predictions, and we observed that such explanations were consistent\nwith prediction results. We also investigated why the language description\nimproved the few-shot image classification performance by comparing the image\nrepresentations and the text representations in the feature spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kyosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishioka_S/0/1/0/all/0/1\">Shuichi Nishioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Velocity Estimation for Automotive Radar Object Detection Networks. (arXiv:2207.03146v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03146","description":"<p>This paper presents a method to learn the Cartesian velocity of objects using\nan object detection network on automotive radar data. The proposed method is\nself-supervised in terms of generating its own training signal for the\nvelocities. Labels are only required for single-frame, oriented bounding boxes\n(OBBs). Labels for the Cartesian velocities or contiguous sequences, which are\nexpensive to obtain, are not required. The general idea is to pre-train an\nobject detection network without velocities using single-frame OBB labels, and\nthen exploit the network's OBB predictions on unlabelled data for velocity\ntraining. In detail, the network's OBB predictions of the unlabelled frames are\nupdated to the timestamp of a labelled frame using the predicted velocities and\nthe distances between the updated OBBs of the unlabelled frame and the OBB\npredictions of the labelled frame are used to generate a self-supervised\ntraining signal for the velocities. The detection network architecture is\nextended by a module to account for the temporal relation of multiple scans and\na module to represent the radars' radial velocity measurements explicitly. A\ntwo-step approach of first training only OBB detection, followed by training\nOBB detection and velocities is used. Further, a pre-training with\npseudo-labels generated from radar radial velocity measurements bootstraps the\nself-supervised method of this paper. Experiments on the publicly available\nnuScenes dataset show that the proposed method almost reaches the velocity\nestimation performance of a fully supervised training, but does not require\nexpensive velocity labels. Furthermore, we outperform a baseline method which\nuses only radial velocity measurements as labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niederlohner_D/0/1/0/all/0/1\">Daniel Niederl&#xf6;hner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_M/0/1/0/all/0/1\">Michael Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1\">Sascha Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_D/0/1/0/all/0/1\">Daniel K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faion_F/0/1/0/all/0/1\">Florian Faion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaser_C/0/1/0/all/0/1\">Claudius Gl&#xe4;ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treptow_A/0/1/0/all/0/1\">Andr&#xe9; Treptow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blume_H/0/1/0/all/0/1\">Holger Blume</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastHebb: Scaling Hebbian Training of Deep Neural Networks to ImageNet Level. (arXiv:2207.03172v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03172","description":"<p>Learning algorithms for Deep Neural Networks are typically based on\nsupervised end-to-end Stochastic Gradient Descent (SGD) training with error\nbackpropagation (backprop). Backprop algorithms require a large number of\nlabelled training samples to achieve high performance. However, in many\nrealistic applications, even if there is plenty of image samples, very few of\nthem are labelled, and semi-supervised sample-efficient training strategies\nhave to be used. Hebbian learning represents a possible approach towards sample\nefficient training; however, in current solutions, it does not scale well to\nlarge datasets. In this paper, we present FastHebb, an efficient and scalable\nsolution for Hebbian learning which achieves higher efficiency by 1) merging\ntogether update computation and aggregation over a batch of inputs, and 2)\nleveraging efficient matrix multiplication algorithms on GPU. We validate our\napproach on different computer vision benchmarks, in a semi-supervised learning\nscenario. FastHebb outperforms previous solutions by up to 50 times in terms of\ntraining speed, and notably, for the first time, we are able to bring Hebbian\nalgorithms to ImageNet scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lagani_G/0/1/0/all/0/1\">Gabriele Lagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1\">Claudio Gennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fassold_H/0/1/0/all/0/1\">Hannes Fassold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1\">Giuseppe Amato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration. (arXiv:2207.03180v1 [eess.IV])","link":"http://arxiv.org/abs/2207.03180","description":"<p>Recently, deep-learning-based approaches have been widely studied for\ndeformable image registration task. However, most efforts directly map the\ncomposite image representation to spatial transformation through the\nconvolutional neural network, ignoring its limited ability to capture spatial\ncorrespondence. On the other hand, Transformer can better characterize the\nspatial relationship with attention mechanism, its long-range dependency may be\nharmful to the registration task, where voxels with too large distances are\nunlikely to be corresponding pairs. In this study, we propose a novel Deformer\nmodule along with a multi-scale framework for the deformable image registration\ntask. The Deformer module is designed to facilitate the mapping from image\nrepresentation to spatial transformation by formulating the displacement vector\nprediction as the weighted summation of several bases. With the multi-scale\nframework to predict the displacement fields in a coarse-to-fine manner,\nsuperior performance can be achieved compared with traditional and\nlearning-based approaches. Comprehensive experiments on two public datasets are\nconducted to demonstrate the effectiveness of the proposed Deformer module as\nwell as the multi-scale framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiashun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_M/0/1/0/all/0/1\">Munan Ning</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xinyu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty of Atmospheric Motion Vectors by Sampling Tempered Posterior Distributions. (arXiv:2207.03182v1 [stat.ME])","link":"http://arxiv.org/abs/2207.03182","description":"<p>Atmospheric motion vectors (AMVs) extracted from satellite imagery are the\nonly wind observations with good global coverage. They are important features\nfor feeding numerical weather prediction (NWP) models. Several Bayesian models\nhave been proposed to estimate AMVs. Although critical for correct assimilation\ninto NWP models, very few methods provide a thorough characterization of the\nestimation errors. The difficulty of estimating errors stems from the\nspecificity of the posterior distribution, which is both very high dimensional,\nand highly ill-conditioned due to a singular likelihood, which becomes critical\nin particular in the case of missing data (unobserved pixels). This work\nstudies the evaluation of the expected error of AMVs using gradient-based\nMarkov Chain Monte Carlo (MCMC) algorithms. Our main contribution is to propose\na tempering strategy, which amounts to sampling a local approximation of the\njoint posterior distribution of AMVs and image variables in the neighborhood of\na point estimate. In addition, we provide efficient preconditioning with the\ncovariance related to the prior family itself (fractional Brownian motion),\nwith possibly different hyper-parameters. From a theoretical point of view, we\nshow that under regularity assumptions, the family of tempered posterior\ndistributions converges in distribution as temperature decreases to an\n{optimal} Gaussian approximation at a point estimate given by the Maximum A\nPosteriori (MAP) log-density. From an empirical perspective, we evaluate the\nproposed approach based on some quantitative Bayesian evaluation criteria. Our\nnumerical simulations performed on synthetic and real meteorological data\nreveal a significant gain in terms of accuracy of the AMV point estimates and\nof their associated expected error estimates, but also a substantial\nacceleration in the convergence speed of the MCMC algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Heas_P/0/1/0/all/0/1\">Patrick H&#xe9;as</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cerou_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric C&#xe9;rou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rousset_M/0/1/0/all/0/1\">Mathias Rousset</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Music-Dance Representation through Explicit-Implicit Rhythm Synchronization. (arXiv:2207.03190v1 [cs.SD])","link":"http://arxiv.org/abs/2207.03190","description":"<p>Although audio-visual representation has been proved to be applicable in many\ndownstream tasks, the representation of dancing videos, which is more specific\nand always accompanied by music with complex auditory contents, remains\nchallenging and uninvestigated. Considering the intrinsic alignment between the\ncadent movement of dancer and music rhythm, we introduce MuDaR, a novel\nMusic-Dance Representation learning framework to perform the synchronization of\nmusic and dance rhythms both in explicit and implicit ways. Specifically, we\nderive the dance rhythms based on visual appearance and motion cues inspired by\nthe music rhythm analysis. Then the visual rhythms are temporally aligned with\nthe music counterparts, which are extracted by the amplitude of sound\nintensity. Meanwhile, we exploit the implicit coherence of rhythms implied in\naudio and visual streams by contrastive learning. The model learns the joint\nembedding by predicting the temporal consistency between audio-visual pairs.\nThe music-dance representation, together with the capability of detecting audio\nand visual rhythms, can further be applied to three downstream tasks: (a) dance\nclassification, (b) music-dance retrieval, and (c) music-dance retargeting.\nExtensive experiments demonstrate that our proposed framework outperforms other\nself-supervised methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiashuo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Junfu Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Ying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCTS with Refinement for Proposals Selection Games in Scene Understanding. (arXiv:2207.03204v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03204","description":"<p>We propose a novel method applicable in many scene understanding problems\nthat adapts the Monte Carlo Tree Search (MCTS) algorithm, originally designed\nto learn to play games of high-state complexity. From a generated pool of\nproposals, our method jointly selects and optimizes proposals that minimize the\nobjective term. In our first application for floor plan reconstruction from\npoint clouds, our method selects and refines the room proposals, modelled as 2D\npolygons, by optimizing on an objective function combining the fitness as\npredicted by a deep network and regularizing terms on the room shapes. We also\nintroduce a novel differentiable method for rendering the polygonal shapes of\nthese proposals. Our evaluations on the recent and challenging Structured3D and\nFloor-SP datasets show significant improvements over the state-of-the-art,\nwithout imposing hard constraints nor assumptions on the floor plan\nconfigurations. In our second application, we extend our approach to\nreconstruct general 3D room layouts from a color image and obtain accurate room\nlayouts. We also show that our differentiable renderer can easily be extended\nfor rendering 3D planar polygons and polygon embeddings. Our method shows high\nperformance on the Matterport3D-Layout dataset, without introducing hard\nconstraints on room layout configurations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stekovic_S/0/1/0/all/0/1\">Sinisa Stekovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rad_M/0/1/0/all/0/1\">Mahdi Rad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_A/0/1/0/all/0/1\">Alireza Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Stream Computer-Generated Image Detection Network Based On Channel Joint And Softpool. (arXiv:2207.03205v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03205","description":"<p>With the development of computer graphics technology, the images synthesized\nby computer software become more and more closer to the photographs. While\ncomputer graphics technology brings us a grand visual feast in the field of\ngames and movies, it may also be utilized by someone with bad intentions to\nguide public opinions and cause political crisis or social unrest. Therefore,\nhow to distinguish the computer-generated graphics (CG) from the photographs\n(PG) has become an important topic in the field of digital image forensics.\nThis paper proposes a dual stream convolutional neural network based on channel\njoint and softpool. The proposed network architecture includes a residual\nmodule for extracting image noise information and a joint channel information\nextraction module for capturing the shallow semantic information of image. In\naddition, we also design a residual structure to enhance feature extraction and\nreduce the loss of information in residual flow. The joint channel information\nextraction module can obtain the shallow semantic information of the input\nimage which can be used as the information supplement block of the residual\nmodule. The whole network uses SoftPool to reduce the information loss of\ndown-sampling for image. Finally, we fuse the two flows to get the\nclassification results. Experiments on SPL2018 and DsTok show that the proposed\nmethod outperforms existing methods, especially on the DsTok dataset. For\nexample, the performance of our model surpasses the state-of-the-art by a large\nmargin of 3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Ziyi Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weiqi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning. (arXiv:2207.03210v1 [eess.IV])","link":"http://arxiv.org/abs/2207.03210","description":"<p>We propose a method for estimating the bone mineral density (BMD) from a\nplain x-ray image. Dual-energy X-ray absorptiometry (DXA) and quantitative\ncomputed tomography (QCT) provide high accuracy in diagnosing osteoporosis;\nhowever, these modalities require special equipment and scan protocols.\nMeasuring BMD from an x-ray image provides an opportunistic screening, which is\npotentially useful for early diagnosis. The previous methods that directly\nlearn the relationship between x-ray images and BMD require a large training\ndataset to achieve high accuracy because of large intensity variations in the\nx-ray images. Therefore, we propose an approach using the QCT for training a\ngenerative adversarial network (GAN) and decomposing an x-ray image into a\nprojection of bone-segmented QCT. The proposed hierarchical learning improved\nthe robustness and accuracy of quantitatively decomposing a small-area target.\nThe evaluation of 200 patients with osteoarthritis using the proposed method,\nwhich we named BMD-GAN, demonstrated a Pearson correlation coefficient of 0.888\nbetween the predicted and ground truth DXA-measured BMD. Besides not requiring\na large-scale training database, another advantage of our method is its\nextensibility to other anatomical areas, such as the vertebrae and rib bones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yi Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1\">Yoshito Otake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uemura_K/0/1/0/all/0/1\">Keisuke Uemura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soufi_M/0/1/0/all/0/1\">Mazen Soufi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takao_M/0/1/0/all/0/1\">Masaki Takao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sugano_N/0/1/0/all/0/1\">Nobuhiko Sugano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sato_Y/0/1/0/all/0/1\">Yoshinobu Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy-Based Feature Extraction For Real-Time Semantic Segmentation. (arXiv:2207.03233v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03233","description":"<p>This paper introduces an efficient patch-based computational module, coined\nEntropy-based Patch Encoder (EPE) module, for resource-constrained semantic\nsegmentation. The EPE module consists of three lightweight fully-convolutional\nencoders, each extracting features from image patches with a different amount\nof entropy. Patches with high entropy are being processed by the encoder with\nthe largest number of parameters, patches with moderate entropy are processed\nby the encoder with a moderate number of parameters, and patches with low\nentropy are processed by the smallest encoder. The intuition behind the module\nis the following: as patches with high entropy contain more information, they\nneed an encoder with more parameters, unlike low entropy patches, which can be\nprocessed using a small encoder. Consequently, processing part of the patches\nvia the smaller encoder can significantly reduce the computational cost of the\nmodule. Experiments show that EPE can boost the performance of existing\nreal-time semantic segmentation models with a slight increase in the\ncomputational cost. Specifically, EPE increases the mIOU performance of DFANet\nA by 0.9% with only 1.2% increase in the number of parameters and the mIOU\nperformance of EDANet by 1% with 10% increase of the model parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2HNet: Joint Denoising and Deblurring with Hierarchical Network for Robust Night Image Restoration. (arXiv:2207.03294v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03294","description":"<p>Night imaging with modern smartphone cameras is troublesome due to low photon\ncount and unavoidable noise in the imaging system. Directly adjusting exposure\ntime and ISO ratings cannot obtain sharp and noise-free images at the same time\nin low-light conditions. Though many methods have been proposed to enhance\nnoisy or blurry night images, their performances on real-world night photos are\nstill unsatisfactory due to two main reasons: 1) Limited information in a\nsingle image and 2) Domain gap between synthetic training images and real-world\nphotos (e.g., differences in blur area and resolution). To exploit the\ninformation from successive long- and short-exposure images, we propose a\nlearning-based pipeline to fuse them. A D2HNet framework is developed to\nrecover a high-quality image by deblurring and enhancing a long-exposure image\nunder the guidance of a short-exposure image. To shrink the domain gap, we\nleverage a two-phase DeblurNet-EnhanceNet architecture, which performs accurate\nblur removal on a fixed low resolution so that it is able to handle large\nranges of blur in different resolution inputs. In addition, we synthesize a\nD2-Dataset from HD videos and experiment on it. The results on the validation\nset and real photos demonstrate our methods achieve better visual quality and\nstate-of-the-art quantitative scores. The D2HNet codes, models, and D2-Dataset\ncan be found at https://github.com/zhaoyuzhi/D2HNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuzhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingdong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuehui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1\">Lai-Man Po</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExpansionNet: exploring the sequence length bottleneck in the Transformer for Image Captioning. (arXiv:2207.03327v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03327","description":"<p>Most recent state of art architectures rely on combinations and variations of\nthree approaches: convolutional, recurrent and self-attentive methods. Our work\nattempts in laying the basis for a new research direction for sequence modeling\nbased upon the idea of modifying the sequence length. In order to do that, we\npropose a new method called ``Expansion Mechanism'' which transforms either\ndynamically or statically the input sequence into a new one featuring a\ndifferent sequence length. Furthermore, we introduce a novel architecture that\nexploits such method and achieves competitive performances on the MS-COCO 2014\ndata set, yielding 134.6 and 131.4 CIDEr-D on the Karpathy test split in the\nensemble and single model configuration respectively and 130 CIDEr-D in the\nofficial online testing server, despite being neither recurrent nor fully\nattentive. At the same time we address the efficiency aspect in our design and\nintroduce a convenient training strategy suitable for most computational\nresources in contrast to the standard one. Source code is available at\nhttps://github.com/jchenghu/ExpansionNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jia Cheng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to Image Synthesis using Stacked Conditional Variational Autoencoders and Conditional Generative Adversarial Networks. (arXiv:2207.03332v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03332","description":"<p>Synthesizing a realistic image from textual description is a major challenge\nin computer vision. Current text to image synthesis approaches falls short of\nproducing a highresolution image that represent a text descriptor. Most\nexisting studies rely either on Generative Adversarial Networks (GANs) or\nVariational Auto Encoders (VAEs). GANs has the capability to produce sharper\nimages but lacks the diversity of outputs, whereas VAEs are good at producing a\ndiverse range of outputs, but the images generated are often blurred. Taking\ninto account the relative advantages of both GANs and VAEs, we proposed a new\nstacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture\nfor synthesizing images conditioned on a text description. This study uses\nConditional VAEs as an initial generator to produce a high-level sketch of the\ntext descriptor. This high-level sketch output from first stage and a text\ndescriptor is used as an input to the conditional GAN network. The second stage\nGAN produces a 256x256 high resolution image. The proposed architecture\nbenefits from a conditioning augmentation and a residual block on the\nConditional GAN network to achieve the results. Multiple experiments were\nconducted using CUB and Oxford-102 dataset and the result of the proposed\napproach is compared against state-ofthe-art techniques such as StackGAN. The\nexperiments illustrate that the proposed method generates a high-resolution\nimage conditioned on text descriptions and yield competitive results based on\nInception and Frechet Inception Score using both datasets\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tibebu_H/0/1/0/all/0/1\">Haileleol Tibebu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1\">Aadin Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Varuna De Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewSOL: A Dataset for Few-Shot Object Learning in Robotic Environments. (arXiv:2207.03333v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03333","description":"<p>We introduce the Few-Shot Object Learning (FewSOL) dataset for object\nrecognition with a few images per object. We captured 336 real-world objects\nwith 9 RGB-D images per object from different views. Object segmentation masks,\nobject poses and object attributes are provided. In addition, synthetic images\ngenerated using 330 3D object models are used to augment the dataset. We\ninvestigated (i) few-shot object classification and (ii) joint object\nsegmentation and few-shot classification with the state-of-the-art methods for\nfew-shot learning and meta-learning using our dataset. The evaluation results\nshow that there is still a large margin to be improved for few-shot object\nclassification in robotic environments. Our dataset can be used to study a set\nof few-shot object recognition problems such as classification, detection and\nsegmentation, shape reconstruction, pose estimation, keypoint correspondences\nand attribute recognition. The dataset and code are available at\nhttps://irvlutd.github.io/FewSOL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+P_J/0/1/0/all/0/1\">Jishnu Jaykumar P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1\">Yu-Wei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Ensemble of Explanations for Weakly-Supervised Pre-Training of Image Segmentation Models. (arXiv:2207.03335v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03335","description":"<p>While fine-tuning pre-trained networks has become a popular way to train\nimage segmentation models, such backbone networks for image segmentation are\nfrequently pre-trained using image classification source datasets, e.g.,\nImageNet. Though image classification datasets could provide the backbone\nnetworks with rich visual features and discriminative ability, they are\nincapable of fully pre-training the target model (i.e., backbone+segmentation\nmodules) in an end-to-end manner. The segmentation modules are left to random\ninitialization in the fine-tuning process due to the lack of segmentation\nlabels in classification datasets. In our work, we propose a method that\nleverages Pseudo Semantic Segmentation Labels (PSSL), to enable the end-to-end\npre-training for image segmentation models based on classification datasets.\nPSSL was inspired by the observation that the explanation results of\nclassification models, obtained through explanation algorithms such as CAM,\nSmoothGrad and LIME, would be close to the pixel clusters of visual objects.\nSpecifically, PSSL is obtained for each image by interpreting the\nclassification results and aggregating an ensemble of explanations queried from\nmultiple classifiers to lower the bias caused by single models. With PSSL for\nevery image of ImageNet, the proposed method leverages a weighted segmentation\nlearning procedure to pre-train the segmentation network en masse. Experiment\nresults show that, with ImageNet accompanied by PSSL as the source dataset, the\nproposed end-to-end pre-training strategy successfully boosts the performance\nof various segmentation models, i.e., PSPNet-ResNet50, DeepLabV3-ResNet50, and\nOCRNet-HRNetW18, on a number of segmentation tasks, such as CamVid, VOC-A,\nVOC-C, ADE20K, and CityScapes, with significant improvements. The source code\nis availabel at https://github.com/PaddlePaddle/PaddleSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorizing Knowledge in Neural Networks. (arXiv:2207.03337v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03337","description":"<p>In this paper, we explore a novel and ambitious knowledge-transfer task,\ntermed Knowledge Factorization~(KF). The core idea of KF lies in the\nmodularization and assemblability of knowledge: given a pretrained network\nmodel as input, KF aims to decompose it into several factor networks, each of\nwhich handles only a dedicated task and maintains task-specific knowledge\nfactorized from the source network. Such factor networks are task-wise\ndisentangled and can be directly assembled, without any fine-tuning, to produce\nthe more competent combined-task networks. In other words, the factor networks\nserve as Lego-brick-like building blocks, allowing us to construct customized\nnetworks in a plug-and-play manner. Specifically, each factor network comprises\ntwo modules, a common-knowledge module that is task-agnostic and shared by all\nfactor networks, alongside with a task-specific module dedicated to the factor\nnetwork itself. We introduce an information-theoretic objective,\nInfoMax-Bottleneck~(IMB), to carry out KF by optimizing the mutual information\nbetween the learned representations and input. Experiments across various\nbenchmarks demonstrate that, the derived factor networks yield gratifying\nperformances on not only the dedicated tasks but also disentanglement, while\nenjoying much better interpretability and modularity. Moreover, the learned\ncommon-knowledge representations give rise to impressive results on transfer\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Softmax-free Linear Transformers. (arXiv:2207.03341v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03341","description":"<p>Vision transformers (ViTs) have pushed the state-of-the-art for various\nvisual recognition tasks by patch-wise image tokenization followed by stacked\nself-attention operations. Employing self-attention modules results in a\nquadratic complexity in both computation and memory usage. Various attempts on\napproximating the self-attention computation with linear complexity have thus\nbeen made in Natural Language Processing. However, an in-depth analysis in this\nwork reveals that they are either theoretically flawed or empirically\nineffective for visual recognition. We identify that their limitations are\nrooted in retaining the softmax self-attention during approximations.\nSpecifically, conventional self-attention is computed by normalizing the scaled\ndot-product between token feature vectors. Preserving the softmax operation\nchallenges any subsequent linearization efforts. Under this insight, a\nSOftmax-Free Transformer (abbreviated as SOFT) is proposed for the first time.\nTo eliminate the softmax operator in self-attention, a Gaussian kernel function\nis adopted to replace the dot-product similarity. This enables a full\nself-attention matrix to be approximated via a low-rank matrix decomposition.\nThe robustness of our approximation is achieved by calculating its\nMoore-Penrose inverse using a Newton-Raphson method. Further, an efficient\nsymmetric normalization is introduced on the low-rank self-attention for\nenhancing model generalizability and transferability. Extensive experiments on\nImageNet, COCO and ADE20K show that our SOFT significantly improves the\ncomputational efficiency of existing ViT variants. Crucially, with a linear\ncomplexity, much longer token sequences are permitted in SOFT, resulting in\nsuperior trade-off between accuracy and complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monkeypox Skin Lesion Detection Using Deep Learning Models: A Feasibility Study. (arXiv:2207.03342v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03342","description":"<p>The recent monkeypox outbreak has become a public health concern due to its\nrapid spread in more than 40 countries outside Africa. Clinical diagnosis of\nmonkeypox in an early stage is challenging due to its similarity with\nchickenpox and measles. In cases where the confirmatory Polymerase Chain\nReaction (PCR) tests are not readily available, computer-assisted detection of\nmonkeypox lesions could be beneficial for surveillance and rapid identification\nof suspected cases. Deep learning methods have been found effective in the\nautomated detection of skin lesions, provided that sufficient training examples\nare available. However, as of now, such datasets are not available for the\nmonkeypox disease. In the current study, we first develop the ``Monkeypox Skin\nLesion Dataset (MSLD)\" consisting skin lesion images of monkeypox, chickenpox,\nand measles. The images are mainly collected from websites, news portals, and\npublicly accessible case reports. Data augmentation is used to increase the\nsample size, and a 3-fold cross-validation experiment is set up. In the next\nstep, several pre-trained deep learning models, namely, VGG-16, ResNet50, and\nInceptionV3 are employed to classify monkeypox and other diseases. An ensemble\nof the three models is also developed. ResNet50 achieves the best overall\naccuracy of $82.96(\\pm4.57\\%)$, while VGG16 and the ensemble system achieved\naccuracies of $81.48(\\pm6.87\\%)$ and $79.26(\\pm1.05\\%)$, respectively. A\nprototype web-application is also developed as an online monkeypox screening\ntool. While the initial results on this limited dataset are promising, a larger\ndemographically diverse dataset is required to further enhance the\ngeneralizability of these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Shams Nafisa Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Md. Tazuddin Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_J/0/1/0/all/0/1\">Joydip Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahan_T/0/1/0/all/0/1\">Tasnim Jahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sani_S/0/1/0/all/0/1\">S. M. Sakeef Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noor_N/0/1/0/all/0/1\">Nawsabah Noor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Taufiq Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Classification of General Movements in Infants Using a Two-stream Spatiotemporal Fusion Network. (arXiv:2207.03344v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03344","description":"<p>The assessment of general movements (GMs) in infants is a useful tool in the\nearly diagnosis of neurodevelopmental disorders. However, its evaluation in\nclinical practice relies on visual inspection by experts, and an automated\nsolution is eagerly awaited. Recently, video-based GMs classification has\nattracted attention, but this approach would be strongly affected by irrelevant\ninformation, such as background clutter in the video. Furthermore, for\nreliability, it is necessary to properly extract the spatiotemporal features of\ninfants during GMs. In this study, we propose an automated GMs classification\nmethod, which consists of preprocessing networks that remove unnecessary\nbackground information from GMs videos and adjust the infant's body position,\nand a subsequent motion classification network based on a two-stream structure.\nThe proposed method can efficiently extract the essential spatiotemporal\nfeatures for GMs classification while preventing overfitting to irrelevant\ninformation for different recording environments. We validated the proposed\nmethod using videos obtained from 100 infants. The experimental results\ndemonstrate that the proposed method outperforms several baseline models and\nthe existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_Y/0/1/0/all/0/1\">Yuki Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furui_A/0/1/0/all/0/1\">Akira Furui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimatani_K/0/1/0/all/0/1\">Koji Shimatani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casadio_M/0/1/0/all/0/1\">Maura Casadio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moretti_P/0/1/0/all/0/1\">Paolo Moretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morasso_P/0/1/0/all/0/1\">Pietro Morasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuji_T/0/1/0/all/0/1\">Toshio Tsuji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A simple normalization technique using window statistics to improve the out-of-distribution generalization in medical images. (arXiv:2207.03366v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03366","description":"<p>Since data scarcity and data heterogeneity are prevailing for medical images,\nwell-trained Convolutional Neural Networks (CNNs) using previous normalization\nmethods may perform poorly when deployed to a new site. However, a reliable\nmodel for real-world applications should be able to generalize well both on\nin-distribution (IND) and out-of-distribution (OOD) data (e.g., the new site\ndata). In this study, we present a novel normalization technique called window\nnormalization (WIN), which is a simple yet effective alternative to existing\nnormalization methods. Specifically, WIN perturbs the normalizing statistics\nwith the local statistics computed on a window of features. This feature-level\naugmentation technique regularizes the models well and improves their OOD\ngeneralization significantly. Taking its advantage, we propose a novel\nself-distillation method called WIN-WIN to further improve the OOD\ngeneralization in classification. WIN-WIN is easily implemented with twice\nforward passes and a consistency constraint, which can be a simple extension\nfor existing methods. Extensive experimental results on various tasks (such as\nglaucoma detection, breast cancer detection, chromosome classification, optic\ndisc and cup segmentation, etc.) and datasets (26 datasets) demonstrate the\ngenerality and effectiveness of our methods. The code is available at\nhttps://github.com/joe1chief/windowNormalizaion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chengfeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songchang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Juan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hefeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1\">Dahong Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Super-Resolution and Inverse Tone-Mapping: A Feature Decomposition Aggregation Network and A New Benchmark. (arXiv:2207.03367v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03367","description":"<p>Joint Super-Resolution and Inverse Tone-Mapping (joint SR-ITM) aims to\nincrease the resolution and dynamic range of low-resolution and standard\ndynamic range images.Recent methods mainly resort to image decomposition\ntechniques with the multi-branch network architecture.However, the rigid\ndecomposition employed by these methods largely restricts their power on\ndiverse images.To exploit its potential power, in this paper, we generalize the\ndecomposition mechanism from the image domain to the broader feature domain. To\nthis end, we propose a lightweight Feature Decomposition Aggregation Network\n(FDAN). In particular, we design a Feature Decomposition Block (FDB), which can\nachieve learnable separation of feature details and contrasts.By cascading\nFDBs, we can build up a Hierarchical Feature Decomposition Group for powerful\nmulti-level feature decomposition.Moreover, we collect a new benchmark dataset\nfor joint SR-ITM, \\ie, SRITM-4K, which is large-scale and provides versatile\nscenarios for sufficient model training and evaluation.Experimental results on\ntwo benchmark datasets demonstrate that our FDAN is efficient and outperforms\nprevious methods on joint SR-ITM.Our code and dataset will be publicly\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gang Xu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuchen Yang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xian-Tong Zhen</a> (3 and 4), <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a> (1) ((1) Nankai University, (2) Institute of Automation, CAS, (3) University of Amsterdam, (4) Inception Institute of Artificial Intelligence)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnosing and Remedying Shot Sensitivity with Cosine Few-Shot Learners. (arXiv:2207.03398v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03398","description":"<p>Few-shot recognition involves training an image classifier to distinguish\nnovel concepts at test time using few examples (shot). Existing approaches\ngenerally assume that the shot number at test time is known in advance. This is\nnot realistic, and the performance of a popular and foundational method has\nbeen shown to suffer when train and test shots do not match. We conduct a\nsystematic empirical study of this phenomenon. In line with prior work, we find\nthat shot sensitivity is broadly present across metric-based few-shot learners,\nbut in contrast to prior work, larger neural architectures provide a degree of\nbuilt-in robustness to varying test shot. More importantly, a simple,\npreviously known but greatly overlooked class of approaches based on cosine\ndistance consistently and greatly improves robustness to shot variation, by\nremoving sensitivity to sample noise. We derive cosine alternatives to popular\nand recent few-shot classifiers, broadening their applicability to realistic\nsettings. These cosine models consistently improve shot-robustness, outperform\nprior shot-robust state of the art, and provide competitive accuracy on a range\nof benchmarks and architectures, including notable gains in the very-low-shot\nregime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wertheimer_D/0/1/0/all/0/1\">Davis Wertheimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Luming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Watermarking for Video Forgery Detection with Improved Imperceptibility and Robustness. (arXiv:2207.03409v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03409","description":"<p>Videos are prone to tampering attacks that alter the meaning and deceive the\naudience. Previous video forgery detection schemes find tiny clues to locate\nthe tampered areas. However, attackers can successfully evade supervision by\ndestroying such clues using video compression or blurring. This paper proposes\na video watermarking network for tampering localization. We jointly train a\n3D-UNet-based watermark embedding network and a decoder that predicts the\ntampering mask. The perturbation made by watermark embedding is close to\nimperceptible. Considering that there is no off-the-shelf differentiable video\ncodec simulator, we propose to mimic video compression by ensembling simulation\nresults of other typical attacks, e.g., JPEG compression and blurring, as an\napproximation. Experimental results demonstrate that our method generates\nwatermarked videos with good imperceptibility and robustly and accurately\nlocates tampered areas within the attacked version.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VecGAN: Image-to-Image Translation with Interpretable Latent Directions. (arXiv:2207.03411v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03411","description":"<p>We propose VecGAN, an image-to-image translation framework for facial\nattribute editing with interpretable latent directions. Facial attribute\nediting task faces the challenges of precise attribute editing with\ncontrollable strength and preservation of the other attributes of an image. For\nthis goal, we design the attribute editing by latent space factorization and\nfor each attribute, we learn a linear direction that is orthogonal to the\nothers. The other component is the controllable strength of the change, a\nscalar value. In our framework, this scalar can be either sampled or encoded\nfrom a reference image by projection. Our work is inspired by the latent space\nfactorization works of fixed pretrained GANs. However, while those models\ncannot be trained end-to-end and struggle to edit encoded images precisely,\nVecGAN is end-to-end trained for image translation task and successful at\nediting an attribute while preserving the others. Our extensive experiments\nshow that VecGAN achieves significant improvements over state-of-the-arts for\nboth local and global edits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalva_Y/0/1/0/all/0/1\">Yusuf Dalva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altindis_S/0/1/0/all/0/1\">Said Fahri Altindis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1\">Aysegul Dundar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Knowledge Driven 3D Dose Prediction Using Moment-Based Loss Function. (arXiv:2207.03414v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03414","description":"<p>Dose volume histogram (DVH) metrics are widely accepted evaluation criteria\nin the clinic. However, incorporating these metrics into deep learning dose\nprediction models is challenging due to their non-convexity and\nnon-differentiability. We propose a novel moment-based loss function for\npredicting 3D dose distribution for the challenging conventional lung intensity\nmodulated radiation therapy (IMRT) plans. The moment-based loss function is\nconvex and differentiable and can easily incorporate DVH metrics in any deep\nlearning framework without computational overhead. The moments can also be\ncustomized to reflect the clinical priorities in 3D dose prediction. For\ninstance, using high-order moments allows better prediction in high-dose areas\nfor serial structures. We used a large dataset of 360 (240 for training, 50 for\nvalidation and 70 for testing) conventional lung patients with 2Gy $\\times$ 30\nfractions to train the deep learning (DL) model using clinically treated plans\nat our institution. We trained a UNet like CNN architecture using computed\ntomography (CT), planning target volume (PTV) and organ-at-risk contours (OAR)\nas input to infer corresponding voxel-wise 3D dose distribution. We evaluated\nthree different loss functions: (1) The popular Mean Absolute Error (MAE) Loss,\n(2) the recently developed MAE + DVH Loss, and (3) the proposed MAE + Moments\nLoss. The quality of the predictions was compared using different DVH metrics\nas well as dose-score and DVH-score, recently introduced by the AAPM\nknowledge-based planning grand challenge. Model with (MAE + Moment) loss\nfunction outperformed the model with MAE loss by significantly improving the\nDVH-score (11%, p$&lt;$0.01) while having similar computational cost. It also\noutperformed the model trained with (MAE+DVH) by significantly improving the\ncomputational cost (48%) and the DVH-score (8%, p$&lt;$0.01).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jhanwar_G/0/1/0/all/0/1\">Gourav Jhanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1\">Navdeep Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahremani_P/0/1/0/all/0/1\">Parmida Ghahremani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarepisheh_M/0/1/0/all/0/1\">Masoud Zarepisheh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion. (arXiv:2207.03430v1 [eess.IV])","link":"http://arxiv.org/abs/2207.03430","description":"<p>Multi-modal medical image completion has been extensively applied to\nalleviate the missing modality issue in a wealth of multi-modal diagnostic\ntasks. However, for most existing synthesis methods, their inferences of\nmissing modalities can collapse into a deterministic mapping from the available\nones, ignoring the uncertainties inherent in the cross-modal relationships.\nHere, we propose the Unified Multi-Modal Conditional Score-based Generative\nModel (UMM-CSGM) to take advantage of Score-based Generative Model (SGM) in\nmodeling and stochastically sampling a target probability distribution, and\nfurther extend SGM to cross-modal conditional synthesis for various\nmissing-modality configurations in a unified framework. Specifically, UMM-CSGM\nemploys a novel multi-in multi-out Conditional Score Network (mm-CSN) to learn\na comprehensive set of cross-modal conditional distributions via conditional\ndiffusion and reverse generation in the complete modality space. In this way,\nthe generation process can be accurately conditioned by all available\ninformation, and can fit all possible configurations of missing modalities in a\nsingle network. Experiments on BraTS19 dataset show that the UMM-CSGM can more\nreliably synthesize the heterogeneous enhancement and irregular area in\ntumor-induced lesions for any missing modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_X/0/1/0/all/0/1\">Xiangxi Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yuning Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yongsheng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_P/0/1/0/all/0/1\">Peng Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Mengkang Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhan_Y/0/1/0/all/0/1\">Yiqiang Zhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Object Detection via Virtual Category Learning. (arXiv:2207.03433v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03433","description":"<p>Due to the costliness of labelled data in real-world applications,\nsemi-supervised object detectors, underpinned by pseudo labelling, are\nappealing. However, handling confusing samples is nontrivial: discarding\nvaluable confusing samples would compromise the model generalisation while\nusing them for training would exacerbate the confirmation bias issue caused by\ninevitable mislabelling. To solve this problem, this paper proposes to use\nconfusing samples proactively without label correction. Specifically, a virtual\ncategory (VC) is assigned to each confusing sample such that they can safely\ncontribute to the model optimisation even without a concrete label. It is\nattributed to specifying the embedding distance between the training sample and\nthe virtual category as the lower bound of the inter-class distance. Moreover,\nwe also modify the localisation loss to allow high-quality boundaries for\nlocation regression. Extensive experiments demonstrate that the proposed VC\nlearning significantly surpasses the state-of-the-art, especially with small\namounts of available labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debattista_K/0/1/0/all/0/1\">Kurt Debattista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LASSIE: Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery. (arXiv:2207.03434v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03434","description":"<p>Creating high-quality articulated 3D models of animals is challenging either\nvia manual creation or using 3D scanning tools. Therefore, techniques to\nreconstruct articulated 3D objects from 2D images are crucial and highly\nuseful. In this work, we propose a practical problem setting to estimate 3D\npose and shape of animals given only a few (10-30) in-the-wild images of a\nparticular animal species (say, horse). Contrary to existing works that rely on\npre-defined template shapes, we do not assume any form of 2D or 3D ground-truth\nannotations, nor do we leverage any multi-view or temporal information.\nMoreover, each input image ensemble can contain animal instances with varying\nposes, backgrounds, illuminations, and textures. Our key insight is that 3D\nparts have much simpler shape compared to the overall animal and that they are\nrobust w.r.t. animal pose articulations. Following these insights, we propose\nLASSIE, a novel optimization framework which discovers 3D parts in a\nself-supervised manner with minimal user intervention. A key driving force\nbehind LASSIE is the enforcing of 2D-3D part consistency using self-supervisory\ndeep features. Experiments on Pascal-Part and self-collected in-the-wild animal\ndatasets demonstrate considerably better 3D reconstructions as well as both 2D\nand 3D part discovery compared to prior arts. Project page:\nchhankyao.github.io/lassie/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chun-Han Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1\">Michael Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Source: Diffusion-Driven Test-Time Adaptation. (arXiv:2207.03442v1 [cs.LG])","link":"http://arxiv.org/abs/2207.03442","description":"<p>Test-time adaptation harnesses test inputs to improve the accuracy of a model\ntrained on source data when tested on shifted target data. Existing methods\nupdate the source model by (re-)training on each target domain. While\neffective, re-training is sensitive to the amount and order of the data and the\nhyperparameters for optimization. We instead update the target data, by\nprojecting all test inputs toward the source domain with a generative diffusion\nmodel. Our diffusion-driven adaptation method, DDA, shares its models for\nclassification and generation across all domains. Both models are trained on\nthe source domain, then fixed during testing. We augment diffusion with image\nguidance and self-ensembling to automatically decide how much to adapt. Input\nadaptation by DDA is more robust than prior model adaptation approaches across\na variety of corruptions, architectures, and data regimes on the ImageNet-C\nbenchmark. With its input-wise updates, DDA succeeds where model adaptation\ndegrades on too little data in small batches, dependent data in non-uniform\norder, or mixed data with multiple corruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jialing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xihui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dequan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness and Bias in Robot Learning. (arXiv:2207.03444v1 [cs.RO])","link":"http://arxiv.org/abs/2207.03444","description":"<p>Machine learning has significantly enhanced the abilities of robots, enabling\nthem to perform a wide range of tasks in human environments and adapt to our\nuncertain real world. Recent works in various domains of machine learning have\nhighlighted the importance of accounting for fairness to ensure that these\nalgorithms do not reproduce human biases and consequently lead to\ndiscriminatory outcomes. With robot learning systems increasingly performing\nmore and more tasks in our everyday lives, it is crucial to understand the\ninfluence of such biases to prevent unintended behavior toward certain groups\nof people. In this work, we present the first survey on fairness in robot\nlearning from an interdisciplinary perspective spanning technical, ethical, and\nlegal challenges. We propose a taxonomy for sources of bias and the resulting\ntypes of discrimination due to them. Using examples from different robot\nlearning domains, we examine scenarios of unfair outcomes and strategies to\nmitigate them. We present early advances in the field by covering different\nfairness definitions, ethical and legal considerations, and methods for fair\nrobot learning. With this work, we aim at paving the road for groundbreaking\ndevelopments in fair robot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Londono_L/0/1/0/all/0/1\">Laura Londo&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Juana Valeria Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertz_N/0/1/0/all/0/1\">Nora Hertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellmeyer_P/0/1/0/all/0/1\">Philipp Kellmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voeneky_S/0/1/0/all/0/1\">Silja Voeneky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to restore images degraded by atmospheric turbulence using uncertainty. (arXiv:2207.03447v1 [eess.IV])","link":"http://arxiv.org/abs/2207.03447","description":"<p>Atmospheric turbulence can significantly degrade the quality of images\nacquired by long-range imaging systems by causing spatially and temporally\nrandom fluctuations in the index of refraction of the atmosphere. Variations in\nthe refractive index causes the captured images to be geometrically distorted\nand blurry. Hence, it is important to compensate for the visual degradation in\nimages caused by atmospheric turbulence. In this paper, we propose a deep\nlearning-based approach for restring a single image degraded by atmospheric\nturbulence. We make use of the epistemic uncertainty based on Monte Carlo\ndropouts to capture regions in the image where the network is having hard time\nrestoring. The estimated uncertainty maps are then used to guide the network to\nobtain the restored image. Extensive experiments are conducted on synthetic and\nreal images to show the significance of the proposed work. Code is available at\n: https://github.com/rajeevyasarla/AT-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yasarla_R/0/1/0/all/0/1\">Rajeev Yasarla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation. (arXiv:2207.03450v1 [eess.IV])","link":"http://arxiv.org/abs/2207.03450","description":"<p>Medical image segmentation is one of the most fundamental tasks concerning\nmedical information analysis. Various solutions have been proposed so far,\nincluding many deep learning-based techniques, such as U-Net, FC-DenseNet, etc.\nHowever, high-precision medical image segmentation remains a highly challenging\ntask due to the existence of inherent magnification and distortion in medical\nimages as well as the presence of lesions with similar density to normal\ntissues. In this paper, we propose TFCNs (Transformers for Fully Convolutional\ndenseNets) to tackle the problem by introducing ResLinear-Transformer\n(RL-Transformer) and Convolutional Linear Attention Block (CLAB) to\nFC-DenseNet. TFCNs is not only able to utilize more latent information from the\nCT images for feature extraction, but also can capture and disseminate semantic\nfeatures and filter non-semantic features more effectively through the CLAB\nmodule. Our experimental results show that TFCNs can achieve state-of-the-art\nperformance with dice scores of 83.72\\% on the Synapse dataset. In addition, we\nevaluate the robustness of TFCNs for lesion area effects on the COVID-19 public\ndatasets. The Python code will be made publicly available on\nhttps://github.com/HUANGLIZI/TFCNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1\">Dihan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Cangbai Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Weice Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_Q/0/1/0/all/0/1\">Qingqi Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qingde Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1\">Jie Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Red PANDA: Disambiguating Anomaly Detection by Removing Nuisance Factors. (arXiv:2207.03478v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03478","description":"<p>Anomaly detection methods strive to discover patterns that differ from the\nnorm in a semantic way. This goal is ambiguous as a data point differing from\nthe norm by an attribute e.g., age, race or gender, may be considered anomalous\nby some operators while others may consider this attribute irrelevant. Breaking\nfrom previous research, we present a new anomaly detection method that allows\noperators to exclude an attribute from being considered as relevant for anomaly\ndetection. Our approach then learns representations which do not contain\ninformation over the nuisance attributes. Anomaly scoring is performed using a\ndensity-based approach. Importantly, our approach does not require specifying\nthe attributes that are relevant for detecting anomalies, which is typically\nimpossible in anomaly detection, but only attributes to ignore. An empirical\ninvestigation is presented verifying the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahana_J/0/1/0/all/0/1\">Jonathan Kahana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection. (arXiv:2207.03482v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03482","description":"<p>Existing open-vocabulary object detectors typically enlarge their vocabulary\nsizes by leveraging different forms of weak supervision. This helps generalize\nto novel objects at inference. Two popular forms of weak-supervision used in\nopen-vocabulary detection (OVD) include pretrained CLIP model and image-level\nsupervision. We note that both these modes of supervision are not optimally\naligned for the detection task: CLIP is trained with image-text pairs and lacks\nprecise localization of objects while the image-level supervision has been used\nwith heuristics that do not accurately specify local object regions. In this\nwork, we propose to address this problem by performing object-centric alignment\nof the language embeddings from the CLIP model. Furthermore, we visually ground\nthe objects with only image-level supervision using a pseudo-labeling process\nthat provides high-quality object proposals and helps expand the vocabulary\nduring training. We establish a bridge between the above two object-alignment\nstrategies via a novel weight transfer function that aggregates their\ncomplimentary strengths. In essence, the proposed model seeks to minimize the\ngap between object and image-centric representations in the OVD setting. On the\nCOCO benchmark, our proposed approach achieves 40.3 AP50 on novel classes, an\nabsolute 11.9 gain over the previous best performance.For LVIS, we surpass the\nstate-of-the-art ViLD model by 5.0 mask AP for rare categories and 3.4 overall.\nCode: https://bit.ly/3byZoQp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1\">Hanoona Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1\">Muhammad Maaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1\">Muhammad Uzair Khattak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Fallen Objects Via Asynchronous Audio-Visual Integration. (arXiv:2207.03483v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03483","description":"<p>The way an object looks and sounds provide complementary reflections of its\nphysical properties. In many settings cues from vision and audition arrive\nasynchronously but must be integrated, as when we hear an object dropped on the\nfloor and then must find it. In this paper, we introduce a setting in which to\nstudy multi-modal object localization in 3D virtual environments. An object is\ndropped somewhere in a room. An embodied robot agent, equipped with a camera\nand microphone, must determine what object has been dropped -- and where -- by\ncombining audio and visual signals with knowledge of the underlying physics. To\nstudy this problem, we have generated a large-scale dataset -- the Fallen\nObjects dataset -- that includes 8000 instances of 30 physical object\ncategories in 64 rooms. The dataset uses the ThreeDWorld platform which can\nsimulate physics-based impact sounds and complex physical interactions between\nobjects in a photorealistic setting. As a first step toward addressing this\nchallenge, we develop a set of embodied agent baselines, based on imitation\nlearning, reinforcement learning, and modular planning, and perform an in-depth\nanalysis of the challenge of this new task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Siyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_J/0/1/0/all/0/1\">Jeremy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alter_S/0/1/0/all/0/1\">Seth Alter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traer_J/0/1/0/all/0/1\">James Traer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDermott_J/0/1/0/all/0/1\">Josh McDermott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAiSEE: Towards User Engagement Recognition in the Wild. (arXiv:1609.01885v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1609.01885","description":"<p>We introduce DAiSEE, the first multi-label video classification dataset\ncomprising of 9068 video snippets captured from 112 users for recognizing the\nuser affective states of boredom, confusion, engagement, and frustration in the\nwild. The dataset has four levels of labels namely - very low, low, high, and\nvery high for each of the affective states, which are crowd annotated and\ncorrelated with a gold standard annotation created using a team of expert\npsychologists. We have also established benchmark results on this dataset using\nstate-of-the-art video classification methods that are available today. We\nbelieve that DAiSEE will provide the research community with challenges in\nfeature extraction, context-based inference, and development of suitable\nmachine learning methods for related tasks, thus providing a springboard for\nfurther research. The dataset is available for download at\nhttps://people.iith.ac.in/vineethnb/resources/daisee/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DCunha_A/0/1/0/all/0/1\">Arjun D&#x27;Cunha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_K/0/1/0/all/0/1\">Kamal Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks. (arXiv:2012.11230v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.11230","description":"<p>Quantizing deep convolutional neural networks for image super-resolution\nsubstantially reduces their computational costs. However, existing works either\nsuffer from a severe performance drop in ultra-low precision of 4 or lower\nbit-widths, or require a heavy fine-tuning process to recover the performance.\nTo our knowledge, this vulnerability to low precisions relies on two\nstatistical observations of feature map values. First, distribution of feature\nmap values varies significantly per channel and per input image. Second,\nfeature maps have outliers that can dominate the quantization error. Based on\nthese observations, we propose a novel distribution-aware quantization scheme\n(DAQ) which facilitates accurate training-free quantization in ultra-low\nprecision. A simple function of DAQ determines dynamic range of feature maps\nand weights with low computational burden. Furthermore, our method enables\nmixed-precision quantization by calculating the relative sensitivity of each\nchannel, without any training process involved. Nonetheless, quantization-aware\ntraining is also applicable for auxiliary performance gain. Our new method\noutperforms recent training-free and even training-based quantization methods\nto the state-of-the-art image super-resolution networks in ultra-low precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Cheeun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baik_S/0/1/0/all/0/1\">Sungyong Baik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Junghun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Depth and Ego-Motion Estimation for Monocular Thermal Video Using Multi-Spectral Consistency Loss. (arXiv:2103.00760v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00760","description":"<p>A thermal camera can robustly capture thermal radiation images under harsh\nlight conditions such as night scenes, tunnels, and disaster scenarios.\nHowever, despite this advantage, neither depth nor ego-motion estimation\nresearch for the thermal camera have not been actively explored so far. In this\npaper, we propose a self-supervised learning method for depth and ego-motion\nestimation from thermal images. The proposed method exploits multi-spectral\nconsistency that consists of temperature and photometric consistency loss. The\ntemperature consistency loss provides a fundamental self-supervisory signal by\nreconstructing clipped and colorized thermal images. Additionally, we design a\ndifferentiable forward warping module that can transform the coordinate system\nof the estimated depth map and relative pose from thermal camera to visible\ncamera. Based on the proposed module, the photometric consistency loss can\nprovide complementary self-supervision to networks. Networks trained with the\nproposed method robustly estimate the depth and pose from monocular thermal\nvideo under low-light and even zero-light conditions. To the best of our\nknowledge, this is the first work to simultaneously estimate both depth and\nego-motion from monocular thermal video in a self-supervised manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_U/0/1/0/all/0/1\">Ukcheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seokju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control. (arXiv:2104.03893v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2104.03893","description":"<p>Objective: For lower arm amputees, robotic prosthetic hands promise to regain\nthe capability to perform daily living activities. Current control methods\nbased on physiological signals such as electromyography (EMG) are prone to\nyielding poor inference outcomes due to motion artifacts, muscle fatigue, and\nmany more. Vision sensors are a major source of information about the\nenvironment state and can play a vital role in inferring feasible and intended\ngestures. However, visual evidence is also susceptible to its own artifacts,\nmost often due to object occlusion, lighting changes, etc. Multimodal evidence\nfusion using physiological and vision sensor measurements is a natural approach\ndue to the complementary strengths of these modalities. Methods: In this paper,\nwe present a Bayesian evidence fusion framework for grasp intent inference\nusing eye-view video, eye-gaze, and EMG from the forearm processed by neural\nnetwork models. We analyze individual and fused performance as a function of\ntime as the hand approaches the object to grasp it. For this purpose, we have\nalso developed novel data processing and augmentation techniques to train\nneural network components. Results: Our results indicate that, on average,\nfusion improves the instantaneous upcoming grasp type classification accuracy\nwhile in the reaching phase by 13.66% and 14.8%, relative to EMG and visual\nevidence individually, resulting in an overall fusion accuracy of 95.3%.\nConclusion: Our experimental data analyses demonstrate that EMG and visual\nevidence show complementary strengths, and as a consequence, fusion of\nmultimodal evidence can outperform each individual evidence modality at any\ngiven time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zandigohar_M/0/1/0/all/0/1\">Mehrshad Zandigohar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharif_M/0/1/0/all/0/1\">Mohammadreza Sharif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunay_S/0/1/0/all/0/1\">Sezen Yagmur Gunay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furmanek_M/0/1/0/all/0/1\">Mariusz P. Furmanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1\">Mathew Yarossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonato_P/0/1/0/all/0/1\">Paolo Bonato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onal_C/0/1/0/all/0/1\">Cagdas Onal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padir_T/0/1/0/all/0/1\">Taskin Padir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schirner_G/0/1/0/all/0/1\">Gunar Schirner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.05642","description":"<p>While the importance of automatic image analysis is continuously increasing,\nrecent meta-research revealed major flaws with respect to algorithm validation.\nPerformance metrics are particularly key for meaningful, objective, and\ntransparent performance assessment and validation of the used automatic\nalgorithms, but relatively little attention has been given to the practical\npitfalls when using specific metrics for a given image analysis task. These are\ntypically related to (1) the disregard of inherent metric properties, such as\nthe behaviour in the presence of class imbalance or small target structures,\n(2) the disregard of inherent data set properties, such as the non-independence\nof the test cases, and (3) the disregard of the actual biomedical domain\ninterest that the metrics should reflect. This living dynamically document has\nthe purpose to illustrate important limitations of performance metrics commonly\napplied in the field of image analysis. In this context, it focuses on\nbiomedical image analysis problems that can be phrased as image-level\nclassification, semantic segmentation, instance segmentation, or object\ndetection task. The current version is based on a Delphi process on metrics\nconducted by an international consortium of image analysis experts from more\nthan 60 institutions worldwide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudre_C/0/1/0/all/0/1\">Carole H. Sudre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Radsch_T/0/1/0/all/0/1\">Tim R&#xe4;dsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1\">Michael Baumgartner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acion_L/0/1/0/all/0/1\">Laura Acion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antonelli_M/0/1/0/all/0/1\">Michela Antonelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bankhead_P/0/1/0/all/0/1\">Peter Bankhead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benis_A/0/1/0/all/0/1\">Arriel Benis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christodoulou_E/0/1/0/all/0/1\">Evangelia Christodoulou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cimini_B/0/1/0/all/0/1\">Beth Cimini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collins_G/0/1/0/all/0/1\">Gary S. Collins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamprecht_F/0/1/0/all/0/1\">Fred Hamprecht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffman_M/0/1/0/all/0/1\">Michael M. Hoffman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huisman_M/0/1/0/all/0/1\">Merel Huisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jannin_P/0/1/0/all/0/1\">Pierre Jannin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kahn_C/0/1/0/all/0/1\">Charles E. Kahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karargyris_A/0/1/0/all/0/1\">Alexandros Karargyris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kavur_E/0/1/0/all/0/1\">Emre Kavur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes Kenngott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kooi_T/0/1/0/all/0/1\">Thijs Kooi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kozubek_M/0/1/0/all/0/1\">Michal Kozubek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Litjens_G/0/1/0/all/0/1\">Geert Litjens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus Maier-Hein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mattson_P/0/1/0/all/0/1\">Peter Mattson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meijering_E/0/1/0/all/0/1\">Erik Meijering</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moher_D/0/1/0/all/0/1\">David Moher</a>, et al. (29 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. (arXiv:2105.07112v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07112","description":"<p>In this paper, we present an efficient and robust deep learning solution for\nnovel view synthesis of complex scenes. In our approach, a 3D scene is\nrepresented as a light field, i.e., a set of rays, each of which has a\ncorresponding color when reaching the image plane. For efficient novel view\nrendering, we adopt a two-plane parameterization of the light field, where each\nray is characterized by a 4D parameter. We then formulate the light field as a\n4D function that maps 4D coordinates to corresponding color values. We train a\ndeep fully connected network to optimize this implicit function and memorize\nthe 3D scene. Then, the scene-specific model is used to synthesize novel views.\nDifferent from previous light field approaches which require dense view\nsampling to reliably render novel views, our method can render novel views by\nsampling rays and querying the color for each ray from the network directly,\nthus enabling high-quality light field rendering with a sparser set of training\nimages. Per-ray depth can be optionally predicted by the network, thus enabling\napplications such as auto refocus. Our novel view synthesis results are\ncomparable to the state-of-the-arts, and even superior in some challenging\nscenes with refraction and reflection. We achieve this while maintaining an\ninteractive frame rate and a small memory footprint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Liangchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Celong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09785","description":"<p>This paper investigates two techniques for developing efficient\nself-supervised vision transformers (EsViT) for visual representation learning.\nFirst, we show through a comprehensive empirical study that multi-stage\narchitectures with sparse self-attentions can significantly reduce modeling\ncomplexity but with a cost of losing the ability to capture fine-grained\ncorrespondences between image regions. Second, we propose a new pre-training\ntask of region matching which allows the model to capture fine-grained region\ndependencies and as a result significantly improves the quality of the learned\nvision representations. Our results show that combining the two techniques,\nEsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,\noutperforming prior arts with around an order magnitude of higher throughput.\nWhen transferring to downstream linear classification tasks, EsViT outperforms\nits supervised counterpart on 17 out of 18 datasets. The code and models are\npublicly available: https://github.com/microsoft/esvit\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TA2N: Two-Stage Action Alignment Network for Few-shot Action Recognition. (arXiv:2107.04782v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04782","description":"<p>Few-shot action recognition aims to recognize novel action classes (query)\nusing just a few samples (support). The majority of current approaches follow\nthe metric learning paradigm, which learns to compare the similarity between\nvideos. Recently, it has been observed that directly measuring this similarity\nis not ideal since different action instances may show distinctive temporal\ndistribution, resulting in severe misalignment issues across query and support\nvideos. In this paper, we arrest this problem from two distinct aspects --\naction duration misalignment and action evolution misalignment. We address them\nsequentially through a Two-stage Action Alignment Network (TA2N). The first\nstage locates the action by learning a temporal affine transform, which warps\neach video feature to its action duration while dismissing the\naction-irrelevant feature (e.g. background). Next, the second stage coordinates\nquery feature to match the spatial-temporal action evolution of support by\nperforming temporally rearrange and spatially offset prediction. Extensive\nexperiments on benchmark datasets show the potential of the proposed method in\nachieving state-of-the-art performance for few-shot action recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1\">Mengjuan Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13802","description":"<p>Depth completion deals with the problem of recovering dense depth maps from\nsparse ones, where color images are often used to facilitate this task. Recent\napproaches mainly focus on image guided learning frameworks to predict dense\ndepth. However, blurry guidance in the image and unclear structure in the depth\nstill impede the performance of the image guided frameworks. To tackle these\nproblems, we explore a repetitive design in our image guided network to\ngradually and sufficiently recover depth values. Specifically, the repetition\nis embodied in both the image guidance branch and depth generation branch. In\nthe former branch, we design a repetitive hourglass network to extract\ndiscriminative image features of complex environments, which can provide\npowerful contextual instruction for depth prediction. In the latter branch, we\nintroduce a repetitive guidance module based on dynamic convolution, in which\nan efficient convolution factorization is proposed to simultaneously reduce its\ncomplexity and progressively model high-frequency structures. Extensive\nexperiments show that our method achieves superior or competitive results on\nKITTI benchmark and NYUv2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baobei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Person Re-identification with Stochastic Training Strategy. (arXiv:2108.06938v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06938","description":"<p>Unsupervised person re-identification (re-ID) has attracted increasing\nresearch interests because of its scalability and possibility for real-world\napplications. State-of-the-art unsupervised re-ID methods usually follow a\nclustering-based strategy, which generates pseudo labels by clustering and\nmaintains a memory to store instance features and represent the centroid of the\nclusters for contrastive learning. This approach suffers two problems. First,\nthe centroid generated by unsupervised learning may not be a perfect prototype.\nForcing images to get closer to the centroid emphasizes the result of\nclustering, which could accumulate clustering errors during iterations. Second,\nprevious methods utilize features obtained at different training iterations to\nrepresent one centroid, which is not consistent with the current training\nsample, since the features are not directly comparable. To this end, we propose\nan unsupervised re-ID approach with a stochastic learning strategy.\nSpecifically, we adopt a stochastic updated memory, where a random instance\nfrom a cluster is used to update the cluster-level memory for contrastive\nlearning. In this way, the relationship between randomly selected pair of\nimages are learned to avoid the training bias caused by unreliable pseudo\nlabels. The stochastic memory is also always up-to-date for classifying to keep\nthe consistency. Besides, to relieve the issue of camera variance, a unified\ndistance matrix is proposed during clustering, where the distance bias from\ndifferent camera domain is reduced and the variances of identities is\nemphasized.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v5 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2109.05539","description":"<p>Brain-inspired computation and information processing alongside compatibility\nwith neuromorphic hardware have made spiking neural networks (SNN) a promising\nmethod for solving learning tasks in machine learning (ML). Spiking neurons are\nonly one of the requirements for building a bio-plausible learning model.\nNetwork architecture and learning rules are other important factors to consider\nwhen developing such artificial agents. In this work, inspired by the human\nvisual pathway and the role of dopamine in learning, we propose a\nreward-modulated locally connected spiking neural network, BioLCNet, for visual\nlearning tasks. To extract visual features from Poisson-distributed spike\ntrains, we used local filters that are more analogous to the biological visual\nsystem compared to convolutional filters with weight sharing. In the decoding\nlayer, we applied a spike population-based voting scheme to determine the\ndecision of the network. We employed Spike-timing-dependent plasticity (STDP)\nfor learning the visual features, and its reward-modulated variant (R-STDP) for\ntraining the decoder based on the reward or punishment feedback signal. For\nevaluation, we first assessed the robustness of our rewarding mechanism to\nvarying target responses in a classical conditioning experiment. Afterwards, we\nevaluated the performance of our network on image classification tasks of MNIST\nand XOR MNIST datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_E/0/1/0/all/0/1\">Erfan Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1\">Saeed Reza Kheradpisheh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Space Time Recurrent Memory Network. (arXiv:2109.06474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06474","description":"<p>Transformers have recently been popular for learning and inference in the\nspatial-temporal domain. However, their performance relies on storing and\napplying attention to the feature tensor of each frame in video. Hence, their\nspace and time complexity increase linearly as the length of video grows, which\ncould be very costly for long videos. We propose a novel visual memory network\narchitecture for the learning and inference problem in the spatial-temporal\ndomain. We maintain a fixed set of memory slots in our memory network and\npropose an algorithm based on Gumbel-Softmax to learn an adaptive strategy to\nupdate this memory. Finally, this architecture is benchmarked on the video\nobject segmentation (VOS) and video prediction problems. We demonstrate that\nour memory architecture achieves state-of-the-art results, outperforming\ntransformer-based methods on VOS and other recent methods on video prediction\nwhile maintaining constant memory capacity independent of the sequence length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fuxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14051","description":"<p>Machine learning models often encounter samples that are diverged from the\ntraining distribution. Failure to recognize an out-of-distribution (OOD)\nsample, and consequently assign that sample to an in-class label significantly\ncompromises the reliability of a model. The problem has gained significant\nattention due to its importance for safety deploying models in open-world\nsettings. Detecting OOD samples is challenging due to the intractability of\nmodeling all possible unknown distributions. To date, several research domains\ntackle the problem of detecting unfamiliar samples, including anomaly\ndetection, novelty detection, one-class learning, open set recognition, and\nout-of-distribution detection. Despite having similar and shared concepts,\nout-of-distribution, open-set, and anomaly detection have been investigated\nindependently. Accordingly, these research avenues have not cross-pollinated,\ncreating research barriers. While some surveys intend to provide an overview of\nthese approaches, they seem to only focus on a specific domain without\nexamining the relationship between different domains. This survey aims to\nprovide a cross-domain and comprehensive review of numerous eminent works in\nrespective areas while identifying their commonalities. Researchers can benefit\nfrom the overview of research advances in different fields and develop future\nmethodology synergistically. Furthermore, to the best of our knowledge, while\nthere are surveys in anomaly detection or one-class learning, there is no\ncomprehensive or up-to-date survey on out-of-distribution detection, which our\nsurvey covers extensively. Finally, having a unified cross-domain perspective,\nwe discuss and shed light on future lines of research, intending to bring these\nfields closer together.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_H/0/1/0/all/0/1\">Hossein Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Human Trajectory Forecasting with a Wearable Camera and Multi-Modal Fusion. (arXiv:2111.00993v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00993","description":"<p>In this paper, we address the problem of forecasting the trajectory of an\negocentric camera wearer (ego-person) in crowded spaces. The trajectory\nforecasting ability learned from the data of different camera wearers walking\naround in the real world can be transferred to assist visually impaired people\nin navigation, as well as to instill human navigation behaviours in mobile\nrobots, enabling better human-robot interactions. To this end, a novel\negocentric human trajectory forecasting dataset was constructed, containing\nreal trajectories of people navigating in crowded spaces wearing a camera, as\nwell as extracted rich contextual data. We extract and utilize three different\nmodalities to forecast the trajectory of the camera wearer, i.e., his/her past\ntrajectory, the past trajectories of nearby people, and the environment such as\nthe scene semantics or the depth of the scene. A Transformer-based\nencoder-decoder neural network model, integrated with a novel cascaded\ncross-attention mechanism that fuses multiple modalities, has been designed to\npredict the future trajectory of the camera wearer. Extensive experiments have\nbeen conducted, with results showing that our model outperforms the\nstate-of-the-art methods in egocentric human trajectory forecasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Ya-Yen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiankai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition and Style Attributes Guided Image Aesthetic Assessment. (arXiv:2111.04647v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.04647","description":"<p>The aesthetic quality of an image is defined as the measure or appreciation\nof the beauty of an image. Aesthetics is inherently a subjective property but\nthere are certain factors that influence it such as, the semantic content of\nthe image, the attributes describing the artistic aspect, the photographic\nsetup used for the shot, etc. In this paper we propose a method for the\nautomatic prediction of the aesthetics of an image that is based on the\nanalysis of the semantic content, the artistic style and the composition of the\nimage. The proposed network includes: a pre-trained network for semantic\nfeatures extraction (the Backbone); a Multi Layer Perceptron (MLP) network that\nrelies on the Backbone features for the prediction of image attributes (the\nAttributeNet); a self-adaptive Hypernetwork that exploits the attributes prior\nencoded into the embedding generated by the AttributeNet to predict the\nparameters of the target network dedicated to aesthetic estimation (the\nAestheticNet). Given an image, the proposed multi-network is able to predict:\nstyle and composition attributes, and aesthetic score distribution. Results on\nthree benchmark datasets demonstrate the effectiveness of the proposed method,\nwhile the ablation study gives a better understanding of the proposed network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Celona_L/0/1/0/all/0/1\">Luigi Celona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardi_M/0/1/0/all/0/1\">Marco Leonardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napoletano_P/0/1/0/all/0/1\">Paolo Napoletano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1\">Alessandro Rozza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Equivalence between Neural Network and Support Vector Machine. (arXiv:2111.06063v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2111.06063","description":"<p>Recent research shows that the dynamics of an infinitely wide neural network\n(NN) trained by gradient descent can be characterized by Neural Tangent Kernel\n(NTK) \\citep{jacot2018neural}. Under the squared loss, the infinite-width NN\ntrained by gradient descent with an infinitely small learning rate is\nequivalent to kernel regression with NTK \\citep{arora2019exact}. However, the\nequivalence is only known for ridge regression currently\n\\citep{arora2019harnessing}, while the equivalence between NN and other kernel\nmachines (KMs), e.g. support vector machine (SVM), remains unknown. Therefore,\nin this work, we propose to establish the equivalence between NN and SVM, and\nspecifically, the infinitely wide NN trained by soft margin loss and the\nstandard soft margin SVM with NTK trained by subgradient descent. Our main\ntheoretical results include establishing the equivalences between NNs and a\nbroad family of $\\ell_2$ regularized KMs with finite-width bounds, which cannot\nbe handled by prior work, and showing that every finite-width NN trained by\nsuch regularized loss functions is approximately a KM. Furthermore, we\ndemonstrate our theory can enable three practical applications, including (i)\n\\textit{non-vacuous} generalization bound of NN via the corresponding KM; (ii)\n\\textit{non-trivial} robustness certificate for the infinite-width NN (while\nexisting robustness verification methods would provide vacuous bounds); (iii)\nintrinsically more robust infinite-width NNs than those from previous kernel\nregression. Our code for the experiments is available at\n\\url{https://github.com/leslie-CH/equiv-nn-svm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yilan Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_L/0/1/0/all/0/1\">Lam M. Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weng_T/0/1/0/all/0/1\">Tsui-Wei Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Image Generation with Mixup-based Distance Learning. (arXiv:2111.11672v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11672","description":"<p>Producing diverse and realistic images with generative models such as GANs\ntypically requires large scale training with vast amount of images. GANs\ntrained with limited data can easily memorize few training samples and display\nundesirable properties like \"stairlike\" latent space where interpolation in the\nlatent space yields discontinuous transitions in the output space. In this\nwork, we consider a challenging task of pretraining-free few-shot image\nsynthesis, and seek to train existing generative models with minimal\noverfitting and mode collapse. We propose mixup-based distance regularization\non the feature space of both a generator and the counterpart discriminator that\nencourages the two players to reason not only about the scarce observed data\npoints but the relative distances in the feature space they reside. Qualitative\nand quantitative evaluation on diverse datasets demonstrates that our method is\ngenerally applicable to existing models to enhance both fidelity and diversity\nunder few-shot setting. Code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chaerin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeesoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Donghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepAdversaries: Examining the Robustness of Deep Learning Models for Galaxy Morphology Classification. (arXiv:2112.14299v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.14299","description":"<p>With increased adoption of supervised deep learning methods for processing\nand analysis of cosmological survey data, the assessment of data perturbation\neffects (that can naturally occur in the data processing and analysis\npipelines) and the development of methods that increase model robustness are\nincreasingly important. In the context of morphological classification of\ngalaxies, we study the effects of perturbations in imaging data. In particular,\nwe examine the consequences of using neural networks when training on baseline\ndata and testing on perturbed data. We consider perturbations associated with\ntwo primary sources: 1) increased observational noise as represented by higher\nlevels of Poisson noise and 2) data processing noise incurred by steps such as\nimage compression or telescope errors as represented by one-pixel adversarial\nattacks. We also test the efficacy of domain adaptation techniques in\nmitigating the perturbation-driven errors. We use classification accuracy,\nlatent space visualizations, and latent space distance to assess model\nrobustness. Without domain adaptation, we find that processing pixel-level\nerrors easily flip the classification into an incorrect class and that higher\nobservational noise makes the model trained on low-noise data unable to\nclassify galaxy morphologies. On the other hand, we show that training with\ndomain adaptation improves model robustness and mitigates the effects of these\nperturbations, improving the classification accuracy by 23% on data with higher\nobservational noise. Domain adaptation also increases by a factor of ~2.3 the\nlatent space distance between the baseline and the incorrectly classified\none-pixel perturbed image, making the model more robust to inadvertent\nperturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciprijanovic_A/0/1/0/all/0/1\">Aleksandra &#x106;iprijanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafkes_D/0/1/0/all/0/1\">Diana Kafkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snyder_G/0/1/0/all/0/1\">Gregory Snyder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_F/0/1/0/all/0/1\">F. Javier S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdue_G/0/1/0/all/0/1\">Gabriel Nathan Perdue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedro_K/0/1/0/all/0/1\">Kevin Pedro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nord_B/0/1/0/all/0/1\">Brian Nord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madireddy_S/0/1/0/all/0/1\">Sandeep Madireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_S/0/1/0/all/0/1\">Stefan M. Wild</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore and Match: A New Paradigm for Temporal Video Grounding with Natural Language. (arXiv:2201.10168v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10168","description":"<p>Temporal Video Grounding (TVG) aims to localize time segments in an untrimmed\nvideo according to natural language queries. In this work, we present a new\nparadigm named Explore-and-Match for TVG that seamlessly unifies two streams of\nTVG methods: proposal-free and proposal-based; the former explores the search\nspace to find segments directly, and the latter matches the predefined\nproposals with ground truths. To achieve this goal, we view TVG as a set\nprediction problem and design an end-to-end trainable Language Video\nTransformer (LVTR) that utilizes the architectural strengths of rich\ncontextualization and parallel decoding for set prediction. The overall\ntraining schedule is balanced by two key losses that play different roles,\nnamely temporal localization loss and set guidance loss. These two losses allow\neach proposal to regress the target segment and identify the target query. More\nspecifically, LVTR first explores the search space to diversify the initial\nproposals, and then matches the proposals to the corresponding targets to align\nthem in a fine-grained manner. The Explore-and-Match scheme successfully\ncombines the strengths of two complementary methods without encoding prior\nknowledge (e.g., non-maximum suppression) into the TVG pipeline. As a result,\nLVTR sets new state-of-the-art results on two TVG benchmarks (ActivityCaptions\nand Charades-STA) with double the inference speed. Codes are available at\nhttps://github.com/sangminwoo/Explore-and-Match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_I/0/1/0/all/0/1\">Inyong Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sumin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minki Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Understanding of Self-Supervised Representations. (arXiv:2203.01881v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01881","description":"<p>Self-supervised learning methods have shown impressive results in downstream\nclassification tasks. However, there is limited work in understanding and\ninterpreting their learned representations. In this paper, we study the\nrepresentation space of several state-of-the-art self-supervised models\nincluding SimCLR, SwaV, MoCo V2 and BYOL. Without the use of class label\ninformation, we first discover discriminative features that are highly active\nfor various subsets of samples and correspond to unique physical attributes in\nimages. We show that, using such discriminative features, one can compress the\nrepresentation space of self-supervised models up to 50% without affecting\ndownstream linear classification significantly. Next, we propose a sample-wise\nSelf-Supervised Representation Quality Score (or, Q-Score) that can be computed\nwithout access to any label information. Q-Score, utilizes discriminative\nfeatures to reliably predict if a given sample is likely to be mis-classified\nin the downstream classification task achieving AUPRC of 0.91 on SimCLR and\nBYOL trained on ImageNet-100. Q-Score can also be used as a regularization term\nto remedy low-quality representations leading up to 8% relative improvement in\naccuracy on all 4 self-supervised baselines on ImageNet-100, CIFAR-10,\nCIFAR-100 and STL-10. Moreover, through heatmap analysis, we show that Q-Score\nregularization enhances discriminative features and reduces feature noise, thus\nimproving model interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1\">Neha Kalibhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_K/0/1/0/all/0/1\">Kanika Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Automotive mmWave Radar Detection Points Based Instance Segmentation. (arXiv:2203.06553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06553","description":"<p>The automotive mmWave radar plays a key role in advanced driver assistance\nsystems (ADAS) and autonomous driving. Deep learning-based instance\nsegmentation enables real-time object identification from the radar detection\npoints. In the conventional training process, accurate annotation is the key.\nHowever, high-quality annotations of radar detection points are challenging to\nachieve due to their ambiguity and sparsity. To address this issue, we propose\na contrastive learning approach for implementing radar detection points-based\ninstance segmentation. We define the positive and negative samples according to\nthe ground-truth label, apply the contrastive loss to train the model first,\nand then perform fine-tuning for the following downstream task. In addition,\nthese two steps can be merged into one, and pseudo labels can be generated for\nthe unlabeled data to improve the performance further. Thus, there are four\ndifferent training settings for our method. Experiments show that when the\nground-truth information is only available for a small proportion of the\ntraining data, our method still achieves a comparable performance to the\napproach trained in a supervised manner with 100% ground-truth information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weiyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training. (arXiv:2203.12602v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12602","description":"<p>Pre-training video transformers on extra large-scale datasets is generally\nrequired to achieve premier performance on relatively small datasets. In this\npaper, we show that video masked autoencoders (VideoMAE) are data-efficient\nlearners for self-supervised video pre-training (SSVP). We are inspired by the\nrecent ImageMAE and propose customized video tube masking with an extremely\nhigh ratio. This simple design makes video reconstruction a more challenging\nself-supervision task, thus encouraging extracting more effective video\nrepresentations during this pre-training process. We obtain three important\nfindings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90%\nto 95%) still yields favorable performance of VideoMAE. The temporally\nredundant video content enables a higher masking ratio than that of images. (2)\nVideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k\nvideos) without using any extra data. (3) VideoMAE shows that data quality is\nmore important than data quantity for SSVP. Domain shift between pre-training\nand target datasets is an important issue. Notably, our VideoMAE with the\nvanilla ViT can achieve 85.8% on Kinetics-400, 75.3% on Something-Something V2,\n90.8% on UCF101, and 61.1% on HMDB51, without using any extra data. Code is\navailable at https://github.com/MCG-NJU/VideoMAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation. (arXiv:2204.07548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07548","description":"<p>Recent works on 3D semantic segmentation propose to exploit the synergy\nbetween images and point clouds by processing each modality with a dedicated\nnetwork and projecting learned 2D features onto 3D points. Merging large-scale\npoint clouds and images raises several challenges, such as constructing a\nmapping between points and pixels, and aggregating features between multiple\nviews. Current methods require mesh reconstruction or specialized sensors to\nrecover occlusions, and use heuristics to select and aggregate available\nimages. In contrast, we propose an end-to-end trainable multi-view aggregation\nmodel leveraging the viewing conditions of 3D points to merge features from\nimages taken at arbitrary positions. Our method can combine standard 2D and 3D\nnetworks and outperforms both 3D models operating on colorized point clouds and\nhybrid 2D/3D networks without requiring colorization, meshing, or true depth\nmaps. We set a new state-of-the-art for large-scale indoor/outdoor semantic\nsegmentation on S3DIS (74.7 mIoU 6-Fold) and on KITTI-360 (58.3 mIoU). Our full\npipeline is accessible at https://github.com/drprojects/DeepViewAgg, and only\nrequires raw 3D scans and a set of images and poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robert_D/0/1/0/all/0/1\">Damien Robert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallet_B/0/1/0/all/0/1\">Bruno Vallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Y-Net: A Spatiospectral Dual-Encoder Networkfor Medical Image Segmentation. (arXiv:2204.07613v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.07613","description":"<p>Automated segmentation of retinal optical coherence tomography (OCT) images\nhas become an important recent direction in machine learning for medical\napplications. We hypothesize that the anatomic structure of layers and their\nhigh-frequency variation in OCT images make retinal OCT a fitting choice for\nextracting spectral-domain features and combining them with spatial domain\nfeatures. In this work, we present $\\Upsilon$-Net, an architecture that\ncombines the frequency domain features with the image domain to improve the\nsegmentation performance of OCT images. The results of this work demonstrate\nthat the introduction of two branches, one for spectral and one for spatial\ndomain features, brings a very significant improvement in fluid segmentation\nperformance and allows outperformance as compared to the well-known U-Net\nmodel. Our improvement was 13% on the fluid segmentation dice score and 1.9% on\nthe average dice score. Finally, removing selected frequency ranges in the\nspectral domain demonstrates the impact of these features on the fluid\nsegmentation outperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Farshad_A/0/1/0/all/0/1\">Azade Farshad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeganeh_Y/0/1/0/all/0/1\">Yousef Yeganeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gehlbach_P/0/1/0/all/0/1\">Peter Gehlbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10965","description":"<p>In this paper, we propose CLIP-Dissect, a new technique to automatically\ndescribe the function of individual hidden neurons inside vision networks.\nCLIP-Dissect leverages recent advances in multimodal vision/language models to\nlabel internal neurons with open-ended concepts without the need for any\nlabeled data or human examples, which are required for existing tools to\nsucceed. We show that CLIP-Dissect provides more accurate descriptions than\nexisting methods for last layer neurons where the ground-truth is available as\nwell as qualitatively good descriptions for hidden layer neurons. In addition,\nour method is very flexible: it is model agnostic, can easily handle new\nconcepts and can be extended to take advantage of better multimodal models in\nthe future. Finally CLIP-Dissect is computationally efficient and can label all\nneurons from five layers of ResNet-50 in just four minutes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oikarinen_T/0/1/0/all/0/1\">Tuomas Oikarinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1\">Tsui-Wei Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An asynchronous event-based algorithm for periodic signals. (arXiv:2205.04691v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04691","description":"<p>In this paper, we present a simple event-oriented algorithm for detection of\npixel-size signals with a known frequency, by the novel technology of an event\ncamera. In addition, we analyze the ability of the algorithm to filter out the\ndesired periodic signals from random fluctuations. We demonstrate this ability\nand show how the algorithm can distinguish, during twilight, between the\nsignals of a streetlight that flicker with frequency of 100 Hz, and sun glitter\noriginating from windows in far-away buildings in the field of view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Ezra_D/0/1/0/all/0/1\">David El-Chai Ben-Ezra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arad_R/0/1/0/all/0/1\">Ron Arad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padowicz_A/0/1/0/all/0/1\">Ayelet Padowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tugendhaft_I/0/1/0/all/0/1\">Israel Tugendhaft</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cardiomegaly Detection using Deep Convolutional Neural Network with U-Net. (arXiv:2205.11515v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.11515","description":"<p>Cardiomegaly is indeed a medical disease in which the heart is enlarged.\nCardiomegaly is better to handle if caught early, so early detection is\ncritical. The chest X-ray, being one of the most often used radiography\nexaminations, has been used to detect and visualize abnormalities of human\norgans for decades. X-ray is also a significant medical diagnosis tool for\ncardiomegaly. Even for domain experts, distinguishing the many types of\ndiseases from the X-ray is a difficult and time-consuming task. Deep learning\nmodels are also most effective when used on huge data sets, yet due to privacy\nconcerns, large datasets are rarely available inside the medical industry. A\nDeep learning-based customized retrained U-Net model for detecting Cardiomegaly\ndisease is presented in this research. In the training phase, chest X-ray\nimages from the \"ChestX-ray8\" open source real dataset are used. To reduce\ncomputing time, this model performs data preprocessing, picture improvement,\nimage compression, and classification before moving on to the training step.\nThe work used a chest x-ray image dataset to simulate and produced a diagnostic\naccuracy of 94%, a sensitivity of 96.2 percent, and a specificity of 92.5\npercent, which beats prior pre-trained model findings for identifying\nCardiomegaly disease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sarpotdar_S/0/1/0/all/0/1\">Soham S.Sarpotdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12693","description":"<p>Self-supervised learning has achieved a great success in the representation\nlearning of visual and textual data. However, the current methods are mainly\nvalidated on the well-curated datasets, which do not exhibit the real-world\nlong-tailed distribution. Recent attempts to consider self-supervised\nlong-tailed learning are made by rebalancing in the loss perspective or the\nmodel perspective, resembling the paradigms in the supervised long-tailed\nlearning. Nevertheless, without the aid of labels, these explorations have not\nshown the expected significant promise due to the limitation in tail sample\ndiscovery or the heuristic structure design. Different from previous works, we\nexplore this direction from an alternative perspective, i.e., the data\nperspective, and propose a novel Boosted Contrastive Learning (BCL) method.\nSpecifically, BCL leverages the memorization effect of deep neural networks to\nautomatically drive the information discrepancy of the sample views in\ncontrastive learning, which is more efficient to enhance the long-tailed\nlearning in the label-unaware context. Extensive experiments on a range of\nbenchmark datasets demonstrate the effectiveness of BCL over several\nstate-of-the-art methods. Our code is available at\nhttps://github.com/MediaBrain-SJTU/BCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13326","description":"<p>This paper describes the methods submitted for evaluation to the SHREC 2022\ntrack on pothole and crack detection in the road pavement. A total of 7\ndifferent runs for the semantic segmentation of the road surface are compared,\n6 from the participants plus a baseline method. All methods exploit Deep\nLearning techniques and their performance is tested using the same environment\n(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic\nsegmentation image/mask pairs and 797 RGB-D video clips collected with the\nlatest depth cameras was made available to the participants. The methods are\nthen evaluated on the 496 image/mask pairs in the validation set, on the 504\npairs in the test set and finally on 8 video clips. The analysis of the results\nis based on quantitative metrics for image segmentation and qualitative\nanalysis of the video clips. The participation and the results show that the\nscenario is of great interest and that the use of RGB-D data is still\nchallenging in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_E/0/1/0/all/0/1\">Elia Moscoso Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranieri_A/0/1/0/all/0/1\">Andrea Ranieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biasotti_S/0/1/0/all/0/1\">Silvia Biasotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chicchon_M/0/1/0/all/0/1\">Miguel Chicchon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sipiran_I/0/1/0/all/0/1\">Ivan Sipiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh-Khoi Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Ho_T/0/1/0/all/0/1\">Thang-Long Nguyen-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai-Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fitting and recognition of geometric primitives in segmented 3D point clouds using a localized voting procedure. (arXiv:2205.15426v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15426","description":"<p>The automatic creation of geometric models from point clouds has numerous\napplications in CAD (e.g., reverse engineering, manufacturing, assembling) and,\nmore in general, in shape modelling and processing. Given a segmented point\ncloud representing a man-made object, we propose a method for recognizing\nsimple geometric primitives and their interrelationships. Our approach is based\non the Hough transform (HT) for its ability to deal with noise, missing parts\nand outliers. In our method we introduce a novel technique for processing\nsegmented point clouds that, through a voting procedure, is able to provide an\ninitial estimate of the geometric parameters characterizing each primitive\ntype. By using these estimates, we localize the search of the optimal solution\nin a dimensionally-reduced parameter space thus making it efficient to extend\nthe HT to more primitives than those that are generally found in the\nliterature, i.e. planes and spheres. Then, we extract a number of geometric\ndescriptors that uniquely characterize a segment, and, on the basis of these\ndescriptors, we show how to aggregate parts of primitives (segments).\nExperiments on both synthetic and industrial scans reveal the robustness of the\nprimitive fitting method and its effectiveness for inferring relations among\nsegments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raffo_A/0/1/0/all/0/1\">Andrea Raffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanengo_C/0/1/0/all/0/1\">Chiara Romanengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcidieno_B/0/1/0/all/0/1\">Bianca Falcidieno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biasotti_S/0/1/0/all/0/1\">Silvia Biasotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metrics reloaded: Pitfalls and recommendations for image analysis validation. (arXiv:2206.01653v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01653","description":"<p>The field of automatic biomedical image analysis crucially depends on robust\nand meaningful performance metrics for algorithm validation. Current metric\nusage, however, is often ill-informed and does not reflect the underlying\ndomain interest. Here, we present a comprehensive framework that guides\nresearchers towards choosing performance metrics in a problem-aware manner.\nSpecifically, we focus on biomedical image analysis problems that can be\ninterpreted as a classification task at image, object or pixel level. The\nframework first compiles domain interest-, target structure-, data set- and\nalgorithm output-related properties of a given problem into a problem\nfingerprint, while also mapping it to the appropriate problem category, namely\nimage-level classification, semantic segmentation, instance segmentation, or\nobject detection. It then guides users through the process of selecting and\napplying a set of appropriate validation metrics while making them aware of\npotential pitfalls related to individual choices. In this paper, we describe\nthe current status of the Metrics Reloaded recommendation framework, with the\ngoal of obtaining constructive feedback from the image analysis community. The\ncurrent version has been developed within an international consortium of more\nthan 60 image analysis experts and will be made openly available as a\nuser-friendly toolkit after community-driven optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulou_E/0/1/0/all/0/1\">Evangelia Christodoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozubek_M/0/1/0/all/0/1\">Michal Kozubek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">Mauricio Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1\">Manuel Wiesenfarth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_M/0/1/0/all/0/1\">Michael Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavur_A/0/1/0/all/0/1\">A. Emre Kavur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radsch_T/0/1/0/all/0/1\">Tim R&#xe4;dsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acion_L/0/1/0/all/0/1\">Laura Acion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonelli_M/0/1/0/all/0/1\">Michela Antonelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bankhead_P/0/1/0/all/0/1\">Peter Bankhead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benis_A/0/1/0/all/0/1\">Arriel Benis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimini_B/0/1/0/all/0/1\">Beth Cimini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_G/0/1/0/all/0/1\">Gary S. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1\">Michael M. Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huisman_M/0/1/0/all/0/1\">Merel Huisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannin_P/0/1/0/all/0/1\">Pierre Jannin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1\">Charles E. Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karargyris_A/0/1/0/all/0/1\">Alexandros Karargyris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes Kenngott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1\">Annette Kopp-Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litjens_G/0/1/0/all/0/1\">Geert Litjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattson_P/0/1/0/all/0/1\">Peter Mattson</a>, et al. (21 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixGen: A New Multi-Modal Data Augmentation. (arXiv:2206.08358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08358","description":"<p>Data augmentation is a necessity to enhance data efficiency in deep learning.\nFor vision-language pre-training, data is only augmented either for images or\nfor text in previous works. In this paper, we present MixGen: a joint data\naugmentation for vision-language representation learning to further improve\ndata efficiency. It generates new image-text pairs with semantic relationships\npreserved by interpolating images and concatenating text. It's simple, and can\nbe plug-and-played into existing pipelines. We evaluate MixGen on four\narchitectures, including CLIP, ViLT, ALBEF and TCL, across five downstream\nvision-language tasks to show its versatility and effectiveness. For example,\nadding MixGen in ALBEF pre-training leads to absolute performance improvements\non downstream tasks: image-text retrieval (+6.2% on COCO fine-tuned and +5.3%\non Flicker30K zero-shot), visual grounding (+0.9% on RefCOCO+), visual\nreasoning (+0.9% on NLVR$^{2}$), visual question answering (+0.3% on VQA2.0),\nand visual entailment (+0.4% on SNLI-VE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xiaoshuai Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel adversarial learning strategy for medical image classification. (arXiv:2206.11501v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.11501","description":"<p>Deep learning (DL) techniques have been extensively utilized for medical\nimage classification. Most DL-based classification networks are generally\nstructured hierarchically and optimized through the minimization of a single\nloss function measured at the end of the networks. However, such a single loss\ndesign could potentially lead to optimization of one specific value of interest\nbut fail to leverage informative features from intermediate layers that might\nbenefit classification performance and reduce the risk of overfitting.\nRecently, auxiliary convolutional neural networks (AuxCNNs) have been employed\non top of traditional classification networks to facilitate the training of\nintermediate layers to improve classification performance and robustness. In\nthis study, we proposed an adversarial learning-based AuxCNN to support the\ntraining of deep neural networks for medical image classification. Two main\ninnovations were adopted in our AuxCNN classification framework. First, the\nproposed AuxCNN architecture includes an image generator and an image\ndiscriminator for extracting more informative image features for medical image\nclassification, motivated by the concept of generative adversarial network\n(GAN) and its impressive ability in approximating target data distribution.\nSecond, a hybrid loss function is designed to guide the model training by\nincorporating different objectives of the classification network and AuxCNN to\nreduce overfitting. Comprehensive experimental studies demonstrated the\nsuperior classification performance of the proposed model. The effect of the\nnetwork-related factors on classification performance was investigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_Z/0/1/0/all/0/1\">Zong Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gasienica_J/0/1/0/all/0/1\">Jacob A. Gasienica</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Potts_J/0/1/0/all/0/1\">Jennifer Potts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thorstad_W/0/1/0/all/0/1\">Wade Thorstad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gay_H/0/1/0/all/0/1\">Hiram Gay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_P/0/1/0/all/0/1\">Pengfei Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaowei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need. (arXiv:2206.11804v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11804","description":"<p>Data diversity and volume are crucial to the success of training deep\nlearning models, while in the medical imaging field, the difficulty and cost of\ndata collection and annotation are especially huge. Specifically in robotic\nsurgery, data scarcity and imbalance have heavily affected the model accuracy\nand limited the design and deployment of deep learning-based surgical\napplications such as surgical instrument segmentation. Considering this, we\nrethink the surgical instrument segmentation task and propose a one-to-many\ndata generation solution that gets rid of the complicated and expensive process\nof data collection and annotation from robotic surgery. In our method, we only\nutilize a single surgical background tissue image and a few open-source\ninstrument images as the seed images and apply multiple augmentations and\nblending techniques to synthesize amounts of image variations. In addition, we\nalso introduce the chained augmentation mixing during training to further\nenhance the data diversities. The proposed approach is evaluated on the real\ndatasets of the EndoVis-2018 and EndoVis-2017 surgical scene segmentation. Our\nempirical analysis suggests that without the high cost of data collection and\nannotation, we can achieve decent surgical instrument segmentation performance.\nMoreover, we also observe that our method can deal with novel instrument\nprediction in the deployment domain. We hope our inspiring results will\nencourage researchers to emphasize data-centric methods to overcome demanding\ndeep learning limitations besides data shortage, such as class imbalance,\ndomain adaptation, and incremental learning. Our code is available at\nhttps://github.com/lofrienger/Single_SurgicalScene_For_Segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">An Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengya Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Schizophrenia with 3D Structural Brain MRI Using Deep Learning. (arXiv:2206.12980v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.12980","description":"<p>Schizophrenia is a chronic neuropsychiatric disorder that causes distinct\nstructural alterations within the brain. We hypothesize that deep learning\napplied to a structural neuroimaging dataset could detect disease-related\nalteration and improve classification and diagnostic accuracy. We tested this\nhypothesis using a single, widely available, and conventional T1-weighted MRI\nscan, from which we extracted the 3D whole-brain structure using standard\npost-processing methods. A deep learning model was then developed, optimized,\nand evaluated on three open datasets with T1-weighted MRI scans of patients\nwith schizophrenia. Our proposed model outperformed the benchmark model, which\nwas also trained with structural MR images using a 3D CNN architecture. Our\nmodel is capable of almost perfectly (area under the ROC curve = 0.987)\ndistinguishing schizophrenia patients from healthy controls on unseen\nstructural MRI scans. Regional analysis localized subcortical regions and\nventricles as the most predictive brain regions. Subcortical structures serve a\npivotal role in cognitive, affective, and social functions in humans, and\nstructural abnormalities of these regions have been associated with\nschizophrenia. Our finding corroborates that schizophrenia is associated with\nwidespread alterations in subcortical brain structure and the subcortical\nstructural information provides prominent features in diagnostic\nclassification. Together, these results further demonstrate the potential of\ndeep learning to improve schizophrenia diagnosis and identify its structural\nneuroimaging signatures from a single, standard T1-weighted brain MRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Junhao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rao_V/0/1/0/all/0/1\">Vishwanatha M. Rao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yanting Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acosta_N/0/1/0/all/0/1\">Nicolas Acosta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_Z/0/1/0/all/0/1\">Zihan Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_P/0/1/0/all/0/1\">Pin-Yu Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chloe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kegeles_L/0/1/0/all/0/1\">Lawrence S. Kegeles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Small_S/0/1/0/all/0/1\">Scott A. Small</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoT-SORT: Robust Associations Multi-Pedestrian Tracking. (arXiv:2206.14651v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.14651","description":"<p>The goal of multi-object tracking (MOT) is detecting and tracking all the\nobjects in a scene, while keeping a unique identifier for each object. In this\npaper, we present a new robust state-of-the-art tracker, which can combine the\nadvantages of motion and appearance information, along with camera-motion\ncompensation, and a more accurate Kalman filter state vector. Our new trackers\nBoT-SORT, and BoT-SORT-ReID rank first in the datasets of MOTChallenge [29, 11]\non both MOT17 and MOT20 test sets, in terms of all the main MOT metrics: MOTA,\nIDF1, and HOTA. For MOT17: 80.5 MOTA, 80.2 IDF1, and 65.0 HOTA are achieved.\nThe source code and the pre-trained models are available at\nhttps://github.com/NirAharon/BOT-SORT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aharon_N/0/1/0/all/0/1\">Nir Aharon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orfaig_R/0/1/0/all/0/1\">Roy Orfaig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobrovsky_B/0/1/0/all/0/1\">Ben-Zion Bobrovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patient-specific modelling, simulation and real time processing for constrictive respiratory diseases. (arXiv:2207.01082v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.01082","description":"<p>Asthma is a common chronic disease of the respiratory system causing\nsignificant disability and societal burden. It affects over 500 million people\nworldwide and generates costs exceeding $USD 56 billion in 2011 in the United\nStates. Managing asthma involves controlling symptoms, preventing\nexacerbations, and maintaining lung function. Improving asthma control affects\nthe daily life of patients and is associated with a reduced risk of\nexacerbations and lung function impairment, reduces the cost of asthma care and\nindirect costs associated with reduced productivity. Understanding the complex\ndynamics of the pulmonary system and the lung's response to disease, injury,\nand treatment is fundamental to the advancement of Asthma treatment.\nComputational models of the respiratory system seek to provide a theoretical\nframework to understand the interaction between structure and function. Their\napplication can improve pulmonary medicine by a patient-specific approach to\nmedicinal methodologies optimizing the delivery given the personalized geometry\nand personalized ventilation patterns while introducing a patient-specific\ntechnique that maximizes drug delivery. A three-fold objective addressed within\nthis dissertation becomes prominent at this point. The first part refers to the\ncomprehension of pulmonary pathophysiology and the mechanics of Asthma and\nsubsequently of constrictive pulmonary conditions in general. The second part\nrefers to the design and implementation of tools that facilitate personalized\nmedicine to improve delivery and effectiveness. Finally, the third part refers\nto the self-management of the condition, meaning that medical personnel and\npatients have access to tools and methods that allow the first party to easily\ntrack the course of the condition and the second party, i.e. the patient to\neasily self-manage it alleviating the significant burden from the health\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DecisioNet -- A Binary-Tree Structured Neural Network. (arXiv:2207.01127v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01127","description":"<p>Deep neural networks (DNNs) and decision trees (DTs) are both\nstate-of-the-art classifiers. DNNs perform well due to their representational\nlearning capabilities, while DTs are computationally efficient as they perform\ninference along one route (root-to-leaf) that is dependent on the input data.\nIn this paper, we present DecisioNet (DN), a binary-tree structured neural\nnetwork. We propose a systematic way to convert an existing DNN into a DN to\ncreate a lightweight version of the original model. DecisioNet takes the best\nof both worlds - it uses neural modules to perform representational learning\nand utilizes its tree structure to perform only a portion of the computations.\nWe evaluate various DN architectures, along with their corresponding baseline\nmodels on the FashionMNIST, CIFAR10, and CIFAR100 datasets. We show that the DN\nvariants achieve similar accuracy while significantly reducing the\ncomputational cost of the original network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gottlieb_N/0/1/0/all/0/1\">Noam Gottlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1\">Michael Werman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selectively increasing the diversity of GAN-generated samples. (arXiv:2207.01561v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01561","description":"<p>Generative Adversarial Networks (GANs) are powerful models able to synthesize\ndata samples closely resembling the distribution of real data, yet the\ndiversity of those generated samples is limited due to the so-called mode\ncollapse phenomenon observed in GANs. Especially prone to mode collapse are\nconditional GANs, which tend to ignore the input noise vector and focus on the\nconditional information. Recent methods proposed to mitigate this limitation\nincrease the diversity of generated samples, yet they reduce the performance of\nthe models when similarity of samples is required. To address this shortcoming,\nwe propose a novel method to selectively increase the diversity of\nGAN-generated samples. By adding a simple, yet effective regularization to the\ntraining loss function we encourage the generator to discover new data modes\nfor inputs related to diverse outputs while generating consistent samples for\nthe remaining ones. More precisely, we maximise the ratio of distances between\ngenerated images and input latent vectors scaling the effect according to the\ndiversity of samples for a given conditional input. We show the superiority of\nour method in a synthetic benchmark as well as a real-life scenario of\nsimulating data from the Zero Degree Calorimeter of ALICE experiment in LHC,\nCERN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubinski_J/0/1/0/all/0/1\">Jan Dubi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1\">Kamil Deja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzel_S/0/1/0/all/0/1\">Sandro Wenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokita_P/0/1/0/all/0/1\">Przemys&#x142;aw Rokita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Coding for Machines with Omnipotent Feature Learning. (arXiv:2207.01932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01932","description":"<p>Image Coding for Machines (ICM) aims to compress images for AI tasks analysis\nrather than meeting human perception. Learning a kind of feature that is both\ngeneral (for AI tasks) and compact (for compression) is pivotal for its\nsuccess. In this paper, we attempt to develop an ICM framework by learning\nuniversal features while also considering compression. We name such features as\nomnipotent features and the corresponding framework as Omni-ICM. Considering\nself-supervised learning (SSL) improves feature generalization, we integrate it\nwith the compression task into the Omni-ICM framework to learn omnipotent\nfeatures. However, it is non-trivial to coordinate semantics modeling in SSL\nand redundancy removing in compression, so we design a novel information\nfiltering (IF) module between them by co-optimization of instance\ndistinguishment and entropy minimization to adaptively drop information that is\nweakly related to AI tasks (e.g., some texture redundancy). Different from\nprevious task-specific solutions, Omni-ICM could directly support AI tasks\nanalysis based on the learned omnipotent features without joint training or\nextra transformation. Albeit simple and intuitive, Omni-ICM significantly\noutperforms existing traditional and learning-based codecs on multiple\nfundamental vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runsen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yixin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02335","description":"<p>Early detection of retinal diseases is one of the most important means of\npreventing partial or permanent blindness in patients. In this research, a\nnovel multi-label classification system is proposed for the detection of\nmultiple retinal diseases, using fundus images collected from a variety of\nsources. First, a new multi-label retinal disease dataset, the MuReD dataset,\nis constructed, using a number of publicly available datasets for fundus\ndisease classification. Next, a sequence of post-processing steps is applied to\nensure the quality of the image data and the range of diseases, present in the\ndataset. For the first time in fundus multi-label disease classification, a\ntransformer-based model optimized through extensive experimentation is used for\nimage analysis and decision making. Numerous experiments are performed to\noptimize the configuration of the proposed system. It is shown that the\napproach performs better than state-of-the-art works on the same task by 7.9%\nand 8.1% in terms of AUC score for disease detection and disease\nclassification, respectively. The obtained results further support the\npotential applications of transformer-based architectures in the medical\nimaging field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1\">M. A. Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlMarzouqi_H/0/1/0/all/0/1\">H. AlMarzouqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liatsis_P/0/1/0/all/0/1\">P. Liatsis</a> (Department of Electrical Engineering and Computer Science, Khalifa University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Counterfactual Image Manipulation via CLIP. (arXiv:2207.02812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02812","description":"<p>Leveraging StyleGAN's expressivity and its disentangled latent codes,\nexisting methods can achieve realistic editing of different visual attributes\nsuch as age and gender of facial images. An intriguing yet challenging problem\narises: Can generative models achieve counterfactual editing against their\nlearnt priors? Due to the lack of counterfactual samples in natural datasets,\nwe investigate this problem in a text-driven manner with\nContrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic\nknowledge even for various counterfactual concepts. Different from in-domain\nmanipulation, counterfactual manipulation requires more comprehensive\nexploitation of semantic knowledge encapsulated in CLIP as well as more\ndelicate handling of editing directions for avoiding being stuck in local\nminimum or undesired editing. To this end, we design a novel contrastive loss\nthat exploits predefined CLIP-space directions to guide the editing toward\ndesired directions from different perspectives. In addition, we design a simple\nyet effective scheme that explicitly maps CLIP embeddings (of target text) to\nthe latent space and fuses them with latent codes for effective latent code\noptimization and accurate editing. Extensive experiments show that our design\nachieves accurate and realistic editing while driving by target texts with\nvarious counterfactual concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}