{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. (arXiv:2202.12312v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12312","description":"<p>Little is known about what makes cross-lingual transfer hard, since factors\nlike tokenization, morphology, and syntax all change at once between languages.\nTo disentangle the impact of these factors, we propose a set of controlled\ntransfer studies: we systematically transform GLUE tasks to alter different\nfactors one at a time, then measure the resulting drops in a pretrained model's\ndownstream performance. In contrast to prior work suggesting little effect from\nsyntax on knowledge transfer, we find significant impacts from syntactic shifts\n(3-6% drop), though models quickly adapt with continued pretraining on a small\ndataset. However, we find that by far the most impactful factor for\ncrosslingual transfer is the challenge of aligning the new embeddings with the\nexisting transformer layers (18% drop), with little additional effect from\nswitching tokenizers (&lt;2% drop) or word morphologies (&lt;2% drop). Moreover,\ncontinued pretraining with a small dataset is not very effective at closing\nthis gap - suggesting that new directions are needed for solving this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1\">Isabel Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1\">Alex Tamkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Meta-Initialization with Task Augmentation for Kindergarten-aged Speech Recognition. (arXiv:2202.12326v1 [eess.AS])","link":"http://arxiv.org/abs/2202.12326","description":"<p>Children's automatic speech recognition (ASR) is always difficult due to, in\npart, the data scarcity problem, especially for kindergarten-aged kids. When\ndata are scarce, the model might overfit to the training data, and hence good\nstarting points for training are essential. Recently, meta-learning was\nproposed to learn model initialization (MI) for ASR tasks of different\nlanguages. This method leads to good performance when the model is adapted to\nan unseen language. However, MI is vulnerable to overfitting on training tasks\n(learner overfitting). It is also unknown whether MI generalizes to other\nlow-resource tasks. In this paper, we validate the effectiveness of MI in\nchildren's ASR and attempt to alleviate the problem of learner overfitting. To\nachieve model-agnostic meta-learning (MAML), we regard children's speech at\neach age as a different task. In terms of learner overfitting, we propose a\ntask-level augmentation method by simulating new ages using frequency warping\ntechniques. Detailed experiments are conducted to show the impact of task\naugmentation on each age for kindergarten-aged speech. As a result, our\napproach achieves a relative word error rate (WER) improvement of 51% over the\nbaseline system with no augmentation or initialization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunzheng Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1\">Ruchao Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alwan_A/0/1/0/all/0/1\">Abeer Alwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation. (arXiv:2202.12350v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12350","description":"<p>Natural language processing (NLP) algorithms have become very successful, but\nthey still struggle when applied to out-of-distribution examples. In this paper\nwe propose a controllable generation approach in order to deal with this domain\nadaptation (DA) challenge. Given an input text example, our DoCoGen algorithm\ngenerates a domain-counterfactual textual example (D-con) - that is similar to\nthe original in all aspects, including the task label, but its domain is\nchanged to a desired one. Importantly, DoCoGen is trained using only unlabeled\nexamples from multiple domains - no NLP task labels or parallel pairs of\ntextual examples and their domain-counterfactuals are required. We use the\nD-cons generated by DoCoGen to augment a sentiment classifier in 20 DA setups,\nwhere source-domain labeled data is scarce. Our model outperforms strong\nbaselines and improves the accuracy of a state-of-the-art unsupervised DA\nalgorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_N/0/1/0/all/0/1\">Nitay Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training. (arXiv:2202.12359v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12359","description":"<p>We present UnifiedQA-v2, a QA model built with the same process as UnifiedQA,\nexcept that it utilizes more supervision -- roughly 3x the number of datasets\nused for UnifiedQA. This generally leads to better in-domain and cross-domain\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordi_Y/0/1/0/all/0/1\">Yeganeh Kordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Combine Instructions in LLVM Compiler. (arXiv:2202.12379v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12379","description":"<p>Instruction combiner (IC) is a critical compiler optimization pass, which\nreplaces a sequence of instructions with an equivalent and optimized\ninstruction sequence at basic block level. There can be thousands of\ninstruction-combining patterns which need to be frequently updated as new\ncoding idioms/applications and novel hardware evolve over time. This results in\nfrequent updates to the IC optimization pass thereby incurring considerable\nhuman effort and high software maintenance costs. To mitigate these challenges\nassociated with the traditional IC, we design and implement a Neural\nInstruction Combiner (NIC) and demonstrate its feasibility by integrating it\ninto the standard LLVM compiler optimization pipeline. NIC leverages neural\nsequence-to-sequence (Seq2Seq) models for generating optimized encoded IR\nsequence from the unoptimized encoded IR sequence. To the best of our\nknowledge, ours is the first work demonstrating the feasibility of a neural\ninstruction combiner built into a full-fledged compiler pipeline. Given the\nnovelty of this task, we built a new dataset for training our NIC neural model.\nWe show that NIC achieves exact match results percentage of 72% for optimized\nsequences as compared to traditional IC and neural machine translation metric\nBleu precision score of 0.94, demonstrating its feasibility in a production\ncompiler pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mannarswamy_S/0/1/0/all/0/1\">Sandya Mannarswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dibyendu Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Prediction of Political Leaning on Social Media with Unsupervised Deep Learning. (arXiv:2202.12382v1 [cs.SI])","link":"http://arxiv.org/abs/2202.12382","description":"<p>Predicting the political leaning of social media users is an increasingly\npopular task, given its usefulness for electoral forecasts, opinion dynamics\nmodels and for studying the political dimension of polarization and\ndisinformation. Here, we propose a novel unsupervised technique for learning\nfine-grained political leaning from the textual content of social media posts.\nOur technique leverages a deep neural network for learning latent political\nideologies in a representation learning task. Then, users are projected in a\nlow-dimensional ideology space where they are subsequently clustered. The\npolitical leaning of a user is automatically derived from the cluster to which\nthe user is assigned. We evaluated our technique in two challenging\nclassification tasks and we compared it to baselines and other state-of-the-art\napproaches. Our technique obtains the best results among all unsupervised\ntechniques, with micro F1 = 0.426 in the 8-class task and micro F1 = 0.772 in\nthe 3-class task. Other than being interesting on their own, our results also\npave the way for the development of new and better unsupervised approaches for\nthe detection of fine-grained political leaning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fagni_T/0/1/0/all/0/1\">Tiziano Fagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cresci_S/0/1/0/all/0/1\">Stefano Cresci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrimBERT: Tailoring BERT for Trade-offs. (arXiv:2202.12411v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12411","description":"<p>Models based on BERT have been extremely successful in solving a variety of\nnatural language processing (NLP) tasks. Unfortunately, many of these large\nmodels require a great deal of computational resources and/or time for\npre-training and fine-tuning which limits wider adoptability. While\nself-attention layers have been well-studied, a strong justification for\ninclusion of the intermediate layers which follow them remains missing in the\nliterature. In this work, we show that reducing the number of intermediate\nlayers in BERT-Base results in minimal fine-tuning accuracy loss of downstream\ntasks while significantly decreasing model size and training time. We further\nmitigate two key bottlenecks, by replacing all softmax operations in the\nself-attention layers with a computationally simpler alternative and removing\nhalf of all layernorm operations. This further decreases the training time\nwhile maintaining a high level of fine-tuning accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Sharath Nittur Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarah_A/0/1/0/all/0/1\">Anthony Sarah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_S/0/1/0/all/0/1\">Sairam Sundaresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction of Large-Scale Misinformation Labeled Datasets from Social Media Discourse using Label Refinement. (arXiv:2202.12413v1 [cs.SI])","link":"http://arxiv.org/abs/2202.12413","description":"<p>Malicious accounts spreading misinformation has led to widespread false and\nmisleading narratives in recent times, especially during the COVID-19 pandemic,\nand social media platforms struggle to eliminate these contents rapidly. This\nis because adapting to new domains requires human intensive fact-checking that\nis slow and difficult to scale. To address this challenge, we propose to\nleverage news-source credibility labels as weak labels for social media posts\nand propose model-guided refinement of labels to construct large-scale, diverse\nmisinformation labeled datasets in new domains. The weak labels can be\ninaccurate at the article or social media post level where the stance of the\nuser does not align with the news source or article credibility. We propose a\nframework to use a detection model self-trained on the initial weak labels with\nuncertainty sampling based on entropy in predictions of the model to identify\npotentially inaccurate labels and correct for them using self-supervision or\nrelabeling. The framework will incorporate social context of the post in terms\nof the community of its associated user for surfacing inaccurate labels towards\nbuilding a large-scale dataset with minimum human effort. To provide labeled\ndatasets with distinction of misleading narratives where information might be\nmissing significant context or has inaccurate ancillary details, the proposed\nframework will use the few labeled samples as class prototypes to separate high\nconfidence samples into false, unproven, mixture, mostly false, mostly true,\ntrue, and debunk information. The approach is demonstrated for providing a\nlarge-scale misinformation dataset on COVID-19 vaccines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Karishma Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep neural networks for fine-grained surveillance of overdose mortality. (arXiv:2202.12448v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12448","description":"<p>Surveillance of drug overdose deaths relies on death certificates for\nidentification of the substances that caused death. Drugs and drug classes can\nbe identified through the International Classification of Diseases, 10th\nRevision (ICD-10) codes present on death certificates. However, ICD-10 codes do\nnot always provide high levels of specificity in drug identification. To\nachieve more fine-grained identification of substances on a death certificate,\nthe free-text cause of death section, completed by the medical certifier, must\nbe analyzed. Current methods for analyzing free-text death certificates rely\nsolely on look-up tables for identifying specific substances, which must be\nfrequently updated and maintained. To improve identification of drugs on death\ncertificates, a deep learning named-entity recognition model was developed,\nwhich achieved an F1-score of 99.13%. This model can identify new drug\nmisspellings and novel substances that are not present on current surveillance\nlook-up tables, enhancing the surveillance of drug overdose deaths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ward_P/0/1/0/all/0/1\">Patrick J. Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1\">April M. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavova_S/0/1/0/all/0/1\">Svetla Slavova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liford_M/0/1/0/all/0/1\">Madison Liford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniels_L/0/1/0/all/0/1\">Lara Daniels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_R/0/1/0/all/0/1\">Ripley Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets. (arXiv:2202.12459v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12459","description":"<p>Detecting toxic or pejorative expressions in online communities has become\none of the main concerns for preventing the users' mental harm. This led to the\ndevelopment of large-scale hate speech detection datasets of various domains,\nwhich are mainly built upon web-crawled texts with labels by crowd workers.\nHowever, for languages other than English, researchers might have to rely on\nonly a small-sized corpus due to the lack of data-driven research of hate\nspeech detection. This sometimes misleads the evaluation of prevalently used\npretrained language models (PLMs) such as BERT, given that PLMs often share the\ndomain of pretraining corpus with the evaluation set, resulting in\nover-representation of the detection performance. Also, the scope of pejorative\nexpressions might be restricted if the dataset is built on a single domain\ntext.\n</p>\n<p>To alleviate the above problems in Korean hate speech detection, we propose\nAPEACH,a method that allows the collection of hate speech generated by\nunspecified users. By controlling the crowd-generation of hate speech and\nadding only a minimum post-labeling, we create a corpus that enables the\ngeneralizable and fair evaluation of hate speech detection regarding text\ndomain and topic. We Compare our outcome with prior work on an annotation-based\ntoxic news comment dataset using publicly available PLMs. We check that our\ndataset is less sensitive to the lexical overlap between the evaluation set and\npretraining corpus of PLMs, showing that it helps mitigate the unexpected\nunder/over-representation of model performance. We distribute our dataset\npublicly online to further facilitate the general-domain hate speech detection\nin Korean.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kichang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Wonjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks. (arXiv:2202.12499v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12499","description":"<p>This paper focuses on the Data Augmentation for low-resource Natural Language\nUnderstanding (NLU) tasks. We propose Prompt-based D}ata Augmentation model\n(PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable\nvectors) in the frozen Pre-trained Language Models (PLMs). This avoids human\neffort in collecting unlabeled in-domain data and maintains the quality of\ngenerated synthetic data. In addition, PromDA generates synthetic data via two\ndifferent views and filters out the low-quality data using NLU models.\nExperiments on four benchmarks show that synthetic data produced by PromDA\nsuccessfully boost up the performance of NLU models which consistently\noutperform several competitive baseline models, including a state-of-the-art\nsemi-supervised model using unlabeled in-domain data. The synthetic data from\nPromDA are also complementary with unlabeled in-domain data. The NLU models can\nbe further improved when they are combined for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asyncval: A Toolkit for Asynchronously Validating Dense Retriever Checkpoints during Training. (arXiv:2202.12510v1 [cs.IR])","link":"http://arxiv.org/abs/2202.12510","description":"<p>The process of model checkpoint validation refers to the evaluation of the\nperformance of a model checkpoint executed on a held-out portion of the\ntraining data while learning the hyperparameters of the model, and is used to\navoid over-fitting and determine when the model has converged so as to stop\ntraining. A simple and efficient strategy to validate deep learning checkpoints\nis the addition of validation loops to execute during training. However, the\nvalidation of dense retrievers (DR) checkpoints is not as trivial -- and the\naddition of validation loops is not efficient. This is because, in order to\naccurately evaluate the performance of a DR checkpoint, the whole document\ncorpus needs to be encoded into vectors using the current checkpoint before any\nactual retrieval operation for checkpoint validation can be performed. This\ncorpus encoding process can be very time-consuming if the document corpus\ncontains millions of documents (e.g., 8.8m for MS MARCO and 21m for Natural\nQuestions). Thus, a naive use of validation loops during training will\nsignificantly increase training time. To address this issue, in this demo\npaper, we propose Asyncval: a Python-based toolkit for efficiently validating\nDR checkpoints during training. Instead of pausing the training loop for\nvalidating DR checkpoints, Asyncval decouples the validation loop from the\ntraining loop, uses another GPU to automatically validate new DR checkpoints\nand thus permits to perform validation asynchronously from training. Asyncval\nalso implements a range of different corpus subset sampling strategies for\nvalidating DR checkpoints; these strategies allow to further speed up the\nvalidation process. We provide an investigation of these methods in terms of\ntheir impact on validation time and validation fidelity. Asyncval is made\navailable as an open-source project at \\url{https://github.com/ielab/asyncval}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Screening Gender Transfer in Neural Machine Translation. (arXiv:2202.12568v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12568","description":"<p>This paper aims at identifying the information flow in state-of-the-art\nmachine translation systems, taking as example the transfer of gender when\ntranslating from French into English. Using a controlled set of examples, we\nexperiment several ways to investigate how gender information circulates in a\nencoder-decoder architecture considering both probing techniques as well as\ninterventions on the internal representations used in the MT system. Our\nresults show that gender information can be found in all token representations\nbuilt by the encoder and the decoder and lead us to conclude that there are\nmultiple pathways for gender transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lichao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballier_N/0/1/0/all/0/1\">Nicolas Ballier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralKG: An Open Source Library for Diverse Representation Learning of Knowledge Graphs. (arXiv:2202.12571v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12571","description":"<p>NeuralKG is an open-source Python-based library for diverse representation\nlearning of knowledge graphs. It implements three different series of Knowledge\nGraph Embedding (KGE) methods, including conventional KGEs, GNN-based KGEs, and\nRule-based KGEs. With a unified framework, NeuralKG successfully reproduces\nlink prediction results of these methods on benchmarks, freeing users from the\nlaborious task of reimplementing them, especially for some methods originally\nwritten in non-python programming languages. Besides, NeuralKG is highly\nconfigurable and extensible. It provides various decoupled modules that can be\nmixed and adapted to each other. Thus with NeuralKG, developers and researchers\ncan quickly implement their own designed models and obtain the optimal training\nmethods to achieve the best performance efficiently. We built an website in\n<a href=\"http://neuralkg.zjukg.cn\">this http URL</a> to organize an open and shared KG representation\nlearning community. The source code is all publicly released at\nhttps://github.com/zjukg/NeuralKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongtao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zezhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yajing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zonggang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Naturally-occurring Corrections and Paraphrases from Wikipedia's Revision History. (arXiv:2202.12575v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12575","description":"<p>Naturally-occurring instances of linguistic phenomena are important both for\ntraining and for evaluating automatic processes on text. When available in\nlarge quantities, they also prove interesting material for linguistic studies.\nIn this article, we present a new resource built from Wikipedia's revision\nhistory, called WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), which\ncontains numerous editings by human contributors, including various corrections\nand rewritings. We discuss the main motivations for building such a resource,\ndescribe how it was built and present initial applications on French.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Max_A/0/1/0/all/0/1\">Aur&#xe9;lien Max</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Multilingual Models for Automatic Speech Recognition. (arXiv:2202.12576v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12576","description":"<p>Although Automatic Speech Recognition (ASR) systems have achieved human-like\nperformance for a few languages, the majority of the world's languages do not\nhave usable systems due to the lack of large speech datasets to train these\nmodels. Cross-lingual transfer is an attractive solution to this problem,\nbecause low-resource languages can potentially benefit from higher-resource\nlanguages either through transfer learning, or being jointly trained in the\nsame multilingual model. The problem of cross-lingual transfer has been well\nstudied in ASR, however, recent advances in Self Supervised Learning are\nopening up avenues for unlabeled speech data to be used in multilingual ASR\nmodels, which can pave the way for improved performance on low-resource\nlanguages. In this paper, we survey the state of the art in multilingual ASR\nmodels that are built with cross-lingual transfer in mind. We present best\npractices for building multilingual models from research across diverse\nlanguages and techniques, discuss open questions and provide recommendations\nfor future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_H/0/1/0/all/0/1\">Hemant Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language technology practitioners as language managers: arbitrating data bias and predictive bias in ASR. (arXiv:2202.12603v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12603","description":"<p>Despite the fact that variation is a fundamental characteristic of natural\nlanguage, automatic speech recognition systems perform systematically worse on\nnon-standardised and marginalised language varieties. In this paper we use the\nlens of language policy to analyse how current practices in training and\ntesting ASR systems in industry lead to the data bias giving rise to these\nsystematic error differences. We believe that this is a useful perspective for\nspeech and language technology practitioners to understand the origins and\nharms of algorithmic bias, and how they can mitigate it. We also propose a\nre-framing of language resources as (public) infrastructure which should not\nsolely be designed for markets, but for, and with meaningful cooperation of,\nspeech communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markl_N/0/1/0/all/0/1\">Nina Markl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNulty_S/0/1/0/all/0/1\">Stephen Joseph McNulty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus. (arXiv:2202.12607v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12607","description":"<p>Most current machine translation models are mainly trained with parallel\ncorpora, and their translation accuracy largely depends on the quality and\nquantity of the corpora. Although there are billions of parallel sentences for\na few language pairs, effectively dealing with most language pairs is difficult\ndue to a lack of publicly available parallel corpora. This paper creates a\nlarge parallel corpus for English-Japanese, a language pair for which only\nlimited resources are available, compared to such resource-rich languages as\nEnglish-German. It introduces a new web-based English-Japanese parallel corpus\nnamed JParaCrawl v3.0. Our new corpus contains more than 21 million unique\nparallel sentence pairs, which is more than twice as many as the previous\nJParaCrawl v2.0 corpus. Through experiments, we empirically show how our new\ncorpus boosts the accuracy of machine translation models on various domains.\nThe JParaCrawl v3.0 corpus will eventually be publicly available online for\nresearch purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morishita_M/0/1/0/all/0/1\">Makoto Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsuki_C/0/1/0/all/0/1\">Chousa Katsuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagata_M/0/1/0/all/0/1\">Masaaki Nagata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Multi-Modal Representations for Ambiguity Detection & Coreference Resolution in the SIMMC 2.0 Challenge. (arXiv:2202.12645v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12645","description":"<p>Anaphoric expressions, such as pronouns and referential descriptions, are\nsituated with respect to the linguistic context of prior turns, as well as, the\nimmediate visual environment. However, a speaker's referential descriptions do\nnot always uniquely identify the referent, leading to ambiguities in need of\nresolution through subsequent clarificational exchanges. Thus, effective\nAmbiguity Detection and Coreference Resolution are key to task success in\nConversational AI. In this paper, we present models for these two tasks as part\nof the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT\nand LXMERT based models, compare them to a number of baselines and provide\nablation experiments. Our results show that (1) language models are able to\nexploit correlations in the data to detect ambiguity; and (2) unimodal\ncoreference resolution models can avoid the need for a vision component,\nthrough the use of smart object representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiyah_Garcia_F/0/1/0/all/0/1\">Francisco Javier Chiyah-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_J/0/1/0/all/0/1\">Jos&#xe9; Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshghi_A/0/1/0/all/0/1\">Arash Eshghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1\">Helen Hastie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning, Natural Language Processing, and Explainable Artificial Intelligence in the Biomedical Domain. (arXiv:2202.12678v1 [cs.AI])","link":"http://arxiv.org/abs/2202.12678","description":"<p>In this article, we first give an introduction to artificial intelligence and\nits applications in biology and medicine in Section 1. Deep learning methods\nare then described in Section 2. We narrow down the focus of the study on\ntextual data in Section 3, where natural language processing and its\napplications in the biomedical domain are described. In Section 4, we give an\nintroduction to explainable artificial intelligence and discuss the importance\nof explainability of artificial intelligence systems, especially in the\nbiomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ask2Mask: Guided Data Selection for Masked Speech Modeling. (arXiv:2202.12719v1 [cs.SD])","link":"http://arxiv.org/abs/2202.12719","description":"<p>Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn\nrepresentations over speech frames which are randomly masked within an\nutterance. While these methods improve performance of Automatic Speech\nRecognition (ASR) systems, they have one major limitation. They treat all\nunsupervised speech samples with equal weight, which hinders learning as not\nall samples have relevant information to learn meaningful representations. In\nthis work, we address this limitation. We propose ask2mask (ATM), a novel\napproach to focus on specific samples during MSM pre-training. ATM employs an\nexternal ASR model or \\textit{scorer} to weight unsupervised input samples in\ntwo different ways: 1) A fine-grained data selection is performed by masking\nover the highly confident input frames as chosen by the scorer. This allows the\nmodel to learn meaningful representations. 2) ATM is further extended to focus\nat utterance-level by weighting the final MSM loss with the utterance-level\nconfidence score. We conduct fine-tuning experiments on two well-benchmarked\ncorpora: LibriSpeech (matching the pre-training data) and Commonvoice,\nTED-LIUM, AMI and CHiME-6 (not matching the pre-training data). The results\nsubstantiate the efficacy of ATM on significantly improving the recognition\nperformance under mismatched conditions (up to 11.6\\% relative over published\nresults and upto 4.46\\% relative over our internal baseline) while still\nyielding modest improvements under matched conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baskar_M/0/1/0/all/0/1\">Murali Karthick Baskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the data requirements of probing. (arXiv:2202.12801v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12801","description":"<p>As large and powerful neural language models are developed, researchers have\nbeen increasingly interested in developing diagnostic tools to probe them.\nThere are many papers with conclusions of the form \"observation X is found in\nmodel Y\", using their own datasets with varying sizes. Larger probing datasets\nbring more reliability, but are also expensive to collect. There is yet to be a\nquantitative method for estimating reasonable probing dataset sizes. We tackle\nthis omission in the context of comparing two probing configurations: after we\nhave collected a small dataset from a pilot study, how many additional data\nsamples are sufficient to distinguish two different configurations? We present\na novel method to estimate the required number of data samples in such\nexperiments and, across several case studies, we verify that our estimations\nhave sufficient statistical power. Our framework helps to systematically\nconstruct probing datasets to diagnose neural NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zining Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Reality of Multi-Lingual Machine Translation. (arXiv:2202.12814v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12814","description":"<p>Our book \"The Reality of Multi-Lingual Machine Translation\" discusses the\nbenefits and perils of using more than two languages in machine translation\nsystems. While focused on the particular task of sequence-to-sequence\nprocessing and multi-task learning, the book targets somewhat beyond the area\nof natural language processing. Machine translation is for us a prime example\nof deep learning applications where human skills and learning capabilities are\ntaken as a benchmark that many try to match and surpass. We document that some\nof the gains observed in multi-lingual translation may result from simpler\neffects than the assumed cross-lingual transfer of knowledge.\n</p>\n<p>In the first, rather general part, the book will lead you through the\nmotivation for multi-linguality, the versatility of deep neural networks\nespecially in sequence-to-sequence tasks to complications of this learning. We\nconclude the general part with warnings against too optimistic and unjustified\nexplanations of the gains that neural networks demonstrate.\n</p>\n<p>In the second part, we fully delve into multi-lingual models, with a\nparticularly careful examination of transfer learning as one of the more\nstraightforward approaches utilizing additional languages. The recent\nmulti-lingual techniques, including massive models, are surveyed and practical\naspects of deploying systems for many languages are discussed. The conclusion\nhighlights the open problem of machine understanding and reminds of two ethical\naspects of building large-scale models: the inclusivity of research and its\necological trace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1\">Dominik Mach&#xe1;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Literature Review about Idea Mining: The Use of Machine-driven Analytics to Generate Ideas. (arXiv:2202.12826v1 [cs.IR])","link":"http://arxiv.org/abs/2202.12826","description":"<p>Idea generation is the core activity of innovation. Digital data sources,\nwhich are sources of innovation, such as patents, publications, social media,\nwebsites, etc., are increasingly growing at unprecedented volume. Manual idea\ngeneration is time-consuming and is affected by the subjectivity of the\nindividuals involved. Therefore, the use machine-driven data analytics\ntechniques to analyze data to generate ideas and support idea generation by\nserving users is useful. The objective of this study is to study state-of\nthe-art machine-driven analytics for idea generation and data sources, hence\nthe result of this study will generally server as a guideline for choosing\ntechniques and data sources. A systematic literature review is conducted to\nidentify relevant scholarly literature from IEEE, Scopus, Web of Science and\nGoogle Scholar. We selected a total of 71 articles and analyzed them\nthematically. The results of this study indicate that idea generation through\nmachine-driven analytics applies text mining, information retrieval (IR),\nartificial intelligence (AI), deep learning, machine learning, statistical\ntechniques, natural language processing (NLP), NLP-based morphological\nanalysis, network analysis, and bibliometric to support idea generation. The\nresults include a list of techniques and procedures in idea generation through\nmachine-driven idea analytics. Additionally, characterization and heuristics\nused in idea generation are summarized. For the future, tools designed to\ngenerate ideas could be explored.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayele_W/0/1/0/all/0/1\">Workneh Y. Ayele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juell_Skielse_G/0/1/0/all/0/1\">Gustaf Juell-Skielse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphology Without Borders: Clause-Level Morphological Annotation. (arXiv:2202.12832v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12832","description":"<p>Morphological tasks use large multi-lingual datasets that organize words into\ninflection tables, which then serve as training and evaluation data for various\ntasks. However, a closer inspection of these data reveals profound\ncross-linguistic inconsistencies, that arise from the lack of a clear\nlinguistic and operational definition of what is a word, and that severely\nimpair the universality of the derived tasks. To overcome this deficiency, we\npropose to view morphology as a clause-level phenomenon, rather than\nword-level. It is anchored in a fixed yet inclusive set of features homogeneous\nacross languages, that encapsulates all functions realized in a saturated\nclause. We deliver MightyMorph, a novel dataset for clause-level morphology\ncovering 4 typologically-different languages: English, German, Turkish and\nHebrew. We use this dataset to derive 3 clause-level morphological tasks:\ninflection, reinflection and analysis. Our experiments show that the\nclause-level tasks are substantially harder than the respective word-level\ntasks, while having comparable complexity across languages. Furthermore,\nredefining morphology to the clause-level provides a neat interface with\ncontextualized language models (LMs) and can be used to probe LMs capacity to\nencode complex morphology. Taken together, this work opens up new horizons in\nthe study of computational morphology, leaving ample space for studying neural\nmorphological modeling cross-linguistically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?. (arXiv:2202.12837v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12837","description":"<p>Large language models (LMs) are able to in-context learn -- perform a new\ntask via inference alone by conditioning on a few input-label pairs\n(demonstrations) and making predictions for new inputs. However, there has been\nlittle understanding of how the model learns and which aspects of the\ndemonstrations contribute to end task performance. In this paper, we show that\nground truth demonstrations are in fact not required -- randomly replacing\nlabels in the demonstrations barely hurts performance, consistently over 12\ndifferent models including GPT-3. Instead, we find that other aspects of the\ndemonstrations are the key drivers of end task performance, including the fact\nthat they provide a few examples of (1) the label space, (2) the distribution\nof the input text, and (3) the overall format of the sequence. Together, our\nanalysis provides a new way of understanding how and why in-context learning\nworks, while opening up new questions about how much can be learned from large\nlanguage models through inference alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xinxi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DataLab: A Platform for Data Analysis and Intervention. (arXiv:2202.12875v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12875","description":"<p>Despite data's crucial role in machine learning, most existing tools and\nresearch tend to focus on systems on top of existing data rather than how to\ninterpret and manipulate data. In this paper, we propose DataLab, a unified\ndata-oriented platform that not only allows users to interactively analyze the\ncharacteristics of data, but also provides a standardized interface for\ndifferent data processing operations. Additionally, in view of the ongoing\nproliferation of datasets, \\toolname has features for dataset recommendation\nand global vision analysis that help researchers form a better view of the data\necosystem. So far, DataLab covers 1,715 datasets and 3,583 of its transformed\nversion (e.g., hyponyms replacement), where 728 datasets support various\nanalyses (e.g., with respect to gender bias) with the help of 140M samples\nannotated by 318 feature functions. DataLab is under active development and\nwill be supported going forward. We have released a web platform, web API,\nPython SDK, PyPI published package and online documentation, which hopefully,\ncan meet the diverse needs of researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vijay Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhoumianze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Negative Sampling for Handling Missing Entity Annotations. (arXiv:2108.11607v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11607","description":"<p>Negative sampling is highly effective in handling missing annotations for\nnamed entity recognition (NER). One of our contributions is an analysis on how\nit makes sense through introducing two insightful concepts: missampling and\nuncertainty. Empirical studies show low missampling rate and high uncertainty\nare both essential for achieving promising performances with negative sampling.\nBased on the sparsity of named entities, we also theoretically derive a lower\nbound for the probability of zero missampling rate, which is only relevant to\nsentence length. The other contribution is an adaptive and weighted sampling\ndistribution that further improves negative sampling via our former analysis.\nExperiments on synthetic datasets and well-annotated datasets (e.g.,\nCoNLL-2003) show that our proposed approach benefits negative sampling in terms\nof F1 score and loss convergence. Besides, models with improved negative\nsampling have achieved new state-of-the-art results on real-world datasets\n(e.g., EC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situated Dialogue Learning through Procedural Environment Generation. (arXiv:2110.03262v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03262","description":"<p>We teach goal-driven agents to interactively act and speak in situated\nenvironments by training on generated curriculums. Our agents operate in LIGHT\n(Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text adventure\ngame wherein an agent perceives and interacts with the world through textual\nnatural language. Goals in this environment take the form of character-based\nquests, consisting of personas and motivations. We augment LIGHT by learning to\nprocedurally generate additional novel textual worlds and quests to create a\ncurriculum of steadily increasing difficulty for training agents to achieve\nsuch goals. In particular, we measure curriculum difficulty in terms of the\nrarity of the quest in the original training distribution -- an easier\nenvironment is one that is more likely to have been found in the unaugmented\ndataset. An ablation study shows that this method of learning from the tail of\na distribution results in significantly higher generalization abilities as\nmeasured by zero-shot performance on never-before-seen quests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Renee Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Augmentation Supported Contrastive Learning of Sentence Representations. (arXiv:2110.08552v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08552","description":"<p>Despite profound successes, contrastive representation learning relies on\ncarefully designed data augmentations using domain specific knowledge. This\nchallenge is magnified in natural language processing where no general rules\nexist for data augmentation due to the discrete nature of natural language. We\ntackle this challenge by presenting a Virtual augmentation Supported\nContrastive Learning of sentence representations (VaSCL). Originating from the\ninterpretation that data augmentation essentially constructs the neighborhoods\nof each training instance, we in turn utilize the neighborhood to generate\neffective data augmentations. Leveraging the large training batch size of\ncontrastive learning, we approximate the neighborhood of an instance via its\nK-nearest in-batch neighbors in the representation space. We then define an\ninstance discrimination task regarding this neighborhood and generate the\nvirtual augmentation in an adversarial training manner. We access the\nperformance of VaSCL on a wide range of downstream tasks, and set a new\nstate-of-the-art for unsupervised sentence representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Learning for Few-Shot Dialogue State Tracking. (arXiv:2201.05780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05780","description":"<p>Collecting dialogue state labels, slots and values, for learning dialogue\nstate tracking (DST) models can be costly, especially with the wide application\nof dialogue systems in new-rising domains. In this paper, we focus on how to\nlearn a DST model efficiently with limited labeled data. We design a prompt\nlearning framework for few-shot DST, which consists of two main components:\nvalue-based prompt and inverse prompt mechanism. This framework aims to utilize\nthe language understanding and generation ability of pre-trained language\nmodels (PLM). First, we design value-based prompt functions to probe the\nDST-related knowledge from PLM, which do not rely on the known ontology of\nslots. Further, an inverse prompt mechanism is utilized to self-check the\n\"prompted\" knowledge and help the PLM understand the essence of DST task\nfurther. Experiments show that our model can generate unseen slots and\noutperforms existing state-of-the-art few-shot methods. It indicates that\nDST-related knowledge can be probed from PLM and utilized to address\nlow-resource DST efficiently with the help of prompt learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06009","description":"<p>Large LMs such as GPT-3, while powerful, are not immune to mistakes, but are\nprohibitively costly to retrain. One failure mode is misinterpreting a user's\ninstruction (e.g., GPT-3 interpreting \"What word is similar to good?\" to mean a\nhomonym, while the user intended a synonym). Our goal is to allow users to\ncorrect such errors directly through interaction -- without retraining. Our\napproach pairs GPT-3 with a growing memory of cases where the model\nmisunderstood the user's intent and was provided with feedback, clarifying the\ninstruction. Given a new query, our memory-enhanced GPT-3 uses feedback from\nsimilar, prior queries to enrich the prompt. Through simple proof-of-concept\nexperiments, we show how a (simulated) user can interactively teach a deployed\nGPT-3, doubling its accuracy on basic lexical tasks (e.g., generate a synonym)\nwhere users query in different, novel (often misunderstood) ways. In such\nscenarios, memory helps avoid repeating similar past mistakes. Our simple idea\nis a first step towards strengthening deployed models, potentially broadening\ntheir utility. All the code and data is available at\nhttps://github.com/madaan/memprompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph. (arXiv:2202.12307v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12307","description":"<p>This paper addresses the unsupervised learning of content-style decomposed\nrepresentation. We first give a definition of style and then model the\ncontent-style representation as a token-level bipartite graph. An unsupervised\nframework, named Retriever, is proposed to learn such representations. First, a\ncross-attention module is employed to retrieve permutation invariant (P.I.)\ninformation, defined as style, from the input data. Second, a vector\nquantization (VQ) module is used, together with man-induced constraints, to\nproduce interpretable content tokens. Last, an innovative link attention module\nserves as the decoder to reconstruct data from the decomposed content and\nstyle, with the help of the linking keys. Being modal-agnostic, the proposed\nRetriever is evaluated in both speech and image domains. The state-of-the-art\nzero-shot voice conversion performance confirms the disentangling ability of\nour framework. Top performance is also achieved in the part discovery task for\nimages, verifying the interpretability of our representation. In addition, the\nvivid part-based style transfer quality demonstrates the potential of Retriever\nto support various fascinating generative tasks. Project page at\nhttps://ydcustc.github.io/retriever-demo/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dacheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Efficient Training of Progressive Generative Adversarial Network using Depthwise Separable Convolution and Super Resolution Generative Adversarial Network. (arXiv:2202.12337v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12337","description":"<p>Generative Adversarial Networks have been employed successfully to generate\nhigh-resolution augmented images of size 1024^2. Although the augmented images\ngenerated are unprecedented, the training time of the model is exceptionally\nhigh. Conventional GAN requires training of both Discriminator as well as the\nGenerator. In Progressive GAN, which is the current state-of-the-art GAN for\nimage augmentation, instead of training the GAN all at once, a new concept of\nprogressing growing of Discriminator and Generator simultaneously, was\nproposed. Although the lower stages such as 4x4 and 8x8 train rather quickly,\nthe later stages consume a tremendous amount of time which could take days to\nfinish the model training. In our paper, we propose a novel pipeline that\ncombines Progressive GAN with slight modifications and Super Resolution GAN.\nSuper Resolution GAN up samples low-resolution images to high-resolution images\nwhich can prove to be a useful resource to reduce the training time\nexponentially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Karwande_A/0/1/0/all/0/1\">Atharva Karwande</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranesh Kulkarni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolhe_T/0/1/0/all/0/1\">Tejas Kolhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1\">Akshay Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamble_S/0/1/0/all/0/1\">Soham Kamble</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset for Natural Disaster Damage Assessment. (arXiv:2202.12361v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12361","description":"<p>Due to climate change, we can observe a recent surge of natural disasters all\naround the world. These disasters are causing disastrous impact on both nature\nand human lives. Economic losses are getting greater due to the hurricanes.\nQuick and prompt response of the rescue teams are crucial in saving human lives\nand reducing economic cost. Deep learning based computer vision techniques can\nhelp in scene understanding, and help rescue teams with precise damage\nassessment. Semantic segmentation, an active research area in computer vision,\ncan put labels to each pixel of an image, and therefore can be a valuable\narsenal in the effort of reducing the impacts of hurricanes. Unfortunately,\navailable datasets for natural disaster damage assessment lack detailed\nannotation of the affected areas, and therefore do not support the deep\nlearning models in total damage assessment. To this end, we introduce the\nRescueNet, a high resolution post disaster dataset, for semantic segmentation\nto assess damages after natural disasters. The RescueNet consists of post\ndisaster images collected after Hurricane Michael. The data is collected using\nUnmanned Aerial Vehicles (UAVs) from several areas impacted by the hurricane.\nThe uniqueness of the RescueNet comes from the fact that this dataset provides\nhigh resolution post-disaster images and comprehensive annotation of each\nimage. While most of the existing dataset offer annotation of only part of the\nscene, like building, road, or river, RescueNet provides pixel level annotation\nof all the classes including building, road, pool, tree, debris, and so on. We\nfurther analyze the usefulness of the dataset by implementing state-of-the-art\nsegmentation models on the RescueNet. The experiments demonstrate that our\ndataset can be valuable in further improvement of the existing methodologies\nfor natural disaster damage assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Tashnim Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1\">Robin Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnemoonfar_M/0/1/0/all/0/1\">Maryam Rahnemoonfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Translation. (arXiv:2202.12362v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12362","description":"<p>Generating images that fit a given text description using machine learning\nhas improved greatly with the release of technologies such as the CLIP\nimage-text encoder model; however, current methods lack artistic control of the\nstyle of image to be generated. We present an approach for generating styled\ndrawings for a given text description where a user can specify a desired\ndrawing style using a sample image. Inspired by a theory in art that style and\ncontent are generally inseparable during the creative process, we propose a\ncoupled approach, known here as StyleCLIPDraw, whereby the drawing is generated\nby optimizing for style and content simultaneously throughout the process as\nopposed to applying style transfer after creating content in a sequence. Based\non human evaluation, the styles of images generated by StyleCLIPDraw are\nstrongly preferred to those by the sequential approach. Although the quality of\ncontent generation degrades for certain styles, overall considering both\ncontent \\textit{and} style, StyleCLIPDraw is found far more preferred,\nindicating the importance of style, look, and feel of machine generated images\nto people as well as indicating that style is coupled in the drawing process\nitself. Our code (https://github.com/pschaldenbrand/StyleCLIPDraw), a\ndemonstration (https://replicate.com/pschaldenbrand/style-clip-draw), and style\nevaluation data\n(https://www.kaggle.com/pittsburghskeet/drawings-with-style-evaluation-styleclipdraw)\nare publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schaldenbrand_P/0/1/0/all/0/1\">Peter Schaldenbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instantaneous Physiological Estimation using Video Transformers. (arXiv:2202.12368v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12368","description":"<p>Video-based physiological signal estimation has been limited primarily to\npredicting episodic scores in windowed intervals. While these intermittent\nvalues are useful, they provide an incomplete picture of patients'\nphysiological status and may lead to late detection of critical conditions. We\npropose a video Transformer for estimating instantaneous heart rate and\nrespiration rate from face videos. Physiological signals are typically\nconfounded by alignment errors in space and time. To overcome this, we\nformulated the loss in the frequency domain. We evaluated the method on the\nlarge scale Vision-for-Vitals (V4V) benchmark. It outperformed both shallow and\ndeep learning based methods for instantaneous respiration rate estimation. In\nthe case of heart-rate estimation, it achieved an instantaneous-MAE of 13.0\nbeats-per-minute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Revanur_A/0/1/0/all/0/1\">Ambareesh Revanur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasari_A/0/1/0/all/0/1\">Ananyananda Dasari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_C/0/1/0/all/0/1\">Conrad S. Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeni_L/0/1/0/all/0/1\">Laszlo A. Jeni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Monocular Depth Estimation and Uncertainty Quantification using Classification Approaches for Regression. (arXiv:2202.12369v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12369","description":"<p>Monocular depth is important in many tasks, such as 3D reconstruction and\nautonomous driving. Deep learning based models achieve state-of-the-art\nperformance in this field. A set of novel approaches for estimating monocular\ndepth consists of transforming the regression task into a classification one.\nHowever, there is a lack of detailed descriptions and comparisons for\nClassification Approaches for Regression (CAR) in the community and no in-depth\nexploration of their potential for uncertainty estimation. To this end, this\npaper will introduce a taxonomy and summary of CAR approaches, a new\nuncertainty estimation solution for CAR, and a set of experiments on depth\naccuracy and uncertainty quantification for CAR-based models on KITTI dataset.\nThe experiments reflect the differences in the portability of various CAR\nmethods on two backbones. Meanwhile, the newly proposed method for uncertainty\nestimation can outperform the ensembling method with only one forward\npropagation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly-Efficient Binary Neural Networks for Visual Place Recognition. (arXiv:2202.12375v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12375","description":"<p>VPR is a fundamental task for autonomous navigation as it enables a robot to\nlocalize itself in the workspace when a known location is detected. Although\naccuracy is an essential requirement for a VPR technique, computational and\nenergy efficiency are not less important for real-world applications. CNN-based\ntechniques archive state-of-the-art VPR performance but are computationally\nintensive and energy demanding. Binary neural networks (BNN) have been recently\nproposed to address VPR efficiently. Although a typical BNN is an order of\nmagnitude more efficient than a CNN, its processing time and energy usage can\nbe further improved. In a typical BNN, the first convolution is not completely\nbinarized for the sake of accuracy. Consequently, the first layer is the\nslowest network stage, requiring a large share of the entire computational\neffort. This paper presents a class of BNNs for VPR that combines depthwise\nseparable factorization and binarization to replace the first convolutional\nlayer to improve computational and energy efficiency. Our best model achieves\nstate-of-the-art VPR performance while spending considerably less time and\nenergy to process an image than a BNN using a non-binary convolution as a first\nstage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrarini_B/0/1/0/all/0/1\">Bruno Ferrarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_Maier_K/0/1/0/all/0/1\">Klaus D. McDonald-Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1\">Shoaib Ehsan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TwistSLAM: Constrained SLAM in Dynamic Environment. (arXiv:2202.12384v1 [cs.RO])","link":"http://arxiv.org/abs/2202.12384","description":"<p>Moving objects are present in most scenes of our life. However they can be\nvery problematic for classical SLAM algorithms that assume the scene to be\nrigid. This assumption limits the applicability of those algorithms as they are\nunable to accurately estimate the camera pose and world structure in many\nscenarios. Some SLAM systems have been proposed to detect and mask out dynamic\nobjects, making the static scene assumption valid. However this information can\nallow the system to track objects within the scene, while tracking the camera,\nwhich can be crucial for some applications. In this paper we present TwistSLAM\na semantic, dynamic, stereo SLAM system that can track dynamic objects in the\nscene. Our algorithm creates clusters of points according to their semantic\nclass. It uses the static parts of the environment to robustly localize the\ncamera and tracks the remaining objects. We propose a new formulation for the\ntracking and the bundle adjustment to take in account the characteristics of\nmechanical joints between clusters to constrain and improve their pose\nestimation. We evaluate our approach on several sequences from a public dataset\nand show that we improve camera and object tracking compared to state of the\nart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mathieu Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchand_E/0/1/0/all/0/1\">Eric Marchand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacete_A/0/1/0/all/0/1\">Amine Kacete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royan_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Royan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provable Stochastic Optimization for Global Contrastive Learning: Small Batch Does Not Harm Performance. (arXiv:2202.12387v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12387","description":"<p>In this paper, we study contrastive learning from an optimization\nperspective, aiming to analyze and address a fundamental issue of existing\ncontrastive learning methods that either rely on a large batch size or a large\ndictionary. We consider a global objective for contrastive learning, which\ncontrasts each positive pair with all negative pairs for an anchor point. From\nthe optimization perspective, we explain why existing methods such as SimCLR\nrequires a large batch size in order to achieve a satisfactory result. In order\nto remove such requirement, we propose a memory-efficient Stochastic\nOptimization algorithm for solving the Global objective of Contrastive Learning\nof Representations, named SogCLR. We show that its optimization error is\nnegligible under a reasonable condition after a sufficient number of iterations\nor is diminishing for a slightly different global contrastive objective.\nEmpirically, we demonstrate that on ImageNet with a batch size 256, SogCLR\nachieves a performance of 69.4% for top-1 linear evaluation accuracy using\nResNet-50, which is on par with SimCLR (69.3%) with a large batch size 8,192.\nWe also attempt to show that the proposed optimization technique is generic and\ncan be applied to solving other contrastive losses, e.g., two-way contrastive\nlosses for bimodal contrastive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuoning Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zihao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Transferable Reward for Query Object Localization with Policy Adaptation. (arXiv:2202.12403v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12403","description":"<p>We propose a reinforcement learning based approach to \\emph{query object\nlocalization}, for which an agent is trained to localize objects of interest\nspecified by a small exemplary set. We learn a transferable reward signal\nformulated using the exemplary set by ordinal metric learning. Our proposed\nmethod enables test-time policy adaptation to new environments where the reward\nsignals are not readily available, and outperforms fine-tuning approaches that\nare limited to annotated images. In addition, the transferable reward allows\nrepurposing the trained agent from one specific class to another class.\nExperiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tingfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shaobo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1\">Martin Renqiang Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fourier-Based Augmentations for Improved Robustness and Uncertainty Calibration. (arXiv:2202.12412v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12412","description":"<p>Diverse data augmentation strategies are a natural approach to improving\nrobustness in computer vision models against unforeseen shifts in data\ndistribution. However, the ability to tailor such strategies to inoculate a\nmodel against specific classes of corruptions or attacks -- without incurring\nsubstantial losses in robustness against other classes of corruptions --\nremains elusive. In this work, we successfully harden a model against\nFourier-based attacks, while producing superior-to-AugMix accuracy and\ncalibration results on both the CIFAR-10-C and CIFAR-100-C datasets;\nclassification error is reduced by over ten percentage points for some\nhigh-severity noise and digital-type corruptions. We achieve this by\nincorporating Fourier-basis perturbations in the AugMix image-augmentation\nframework. Thus we demonstrate that the AugMix framework can be tailored to\neffectively target particular distribution shifts, while boosting overall model\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soklaski_R/0/1/0/all/0/1\">Ryan Soklaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_M/0/1/0/all/0/1\">Michael Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal channel selection with discrete QCQP. (arXiv:2202.12417v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12417","description":"<p>Reducing the high computational cost of large convolutional neural networks\nis crucial when deploying the networks to resource-constrained environments. We\nfirst show the greedy approach of recent channel pruning methods ignores the\ninherent quadratic coupling between channels in the neighboring layers and\ncannot safely remove inactive weights during the pruning procedure.\nFurthermore, due to these inactive weights, the greedy methods cannot guarantee\nto satisfy the given resource constraints and deviate with the true objective.\nIn this regard, we propose a novel channel selection method that optimally\nselects channels via discrete QCQP, which provably prevents any inactive\nweights and guarantees to meet the resource constraints tightly in terms of\nFLOPs, memory usage, and network size. We also propose a quadratic model that\naccurately estimates the actual inference time of the pruned network, which\nallows us to adopt inference time as a resource constraint option. Furthermore,\nwe generalize our method to extend the selection granularity beyond channels\nand handle non-sequential connections. Our experiments on CIFAR-10 and ImageNet\nshow our proposed pruning method outperforms other fixed-importance channel\npruning methods on various network architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yeonwoo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Deokjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_G/0/1/0/all/0/1\">Gaon An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_C/0/1/0/all/0/1\">Changyong Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hyun Oh Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?. (arXiv:2202.12426v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12426","description":"<p>While several works have studied the vulnerability of automated FRS and have\nproposed morphing attack detection (MAD) methods, very few have focused on\nstudying the human ability to detect morphing attacks. The examiner/observer's\nface morph detection ability is based on their observation, domain knowledge,\nexperience, and familiarity with the problem, and no works report the detailed\nfindings from observers who check identity documents as a part of their\neveryday professional life. This work creates a new benchmark database of\nrealistic morphing attacks from 48 unique subjects leading to 400 morphed\nimages presented to the observers in a Differential-MAD (D-MAD) setting. Unlike\nthe existing databases, the newly created morphed image database has been\ncreated with careful considerations to age, gender and ethnicity to create\nrealistic morph attacks. Further, unlike the previous works, we also capture\nten images from Automated Border Control (ABC) gates to mimic the realistic\nD-MAD setting leading to 400 probe images in border crossing scenarios. The\nnewly created dataset is further used to study the ability of human observers'\nability to detect morphed images. In addition, a new dataset of 180 morphed\nimages is also created using the FRGCv2 dataset under the Single Image-MAD\n(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a\nnew evaluation platform is created to conduct S-MAD and D-MAD analysis. The\nbenchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who\nare primarily governmental employees from more than 40 countries. The analysis\nprovides interesting insights and points to expert observers' missing\ncompetence and failure to detect a considerable amount of morphing attacks.\nHuman observers tend to detect morphed images to a lower accuracy as compared\nto the automated MAD algorithms evaluated in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godage_S/0/1/0/all/0/1\">Sankini Rancha Godage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovaasda_F/0/1/0/all/0/1\">Fr&#xf8;y L&#xf8;v&#xe5;sda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Sushma Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Video Segmentation Models with Per-frame Inference. (arXiv:2202.12427v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12427","description":"<p>Most existing real-time deep models trained with each frame independently may\nproduce inconsistent results across the temporal axis when tested on a video\nsequence. A few methods take the correlations in the video sequence into\naccount,e.g., by propagating the results to the neighboring frames using\noptical flow or extracting frame representations using multi-frame information,\nwhich may lead to inaccurate results or unbalanced latency. In this work, we\nfocus on improving the temporal consistency without introducing computation\noverhead in inference. To this end, we perform inference at each frame.\nTemporal consistency is achieved by learning from video frames with extra\nconstraints during the training phase. introduced for inference. We propose\nseveral techniques to learn from the video sequence, including a temporal\nconsistency loss and online/offline knowledge distillation methods. On the task\nof semantic video segmentation, weighing among accuracy, temporal smoothness,\nand efficiency, our proposed method outperforms keyframe-based methods and a\nfew baseline methods that are trained with each frame independently, on\ndatasets including Cityscapes, Camvid, and 300VW-Mask. We further apply our\ntraining method to video instance segmentation on YouTubeVISand develop an\napplication of portrait matting in video sequences, by segmenting temporally\nconsistent instance-level trimaps across frames. Experiments show superior\nqualitative and quantitative results. Code is available at:\nhttps://git.io/vidseg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changqian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Adversarial Robustness from Feature Maps of Convolutional Layers. (arXiv:2202.12435v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12435","description":"<p>The adversarial robustness of a neural network mainly relies on two factors,\none is the feature representation capacity of the network, and the other is its\nresistance ability to perturbations. In this paper, we study the\nanti-perturbation ability of the network from the feature maps of convolutional\nlayers. Our theoretical analysis discovers that larger convolutional features\nbefore average pooling can contribute to better resistance to perturbations,\nbut the conclusion is not true for max pooling. Based on the theoretical\nfindings, we present two feasible ways to improve the robustness of existing\nneural networks. The proposed approaches are very simple and only require\nupsampling the inputs or modifying the stride configuration of convolution\noperators. We test our approaches on several benchmark neural network\narchitectures, including AlexNet, VGG16, RestNet18 and PreActResNet18, and\nachieve non-trivial improvements on both natural accuracy and robustness under\nvarious attacks. Our study brings new insights into the design of robust neural\nnetworks. The code is available at \\url{https://github.com/MTandHJ/rcm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-aware Unsupervised Tagged-to-Cine MRI Synthesis with Self Disentanglement. (arXiv:2202.12474v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12474","description":"<p>Cycle reconstruction regularized adversarial training -- e.g., CycleGAN,\nDiscoGAN, and DualGAN -- has been widely used for image style transfer with\nunpaired training data. Several recent works, however, have shown that local\ndistortions are frequent, and structural consistency cannot be guaranteed.\nTargeting this issue, prior works usually relied on additional segmentation or\nconsistent feature extraction steps that are task-specific. To counter this,\nthis work aims to learn a general add-on structural feature extractor, by\nexplicitly enforcing the structural alignment between an input and its\nsynthesized image. Specifically, we propose a novel input-output image patches\nself-training scheme to achieve a disentanglement of underlying anatomical\nstructures and imaging modalities. The translator and structure encoder are\nupdated, following an alternating training protocol. In addition, the\ninformation w.r.t. imaging modality can be eliminated with an asymmetric\nadversarial game. We train, validate, and test our network on 1,768, 416, and\n1,560 unpaired subject-independent slices of tagged and cine magnetic resonance\nimaging from a total of twenty healthy subjects, respectively, demonstrating\nsuperior performance over competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stone_M/0/1/0/all/0/1\">Maureen Stone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn From the Past: Experience Ensemble Knowledge Distillation. (arXiv:2202.12488v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12488","description":"<p>Traditional knowledge distillation transfers \"dark knowledge\" of a\npre-trained teacher network to a student network, and ignores the knowledge in\nthe training process of the teacher, which we call teacher's experience.\nHowever, in realistic educational scenarios, learning experience is often more\nimportant than learning results. In this work, we propose a novel knowledge\ndistillation method by integrating the teacher's experience for knowledge\ntransfer, named experience ensemble knowledge distillation (EEKD). We save a\nmoderate number of intermediate models from the training process of the teacher\nmodel uniformly, and then integrate the knowledge of these intermediate models\nby ensemble technique. A self-attention module is used to adaptively assign\nweights to different intermediate models in the process of knowledge transfer.\nThree principles of constructing EEKD on the quality, weights and number of\nintermediate models are explored. A surprising conclusion is found that strong\nensemble teachers do not necessarily produce strong students. The experimental\nresults on CIFAR-100 and ImageNet show that EEKD outperforms the mainstream\nknowledge distillation methods and achieves the state-of-the-art. In\nparticular, EEKD even surpasses the standard ensemble distillation on the\npremise of saving training cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monogenic Wavelet Scattering Network for Texture Image Classification. (arXiv:2202.12491v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12491","description":"<p>The scattering transform network (STN), which has a similar structure as that\nof a popular convolutional neural network except its use of predefined\nconvolution filters and a small number of layers, can generates a robust\nrepresentation of an input signal relative to small deformations. We propose a\nnovel Monogenic Wavelet Scattering Network (MWSN) for 2D texture image\nclassification through a cascade of monogenic wavelet filtering with nonlinear\nmodulus and averaging operators by replacing the 2D Morlet wavelet filtering in\nthe standard STN. Our MWSN can extract useful hierarchical and directional\nfeatures with interpretable coefficients, which can be further compressed by\nPCA and fed into a classifier. Using the CUReT texture image database, we\ndemonstrate the superior performance of our MWSN over the standard STN. This\nperformance improvement can be explained by the natural extension of 1D\nanalyticity to 2D monogenicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chak_W/0/1/0/all/0/1\">Wai Ho Chak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saito_N/0/1/0/all/0/1\">Naoki Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Optimizer for Diffeomorphic Image Registration. (arXiv:2202.12498v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12498","description":"<p>Diffeomorphic image registration is the underlying technology in medical\nimage processing which enables the invertibility and point-to-point\ncorrespondence. Recently, numerous learning-based methods utilizing\nconvolutional neural networks (CNNs) have been proposed for registration\nproblems. Compared with the speed boosting, accuracy improvement brought by the\ncomplicated CNN-based methods is minor. To tackle this problem, we propose a\nrapid and accurate Implicit Optimizer for Diffeomorphic Image Registration\n(IDIR) which utilizes the Deep Implicit Function as the neural velocity field\n(NVF) whose input is the point coordinate p and output is velocity vector at\nthat point v. To reduce the huge memory consumption brought by NVF for 3D\nvolumes, a sparse sampling is employed to the framework. We evaluate our method\non two 3D large-scale MR brain scan datasets, the results show that our\nproposed method provides faster and better registration results than\nconventional image registration approaches and outperforms the learning-based\nmethods by a significant margin while maintaining the desired diffeomorphic\nproperties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanlin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RRL:Regional Rotation Layer in Convolutional Neural Networks. (arXiv:2202.12509v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12509","description":"<p>Convolutional Neural Networks (CNNs) perform very well in image\nclassification and object detection in recent years, but even the most advanced\nmodels have limited rotation invariance. Known solutions include the\nenhancement of training data and the increase of rotation invariance by\nglobally merging the rotation equivariant features. These methods either\nincrease the workload of training or increase the number of model parameters.\nTo address this problem, this paper proposes a module that can be inserted into\nthe existing networks, and directly incorporates the rotation invariance into\nthe feature extraction layers of the CNNs. This module does not have learnable\nparameters and will not increase the complexity of the model. At the same time,\nonly by training the upright data, it can perform well on the rotated testing\nset. These advantages will be suitable for fields such as biomedicine and\nastronomy where it is difficult to obtain upright samples or the target has no\ndirectionality. Evaluate our module with LeNet-5, ResNet-18 and tiny-yolov3, we\nget impressive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zongbo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingwang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaixu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TeachAugment: Data Augmentation Optimization Using Teacher Knowledge. (arXiv:2202.12513v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12513","description":"<p>Optimization of image transformation functions for the purpose of data\naugmentation has been intensively studied. In particular, adversarial data\naugmentation strategies, which search augmentation maximizing task loss, show\nsignificant improvement in the model generalization for many tasks. However,\nthe existing methods require careful parameter tuning to avoid excessively\nstrong deformations that take away image features critical for acquiring\ngeneralization. In this paper, we propose a data augmentation optimization\nmethod based on the adversarial strategy called TeachAugment, which can produce\ninformative transformed images to the model without requiring careful tuning by\nleveraging a teacher model. Specifically, the augmentation is searched so that\naugmented images are adversarial for the target model and recognizable for the\nteacher model. We also propose data augmentation using neural networks, which\nsimplifies the search space design and allows for updating of the data\naugmentation using the gradient method. We show that TeachAugment outperforms\nexisting methods in experiments of image classification, semantic segmentation,\nand unsupervised representation learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Teppei Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful learning with sure data for lung nodule diagnosis. (arXiv:2202.12515v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12515","description":"<p>Recent evolution in deep learning has proven its value for CT-based lung\nnodule classification. Most current techniques are intrinsically black-box\nsystems, suffering from two generalizability issues in clinical practice.\nFirst, benign-malignant discrimination is often assessed by human observers\nwithout pathologic diagnoses at the nodule level. We termed these data as\n\"unsure data\". Second, a classifier does not necessarily acquire reliable\nnodule features for stable learning and robust prediction with patch-level\nlabels during learning. In this study, we construct a sure dataset with\npathologically-confirmed labels and propose a collaborative learning framework\nto facilitate sure nodule classification by integrating unsure data knowledge\nthrough nodule segmentation and malignancy score regression. A loss function is\ndesigned to learn reliable features by introducing interpretability constraints\nregulated with nodule segmentation maps. Furthermore, based on model inference\nresults that reflect the understanding from both machine and experts, we\nexplore a new nodule analysis method for similar historical nodule retrieval\nand interpretable diagnosis. Detailed experimental results demonstrate that our\napproach is beneficial for achieving improved performance coupled with faithful\nmodel reasoning for lung cancer prediction. Extensive cross-evaluation results\nfurther illustrate the effect of unsure data for deep-learning-based methods in\nlung nodule classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hanxiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_Y/0/1/0/all/0/1\">Yulei Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_F/0/1/0/all/0/1\">Feng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhexin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yun Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network. (arXiv:2202.12519v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12519","description":"<p>Nowadays, hand gesture recognition has become an alternative for\nhuman-machine interaction. It has covered a large area of applications like 3D\ngame technology, sign language interpreting, VR (virtual reality) environment,\nand robotics. But detection of the hand portion has become a challenging task\nin computer vision and pattern recognition communities. Deep learning algorithm\nlike convolutional neural network (CNN) architecture has become a very popular\nchoice for classification tasks, but CNN architectures suffer from some\nproblems like high variance during prediction, overfitting problem and also\nprediction errors. To overcome these problems, an ensemble of CNN-based\napproaches is presented in this paper. Firstly, the gesture portion is detected\nby using the background separation method based on binary thresholding. After\nthat, the contour portion is extracted, and the hand region is segmented. Then,\nthe images have been resized and fed into three individual CNN models to train\nthem in parallel. In the last part, the output scores of CNN models are\naveraged to construct an optimal ensemble model for the final prediction. Two\npublicly available datasets (labeled as Dataset-1 and Dataset-2) containing\ninfrared images and one self-constructed dataset have been used to validate the\nproposed system. Experimental results are compared with the existing\nstate-of-the-art approaches, and it is observed that our proposed ensemble\nmodel outperforms other existing proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1\">Abir Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1\">Tapas Kumar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_R/0/1/0/all/0/1\">Ratnakar Dash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Dual Correlation Reduction Network. (arXiv:2202.12533v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12533","description":"<p>Deep graph clustering, which aims to reveal the underlying graph structure\nand divide the nodes into different clusters without human annotations, is a\nfundamental yet challenging task. However, we observed that the existing\nmethods suffer from the representation collapse problem and easily tend to\nencode samples with different classes into the same latent embedding.\nConsequently, the discriminative capability of nodes is limited, resulting in\nsub-optimal clustering performance. To address this problem, we propose a novel\ndeep graph clustering algorithm termed Improved Dual Correlation Reduction\nNetwork (IDCRN) through improving the discriminative capability of samples.\nSpecifically, by approximating the cross-view feature correlation matrix to an\nidentity matrix, we reduce the redundancy between different dimensions of\nfeatures, thus improving the discriminative capability of the latent space\nexplicitly. Meanwhile, the cross-view sample correlation matrix is forced to\napproximate the designed clustering-refined adjacency matrix to guide the\nlearned latent representation to recover the affinity matrix even across views,\nthus enhancing the discriminative capability of features implicitly. Moreover,\nwe avoid the collapsed representation caused by the over-smoothing issue in\nGraph Convolutional Networks (GCNs) through an introduced propagation\nregularization term, enabling IDCRN to capture the long-range information with\nthe shallow network structure. Extensive experimental results on six benchmarks\nhave demonstrated the effectiveness and the efficiency of IDCRN compared to the\nexisting state-of-the-art deep graph clustering algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xihong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ensemble Approach for Patient Prognosis of Head and Neck Tumor Using Multimodal Data. (arXiv:2202.12537v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12537","description":"<p>Accurate prognosis of a tumor can help doctors provide a proper course of\ntreatment and, therefore, save the lives of many. Traditional machine learning\nalgorithms have been eminently useful in crafting prognostic models in the last\nfew decades. Recently, deep learning algorithms have shown significant\nimprovement when developing diagnosis and prognosis solutions to different\nhealthcare problems. However, most of these solutions rely solely on either\nimaging or clinical data. Utilizing patient tabular data such as demographics\nand patient medical history alongside imaging data in a multimodal approach to\nsolve a prognosis task has started to gain more interest recently and has the\npotential to create more accurate solutions. The main issue when using clinical\nand imaging data to train a deep learning model is to decide on how to combine\nthe information from these sources. We propose a multimodal network that\nensembles deep multi-task logistic regression (MTLR), Cox proportional hazard\n(CoxPH) and CNN models to predict prognostic outcomes for patients with head\nand neck tumors using patients' clinical and imaging (CT and PET) data.\nFeatures from CT and PET scans are fused and then combined with patients'\nelectronic health records for the prediction. The proposed model is trained and\ntested on 224 and 101 patient records respectively. Experimental results show\nthat our proposed ensemble solution achieves a C-index of 0.72 on The HECKTOR\ntest set that saved us the first place in prognosis task of the HECKTOR\nchallenge. The full implementation based on PyTorch is available on\n\\url{https://github.com/numanai/BioMedIA-Hecktor2021}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saeed_N/0/1/0/all/0/1\">Numan Saeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majzoub_R/0/1/0/all/0/1\">Roba Al Majzoub</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sobirov_I/0/1/0/all/0/1\">Ikboljon Sobirov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"6D Rotation Representation For Unconstrained Head Pose Estimation. (arXiv:2202.12555v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12555","description":"<p>In this paper, we present a method for unconstrained end-to-end head pose\nestimation. We address the problem of ambiguous rotation labels by introducing\nthe rotation matrix formalism for our ground truth data and propose a\ncontinuous 6D rotation matrix representation for efficient and robust direct\nregression. This way, our method can learn the full rotation appearance which\nis contrary to previous approaches that restrict the pose prediction to a\nnarrow-angle for satisfactory results. In addition, we propose a geodesic\ndistance-based loss to penalize our network with respect to the SO(3) manifold\ngeometry. Experiments on the public AFLW2000 and BIWI datasets demonstrate that\nour proposed method significantly outperforms other state-of-the-art methods by\nup to 20\\%. We open-source our training and testing code along with our\npre-trained models: https://github.com/thohemp/6DRepNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hempel_T/0/1/0/all/0/1\">Thorsten Hempel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelrahman_A/0/1/0/all/0/1\">Ahmed A. Abdelrahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Hamadi_A/0/1/0/all/0/1\">Ayoub Al-Hamadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An exploration of the performances achievable by combining unsupervised background subtraction algorithms. (arXiv:2202.12563v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12563","description":"<p>Background subtraction (BGS) is a common choice for performing motion\ndetection in video. Hundreds of BGS algorithms are released every year, but\ncombining them to detect motion remains largely unexplored. We found that\ncombination strategies allow to capitalize on this massive amount of available\nBGS algorithms, and offer significant space for performance improvement. In\nthis paper, we explore sets of performances achievable by 6 strategies\ncombining, pixelwise, the outputs of 26 unsupervised BGS algorithms, on the\nCDnet 2014 dataset, both in the ROC space and in terms of the F1 score. The\nchosen strategies are representative for a large panel of strategies, including\nboth deterministic and non-deterministic ones, voting and learning. In our\nexperiments, we compare our results with the state-of-the-art combinations\nIUTIS-5 and CNN-SFC, and report six conclusions, among which the existence of\nan important gap between the performances of the individual algorithms and the\nbest performances achievable by combining them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pierard_S/0/1/0/all/0/1\">S&#xe9;bastien Pi&#xe9;rard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braham_M/0/1/0/all/0/1\">Marc Braham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Intensity Order Transformation for Robust Curvilinear Object Segmentation. (arXiv:2202.12587v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12587","description":"<p>Segmentation of curvilinear structures is important in many applications,\nsuch as retinal blood vessel segmentation for early detection of vessel\ndiseases and pavement crack segmentation for road condition evaluation and\nmaintenance. Currently, deep learning-based methods have achieved impressive\nperformance on these tasks. Yet, most of them mainly focus on finding powerful\ndeep architectures but ignore capturing the inherent curvilinear structure\nfeature (e.g., the curvilinear structure is darker than the context) for a more\nrobust representation. In consequence, the performance usually drops a lot on\ncross-datasets, which poses great challenges in practice. In this paper, we aim\nto improve the generalizability by introducing a novel local intensity order\ntransformation (LIOT). Specifically, we transfer a gray-scale image into a\ncontrast-invariant four-channel image based on the intensity order between each\npixel and its nearby pixels along with the four (horizontal and vertical)\ndirections. This results in a representation that preserves the inherent\ncharacteristic of the curvilinear structure while being robust to contrast\nchanges. Cross-dataset evaluation on three retinal blood vessel segmentation\ndatasets demonstrates that LIOT improves the generalizability of some\nstate-of-the-art methods. Additionally, the cross-dataset evaluation between\nretinal blood vessel segmentation and pavement crack segmentation shows that\nLIOT is able to preserve the inherent characteristic of curvilinear structure\nwith large appearance gaps. An implementation of the proposed method is\navailable at https://github.com/TY-Shi/LIOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_T/0/1/0/all/0/1\">Tianyi Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boutry_N/0/1/0/all/0/1\">Nicolas Boutry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yongchao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geraud_T/0/1/0/all/0/1\">Thierry G&#xe9;raud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning. (arXiv:2202.12588v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12588","description":"<p>The expensive annotation cost is notoriously known as a main constraint for\nthe development of the point cloud semantic segmentation technique. In this\npaper, we propose a novel active learning-based method to tackle this problem.\nDubbed SSDR-AL, our method groups the original point clouds into superpoints\nand selects the most informative and representative ones for label acquisition.\nWe achieve the selection mechanism via a graph reasoning network that considers\nboth the spatial and structural diversity of the superpoints. To deploy SSDR-AL\nin a more practical scenario, we design a noise aware iterative labeling scheme\nto confront the \"noisy annotation\" problem introduced by previous dominant\nlabeling methods in superpoints. Extensive experiments on two point cloud\nbenchmarks demonstrate the effectiveness of SSDR-AL in the semantic\nsegmentation task. Particularly, SSDR-AL significantly outperforms the baseline\nmethod when the labeled sets are small, where SSDR-AL requires only $5.7\\%$ and\n$1.9\\%$ annotation costs to achieve the performance of $90\\%$ fully supervised\nlearning on S3DIS and Semantic3D datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1\">Feifei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yulei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras with Negative Plane. (arXiv:2202.12613v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12613","description":"<p>Visual-inertial-odometry has attracted extensive attention in the field of\nautonomous driving and robotics. The size of Field of View (FoV) plays an\nimportant role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a\nlarge FoV enables to perceive a wide range of surrounding scene elements and\nfeatures. However, when the field of the camera reaches the negative half\nplane, one cannot simply use [u,v,1]^T to represent the image feature points\nanymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for\ncameras with extremely large FoV. We leverage a three-dimensional vector with\nunit length to represent feature points, and design a series of algorithms to\novercome this challenge. To address the scarcity of panoramic visual odometry\ndatasets with ground-truth location and pose, we present the PALVIO dataset,\ncollected with a Panoramic Annular Lens (PAL) system with an entire FoV of\n360x(40-120) degrees and an IMU sensor. With a comprehensive variety of\nexperiments, the proposed LF-VIO is verified on both the established PALVIO\nbenchmark and a public fisheye camera dataset with a FoV of 360x(0-93.5)\ndegrees. LF-VIO outperforms state-of-the-art visual-inertial-odometry methods.\nOur dataset and code are made publicly available at\nhttps://github.com/flysoaryun/LF-VIO\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Answering and Explanation for Visual Commonsense Reasoning. (arXiv:2202.12626v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12626","description":"<p>Visual Commonsense Reasoning (VCR), deemed as one challenging extension of\nthe Visual Question Answering (VQA), endeavors to pursue a more high-level\nvisual comprehension. It is composed of two indispensable processes: question\nanswering over a given image and rationale inference for answer explanation.\nOver the years, a variety of methods tackling VCR have advanced the performance\non the benchmark dataset. Despite significant as these methods are, they often\ntreat the two processes in a separate manner and hence decompose the VCR into\ntwo irrelevant VQA instances. As a result, the pivotal connection between\nquestion answering and rationale inference is interrupted, rendering existing\nefforts less faithful on visual reasoning. To empirically study this issue, we\nperform some in-depth explorations in terms of both language shortcuts and\ngeneralization capability to verify the pitfalls of this treatment. Based on\nour findings, in this paper, we present a plug-and-play knowledge distillation\nenhanced framework to couple the question answering and rationale inference\nprocesses. The key contribution is the introduction of a novel branch, which\nserves as the bridge to conduct processes connecting. Given that our framework\nis model-agnostic, we apply it to the existing popular baselines and validate\nits effectiveness on the benchmark dataset. As detailed in the experimental\nresults, when equipped with our framework, these baselines achieve consistent\nand significant performance improvements, demonstrating the viability of\nprocesses coupling, as well as the superiority of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kejie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting 4D Liver MRI for MR-guided Interventions. (arXiv:2202.12628v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12628","description":"<p>Organ motion poses an unresolved challenge in image-guided interventions. In\nthe pursuit of solving this problem, the research field of time-resolved\nvolumetric magnetic resonance imaging (4D MRI) has evolved. However, current\ntechniques are unsuitable for most interventional settings because they lack\nsufficient temporal and/or spatial resolution or have long acquisition times.\nIn this work, we propose a novel approach for real-time, high-resolution 4D MRI\nwith large fields of view for MR-guided interventions. To this end, we trained\na convolutional neural network (CNN) end-to-end to predict a 3D liver MRI that\ncorrectly predicts the liver's respiratory state from a live 2D navigator MRI\nof a subject. Our method can be used in two ways: First, it can reconstruct\nnear real-time 4D MRI with high quality and high resolution (209x128x128 matrix\nsize with isotropic 1.8mm voxel size and 0.6s/volume) given a dynamic\ninterventional 2D navigator slice for guidance during an intervention. Second,\nit can be used for retrospective 4D reconstruction with a temporal resolution\nof below 0.2s/volume for motion analysis and use in radiation therapy. We\nreport a mean target registration error (TRE) of 1.19 $\\pm$0.74mm, which is\nbelow voxel size. We compare our results with a state-of-the-art retrospective\n4D MRI reconstruction. Visual evaluation shows comparable quality. We show that\nsmall training sizes with short acquisition times down to 2min can already\nachieve promising results and 24min are sufficient for high quality results.\nBecause our method can be readily combined with earlier methods, acquisition\ntime can be further decreased while also limiting quality loss. We show that an\nend-to-end, deep learning formulation is highly promising for 4D MRI\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gulamhussene_G/0/1/0/all/0/1\">Gino Gulamhussene</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meyer_A/0/1/0/all/0/1\">Anneke Meyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rak_M/0/1/0/all/0/1\">Marko Rak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bashkanov_O/0/1/0/all/0/1\">Oleksii Bashkanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Omari_J/0/1/0/all/0/1\">Jazan Omari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pech_M/0/1/0/all/0/1\">Maciej Pech</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_C/0/1/0/all/0/1\">Christian Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Dirichlet uncertainty for unsupervised out-of-distribution detection of eye fundus photographs in glaucoma screening. (arXiv:2202.12634v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12634","description":"<p>The development of automatic tools for early glaucoma diagnosis with color\nfundus photographs can significantly reduce the impact of this disease.\nHowever, current state-of-the-art solutions are not robust to real-world\nscenarios, providing over-confident predictions for out-of-distribution cases.\nWith this in mind, we propose a model based on the Dirichlet distribution that\nallows to obtain class-wise probabilities together with an uncertainty\nestimation without exposure to out-of-distribution cases. We demonstrate our\napproach on the AIROGS challenge. At the start of the final test phase (8 Feb.\n2022), our method had the highest average score among all submissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_T/0/1/0/all/0/1\">Teresa Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aresta_G/0/1/0/all/0/1\">Guilherme Aresta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Amharic Handwritten Word Recognition Using Auxiliary Task. (arXiv:2202.12687v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12687","description":"<p>Amharic is one of the official languages of the Federal Democratic Republic\nof Ethiopia. It is one of the languages that use an Ethiopic script which is\nderived from Gee'z, ancient and currently a liturgical language. Amharic is\nalso one of the most widely used literature-rich languages of Ethiopia. There\nare very limited innovative and customized research works in Amharic optical\ncharacter recognition (OCR) in general and Amharic handwritten text recognition\nin particular. In this study, Amharic handwritten word recognition will be\ninvestigated. State-of-the-art deep learning techniques including convolutional\nneural networks together with recurrent neural networks and connectionist\ntemporal classification (CTC) loss were used to make the recognition in an\nend-to-end fashion. More importantly, an innovative way of complementing the\nloss function using the auxiliary task from the row-wise similarities of the\nAmharic alphabet was tested to show a significant recognition improvement over\na baseline method. Such findings will promote innovative problem-specific\nsolutions as well as will open insight to a generalized solution that emerges\nfrom problem-specific domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gondere_M/0/1/0/all/0/1\">Mesay Samuel Gondere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Durga Prasad Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boltena_A/0/1/0/all/0/1\">Abiot Sinamo Boltena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Modality Bias Recognition and Reduction. (arXiv:2202.12690v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12690","description":"<p>Making each modality in multi-modal data contribute is of vital importance to\nlearning a versatile multi-modal model. Existing methods, however, are often\ndominated by one or few of modalities during model training, resulting in\nsub-optimal performance. In this paper, we refer to this problem as modality\nbias and attempt to study it in the context of multi-modal classification\nsystematically and comprehensively. After stepping into several empirical\nanalysis, we recognize that one modality affects the model prediction more just\nbecause this modality has a spurious correlation with instance labels. In order\nto primarily facilitate the evaluation on the modality bias problem, we\nconstruct two datasets respectively for the colored digit recognition and video\naction recognition tasks in line with the Out-of-Distribution (OoD) protocol.\nCollaborating with the benchmarks in the visual question answering task, we\nempirically justify the performance degradation of the existing methods on\nthese OoD datasets, which serves as evidence to justify the modality bias\nlearning. In addition, to overcome this problem, we propose a plug-and-play\nloss function method, whereby the feature space for each label is adaptively\nlearned according to the training set statistics. Thereafter, we apply this\nmethod on eight baselines in total to test its effectiveness. From the results\non four datasets regarding the above three tasks, our method yields remarkable\nperformance improvements compared with the baselines, demonstrating its\nsuperiority on reducing the modality bias problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Harry Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs. (arXiv:2202.12692v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12692","description":"<p>Reconstructing perceived natural images from fMRI signals is one of the most\nengaging topics of neural decoding research. Prior studies had success in\nreconstructing either the low-level image features or the semantic/high-level\naspects, but rarely both. In this study, we utilized an Instance-Conditioned\nGAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate\nsemantic attributes and preserved low-level details. The IC-GAN model takes as\ninput a 119-dim noise vector and a 2048-dim instance feature vector extracted\nfrom a target image via a self-supervised learning model (SwAV ResNet-50);\nthese instance features act as a conditioning for IC-GAN image generation,\nwhile the noise vector introduces variability between samples. We trained ridge\nregression models to predict instance features, noise vectors, and dense\nvectors (the output of the first dense layer of the IC-GAN generator) of\nstimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to\nreconstruct novel test images based on these fMRI-predicted variables. The\ngenerated images presented state-of-the-art results in terms of capturing the\nsemantic attributes of the original test images while remaining relatively\nfaithful to low-level image details. Finally, we use the learned regression\nmodel and the IC-GAN generator to systematically explore and visualize the\nsemantic features that maximally drive each of several regions-of-interest in\nthe human brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozcelik_F/0/1/0/all/0/1\">Furkan Ozcelik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1\">Bhavin Choksi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozafari_M/0/1/0/all/0/1\">Milad Mozafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_L/0/1/0/all/0/1\">Leila Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online handwriting, signature and touch dynamics: tasks and potential applications in the field of security and health. (arXiv:2202.12693v1 [cs.CR])","link":"http://arxiv.org/abs/2202.12693","description":"<p>Background: An advantageous property of behavioural signals ,e.g.\nhandwriting, in contrast to morphological ones, such as iris, fingerprint, hand\ngeometry, etc., is the possibility to ask a user for a very rich amount of\ndifferent tasks. Methods: This article summarises recent findings and\napplications of different handwriting and drawing tasks in the field of\nsecurity and health. More specifically, it is focused on on-line handwriting\nand hand-based interaction, i.e. signals that utilise a digitizing device\n(specific devoted or general-purpose tablet/smartphone) during the realization\nof the tasks. Such devices permit the acquisition of on-surface dynamics as\nwell as in-air movements in time, thus providing complex and richer information\nwhen compared to the conventional pen and paper method. Conclusions: Although\nthe scientific literature reports a wide range of tasks and applications, in\nthis paper, we summarize only those providing competitive results (e.g. in\nterms of discrimination power) and having a significant impact in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Jiri Mekyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Impedovo_D/0/1/0/all/0/1\">Donato Impedovo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The effect of fatigue on the performance of online writer recognition. (arXiv:2202.12694v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12694","description":"<p>Background: The performance of biometric modalities based on things done by\nthe subject, like signature and text-based recognition, may be affected by the\nsubject state. Fatigue is one of the conditions that can significantly affect\nthe outcome of handwriting tasks. Recent research has already shown that\nphysical fatigue produces measurable differences in some features extracted\nfrom common writing and drawing tasks. It is important to establish to which\nextent physical fatigue contributes to the intra-person variability observed in\nthese biometric modalities and also to know whether the performance of\nrecognition methods is affected by fatigue. Goal: In this paper we assess the\nimpact of fatigue on intra-user variability and on the performance of\nsignature-based and text-based writer recognition approaches encompassing both\nidentification and verification. Methods: Several signature and text\nrecognition methods are considered and applied to samples gathered after\ndifferent levels of induced fatigue, measured by metabolic and mechanical\nassessment and, also by subjective perception. The recognition methods are\nDynamic Time Warping and Multi Section Vector Quantization, for signatures, and\nAllographic Text-Dependent Recognition for text in capital letters. For each\nfatigue level, the identification and verification performance of these methods\nis measured. Results: Signature shows no statistically significant intra-user\nimpact, but text does. On the other hand, performance of signature-based\nrecognition approaches is negatively impacted by fatigue whereas the impact is\nnot noticeable in text-based recognition, provided long enough sequences are\nconsidered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sesa_Nogueras_E/0/1/0/all/0/1\">Enric Sesa-Nogueras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garnacho_Castano_M/0/1/0/all/0/1\">Manuel-Vicente Garnacho-Casta&#xf1;o</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesizing Photorealistic Images with Deep Generative Learning. (arXiv:2202.12752v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12752","description":"<p>The goal of this thesis is to present my research contributions towards\nsolving various visual synthesis and generation tasks, comprising image\ntranslation, image completion, and completed scene decomposition. This thesis\nconsists of five pieces of work, each of which presents a new learning-based\napproach for synthesizing images with plausible content as well as visually\nrealistic appearance. Each work demonstrates the superiority of the proposed\napproach on image synthesis, with some further contributing to other tasks,\nsuch as depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanxia Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data refinement for fully unsupervised visual inspection using pre-trained networks. (arXiv:2202.12759v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12759","description":"<p>Anomaly detection has recently seen great progress in the field of visual\ninspection. More specifically, the use of classical outlier detection\ntechniques on features extracted by deep pre-trained neural networks have been\nshown to deliver remarkable performances on the MVTec Anomaly Detection (MVTec\nAD) dataset. However, like most other anomaly detection strategies, these\npre-trained methods assume all training data to be normal. As a consequence,\nthey cannot be considered as fully unsupervised. There exists to our knowledge\nno work studying these pre-trained methods under fully unsupervised setting. In\nthis work, we first assess the robustness of these pre-trained methods to fully\nunsupervised context, using polluted training sets (i.e. containing defective\nsamples), and show that these methods are more robust to pollution compared to\nmethods such as CutPaste. We then propose SROC, a Simple Refinement strategy\nfor One Class classification. SROC enables to remove most of the polluted\nimages from the training set, and to recover some of the lost AUC. We further\nshow that our simple heuristic competes with, and even outperforms much more\ncomplex strategies from the existing literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cordier_A/0/1/0/all/0/1\">Antoine Cordier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaoui_B/0/1/0/all/0/1\">Benjamin Missaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_P/0/1/0/all/0/1\">Pierre Gutierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Safe, Real-Time Systems: Stereo vs Images and LiDAR for 3D Object Detection. (arXiv:2202.12773v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12773","description":"<p>As object detectors rapidly improve, attention has expanded past image-only\nnetworks to include a range of 3D and multimodal frameworks, especially ones\nthat incorporate LiDAR. However, due to cost, logistics, and even some safety\nconsiderations, stereo can be an appealing alternative. Towards understanding\nthe efficacy of stereo as a replacement for monocular input or LiDAR in object\ndetectors, we show that multimodal learning with traditional disparity\nalgorithms can improve image-based results without increasing the number of\nparameters, and that learning over stereo error can impart similar 3D\nlocalization power to LiDAR in certain contexts. Furthermore, doing so also has\ncalibration benefits with respect to image-only methods. We benchmark on the\npublic dataset KITTI, and in doing so, reveal a few small but common\nalgorithmic mistakes currently used in computing metrics on that set, and offer\nefficient, provably correct alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levine_M/0/1/0/all/0/1\">Matthew Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12785","description":"<p>Calibrated confidence estimates obtained from neural networks are crucial,\nparticularly for safety-critical applications such as autonomous driving or\nmedical image diagnosis. However, although the task of confidence calibration\nhas been investigated on classification problems, thorough in\\-ves\\-tiga\\-tions\non object detection and segmentation problems are still missing. Therefore, we\nfocus on the investigation of confidence calibration for object detection and\nsegmentation models in this chapter. We introduce the concept of multivariate\nconfidence calibration that is an extension of well-known calibration methods\nto the task of object detection and segmentation. This allows for an extended\nconfidence calibration that is also aware of additional features such as\nbounding box/pixel position, shape information, etc. Furthermore, we extend the\nexpected calibration error (ECE) to measure mis\\-ca\\-li\\-bra\\-tion of object\ndetection and segmentation models. We examine several network architectures on\nMS COCO as well as on Cityscapes and show that especially object detection as\nwell as instance segmentation models are intrinsically miscalibrated given the\nintroduced definition of calibration. Using our proposed calibration methods,\nwe have been able to improve calibration so that it also has a positive impact\non the quality of segmentation masks as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuppers_F/0/1/0/all/0/1\">Fabian K&#xfc;ppers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haselhoff_A/0/1/0/all/0/1\">Anselm Haselhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronenberger_J/0/1/0/all/0/1\">Jan Kronenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jonas Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensing accident-prone features in urban scenes for proactive driving and accident prevention. (arXiv:2202.12788v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12788","description":"<p>In urban cities, visual information along and on roadways is likely to\ndistract drivers and leads to missing traffic signs and other accident-prone\nfeatures. As a solution to avoid accidents due to missing these visual cues,\nthis paper proposes a visual notification of accident-prone features to\ndrivers, based on real-time images obtained via dashcam. For this purpose,\nGoogle Street View images around accident hotspots (areas of dense accident\noccurrence) identified by accident dataset are used to train a family of deep\nconvolutional neural networks (CNNs). Trained CNNs are able to detect\naccident-prone features and classify a given urban scene into an accident\nhotspot and a non-hotspot (area of sparse accident occurrence). For given\naccident hotspot, the trained CNNs can classify it into an accident hotspot\nwith the accuracy up to 90%. The capability of detecting accident-prone\nfeatures by the family of CNNs is analyzed by a comparative study of four\ndifferent class activation map (CAM) methods, which are used to inspect\nspecific accident-prone features causing the decision of CNNs, and pixel-level\nobject class classification. The outputs of CAM methods are processed by an\nimage processing pipeline to extract only the accident-prone features that are\nexplainable to drivers with the help of visual notification system. To prove\nthe efficacy of accident-prone features, an ablation study is conducted.\nAblation of accident-prone features taking 7.7%, on average, of total area in\neach image sample causes up to 13.7% more chance of given area to be classified\nas a non-hotspot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sumit Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_P/0/1/0/all/0/1\">Praveen Kumar Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vecchietti_L/0/1/0/all/0/1\">Luiz Felipe Vecchietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Har_D/0/1/0/all/0/1\">Dongsoo Har</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving generalization with synthetic training data for deep learning based quality inspection. (arXiv:2202.12818v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12818","description":"<p>Automating quality inspection with computer vision techniques is often a very\ndata-demanding task. Specifically, supervised deep learning requires a large\namount of annotated images for training. In practice, collecting and annotating\nsuch data is not only costly and laborious, but also inefficient, given the\nfact that only a few instances may be available for certain defect classes. If\nworking with video frames can increase the number of these instances, it has a\nmajor disadvantage: the resulting images will be highly correlated with one\nanother. As a consequence, models trained under such constraints are expected\nto be very sensitive to input distribution changes, which may be caused in\npractice by changes in the acquisition system (cameras, lights), in the parts\nor in the defects aspect. In this work, we demonstrate the use of randomly\ngenerated synthetic training images can help tackle domain instability issues,\nmaking the trained models more robust to contextual changes. We detail both our\nsynthetic data generation pipeline and our deep learning methodology for\nanswering these questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cordier_A/0/1/0/all/0/1\">Antoine Cordier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_P/0/1/0/all/0/1\">Pierre Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plessis_V/0/1/0/all/0/1\">Victoire Plessis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralFusion: Neural Volumetric Rendering under Human-object Interactions. (arXiv:2202.12825v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12825","description":"<p>4D reconstruction and rendering of human activities is critical for immersive\nVR/AR experience. Recent advances still fail to recover fine geometry and\ntexture results with the level of detail present in the input images from\nsparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a\nreal-time neural human performance capture and rendering system to generate\nboth high-quality geometry and photo-realistic texture of human activities in\narbitrary novel views. We propose a neural geometry generation scheme with a\nhierarchical sampling strategy for real-time implicit geometry inference, as\nwell as a novel neural blending scheme to generate high resolution (e.g., 1k)\nand photo-realistic texture results in the novel views. Furthermore, we adopt\nneural normal blending to enhance geometry details and formulate our neural\ngeometry and texture rendering into a multi-task learning framework. Extensive\nexperiments demonstrate the effectiveness of our approach to achieve\nhigh-quality geometry and photo-realistic free view-point reconstruction for\nchallenging human performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Suyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guoxing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhuo Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kaiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RELMOBNET: A Robust Two-Stage End-To-End Training Approach For MOBILENETV3 Based Relative Camera Pose Estimation. (arXiv:2202.12838v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12838","description":"<p>Relative camera pose estimation plays a pivotal role in dealing with 3D\nreconstruction and visual localization. To address this, we propose a Siamese\nnetwork based on MobileNetV3-Large for an end-to-end relative camera pose\nregression independent of camera parameters. The proposed network uses pair of\nimages taken at different locations in the same scene to estimate the 3D\ntranslation vector and rotation vector in unit quaternion. To increase the\ngenerality of the model, rather than training it for a single scene, data for\nfour scenes are combined to train a single universal model to estimate the\nrelative pose. Further for independency of hyperparameter weighing between\ntranslation and rotation loss is not used. Instead we use the novel two-stage\ntraining procedure to learn the balance implicitly with faster convergence. We\ncompare the results obtained with the Cambridge Landmarks dataset, comprising\nof different scenes, with existing CNN-based regression methods as baselines,\ne.g., RPNet and RCPNet. The findings indicate that, when compared to RCPNet,\nproposed model improves the estimation of the translation vector by a\npercentage change of 16.11%, 28.88%, 52.27% on the Kings College, Old Hospital,\nSt Marys Church scenes from Cambridge Landmarks dataset, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_P/0/1/0/all/0/1\">Praveen Kumar Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sumit Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vecchietti_L/0/1/0/all/0/1\">Luiz Felipe Vecchietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Har_D/0/1/0/all/0/1\">Dongsoo Har</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARIA: Adversarially Robust Image Attribution for Content Provenance. (arXiv:2202.12860v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12860","description":"<p>Image attribution -- matching an image back to a trusted source -- is an\nemerging tool in the fight against online misinformation. Deep visual\nfingerprinting models have recently been explored for this purpose. However,\nthey are not robust to tiny input perturbations known as adversarial examples.\nFirst we illustrate how to generate valid adversarial images that can easily\ncause incorrect image attribution. Then we describe an approach to prevent\nimperceptible adversarial attacks on deep visual fingerprinting models, via\nrobust contrastive learning. The proposed training procedure leverages training\non $\\ell_\\infty$-bounded adversarial examples, it is conceptually simple and\nincurs only a small computational overhead. The resulting models are\nsubstantially more robust, are accurate even on unperturbed images, and perform\nwell even over a database with millions of images. In particular, we achieve\n91.6% standard and 85.1% adversarial recall under $\\ell_\\infty$-bounded\nperturbations on manipulated images compared to 80.1% and 0.0% from prior work.\nWe also show that robustness generalizes to other types of imperceptible\nperturbations unseen during training. Finally, we show how to train an\nadversarially robust image comparator model for detecting editorial changes in\nmatched images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1\">Maksym Andriushchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyang Rebecca Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oxholm_G/0/1/0/all/0/1\">Geoffrey Oxholm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gittings_T/0/1/0/all/0/1\">Thomas Gittings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1\">Nicolas Flammarion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Identify Perceptual Bugs in 3D Video Games. (arXiv:2202.12884v1 [cs.SE])","link":"http://arxiv.org/abs/2202.12884","description":"<p>Automated Bug Detection (ABD) in video games is composed of two distinct but\ncomplementary problems: automated game exploration and bug identification.\nAutomated game exploration has received much recent attention, spurred on by\ndevelopments in fields such as reinforcement learning. The complementary\nproblem of identifying the bugs present in a player's experience has for the\nmost part relied on the manual specification of rules. Although it is widely\nrecognised that many bugs of interest cannot be identified with such methods,\nlittle progress has been made in this direction. In this work we show that it\nis possible to identify a range of perceptual bugs using learning-based methods\nby making use of only the rendered game screen as seen by the player. To\nsupport our work, we have developed World of Bugs (WOB) an open platform for\ntesting ABD methods in 3D game environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilkins_B/0/1/0/all/0/1\">Benedict Wilkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathis_K/0/1/0/all/0/1\">Kostas Stathis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageGCN: Multi-Relational Image Graph Convolutional Networks for Disease Identification with Chest X-rays. (arXiv:1904.00325v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.00325","description":"<p>Image representation is a fundamental task in computer vision. However, most\nof the existing approaches for image representation ignore the relations\nbetween images and consider each input image independently. Intuitively,\nrelations between images can help to understand the images and maintain model\nconsistency over related images, leading to better explainability. In this\npaper, we consider modeling the image-level relations to generate more\ninformative image representations, and propose ImageGCN, an end-to-end graph\nconvolutional network framework for inductive multi-relational image modeling.\nWe apply ImageGCN to chest X-ray images where rich relational information is\navailable for disease identification. Unlike previous image representation\nmodels, ImageGCN learns the representation of an image using both its original\npixel features and its relationship with other images. Besides learning\ninformative representations for images, ImageGCN can also be used for object\ndetection in a weakly supervised manner. The experimental results on 3\nopen-source x-ray datasets, ChestX-ray14, CheXpert and MIMIC-CXR demonstrate\nthat ImageGCN can outperform respective baselines in both disease\nidentification and localization tasks and can achieve comparable and often\nbetter results than the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengsheng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Liang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection as Regression: Certified Object Detection by Median Smoothing. (arXiv:2007.03730v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.03730","description":"<p>Despite the vulnerability of object detectors to adversarial attacks, very\nfew defenses are known to date. While adversarial training can improve the\nempirical robustness of image classifiers, a direct extension to object\ndetection is very expensive. This work is motivated by recent progress on\ncertified classification by randomized smoothing. We start by presenting a\nreduction from object detection to a regression problem. Then, to enable\ncertified regression, where standard mean smoothing fails, we propose median\nsmoothing, which is of independent interest. We obtain the first\nmodel-agnostic, training-free, and certified defense for object detection\nagainst $\\ell_2$-bounded attacks. The code for all experiments in the paper is\navailable at <a href=\"http://github.com/Ping-C/CertifiedObjectDetection\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1\">Ping-yeh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curry_M/0/1/0/all/0/1\">Michael J. Curry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1\">Ahmed Abdelkader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aounon Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John Dickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-VQ: 'Patching Up' the Video Quality Problem. (arXiv:2011.13544v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13544","description":"<p>No-reference (NR) perceptual video quality assessment (VQA) is a complex,\nunsolved, and important problem to social and streaming media applications.\nEfficient and accurate video quality predictors are needed to monitor and guide\nthe processing of billions of shared, often imperfect, user-generated content\n(UGC). Unfortunately, current NR models are limited in their prediction\ncapabilities on real-world, \"in-the-wild\" UGC video data. To advance progress\non this problem, we created the largest (by far) subjective video quality\ndataset, containing 39, 000 realworld distorted videos and 117, 000 space-time\nlocalized video patches ('v-patches'), and 5.5M human perceptual quality\nannotations. Using this, we created two unique NR-VQA models: (a) a\nlocal-to-global region-based NR VQA architecture (called PVQ) that learns to\npredict global video quality and achieves state-of-the-art performance on 3 UGC\ndatasets, and (b) a first-of-a-kind space-time video quality mapping engine\n(called PVQ Mapper) that helps localize and visualize perceptual distortions in\nspace and time. We will make the new database and prediction models available\nimmediately following the review process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zhenqiang Ying</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_M/0/1/0/all/0/1\">Maniratnam Mandal</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1\">Deepti Ghadiyaram</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a> (1) ((1) University of Texas at Austin, (2) Facebook AI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fill-in-the-Blank: A Challenging Video Understanding Evaluation Framework. (arXiv:2104.04182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04182","description":"<p>We propose fill-in-the-blanks as a video understanding evaluation framework.\nThe task tests a model's understanding of a video by requiring the model to\npredict a masked noun phrase in the caption of the video, given the video and\nthe surrounding text. To this end, we introduce a novel dataset consisting of\n28,000 videos and fill-in-the-blank tests with multiple correct answers. The\ntask and the dataset are challenging for the current state-of-the-art systems\nto solve. This task also does not share the weaknesses of the current state of\nthe art language-informed video understanding tasks, namely: (1) video question\nanswering using multiple-choice questions, where models perform relatively well\nbecause they exploit linguistic biases in the task formulation; and (2) video\ncaptioning, which relies on an open-ended evaluation framework that is often\ninaccurate because system answers may be perceived as incorrect if they differ\nin form from the ground truth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pingxuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_I/0/1/0/all/0/1\">Ian Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stroud_J/0/1/0/all/0/1\">Jonathan C. Stroud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust fine-tuning of zero-shot models. (arXiv:2109.01903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01903","description":"<p>Large pre-trained models such as CLIP or ALIGN offer consistent accuracy\nacross a range of data distributions when performing zero-shot inference (i.e.,\nwithout fine-tuning on a specific dataset). Although existing fine-tuning\nmethods substantially improve accuracy on a given target distribution, they\noften reduce robustness to distribution shifts. We address this tension by\nintroducing a simple and effective method for improving robustness while\nfine-tuning: ensembling the weights of the zero-shot and fine-tuned models\n(WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy\nimprovements under distribution shift, while preserving high accuracy on the\ntarget distribution. On ImageNet and five derived distribution shifts, WiSE-FT\nimproves accuracy under distribution shift by 4 to 6 percentage points (pp)\nover prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves\nsimilarly large robustness gains (2 to 23 pp) on a diverse set of six further\ndistribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard\nfine-tuning on seven commonly used transfer learning datasets. These\nimprovements come at no additional computational cost during fine-tuning or\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_R/0/1/0/all/0/1\">Raphael Gontijo Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Hilti SLAM Challenge Dataset. (arXiv:2109.11316v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.11316","description":"<p>Research in Simultaneous Localization and Mapping (SLAM) has made outstanding\nprogress over the past years. SLAM systems are nowadays transitioning from\nacademic to real-world applications. However, this transition has posed new\ndemanding challenges in terms of accuracy and robustness. To develop new SLAM\nsystems that are able to address these challenges, new datasets containing\ncutting-edge hardware and realistic scenarios are required. We propose the\nHilti SLAM Challenge Dataset. Our dataset contains indoor sequences of offices,\nlabs, and construction environments and outdoor sequences of construction sites\nand parking areas. All these sequences are characterized by featureless areas\nand varying illumination conditions that are typical in real-world scenarios\nand pose great challenges to SLAM algorithms that have been developed in\nconfined lab environments. Accurate ground truth, at millimeter level, is\nprovided for each sequence. The sensor platform used to record the data\nincludes a number of visual, lidar, and inertial sensors, which are spatially\nand temporally calibrated. The purpose of this dataset is to foster the\nresearch in sensor fusion to develop SLAM algorithms that can be deployed in\ntasks where high accuracy and robustness are required, e.g., in construction\nenvironments. Many academic and industrial groups tested their SLAM systems on\nthe proposed dataset in the Hilti SLAM Challenge. The results of the challenge,\nwhich are summarized in this paper, show that the proposed dataset is an\nimportant asset in the development of new SLAM algorithms that are ready to be\ndeployed in the real-world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helmberger_M/0/1/0/all/0/1\">Michael Helmberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_K/0/1/0/all/0/1\">Kristian Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berner_B/0/1/0/all/0/1\">Beda Berner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nitish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yufeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cioffi_G/0/1/0/all/0/1\">Giovanni Cioffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04079","description":"<p>Accurate and reliable lane detection is vital for the safe performance of\nlane-keeping assistance and lane departure warning systems. However, under\ncertain challenging circumstances, it is difficult to get satisfactory\nperformance in accurately detecting the lanes from one single image as mostly\ndone in current literature. Since lane markings are continuous lines, the lanes\nthat are difficult to be accurately detected in the current single image can\npotentially be better deduced if information from previous frames is\nincorporated. This study proposes a novel hybrid spatial-temporal (ST)\nsequence-to-one deep learning architecture. This architecture makes full use of\nthe ST information in multiple continuous image frames to detect the lane\nmarkings in the very last frame. Specifically, the hybrid model integrates the\nfollowing aspects: (a) the single image feature extraction module equipped with\nthe spatial convolutional neural network; (b) the ST feature integration module\nconstructed by ST recurrent neural network; (c) the encoder-decoder structure,\nwhich makes this image segmentation problem work in an end-to-end supervised\nlearning format. Extensive experiments reveal that the proposed model\narchitecture can effectively handle challenging driving scenes and outperforms\navailable state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yongqi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Sandeep Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arem_B/0/1/0/all/0/1\">Bart van Arem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farah_H/0/1/0/all/0/1\">Haneen Farah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIgLib & SmokeyNet: Dataset and Deep Learning Model for Real-Time Wildland Fire Smoke Detection. (arXiv:2112.08598v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08598","description":"<p>The size and frequency of wildland fires in the western United States have\ndramatically increased in recent years. On high-fire-risk days, a small fire\nignition can rapidly grow and become out of control. Early detection of fire\nignitions from initial smoke can assist the response to such fires before they\nbecome difficult to manage. Past deep learning approaches for wildfire smoke\ndetection have suffered from small or unreliable datasets that make it\ndifficult to extrapolate performance to real-world scenarios. In this work, we\npresent the Fire Ignition Library (FIgLib), a publicly available dataset of\nnearly 25,000 labeled wildfire smoke images as seen from fixed-view cameras\ndeployed in Southern California. We also introduce SmokeyNet, a novel deep\nlearning architecture using spatiotemporal information from camera imagery for\nreal-time wildfire smoke detection. When trained on the FIgLib dataset,\nSmokeyNet outperforms comparable baselines and rivals human performance. We\nhope that the availability of the FIgLib dataset and the SmokeyNet architecture\nwill inspire further research into deep learning methods for wildfire smoke\ndetection, leading to automated notification systems that reduce the time to\nwildfire response.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dewangan_A/0/1/0/all/0/1\">Anshuman Dewangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_Y/0/1/0/all/0/1\">Yash Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_H/0/1/0/all/0/1\">Hans-Werner Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernon_F/0/1/0/all/0/1\">Frank Vernon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Ismael Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altintas_I/0/1/0/all/0/1\">Ilkay Altintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cottrell_G/0/1/0/all/0/1\">Garrison W. Cottrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Mai H. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filtering In Neural Implicit Functions. (arXiv:2201.13013v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13013","description":"<p>Neural implicit functions are highly effective for representing many kinds of\ndata, including images and 3D surfaces. However, the implicit functions learned\nby neural networks usually include over-smoothed patches or noisy artifacts\ninto the results if the data has many scales of details or a wide range of\nfrequencies. Adapting the functions containing both noise and over-smoothed\nregions may suffer from either over smoothing or noisy issues. To overcome this\nchallenge, we propose a new framework, coined FINN, that integrates a filtering\nmodule into the neural network to perform data reconstruction while filtering\nartifacts. The filtering module has a smoothing operator that acts on the\nintermediate results of the network and a recovering operator that brings\ndistinct details from the input back to the regions overly smoothed. The\nproposed method significantly alleviates over smoothing or noisy issues. We\ndemonstrate the advantage of the FINN on the tasks of image regression and\nsurface reconstruction and showcases significant improvement compared to\nstate-of-the-art methods. In addition, FINN also yields better performance in\nboth convergence speed and network stability. Source code is available at\nhttps://github.com/yixin26/FINN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yixin Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PARCEL: Physics-based unsupervised contrastive representation learning for parallel MR imaging. (arXiv:2202.01494v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.01494","description":"<p>With the successful application of deep learning to magnetic resonance (MR)\nimaging, parallel imaging techniques based on neural networks have attracted\nwide attention. However, in the absence of high-quality, fully sampled datasets\nfor training, the performance of these methods is limited. To address this\nissue, this paper proposes a Physics-bAsed unsupeRvised Contrastive\nrEpresentation Learning (PARCEL) method to speed up parallel MR imaging.\nSpecifically, PARCEL has a parallel framework to contrastively learn two\nbranches of model-based unrolling networks directly from augmented undersampled\nk-space data. A sophisticated co-training loss with three essential components\nhas been designed to guide the two networks in capturing the inherent features\nand representations for MR images. And the final MR image is reconstructed with\nthe trained contrastive networks. PARCEL was evaluated on in vivo datasets and\ncompared to five state-of-the-art methods. The results show that PARCEL is able\nto learn useful representations for more accurate MR reconstructions without\nrelying on fully sampled datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_R/0/1/0/all/0/1\">Ruoyou Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1\">Juan Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_Y/0/1/0/all/0/1\">Yan Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hairong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Prediction Problem Archive. (arXiv:2202.03574v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.03574","description":"<p>Structured prediction problems are one of the fundamental tools in machine\nlearning. In order to facilitate algorithm development for their numerical\nsolution, we collect in one place a large number of datasets in easy to read\nformats for a diverse set of problem classes. We provide archival links to\ndatasets, description of the considered problems and problem formats, and a\nshort summary of problem characteristics including size, number of instances\netc. For reference we also give a non-exhaustive selection of algorithms\nproposed in the literature for their solution. We hope that this central\nrepository will make benchmarking and comparison to established works easier.\nWe welcome submission of interesting new datasets and algorithms for inclusion\nin our archive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hornakova_A/0/1/0/all/0/1\">Andrea Hornakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roetzer_P/0/1/0/all/0/1\">Paul Roetzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savchynskyy_B/0/1/0/all/0/1\">Bogdan Savchynskyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Specific Attention is one more thing you need for object detection. (arXiv:2202.09048v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09048","description":"<p>Various models have been proposed to solve the object detection problem.\nHowever, most of them require many hand-designed components to demonstrate good\nperformance. To mitigate these issues, Transformer based DETR and its variant\nDeformable DETR were suggested. They solved much of the complex issue of\ndesigning a head of object detection model but it has not been generally clear\nthat the Transformer-based models could be considered as the state-of-the-art\nmethod in object detection without doubt. Furthermore, as DETR adapted\nTransformer method only for the detection head, but still with including CNN\nfor the backbone body, it has not been certain that it would be possible to\nbuild the competent end-to-end pipeline with the combination of attention\nmodules. In this paper, we propose that combining several attention modules\nwith our new Task Specific Split Transformer(TSST) is a fairly good enough\nmethod to produce the best COCO results without traditionally hand-designed\ncomponents. By splitting generally purposed attention module into two separated\nmission specific attention module, the proposed method addresses the way to\ndesign simpler object detection models than before. Extensive experiments on\nthe COCO benchmark demonstrate the effectiveness of our approach. Code is\nreleased at https://github.com/navervision/tsst\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang Yon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modern Augmented Reality: Applications, Trends, and Future Directions. (arXiv:2202.09450v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09450","description":"<p>Augmented reality (AR) is one of the relatively old, yet trending areas in\nthe intersection of computer vision and computer graphics with numerous\napplications in several areas, from gaming and entertainment, to education and\nhealthcare. Although it has been around for nearly fifty years, it has seen a\nlot of interest by the research community in the recent years, mainly because\nof the huge success of deep learning models for various computer vision and AR\napplications, which made creating new generations of AR technologies possible.\nThis work tries to provide an overview of modern augmented reality, from both\napplication-level and technical perspective. We first give an overview of main\nAR applications, grouped into more than ten categories. We then give an\noverview of around 100 recent promising machine learning based works developed\nfor AR systems, such as deep learning works for AR shopping (clothing, makeup),\nAR based image filters (such as Snapchat's lenses), AR animations, and more. In\nthe end we discuss about some of the current challenges in AR domain, and the\nfuture directions in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minaee_S/0/1/0/all/0/1\">Shervin Minaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Winning Solution to the iFLYTEK Challenge 2021 Cultivated Land Extraction from High-Resolution Remote Sensing Image. (arXiv:2202.10974v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10974","description":"<p>Extracting cultivated land accurately from high-resolution remote images is a\nbasic task for precision agriculture. This report introduces our solution to\nthe iFLYTEK challenge 2021 cultivated land extraction from high-resolution\nremote sensing image. The challenge requires segmenting cultivated land objects\nin very high-resolution multispectral remote sensing images. We established a\nhighly effective and efficient pipeline to solve this problem. We first divided\nthe original images into small tiles and separately performed instance\nsegmentation on each tile. We explored several instance segmentation algorithms\nthat work well on natural images and developed a set of effective methods that\nare applicable to remote sensing images. Then we merged the prediction results\nof all small tiles into seamless, continuous segmentation results through our\nproposed overlap-tile fusion strategy. We achieved the first place among 486\nteams in the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqiu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-Based Affordance Detection via Cyclic Bilateral Interaction. (arXiv:2202.12076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12076","description":"<p>Affordance detection, which refers to perceiving objects with potential\naction possibilities in images, is a challenging task since the possible\naffordance depends on the person's purpose in real-world application scenarios.\nThe existing works mainly extract the inherent human-object dependencies from\nimage/video to accommodate affordance properties that change dynamically. In\nthis paper, we explore to perceive affordance from a vision-language\nperspective and consider the challenging phrase-based affordance detection\nproblem,i.e., given a set of phrases describing the action purposes, all the\nobject regions in a scene with the same affordance should be detected. To this\nend, we propose a cyclic bilateral consistency enhancement network (CBCE-Net)\nto align language and vision features progressively. Specifically, the\npresented CBCE-Net consists of a mutual guided vision-language module that\nupdates the common features of vision and language in a progressive manner, and\na cyclic interaction module (CIM) that facilitates the perception of possible\ninteraction with objects in a cyclic manner. In addition, we extend the public\nPurpose-driven Affordance Dataset (PAD) by annotating affordance categories\nwith short phrases. The contrastive experimental results demonstrate the\nsuperiority of our method over nine typical methods from four relevant fields\nin terms of both objective metrics and visual quality. The related code and\ndataset will be released at \\url{https://github.com/lulsheng/CBCE-Net}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liangsheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}