{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"How to Dissect a Muppet: The Structure of Transformer Embedding Spaces. (arXiv:2206.03529v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03529","description":"<p>Pretrained embeddings based on the Transformer architecture have taken the\nNLP community by storm. We show that they can mathematically be reframed as a\nsum of vector factors and showcase how to use this reframing to study the\nimpact of each component. We provide evidence that multi-head attentions and\nfeed-forwards are not equally useful in all downstream applications, as well as\na quantitative overview of the effects of finetuning on the overall embedding\nspace. This approach allows us to draw connections to a wide range of previous\nstudies, from vector space anisotropy to attention weights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mickus_T/0/1/0/all/0/1\">Timothee Mickus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_M/0/1/0/all/0/1\">Mathieu Constant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guidelines and a Corpus for Extracting Biographical Events. (arXiv:2206.03547v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03547","description":"<p>Despite biographies are widely spread within the Semantic Web, resources and\napproaches to automatically extract biographical events are limited. Such\nlimitation reduces the amount of structured, machine-readable biographical\ninformation, especially about people belonging to underrepresented groups. Our\nwork challenges this limitation by providing a set of guidelines for the\nsemantic annotation of life events. The guidelines are designed to be\ninteroperable with existing ISO-standards for semantic annotation: ISO-TimeML\n(ISO-24617-1), and SemAF (ISO-24617-4). Guidelines were tested through an\nannotation task of Wikipedia biographies of underrepresented writers, namely\nauthors born in non-Western countries, migrants, or belonging to ethnic\nminorities. 1,000 sentences were annotated by 4 annotators with an average\nInter-Annotator Agreement of 0.825. The resulting corpus was mapped on\nOntoNotes. Such mapping allowed to to expand our corpus, showing that already\nexisting resources may be exploited for the biographical event extraction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stranisci_M/0/1/0/all/0/1\">Marco Antonio Stranisci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensa_E/0/1/0/all/0/1\">Enrico Mensa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diakite_O/0/1/0/all/0/1\">Ousmane Diakite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radicioni_D/0/1/0/all/0/1\">Daniele Radicioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damiano_R/0/1/0/all/0/1\">Rossana Damiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"1Cademy at Semeval-2022 Task 1: Investigating the Effectiveness of Multilingual, Multitask, and Language-Agnostic Tricks for the Reverse Dictionary Task. (arXiv:2206.03702v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03702","description":"<p>This paper describes our system for the SemEval2022 task of matching\ndictionary glosses to word embeddings. We focus on the Reverse Dictionary Track\nof the competition, which maps multilingual glosses to reconstructed vector\nrepresentations. More specifically, models convert the input of sentences to\nthree types of embeddings: SGNS, Char, and Electra. We propose several\nexperiments for applying neural network cells, general multilingual and\nmultitask structures, and language-agnostic tricks to the task. We also provide\ncomparisons over different types of word embeddings and ablation studies to\nsuggest helpful strategies. Our initial transformer-based model achieves\nrelatively low performance. However, trials on different retokenization\nmethodologies indicate improved performance. Our proposed Elmobased monolingual\nmodel achieves the highest outcome, and its multitask, and multilingual\nvarieties show competitive results as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lashkarashvili_N/0/1/0/all/0/1\">Nineli Lashkarashvili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning. (arXiv:2206.03715v1 [cs.AI])","link":"http://arxiv.org/abs/2206.03715","description":"<p>Commonsense reasoning systems should be able to generalize to diverse\nreasoning cases. However, most state-of-the-art approaches depend on expensive\ndata annotations and overfit to a specific benchmark without learning how to\nperform general semantic reasoning. To overcome these drawbacks, zero-shot QA\nsystems have shown promise as a robust learning scheme by transforming a\ncommonsense knowledge graph (KG) into synthetic QA-form samples for model\ntraining. Considering the increasing type of different commonsense KGs, this\npaper aims to extend the zero-shot transfer learning scenario into\nmultiple-source settings, where different KGs can be utilized synergetically.\nTowards this goal, we propose to mitigate the loss of knowledge from the\ninterference among the different knowledge sources, by developing a modular\nvariant of the knowledge aggregation as a new zero-shot commonsense reasoning\nframework. Results on five commonsense reasoning benchmarks demonstrate the\nefficacy of our framework, improving the performance with multiple KGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yu Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_B/0/1/0/all/0/1\">Beong-woo Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1\">Jinyoung Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Set Interdependence Transformer: Set-to-Sequence Neural Networks for Permutation Learning and Structure Prediction. (arXiv:2206.03720v1 [cs.LG])","link":"http://arxiv.org/abs/2206.03720","description":"<p>The task of learning to map an input set onto a permuted sequence of its\nelements is challenging for neural networks. Set-to-sequence problems occur in\nnatural language processing, computer vision and structure prediction, where\ninteractions between elements of large sets define the optimal output. Models\nmust exhibit relational reasoning, handle varying cardinalities and manage\ncombinatorial complexity. Previous attention-based methods require $n$ layers\nof their set transformations to explicitly represent $n$-th order relations.\nOur aim is to enhance their ability to efficiently model higher-order\ninteractions through an additional interdependence component. We propose a\nnovel neural set encoding method called the Set Interdependence Transformer,\ncapable of relating the set's permutation invariant representation to its\nelements within sets of any cardinality. We combine it with a permutation\nlearning module into a complete, 3-part set-to-sequence model and demonstrate\nits state-of-the-art performance on a number of tasks. These range from\ncombinatorial optimization problems, through permutation learning challenges on\nboth synthetic and established NLP datasets for sentence ordering, to a novel\ndomain of product catalog structure prediction. Additionally, the network's\nability to generalize to unseen sequence lengths is investigated and a\ncomparative empirical analysis of the existing methods' ability to learn\nhigher-order interactions is provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jurewicz_M/0/1/0/all/0/1\">Mateusz Jurewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Zero-Shot Cross-Lingual Transfer in Legal Topic Classification. (arXiv:2206.03785v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03785","description":"<p>We consider zero-shot cross-lingual transfer in legal topic classification\nusing the recent MultiEURLEX dataset. Since the original dataset contains\nparallel documents, which is unrealistic for zero-shot cross-lingual transfer,\nwe develop a new version of the dataset without parallel documents. We use it\nto show that translation-based methods vastly outperform cross-lingual\nfine-tuning of multilingually pre-trained models, the best previous zero-shot\ntransfer method for MultiEURLEX. We also develop a bilingual teacher-student\nzero-shot transfer approach, which exploits additional unlabeled documents of\nthe target language and performs better than a model fine-tuned directly on\nlabeled target language documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xenouleas_S/0/1/0/all/0/1\">Stratos Xenouleas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoukara_A/0/1/0/all/0/1\">Alexia Tsoukara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagiotakis_G/0/1/0/all/0/1\">Giannis Panagiotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Open corpus of the Veps and Karelian languages: overview and applications. (arXiv:2206.03870v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03870","description":"<p>A growing priority in the study of Baltic-Finnic languages of the Republic of\nKarelia has been the methods and tools of corpus linguistics. Since 2016,\nlinguists, mathematicians, and programmers at the Karelian Research Centre have\nbeen working with the Open Corpus of the Veps and Karelian Languages (VepKar),\nwhich is an extension of the Veps Corpus created in 2009. The VepKar corpus\ncomprises texts in Karelian and Veps, multifunctional dictionaries linked to\nthem, and software with an advanced system of search using various criteria of\nthe texts (language, genre, etc.) and numerous linguistic categories (lexical\nand grammatical search in texts was implemented thanks to the generator of word\nforms that we created earlier). A corpus of 3000 texts was compiled, texts were\nuploaded and marked up, the system for classifying texts into languages,\ndialects, types and genres was introduced, and the word-form generator was\ncreated. Future plans include developing a speech module for working with audio\nrecordings and a syntactic tagging module using morphological analysis outputs.\nOwing to continuous functional advancements in the corpus manager and ongoing\nVepKar enrichment with new material and text markup, users can handle a wide\nrange of scientific and applied tasks. In creating the universal national\nVepKar corpus, its developers and managers strive to preserve and exhibit as\nfully as possible the state of the Veps and Karelian languages in the 19th-21st\ncenturies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boyko_T/0/1/0/all/0/1\">Tatyana Boyko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaitseva_N/0/1/0/all/0/1\">Nina Zaitseva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krizhanovskaya_N/0/1/0/all/0/1\">Natalia Krizhanovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krizhanovsky_A/0/1/0/all/0/1\">Andrew Krizhanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_I/0/1/0/all/0/1\">Irina Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellinen_N/0/1/0/all/0/1\">Nataliya Pellinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodionova_A/0/1/0/all/0/1\">Aleksandra Rodionova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counseling Summarization using Mental Health Knowledge Guided Utterance Filtering. (arXiv:2206.03886v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03886","description":"<p>The psychotherapy intervention technique is a multifaceted conversation\nbetween a therapist and a patient. Unlike general clinical discussions,\npsychotherapy's core components (viz. symptoms) are hard to distinguish, thus\nbecoming a complex problem to summarize later. A structured counseling\nconversation may contain discussions about symptoms, history of mental health\nissues, or the discovery of the patient's behavior. It may also contain\ndiscussion filler words irrelevant to a clinical summary. We refer to these\nelements of structured psychotherapy as counseling components. In this paper,\nthe aim is mental health counseling summarization to build upon domain\nknowledge and to help clinicians quickly glean meaning. We create a new dataset\nafter annotating 12.9K utterances of counseling components and reference\nsummaries for each dialogue. Further, we propose ConSum, a novel\ncounseling-component guided summarization model. ConSum undergoes three\nindependent modules. First, to assess the presence of depressive symptoms, it\nfilters utterances utilizing the Patient Health Questionnaire (PHQ-9), while\nthe second and third modules aim to classify counseling components. At last, we\npropose a problem-specific Mental Health Information Capture (MHIC) evaluation\nmetric for counseling summaries. Our comparative study shows that we improve on\nperformance and generate cohesive, semantic, and coherent summaries. We\ncomprehensively analyze the generated summaries to investigate the capturing of\npsychotherapy elements. Human and clinical evaluations on the summary show that\nConSum generates quality summary. Further, mental health experts validate the\nclinical acceptability of the ConSum. Lastly, we discuss the uniqueness in\nmental health counseling summarization in the real world and show evidences of\nits deployment on an online application with the support of mpathic.ai\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Aseem Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_T/0/1/0/all/0/1\">Tharun Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peregrine_S/0/1/0/all/0/1\">Sarah Peregrine</a> (Grin) <a href=\"http://arxiv.org/find/cs/1/au:+Lord/0/1/0/all/0/1\">Lord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Prompting Toward Controllable Response Generation. (arXiv:2206.03931v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03931","description":"<p>Much literature has shown that prompt-based learning is an efficient method\nto make use of the large pre-trained language model. Recent works also exhibit\nthe possibility of steering a chatbot's output by plugging in an appropriate\nprompt. Gradient-based methods are often used to perturb the prompts. However,\nsome language models are not even available to the public. In this work, we\nfirst explored the combination of prompting and reinforcement learning (RL) to\nsteer models' generation without accessing any of the models' parameters.\nSecond, to reduce the training effort and enhance the generalizability to the\nunseen task, we apply multi-task learning to make the model learn to generalize\nto new tasks better. The experiment results show that our proposed method can\nsuccessfully control several state-of-the-art (SOTA) dialogue models without\naccessing their parameters. Furthermore, the model demonstrates the strong\nability to quickly adapt to an unseen task in fewer steps than the baseline\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hsuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1\">Pohan Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Cheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_C/0/1/0/all/0/1\">Chung Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shang-Tse Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TURJUMAN: A Public Toolkit for Neural Arabic Machine Translation. (arXiv:2206.03933v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03933","description":"<p>We present TURJUMAN, a neural toolkit for translating from 20 languages into\nModern Standard Arabic (MSA). TURJUMAN exploits the recently-introduced\ntext-to-text Transformer AraT5 model, endowing it with a powerful ability to\ndecode into Arabic. The toolkit offers the possibility of employing a number of\ndiverse decoding methods, making it suited for acquiring paraphrases for the\nMSA translations as an added value. To train TURJUMAN, we sample from publicly\navailable parallel data employing a simple semantic similarity method to ensure\ndata quality. This allows us to prepare and release AraOPUS-20, a new machine\ntranslation benchmark. We publicly release our translation toolkit (TURJUMAN)\nas well as our benchmark dataset (AraOPUS-20).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models. (arXiv:2206.03945v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03945","description":"<p>Motivations for methods in explainable artificial intelligence (XAI) often\ninclude detecting, quantifying and mitigating bias, and contributing to making\nmachine learning models fairer. However, exactly how an XAI method can help in\ncombating biases is often left unspecified. In this paper, we briefly review\ntrends in explainability and fairness in NLP research, identify the current\npractices in which explainability methods are applied to detect and mitigate\nbias, and investigate the barriers preventing XAI methods from being used more\nwidely in tackling fairness issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balkir_E/0/1/0/all/0/1\">Esma Balkir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proactively Reducing the Hate Intensity of Online Posts via Hate Speech Normalization. (arXiv:2206.04007v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04007","description":"<p>Curbing online hate speech has become the need of the hour; however, a\nblanket ban on such activities is infeasible for several geopolitical and\ncultural reasons. To reduce the severity of the problem, in this paper, we\nintroduce a novel task, hate speech normalization, that aims to weaken the\nintensity of hatred exhibited by an online post. The intention of hate speech\nnormalization is not to support hate but instead to provide the users with a\nstepping stone towards non-hate while giving online platforms more time to\nmonitor any improvement in the user's behavior.\n</p>\n<p>To this end, we manually curated a parallel corpus - hate texts and their\nnormalized counterparts (a normalized text is less hateful and more benign). We\nintroduce NACL, a simple yet efficient hate speech normalization model that\noperates in three stages - first, it measures the hate intensity of the\noriginal sample; second, it identifies the hate span(s) within it; and finally,\nit reduces hate intensity by paraphrasing the hate spans. We perform extensive\nexperiments to measure the efficacy of NACL via three-way evaluation\n(intrinsic, extrinsic, and human-study). We observe that NACL outperforms six\nbaselines - NACL yields a score of 0.1365 RMSE for the intensity prediction,\n0.622 F1-score in the span identification, and 82.27 BLEU and 80.05 perplexity\nfor the normalized text generation. We further show the generalizability of\nNACL across other platforms (Reddit, Facebook, Gab). An interactive prototype\nof NACL was put together for the user study. Further, the tool is being\ndeployed in a real-world setting at Wipro AI as a part of its mission to tackle\nharmful content on online platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masud_S/0/1/0/all/0/1\">Sarah Masud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedi_M/0/1/0/all/0/1\">Manjot Bedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Aflah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v1 [cs.CY])","link":"http://arxiv.org/abs/2206.04039","description":"<p>In recent years, machine learning (ML) has come to rely more heavily on\ncrowdworkers, both for building bigger datasets and for addressing research\nquestions requiring human interaction or judgment. Owing to the diverse tasks\nperformed by crowdworkers, and the myriad ways the resulting datasets are used,\nit can be difficult to determine when these individuals are best thought of as\nworkers, versus as human subjects. These difficulties are compounded by\nconflicting policies, with some institutions and researchers treating all ML\ncrowdwork as human subjects research, and other institutions holding that ML\ncrowdworkers rarely constitute human subjects. Additionally, few ML papers\ninvolving crowdwork mention IRB oversight, raising the prospect that many might\nnot be in compliance with ethical and regulatory requirements. In this paper,\nwe focus on research in natural language processing to investigate the\nappropriate designation of crowdsourcing studies and the unique challenges that\nML research poses for research oversight. Crucially, under the U.S. Common\nRule, these judgments hinge on determinations of \"aboutness\", both whom (or\nwhat) the collected data is about and whom (or what) the analysis is about. We\nhighlight two challenges posed by ML: (1) the same set of workers can serve\nmultiple roles and provide many sorts of information; and (2) compared to the\nlife sciences and social sciences, ML research tends to embrace a dynamic\nworkflow, where research questions are seldom stated ex ante and data sharing\nopens the door for future studies to ask questions about different targets from\nthe original study. In particular, our analysis exposes a potential loophole in\nthe Common Rule, where researchers can elude research ethics oversight by\nsplitting data collection and analysis into distinct studies. We offer several\npolicy recommendations to address these concerns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_D/0/1/0/all/0/1\">Divyansh Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+London_A/0/1/0/all/0/1\">Alex John London</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STable: Table Generation Framework for Encoder-Decoder Models. (arXiv:2206.04045v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04045","description":"<p>The output structure of database-like tables, consisting of values structured\nin horizontal rows and vertical columns identifiable by name, can cover a wide\nrange of NLP tasks. Following this constatation, we propose a framework for\ntext-to-table neural models applicable to problems such as extraction of line\nitems, joint entity and relation extraction, or knowledge base population. The\npermutation-based decoder of our proposal is a generalized sequential method\nthat comprehends information from all cells in the table. The training\nmaximizes the expected log-likelihood for a table's content across all random\npermutations of the factorization order. During the content inference, we\nexploit the model's ability to generate cells in any order by searching over\npossible orderings to maximize the model's confidence and avoid substantial\nerror accumulation, which other sequential models are prone to. Experiments\ndemonstrate a high practical value of the framework, which establishes\nstate-of-the-art results on several challenging datasets, outperforming\nprevious solutions by up to 15%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turski_M/0/1/0/all/0/1\">Micha&#x142; Turski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1\">Tomasz Dwojak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1\">Gabriela Pa&#x142;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szyndler_K/0/1/0/all/0/1\">Karolina Szyndler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1\">Dawid Jurkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garncarek_L/0/1/0/all/0/1\">&#x141;ukasz Garncarek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Personality Prediction; an Enhanced Method Using Ensemble Modeling. (arXiv:2007.04571v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.04571","description":"<p>Human personality is significantly represented by those words which he/she\nuses in his/her speech or writing. As a consequence of spreading the\ninformation infrastructures (specifically the Internet and social media), human\ncommunications have reformed notably from face to face communication.\nGenerally, Automatic Personality Prediction (or Perception) (APP) is the\nautomated forecasting of the personality on different types of human\ngenerated/exchanged contents (like text, speech, image, video, etc.). The major\nobjective of this study is to enhance the accuracy of APP from the text. To\nthis end, we suggest five new APP methods including term frequency\nvector-based, ontology-based, enriched ontology-based, latent semantic analysis\n(LSA)-based, and deep learning-based (BiLSTM) methods. These methods as the\nbase ones, contribute to each other to enhance the APP accuracy through\nensemble modeling (stacking) based on a hierarchical attention network (HAN) as\nthe meta-model. The results show that ensemble modeling enhances the accuracy\nof APP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohammad-Ali Balafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1\">Meysam Asgari-Chenaghlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1\">Ali-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1\">Narjes Nikzad-Khasmakhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1\">Mehrdad Ranjbar-Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Nagadeh_Z/0/1/0/all/0/1\">Zoleikha Jahanbakhsh-Nagadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafarani_Moattar_E/0/1/0/all/0/1\">Elnaz Zafarani-Moattar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahkar_Farshi_T/0/1/0/all/0/1\">Taymaz Rahkar-Farshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings. (arXiv:2106.14361v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14361","description":"<p>Learning representations of words in a continuous space is perhaps the most\nfundamental task in NLP, however words interact in ways much richer than vector\ndot product similarity can provide. Many relationships between words can be\nexpressed set-theoretically, for example, adjective-noun compounds (eg. \"red\ncars\"$\\subseteq$\"cars\") and homographs (eg. \"tongue\"$\\cap$\"body\" should be\nsimilar to \"mouth\", while \"tongue\"$\\cap$\"language\" should be similar to\n\"dialect\") have natural set-theoretic interpretations. Box embeddings are a\nnovel region-based representation which provide the capability to perform these\nset-theoretic operations. In this work, we provide a fuzzy-set interpretation\nof box embeddings, and learn box representations of words using a set-theoretic\ntraining objective. We demonstrate improved performance on various word\nsimilarity tasks, particularly on less common words, and perform a quantitative\nand qualitative analysis exploring the additional unique expressivity provided\nby Word2Box.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Shib Sankar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1\">Michael Boratko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atmakuri_S/0/1/0/all/0/1\">Shriya Atmakuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Dhruvesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11193","description":"<p>Standard pretrained language models operate on sequences of subword tokens\nwithout direct access to the characters that compose each token's string\nrepresentation. We probe the embedding layer of pretrained language models and\nshow that models learn the internal character composition of whole word and\nsubword tokens to a surprising extent, without ever seeing the characters\ncoupled with the tokens. Our results show that the embedding layer of RoBERTa\nholds enough information to accurately spell up to a third of the vocabulary\nand reach high average character ngram overlap on all token types. We further\ntest whether enriching subword models with additional character information can\nimprove language modeling, and observe that this method has a near-identical\nlearning curve as training without spelling-based enrichment. Overall, our\nresults suggest that language modeling objectives incentivize the model to\nimplicitly learn some notion of spelling, and that explicitly teaching the\nmodel how to spell does not appear to enhance its performance on such tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Itzhak_I/0/1/0/all/0/1\">Itay Itzhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings. (arXiv:2109.03127v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03127","description":"<p>Recent studies have determined that the learned token embeddings of\nlarge-scale neural language models are degenerated to be anisotropic with a\nnarrow-cone shape. This phenomenon, called the representation degeneration\nproblem, facilitates an increase in the overall similarity between token\nembeddings that negatively affect the performance of the models. Although the\nexisting methods that address the degeneration problem based on observations of\nthe phenomenon triggered by the problem improves the performance of the text\ngeneration, the training dynamics of token embeddings behind the degeneration\nproblem are still not explored. In this study, we analyze the training dynamics\nof the token embeddings focusing on rare token embedding. We demonstrate that\nthe specific part of the gradient for rare token embeddings is the key cause of\nthe degeneration problem for all tokens during training stage. Based on the\nanalysis, we propose a novel method called, adaptive gradient gating (AGG). AGG\naddresses the degeneration problem by gating the specific part of the gradient\nfor rare token embeddings. Experimental results from language modeling, word\nsimilarity, and machine translation tasks quantitatively and qualitatively\nverify the effectiveness of AGG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sangwon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jongyoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-min Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_W/0/1/0/all/0/1\">Woo-Jong Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08501","description":"<p>Implicit knowledge, such as common sense, is key to fluid human\nconversations. Current neural response generation (RG) models are trained to\ngenerate responses directly, omitting unstated implicit knowledge. In this\npaper, we present Think-Before-Speaking (TBS), a generative approach to first\nexternalize implicit commonsense knowledge (think) and use this knowledge to\ngenerate responses (speak). We expect that externalizing implicit knowledge\nallows more efficient learning, produces more informative responses, and\nenables more explainable models. We analyze different choices to collect\nknowledge-aligned dialogues, represent implicit knowledge, and transition\nbetween knowledge and dialogues. Empirical results show TBS models outperform\nend-to-end and knowledge-augmented RG baselines on most automatic metrics and\ngenerate more informative, specific, and commonsense-following responses, as\nevaluated by human annotators. TBS also generates knowledge that makes sense\nand is relevant to the dialogue around 85\\% of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-language Information Retrieval. (arXiv:2111.05988v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2111.05988","description":"<p>Two key assumptions shape the usual view of ranked retrieval: (1) that the\nsearcher can choose words for their query that might appear in the documents\nthat they wish to see, and (2) that ranking retrieved documents will suffice\nbecause the searcher will be able to recognize those which they wished to find.\nWhen the documents to be searched are in a language not known by the searcher,\nneither assumption is true. In such cases, Cross-Language Information Retrieval\n(CLIR) is needed. This chapter reviews the state of the art for CLIR and\noutlines some open research questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galuscakova_P/0/1/0/all/0/1\">Petra Galu&#x161;&#x10d;&#xe1;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1\">Douglas W. Oard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Enhanced Contrastive Learning for Radiology Findings Summarization. (arXiv:2204.00203v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00203","description":"<p>The impression section of a radiology report summarizes the most prominent\nobservation from the findings section and is the most important section for\nradiologists to communicate to physicians. Summarizing findings is\ntime-consuming and can be prone to error for inexperienced radiologists, and\nthus automatic impression generation has attracted substantial attention. With\nthe encoder-decoder framework, most previous studies explore incorporating\nextra knowledge (e.g., static pre-defined clinical ontologies or extra\nbackground information). Yet, they encode such knowledge by a separate encoder\nto treat it as an extra input to their models, which is limited in leveraging\ntheir relations with the original findings. To address the limitation, we\npropose a unified framework for exploiting both extra knowledge and the\noriginal findings in an integrated way so that the critical information (i.e.,\nkey words and their relations) can be extracted in an appropriate way to\nfacilitate impression generation. In detail, for each input findings, it is\nencoded by a text encoder, and a graph is constructed through its entities and\ndependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is\nadopted to model relation information in the constructed graph. Finally, to\nemphasize the key words in the findings, contrastive learning is introduced to\nmap positive samples (constructed by masking non-key words) closer and push\napart negative ones (constructed by masking key words). The experimental\nresults on OpenI and MIMIC-CXR confirm the effectiveness of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREER: A Large-Scale Corpus for Relation Extraction and Entity Recognition. (arXiv:2204.12710v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12710","description":"<p>We describe the design and use of the CREER dataset, a large corpus annotated\nwith rich English grammar and semantic attributes. The CREER dataset uses the\nStanford CoreNLP Annotator to capture rich language structures from Wikipedia\nplain text. This dataset follows widely used linguistic and semantic\nannotations so that it can be used for not only most natural language\nprocessing tasks but also scaling the dataset. This large supervised dataset\ncan serve as the basis for improving the performance of NLP tasks in the\nfuture. We publicize the dataset through the link:\nhttps://140.116.82.111/share.cgi?ssid=000 dOJ4\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Siou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chung-Hsien Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generic and Trend-aware Curriculum Learning for Relation Extraction in Graph Neural Networks. (arXiv:2205.08625v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.08625","description":"<p>We present a generic and trend-aware curriculum learning approach for graph\nneural networks. It extends existing approaches by incorporating sample-level\nloss trends to better discriminate easier from harder samples and schedule them\nfor training. The model effectively integrates textual and structural\ninformation for relation extraction in text graphs. Experimental results show\nthat the model provides robust estimations of sample difficulty and shows\nsizable improvement over the state-of-the-art approaches across several\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vakil_N/0/1/0/all/0/1\">Nidhi Vakil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1\">Hadi Amiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Disagreement in Automatic Data Labelling for Semi-Supervised Learning in Clinical Natural Language Processing. (arXiv:2205.14761v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14761","description":"<p>Computational models providing accurate estimates of their uncertainty are\ncrucial for risk management associated with decision making in healthcare\ncontexts. This is especially true since many state-of-the-art systems are\ntrained using the data which has been labelled automatically (self-supervised\nmode) and tend to overfit. In this work, we investigate the quality of\nuncertainty estimates from a range of current state-of-the-art predictive\nmodels applied to the problem of observation detection in radiology reports.\nThis problem remains understudied for Natural Language Processing in the\nhealthcare domain. We demonstrate that Gaussian Processes (GPs) provide\nsuperior performance in quantifying the risks of 3 uncertainty labels based on\nthe negative log predictive probability (NLPP) evaluation metric and mean\nmaximum predicted confidence levels (MMPCL), whilst retaining strong predictive\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongshu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seedat_N/0/1/0/all/0/1\">Nabeel Seedat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models. (arXiv:2205.15223v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15223","description":"<p>Pre-trained masked language models successfully perform few-shot learning by\nformulating downstream tasks as text infilling. However, as a strong\nalternative in full-shot settings, discriminative pre-trained models like\nELECTRA do not fit into the paradigm. In this work, we adapt prompt-based\nfew-shot learning to ELECTRA and show that it outperforms masked language\nmodels in a wide range of tasks. ELECTRA is pre-trained to distinguish if a\ntoken is generated or original. We naturally extend that to prompt-based\nfew-shot learning by training to score the originality of the target options\nwithout introducing new parameters. Our method can be easily adapted to tasks\ninvolving multi-token predictions without extra computation overhead. Analysis\nshows that ELECTRA learns distributions that align better with downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v1 [cs.AI])","link":"http://arxiv.org/abs/2206.03491","description":"<p>Explaining is a human knowledge transfer process regarding a phenomenon\nbetween an explainer and an explainee. Each word used to explain this\nphenomenon must be carefully selected by the explainer in accordance with the\ncurrent explainee phenomenon-related knowledge level and the phenomenon itself\nin order to have a high understanding from the explainee of the phenomenon.\nNowadays, deep models, especially graph neural networks, have a major place in\ndaily life even in critical applications. In such context, those models need to\nhave a human high interpretability also referred as being explainable, in order\nto improve usage trustability of them in sensitive cases. Explaining is also a\nhuman dependent task and methods that explain deep model behavior must include\nthese social-related concerns for providing profitable and quality\nexplanations. Current explaining methods often occlude such social aspect for\nproviding their explanations and only focus on the signal aspect of the\nquestion. In this contribution we propose a reliable social-aware explaining\nmethod suited for graph neural network that includes this social feature as a\nmodular concept generator and by both leveraging signal and graph domain aspect\nthanks to an eigencentrality concept ordering approach. Besides our method\ntakes into account the human-dependent aspect underlying any explanation\nprocess, we also reach high score regarding state-of-the-art objective metrics\nassessing explanation methods for graph neural networks models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bourdon_P/0/1/0/all/0/1\">Pascal Bourdon</a> (XLIM-ASALI), <a href=\"http://arxiv.org/find/cs/1/au:+Helbert_D/0/1/0/all/0/1\">David Helbert</a> (XLIM-ASALI), <a href=\"http://arxiv.org/find/cs/1/au:+Raison_A/0/1/0/all/0/1\">Adrien Raison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03544","description":"<p>Reconstructing natural videos from fMRI brain recordings is very challenging,\nfor two main reasons: (i) As fMRI data acquisition is diffcult, we only have a\nlimited amount of supervised samples, which is not enough to cover the huge\nspace of natural videos; and (ii) The temporal resolution of fMRI recordings is\nmuch lower than the frame rate of natural videos. In this paper, we propose a\nselfsupervised approach for natural movie reconstruction. By employing cycle\nconsistency over Encoding-Decoding natural videos, we can: (i) exploit the full\nframerate of the training videos, and not be limited only to clips that\ncorrespond to fMRI recordings; (ii) exploit massive amounts of external natural\nvideos which the subjects never saw inside the fMRI machine. These enable\nincreasing the applicable training data by several orders of magnitude,\nintroducing natural video priors to the decoding network, as well as temporal\ncoherence. Our approach signifcantly outperforms competing methods, since those\ntrain only on the limited supervised data. We further introduce a new and\nsimple temporal prior of natural videos, which when folded into our fMRI\ndecoder further allows us to reconstruct videos at a higher framerate (HFR) of\nup to x8 of the original fMRI sample rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kupershmidt_G/0/1/0/all/0/1\">Ganit Kupershmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beliy_R/0/1/0/all/0/1\">Roman Beliy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1\">Guy Gaziv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contributor-Aware Defenses Against Adversarial Backdoor Attacks. (arXiv:2206.03583v1 [cs.CR])","link":"http://arxiv.org/abs/2206.03583","description":"<p>Deep neural networks for image classification are well-known to be vulnerable\nto adversarial attacks. One such attack that has garnered recent attention is\nthe adversarial backdoor attack, which has demonstrated the capability to\nperform targeted misclassification of specific examples. In particular,\nbackdoor attacks attempt to force a model to learn spurious relations between\nbackdoor trigger patterns and false labels. In response to this threat,\nnumerous defensive measures have been proposed; however, defenses against\nbackdoor attacks focus on backdoor pattern detection, which may be unreliable\nagainst novel or unexpected types of backdoor pattern designs. We introduce a\nnovel re-contextualization of the adversarial setting, where the presence of an\nadversary implicitly admits the existence of multiple database contributors.\nThen, under the mild assumption of contributor awareness, it becomes possible\nto exploit this knowledge to defend against backdoor attacks by destroying the\nfalse label associations. We propose a contributor-aware universal defensive\nframework for learning in the presence of multiple, potentially adversarial\ndata sources that utilizes semi-supervised ensembles and learning from crowds\nto filter the false labels produced by adversarial triggers. Importantly, this\ndefensive strategy is agnostic to backdoor pattern design, as it functions\nwithout needing -- or even attempting -- to perform either adversary\nidentification or backdoor pattern detection during either training or\ninference. Our empirical studies demonstrate the robustness of the proposed\nframework against adversarial backdoor attacks from multiple simultaneous\nadversaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dawson_G/0/1/0/all/0/1\">Glenn Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umer_M/0/1/0/all/0/1\">Muhammad Umer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polikar_R/0/1/0/all/0/1\">Robi Polikar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White-box Membership Attack Against Machine Learning Based Retinopathy Classification. (arXiv:2206.03584v1 [cs.CR])","link":"http://arxiv.org/abs/2206.03584","description":"<p>The advances in machine learning (ML) have greatly improved AI-based\ndiagnosis aid systems in medical imaging. However, being based on collecting\nmedical data specific to individuals induces several security issues,\nespecially in terms of privacy. Even though the owner of the images like a\nhospital put in place strict privacy protection provisions at the level of its\ninformation system, the model trained over his images still holds disclosure\npotential. The trained model may be accessible to an attacker as: 1) White-box:\naccessing to the model architecture and parameters; 2) Black box: where he can\nonly query the model with his own inputs through an appropriate interface.\nExisting attack methods include: feature estimation attacks (FEA), membership\ninference attack (MIA), model memorization attack (MMA) and identification\nattacks (IA). In this work we focus on MIA against a model that has been\ntrained to detect diabetic retinopathy from retinal images. Diabetic\nretinopathy is a condition that can cause vision loss and blindness in the\npeople who have diabetes. MIA is the process of determining whether a data\nsample comes from the training data set of a trained ML model or not. From a\nprivacy perspective in our use case where a diabetic retinopathy classification\nmodel is given to partners that have at their disposal images along with\npatients' identifiers, inferring the membership status of a data sample can\nhelp to state if a patient has contributed or not to the training of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_M/0/1/0/all/0/1\">Mounia Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellafqira_R/0/1/0/all/0/1\">Reda Bellafqira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quellec_G/0/1/0/all/0/1\">Gwenol&#xe9; Quellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coatrieux_G/0/1/0/all/0/1\">Gouenou Coatrieux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ObPose: Leveraging Canonical Pose for Object-Centric Scene Inference in 3D. (arXiv:2206.03591v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03591","description":"<p>We present ObPose, an unsupervised object-centric generative model that\nlearns to segment 3D objects from RGB-D video in an unsupervised manner.\nInspired by prior art in 2D representation learning, ObPose considers a\nfactorised latent space, separately encoding object-wise location (where) and\nappearance (what) information. In particular, ObPose leverages an object's\ncanonical pose, defined via a minimum volume principle, as a novel inductive\nbias for learning the where component. To achieve this, we propose an\nefficient, voxelised approximation approach to recover the object shape\ndirectly from a neural radiance field (NeRF). As a consequence, ObPose models\nscenes as compositions of NeRFs representing individual objects. When evaluated\non the YCB dataset for unsupervised scene segmentation, ObPose outperforms the\ncurrent state-of-the-art in 3D scene inference (ObSuRF) by a significant margin\nin terms of segmentation quality for both video inputs as well as for\nmulti-view static scenes. In addition, the design choices made in the ObPose\nencoder are validated with relevant ablations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yizhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_O/0/1/0/all/0/1\">Oiwi Parker Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1\">Ingmar Posner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Compression via Effective Filter Analysis and Hierarchical Pruning. (arXiv:2206.03596v1 [cs.LG])","link":"http://arxiv.org/abs/2206.03596","description":"<p>Network compression is crucial to making the deep networks to be more\nefficient, faster, and generalizable to low-end hardware. Current network\ncompression methods have two open problems: first, there lacks a theoretical\nframework to estimate the maximum compression rate; second, some layers may get\nover-prunned, resulting in significant network performance drop. To solve these\ntwo problems, this study propose a gradient-matrix singularity analysis-based\nmethod to estimate the maximum network redundancy. Guided by that maximum rate,\na novel and efficient hierarchical network pruning algorithm is developed to\nmaximally condense the neuronal network structure without sacrificing network\nperformance. Substantial experiments are performed to demonstrate the efficacy\nof the new method for pruning several advanced convolutional neural network\n(CNN) architectures. Compared to existing pruning methods, the proposed pruning\nalgorithm achieved state-of-the-art performance. At the same or similar\ncompression ratio, the new method provided the highest network prediction\naccuracy as compared to other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Li Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ze Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Ring to Bring Them All: Towards Open-Set Recognition under Domain Shift. (arXiv:2206.03600v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03600","description":"<p>In this paper, we investigate $\\textit{open-set recognition}$ with domain\nshift, where the final goal is to achieve $\\textit{Source-free Universal Domain\nAdaptation}$ (SF-UNDA), which addresses the situation where there exist both\ndomain and category shifts between source and target domains. Under the SF-UNDA\nsetting, the model cannot access source data anymore during target adaptation,\nwhich aims to address data privacy concerns. We propose a novel training scheme\nto learn a ($n$+1)-way classifier to predict the $n$ source classes and the\nunknown class, where samples of only known source categories are available for\ntraining. Furthermore, for target adaptation, we simply adopt a weighted\nentropy minimization to adapt the source pretrained model to the unlabeled\ntarget domain without source data. In experiments, we show: $\\textbf{1)}$ After\nsource training, the resulting source model can get excellent performance for\n$\\textit{open-set single domain generalization}$ and also $\\textit{open-set\nrecognition}$ tasks; $\\textbf{2)}$ After target adaptation, our method\nsurpasses current UNDA approaches which demand source data during adaptation on\nseveral benchmarks. The versatility to several different tasks strongly proves\nthe efficacy and generalization ability of our method. $\\textbf{3)}$ When\naugmented with a closed-set domain adaptation approach during target\nadaptation, our source-free method further outperforms the current\nstate-of-the-art UNDA method by 2.5%, 7.2% and 13% on Office-31, Office-Home\nand VisDA respectively. Code will be available in\nhttps://github.com/Albert0147/OneRing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new method incorporating deep learning with shape priors for left ventricular segmentation in myocardial perfusion SPECT images. (arXiv:2206.03603v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03603","description":"<p>Background: The assessment of left ventricular (LV) function by myocardial\nperfusion SPECT (MPS) relies on accurate myocardial segmentation. The purpose\nof this paper is to develop and validate a new method incorporating deep\nlearning with shape priors to accurately extract the LV myocardium for\nautomatic measurement of LV functional parameters. Methods: A segmentation\narchitecture that integrates a three-dimensional (3D) V-Net with a shape\ndeformation module was developed. Using the shape priors generated by a dynamic\nprogramming (DP) algorithm, the model output was then constrained and guided\nduring the model training for quick convergence and improved performance. A\nstratified 5-fold cross-validation was used to train and validate our models.\nResults: Results of our proposed method agree well with those from the ground\ntruth. Our proposed model achieved a Dice similarity coefficient (DSC) of\n0.9573(0.0244), 0.9821(0.0137), and 0.9903(0.0041), a Hausdorff distances (HD)\nof 6.7529(2.7334) mm, 7.2507(3.1952) mm, and 7.6121(3.0134) mm in extracting\nthe endocardium, myocardium, and epicardium, respectively. Conclusion: Our\nproposed method achieved a high accuracy in extracting LV myocardial contours\nand assessing LV function.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_F/0/1/0/all/0/1\">Fubao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1\">Jinyu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1\">Shaojie Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nan_J/0/1/0/all/0/1\">Jiaofen Nan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yanting Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongqiang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jianzhou Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zenghong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhixin Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weihua Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predictive Modeling of Charge Levels for Battery Electric Vehicles using CNN EfficientNet and IGTD Algorithm. (arXiv:2206.03612v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03612","description":"<p>Convolutional Neural Networks (CNN) have been a good solution for\nunderstanding a vast image dataset. As the increased number of battery-equipped\nelectric vehicles is flourishing globally, there has been much research on\nunderstanding which charge levels electric vehicle drivers would choose to\ncharge their vehicles to get to their destination without any prevention. We\nimplemented deep learning approaches to analyze the tabular datasets to\nunderstand their state of charge and which charge levels they would choose. In\naddition, we implemented the Image Generator for Tabular Dataset algorithm to\nutilize tabular datasets as image datasets to train convolutional neural\nnetworks. Also, we integrated other CNN architecture such as EfficientNet to\nprove that CNN is a great learner for reading information from images that were\nconverted from the tabular dataset, and able to predict charge levels for\nbattery-equipped electric vehicles. We also evaluated several optimization\nmethods to enhance the learning rate of the models and examined further\nanalysis on improving the model architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chongzhou Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_D/0/1/0/all/0/1\">David Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving into the Pre-training Paradigm of Monocular 3D Object Detection. (arXiv:2206.03657v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03657","description":"<p>The labels of monocular 3D object detection (M3OD) are expensive to obtain.\nMeanwhile, there usually exists numerous unlabeled data in practical\napplications, and pre-training is an efficient way of exploiting the knowledge\nin unlabeled data. However, the pre-training paradigm for M3OD is hardly\nstudied. We aim to bridge this gap in this work. To this end, we first draw two\nobservations: (1) The guideline of devising pre-training tasks is imitating the\nrepresentation of the target task. (2) Combining depth estimation and 2D object\ndetection is a promising M3OD pre-training baseline. Afterwards, following the\nguideline, we propose several strategies to further improve this baseline,\nwhich mainly include target guided semi-dense depth estimation, keypoint-aware\n2D object detection, and class-level loss adjustment. Combining all the\ndeveloped techniques, the obtained pre-training framework produces pre-trained\nbackbones that improve M3OD performance significantly on both the KITTI-3D and\nnuScenes benchmarks. For example, by applying a DLA34 backbone to a naive\ncenter-based M3OD detector, the moderate ${\\rm AP}_{3D}70$ score of Car on the\nKITTI-3D testing set is boosted by 18.71\\% and the NDS score on the nuScenes\nvalidation set is improved by 40.41\\% relatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuanrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1\">En Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Hyper-Initializer for All Network Architectures in Medical Image Analysis. (arXiv:2206.03661v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03661","description":"<p>Pre-training is essential to deep learning model performance, especially in\nmedical image analysis tasks where limited training data are available.\nHowever, existing pre-training methods are inflexible as the pre-trained\nweights of one model cannot be reused by other network architectures. In this\npaper, we propose an architecture-irrelevant hyper-initializer, which can\ninitialize any given network architecture well after being pre-trained for only\nonce. The proposed initializer is a hypernetwork which takes a downstream\narchitecture as input graphs and outputs the initialization parameters of the\nrespective architecture. We show the effectiveness and efficiency of the\nhyper-initializer through extensive experimental results on multiple medical\nimaging modalities, especially in data-limited fields. Moreover, we prove that\nthe proposed algorithm can be reused as a favorable plug-and-play initializer\nfor any downstream architecture and task (both classification and segmentation)\nof the same modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fangxin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking. (arXiv:2206.03666v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03666","description":"<p>Monocular image-based 3D perception has become an active research area in\nrecent years owing to its applications in autonomous driving. Approaches to\nmonocular 3D perception including detection and tracking, however, often yield\ninferior performance when compared to LiDAR-based techniques. Through\nsystematic analysis, we identified that per-object depth estimation accuracy is\na major factor bounding the performance. Motivated by this observation, we\npropose a multi-level fusion method that combines different representations\n(RGB and pseudo-LiDAR) and temporal information across multiple frames for\nobjects (tracklets) to enhance per-object depth estimation. Our proposed fusion\nmethod achieves the state-of-the-art performance of per-object depth estimation\non the Waymo Open Dataset, the KITTI detection dataset, and the KITTI MOT\ndataset. We further demonstrate that by simply replacing estimated depth with\nfusion-enhanced depth, we can achieve significant improvements in monocular 3D\nperception tasks, including detection and tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Ruichi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kretzschmar_H/0/1/0/all/0/1\">Henrik Kretzschmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Charles R. Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayvaci_A/0/1/0/all/0/1\">Alper Ayvaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cower_D/0/1/0/all/0/1\">Dillon Cower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yurong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Han Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Congcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVIDx CXR-3: A Large-Scale, Open-Source Benchmark Dataset of Chest X-ray Images for Computer-Aided COVID-19 Diagnostics. (arXiv:2206.03671v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03671","description":"<p>After more than two years since the beginning of the COVID-19 pandemic, the\npressure of this crisis continues to devastate globally. The use of chest X-ray\n(CXR) imaging as a complementary screening strategy to RT-PCR testing is not\nonly prevailing but has greatly increased due to its routine clinical use for\nrespiratory complaints. Thus far, many visual perception models have been\nproposed for COVID-19 screening based on CXR imaging. Nevertheless, the\naccuracy and the generalization capacity of these models are very much\ndependent on the diversity and the size of the dataset they were trained on.\nMotivated by this, we introduce COVIDx CXR-3, a large-scale benchmark dataset\nof CXR images for supporting COVID-19 computer vision research. COVIDx CXR-3 is\ncomposed of 30,386 CXR images from a multinational cohort of 17,026 patients\nfrom at least 51 countries, making it, to the best of our knowledge, the most\nextensive, most diverse COVID-19 CXR dataset in open access form. Here, we\nprovide comprehensive details on the various aspects of the proposed dataset\nincluding patient demographics, imaging views, and infection types. The hope is\nthat COVIDx CXR-3 can assist scientists in advancing computer vision research\nagainst the COVID-19 pandemic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tuinstra_T/0/1/0/all/0/1\">Tia Tuinstra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aboutalebi_H/0/1/0/all/0/1\">Hossein Aboutalebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1\">Andy Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of 3D Scene Flow from Monocular Camera. (arXiv:2206.03673v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03673","description":"<p>Scene flow represents the motion of points in the 3D space, which is the\ncounterpart of the optical flow that represents the motion of pixels in the 2D\nimage. However, it is difficult to obtain the ground truth of scene flow in the\nreal scenes, and recent studies are based on synthetic data for training.\nTherefore, how to train a scene flow network with unsupervised methods based on\nreal-world data shows crucial significance. A novel unsupervised learning\nmethod for scene flow is proposed in this paper, which utilizes the images of\ntwo consecutive frames taken by monocular camera without the ground truth of\nscene flow for training. Our method realizes the goal that training scene flow\nnetwork with real-world data, which bridges the gap between training data and\ntest data and broadens the scope of available data for training. Unsupervised\nlearning of scene flow in this paper mainly consists of two parts: (i) depth\nestimation and camera pose estimation, and (ii) scene flow estimation based on\nfour different loss functions. Depth estimation and camera pose estimation\nobtain the depth maps and camera pose between two consecutive frames, which\nprovide further information for the next scene flow estimation. After that, we\nused depth consistency loss, dynamic-static consistency loss, Chamfer loss, and\nLaplacian regularization loss to carry out unsupervised training of the scene\nflow network. To our knowledge, this is the first paper that realizes the\nunsupervised learning of 3D scene flow from monocular camera. The experiment\nresults on KITTI show that our method for unsupervised learning of scene flow\nmeets great performance compared to traditional methods Iterative Closest Point\n(ICP) and Fast Global Registration (FGR). The source code is available at:\nhttps://github.com/IRMVLab/3DUnMonoFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xiaoyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruiqi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UHD Image Deblurring via Multi-scale Cubic-Mixer. (arXiv:2206.03678v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03678","description":"<p>Currently, transformer-based algorithms are making a splash in the domain of\nimage deblurring. Their achievement depends on the self-attention mechanism\nwith CNN stem to model long range dependencies between tokens. Unfortunately,\nthis ear-pleasing pipeline introduces high computational complexity and makes\nit difficult to run an ultra-high-definition image on a single GPU in real\ntime. To trade-off accuracy and efficiency, the input degraded image is\ncomputed cyclically over three dimensional ($C$, $W$, and $H$) signals without\na self-attention mechanism. We term this deep network as Multi-scale\nCubic-Mixer, which is acted on both the real and imaginary components after\nfast Fourier transform to estimate the Fourier coefficients and thus obtain a\ndeblurred image. Furthermore, we combine the multi-scale cubic-mixer with a\nslicing strategy to generate high-quality results at a much lower computational\ncost. Experimental results demonstrate that the proposed algorithm performs\nfavorably against the state-of-the-art deblurring approaches on the several\nbenchmarks and a new ultra-high-definition dataset in terms of accuracy and\nspeed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiuyi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DebiasBench: Benchmark for Fair Comparison of Debiasing in Image Classification. (arXiv:2206.03680v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03680","description":"<p>Image classifiers often rely overly on peripheral attributes that have a\nstrong correlation with the target class (i.e., dataset bias) when making\npredictions. Recently, a myriad of studies focus on mitigating such dataset\nbias, the task of which is referred to as debiasing. However, these debiasing\nmethods often have inconsistent experimental settings (e.g., datasets and\nneural network architectures). Additionally, most of the previous studies in\ndebiasing do not specify how they select their model parameters which involve\nearly stopping and hyper-parameter tuning. The goal of this paper is to\nstandardize the inconsistent experimental settings and propose a consistent\nmodel parameter selection criterion for debiasing. Based on such unified\nexperimental settings and model parameter selection criterion, we build a\nbenchmark named DebiasBench which includes five datasets and seven debiasing\nmethods. We carefully conduct extensive experiments in various aspects and show\nthat different state-of-the-art methods work best in different datasets,\nrespectively. Even, the vanilla method, the method with no debiasing module,\nalso shows competitive results in datasets with low bias severity. We publicly\nrelease the implementation of existing debiasing methods in DebiasBench to\nencourage future researchers in debiasing to conduct fair comparisons and\nfurther push the state-of-the-art performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Model for Multi-class Anomaly Detection. (arXiv:2206.03687v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03687","description":"<p>Despite the rapid advance of unsupervised anomaly detection, existing methods\nrequire to train separate models for different objects. In this work, we\npresent UniAD that accomplishes anomaly detection for multiple classes with a\nunified framework. Under such a challenging setting, popular reconstruction\nnetworks may fall into an \"identical shortcut\", where both normal and anomalous\nsamples can be well recovered, and hence fail to spot outliers. To tackle this\nobstacle, we make three improvements. First, we revisit the formulations of\nfully-connected layer, convolutional layer, as well as attention layer, and\nconfirm the important role of query embedding (i.e., within attention layer) in\npreventing the network from learning the shortcut. We therefore come up with a\nlayer-wise query decoder to help model the multi-class distribution. Second, we\nemploy a neighbor masked attention module to further avoid the information leak\nfrom the input feature to the reconstructed output feature. Third, we propose a\nfeature jittering strategy that urges the model to recover the correct message\neven with noisy inputs. We evaluate our algorithm on MVTec-AD and CIFAR-10\ndatasets, where we surpass the state-of-the-art alternatives by a sufficiently\nlarge margin. For example, when learning a unified model for 15 categories in\nMVTec-AD, we surpass the second competitor on the tasks of both anomaly\ndetection (from 88.1% to 96.5%) and anomaly localization (from 89.5% to 96.8%).\nCode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhiyuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xinyi Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Deep Ensemble Method for Real-world Image Denoising. (arXiv:2206.03691v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03691","description":"<p>Recently, deep learning-based image denoising methods have achieved promising\nperformance on test data with the same distribution as training set, where\nvarious denoising models based on synthetic or collected real-world training\ndata have been learned. However, when handling real-world noisy images, the\ndenoising performance is still limited. In this paper, we propose a simple yet\neffective Bayesian deep ensemble (BDE) method for real-world image denoising,\nwhere several representative deep denoisers pre-trained with various training\ndata settings can be fused to improve robustness. The foundation of BDE is that\nreal-world image noises are highly signal-dependent, and heterogeneous noises\nin a real-world noisy image can be separately handled by different denoisers.\nIn particular, we take well-trained CBDNet, NBNet, HINet, Uformer and GMSNet\ninto denoiser pool, and a U-Net is adopted to predict pixel-wise weighting maps\nto fuse these denoisers. Instead of solely learning pixel-wise weighting maps,\nBayesian deep learning strategy is introduced to predict weighting uncertainty\nas well as weighting map, by which prediction variance can be modeled for\nimproving robustness on real-world noisy images. Extensive experiments have\nshown that real-world noises can be better removed by fusing existing denoisers\ninstead of training a big denoiser with expensive cost. On DND dataset, our BDE\nachieves +0.28~dB PSNR gain over the state-of-the-art denoising method.\nMoreover, we note that our BDE denoiser based on different Gaussian noise\nlevels outperforms state-of-the-art CBDNet when applying to real-world noisy\nimages. Furthermore, our BDE can be extended to other image restoration tasks,\nand achieves +0.30dB, +0.18dB and +0.12dB PSNR gains on benchmark datasets for\nimage deblurring, image deraining and single image super-resolution,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Face Restoration: Benchmark Datasets and a Baseline Model. (arXiv:2206.03697v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03697","description":"<p>Blind Face Restoration (BFR) aims to construct a high-quality (HQ) face image\nfrom its corresponding low-quality (LQ) input. Recently, many BFR methods have\nbeen proposed and they have achieved remarkable success. However, these methods\nare trained or evaluated on privately synthesized datasets, which makes it\ninfeasible for the subsequent approaches to fairly compare with them. To\naddress this problem, we first synthesize two blind face restoration benchmark\ndatasets called EDFace-Celeb-1M (BFR128) and EDFace-Celeb-150K (BFR512).\nState-of-the-art methods are benchmarked on them under five settings including\nblur, noise, low resolution, JPEG compression artifacts, and the combination of\nthem (full degradation). To make the comparison more comprehensive, five\nwidely-used quantitative metrics and two task-driven metrics including Average\nFace Landmark Distance (AFLD) and Average Face ID Cosine Similarity (AFICS) are\napplied. Furthermore, we develop an effective baseline model called Swin\nTransformer U-Net (STUNet). The STUNet with U-net architecture applies an\nattention mechanism and a shifted windowing scheme to capture long-range pixel\ninteractions and focus more on significant features while still being trained\nefficiently. Experimental results show that the proposed baseline method\nperforms favourably against the SOTA methods on various BFR tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Puyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoren Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do we learn? Debunking the Myth of Unsupervised Outlier Detection. (arXiv:2206.03698v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03698","description":"<p>Even though auto-encoders (AEs) have the desirable property of learning\ncompact representations without labels and have been widely applied to\nout-of-distribution (OoD) detection, they are generally still poorly understood\nand are used incorrectly in detecting outliers where the normal and abnormal\ndistributions are strongly overlapping. In general, the learned manifold is\nassumed to contain key information that is only important for describing\nsamples within the training distribution, and that the reconstruction of\noutliers leads to high residual errors. However, recent work suggests that AEs\nare likely to be even better at reconstructing some types of OoD samples. In\nthis work, we challenge this assumption and investigate what auto-encoders\nactually learn when they are posed to solve two different tasks. First, we\npropose two metrics based on the Fr\\'echet inception distance (FID) and\nconfidence scores of a trained classifier to assess whether AEs can learn the\ntraining distribution and reliably recognize samples from other domains.\nSecond, we investigate whether AEs are able to synthesize normal images from\nsamples with abnormal regions, on a more challenging lung pathology detection\ntask. We have found that state-of-the-art (SOTA) AEs are either unable to\nconstrain the latent manifold and allow reconstruction of abnormal patterns, or\nthey are failing to accurately restore the inputs from their latent\ndistribution, resulting in blurred or misaligned reconstructions. We propose\nnovel deformable auto-encoders (MorphAEus) to learn perceptually aware global\nimage priors and locally adapt their morphometry based on estimated dense\ndeformation fields. We demonstrate superior performance over unsupervised\nmethods in detecting OoD and pathology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bercea_C/0/1/0/all/0/1\">Cosmin I. Bercea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypernetwork-based Personalized Federated Learning for Multi-Institutional CT Imaging. (arXiv:2206.03709v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03709","description":"<p>Computed tomography (CT) is of great importance in clinical practice due to\nits powerful ability to provide patients' anatomical information without any\ninvasive inspection, but its potential radiation risk is raising people's\nconcerns. Deep learning-based methods are considered promising in CT\nreconstruction, but these network models are usually trained with the measured\ndata obtained from specific scanning protocol and need to centralizedly collect\nlarge amounts of data, which will lead to serious data domain shift, and\nprivacy concerns. To relieve these problems, in this paper, we propose a\nhypernetwork-based federated learning method for personalized CT imaging,\ndubbed as HyperFed. The basic assumption of HyperFed is that the optimization\nproblem for each institution can be divided into two parts: the local data\nadaption problem and the global CT imaging problem, which are implemented by an\ninstitution-specific hypernetwork and a global-sharing imaging network,\nrespectively. The purpose of global-sharing imaging network is to learn stable\nand effective common features from different institutions. The\ninstitution-specific hypernetwork is carefully designed to obtain\nhyperparameters to condition the global-sharing imaging network for\npersonalized local CT reconstruction. Experiments show that HyperFed achieves\ncompetitive performance in CT reconstruction compared with several other\nstate-of-the-art methods. It is believed as a promising direction to improve CT\nimaging quality and achieve personalized demands of different institutions or\nscanners without privacy data sharing. The codes will be released at\nhttps://github.com/Zi-YuanYang/HyperFed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyuan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1\">Wenjun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zexin Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet Regularization Benefits Adversarial Training. (arXiv:2206.03727v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03727","description":"<p>Adversarial training methods are state-of-the-art (SOTA) empirical defense\nmethods against adversarial examples. Many regularization methods have been\nproven to be effective with the combination of adversarial training.\nNevertheless, such regularization methods are implemented in the time domain.\nSince adversarial vulnerability can be regarded as a high-frequency phenomenon,\nit is essential to regulate the adversarially-trained neural network models in\nthe frequency domain. Faced with these challenges, we make a theoretical\nanalysis on the regularization property of wavelets which can enhance\nadversarial training. We propose a wavelet regularization method based on the\nHaar wavelet decomposition which is named Wavelet Average Pooling. This wavelet\nregularization module is integrated into the wide residual neural network so\nthat a new WideWaveletResNet model is formed. On the datasets of CIFAR-10 and\nCIFAR-100, our proposed Adversarial Wavelet Training method realizes\nconsiderable robustness under different types of attacks. It verifies the\nassumption that our wavelet regularization method can enhance adversarial\nrobustness especially in the deep wide neural networks. The visualization\nexperiments of the Frequency Principle (F-Principle) and interpretability are\nimplemented to show the effectiveness of our method. A detailed comparison\nbased on different wavelet base functions is presented. The code is available\nat the repository:\n\\url{https://github.com/momo1986/AdversarialWaveletTraining}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Ontology Embedding for Zero-shot Learning. (arXiv:2206.03739v1 [cs.AI])","link":"http://arxiv.org/abs/2206.03739","description":"<p>Knowledge Graph (KG) and its variant of ontology have been widely used for\nknowledge representation, and have shown to be quite effective in augmenting\nZero-shot Learning (ZSL). However, existing ZSL methods that utilize KGs all\nneglect the intrinsic complexity of inter-class relationships represented in\nKGs. One typical feature is that a class is often related to other classes in\ndifferent semantic aspects. In this paper, we focus on ontologies for\naugmenting ZSL, and propose to learn disentangled ontology embeddings guided by\nontology properties to capture and utilize more fine-grained class\nrelationships in different aspects. We also contribute a new ZSL framework\nnamed DOZSL, which contains two new ZSL solutions based on generative models\nand graph propagation models, respectively, for effectively utilizing the\ndisentangled ontology embeddings. Extensive evaluations have been conducted on\nfive benchmarks across zero-shot image classification (ZS-IMGC) and zero-shot\nKG completion (ZS-KGC). DOZSL often achieves better performance than the\nstate-of-the-art, and its components have been verified by ablation studies and\ncase studies. Our codes and datasets are available at\nhttps://github.com/zjukg/DOZSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yuxia Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yajing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Loss Matters in Weakly Supervised Multi-Label Classification. (arXiv:2206.03740v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03740","description":"<p>Weakly supervised multi-label classification (WSML) task, which is to learn a\nmulti-label classification using partially observed labels per image, is\nbecoming increasingly important due to its huge annotation cost. In this work,\nwe first regard unobserved labels as negative labels, casting the WSML task\ninto noisy multi-label classification. From this point of view, we empirically\nobserve that memorization effect, which was first discovered in a noisy\nmulti-class setting, also occurs in a multi-label setting. That is, the model\nfirst learns the representation of clean labels, and then starts memorizing\nnoisy labels. Based on this finding, we propose novel methods for WSML which\nreject or correct the large loss samples to prevent model from memorizing the\nnoisy label. Without heavy and complex components, our proposed methods\noutperform previous state-of-the-art WSML methods on several partial label\nsettings including Pascal VOC 2012, MS COCO, NUSWIDE, CUB, and OpenImages V3\ndatasets. Various analysis also show that our methodology actually works well,\nvalidating that treating large loss properly matters in a weakly supervised\nmulti-label classification. Our code is available at\nhttps://github.com/snucml/LargeLossMatters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungwoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Task Agnostic Temporal Consistency Correction. (arXiv:2206.03753v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03753","description":"<p>Due to the scarcity of video processing methodologies, image processing\noperations are naively extended to the video domain by processing each frame\nindependently. This disregard for the temporal connection in video processing\noften leads to severe temporal inconsistencies. State-of-the-art techniques\nthat address these inconsistencies rely on the availability of unprocessed\nvideos to siphon consistent video dynamics to restore the temporal consistency\nof frame-wise processed videos. We propose a novel general framework for this\ntask that learns to infer consistent motion dynamics from inconsistent videos\nto mitigate the temporal flicker while preserving the perceptual quality for\nboth the temporally neighboring and relatively distant frames. The proposed\nframework produces state-of-the-art results on two large-scale datasets, DAVIS\nand videvo.net, processed by numerous image processing tasks in a feed-forward\nmanner. The code and the trained models will be released upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Muhammad Kashif Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae Hyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PixSelect: Less but Reliable Pixels for Accurate and Efficient Localization. (arXiv:2206.03775v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03775","description":"<p>Accurate camera pose estimation is a fundamental requirement for numerous\napplications, such as autonomous driving, mobile robotics, and augmented\nreality. In this work, we address the problem of estimating the global 6 DoF\ncamera pose from a single RGB image in a given environment. Previous works\nconsider every part of the image valuable for localization. However, many image\nregions such as the sky, occlusions, and repetitive non-distinguishable\npatterns cannot be utilized for localization. In addition to adding unnecessary\ncomputation efforts, extracting and matching features from such regions produce\nmany wrong matches which in turn degrades the localization accuracy and\nefficiency. Our work addresses this particular issue and shows by exploiting an\ninteresting concept of sparse 3D models that we can exploit discriminatory\nenvironment parts and avoid useless image regions for the sake of a single\nimage localization. Interestingly, through avoiding selecting keypoints from\nnon-reliable image regions such as trees, bushes, cars, pedestrians, and\nocclusions, our work acts naturally as an outlier filter. This makes our system\nhighly efficient in that minimal set of correspondences is needed and highly\naccurate as the number of outliers is low. Our work exceeds state-ofthe-art\nmethods on outdoor Cambridge Landmarks dataset. With only relying on single\nimage at inference, it outweighs in terms of accuracy methods that exploit pose\npriors and/or reference 3D models while being much faster. By choosing as\nlittle as 100 correspondences, it surpasses similar methods that localize from\nthousands of correspondences, while being more efficient. In particular, it\nachieves, compared to these methods, an improvement of localization by 33% on\nOldHospital scene. Furthermore, It outstands direct pose regressors even those\nthat learn from sequence of images\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altillawi_M/0/1/0/all/0/1\">Mohammad Altillawi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Digital Terrain Models from Point Clouds: ALS2DTM Dataset and Rasterization-based GAN. (arXiv:2206.03778v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03778","description":"<p>Despite the popularity of deep neural networks in various domains, the\nextraction of digital terrain models (DTMs) from airborne laser scanning (ALS)\npoint clouds is still challenging. This might be due to the lack of dedicated\nlarge-scale annotated dataset and the data-structure discrepancy between point\nclouds and DTMs. To promote data-driven DTM extraction, this paper collects\nfrom open sources a large-scale dataset of ALS point clouds and corresponding\nDTMs with various urban, forested, and mountainous scenes. A baseline method is\nproposed as the first attempt to train a Deep neural network to extract digital\nTerrain models directly from ALS point clouds via Rasterization techniques,\ncoined DeepTerRa. Extensive studies with well-established methods are performed\nto benchmark the dataset and analyze the challenges in learning to extract DTM\nfrom point clouds. The experimental results show the interest of the agnostic\ndata-driven approach, with sub-metric error level compared to methods designed\nfor DTM extraction. The data and source code is provided at\nhttps://lhoangan.github.io/deepterra/ for reproducibility and further similar\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Ho&#xe0;ng-&#xc2;n L&#xea;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guiotte_F/0/1/0/all/0/1\">Florent Guiotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh-Tan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1\">S&#xe9;bastien Lef&#xe8;vre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corpetti_T/0/1/0/all/0/1\">Thomas Corpetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation. (arXiv:2206.03789v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03789","description":"<p>Referring video object segmentation aims to predict foreground labels for\nobjects referred by natural language expressions in videos. Previous methods\neither depend on 3D ConvNets or incorporate additional 2D ConvNets as encoders\nto extract mixed spatial-temporal features. However, these methods suffer from\nspatial misalignment or false distractors due to delayed and implicit\nspatial-temporal interaction occurring in the decoding phase. To tackle these\nlimitations, we propose a Language-Bridged Duplex Transfer (LBDT) module which\nutilizes language as an intermediary bridge to accomplish explicit and adaptive\nspatial-temporal interaction earlier in the encoding phase. Concretely,\ncross-modal attention is performed among the temporal encoder, referring words\nand the spatial encoder to aggregate and transfer language-relevant motion and\nappearance information. In addition, we also propose a Bilateral Channel\nActivation (BCA) module in the decoding phase for further denoising and\nhighlighting the spatial-temporal consistent features via channel-wise\nactivation. Extensive experiments show our method achieves new state-of-the-art\nperformances on four popular benchmarks with 6.8% and 6.9% absolute AP gains on\nA2D Sentences and J-HMDB Sentences respectively, while consuming around 7x less\ncomputational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zihan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_T/0/1/0/all/0/1\">Tianrui Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junshi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoming Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps. (arXiv:2206.03799v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03799","description":"<p>Self-supervised monocular depth estimation has been a subject of intense\nstudy in recent years, because of its applications in robotics and autonomous\ndriving. Much of the recent work focuses on improving depth estimation by\nincreasing architecture complexity. This paper shows that state-of-the-art\nperformance can also be achieved by improving the learning process rather than\nincreasing model complexity. More specifically, we propose (i) only using\ninvariant pose loss for the first few epochs during training, (ii) disregarding\nsmall potentially dynamic objects when training, and (iii) employing an\nappearance-based approach to separately estimate object pose for truly dynamic\nobjects. We demonstrate that these simplifications reduce GPU memory usage by\n29% and result in qualitatively and quantitatively improved depth maps\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_K/0/1/0/all/0/1\">Kieran Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogiatzis_G/0/1/0/all/0/1\">George Vogiatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manso_L/0/1/0/all/0/1\">Luis J. Manso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Windows Are Significant: Learning from Mediastinal Window and Focusing on Lung Window. (arXiv:2206.03803v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03803","description":"<p>Since the pandemic of COVID-19, several deep learning methods were proposed\nto analyze the chest Computed Tomography (CT) for diagnosis. In the current\nsituation, the disease course classification is significant for medical\npersonnel to decide the treatment. Most previous deep-learning-based methods\nextract features observed from the lung window. However, it has been proved\nthat some appearances related to diagnosis can be observed better from the\nmediastinal window rather than the lung window, e.g., the pulmonary\nconsolidation happens more in severe symptoms. In this paper, we propose a\nnovel Dual Window RCNN Network (DWRNet), which mainly learns the distinctive\nfeatures from the successive mediastinal window. Regarding the features\nextracted from the lung window, we introduce the Lung Window Attention Block\n(LWA Block) to pay additional attention to them for enhancing the\nmediastinal-window features. Moreover, instead of picking up specific slices\nfrom the whole CT slices, we use a Recurrent CNN and analyze successive slices\nas videos. Experimental results show that the fused and representative features\nimprove the predictions of disease course by reaching the accuracy of 90.57%,\nagainst the baseline with an accuracy of 84.86%. Ablation studies demonstrate\nthat combined dual window features are more efficient than lung-window features\nalone, while paying attention to lung-window features can improve the model's\nstability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qiuli Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03820","description":"<p>Intra-voxel incoherent motion (IVIM) analysis of fetal lungs\nDiffusion-Weighted MRI (DWI) data shows potential in providing quantitative\nimaging bio-markers that reflect, indirectly, diffusion and pseudo-diffusion\nfor non-invasive fetal lung maturation assessment. However, long acquisition\ntimes, due to the large number of different \"b-value\" images required for IVIM\nanalysis, precluded clinical feasibility. We introduce SUPER-IVIM-DC a\ndeep-neural-networks (DNN) approach which couples supervised loss with a\ndata-consistency term to enable IVIM analysis of DWI data acquired with a\nlimited number of b-values. We demonstrated the added-value of SUPER-IVIM-DC\nover both classical and recent DNN approaches for IVIM analysis through\nnumerical simulations, healthy volunteer study, and IVIM analysis of fetal lung\nmaturation from fetal DWI data. % add results Our numerical simulations and\nhealthy volunteer study show that SUPER-IVIM-DC estimates of the IVIM model\nparameters from limited DWI data had lower normalized root mean-squared error\ncompared to previous DNN-based approaches. Further, SUPER-IVIM-DC estimates of\nthe pseudo-diffusion fraction parameter from limited DWI data of fetal lungs\ncorrelate better with gestational age compared to both to classical and\nDNN-based approaches (0.242 vs. -0.079 and 0.239). SUPER-IVIM-DC has the\npotential to reduce the long acquisition times associated with IVIM analysis of\nDWI data and to provide clinically feasible bio-markers for non-invasive fetal\nlung maturity assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korngut_N/0/1/0/all/0/1\">Noam Korngut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotman_E/0/1/0/all/0/1\">Elad Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afacan_O/0/1/0/all/0/1\">Onur Afacan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1\">Sila Kurugol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaffrani_Reznikov_Y/0/1/0/all/0/1\">Yael Zaffrani-Reznikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemirovsky_Rotman_S/0/1/0/all/0/1\">Shira Nemirovsky-Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warfield_S/0/1/0/all/0/1\">Simon Warfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v1 [cs.LG])","link":"http://arxiv.org/abs/2206.03826","description":"<p>For unsupervised pretraining, mask-reconstruction pretraining (MRP)\napproaches randomly mask input patches and then reconstruct pixels or semantic\nfeatures of these masked patches via an auto-encoder. Then for a downstream\ntask, supervised fine-tuning the pretrained encoder remarkably surpasses the\nconventional supervised learning (SL) trained from scratch. However, it is\nstill unclear 1) how MRP performs semantic learning in the pretraining phase\nand 2) why it helps in downstream tasks. To solve these problems, we\ntheoretically show that on an auto-encoder of a two/one-layered convolution\nencoder/decoder, MRP can capture all discriminative semantics in the\npretraining dataset, and accordingly show its provable improvement over SL on\nthe classification downstream task. Specifically, we assume that pretraining\ndataset contains multi-view samples of ratio $1-\\mu$ and single-view samples of\nratio $\\mu$, where multi/single-view samples has multiple/single discriminative\nsemantics. Then for pretraining, we prove that 1) the convolution kernels of\nthe MRP encoder captures all discriminative semantics in the pretraining data;\nand 2) a convolution kernel captures at most one semantic. Accordingly, in the\ndownstream supervised fine-tuning, most semantics would be captured and\ndifferent semantics would not be fused together. This helps the downstream\nfine-tuned network to easily establish the relation between kernels and\nsemantic class labels. In this way, the fine-tuned encoder in MRP provably\nachieves zero test error with high probability for both multi-view and\nsingle-view test data. In contrast, as proved by~[3], conventional SL can only\nobtain a test accuracy between around $0.5\\mu$ for single-view test data. These\nresults together explain the benefits of MRP in downstream tasks. Experimental\nresults testify to multi-view data assumptions and our theoretical\nimplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiachun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Myocardial Motion Tracking via Latent Space Exploration with Biomechanics-informed Prior. (arXiv:2206.03830v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03830","description":"<p>Myocardial motion and deformation are rich descriptors that characterize\ncardiac function. Image registration, as the most commonly used technique for\nmyocardial motion tracking, is an ill-posed inverse problem which often\nrequires prior assumptions on the solution space. In contrast to most existing\napproaches which impose explicit generic regularization such as smoothness, in\nthis work we propose a novel method that can implicitly learn an\napplication-specific biomechanics-informed prior and embed it into a neural\nnetwork-parameterized transformation model. Particularly, the proposed method\nleverages a variational autoencoder-based generative model to learn a manifold\nfor biomechanically plausible deformations. The motion tracking then can be\nperformed via traversing the learnt manifold to search for the optimal\ntransformations while considering the sequence information. The proposed method\nis validated on three public cardiac cine MRI datasets with comprehensive\nevaluations. The results demonstrate that the proposed method can outperform\nother approaches, yielding higher motion tracking accuracy with reasonable\nvolume preservation and better generalizability to varying data distributions.\nIt also enables better estimates of myocardial strains, which indicates the\npotential of the method in characterizing spatiotemporal signatures for\nunderstanding cardiovascular diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior. (arXiv:2206.03858v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03858","description":"<p>Inverse rendering is an ill-posed problem. Previous work has sought to\nresolve this by focussing on priors for object or scene shape or appearance. In\nthis work, we instead focus on a prior for natural illuminations. Current\nmethods rely on spherical harmonic lighting or other generic representations\nand, at best, a simplistic prior on the parameters. We propose a conditional\nneural field representation based on a variational auto-decoder with a SIREN\nnetwork and, extending Vector Neurons, build equivariance directly into the\nnetwork. Using this we develop a rotation-equivariant, high dynamic range (HDR)\nneural illumination model that is compact and able to express complex,\nhigh-frequency features of natural environment maps. Training our model on a\ncurated dataset of 1.6K HDR environment maps of natural scenes, we compare it\nagainst traditional representations, demonstrate its applicability for an\ninverse rendering task and show environment map completion from partial\nobservations. A PyTorch implementation, our dataset and trained models can be\nfound at jadgardner.github.io/RENI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">James A. D. Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1\">William A. P. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orthonormal Convolutions for the Rotation Based Iterative Gaussianization. (arXiv:2206.03860v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03860","description":"<p>In this paper we elaborate an extension of rotation-based iterative\nGaussianization, RBIG, which makes image Gaussianization possible. Although\nRBIG has been successfully applied to many tasks, it is limited to medium\ndimensionality data (on the order of a thousand dimensions). In images its\napplication has been restricted to small image patches or isolated pixels,\nbecause rotation in RBIG is based on principal or independent component\nanalysis and these transformations are difficult to learn and scale. Here we\npresent the \\emph{Convolutional RBIG}: an extension that alleviates this issue\nby imposing that the rotation in RBIG is a convolution. We propose to learn\nconvolutional rotations (i.e. orthonormal convolutions) by optimising for the\nreconstruction loss between the input and an approximate inverse of the\ntransformation using the transposed convolution operation. Additionally, we\nsuggest different regularizers in learning these orthonormal convolutions. For\nexample, imposing sparsity in the activations leads to a transformation that\nextends convolutional independent component analysis to multilayer\narchitectures. We also highlight how statistical properties of the data, such\nas multivariate mutual information, can be obtained from \\emph{Convolutional\nRBIG}. We illustrate the behavior of the transform with a simple example of\ntexture synthesis, and analyze its properties by visualizing the stimuli that\nmaximize the response in certain feature and layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1\">Valero Laparra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1\">Alexander Hepburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">J. Emmanuel Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1\">Jes&#xfa;s Malo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Quality Assessment for Fine-Grained Compressed Images. (arXiv:2206.03862v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03862","description":"<p>Recent years have witnessed the rapid development of image storage and\ntransmission systems, in which image compression plays an important role.\nGenerally speaking, image compression algorithms are developed to ensure good\nvisual quality at limited bit rates. However, due to the different compression\noptimization methods, the compressed images may have different levels of\nquality, which needs to be evaluated quantificationally. Nowadays, the\nmainstream full-reference (FR) metrics are effective to predict the quality of\ncompressed images at coarse-grained levels (the bit rates differences of\ncompressed images are obvious), however, they may perform poorly for\nfine-grained compressed images whose bit rates differences are quite subtle.\nTherefore, to better improve the Quality of Experience (QoE) and provide useful\nguidance for compression algorithms, we propose a full-reference image quality\nassessment (FR-IQA) method for compressed images of fine-grained levels.\nSpecifically, the reference images and compressed images are first converted to\n$YCbCr$ color space. The gradient features are extracted from regions that are\nsensitive to compression artifacts. Then we employ the Log-Gabor transformation\nto further analyze the texture difference. Finally, the obtained features are\nfused into a quality score. The proposed method is validated on the\nfine-grained compression image quality assessment (FGIQA) database, which is\nespecially constructed for assessing the quality of compressed images with\nclose bit rates. The experimental results show that our metric outperforms\nmainstream FR-IQA metrics on the FGIQA database. We also test our method on\nother commonly used compression IQA databases and the results show that our\nmethod obtains competitive performance on the coarse-grained compression IQA\ndatabases as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive GANomaly: Anomaly detection with progressively growing GANs. (arXiv:2206.03876v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03876","description":"<p>In medical imaging, obtaining large amounts of labeled data is often a\nhurdle, because annotations and pathologies are scarce. Anomaly detection is a\nmethod that is capable of detecting unseen abnormal data while only being\ntrained on normal (unannotated) data. Several algorithms based on generative\nadversarial networks (GANs) exist to perform this task, yet certain limitations\nare in place because of the instability of GANs. This paper proposes a new\nmethod by combining an existing method, GANomaly, with progressively growing\nGANs. The latter is known to be more stable, considering its ability to\ngenerate high-resolution images. The method is tested using Fashion MNIST,\nMedical Out-of-Distribution Analysis Challenge (MOOD), and in-house brain MRI;\nusing patches of sizes 16x16 and 32x32. Progressive GANomaly outperforms a\none-class SVM or regular GANomaly on Fashion MNIST. Artificial anomalies are\ncreated in MOOD images with varying intensities and diameters. Progressive\nGANomaly detected the most anomalies with varying intensity and size.\nAdditionally, the intermittent reconstructions are proven to be better from\nprogressive GANomaly. On the in-house brain MRI dataset, regular GANomaly\noutperformed the other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madzia_Madzou_D/0/1/0/all/0/1\">Djennifer K. Madzia-Madzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical Image Segmentation. (arXiv:2206.03888v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03888","description":"<p>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to an unlabeled target domain. Contrastive learning\n(CL) in the context of UDA can help to better separate classes in feature\nspace. However, in image segmentation, the large memory footprint due to the\ncomputation of the pixel-wise contrastive loss makes it prohibitive to use.\nFurthermore, labeled target data is not easily available in medical imaging,\nand obtaining new samples is not economical. As a result, in this work, we\ntackle a more challenging UDA task when there are only a few (fewshot) or a\nsingle (oneshot) image available from the target domain. We apply a style\ntransfer module to mitigate the scarcity of target samples. Then, to align the\nsource and target features and tackle the memory issue of the traditional\ncontrastive loss, we propose the centroid-based contrastive learning (CCL) and\na centroid norm regularizer (CNR) to optimize the contrastive pairs in both\ndirection and magnitude. In addition, we propose multi-partition centroid\ncontrastive learning (MPCCL) to further reduce the variance in the target\nfeatures. Fewshot evaluation on MS-CMRSeg dataset demonstrates that ConFUDA\nimproves the segmentation performance by 0.34 of the Dice score on the target\ndomain compared with the baseline, and 0.31 Dice score improvement in a more\nrigorous oneshot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Mingxuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vesal_S/0/1/0/all/0/1\">Sulaiman Vesal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_M/0/1/0/all/0/1\">Mareike Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhaoya Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_F/0/1/0/all/0/1\">Fabian Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosti_R/0/1/0/all/0/1\">Ronak Kosti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrivHAR: Recognizing Human Actions From Privacy-preserving Lens. (arXiv:2206.03891v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03891","description":"<p>The accelerated use of digital cameras prompts an increasing concern about\nprivacy and security, particularly in applications such as action recognition.\nIn this paper, we propose an optimizing framework to provide robust visual\nprivacy protection along the human action recognition pipeline. Our framework\nparameterizes the camera lens to successfully degrade the quality of the videos\nto inhibit privacy attributes and protect against adversarial attacks while\nmaintaining relevant features for activity recognition. We validate our\napproach with extensive simulations and hardware experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hinojosa_C/0/1/0/all/0/1\">Carlos Hinojosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquez_M/0/1/0/all/0/1\">Miguel Marquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arguello_H/0/1/0/all/0/1\">Henry Arguello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1\">Juan Carlos Niebles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans. (arXiv:2206.03900v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03900","description":"<p>Registration of pre-operative and post-recurrence brain images is often\nneeded to evaluate the effectiveness of brain gliomas treatment. While recent\ndeep learning-based deformable registration methods have achieved remarkable\nsuccess with healthy brain images, most of them would be unable to accurately\nalign images with pathologies due to the absent correspondences in the\nreference image. In this paper, we propose a deep learning-based deformable\nregistration method that jointly estimates regions with absent correspondence\nand bidirectional deformation fields. A forward-backward consistency constraint\nis used to aid in the localization of the resection and recurrence region from\nvoxels with absence correspondences in the two images. Results on 3D clinical\ndata from the BraTS-Reg challenge demonstrate our method can improve image\nalignment compared to traditional and deep learning-based registration\napproaches with or without cost function masking strategy. The source code is\navailable at https://github.com/cwmok/DIRAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mok_T/0/1/0/all/0/1\">Tony C. W. Mok</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_A/0/1/0/all/0/1\">Albert C. S. Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Triangulation with Spherical Projection for Omnidirectional Cameras. (arXiv:2206.03928v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03928","description":"<p>In this paper, it is proposed to solve the problem of triangulation for\ncalibrated omnidirectional cameras through the optimisation of ray-pairs on the\nprojective sphere. The proposed solution boils down to finding the roots of a\nquadratic function, and as such is closed form, completely non-iterative and\ncomputationally inexpensive when compared to previous methods. In addition,\neven thought the motivation is clearly to solve the triangulation problem for\nomnidirectional cameras, it is demonstrated that the proposed methods can be\napplied to non-omnidirectional, narrow field-of-view cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1\">Ciar&#xe1;n Eising</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays. (arXiv:2206.03935v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03935","description":"<p>Chest X-ray (CXR) is the most typical radiological exam for diagnosis of\nvarious diseases. Due to the expensive and time-consuming annotations,\ndetecting anomalies in CXRs in an unsupervised fashion is very promising.\nHowever, almost all of the existing methods consider anomaly detection as a\nOne-Class Classification (OCC) problem. They model the distribution of only\nknown normal images during training and identify the samples not conforming to\nnormal profile as anomalies in the testing phase. A large number of unlabeled\nimages containing anomalies are thus ignored in the training phase, although\nthey are easy to obtain in clinical practice. In this paper, we propose a novel\nstrategy, Dual-distribution Discrepancy for Anomaly Detection (DDAD), utilizing\nboth known normal images and unlabeled images. The proposed method consists of\ntwo modules, denoted as A and B. During training, module A takes both known\nnormal and unlabeled images as inputs, capturing anomalous features from\nunlabeled images in some way, while module B models the distribution of only\nknown normal images. Subsequently, the inter-discrepancy between modules A and\nB, and intra-discrepancy inside module B are designed as anomaly scores to\nindicate anomalies. Experiments on three CXR datasets demonstrate that the\nproposed DDAD achieves consistent, significant gains and outperforms\nstate-of-the-art methods. Code is available at\nhttps://github.com/caiyu6666/DDAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yu Cai</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a> (1 and 3) ((1) Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China, (2) School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China, (3) Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-Adapted CNNs for RGB-D Semantic Segmentation. (arXiv:2206.03939v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03939","description":"<p>Recent RGB-D semantic segmentation has motivated research interest thanks to\nthe accessibility of complementary modalities from the input side. Existing\nworks often adopt a two-stream architecture that processes photometric and\ngeometric information in parallel, with few methods explicitly leveraging the\ncontribution of depth cues to adjust the sampling position on RGB images. In\nthis paper, we propose a novel framework to incorporate the depth information\nin the RGB convolutional neural network (CNN), termed Z-ACN (Depth-Adapted\nCNN). Specifically, our Z-ACN generates a 2D depth-adapted offset which is\nfully constrained by low-level features to guide the feature extraction on RGB\nimages. With the generated offset, we introduce two intuitive and effective\noperations to replace basic CNN operators: depth-adapted convolution and\ndepth-adapted average pooling. Extensive experiments on both indoor and outdoor\nsemantic segmentation tasks demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zongwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allibert_G/0/1/0/all/0/1\">Guillaume Allibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolz_C/0/1/0/all/0/1\">Christophe Stolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demonceaux_C/0/1/0/all/0/1\">C&#xe9;dric Demonceaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Environment Perception for Automated Driving: A Unified Learning Pipeline for Visual-Infrared Object Detection. (arXiv:2206.03943v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03943","description":"<p>The RGB complementary metal-oxidesemiconductor (CMOS) sensor works within the\nvisible light spectrum. Therefore it is very sensitive to environmental light\nconditions. On the contrary, a long-wave infrared (LWIR) sensor operating in\n8-14 micro meter spectral band, functions independent of visible light.\n</p>\n<p>In this paper, we exploit both visual and thermal perception units for robust\nobject detection purposes. After delicate synchronization and (cross-) labeling\nof the FLIR [1] dataset, this multi-modal perception data passes through a\nconvolutional neural network (CNN) to detect three critical objects on the\nroad, namely pedestrians, bicycles, and cars. After evaluation of RGB and\ninfrared (thermal and infrared are often used interchangeably) sensors\nseparately, various network structures are compared to fuse the data at the\nfeature level effectively. Our RGB-thermal (RGBT) fusion network, which takes\nadvantage of a novel entropy-block attention module (EBAM), outperforms the\nstate-of-the-art network [2] by 10% with 82.9% mAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vadidar_M/0/1/0/all/0/1\">Mohsen Vadidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariminezhad_A/0/1/0/all/0/1\">Ali Kariminezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_C/0/1/0/all/0/1\">Christian Mayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloeker_L/0/1/0/all/0/1\">Laurent Kloeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1\">Lutz Eckstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-Distribution Detection with Class Ratio Estimation. (arXiv:2206.03955v1 [stat.ML])","link":"http://arxiv.org/abs/2206.03955","description":"<p>Density-based Out-of-distribution (OOD) detection has recently been shown\nunreliable for the task of detecting OOD images. Various density ratio based\napproaches achieve good empirical performance, however methods typically lack a\nprincipled probabilistic modelling explanation. In this work, we propose to\nunify density ratio based methods under a novel framework that builds\nenergy-based models and employs differing base distributions. Under our\nframework, the density ratio can be viewed as the unnormalized density of an\nimplicit semantic distribution. Further, we propose to directly estimate the\ndensity ratio of a data sample through class ratio estimation. We report\ncompetitive results on OOD image problems in comparison with recent work that\nalternatively requires training of deep generative models for the task. Our\napproach enables a simple and yet effective path towards solving the OOD\ndetection problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1\">Mingtian Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_A/0/1/0/all/0/1\">Andi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xiao_T/0/1/0/all/0/1\">Tim Z. Xiao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1\">Yitong Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03970","description":"<p>Behavior prediction models have proliferated in recent years, especially in\nthe popular real-world robotics application of autonomous driving, where\nrepresenting the distribution over possible futures of moving agents is\nessential for safe and comfortable motion planning. In these models, the choice\nof coordinate frames to represent inputs and outputs has crucial trade offs\nwhich broadly fall into one of two categories. Agent-centric models transform\ninputs and perform inference in agent-centric coordinates. These models are\nintrinsically invariant to translation and rotation between scene elements, are\nbest-performing on public leaderboards, but scale quadratically with the number\nof agents and scene elements. Scene-centric models use a fixed coordinate\nsystem to process all agents. This gives them the advantage of sharing\nrepresentations among all agents, offering efficient amortized inference\ncomputation which scales linearly with the number of agents. However, these\nmodels have to learn invariance to translation and rotation between scene\nelements, and typically underperform agent-centric models. In this work, we\ndevelop knowledge distillation techniques between probabilistic motion\nforecasting models, and apply these techniques to close the gap in performance\nbetween agent-centric and scene-centric models. This improves scene-centric\nmodel performance by 13.2% on the public Argoverse benchmark, 7.8% on Waymo\nOpen Dataset and up to 9.4% on a large In-House dataset. These improved\nscene-centric models rank highly in public leaderboards and are up to 15 times\nmore efficient than their agent-centric teacher counterparts in busy scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">DiJia Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douillard_B/0/1/0/all/0/1\">Bertrand Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Cheolho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Benjamin Sapp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-based Object-centric Transformers for Efficient Video Generation. (arXiv:2206.04003v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04003","description":"<p>In this work, we present Patch-based Object-centric Video Transformer (POVT),\na novel region-based video generation architecture that leverages\nobject-centric information to efficiently model temporal dynamics in videos. We\nbuild upon prior work in video prediction via an autoregressive transformer\nover the discrete latent space of compressed videos, with an added modification\nto model object-centric information via bounding boxes. Due to better\ncompressibility of object-centric representations, we can improve training\nefficiency by allowing the model to only access object information for longer\nhorizon temporal information. When evaluated on various difficult\nobject-centric datasets, our method achieves better or equal performance to\nother video generation models, while remaining computationally more efficient\nand scalable. In addition, we show that our method is able to perform\nobject-centric controllability through bounding box manipulation, which may aid\ndownstream tasks such as video editing, or visual planning. Samples are\navailable at\nhttps://sites.google.com/view/povt-public}{https://sites.google.com/view/povt-public\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wilson Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_R/0/1/0/all/0/1\">Ryo Okumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Audio-Visual Learning of Environment Acoustics. (arXiv:2206.04006v1 [cs.SD])","link":"http://arxiv.org/abs/2206.04006","description":"<p>Room impulse response (RIR) functions capture how the surrounding physical\nenvironment transforms the sounds heard by a listener, with implications for\nvarious applications in AR, VR, and robotics. Whereas traditional methods to\nestimate RIRs assume dense geometry and/or sound measurements throughout the\nenvironment, we explore how to infer RIRs based on a sparse set of images and\nechoes observed in the space. Towards that goal, we introduce a\ntransformer-based method that uses self-attention to build a rich acoustic\ncontext, then predicts RIRs of arbitrary query source-receiver locations\nthrough cross-attention. Additionally, we design a novel training objective\nthat improves the match in the acoustic signature between the RIR predictions\nand the targets. In experiments using a state-of-the-art audio-visual simulator\nfor 3D environments, we demonstrate that our method successfully generates\narbitrary RIRs, outperforming state-of-the-art methods and--in a major\ndeparture from traditional methods--generalizing to novel environments in a\nfew-shot manner. Project: <a href=\"http://vision.cs.utexas.edu/projects/fs_rir.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1\">Sagnik Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Halah_Z/0/1/0/all/0/1\">Ziad Al-Halah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SYNERgy between SYNaptic consolidation and Experience Replay for general continual learning. (arXiv:2206.04016v1 [cs.NE])","link":"http://arxiv.org/abs/2206.04016","description":"<p>Continual learning (CL) in the brain is facilitated by a complex set of\nmechanisms. This includes the interplay of multiple memory systems for\nconsolidating information as posited by the complementary learning systems\n(CLS) theory and synaptic consolidation for protecting the acquired knowledge\nfrom erasure. Thus, we propose a general CL method that creates a synergy\nbetween SYNaptic consolidation and dual memory Experience Replay (SYNERgy). Our\nmethod maintains a semantic memory that accumulates and consolidates\ninformation across the tasks and interacts with the episodic memory for\neffective replay. It further employs synaptic consolidation by tracking the\nimportance of parameters during the training trajectory and anchoring them to\nthe consolidated parameters in the semantic memory. To the best of our\nknowledge, our study is the first to employ dual memory experience replay in\nconjunction with synaptic consolidation that is suitable for general CL whereby\nthe network does not utilize task boundaries or task labels during training or\ninference. Our evaluation on various challenging CL scenarios and\ncharacteristics analyses demonstrate the efficacy of incorporating both\nsynaptic consolidation and CLS theory in enabling effective CL in DNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarfraz_F/0/1/0/all/0/1\">Fahad Sarfraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CO^3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving. (arXiv:2206.04028v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04028","description":"<p>Unsupervised contrastive learning for indoor-scene point clouds has achieved\ngreat successes. However, unsupervised learning point clouds in outdoor scenes\nremains challenging because previous methods need to reconstruct the whole\nscene and capture partial views for the contrastive objective. This is\ninfeasible in outdoor scenes with moving objects, obstacles, and sensors. In\nthis paper, we propose CO^3, namely Cooperative Contrastive Learning and\nContextual Shape Prediction, to learn 3D representation for outdoor-scene point\nclouds in an unsupervised manner. CO^3 has several merits compared to existing\nmethods. (1) It utilizes LiDAR point clouds from vehicle-side and\ninfrastructure-side to build views that differ enough but meanwhile maintain\ncommon semantic information for contrastive learning, which are more\nappropriate than views built by previous methods. (2) Alongside the contrastive\nobjective, shape context prediction is proposed as pre-training goal and brings\nmore task-relevant information for unsupervised 3D point cloud representation\nlearning, which are beneficial when transferring the learned representation to\ndownstream detection tasks. (3) As compared to previous methods, representation\nlearned by CO^3 is able to be transferred to different outdoor scene dataset\ncollected by different type of LiDAR sensors. (4) CO^3 improves current\nstate-of-the-art methods on both Once and KITTI datasets by up to 2.58 mAP.\nCodes and models will be released. We believe CO^3 will facilitate\nunderstanding LiDAR point clouds in outdoor scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yao Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenhan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04029","description":"<p>Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. The key idea is to produce high-quality images by\nrecurrently adding Gaussian noises and gradients to a Gaussian sample until\nconverging to the target distribution, a.k.a. the diffusion sampling. To ensure\nstability of convergence in sampling and generation quality, however, this\nsequential sampling process has to take a small step size and many sampling\niterations (e.g., 2000). Several acceleration methods have been proposed with\nfocus on low-resolution generation. In this work, we consider the acceleration\nof high-resolution generation with SGMs, a more challenging yet more important\nproblem. We prove theoretically that this slow convergence drawback is\nprimarily due to the ignorance of the target distribution. Further, we\nintroduce a novel Target Distribution Aware Sampling (TDAS) method by\nleveraging the structural priors in space and frequency domains. Extensive\nexperiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can\nconsistently accelerate state-of-the-art SGMs, particularly on more challenging\nhigh resolution (1024x1024) image generation tasks by up to 18.4x, whilst\nlargely maintaining the synthesis quality. With fewer sampling iterations, TDAS\ncan still generate good quality images. In contrast, the existing methods\ndegrade drastically or even fails completely\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved One millisecond Mobile Backbone. (arXiv:2206.04040v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04040","description":"<p>Efficient neural network backbones for mobile devices are often optimized for\nmetrics such as FLOPs or parameter count. However, these metrics may not\ncorrelate well with latency of the network when deployed on a mobile device.\nTherefore, we perform extensive analysis of different metrics by deploying\nseveral mobile-friendly networks on a mobile device. We identify and analyze\narchitectural and optimization bottlenecks in recent efficient neural networks\nand provide ways to mitigate these bottlenecks. To this end, we design an\nefficient backbone MobileOne, with variants achieving an inference time under 1\nms on an iPhone12 with 75.9% top-1 accuracy on ImageNet. We show that MobileOne\nachieves state-of-the-art performance within the efficient architectures while\nbeing many times faster on mobile. Our best model obtains similar performance\non ImageNet as MobileFormer while being 38x faster. Our model obtains 2.3%\nbetter top-1 accuracy on ImageNet than EfficientNet at similar latency.\nFurthermore, we show that our model generalizes to multiple tasks - image\nclassification, object detection, and semantic segmentation with significant\nimprovements in latency and accuracy as compared to existing efficient\narchitectures when deployed on a mobile device.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasu_P/0/1/0/all/0/1\">Pavan Kumar Anasosalu Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_J/0/1/0/all/0/1\">James Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jeff Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1\">Oncel Tuzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Anurag Ranjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Ego 3D Representation as Ray Tracing. (arXiv:2206.04042v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04042","description":"<p>A self-driving perception model aims to extract 3D semantic representations\nfrom multiple cameras collectively into the bird's-eye-view (BEV) coordinate\nframe of the ego car in order to ground downstream planner. Existing perception\nmethods often rely on error-prone depth estimation of the whole scene or\nlearning sparse virtual 3D representations without the target geometry\nstructure, both of which remain limited in performance and/or capability. In\nthis paper, we present a novel end-to-end architecture for ego 3D\nrepresentation learning from an arbitrary number of unconstrained camera views.\nInspired by the ray tracing principle, we design a polarized grid of \"imaginary\neyes\" as the learnable ego 3D representation and formulate the learning process\nwith the adaptive attention mechanism in conjunction with the 3D-to-2D\nprojection. Critically, this formulation allows extracting rich 3D\nrepresentation from 2D images without any depth supervision, and with the\nbuilt-in geometry structure consistent w.r.t. BEV. Despite its simplicity and\nversatility, extensive experiments on standard BEV visual tasks (e.g.,\ncamera-based 3D object detection and BEV segmentation) show that our model\noutperforms all state-of-the-art alternatives significantly, with an extra\nadvantage in computational efficiency from multi-task learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zheyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04046","description":"<p>Domain generalization (DG) aims at learning generalizable models under\ndistribution shifts to avoid redundantly overfitting massive training data.\nPrevious works with complex loss design and gradient constraint have not yet\nled to empirical success on large-scale benchmarks. In this work, we reveal the\nmixture-of-experts (MoE) model's generalizability on DG by leveraging to\ndistributively handle multiple aspects of the predictive features across\ndomains. To this end, we propose Sparse Fusion Mixture-of-Experts (SF-MoE),\nwhich incorporates sparsity and fusion mechanisms into the MoE framework to\nkeep the model both sparse and predictive. SF-MoE has two dedicated modules: 1)\nsparse block and 2) fusion block, which disentangle and aggregate the diverse\nlearned signals of an object, respectively. Extensive experiments demonstrate\nthat SF-MoE is a domain-generalizable learner on large-scale benchmarks. It\noutperforms state-of-the-art counterparts by more than 2% across 5 large-scale\nDG datasets (e.g., DomainNet), with the same or even lower computational costs.\nWe further reveal the internal mechanism of SF-MoE from distributed\nrepresentation perspective (e.g., visual attributes). We hope this framework\ncould facilitate future research to push generalizable object recognition to\nthe real world. Code and models are released at\nhttps://github.com/Luodian/SF-MoE-DG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiawei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Target Detection and Recognition Method in Aerial Remote-sensing Images Based on Multiangle Regions-of-Interest. (arXiv:1907.09320v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.09320","description":"<p>Recently, deep learning technology have been extensively used in the field of\nimage recognition. However, its main application is the recognition and\ndetection of ordinary pictures and common scenes. It is challenging to\neffectively and expediently analyze remote-sensing images obtained by the image\nacquisition systems on unmanned aerial vehicles (UAVs), which includes the\nidentification of the target and calculation of its position. Aerial remote\nsensing images have different shooting angles and methods compared with\nordinary pictures or images, which makes remote-sensing images play an\nirreplaceable role in some areas. In this study, a new target detection and\nrecognition method in remote-sensing images is proposed based on deep\nconvolution neural network (CNN) for the provision of multilevel information of\nimages in combination with a region proposal network used to generate\nmultiangle regions-of-interest. The proposed method generated results that were\nmuch more accurate and precise than those obtained with traditional ways. This\ndemonstrated that the model proposed herein displays tremendous applicability\npotential in remote-sensing image recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shan_G/0/1/0/all/0/1\">Guangcun Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Wei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Congcong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qizi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blacklight: Defending Black-Box Adversarial Attacks on Deep Neural Networks. (arXiv:2006.14042v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2006.14042","description":"<p>Deep learning systems are known to be vulnerable to adversarial examples. In\nparticular, query-based black-box attacks do not require knowledge of the deep\nlearning model, but can compute adversarial examples over the network by\nsubmitting queries and inspecting returns. Recent work largely improves the\nefficiency of those attacks, demonstrating their practicality on today's\nML-as-a-service platforms.\n</p>\n<p>We propose Blacklight, a new defense against query-based black-box\nadversarial attacks. The fundamental insight driving our design is that, to\ncompute adversarial examples, these attacks perform iterative optimization over\nthe network, producing image queries highly similar in the input space.\nBlacklight detects query-based black-box attacks by detecting highly similar\nqueries, using an efficient similarity engine operating on probabilistic\ncontent fingerprints. We evaluate Blacklight against eight state-of-the-art\nattacks, across a variety of models and image classification tasks. Blacklight\nidentifies them all, often after only a handful of queries. By rejecting all\ndetected queries, Blacklight prevents any attack to complete, even when\nattackers persist to submit queries after account ban or query rejection.\nBlacklight is also robust against several powerful countermeasures, including\nan optimal black-box attack that approximates white-box attacks in efficiency.\nFinally, we illustrate how Blacklight generalizes to other domains like text\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shawn Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenger_E/0/1/0/all/0/1\">Emily Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Ben Y. Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Topography Characterization Using a Simple Optical Device and Artificial Neural Networks. (arXiv:2103.08482v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08482","description":"<p>State-of-the-art methods for quantifying wear in cylinder liners of large\ninternal combustion engines for stationary power generation require disassembly\nand cutting of the examined liner. This is followed by laboratory-based\nhigh-resolution microscopic surface depth measurement that quantitatively\nevaluates wear based on bearing load curves (also known as Abbott-Firestone\ncurves). Such reference methods are destructive, time-consuming and costly. The\ngoal of the research presented here is to develop nondestructive yet reliable\nmethods for quantifying the surface topography. A novel machine learning\nframework is proposed that allows prediction of the bearing load curves\nrepresenting the depth profiles from reflection RGB images of the liner\nsurface. These images can be collected with a simple handheld microscope. A\njoint deep learning approach involving two neural network modules optimizes the\nprediction quality of surface roughness parameters as well. The network stack\nis trained using a custom-built database containing 422 perfectly aligned depth\nprofile and reflection image pairs of liner surfaces of large gas engines. The\nobserved success of the method suggests its great potential for on-site wear\nassessment of engines during service.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_M/0/1/0/all/0/1\">Matthias Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravova_A/0/1/0/all/0/1\">Ad&#xe9;la Moravov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesling_C/0/1/0/all/0/1\">Constantin Kiesling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_M/0/1/0/all/0/1\">Martin Kober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fimml_W/0/1/0/all/0/1\">Wolfgang Fimml</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05704","description":"<p>With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, there has been a corresponding growth in\nparameter size and amounts of training data. Many have come to believe that\nbecause of this, transformers are not suitable for small sets of data. This\ntrend leads to concerns such as: limited availability of data in certain\nscientific domains and the exclusion of those with limited resource from\nresearch in the field. In this paper, we aim to present an approach for\nsmall-scale learning by introducing Compact Transformers. We show for the first\ntime that with the right size, convolutional tokenization, transformers can\navoid overfitting and outperform state-of-the-art CNNs on small datasets. Our\nmodels are flexible in terms of model size, and can have as little as 0.28M\nparameters while achieving competitive results. Our best model can reach 98%\naccuracy when training from scratch on CIFAR-10 with only 3.7M parameters,\nwhich is a significant improvement in data-efficiency over previous Transformer\nbased models being over 10x smaller than other transformers and is 15% the size\nof ResNet50 while achieving similar performance. CCT also outperforms many\nmodern CNN based approaches, and even some recent NAS-based approaches.\nAdditionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1\naccuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy\nwith 29% as many parameters as ViT), as well as NLP tasks. Our simple and\ncompact design for transformers makes them more feasible to study for those\nwith limited computing resources and/or dealing with small datasets, while\nextending existing research efforts in data efficient transformers. Our code\nand pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Compact-Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutions for Spatial Interaction Modeling. (arXiv:2104.07182v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07182","description":"<p>In many different fields interactions between objects play a critical role in\ndetermining their behavior. Graph neural networks (GNNs) have emerged as a\npowerful tool for modeling interactions, although often at the cost of adding\nconsiderable complexity and latency. In this paper, we consider the problem of\nspatial interaction modeling in the context of predicting the motion of actors\naround autonomous vehicles, and investigate alternatives to GNNs. We revisit 2D\nconvolutions and show that they can demonstrate comparable performance to graph\nnetworks in modeling spatial interactions with lower latency, thus providing an\neffective and efficient alternative in time-critical systems. Moreover, we\npropose a novel interaction loss to further improve the interaction modeling of\nthe considered methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_D/0/1/0/all/0/1\">David Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallespi_Gonzalez_C/0/1/0/all/0/1\">Carlos Vallespi-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wellington_C/0/1/0/all/0/1\">Carl Wellington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djuric_N/0/1/0/all/0/1\">Nemanja Djuric</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure DNN Accelerators. (arXiv:2104.08323v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.08323","description":"<p>Deep neural network (DNN) accelerators received considerable attention in\nrecent years due to the potential to save energy compared to mainstream\nhardware. Low-voltage operation of DNN accelerators allows to further reduce\nenergy consumption, however, causes bit-level failures in the memory storing\nthe quantized weights. Furthermore, DNN accelerators are vulnerable to\nadversarial attacks on voltage controllers or individual bits. In this paper,\nwe show that a combination of robust fixed-point quantization, weight clipping,\nas well as random bit error training (RandBET) or adversarial bit error\ntraining (AdvBET) improves robustness against random or adversarial bit errors\nin quantized DNN weights significantly. This leads not only to high energy\nsavings for low-voltage operation as well as low-precision quantization, but\nalso improves security of DNN accelerators. In contrast to related work, our\napproach generalizes across operating voltages and accelerators and does not\nrequire hardware changes. Moreover, we present a novel adversarial bit error\nattack and are able to obtain robustness against both targeted and untargeted\nbit-level attacks. Without losing more than 0.8%/2% in test accuracy, we can\nreduce energy consumption on CIFAR10 by 20%/30% for 8/4-bit quantization.\nAllowing up to 320 adversarial bit errors, we reduce test error from above 90%\n(chance level) to 26.22%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandramoorthy_N/0/1/0/all/0/1\">Nandhini Chandramoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathological Image Detection. (arXiv:2104.14528v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14528","description":"<p>In this paper, a multi-scale visual transformer model, referred as\nGasHis-Transformer, is proposed for Gastric Histopathological Image Detection\n(GHID), which enables the automatic global detection of gastric cancer images.\nGasHis-Transformer model consists of two key modules designed to extract global\nand local information using a position-encoded transformer model and a\nconvolutional neural network with local convolution, respectively. A publicly\navailable hematoxylin and eosin (H&amp;E) stained gastric histopathological image\ndataset is used in the experiment. Furthermore, a Dropconnect based lightweight\nnetwork is proposed to reduce the model size and training time of\nGasHis-Transformer for clinical applications with improved confidence.\nMoreover, a series of contrast and extended experiments verify the robustness,\nextensibility and stability of GasHis-Transformer. In conclusion,\nGasHis-Transformer demonstrates high global detection performance and shows its\nsignificant potential in GHID task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_S/0/1/0/all/0/1\">Shiliang Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Online Learning System for Wireless Charging Alignment using Surround-view Fisheye Cameras. (arXiv:2105.12763v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12763","description":"<p>Electric Vehicles are increasingly common, with inductive chargepads being\nconsidered a convenient and efficient means of charging electric vehicles.\nHowever, drivers are typically poor at aligning the vehicle to the necessary\naccuracy for efficient inductive charging, making the automated alignment of\nthe two charging plates desirable. In parallel to the electrification of the\nvehicular fleet, automated parking systems that make use of surround-view\ncamera systems are becoming increasingly popular. In this work, we propose a\nsystem based on the surround-view camera architecture to detect, localize, and\nautomatically align the vehicle with the inductive chargepad. The visual design\nof the chargepads is not standardized and not necessarily known beforehand.\nTherefore, a system that relies on offline training will fail in some\nsituations. Thus, we propose a self-supervised online learning method that\nleverages the driver's actions when manually aligning the vehicle with the\nchargepad and combine it with weak supervision from semantic segmentation and\ndepth to learn a classifier to auto-annotate the chargepad in the video for\nfurther training. In this way, when faced with a previously unseen chargepad,\nthe driver needs only manually align the vehicle a single time. As the\nchargepad is flat on the ground, it is not easy to detect it from a distance.\nThus, we propose using a Visual SLAM pipeline to learn landmarks relative to\nthe chargepad to enable alignment from a greater range. We demonstrate the\nworking system on an automated vehicle as illustrated in the video at\nhttps://youtu.be/_cLCmkW4UYo. To encourage further research, we will share a\nchargepad dataset used in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahal_A/0/1/0/all/0/1\">Ashok Dahal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1\">Ciaran Eising</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset, and Toolkit. (arXiv:2106.14922v2 [astro-ph.IM] UPDATED)","link":"http://arxiv.org/abs/2106.14922","description":"<p>Rejecting cosmic rays (CRs) is essential for the scientific interpretation of\nCCD-captured data, but detecting CRs in single-exposure images has remained\nchallenging. Conventional CR detectors require experimental parameter tuning\nfor different instruments, and recent deep learning methods only produce\ninstrument-specific models that suffer from performance loss on telescopes not\nincluded in the training data. In this work, we present Cosmic-CoNN, a generic\nCR detector deployed for 24 telescopes at the Las Cumbres Observatory (LCO). We\nfirst leverage thousands of images from LCO's global telescope network to build\na large, diverse ground-based CR dataset for rich coverage of instruments and\nCR features. We then optimize a neural network and propose a novel\nMedian-Weighted loss function for CR detection to train a generic model that\nachieves a 99.91% true-positive detection rate on LCO imaging data and\nmaintains over 96.40% on unseen data from Gemini GMOS-N/S, with a\nfalse-positive rate of 0.01%. We also build a suite of tools including an\ninteractive CR mask visualization and editing interface, console commands, and\nPython APIs to make automatic, robust CR detection widely accessible by the\ncommunity of astronomers. Our dataset, open-source codebase, and trained models\nare available at https://github.com/cy-xu/cosmic-conn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Xu_C/0/1/0/all/0/1\">Chengyuan Xu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+McCully_C/0/1/0/all/0/1\">Curtis McCully</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dong_B/0/1/0/all/0/1\">Boning Dong</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Howell_D/0/1/0/all/0/1\">D. Andrew Howell</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sen_P/0/1/0/all/0/1\">Pradeep Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PU-Flow: a Point Cloud Upsampling Network with Normalizing Flows. (arXiv:2107.05893v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05893","description":"<p>Point cloud upsampling aims to generate dense point clouds from given sparse\nones, which is a challenging task due to the irregular and unordered nature of\npoint sets. To address this issue, we present a novel deep learning-based\nmodel, called PU-Flow, which incorporates normalizing flows and weight\nprediction techniques to produce dense points uniformly distributed on the\nunderlying surface. Specifically, we exploit the invertible characteristics of\nnormalizing flows to transform points between Euclidean and latent spaces and\nformulate the upsampling process as ensemble of neighbouring points in a latent\nspace, where the ensemble weights are adaptively learned from local geometric\ncontext. Extensive experiments show that our method is competitive and, in most\ntest cases, it outperforms state-of-the-art methods in terms of reconstruction\nquality, proximity-to-surface accuracy, and computation efficiency. The source\ncode will be publicly available at https://github.com/unknownue/pu-flow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Aihua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zihui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yaqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribution of Predictive Uncertainties in Classification Models. (arXiv:2107.08756v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.08756","description":"<p>Predictive uncertainties in classification tasks are often a consequence of\nmodel inadequacy or insufficient training data. In popular applications, such\nas image processing, we are often required to scrutinise these uncertainties by\nmeaningfully attributing them to input features. This helps to improve\ninterpretability assessments. However, there exist few effective frameworks for\nthis purpose. Vanilla forms of popular methods for the provision of saliency\nmasks, such as SHAP or integrated gradients, adapt poorly to target measures of\nuncertainty. Thus, state-of-the-art tools instead proceed by creating\ncounterfactual or adversarial feature vectors, and assign attributions by\ndirect comparison to original images. In this paper, we present a novel\nframework that combines path integrals, counterfactual explanations and\ngenerative models, in order to procure attributions that contain few observable\nartefacts or noise. We evidence that this outperforms existing alternatives\nthrough quantitative evaluations with popular benchmarking methods and data\nsets of varying complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Iker Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1\">Piotr Skalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1\">Alec Barns-Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jason Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1\">David Sutton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images. (arXiv:2109.10778v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10778","description":"<p>Annotating cancerous regions in whole-slide images (WSIs) of pathology\nsamples plays a critical role in clinical diagnosis, biomedical research, and\nmachine learning algorithms development. However, generating exhaustive and\naccurate annotations is labor-intensive, challenging, and costly. Drawing only\ncoarse and approximate annotations is a much easier task, less costly, and it\nalleviates pathologists' workload. In this paper, we study the problem of\nrefining these approximate annotations in digital pathology to obtain more\naccurate ones. Some previous works have explored obtaining machine learning\nmodels from these inaccurate annotations, but few of them tackle the refinement\nproblem where the mislabeled regions should be explicitly identified and\ncorrected, and all of them require a -- often very large -- number of training\nsamples. We present a method, named Label Cleaning Multiple Instance Learning\n(LC-MIL), to refine coarse annotations on a single WSI without the need of\nexternal training data. Patches cropped from a WSI with inaccurate labels are\nprocessed jointly within a multiple instance learning framework, mitigating\ntheir impact on the predictive model and refining the segmentation. Our\nexperiments on a heterogeneous WSI set with breast cancer lymph node\nmetastasis, liver cancer, and colorectal cancer samples show that LC-MIL\nsignificantly refines the coarse annotations, outperforming state-of-the-art\nalternatives, even while learning from a single slide. Moreover, we demonstrate\nhow real annotations drawn by pathologists can be efficiently refined and\nimproved by the proposed approach. All these results demonstrate that LC-MIL is\na promising, light-weight tool to provide fine-grained annotations from\ncoarsely annotated pathology sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saoud_C/0/1/0/all/0/1\">Carla Saoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wangsiricharoen_S/0/1/0/all/0/1\">Sintawat Wangsiricharoen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_A/0/1/0/all/0/1\">Aaron W. James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popel_A/0/1/0/all/0/1\">Aleksander S. Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1\">Jeremias Sulam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDFace-Celeb-1M: Benchmarking Face Hallucination with a Million-scale Dataset. (arXiv:2110.05031v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05031","description":"<p>Recent deep face hallucination methods show stunning performance in\nsuper-resolving severely degraded facial images, even surpassing human ability.\nHowever, these algorithms are mainly evaluated on non-public synthetic\ndatasets. It is thus unclear how these algorithms perform on public face\nhallucination datasets. Meanwhile, most of the existing datasets do not well\nconsider the distribution of races, which makes face hallucination methods\ntrained on these datasets biased toward some specific races. To address the\nabove two problems, in this paper, we build a public Ethnically Diverse Face\ndataset, EDFace-Celeb-1M, and design a benchmark task for face hallucination.\nOur dataset includes 1.7 million photos that cover different countries, with\nbalanced race composition. To the best of our knowledge, it is the largest and\npublicly available face hallucination dataset in the wild. Associated with this\ndataset, this paper also contributes various evaluation protocols and provides\ncomprehensive analysis to benchmark the existing state-of-the-art methods. The\nbenchmark evaluations demonstrate the performance and limitations of\nstate-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Galaxy Morphology Tools from Deep Supervised Representation Learning. (arXiv:2110.12735v2 [astro-ph.GA] UPDATED)","link":"http://arxiv.org/abs/2110.12735","description":"<p>Astronomers have typically set out to solve supervised machine learning\nproblems by creating their own representations from scratch. We show that deep\nlearning models trained to answer every Galaxy Zoo DECaLS question learn\nmeaningful semantic representations of galaxies that are useful for new tasks\non which the models were never trained. We exploit these representations to\noutperform several recent approaches at practical tasks crucial for\ninvestigating large galaxy samples. The first task is identifying galaxies of\nsimilar morphology to a query galaxy. Given a single galaxy assigned a free\ntext tag by humans (e.g. \"#diffuse\"), we can find galaxies matching that tag\nfor most tags. The second task is identifying the most interesting anomalies to\na particular researcher. Our approach is 100% accurate at identifying the most\ninteresting 100 anomalies (as judged by Galaxy Zoo 2 volunteers). The third\ntask is adapting a model to solve a new task using only a small number of\nnewly-labelled galaxies. Models fine-tuned from our representation are better\nable to identify ring galaxies than models fine-tuned from terrestrial images\n(ImageNet) or trained from scratch. We solve each task with very few new\nlabels; either one (for the similarity search) or several hundred (for anomaly\ndetection or fine-tuning). This challenges the longstanding view that deep\nsupervised methods require new large labelled datasets for practical use in\nastronomy. To help the community benefit from our pretrained models, we release\nour fine-tuning code Zoobot. Zoobot is accessible to researchers with no prior\nexperience in deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Walmsley_M/0/1/0/all/0/1\">Mike Walmsley</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Scaife_A/0/1/0/all/0/1\">Anna M. M. Scaife</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lintott_C/0/1/0/all/0/1\">Chris Lintott</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lochner_M/0/1/0/all/0/1\">Michelle Lochner</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Etsebeth_V/0/1/0/all/0/1\">Verlon Etsebeth</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Geron_T/0/1/0/all/0/1\">Tobias G&#xe9;ron</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dickinson_H/0/1/0/all/0/1\">Hugh Dickinson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Fortson_L/0/1/0/all/0/1\">Lucy Fortson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kruk_S/0/1/0/all/0/1\">Sandor Kruk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Masters_K/0/1/0/all/0/1\">Karen L. Masters</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Mantha_K/0/1/0/all/0/1\">Kameswara Bharadwaj Mantha</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Simmons_B/0/1/0/all/0/1\">Brooke D. Simmons</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Pruned Structure and Weights Simultaneously from Scratch: an Attention based Approach. (arXiv:2111.02399v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.02399","description":"<p>As a deep learning model typically contains millions of trainable weights,\nthere has been a growing demand for a more efficient network structure with\nreduced storage space and improved run-time efficiency. Pruning is one of the\nmost popular network compression techniques. In this paper, we propose a novel\nunstructured pruning pipeline, Attention-based Simultaneous sparse structure\nand Weight Learning (ASWL). Unlike traditional channel-wise or weight-wise\nattention mechanism, ASWL proposed an efficient algorithm to calculate the\npruning ratio through layer-wise attention for each layer, and both weights for\nthe dense network and the sparse network are tracked so that the pruned\nstructure is simultaneously learned from randomly initialized weights. Our\nexperiments on MNIST, Cifar10, and ImageNet show that ASWL achieves superior\npruning results in terms of accuracy, pruning ratio and operating efficiency\nwhen compared with state-of-the-art network pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qisheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weisong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HEAT: Holistic Edge Attention Transformer for Structured Reconstruction. (arXiv:2111.15143v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15143","description":"<p>This paper presents a novel attention-based neural network for structured\nreconstruction, which takes a 2D raster image as an input and reconstructs a\nplanar graph depicting an underlying geometric structure. The approach detects\ncorners and classifies edge candidates between corners in an end-to-end manner.\nOur contribution is a holistic edge classification architecture, which 1)\ninitializes the feature of an edge candidate by a trigonometric positional\nencoding of its end-points; 2) fuses image feature to each edge candidate by\ndeformable attention; 3) employs two weight-sharing Transformer decoders to\nlearn holistic structural patterns over the graph edge candidates; and 4) is\ntrained with a masked learning strategy. The corner detector is a variant of\nthe edge classification architecture, adapted to operate on pixels as corner\ncandidates. We conduct experiments on two structured reconstruction tasks:\noutdoor building architecture and indoor floorplan planar graph reconstruction.\nExtensive qualitative and quantitative evaluations demonstrate the superiority\nof our approach over the state of the art. Code and pre-trained models are\navailable at https://heat-structured-reconstruction.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiacheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1\">Yasutaka Furukawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides. (arXiv:2112.02222v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.02222","description":"<p>Objectives: To develop and validate a deep learning (DL)-based primary tumor\nbiopsy signature for predicting axillary lymph node (ALN) metastasis\npreoperatively in early breast cancer (EBC) patients with clinically negative\nALN.\n</p>\n<p>Methods: A total of 1,058 EBC patients with pathologically confirmed ALN\nstatus were enrolled from May 2010 to August 2020. A DL core-needle biopsy\n(DL-CNB) model was built on the attention-based multiple instance-learning\n(AMIL) framework to predict ALN status utilizing the DL features, which were\nextracted from the cancer areas of digitized whole-slide images (WSIs) of\nbreast CNB specimens annotated by two pathologists. Accuracy, sensitivity,\nspecificity, receiver operating characteristic (ROC) curves, and areas under\nthe ROC curve (AUCs) were analyzed to evaluate our model.\n</p>\n<p>Results: The best-performing DL-CNB model with VGG16_BN as the feature\nextractor achieved an AUC of 0.816 (95% confidence interval (CI): 0.758, 0.865)\nin predicting positive ALN metastasis in the independent test cohort.\nFurthermore, our model incorporating the clinical data, which was called\nDL-CNB+C, yielded the best accuracy of 0.831 (95%CI: 0.775, 0.878), especially\nfor patients younger than 50 years (AUC: 0.918, 95%CI: 0.825, 0.971). The\ninterpretation of DL-CNB model showed that the top signatures most predictive\nof ALN metastasis were characterized by the nucleus features including density\n($p$ = 0.015), circumference ($p$ = 0.009), circularity ($p$ = 0.010), and\norientation ($p$ = 0.012).\n</p>\n<p>Conclusion: Our study provides a novel DL-based biomarker on primary tumor\nCNB slides to predict the metastatic status of ALN preoperatively for patients\nwith EBC. The codes and dataset are available at\nhttps://github.com/bupt-ai-cz/BALNMP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_C/0/1/0/all/0/1\">Chuang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_W/0/1/0/all/0/1\">Wenqi Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1\">Hongchuan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongyue Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_M/0/1/0/all/0/1\">Mulan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Hierarchical Refinement and Augmentation for Unsupervised Learning of Depth and Pose from Monocular Video. (arXiv:2112.03045v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03045","description":"<p>Depth and ego-motion estimations are essential for the localization and\nnavigation of autonomous robots and autonomous driving. Recent studies make it\npossible to learn the per-pixel depth and ego-motion from the unlabeled\nmonocular video. A novel unsupervised training framework is proposed with 3D\nhierarchical refinement and augmentation using explicit 3D geometry. In this\nframework, the depth and pose estimations are hierarchically and mutually\ncoupled to refine the estimated pose layer by layer. The intermediate view\nimage is proposed and synthesized by warping the pixels in an image with the\nestimated depth and coarse pose. Then, the residual pose transformation can be\nestimated from the new view image and the image of the adjacent frame to refine\nthe coarse pose. The iterative refinement is implemented in a differentiable\nmanner in this paper, making the whole framework optimized uniformly.\nMeanwhile, a new image augmentation method is proposed for the pose estimation\nby synthesizing a new view image, which creatively augments the pose in 3D\nspace but gets a new augmented 2D image. The experiments on KITTI demonstrate\nthat our depth estimation achieves state-of-the-art performance and even\nsurpasses recent approaches that utilize other auxiliary tasks. Our visual\nodometry outperforms all recent unsupervised monocular learning-based methods\nand achieves competitive performance to the geometry-based method, ORB-SLAM2\nwith back-end optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jiquan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shijie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothness and effective regularizations in learned embeddings for shape matching. (arXiv:2112.07289v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07289","description":"<p>Many innovative applications require establishing correspondences among 3D\ngeometric objects. However, the countless possible deformations of smooth\nsurfaces make shape matching a challenging task. Finding an embedding to\nrepresent the different shapes in high-dimensional space where the matching is\neasier to solve is a well-trodden path that has given many outstanding\nsolutions. Recently, a new trend has shown advantages in learning such\nrepresentations. This novel idea motivated us to investigate which properties\ndifferentiate these data-driven embeddings and which ones promote\nstate-of-the-art results. In this study, we analyze, for the first time,\nproperties that arise in data-driven learned embedding and their relation to\nthe shape-matching task. Our discoveries highlight the close link between\nmatching and smoothness, which naturally emerge from training. Also, we\ndemonstrate the relation between the orthogonality of the embedding and the\nbijectivity of the correspondence. Our experiments show exciting results,\novercoming well-established alternatives and shedding a different light on\nrelevant contexts and properties for learned embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attaiki_S/0/1/0/all/0/1\">Souhaib Attaiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Greedy De-bias Learning. (arXiv:2112.10572v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.10572","description":"<p>Neural networks often make predictions relying on the spurious correlations\nfrom the datasets rather than the intrinsic properties of the task of interest,\nfacing sharp degradation on out-of-distribution (OOD) test data. Existing\nde-bias learning frameworks try to capture specific dataset bias by annotations\nbut they fail to handle complicated OOD scenarios. Others implicitly identify\nthe dataset bias by special design low capability biased models or losses, but\nthey degrade when the training and testing data are from the same distribution.\nIn this paper, we propose a General Greedy De-bias learning framework (GGD),\nwhich greedily trains the biased models and the base model. The base model is\nencouraged to focus on examples that are hard to solve with biased models, thus\nremaining robust against spurious correlations in the test stage. GGD largely\nimproves models' OOD generalization ability on various tasks, but sometimes\nover-estimates the bias level and degrades on the in-distribution test. We\nfurther re-analyze the ensemble process of GGD and introduce the Curriculum\nRegularization inspired by curriculum learning, which achieves a good trade-off\nbetween in-distribution and out-of-distribution performance. Extensive\nexperiments on image classification, adversarial question answering, and visual\nquestion answering demonstrate the effectiveness of our method. GGD can learn a\nmore robust base model under the settings of both task-specific biased models\nwith prior knowledge and self-ensemble biased model without prior knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04756","description":"<p>In this paper, we developed the solution of roadside LiDAR object detection\nusing a combination of two unsupervised learning algorithms. The 3D point\nclouds are firstly converted into spherical coordinates and filled into the\nelevation-azimuth matrix using a hash function. After that, the raw LiDAR data\nwere rearranged into new data structures to store the information of range,\nazimuth, and intensity. Then, the Dynamic Mode Decomposition method is applied\nto decompose the LiDAR data into low-rank backgrounds and sparse foregrounds\nbased on intensity channel pattern recognition. The Coarse Fine Triangle\nAlgorithm (CFTA) automatically finds the dividing value to separate the moving\ntargets from static background according to range information. After intensity\nand range background subtraction, the foreground moving objects will be\ndetected using a density-based detector and encoded into the state-space model\nfor tracking. The output of the proposed solution includes vehicle trajectories\nthat can enable many mobility and safety applications. The method was validated\nat both path and point levels and outperformed the state-of-the-art. In\ncontrast to the previous methods that process directly on the scattered and\ndiscrete point clouds, the dynamic classification method can establish the less\nsophisticated linear relationship of the 3D measurement data, which captures\nthe spatial-temporal structure that we often desire.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peter J. Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Density Estimation from Schlieren Images through Machine Learning. (arXiv:2201.05233v2 [physics.flu-dyn] UPDATED)","link":"http://arxiv.org/abs/2201.05233","description":"<p>This study proposes a radically alternate approach for extracting\nquantitative information from schlieren images. The method uses a scaled,\nderivative enhanced Gaussian process model to obtain true density estimates\nfrom two corresponding schlieren images with the knife-edge at horizontal and\nvertical orientations. We illustrate our approach on schlieren images taken\nfrom a wind tunnel sting model, a supersonic aircraft in flight, and a\nhigh-order numerical shock tube simulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Ubald_B/0/1/0/all/0/1\">Bryn Noel Ubald</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Seshadri_P/0/1/0/all/0/1\">Pranay Seshadri</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Duncan_A/0/1/0/all/0/1\">Andrew Duncan</a> (1 and 2) ((1) The Alan Turing Institute, (2) Imperial College London)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12170","description":"<p>Real-time estimation of actual object depth is an essential module for\nvarious autonomous system tasks such as 3D reconstruction, scene understanding\nand condition assessment. During the last decade of machine learning, extensive\ndeployment of deep learning methods to computer vision tasks has yielded\napproaches that succeed in achieving realistic depth synthesis out of a simple\nRGB modality. Most of these models are based on paired RGB-depth data and/or\nthe availability of video sequences and stereo images. The lack of sequences,\nstereo data and RGB-depth pairs makes depth estimation a fully unsupervised\nsingle-image transfer problem that has barely been explored so far. This study\nbuilds on recent advances in the field of generative neural networks in order\nto establish fully unsupervised single-shot depth estimation. Two generators\nfor RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously\noptimized using the Wasserstein-1 distance, a novel perceptual reconstruction\nterm and hand-crafted image filters. We comprehensively evaluate the models\nusing industrial surface depth data as well as the Texas 3D Face Recognition\nDatabase, the CelebAMask-HQ database of human portraits and the SURREAL dataset\nthat records body depth. For each evaluation dataset the proposed method shows\na significant increase in depth accuracy compared to state-of-the-art\nsingle-image transfer methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_M/0/1/0/all/0/1\">Matthias Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistically Robust Learning: Balancing Average- and Worst-case Performance. (arXiv:2202.01136v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01136","description":"<p>Many of the successes of machine learning are based on minimizing an averaged\nloss function. However, it is well-known that this paradigm suffers from\nrobustness issues that hinder its applicability in safety-critical domains.\nThese issues are often addressed by training against worst-case perturbations\nof data, a technique known as adversarial training. Although empirically\neffective, adversarial training can be overly conservative, leading to\nunfavorable trade-offs between nominal performance and robustness. To this end,\nin this paper we propose a framework called probabilistic robustness that\nbridges the gap between the accurate, yet brittle average case and the robust,\nyet conservative worst case by enforcing robustness to most rather than to all\nperturbations. From a theoretical point of view, this framework overcomes the\ntrade-offs between the performance and the sample-complexity of worst-case and\naverage-case learning. From a practical point of view, we propose a novel\nalgorithm based on risk-aware optimization that effectively balances average-\nand worst-case performance at a considerably lower computational cost relative\nto adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate\nthe advantages of this framework on the spectrum from average- to worst-case\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chamon_L/0/1/0/all/0/1\">Luiz F. O. Chamon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Condensation with Contrastive Signals. (arXiv:2202.02916v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02916","description":"<p>Recent studies have demonstrated that gradient matching-based dataset\nsynthesis, or dataset condensation (DC), methods can achieve state-of-the-art\nperformance when applied to data-efficient learning tasks. However, in this\nstudy, we prove that the existing DC methods can perform worse than the random\nselection method when task-irrelevant information forms a significant part of\nthe training dataset. We attribute this to the lack of participation of the\ncontrastive signals between the classes resulting from the class-wise gradient\nmatching strategy. To address this problem, we propose Dataset Condensation\nwith Contrastive signals (DCC) by modifying the loss function to enable the DC\nmethods to effectively capture the differences between classes. In addition, we\nanalyze the new loss function in terms of training dynamics by tracking the\nkernel velocity. Furthermore, we introduce a bi-level warm-up strategy to\nstabilize the optimization. Our experimental results indicate that while the\nexisting methods are ineffective for fine-grained image classification tasks,\nthe proposed method can successfully generate informative synthetic datasets\nfor the same tasks. Moreover, we demonstrate that the proposed method\noutperforms the baselines even on benchmark datasets such as SVHN, CIFAR-10,\nand CIFAR-100. Finally, we demonstrate the high applicability of the proposed\nmethod by applying it to continual learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Saehyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sangwon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOST-Net: A Memory Oriented Style Transfer Network for Face Sketch Synthesis. (arXiv:2202.03596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03596","description":"<p>Face sketch synthesis has been widely used in multi-media entertainment and\nlaw enforcement. Despite the recent developments in deep neural networks,\naccurate and realistic face sketch synthesis is still a challenging task due to\nthe diversity and complexity of human faces. Current image-to-image\ntranslation-based face sketch synthesis frequently encounters over-fitting\nproblems when it comes to small-scale datasets. To tackle this problem, we\npresent an end-to-end Memory Oriented Style Transfer Network (MOST-Net) for\nface sketch synthesis which can produce high-fidelity sketches with limited\ndata. Specifically, an external self-supervised dynamic memory module is\nintroduced to capture the domain alignment knowledge in the long term. In this\nway, our proposed model could obtain the domain-transfer ability by\nestablishing the durable relationship between faces and corresponding sketches\non the feature level. Furthermore, we design a novel Memory Refinement Loss (MR\nLoss) for feature alignment in the memory module, which enhances the accuracy\nof memory slots in an unsupervised manner. Extensive experiments on the CUFS\nand the CUFSF datasets show that our MOST-Net achieves state-of-the-art\nperformance, especially in terms of the Structural Similarity Index(SSIM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1\">Fan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do Vision Transformers Work?. (arXiv:2202.06709v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06709","description":"<p>The success of multi-head self-attentions (MSAs) for computer vision is now\nindisputable. However, little is known about how MSAs work. We present\nfundamental explanations to help better understand the nature of MSAs. In\nparticular, we demonstrate the following properties of MSAs and Vision\nTransformers (ViTs): (1) MSAs improve not only accuracy but also generalization\nby flattening the loss landscapes. Such improvement is primarily attributable\nto their data specificity, not long-range dependency. On the other hand, ViTs\nsuffer from non-convex losses. Large datasets and loss landscape smoothing\nmethods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.\nFor example, MSAs are low-pass filters, but Convs are high-pass filters.\nTherefore, MSAs and Convs are complementary; (3) Multi-stage neural networks\nbehave like a series connection of small individual models. In addition, MSAs\nat the end of a stage play a key role in prediction. Based on these insights,\nwe propose AlterNet, a model in which Conv blocks at the end of a stage are\nreplaced with MSA blocks. AlterNet outperforms CNNs not only in large data\nregimes but also in small data regimes. The code is available at\nhttps://github.com/xxxnell/how-do-vits-work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07662","description":"<p>In object detection, false negatives arise when a detector fails to detect a\ntarget object. To understand why object detectors produce false negatives, we\nidentify five 'false negative mechanisms', where each mechanism describes how a\nspecific component inside the detector architecture failed. Focusing on\ntwo-stage and one-stage anchor-box object detector architectures, we introduce\na framework for quantifying these false negative mechanisms. Using this\nframework, we investigate why Faster R-CNN and RetinaNet fail to detect objects\nin benchmark vision datasets and robotics datasets. We show that a detector's\nfalse negative mechanisms differ significantly between computer vision\nbenchmark datasets and robotics deployment scenarios. This has implications for\nthe translation of object detectors developed for benchmark datasets to\nrobotics applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">Dimity Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1\">Peyman Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_M/0/1/0/all/0/1\">Mark Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildie_M/0/1/0/all/0/1\">Matt Wildie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurdak_R/0/1/0/all/0/1\">Raja Jurdak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iPLAN: Interactive and Procedural Layout Planning. (arXiv:2203.14412v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14412","description":"<p>Layout design is ubiquitous in many applications, e.g. architecture/urban\nplanning, etc, which involves a lengthy iterative design process. Recently,\ndeep learning has been leveraged to automatically generate layouts via image\ngeneration, showing a huge potential to free designers from laborious routines.\nWhile automatic generation can greatly boost productivity, designer input is\nundoubtedly crucial. An ideal AI-aided design tool should automate repetitive\nroutines, and meanwhile accept human guidance and provide smart/proactive\nsuggestions. However, the capability of involving humans into the loop has been\nlargely ignored in existing methods which are mostly end-to-end approaches. To\nthis end, we propose a new human-in-the-loop generative model, iPLAN, which is\ncapable of automatically generating layouts, but also interacting with\ndesigners throughout the whole procedure, enabling humans and AI to co-evolve a\nsketchy idea gradually into the final design. iPLAN is evaluated on diverse\ndatasets and compared with existing methods. The results show that iPLAN has\nhigh fidelity in producing similar layouts to those from human designers, great\nflexibility in accepting designer inputs and providing design suggestions\naccordingly, and strong generalizability when facing unseen design tasks and\nlimited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Feixiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Transformer Tracker for Object Tracking. (arXiv:2203.15175v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15175","description":"<p>As an important area in computer vision, object tracking has formed two\nseparate communities that respectively study Single Object Tracking (SOT) and\nMultiple Object Tracking (MOT). However, current methods in one tracking\nscenario are not easily adapted to the other due to the divergent training\ndatasets and tracking objects of both tasks. Although UniTrack\n\\cite{wang2021different} demonstrates that a shared appearance model with\nmultiple heads can be used to tackle individual tracking tasks, it fails to\nexploit the large-scale tracking datasets for training and performs poorly on\nsingle object tracking. In this work, we present the Unified Transformer\nTracker (UTT) to address tracking problems in different scenarios with one\nparadigm. A track transformer is developed in our UTT to track the target in\nboth SOT and MOT. The correlation between the target and tracking frame\nfeatures is exploited to localize the target. We demonstrate that both SOT and\nMOT tasks can be solved within this framework. The model can be simultaneously\nend-to-end trained by alternatively optimizing the SOT and MOT objectives on\nthe datasets of individual tasks. Extensive experiments are conducted on\nseveral benchmarks with a unified model trained on SOT and MOT datasets. Code\nwill be available at https://github.com/Flowerfan/Trackron.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yilei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Mixture of Experts. (arXiv:2204.09636v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09636","description":"<p>Mixture of Experts (MoE) is able to scale up vision transformers effectively.\nHowever, it requires prohibiting computation resources to train a large MoE\ntransformer. In this paper, we propose Residual Mixture of Experts (RMoE), an\nefficient training pipeline for MoE vision transformers on downstream tasks,\nsuch as segmentation and detection. RMoE achieves comparable results with the\nupper-bound MoE training, while only introducing minor additional training cost\nthan the lower-bound non-MoE training pipelines. The efficiency is supported by\nour key observation: the weights of an MoE transformer can be factored into an\ninput-independent core and an input-dependent residual. Compared with the\nweight core, the weight residual can be efficiently trained with much less\ncomputation resource, e.g., finetuning on the downstream data. We show that,\ncompared with the current MoE training pipeline, we get comparable results\nwhile saving over 30% training cost. When compared with state-of-the-art non-\nMoE transformers, such as Swin-T / CvT-13 / Swin-L, we get +1.1 / 0.9 / 1.0\nmIoU gain on ADE20K segmentation and +1.4 / 1.6 / 0.6 AP gain on MS-COCO object\ndetection task with less than 3% additional training cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lemeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Iterative Labeling Method for Annotating Fisheries Imagery. (arXiv:2204.12934v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.12934","description":"<p>In this paper, we present a methodology for fisheries-related data that\nallows us to converge on a labeled image dataset by iterating over the dataset\nwith multiple training and production loops that can exploit crowdsourcing\ninterfaces. We present our algorithm and its results on two separate sets of\nimage data collected using the Seabed autonomous underwater vehicle. The first\ndataset comprises of 2,026 completely unlabeled images, while the second\nconsists of 21,968 images that were point annotated by experts. Our results\nindicate that training with a small subset and iterating on that to build a\nlarger set of labeled data allows us to converge to a fully annotated dataset\nwith a small number of iterations. Even in the case of a dataset labeled by\nexperts, a single iteration of the methodology improves the labels by\ndiscovering additional complicated examples of labels associated with fish that\noverlap, are very small, or obscured by the contrast limitations associated\nwith underwater imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaveti_P/0/1/0/all/0/1\">Pushyami Kaveti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Hanumant Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_A/0/1/0/all/0/1\">Abigail Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fruh_E/0/1/0/all/0/1\">Erica Fruh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarke_M/0/1/0/all/0/1\">M. Elizabeth Clarke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning. (arXiv:2205.00272v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00272","description":"<p>Visual grounding is a task to locate the target indicated by a natural\nlanguage expression. Existing methods extend the generic object detection\nframework to this problem. They base the visual grounding on the features from\npre-generated proposals or anchors, and fuse these features with the text\nembeddings to locate the target mentioned by the text. However, modeling the\nvisual features from these predefined locations may fail to fully exploit the\nvisual context and attribute information in the text query, which limits their\nperformance. In this paper, we propose a transformer-based framework for\naccurate visual grounding by establishing text-conditioned discriminative\nfeatures and performing multi-stage cross-modal reasoning. Specifically, we\ndevelop a visual-linguistic verification module to focus the visual features on\nregions relevant to the textual descriptions while suppressing the unrelated\nareas. A language-guided feature encoder is also devised to aggregate the\nvisual contexts of the target object to improve the object's distinctiveness.\nTo retrieve the target from the encoded visual features, we further propose a\nmulti-stage cross-modal decoder to iteratively speculate on the correlations\nbetween the image and text for accurate target localization. Extensive\nexperiments on five widely used datasets validate the efficacy of our proposed\ncomponents and demonstrate state-of-the-art performance. Our code is public at\nhttps://github.com/yangli18/VLTVG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunfeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning. (arXiv:2205.04363v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04363","description":"<p>Significant progress has been made on visual captioning, largely relying on\npre-trained features and later fixed object detectors that serve as rich inputs\nto auto-regressive models. A key limitation of such methods, however, is that\nthe output of the model is conditioned only on the object detector's outputs.\nThe assumption that such outputs can represent all necessary information is\nunrealistic, especially when the detector is transferred across datasets. In\nthis work, we reason about the graphical model induced by this assumption, and\npropose to add an auxiliary input to represent missing information such as\nobject relationships. We specifically propose to mine attributes and\nrelationships from the Visual Genome dataset and condition the captioning model\non them. Crucially, we propose (and show to be important) the use of a\nmulti-modal pre-trained model (CLIP) to retrieve such contextual descriptions.\nFurther, object detector models are frozen and do not have sufficient richness\nto allow the captioning model to properly ground them. As a result, we propose\nto condition both the detector and description outputs on the image, and show\nqualitatively and quantitatively that this can improve grounding. We validate\nour method on image captioning, perform thorough analyses of each component and\nimportance of the pre-trained multi-modal model, and demonstrate significant\nimprovements over the current state of the art, specifically +7.5% in CIDEr and\n+1.3% in BLEU-4 metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">Chia-Wen Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Video Deblurring via Lightweight Motion Compensation. (arXiv:2205.12634v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12634","description":"<p>While motion compensation greatly improves video deblurring quality,\nseparately performing motion compensation and video deblurring demands huge\ncomputational overhead. This paper proposes a real-time video deblurring\nframework consisting of a lightweight multi-task unit that supports both video\ndeblurring and motion compensation in an efficient way. The multi-task unit is\nspecifically designed to handle large portions of the two tasks using a single\nshared network, and consists of a multi-task detail network and simple networks\nfor deblurring and motion compensation. The multi-task unit minimizes the cost\nof incorporating motion compensation into video deblurring and enables\nreal-time deblurring. Moreover, by stacking multiple multi-task units, our\nframework provides flexible control between the cost and deblurring quality. We\nexperimentally validate the state-of-the-art deblurring quality of our\napproach, which runs at a much faster speed compared to previous methods, and\nshow practical real-time performance (30.99dB@30fps measured in the DVD\ndataset).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Hyeongseok Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Interpretability via Polynomials. (arXiv:2205.14108v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14108","description":"<p>Generalized Additive Models (GAMs) have quickly become the leading choice for\nfully-interpretable machine learning. However, unlike uninterpretable methods\nsuch as DNNs, they lack expressive power and easy scalability, and are hence\nnot a feasible alternative for real-world tasks. We present a new class of GAMs\nthat use tensor rank decompositions of polynomials to learn powerful, {\\em\nfully-interpretable} models. Our approach, titled Scalable Polynomial Additive\nModels (SPAM) is effortlessly scalable and models {\\em all} higher-order\nfeature interactions without a combinatorial parameter explosion. SPAM\noutperforms all current interpretable approaches, and matches DNN/XGBoost\nperformance on a series of real-world benchmarks with up to hundreds of\nthousands of features. We demonstrate by human subject evaluations that SPAMs\nare demonstrably more interpretable in practice, and are hence an effortless\nreplacement for DNNs for creating interpretable and high-performance systems\nsuitable for large-scale machine learning. Source code is available at\nhttps://github.com/facebookresearch/nbm-spam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhimanyu Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Basis Models for Interpretability. (arXiv:2205.14120v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14120","description":"<p>Due to the widespread use of complex machine learning models in real-world\napplications, it is becoming critical to explain model predictions. However,\nthese models are typically black-box deep neural networks, explained post-hoc\nvia methods with known faithfulness limitations. Generalized Additive Models\n(GAMs) are an inherently interpretable class of models that address this\nlimitation by learning a non-linear shape function for each feature separately,\nfollowed by a linear model on top. However, these models are typically\ndifficult to train, require numerous parameters, and are difficult to scale.\n</p>\n<p>We propose an entirely new subfamily of GAMs that utilizes basis\ndecomposition of shape functions. A small number of basis functions are shared\namong all features, and are learned jointly for a given task, thus making our\nmodel scale much better to large-scale data with high-dimensional features,\nespecially when features are sparse. We propose an architecture denoted as the\nNeural Basis Model (NBM) which uses a single neural network to learn these\nbases. On a variety of tabular and image datasets, we demonstrate that for\ninterpretable machine learning, NBMs are the state-of-the-art in accuracy,\nmodel size, and, throughput and can easily model all higher-order feature\ninteractions.\n</p>\n<p>Source code is available at https://github.com/facebookresearch/nbm-spam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhimanyu Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point RCNN: An Angle-Free Framework for Rotated Object Detection. (arXiv:2205.14328v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14328","description":"<p>Rotated object detection in aerial images is still challenging due to\narbitrary orientations, large scale and aspect ratio variations, and extreme\ndensity of objects. Existing state-of-the-art rotated object detection methods\nmainly rely on angle-based detectors. However, angle regression can easily\nsuffer from the long-standing boundary problem. To tackle this problem, we\npropose a purely angle-free framework for rotated object detection, called\nPoint RCNN, which mainly consists of PointRPN and PointReg. In particular,\nPointRPN generates accurate rotated RoIs (RRoIs) by converting the learned\nrepresentative points with a coarse-to-fine manner, which is motivated by\nRepPoints. Based on the learned RRoIs, PointReg performs corner points\nrefinement for more accurate detection. In addition, aerial images are often\nseverely unbalanced in categories, and existing methods almost ignore this\nissue. In this paper, we also experimentally verify that re-sampling the images\nof the rare categories will stabilize training and further improve the\ndetection performance. Experiments demonstrate that our Point RCNN achieves the\nnew state-of-the-art detection performance on commonly used aerial datasets,\nincluding DOTA-v1.0, DOTA-v1.5, and HRSC2016.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chaohui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Point-Based Radiance Fields for Efficient View Synthesis. (arXiv:2205.14330v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14330","description":"<p>We propose a differentiable rendering algorithm for efficient novel view\nsynthesis. By departing from volume-based representations in favor of a learned\npoint representation, we improve on existing methods more than an order of\nmagnitude in memory and runtime, both in training and inference. The method\nbegins with a uniformly-sampled random point cloud and learns per-point\nposition and view-dependent appearance, using a differentiable splat-based\nrenderer to evolve the model to match a set of input images. Our method is up\nto 300x faster than NeRF in both training and inference, with only a marginal\nsacrifice in quality, while using less than 10~MB of memory for a static scene.\nFor dynamic scenes, our method trains two orders of magnitude faster than\nSTNeRF and renders at near interactive rate, while maintaining high image\nquality and temporal coherence even without imposing any temporal-coherency\nregularizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1\">Szymon Rusinkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01992","description":"<p>Detection of object anomalies is crucial in industrial processes, but\nunsupervised anomaly detection and localization is particularly important due\nto the difficulty of obtaining a large number of defective samples and the\nunpredictable types of anomalies in real life. Among the existing unsupervised\nanomaly detection and localization methods, the NF-based scheme has achieved\nbetter results. However, the two subnets (complex functions) $s_{i}(u_{i})$ and\n$t_{i}(u_{i})$ in NF are usually multilayer perceptrons, which need to squeeze\nthe input visual features from 2D flattening to 1D, destroying the spatial\nlocation relationship in the feature map and losing the spatial structure\ninformation. In order to retain and effectively extract spatial structure\ninformation, we design in this study a complex function model with alternating\nCBAM embedded in a stacked $3\\times3$ full convolution, which is able to retain\nand effectively extract spatial structure information in the normalized flow\nmodel. Extensive experimental results on the MVTec AD dataset show that\nCAINNFlow achieves advanced levels of accuracy and inference efficiency based\non CNN and Transformer backbone networks as feature extractors, and CAINNFlow\nachieves a pixel-level AUC of $98.64\\%$ for anomaly detection in MVTec AD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ruiqing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mengyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dongyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingrong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qianjin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Linghan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning. (arXiv:2206.02261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02261","description":"<p>This paper combines deep learning techniques for species detection, 3D model\nfitting, and metric learning in one pipeline to perform individual animal\nidentification from photographs by exploiting unique coat patterns. This is the\nfirst work to attempt this and, compared to traditional 2D bounding box or\nsegmentation based CNN identification pipelines, the approach provides\neffective and explicit view-point normalisation and allows for a straight\nforward visualisation of the learned biometric population space. Note that due\nto the use of metric learning the pipeline is also readily applicable to open\nset and zero shot re-identification scenarios. We apply the proposed approach\nto individual Grevy's zebra (Equus grevyi) identification and show in a small\nstudy on the SMALST dataset that the use of 3D model fitting can indeed benefit\nperformance. In particular, back-projected textures from 3D fitted models\nimprove identification accuracy from 48.0% to 56.8% compared to 2D bounding box\napproaches for the dataset. Whilst the study is far too small accurately to\nestimate the full performance potential achievable in larger-scale real-world\napplication settings and in comparisons against polished tools, our work lays\nthe conceptual and practical foundations for a next step in animal biometrics\ntowards deep metric learning driven, fully 3D-aware animal identification in\nopen population settings. We publish network weights and relevant facilitating\nsource code with this paper for full reproducibility and as inspiration for\nfurther research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stennett_M/0/1/0/all/0/1\">Maria Stennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubenstein_D/0/1/0/all/0/1\">Daniel I. Rubenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data. (arXiv:2206.02353v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.02353","description":"<p>Recently, Self-Supervised Representation Learning (SSRL) has attracted much\nattention in the field of computer vision, speech, natural language processing\n(NLP), and recently, with other types of modalities, including time series from\nsensors. The popularity of self-supervised learning is driven by the fact that\ntraditional models typically require a huge amount of well-annotated data for\ntraining. Acquiring annotated data can be a difficult and costly process.\nSelf-supervised methods have been introduced to improve the efficiency of\ntraining data through discriminative pre-training of models using supervisory\nsignals that have been freely obtained from the raw data. Unlike existing\nreviews of SSRL that have pre-dominately focused upon methods in the fields of\nCV or NLP for a single modality, we aim to provide the first comprehensive\nreview of multimodal self-supervised learning methods for temporal data. To\nthis end, we 1) provide a comprehensive categorization of existing SSRL\nmethods, 2) introduce a generic pipeline by defining the key components of a\nSSRL framework, 3) compare existing models in terms of their objective\nfunction, network architecture and potential applications, and 4) review\nexisting multimodal techniques in each category and various modalities.\nFinally, we present existing weaknesses and future opportunities. We believe\nour work develops a perspective on the requirements of SSRL in domains that\nutilise multimodal and/or temporal data\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deldari_S/0/1/0/all/0/1\">Shohreh Deldari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Aaqib Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiayuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1\">Daniel V. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Techniques for Visual Counting. (arXiv:2206.03033v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03033","description":"<p>In this dissertation, we investigated and enhanced Deep Learning (DL)\ntechniques for counting objects, like pedestrians, cells or vehicles, in still\nimages or video frames. In particular, we tackled the challenge related to the\nlack of data needed for training current DL-based solutions. Given that the\nbudget for labeling is limited, data scarcity still represents an open problem\nthat prevents the scalability of existing solutions based on the supervised\nlearning of neural networks and that is responsible for a significant drop in\nperformance at inference time when new scenarios are presented to these\nalgorithms. We introduced solutions addressing this issue from several\ncomplementary sides, collecting datasets gathered from virtual environments\nautomatically labeled, proposing Domain Adaptation strategies aiming at\nmitigating the domain gap existing between the training and test data\ndistributions, and presenting a counting strategy in a weakly labeled data\nscenario, i.e., in the presence of non-negligible disagreement between multiple\nannotators. Moreover, we tackled the non-trivial engineering challenges coming\nout of the adoption of Convolutional Neural Network-based techniques in\nenvironments with limited power resources, introducing solutions for counting\nvehicles and pedestrians directly onboard embedded vision systems, i.e.,\ndevices equipped with constrained computational capabilities that can capture\nimages and elaborate them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciampi_L/0/1/0/all/0/1\">Luca Ciampi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Robust Non-Rigid Registration Using Accelerated Majorization-Minimization. (arXiv:2206.03410v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03410","description":"<p>Non-rigid registration, which deforms a source shape in a non-rigid way to\nalign with a target shape, is a classical problem in computer vision. Such\nproblems can be challenging because of imperfect data (noise, outliers and\npartial overlap) and high degrees of freedom. Existing methods typically adopt\nthe $\\ell_{p}$ type robust norm to measure the alignment error and regularize\nthe smoothness of deformation, and use a proximal algorithm to solve the\nresulting non-smooth optimization problem. However, the slow convergence of\nsuch algorithms limits their wide applications. In this paper, we propose a\nformulation for robust non-rigid registration based on a globally smooth robust\nnorm for alignment and regularization, which can effectively handle outliers\nand partial overlaps. The problem is solved using the majorization-minimization\nalgorithm, which reduces each iteration to a convex quadratic problem with a\nclosed-form solution. We further apply Anderson acceleration to speed up the\nconvergence of the solver, enabling the solver to run efficiently on devices\nwith limited compute capability. Extensive experiments demonstrate the\neffectiveness of our method for non-rigid alignment between two shapes with\noutliers and partial overlaps, with quantitative evaluation showing that it\noutperforms state-of-the-art methods in terms of registration accuracy and\ncomputational speed. The source code is available at\nhttps://github.com/yaoyx689/AMM_NRR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuxin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}