{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A gentle introduction to Quantum Natural Language Processing. (arXiv:2202.11766v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11766","description":"<p>The main goal of this master's thesis is to introduce Quantum Natural\nLanguage Processing (QNLP) in a way understandable by both the NLP engineer and\nthe quantum computing practitioner. QNLP is a recent application of quantum\ncomputing that aims at representing sentences' meaning as vectors encoded into\nquantum computers. To achieve this, the distributional meaning of words is\nextended by the compositional meaning of sentences (DisCoCat model) : the\nvectors representing words' meanings are composed through the syntactic\nstructure of the sentence. This is done using an algorithm based on tensor\nproducts. We see that this algorithm is inefficient on classical computers but\nscales well using quantum circuits. After exposing the practical details of its\nimplementation, we go through three use-cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shervin Le Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santana_S/0/1/0/all/0/1\">Senaida Hern&#xe1;ndez Santana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1\">Giannicola Scarpa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Unstructured Text to Causal Knowledge Graphs: A Transformer-Based Approach. (arXiv:2202.11768v1 [cs.AI])","link":"http://arxiv.org/abs/2202.11768","description":"<p>Qualitative causal relationships compactly express the direction, dependency,\ntemporal constraints, and monotonicity constraints of discrete or continuous\ninteractions in the world. In everyday or academic language, we may express\ninteractions between quantities (e.g., sleep decreases stress), between\ndiscrete events or entities (e.g., a protein inhibits another protein's\ntranscription), or between intentional or functional factors (e.g., hospital\npatients pray to relieve their pain). Extracting and representing these diverse\ncausal relations are critical for cognitive systems that operate in domains\nspanning from scientific discovery to social science. This paper presents a\ntransformer-based NLP architecture that jointly extracts knowledge graphs\nincluding (1) variables or factors described in language, (2) qualitative\ncausal relationships over these variables, (3) qualifiers and magnitudes that\nconstrain these causal relationships, and (4) word senses to localize each\nextracted node within a large ontology. We do not claim that our\ntransformer-based architecture is itself a cognitive system; however, we\nprovide evidence of its accurate knowledge graph extraction in real-world\ndomains and the practicality of its resulting knowledge graphs for cognitive\nsystems that perform graph-based reasoning. We demonstrate this approach and\ninclude promising results in two use cases, processing textual inputs from\nacademic publications, news articles, and social media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_S/0/1/0/all/0/1\">Scott Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_I/0/1/0/all/0/1\">Ian Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarathy_V/0/1/0/all/0/1\">Vasanth Sarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmer_Galunder_S/0/1/0/all/0/1\">Sonja Schmer-Galunder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using natural language prompts for machine translation. (arXiv:2202.11822v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11822","description":"<p>We explore the use of natural language prompts for controlling various\naspects of the outputs generated by machine translation models. We demonstrate\nthat natural language prompts allow us to influence properties like formality\nor specific dialect of the output. We show that using language names to control\nthe output language of multilingual translation models enables positive\ntransfer for unseen language pairs. This unlocks the ability to translate into\nlanguages not seen during fine-tuning by using their English names. We\ninvestigate how scale, number of pre-training steps, number of languages in\nfine-tuning, and language similarity affect this phenomenon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First is Better Than Last for Training Data Influence. (arXiv:2202.11844v1 [cs.LG])","link":"http://arxiv.org/abs/2202.11844","description":"<p>The ability to identify influential training examples enables us to debug\ntraining data and explain model behavior. Existing techniques are based on the\nflow of influence through the model parameters. For large models in NLP\napplications, it is often computationally infeasible to study this flow through\nall model parameters, therefore techniques usually pick the last layer of\nweights. Our first observation is that for classification problems, the last\nlayer is reductive and does not encode sufficient input level information.\nDeleting influential examples, according to this measure, typically does not\nchange the model's behavior much. We propose a technique called TracIn-WE that\nmodifies a method called TracIn to operate on the word embedding layer instead\nof the last layer. This could potentially have the opposite concern, that the\nword embedding layer does not encode sufficient high level information.\nHowever, we find that gradients (unlike embeddings) do not suffer from this,\npossibly because they chain through higher layers. We show that TracIn-WE\nsignificantly outperforms other data influence methods applied on the last\nlayer by 4-10 times on the case deletion evaluation on three language\nclassification tasks. In addition, TracIn-WE can produce scores not just at the\ntraining data level, but at the word training data level, a further aid in\ndebugging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chih-Kuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taly_A/0/1/0/all/0/1\">Ankur Taly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararajan_M/0/1/0/all/0/1\">Mukund Sundararajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAISE: Conversational Agent for Image Search and Editing. (arXiv:2202.11847v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11847","description":"<p>Demand for image editing has been increasing as users' desire for expression\nis also increasing. However, for most users, image editing tools are not easy\nto use since the tools require certain expertise in photo effects and have\ncomplex interfaces. Hence, users might need someone to help edit their images,\nbut having a personal dedicated human assistant for every user is impossible to\nscale. For that reason, an automated assistant system for image editing is\ndesirable. Additionally, users want more image sources for diverse image\nediting works, and integrating an image search functionality into the editing\ntool is a potential remedy for this demand. Thus, we propose a dataset of an\nautomated Conversational Agent for Image Search and Editing (CAISE). To our\nknowledge, this is the first dataset that provides conversational image search\nand editing annotations, where the agent holds a grounded conversation with\nusers and helps them to search and edit images according to their requests. To\nbuild such a system, we first collect image search and editing conversations\nbetween pairs of annotators. The assistant-annotators are equipped with a\ncustomized image search and editing tool to address the requests from the\nuser-annotators. The functions that the assistant-annotators conduct with the\ntool are recorded as executable commands, allowing the trained system to be\nuseful for real-world application execution. We also introduce a\ngenerator-extractor baseline model for this task, which can adaptively select\nthe source of the next token (i.e., from the vocabulary or from textual/visual\ncontexts) for the executable command. This serves as a strong starting point\nwhile still leaving a large human-machine performance gap for useful future\nwork. Our code and dataset are publicly available at:\nhttps://github.com/hyounghk/CAISE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyounghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doo Soon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some Stylometric Remarks on Ovid's Heroides and the Epistula Sapphus. (arXiv:2202.11864v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11864","description":"<p>This article aims to contribute to two well-worn areas of debate in classical\nLatin philology, relating to Ovid's Heroides. The first is the question of the\nauthenticity (and, to a lesser extent the correct position) of the letter\nplaced fifteenth by almost every editor -- the so-called Epistula Sapphus\n(henceforth ES). The secondary question, although perhaps now less fervently\ndebated, is the authenticity of the 'Double Heroides', placed by those who\naccept them as letters 16-21. I employ a variety of methods drawn from the\ndomain of computational stylometry to consider the poetics and the\nlexico-grammatical features of these elegiac poems in the broader context of a\ncorpus of 'shorter' (from 20 to 546 lines) elegiac works from five authors (266\npoems in all) comprising more or less all of the non-fragmentary classical\ncorpus. Based on a variety of techniques, every measure gives clear indication\nthat the poetic style of the Heroides is Ovidian, but distinctive; they can be\naccurately isolated from Ovid more broadly. The Single and Double Heroides\nsplit into two clear groups, with the ES grouped consistently with the single\nletters. Furthermore, by comparing the style of the letters with the 'early'\n(although there are complications in this label) works of the Amores and the\nlate works of the Ex Ponto, the evidence supports sequential composition --\nmeaning that the ES is correctly placed -- and, further, supports the growing\nconsensus that the double letters were composed significantly later, in exile.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagy_B/0/1/0/all/0/1\">Ben Nagy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using calibrator to improve robustness in Machine Reading Comprehension. (arXiv:2202.11865v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11865","description":"<p>Machine Reading Comprehension(MRC) has achieved a remarkable result since\nsome powerful models, such as BERT, are proposed. However, these models are not\nrobust enough and vulnerable to adversarial input perturbation and\ngeneralization examples. Some works tried to improve the performance on\nspecific types of data by adding some related examples into training data while\nit leads to degradation on the original dataset, because the shift of data\ndistribution makes the answer ranking based on the softmax probability of model\nunreliable. In this paper, we propose a method to improve the robustness by\nusing a calibrator as the post-hoc reranker, which is implemented based on\nXGBoost model. The calibrator combines both manual features and representation\nlearning features to rerank candidate results. Experimental results on\nadversarial datasets show that our model can achieve performance improvement by\nmore than 10\\% and also make improvement on the original and generalization\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phase Continuity: Learning Derivatives of Phase Spectrum for Speech Enhancement. (arXiv:2202.11918v1 [cs.SD])","link":"http://arxiv.org/abs/2202.11918","description":"<p>Modern neural speech enhancement models usually include various forms of\nphase information in their training loss terms, either explicitly or\nimplicitly. However, these loss terms are typically designed to reduce the\ndistortion of phase spectrum values at specific frequencies, which ensures they\ndo not significantly affect the quality of the enhanced speech. In this paper,\nwe propose an effective phase reconstruction strategy for neural speech\nenhancement that can operate in noisy environments. Specifically, we introduce\na phase continuity loss that considers relative phase variations across the\ntime and frequency axes. By including this phase continuity loss in a\nstate-of-the-art neural speech enhancement system trained with reconstruction\nloss and a number of magnitude spectral losses, we show that our proposed\nmethod further improves the quality of enhanced speech signals over the\nbaseline, especially when training is done jointly with a magnitude spectrum\nloss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hyewon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Hyeon-Kyeong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Soo-Whan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hong-Goo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender. (arXiv:2202.11923v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11923","description":"<p>The world of pronouns is changing. From a closed class of words with few\nmembers to a much more open set of terms to reflect identities. However,\nNatural Language Processing (NLP) is barely reflecting this linguistic shift,\neven though recent work outlined the harms of gender-exclusive language\ntechnology. Particularly problematic is the current modeling 3rd person\npronouns, as it largely ignores various phenomena like neopronouns, i.e.,\npronoun sets that are novel and not (yet) widely established. This omission\ncontributes to the discrimination of marginalized and underrepresented groups,\ne.g., non-binary individuals. However, other identity-expression phenomena\nbeyond gender are also ignored by current NLP technology. In this paper, we\nprovide an overview of 3rd person pronoun issues for NLP. Based on our\nobservations and ethical considerations, we define a series of desiderata for\nmodeling pronouns in language technology. We evaluate existing and novel\nmodeling approaches w.r.t. these desiderata qualitatively, and quantify the\nimpact of a more discrimination-free approach on established benchmark data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crowley_A/0/1/0/all/0/1\">Archie Crowley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Segmentation on Discovered Phone Units with Dynamic Programming and Self-Supervised Scoring. (arXiv:2202.11929v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11929","description":"<p>Recent work on unsupervised speech segmentation has used self-supervised\nmodels with a phone segmentation module and a word segmentation module that are\ntrained jointly. This paper compares this joint methodology with an older idea:\nbottom-up phone-like unit discovery is performed first, and symbolic word\nsegmentation is then performed on top of the discovered units (without\ninfluencing the lower level). I specifically describe a duration-penalized\ndynamic programming (DPDP) procedure that can be used for either phone or word\nsegmentation by changing the self-supervised scoring network that gives segment\ncosts. For phone discovery, DPDP is applied with a contrastive predictive\ncoding clustering model, while for word segmentation it is used with an\nautoencoding recurrent neural network. The two models are chained in order to\nsegment speech. This approach gives comparable word segmentation results to\nstate-of-the-art joint self-supervised models on an English benchmark. On\nFrench and Mandarin data, it outperforms previous systems on the ZeroSpeech\nbenchmarks. Analysis shows that the chained DPDP system segments shorter filler\nwords well, but longer words might require an external top-down signal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization Requires Compositional Parsers. (arXiv:2202.11937v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11937","description":"<p>A rapidly growing body of research on compositional generalization\ninvestigates the ability of a semantic parser to dynamically recombine\nlinguistic elements seen in training into unseen sequences. We present a\nsystematic comparison of sequence-to-sequence models and models guided by\ncompositional principles on the recent COGS corpus (Kim and Linzen, 2020).\nThough seq2seq models can perform well on lexical tasks, they perform with\nnear-zero accuracy on structural generalization tasks that require novel\nsyntactic structures; this holds true even when they are trained to predict\nsyntax instead of semantics. In contrast, compositional models achieve\nnear-perfect accuracy on structural generalization; we present new results\nconfirming this from the AM parser (Groschwitz et al., 2021). Our findings show\nstructural generalization is a key measure of compositional generalization and\nrequires models that are aware of complex structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weissenhorn_P/0/1/0/all/0/1\">Pia Wei&#xdf;enhorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuekun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donatelli_L/0/1/0/all/0/1\">Lucia Donatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1\">Alexander Koller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better. (arXiv:2202.12024v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12024","description":"<p>Effectively finetuning pretrained language models (PLMs) is critical for\ntheir success in downstream tasks. However, PLMs may have risks in overfitting\npretraining signals, and there are some gaps between downstream tasks and the\npretraining tasks. It can be difficult for vanilla finetuning methods to\novercome the barrier between pretraining and downstream tasks, which leads to\nsuboptimal performance. In this paper, we propose a very simple yet effective\nmethod named NoisyTune which can help better finetune PLMs in downstream tasks\nby adding some noise to the parameters of PLMs before finetuning. More\nspecifically, we propose a matrix-wise perturbing method by adding different\nuniform noises according to the standard deviations of different parameter\nmatrices, which can consider the varied characteristics of different types of\nparameters in PLMs. Extensive experiments on the GLUE English benchmark and the\nXTREME multilingual benchmark show that NoisyTune can consistently improve the\nperformance of different PLMs in many downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Unfairness of DP-SGD Across Settings. (arXiv:2202.12058v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12058","description":"<p>End users and regulators require private and fair artificial intelligence\nmodels, but previous work suggests these objectives may be at odds. We use the\nCivilComments to evaluate the impact of applying the {\\em de facto} standard\napproach to privacy, DP-SGD, across several fairness metrics. We evaluate three\nimplementations of DP-SGD: for dimensionality reduction (PCA), linear\nclassification (logistic regression), and robust deep learning (Group-DRO). We\nestablish a negative, logarithmic correlation between privacy and fairness in\nthe case of linear classification and robust deep learning. DP-SGD had no\nsignificant impact on fairness for PCA, but upon inspection, also did not seem\nto lead to private representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noe_F/0/1/0/all/0/1\">Frederik Noe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herskind_R/0/1/0/all/0/1\">Rasmus Herskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KESA: A Knowledge Enhanced Approach For Sentiment Analysis. (arXiv:2202.12093v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12093","description":"<p>Though some recent works focus on injecting sentiment knowledge into\npre-trained language models, they usually design mask and reconstruction tasks\nin the post-training phase. In this paper, we aim to benefit from sentiment\nknowledge in a lighter way. To achieve this goal, we study sentence-level\nsentiment analysis and, correspondingly, propose two sentiment-aware auxiliary\ntasks named sentiment word cloze and conditional sentiment prediction. The\nfirst task learns to select the correct sentiment words within the input, given\nthe overall sentiment polarity as prior knowledge. On the contrary, the second\ntask predicts the overall sentiment polarity given the sentiment polarity of\nthe word as prior knowledge. In addition, two kinds of label combination\nmethods are investigated to unify multiple types of labels in each task. We\nargue that more information can promote the models to learn more profound\nsemantic representation. We implement it in a straightforward way to verify\nthis hypothesis. The experimental results demonstrate that our approach\nconsistently outperforms pre-trained models and is additive to existing\nknowledge-enhanced post-trained models. The code and data are released at\nhttps://github.com/lshowway/KESA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qinghua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems. (arXiv:2202.12107v1 [cs.AI])","link":"http://arxiv.org/abs/2202.12107","description":"<p>Our work is the first attempt to apply Natural Language Processing to\nautomate the development of simulation models of logistics systems. We\ndemonstrated that the framework built on top of the fine-tuned\nTransdormer-based language model could produce functionally valid simulations\nof queuing and inventory control systems given the verbal description. The\nproposed framework has the potential to remove the tedium of programming and\nallow experts to focus on the high-level consideration of the problem and\nholistic thinking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jackson_I/0/1/0/all/0/1\">Ilya Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenz_M/0/1/0/all/0/1\">Maria Jesus Saenz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction. (arXiv:2202.12109v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12109","description":"<p>In this paper, we propose an effective yet efficient model PAIE for both\nsentence-level and document-level Event Argument Extraction (EAE), which also\ngeneralizes well when there is a lack of training data. On the one hand, PAIE\nutilizes prompt tuning for extractive objectives to take the best advantages of\nPre-trained Language Models (PLMs). It introduces two span selectors based on\nprompt to select start/end tokens among input texts for each role. On the other\nhand, we capture argument interactions via multi-role prompts, and conduct\njoint optimization with optimal span assignments via a bipartite matching loss.\nAlso, with flexible prompt design, PAIE can extract multiple arguments with the\nsame role, instead of conventional heuristic threshold tuning. We have\nconducted extensive experiments on three benchmarks, including both sentence-\nand document-level EAE. The results present a promising improvements from PAIE\n(1.1% and 3.8% F1 gains on average in sentence-level and document-level\nrespectively). Further analysis demonstrates the efficiency, generalization to\nfew-shot settings and effectiveness of different extractive prompt tuning\nstrategies. We will release our codes upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yubo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"splink\" is happy and \"phrouth\" is scary: Emotion Intensity Analysis for Nonsense Words. (arXiv:2202.12132v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12132","description":"<p>People associate affective meanings to words -- \"death\" is scary and sad\nwhile \"party\" is connotated with surprise and joy. This raises the question if\nthe association is purely a product of the learned affective imports inherent\nto semantic meanings, or is also an effect of other features of words, e.g.,\nmorphological and phonological patterns. We approach this question with an\nannotation-based analysis leveraging nonsense words. Specifically, we conduct a\nbest-worst scaling crowdsourcing study in which participants assign intensity\nscores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense\nwords and, for comparison of the results to previous work, to 68 real words.\nBased on this resource, we develop character-level and phonology-based\nintensity regressors and evaluate them on real and nonsense words, and across\nthese categories (making use of the NRC emotion intensity lexicon of 7493\nwords). The data analysis reveals that some phonetic patterns show clear\ndifferences between emotion intensities. For instance, s as a first phoneme\ncontributes to joy, sh to surprise, p as last phoneme more to disgust than to\nanger and fear. In the modelling experiments, a regressor trained on real words\nfrom the NRC emotion intensity lexicon shows a higher performance (r = 0.17)\nthan regressors that aim at learning the emotion connotation purely from\nnonsense words. We conclude that humans do associate affective meaning to words\nbased on surface patterns, but also based on similarities to existing words\n(\"juy\" to \"joy\", or \"flike\" to \"like\").\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabbatino_V/0/1/0/all/0/1\">Valentino Sabbatino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweitzer_A/0/1/0/all/0/1\">Antje Schweitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How reparametrization trick broke differentially-private text representation leaning. (arXiv:2202.12138v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12138","description":"<p>As privacy gains traction in the NLP community, researchers have started\nadopting various approaches to privacy-preserving methods. One of the favorite\nprivacy frameworks, differential privacy (DP), is perhaps the most compelling\nthanks to its fundamental theoretical guarantees. Despite the apparent\nsimplicity of the general concept of differential privacy, it seems non-trivial\nto get it right when applying it to NLP. In this short paper, we formally\nanalyze several recent NLP papers proposing text representation learning using\nDPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and\nreveal their false claims of being differentially private. Furthermore, we also\nshow a simple yet general empirical sanity check to determine whether a given\nimplementation of a DP mechanism almost certainly violates the privacy loss\nguarantees. Our main goal is to raise awareness and help the community\nunderstand potential pitfalls of applying differential privacy to text\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words. (arXiv:2202.12142v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12142","description":"<p>The standard BERT adopts subword-based tokenization, which may break a word\ninto two or more wordpieces (e.g., converting \"lossless\" to \"loss\" and \"less\").\nThis will bring inconvenience in following situations: (1) what is the best way\nto obtain the contextual vector of a word that is divided into multiple\nwordpieces? (2) how to predict a word via cloze test without knowing the number\nof wordpieces in advance? In this work, we explore the possibility of\ndeveloping BERT-style pretrained model over a vocabulary of words instead of\nwordpieces. We call such word-level BERT model as WordBERT. We train models\nwith different vocabulary sizes, initialization configurations and languages.\nResults show that, compared to standard wordpiece-based BERT, WordBERT makes\nsignificant improvements on cloze test and machine reading comprehension. On\nmany other natural language understanding tasks, including POS tagging,\nchunking and NER, WordBERT consistently performs better than BERT. Model\nanalysis indicates that the major advantage of WordBERT over BERT lies in the\nunderstanding for low-frequency words and rare words. Furthermore, since the\npipeline is language-independent, we train WordBERT for Chinese language and\nobtain significant gains on five natural language understanding datasets.\nLastly, the analyse on inference speed illustrates WordBERT has comparable time\ncost to BERT in natural language understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Junwei Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An NLP Solution to Foster the Use of Information in Electronic Health Records for Efficiency in Decision-Making in Hospital Care. (arXiv:2202.12159v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12159","description":"<p>The project aimed to define the rules and develop a technological solution to\nautomatically identify a set of attributes within free-text clinical records\nwritten in Portuguese. The first application developed and implemented on this\nbasis was a structured summary of a patient's clinical history, including\nprevious diagnoses and procedures, usual medication, and relevant\ncharacteristics or conditions for clinical decisions, such as allergies, being\nunder anticoagulant therapy, etc. The project's goal was achieved by a\nmultidisciplinary team that included clinicians, epidemiologists, computational\nlinguists, machine learning researchers and software engineers, bringing\ntogether the expertise and perspectives of a public hospital, the university\nand the private sector. Relevant benefits to users and patients are related\nwith facilitated access to the patient's history, which translates into\nexhaustiveness in apprehending the patient's clinical past and efficiency due\nto time saving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leite_Moreira_A/0/1/0/all/0/1\">Adelino Leite-Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_A/0/1/0/all/0/1\">Afonso Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrosa_A/0/1/0/all/0/1\">Afonso Pedrosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_Sousa_A/0/1/0/all/0/1\">Am&#xe2;ndio Rocha-Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_A/0/1/0/all/0/1\">Ana Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amaral_Gomes_A/0/1/0/all/0/1\">Andr&#xe9; Amaral-Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_C/0/1/0/all/0/1\">Cl&#xe1;udia Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueira_H/0/1/0/all/0/1\">Helena Figueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_N/0/1/0/all/0/1\">Nuno Rocha Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_P/0/1/0/all/0/1\">Pedro Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimenta_T/0/1/0/all/0/1\">Tiago Pimenta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-attention for incomplete utterance rewriting. (arXiv:2202.12160v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12160","description":"<p>Incomplete utterance rewriting (IUR) has recently become an essential task in\nNLP, aiming to complement the incomplete utterance with sufficient context\ninformation for comprehension. In this paper, we propose a novel method by\ndirectly extracting the coreference and omission relationship from the\nself-attention weight matrix of the transformer instead of word embeddings and\nedit the original text accordingly to generate the complete utterance.\nBenefiting from the rich information in the self-attention weight matrix, our\nmethod achieved competitive results on public IUR datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhitao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v1 [eess.AS])","link":"http://arxiv.org/abs/2202.12163","description":"<p>In this paper, we introduce a novel language identification system based on\nconformer layers. We propose an attentive temporal pooling mechanism to allow\nthe model to carry information in long-form audio via a recurrent form, such\nthat the inference can be performed in a streaming fashion. Additionally, a\nsimple domain adaptation mechanism is introduced to allow adapting an existing\nlanguage identification model to a new domain where the prior language\ndistribution is different. We perform a comparative study of different model\ntopologies under different constraints of model size, and find that\nconformer-base models outperform LSTM and transformer based models. Our\nexperiments also show that attentive temporal pooling and domain adaptation\nsignificantly improve the model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yiling Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming a Theoretical Limitation of Self-Attention. (arXiv:2202.12172v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12172","description":"<p>Although transformers are remarkably effective for many tasks, there are some\nsurprisingly easy-looking regular languages that they struggle with. Hahn shows\nthat for languages where acceptance depends on a single input symbol, a\ntransformer's classification decisions become less and less confident (that is,\nwith cross-entropy approaching 1 bit per string) as input strings get longer\nand longer. We examine this limitation using two languages: PARITY, the\nlanguage of bit strings with an odd number of 1s, and FIRST, the language of\nbit strings starting with a 1. We demonstrate three ways of overcoming the\nlimitation suggested by Hahn's lemma. First, we settle an open question by\nconstructing a transformer that recognizes PARITY with perfect accuracy, and\nsimilarly for FIRST. Second, we use layer normalization to bring the\ncross-entropy of both models arbitrarily close to zero. Third, when\ntransformers need to focus on a single position, as for FIRST, we find that\nthey can fail to generalize to longer strings; we offer a simple remedy to this\nproblem that also improves length generalization in machine translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cholak_P/0/1/0/all/0/1\">Peter Cholak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review. (arXiv:2202.12205v1 [cs.AI])","link":"http://arxiv.org/abs/2202.12205","description":"<p>Advocates for Neuro-Symbolic AI (NeSy) assert that combining deep learning\nwith symbolic reasoning will lead to stronger AI than either paradigm on its\nown. As successful as deep learning has been, it is generally accepted that\neven our best deep learning systems are not very good at abstract reasoning.\nAnd since reasoning is inextricably linked to language, it makes intuitive\nsense that Natural Language Processing (NLP), would be a particularly\nwell-suited candidate for NeSy. We conduct a structured review of studies\nimplementing NeSy for NLP, challenges and future directions, and aim to answer\nthe question of whether NeSy is indeed meeting its promises: reasoning,\nout-of-distribution generalization, interpretability, learning and reasoning\nfrom small data, and transferability to new domains. We examine the impact of\nknowledge representation, such as rules and semantic networks, language\nstructure and relational structure, and whether implicit or explicit reasoning\ncontributes to higher promise scores. We find that knowledge encoded in\nrelational structures and explicit reasoning tend to lead to more NeSy goals\nbeing satisfied. We also advocate for a more methodical approach to the\napplication of theories of reasoning, which we hope can reduce some of the\nfriction between the symbolic and sub-symbolic schools of AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_K/0/1/0/all/0/1\">Kyle Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1\">Aparna Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozic_B/0/1/0/all/0/1\">Bojan Bo&#x17e;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longo_L/0/1/0/all/0/1\">Luca Longo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTVision -- A Parameter-Efficient Approach for Question Answering. (arXiv:2202.12210v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12210","description":"<p>We present a highly parameter efficient approach for Question Answering that\nsignificantly reduces the need for extended BERT fine-tuning. Our method uses\ninformation from the hidden state activations of each BERT transformer layer,\nwhich is discarded during typical BERT inference. Our best model achieves\nmaximal BERT performance at a fraction of the training time and GPU or TPU\nexpense. Performance is further improved by ensembling our model with BERTs\npredictions. Furthermore, we find that near optimal performance can be achieved\nfor QA span annotation using less training data. Our experiments show that this\napproach works well not only for span annotation, but also for classification,\nsuggesting that it may be extensible to a wider range of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Siduo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benge_C/0/1/0/all/0/1\">Cristopher Benge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_W/0/1/0/all/0/1\">William Casey King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing BERT's priors with serial reproduction chains. (arXiv:2202.12226v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12226","description":"<p>We can learn as much about language models from what they say as we learn\nfrom their performance on targeted benchmarks. Sampling is a promising\nbottom-up method for probing, but generating samples from successful models\nlike BERT remains challenging. Taking inspiration from theories of iterated\nlearning in cognitive science, we explore the use of serial reproduction chains\nto probe BERT's priors. Although the masked language modeling objective does\nnot guarantee a consistent joint distribution, we observe that a unique and\nconsistent estimator of the ground-truth joint distribution may be obtained by\na GSN sampler, which randomly selects which word to mask and reconstruct on\neach step. We compare the lexical and syntactic statistics of sentences from\nthe resulting prior distribution against those of the ground-truth corpus\ndistribution and elicit a large empirical sample of naturalness judgments to\ninvestigate how, exactly, the model deviates from human speakers. Our findings\nsuggest the need to move beyond top-down evaluation methods toward bottom-up\nprobing to capture the full richness of what has been learned about language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamakoshi_T/0/1/0/all/0/1\">Takateru Yamakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural reality of argument structure constructions. (arXiv:2202.12246v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12246","description":"<p>In lexicalist linguistic theories, argument structure is assumed to be\npredictable from the meaning of verbs. As a result, the verb is the primary\ndeterminant of the meaning of a clause. In contrast, construction grammarians\npropose that argument structure is encoded in constructions (or form-meaning\npairs) that are distinct from verbs. Decades of psycholinguistic research have\nproduced substantial empirical evidence in favor of the construction view. Here\nwe adapt several psycholinguistic studies to probe for the existence of\nargument structure constructions (ASCs) in Transformer-based language models\n(LMs). First, using a sentence sorting experiment, we find that sentences\nsharing the same construction are closer in embedding space than sentences\nsharing the same verb. Furthermore, LMs increasingly prefer grouping by\nconstruction with more input data, mirroring the behaviour of non-native\nlanguage learners. Second, in a \"Jabberwocky\" priming-based experiment, we find\nthat LMs associate ASCs with meaning, even in semantically nonsensical\nsentences. Our work offers the first evidence for ASCs in LMs and highlights\nthe potential to devise novel probing methods grounded in psycholinguistic\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zining Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_G/0/1/0/all/0/1\">Guillaume Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward More Meaningful Resources for Lower-resourced Languages. (arXiv:2202.12288v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12288","description":"<p>In this position paper, we describe our perspective on how meaningful\nresources for lower-resourced languages should be developed in connection with\nthe speakers of those languages. We first examine two massively multilingual\nresources in detail. We explore the contents of the names stored in Wikidata\nfor a few lower-resourced languages and find that many of them are not in fact\nin the languages they claim to be and require non-trivial effort to correct. We\ndiscuss quality issues present in WikiAnn and evaluate whether it is a useful\nsupplement to hand annotated data. We then discuss the importance of creating\nannotation for lower-resourced languages in a thoughtful and ethical way that\nincludes the languages' speakers as part of the development process. We\nconclude with recommended guidelines for resource development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holley_N/0/1/0/all/0/1\">Nolan Holley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleva_J/0/1/0/all/0/1\">Jonne S&#xe4;lev&#xe4;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Failures of Large Language Models via Human Cognitive Biases. (arXiv:2202.12299v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12299","description":"<p>Large language models generate complex, open-ended outputs: instead of\noutputting a single class, they can write summaries, generate dialogue, and\nproduce working code. In order to study the reliability of these open-ended\nsystems, we must understand not just when they fail, but also how they fail. To\napproach this, we draw inspiration from human cognitive biases -- systematic\npatterns of deviation from rational judgement. Specifically, we use cognitive\nbiases to (i) identify inputs that models are likely to err on, and (ii)\ndevelop tests to qualitatively characterize their errors on these inputs. Using\ncode generation as a case study, we find that OpenAI's Codex errs predictably\nbased on how the input prompt is framed, adjusts outputs towards anchors, and\nis biased towards outputs that mimic frequent training examples. We then use\nour framework to uncover high-impact errors such as incorrectly deleting files.\nOur experiments suggest that cognitive science can be a useful jumping-off\npoint to better understand how contemporary machine learning systems behave.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Erik Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embeddings-Based Clustering for Target Specific Stances: The Case of a Polarized Turkey. (arXiv:2005.09649v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2005.09649","description":"<p>On June 24, 2018, Turkey conducted a highly consequential election in which\nthe Turkish people elected their president and parliament in the first election\nunder a new presidential system. During the election period, the Turkish people\nextensively shared their political opinions on Twitter. One aspect of\npolarization among the electorate was support for or opposition to the\nreelection of Recep Tayyip Erdo\\u{g}an. In this paper, we present an\nunsupervised method for target-specific stance detection in a polarized\nsetting, specifically Turkish politics, achieving 90% precision in identifying\nuser stances, while maintaining more than 80% recall. The method involves\nrepresenting users in an embedding space using Google's Convolutional Neural\nNetwork (CNN) based multilingual universal sentence encoder. The\nrepresentations are then projected onto a lower dimensional space in a manner\nthat reflects similarities and are consequently clustered. We show the\neffectiveness of our method in properly clustering users of divergent groups\nacross multiple targets that include political figures, different groups, and\nparties. We perform our analysis on a large dataset of 108M Turkish\nelection-related tweets along with the timeline tweets of 168k Turkish users,\nwho authored 213M tweets. Given the resultant user stances, we are able to\nobserve correlations between topics and compute topic polarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rashed_A/0/1/0/all/0/1\">Ammar Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1\">Mucahid Kutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayrak_C/0/1/0/all/0/1\">Cans&#x131;n Bayrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum circuit design for universal distribution using a superposition of classical automata. (arXiv:2006.00987v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2006.00987","description":"<p>In this research, we present a quantum circuit design and implementation for\na parallel universal linear bounded automata. This circuit is able to\naccelerate the inference of algorithmic structures in data for discovering\ncausal generative models. The computation model is practically restricted in\ntime and space resources. A classical exhaustive enumeration of all possible\nprograms on the automata is shown for a couple of example cases. The precise\nquantum circuit design that allows executing a superposition of programs, along\nwith a superposition of inputs as in the standard quantum Turing machine\nformulation, is presented. This is the first time, a superposition of classical\nautomata is implemented on the circuit model of quantum computation, having the\ncorresponding mechanistic parts of a classical Turing machine. The\nsuperposition of programs allows our model to be used for experimenting with\nthe space of program-output behaviors in algorithmic information theory. Our\nimplementations on OpenQL and Qiskit quantum programming language is copy-left\nand is publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sarkar_A/0/1/0/all/0/1\">Aritra Sarkar</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Al_Ars_Z/0/1/0/all/0/1\">Zaid Al-Ars</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bertels_K/0/1/0/all/0/1\">Koen Bertels</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-supervised Representation Learning of Sentence Structure for Authorship Attribution. (arXiv:2010.06786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.06786","description":"<p>Syntactic structure of sentences in a document substantially informs about\nits authorial writing style. Sentence representation learning has been widely\nexplored in recent years and it has been shown that it improves the\ngeneralization of different downstream tasks across many domains. Even though\nutilizing probing methods in several studies suggests that these learned\ncontextual representations implicitly encode some amount of syntax, explicit\nsyntactic information further improves the performance of deep neural models in\nthe domain of authorship attribution. These observations have motivated us to\ninvestigate the explicit representation learning of syntactic structure of\nsentences. In this paper, we propose a self-supervised framework for learning\nstructural representations of sentences. The self-supervised network contains\ntwo components; a lexical sub-network and a syntactic sub-network which take\nthe sequence of words and their corresponding structural labels as the input,\nrespectively. Due to the n-to-1 mapping of words to their structural labels,\neach word will be embedded into a vector representation which mainly carries\nstructural information. We evaluate the learned structural representations of\nsentences using different probing tasks, and subsequently utilize them in the\nauthorship attribution task. Our experimental results indicate that the\nstructural embeddings significantly improve the classification tasks when\nconcatenated with the existing pre-trained word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafariakinabad_F/0/1/0/all/0/1\">Fereshteh Jafariakinabad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_K/0/1/0/all/0/1\">Kien A. Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling. (arXiv:2105.06597v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06597","description":"<p>Recent advances in large-scale pre-training such as GPT-3 allow seemingly\nhigh quality text to be generated from a given prompt. However, such generation\nsystems often suffer from problems of hallucinated facts, and are not\ninherently designed to incorporate useful external information. Grounded\ngeneration models appear to offer remedies, but their training typically relies\non rarely-available parallel data where information-relevant documents are\nprovided for context. We propose a framework that alleviates this data\nconstraint by jointly training a grounded generator and document retriever on\nthe language model signal. The model learns to reward retrieval of the\ndocuments with the highest utility in generation, and attentively combines them\nusing a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We\ndemonstrate that both generator and retriever can take advantage of this joint\ntraining and work synergistically to produce more informative and relevant text\nin both prose and dialogue generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Siqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1\">Chris Brockett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing. (arXiv:2110.07205v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.07205","description":"<p>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in\npre-trained natural language processing models, we propose a unified-modal\nSpeechT5 framework that explores the encoder-decoder pre-training for\nself-supervised speech/text representation learning. The SpeechT5 framework\nconsists of a shared encoder-decoder network and six modal-specific\n(speech/text) pre/post-nets. After preprocessing the input speech/text through\nthe pre-nets, the shared encoder-decoder network models the\nsequence-to-sequence transformation, and then the post-nets generate the output\nin the speech/text modality based on the output of the decoder. Leveraging\nlarge-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a\nunified-modal representation, hoping to improve the modeling capability for\nboth speech and text. To align the textual and speech information into this\nunified semantic space, we propose a cross-modal vector quantization approach\nthat randomly mixes up speech/text states with latent units as the interface\nbetween encoder and decoder. Extensive evaluations show the superiority of the\nproposed SpeechT5 framework on a wide variety of spoken language processing\ntasks, including automatic speech recognition, speech synthesis, speech\ntranslation, voice conversion, speech enhancement, and speaker identification.\nWe will release our code and model at https://github.com/microsoft/SpeechT5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ao_J/0/1/0/all/0/1\">Junyi Ao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying causal relations in tweets using deep learning: Use case on diabetes-related tweets from 2017-2021. (arXiv:2111.01225v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01225","description":"<p>Objective: Leveraging machine learning methods, we aim to extract both\nexplicit and implicit cause-effect associations in patient-reported,\ndiabetes-related tweets and provide a tool to better understand opinion,\nfeelings and observations shared within the diabetes online community from a\ncausality perspective. Materials and Methods: More than 30 million\ndiabetes-related tweets in English were collected between April 2017 and\nJanuary 2021. Deep learning and natural language processing methods were\napplied to focus on tweets with personal and emotional content. A\ncause-effect-tweet dataset was manually labeled and used to train 1) a\nfine-tuned Bertweet model to detect causal sentences containing a causal\nassociation 2) a CRF model with BERT based features to extract possible\ncause-effect associations. Causes and effects were clustered in a\nsemi-supervised approach and visualised in an interactive cause-effect-network.\nResults: Causal sentences were detected with a recall of 68% in an imbalanced\ndataset. A CRF model with BERT based features outperformed a fine-tuned BERT\nmodel for cause-effect detection with a macro recall of 68%. This led to 96,676\nsentences with cause-effect associations. \"Diabetes\" was identified as the\ncentral cluster followed by \"Death\" and \"Insulin\". Insulin pricing related\ncauses were frequently associated with \"Death\". Conclusions: A novel\nmethodology was developed to detect causal sentences and identify both explicit\nand implicit, single and multi-word cause and corresponding effect as expressed\nin diabetes-related tweets leveraging BERT-based architectures and visualised\nas cause-effect-network. Extracting causal associations on real-life, patient\nreported outcomes in social media data provides a useful complementary source\nof information in diabetes research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahne_A/0/1/0/all/0/1\">Adrian Ahne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tannier_X/0/1/0/all/0/1\">Xavier Tannier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_M/0/1/0/all/0/1\">Md Imbessat Hassan Rizvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czernichow_T/0/1/0/all/0/1\">Thomas Czernichow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orchard_F/0/1/0/all/0/1\">Francisco Orchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bour_C/0/1/0/all/0/1\">Charline Bour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagherazzi_G/0/1/0/all/0/1\">Guy Fagherazzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Step-unrolled Denoising Autoencoders for Text Generation. (arXiv:2112.06749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06749","description":"<p>In this paper we propose a new generative model of text, Step-unrolled\nDenoising Autoencoder (SUNDAE), that does not rely on autoregressive models.\nSimilarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a\nsequence of tokens, starting from random inputs and improving them each time\nuntil convergence. We present a simple new improvement operator that converges\nin fewer iterations than diffusion methods, while qualitatively producing\nbetter samples on natural language datasets. SUNDAE achieves state-of-the-art\nresults (among non-autoregressive methods) on the WMT'14 English-to-German\ntranslation task and good qualitative results on unconditional language\nmodeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python\ncode from GitHub. The non-autoregressive nature of SUNDAE opens up\npossibilities beyond left-to-right prompted generation, by filling in arbitrary\nblank patterns in a template.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savinov_N/0/1/0/all/0/1\">Nikolay Savinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Junyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binkowski_M/0/1/0/all/0/1\">Mikolaj Binkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1\">Aaron van den Oord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Memorization Across Neural Language Models. (arXiv:2202.07646v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.07646","description":"<p>Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n</p>\n<p>We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09662","description":"<p>Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faal_F/0/1/0/all/0/1\">Farshid Faal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_K/0/1/0/all/0/1\">Ketra Schmitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation. (arXiv:2202.11742v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11742","description":"<p>Following language instructions to navigate in unseen environments is a\nchallenging problem for autonomous embodied agents. The agent not only needs to\nground languages in visual scenes, but also should explore the environment to\nreach its target. In this work, we propose a dual-scale graph transformer\n(DUET) for joint long-term action planning and fine-grained cross-modal\nunderstanding. We build a topological map on-the-fly to enable efficient\nexploration in global action space. To balance the complexity of large action\nspace reasoning and fine-grained language grounding, we dynamically combine a\nfine-scale encoding over local observations and a coarse-scale encoding on a\nglobal map via graph transformers. The proposed approach, DUET, significantly\noutperforms state-of-the-art methods on goal-oriented vision-and-language\nnavigation (VLN) benchmarks REVERIE and SOON. It also improves the success rate\non the fine-grained VLN benchmark R2R.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1\">Pierre-Louis Guhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1\">Makarand Tapaswi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Image Super-Resolution Methods: A Survey. (arXiv:2202.11763v1 [eess.IV])","link":"http://arxiv.org/abs/2202.11763","description":"<p>Super-resolution (SR), the process of obtaining high-resolution images from\none or more low-resolution observations of the same scene, has been a very\npopular topic of research in the last few decades in both signal processing and\nimage processing areas. Due to the recent developments in Convolutional Neural\nNetworks, the popularity of SR algorithms has skyrocketed as the barrier of\nentry has been lowered significantly. Recently, this popularity has spread into\nvideo processing areas to the lengths of developing SR models that work in\nreal-time. In this paper, we compare different SR models that specialize in\nsingle image processing and will take a glance at how they evolved to take on\nmany different objectives and shapes over the years.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maral_B/0/1/0/all/0/1\">Bahattin Can Maral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When do GANs replicate? On the choice of dataset size. (arXiv:2202.11765v1 [cs.LG])","link":"http://arxiv.org/abs/2202.11765","description":"<p>Do GANs replicate training images? Previous studies have shown that GANs do\nnot seem to replicate training data without significant change in the training\nprocedure. This leads to a series of research on the exact condition needed for\nGANs to overfit to the training data. Although a number of factors has been\ntheoretically or empirically identified, the effect of dataset size and\ncomplexity on GANs replication is still unknown. With empirical evidence from\nBigGAN and StyleGAN2, on datasets CelebA, Flower and LSUN-bedroom, we show that\ndataset size and its complexity play an important role in GANs replication and\nperceptual quality of the generated images. We further quantify this\nrelationship, discovering that replication percentage decays exponentially with\nrespect to dataset size and complexity, with a shared decaying factor across\nGAN-dataset combinations. Meanwhile, the perceptual image quality follows a\nU-shape trend w.r.t dataset size. This finding leads to a practical tool for\none-shot estimation on minimal dataset size to prevent GAN replication which\ncan be used to guide datasets construction and selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qianli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benitez_Quiroz_F/0/1/0/all/0/1\">Fabian Benitez-Quiroz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Aleix Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Multiple and Diverse Directions for Cognitive Image Properties. (arXiv:2202.11772v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11772","description":"<p>Recent research has shown that it is possible to find interpretable\ndirections in the latent spaces of pre-trained GANs. These directions enable\ncontrollable generation and support a variety of semantic editing operations.\nWhile previous work has focused on discovering a single direction that performs\na desired editing operation such as zoom-in, limited work has been done on the\ndiscovery of multiple and diverse directions that can achieve the desired edit.\nIn this work, we propose a novel framework that discovers multiple and diverse\ndirections for a given property of interest. In particular, we focus on the\nmanipulation of cognitive properties such as Memorability, Emotional Valence\nand Aesthetics. We show with extensive experiments that our method successfully\nmanipulates these properties while producing diverse outputs. Our project page\nand source code can be found at <a href=\"http://catlab-team.github.io/latentcognitive.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocasari_U/0/1/0/all/0/1\">Umut Kocasari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bag_A/0/1/0/all/0/1\">Alperen Bag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuksel_O/0/1/0/all/0/1\">Oguz Kaan Yuksel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanardag_P/0/1/0/all/0/1\">Pinar Yanardag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Art Creation with Multi-Conditional StyleGANs. (arXiv:2202.11777v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11777","description":"<p>Creating meaningful art is often viewed as a uniquely human endeavor. A human\nartist needs a combination of unique skills, understanding, and genuine\nintention to create artworks that evoke deep feelings and emotions. In this\npaper, we introduce a multi-conditional Generative Adversarial Network (GAN)\napproach trained on large amounts of human paintings to synthesize\nrealistic-looking paintings that emulate human art. Our approach is based on\nthe StyleGAN neural network architecture, but incorporates a custom\nmulti-conditional control mechanism that provides fine-granular control over\ncharacteristics of the generated paintings, e.g., with regard to the perceived\nemotion evoked in a spectator. For better control, we introduce the conditional\ntruncation trick, which adapts the standard truncation trick for the\nconditional setting and diverse datasets. Finally, we develop a diverse set of\nevaluation techniques tailored to multi-conditional generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobler_K/0/1/0/all/0/1\">Konstantin Dobler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubscher_F/0/1/0/all/0/1\">Florian H&#xfc;bscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westphal_J/0/1/0/all/0/1\">Jan Westphal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sierra_Munera_A/0/1/0/all/0/1\">Alejandro Sierra-M&#xfa;nera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krestel_R/0/1/0/all/0/1\">Ralf Krestel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-guided Disease Classification. (arXiv:2202.11781v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11781","description":"<p>In this work, we present RadioTransformer, a novel visual attention-driven\ntransformer framework, that leverages radiologists' gaze patterns and models\ntheir visuo-cognitive behavior for disease diagnosis on chest radiographs.\nDomain experts, such as radiologists, rely on visual information for medical\nimage interpretation. On the other hand, deep neural networks have demonstrated\nsignificant promise in similar tasks even where visual interpretation is\nchallenging. Eye-gaze tracking has been used to capture the viewing behavior of\ndomain experts, lending insights into the complexity of visual search. However,\ndeep learning frameworks, even those that rely on attention mechanisms, do not\nleverage this rich domain information. RadioTransformer fills this critical gap\nby learning from radiologists' visual search patterns, encoded as 'human visual\nattention regions' in a cascaded global-focal transformer framework. The\noverall 'global' image characteristics and the more detailed 'local' features\nare captured by the proposed global and focal modules, respectively. We\nexperimentally validate the efficacy of our student-teacher approach for 8\ndatasets involving different disease classification tasks where eye-gaze data\nis not available during the inference phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_M/0/1/0/all/0/1\">Moinak Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shubham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nuclei panoptic segmentation and composition regression with multi-task deep neural networks. (arXiv:2202.11804v1 [eess.IV])","link":"http://arxiv.org/abs/2202.11804","description":"<p>Nuclear segmentation, classification and quantification within Haematoxylin &amp;\nEosin stained histology images enables the extraction of interpretable\ncell-based features that can be used in downstream explainable models in\ncomputational pathology. The Colon Nuclei Identification and Counting (CoNIC)\nChallenge is held to help drive forward research and innovation for automatic\nnuclei recognition in computational pathology. This report describes our\nproposed method submitted to the CoNIC challenge. Our method employs a\nmulti-task learning framework, which performs a panoptic segmentation task and\na regression task. For the panoptic segmentation task, we use encoder-decoder\ntype deep neural networks predicting a direction map in addition to a\nsegmentation map in order to separate neighboring nuclei into different\ninstances\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kondo_S/0/1/0/all/0/1\">Satoshi Kondo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kasai_S/0/1/0/all/0/1\">Satoshi Kasai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A modification of the conjugate direction method for motion estimation. (arXiv:2202.11831v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11831","description":"<p>A comparative study of different block matching alternatives for motion\nestimation is presented. The study is focused on computational burden and\nobjective measures on the accuracy of prediction. Together with existing\nalgorithms several new variations have been tested. An interesting modification\nof the conjugate direction method previously related in literature is reported.\nThis new algorithm shows a good trade-off between computational complexity and\naccuracy of motion vector estimation. Computational complexity is evaluated\nusing a sequence of artificial images designed to incorporate a great variety\nof motion vectors. The performance of block matching methods has been measured\nin terms of the entropy in the error signal between the motion compensated and\nthe original frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarres_Ruiz_F/0/1/0/all/0/1\">Francesc Tarres-Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Near Perfect GAN Inversion. (arXiv:2202.11833v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11833","description":"<p>To edit a real photo using Generative Adversarial Networks (GANs), we need a\nGAN inversion algorithm to identify the latent vector that perfectly reproduces\nit. Unfortunately, whereas existing inversion algorithms can synthesize images\nsimilar to real photos, they cannot generate the identical clones needed in\nmost applications. Here, we derive an algorithm that achieves near perfect\nreconstructions of photos. Rather than relying on encoder- or\noptimization-based methods to find an inverse mapping on a fixed generator\n$G(\\cdot)$, we derive an approach to locally adjust $G(\\cdot)$ to more\noptimally represent the photos we wish to synthesize. This is done by locally\ntweaking the learned mapping $G(\\cdot)$ s.t. $\\| {\\bf x} - G({\\bf z})\n\\|&lt;\\epsilon$, with ${\\bf x}$ the photo we wish to reproduce, ${\\bf z}$ the\nlatent vector, $\\|\\cdot\\|$ an appropriate metric, and $\\epsilon &gt; 0$ a small\nscalar. We show that this approach can not only produce synthetic images that\nare indistinguishable from the real photos we wish to replicate, but that these\nimages are readily editable. We demonstrate the effectiveness of the derived\nalgorithm on a variety of datasets including human faces, animals, and cars,\nand discuss its importance for diversity and inclusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qianli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1\">Viraj Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Raghudeep Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Aleix Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanatory Paradigms in Neural Networks. (arXiv:2202.11838v1 [cs.LG])","link":"http://arxiv.org/abs/2202.11838","description":"<p>In this article, we present a leap-forward expansion to the study of\nexplainability in neural networks by considering explanations as answers to\nabstract reasoning-based questions. With $P$ as the prediction from a neural\nnetwork, these questions are `Why P?', `What if not P?', and `Why P, rather\nthan Q?' for a given contrast prediction $Q$. The answers to these questions\nare observed correlations, observed counterfactuals, and observed contrastive\nexplanations respectively. Together, these explanations constitute the\nabductive reasoning scheme. We term the three explanatory schemes as observed\nexplanatory paradigms. The term observed refers to the specific case of\npost-hoc explainability, when an explanatory technique explains the decision\n$P$ after a trained neural network has made the decision $P$. The primary\nadvantage of viewing explanations through the lens of abductive reasoning-based\nquestions is that explanations can be used as reasons while making decisions.\nThe post-hoc field of explainability, that previously only justified decisions,\nbecomes active by being involved in the decision making process and providing\nlimited, but relevant and contextual interventions. The contributions of this\narticle are: ($i$) realizing explanations as reasoning paradigms, ($ii$)\nproviding a probabilistic definition of observed explanations and their\ncompleteness, ($iii$) creating a taxonomy for evaluation of explanations, and\n($iv$) positioning gradient-based complete explanainability's replicability and\nreproducibility across multiple applications and data modalities, ($v$) code\nrepositories, publicly available at\nhttps://github.com/olivesgatech/Explanatory-Paradigms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1\">Ghassan AlRegib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhushankar_M/0/1/0/all/0/1\">Mohit Prabhushankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAISE: Conversational Agent for Image Search and Editing. (arXiv:2202.11847v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11847","description":"<p>Demand for image editing has been increasing as users' desire for expression\nis also increasing. However, for most users, image editing tools are not easy\nto use since the tools require certain expertise in photo effects and have\ncomplex interfaces. Hence, users might need someone to help edit their images,\nbut having a personal dedicated human assistant for every user is impossible to\nscale. For that reason, an automated assistant system for image editing is\ndesirable. Additionally, users want more image sources for diverse image\nediting works, and integrating an image search functionality into the editing\ntool is a potential remedy for this demand. Thus, we propose a dataset of an\nautomated Conversational Agent for Image Search and Editing (CAISE). To our\nknowledge, this is the first dataset that provides conversational image search\nand editing annotations, where the agent holds a grounded conversation with\nusers and helps them to search and edit images according to their requests. To\nbuild such a system, we first collect image search and editing conversations\nbetween pairs of annotators. The assistant-annotators are equipped with a\ncustomized image search and editing tool to address the requests from the\nuser-annotators. The functions that the assistant-annotators conduct with the\ntool are recorded as executable commands, allowing the trained system to be\nuseful for real-world application execution. We also introduce a\ngenerator-extractor baseline model for this task, which can adaptively select\nthe source of the next token (i.e., from the vocabulary or from textual/visual\ncontexts) for the executable command. This serves as a strong starting point\nwhile still leaving a large human-machine performance gap for useful future\nwork. Our code and dataset are publicly available at:\nhttps://github.com/hyounghk/CAISE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyounghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doo Soon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11855","description":"<p>We present a method to learn compositional predictive models from image\nobservations based on implicit object encoders, Neural Radiance Fields (NeRFs),\nand graph neural networks. A central question in learning dynamic models from\nsensor observations is on which representations predictions should be\nperformed. NeRFs have become a popular choice for representing scenes due to\ntheir strong 3D prior. However, most NeRF approaches are trained on a single\nscene, representing the whole scene with a global model, making generalization\nto novel scenes, containing different numbers of objects, challenging. Instead,\nwe present a compositional, object-centric auto-encoder framework that maps\nmultiple views of the scene to a \\emph{set} of latent vectors representing each\nobject separately. The latent vectors parameterize individual NeRF models from\nwhich the scene can be reconstructed and rendered from novel viewpoints. We\ntrain a graph neural network dynamics model in the latent space to achieve\ncompositionality for dynamics prediction. A key feature of our approach is that\nthe learned 3D information of the scene through the NeRF model enables us to\nincorporate structural priors in learning the dynamics models, making long-term\npredictions more stable. The model can further be used to synthesize new scenes\nfrom individual object observations. For planning, we utilize RRTs in the\nlearned latent space, where we can exploit our model and the implicit object\nencoder to make sampling the latent space informative and more efficient. In\nthe experiments, we show that the model outperforms several baselines on a\npushing task containing many objects. Video:\nhttps://dannydriess.github.io/compnerfdyn/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1\">Danny Driess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedrake_R/0/1/0/all/0/1\">Russ Tedrake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1\">Marc Toussaint</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CG-SSD: Corner Guided Single Stage 3D Object Detection from LiDAR Point Cloud. (arXiv:2202.11868v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11868","description":"<p>At present, the anchor-based or anchor-free models that use LiDAR point\nclouds for 3D object detection use the center assigner strategy to infer the 3D\nbounding boxes. However, in a real world scene, the LiDAR can only acquire a\nlimited object surface point clouds, but the center point of the object does\nnot exist. Obtaining the object by aggregating the incomplete surface point\nclouds will bring a loss of accuracy in direction and dimension estimation. To\naddress this problem, we propose a corner-guided anchor-free single-stage 3D\nobject detection model (CG-SSD ).Firstly, 3D sparse convolution backbone\nnetwork composed of residual layers and sub-manifold sparse convolutional\nlayers are used to construct bird's eye view (BEV) features for further deeper\nfeature mining by a lite U-shaped network; Secondly, a novel corner-guided\nauxiliary module (CGAM) is proposed to incorporate corner supervision signals\ninto the neural network. CGAM is explicitly designed and trained to detect\npartially visible and invisible corners to obtains a more accurate object\nfeature representation, especially for small or partial occluded objects;\nFinally, the deep features from both the backbone networks and CGAM module are\nconcatenated and fed into the head module to predict the classification and 3D\nbounding boxes of the objects in the scene. The experiments demonstrate CG-SSD\nachieves the state-of-art performance on the ONCE benchmark for supervised 3D\nobject detection using single frame point cloud data, with 62.77%mAP.\nAdditionally, the experiments on ONCE and Waymo Open Dataset show that CGAM can\nbe extended to most anchor-based models which use the BEV feature to detect\nobjects, as a plug-in and bring +1.17%-+14.27%AP improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruiqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bisheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yangzi Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zongtian Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Benchmark for Household Garbage Image Recognition. (arXiv:2202.11878v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11878","description":"<p>Household garbage images are usually faced with complex backgrounds, variable\nilluminations, diverse angles, and changeable shapes, which bring a great\ndifficulty in garbage image classification. Due to the ability to discover\nproblem-specific features, deep learning and especially convolutional neural\nnetworks (CNNs) have been successfully and widely used for image representation\nlearning. However, available and stable household garbage datasets are\ninsufficient, which seriously limits the development of research and\napplication. Besides, the state of the art in the field of garbage image\nclassification is not entirely clear. To solve this problem, in this study, we\nbuilt a new open benchmark dataset for household garbage image classification\nby simulating different lightings, backgrounds, angles, and shapes. This\ndataset is named 30 Classes of Household Garbage Images (HGI-30), which\ncontains 18,000 images of 30 household garbage classes. The publicly available\nHGI-30 dataset allows researchers to develop accurate and robust methods for\nhousehold garbage recognition. We also conducted experiments and performance\nanalysis of the state-of-the-art deep CNN methods on HGI-30, which serves as\nbaseline results on this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhize Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huanyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Le Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lixiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Ming Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Note on Machine Learning Approach for Computational Imaging. (arXiv:2202.11883v1 [eess.IV])","link":"http://arxiv.org/abs/2202.11883","description":"<p>Computational imaging has been playing a vital role in the development of\nnatural sciences. Advances in sensory, information, and computer technologies\nhave further extended the scope of influence of imaging, making digital images\nan essential component of our daily lives. For the past three decades, we have\nwitnessed phenomenal developments of mathematical and machine learning methods\nin computational imaging. In this note, we will review some of the recent\ndevelopments of the machine learning approach for computational imaging and\ndiscuss its differences and relations to the mathematical approach. We will\ndemonstrate how we may combine the wisdom from both approaches, discuss the\nmerits and potentials of such a combination and present some of the new\ncomputational and theoretical challenges it brings about.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction. (arXiv:2202.11884v1 [cs.RO])","link":"http://arxiv.org/abs/2202.11884","description":"<p>Predicting future motions of road participants is an important task for\ndriving autonomously in urban scenes. Existing models excel at predicting\nmarginal trajectories for single agents, yet it remains an open question to\njointly predict scene compliant trajectories over multiple agents. The\nchallenge is due to exponentially increasing prediction space as a function of\nthe number of agents. In this work, we exploit the underlying relations between\ninteracting agents and decouple the joint prediction problem into marginal\nprediction problems. Our proposed approach M2I first classifies interacting\nagents as pairs of influencers and reactors, and then leverages a marginal\nprediction model and a conditional prediction model to predict trajectories for\nthe influencers and reactors, respectively. The predictions from interacting\nagents are combined and selected according to their joint likelihoods.\nExperiments show that our simple but effective approach achieves\nstate-of-the-art performance on the Waymo Open Motion Dataset interactive\nprediction benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Junru Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian C. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A spectral-spatial fusion anomaly detection method for hyperspectral imagery. (arXiv:2202.11889v1 [eess.IV])","link":"http://arxiv.org/abs/2202.11889","description":"<p>In hyperspectral, high-quality spectral signals convey subtle spectral\ndifferences to distinguish similar materials, thereby providing unique\nadvantage for anomaly detection. Hence fine spectra of anomalous pixels can be\neffectively screened out from heterogeneous background pixels. Since the same\nmaterials have similar characteristics in spatial and spectral dimension,\ndetection performance can be significantly enhanced by jointing spatial and\nspectral information. In this paper, a spectralspatial fusion anomaly detection\n(SSFAD) method is proposed for hyperspectral imagery. First, original spectral\nsignals are mapped to a local linear background space composed of median and\nmean with high confidence, where saliency weight and feature enhancement\nstrategies are implemented to obtain an initial detection map in spectral\ndomain. Futhermore, to make full use of similarity information of local\nbackground around testing pixel, a new detector is designed to extract the\nlocal similarity spatial features of patch images in spatial domain. Finally,\nanomalies are detected by adaptively combining the spectral and spatial\ndetection maps. The experimental results demonstrate that our proposed method\nhas superior detection performance than traditional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hou_Z/0/1/0/all/0/1\">Zengfu Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_T/0/1/0/all/0/1\">Ting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HMD-EgoPose: Head-Mounted Display-Based Egocentric Marker-Less Tool and Hand Pose Estimation for Augmented Surgical Guidance. (arXiv:2202.11891v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11891","description":"<p>The success or failure of modern computer-assisted surgery procedures hinges\non the precise six-degree-of-freedom (6DoF) position and orientation (pose)\nestimation of tracked instruments and tissue. In this paper, we present\nHMD-EgoPose, a single-shot learning-based approach to hand and object pose\nestimation and demonstrate state-of-the-art performance on a benchmark dataset\nfor monocular red-green-blue (RGB) 6DoF marker-less hand and surgical\ninstrument pose tracking. Further, we reveal the capacity of our HMD-EgoPose\nframework for 6DoF near real-time pose estimation on a commercially available\noptical see-through head-mounted display (OST-HMD) through a low-latency\nstreaming approach. Our framework utilized an efficient convolutional neural\nnetwork (CNN) backbone for multi-scale feature extraction and a set of\nsubnetworks to jointly learn the 6DoF pose representation of the rigid surgical\ndrill instrument and the grasping orientation of the hand of a user. To make\nour approach accessible to a commercially available OST-HMD, the Microsoft\nHoloLens 2, we created a pipeline for low-latency video and data communication\nwith a high-performance computing workstation capable of optimized network\ninference. HMD-EgoPose outperformed current state-of-the-art approaches on a\nbenchmark dataset for surgical tool pose estimation, achieving an average tool\n3D vertex error of 11.0 mm on real data and furthering the progress towards a\nclinically viable marker-free tracking strategy. Through our low-latency\nstreaming approach, we achieved a round trip latency of 202.5 ms for pose\nestimation and augmented visualization of the tracked model when integrated\nwith the OST-HMD. Our single-shot learned approach was robust to occlusion and\ncomplex surfaces and improved on current state-of-the-art approaches to\nmarker-less tool and hand pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doughty_M/0/1/0/all/0/1\">Mitchell Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghugre_N/0/1/0/all/0/1\">Nilesh R. Ghugre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Memorability of Face Images. (arXiv:2202.11896v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11896","description":"<p>Everyday, we are bombarded with many photographs of faces, whether on social\nmedia, television, or smartphones. From an evolutionary perspective, faces are\nintended to be remembered, mainly due to survival and personal relevance.\nHowever, all these faces do not have the equal opportunity to stick in our\nminds. It has been shown that memorability is an intrinsic feature of an image\nbut yet, it is largely unknown what attributes make an image more memorable. In\nthis work, we aimed to address this question by proposing a fast approach to\nmodify and control the memorability of face images. In our proposed method, we\nfirst found a hyperplane in the latent space of StyleGAN to separate high and\nlow memorable images. We then modified the image memorability (while\nmaintaining the identity and other facial features such as age, emotion, etc.)\nby moving in the positive or negative direction of this hyperplane normal\nvector. We further analyzed how different layers of the StyleGAN augmented\nlatent space contribute to face memorability. These analyses showed how each\nindividual face attribute makes an image more or less memorable. Most\nimportantly, we evaluated our proposed method for both real and synthesized\nface images. The proposed method successfully modifies and controls the\nmemorability of real human faces as well as unreal synthesized faces. Our\nproposed method can be employed in photograph editing applications for social\nmedia, learning aids, or advertisement purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Younesi_M/0/1/0/all/0/1\">Mohammad Younesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1\">Yalda Mohsenzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Robustness of Convolutional Neural Networks Using Element-Wise Activation Scaling. (arXiv:2202.11898v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11898","description":"<p>Recent works reveal that re-calibrating the intermediate activation of\nadversarial examples can improve the adversarial robustness of a CNN model. The\nstate of the arts [Baiet al., 2021] and [Yanet al., 2021] explores this feature\nat the channel level, i.e. the activation of a channel is uniformly scaled by a\nfactor. In this paper, we investigate the intermediate activation manipulation\nat a more fine-grained level. Instead of uniformly scaling the activation, we\nindividually adjust each element within an activation and thus propose\nElement-Wise Activation Scaling, dubbed EWAS, to improve CNNs' adversarial\nrobustness. Experimental results on ResNet-18 and WideResNet with CIFAR10 and\nSVHN show that EWAS significantly improves the robustness accuracy. Especially\nfor ResNet18 on CIFAR10, EWAS increases the adversarial accuracy by 37.65% to\n82.35% against C&amp;W attack. EWAS is simple yet very effective in terms of\nimproving robustness. The codes are anonymously available at\nhttps://anonymous.4open.science/r/EWAS-DD64.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi-Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLRNet: Semi-Supervised Semantic Segmentation Via Label Reuse for Human Decomposition Images. (arXiv:2202.11900v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11900","description":"<p>Semantic segmentation is a challenging computer vision task demanding a\nsignificant amount of pixel-level annotated data. Producing such data is a\ntime-consuming and costly process, especially for domains with a scarcity of\nexperts, such as medicine or forensic anthropology. While numerous\nsemi-supervised approaches have been developed to make the most from the\nlimited labeled data and ample amount of unlabeled data, domain-specific\nreal-world datasets often have characteristics that both reduce the\neffectiveness of off-the-shelf state-of-the-art methods and also provide\nopportunities to create new methods that exploit these characteristics. We\npropose and evaluate a semi-supervised method that reuses available labels for\nunlabeled images of a dataset by exploiting existing similarities, while\ndynamically weighting the impact of these reused labels in the training\nprocess. We evaluate our method on a large dataset of human decomposition\nimages and find that our method, while conceptually simple, outperforms\nstate-of-the-art consistency and pseudo-labeling-based methods for the\nsegmentation of this dataset. This paper includes graphic content of human\ndecomposition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1\">Sara Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenning Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_K/0/1/0/all/0/1\">Kelley Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steadman_D/0/1/0/all/0/1\">Dawnie Steadman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mockus_A/0/1/0/all/0/1\">Audris Mockus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-driven Planner for Exploration and Navigation. (arXiv:2202.11907v1 [cs.RO])","link":"http://arxiv.org/abs/2202.11907","description":"<p>We consider the problems of exploration and point-goal navigation in\npreviously unseen environments, where the spatial complexity of indoor scenes\nand partial observability constitute these tasks challenging. We argue that\nlearning occupancy priors over indoor maps provides significant advantages\ntowards addressing these problems. To this end, we present a novel planning\nframework that first learns to generate occupancy maps beyond the field-of-view\nof the agent, and second leverages the model uncertainty over the generated\nareas to formulate path selection policies for each task of interest. For\npoint-goal navigation the policy chooses paths with an upper confidence bound\npolicy for efficient and traversable paths, while for exploration the policy\nmaximizes model uncertainty over candidate paths. We perform experiments in the\nvisually realistic environments of Matterport3D using the Habitat simulator and\ndemonstrate: 1) Improved results on exploration and map quality metrics over\ncompetitive methods, and 2) The effectiveness of our planning module when\npaired with the state-of-the-art DD-PPO method for the point-goal navigation\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1\">Georgios Georgakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_B/0/1/0/all/0/1\">Bernadette Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arapin_A/0/1/0/all/0/1\">Anton Arapin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1\">Nikolai Matni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Transformer Meets Robotic Grasping: Exploits Context for Efficient Grasp Detection. (arXiv:2202.11911v1 [cs.RO])","link":"http://arxiv.org/abs/2202.11911","description":"<p>In this paper, we present a transformer-based architecture, namely TF-Grasp,\nfor robotic grasp detection. The developed TF-Grasp framework has two elaborate\ndesigns making it well suitable for visual grasping tasks. The first key design\nis that we adopt the local window attention to capture local contextual\ninformation and detailed features of graspable objects. Then, we apply the\ncross window attention to model the long-term dependencies between distant\npixels. Object knowledge, environmental configuration, and relationships\nbetween different visual entities are aggregated for subsequent grasp\ndetection. The second key design is that we build a hierarchical\nencoder-decoder architecture with skip-connections, delivering shallow features\nfrom encoder to decoder to enable a multi-scale feature fusion. Due to the\npowerful attention mechanism, the TF-Grasp can simultaneously obtain the local\ninformation (i.e., the contours of objects), and model long-term connections\nsuch as the relationships between distinct visual concepts in clutter.\nExtensive computational experiments demonstrate that the TF-Grasp achieves\nsuperior results versus state-of-art grasping convolutional models and attain a\nhigher accuracy of 97.99% and 94.6% on Cornell and Jacquard grasping datasets,\nrespectively. Real-world experiments using a 7DoF Franka Emika Panda robot also\ndemonstrate its capability of grasping unseen objects in a variety of\nscenarios. The code and pre-trained models will be available at\nhttps://github.com/WangShaoSUN/grasp-transformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhangli Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1\">Zhen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpolation-based Contrastive Learning for Few-Label Semi-Supervised Learning. (arXiv:2202.11915v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11915","description":"<p>Semi-supervised learning (SSL) has long been proved to be an effective\ntechnique to construct powerful models with limited labels. In the existing\nliterature, consistency regularization-based methods, which force the perturbed\nsamples to have similar predictions with the original ones have attracted much\nattention for their promising accuracy. However, we observe that, the\nperformance of such methods decreases drastically when the labels get extremely\nlimited, e.g., 2 or 3 labels for each category. Our empirical study finds that\nthe main problem lies with the drifting of semantic information in the\nprocedure of data augmentation. The problem can be alleviated when enough\nsupervision is provided. However, when little guidance is available, the\nincorrect regularization would mislead the network and undermine the\nperformance of the algorithm. To tackle the problem, we (1) propose an\ninterpolation-based method to construct more reliable positive sample pairs;\n(2) design a novel contrastive loss to guide the embedding of the learned\nnetwork to change linearly between samples so as to improve the discriminative\ncapability of the network by enlarging the margin decision boundaries. Since no\ndestructive regularization is introduced, the performance of our proposed\nalgorithm is largely improved. Specifically, the proposed algorithm outperforms\nthe second best algorithm (Comatch) with 5.3% by achieving 88.73%\nclassification accuracy when only two labels are available for each class on\nthe CIFAR-10 dataset. Moreover, we further prove the generality of the proposed\nmethod by improving the performance of the existing state-of-the-art algorithms\nconsiderably with our proposed strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaochang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-scaling Vision Transformers without Training. (arXiv:2202.11921v1 [cs.LG])","link":"http://arxiv.org/abs/2202.11921","description":"<p>This work targets automated designing and scaling of Vision Transformers\n(ViTs). The motivation comes from two pain spots: 1) the lack of efficient and\nprincipled methods for designing and scaling ViTs; 2) the tremendous\ncomputational cost of training ViT that is much heavier than its convolution\ncounterpart. To tackle these issues, we propose As-ViT, an auto-scaling\nframework for ViTs without training, which automatically discovers and scales\nup ViTs in an efficient and principled manner. Specifically, we first design a\n\"seed\" ViT topology by leveraging a training-free search process. This\nextremely fast search is fulfilled by a comprehensive study of ViT's network\ncomplexity, yielding a strong Kendall-tau correlation with ground-truth\naccuracies. Second, starting from the \"seed\" topology, we automate the scaling\nrule for ViTs by growing widths/depths to different ViT layers. This results in\na series of architectures with different numbers of parameters in a single run.\nFinally, based on the observation that ViTs can tolerate coarse tokenization in\nearly training stages, we propose a progressive tokenization strategy to train\nViTs faster and cheaper. As a unified framework, As-ViT achieves strong\nperformance on classification (83.5% top1 on ImageNet-1k) and detection (52.7%\nmAP on COCO) without any manual crafting nor scaling of ViT architectures: the\nend-to-end model design and scaling process cost only 12 hours on one V100 GPU.\nOur code is available at https://github.com/VITA-Group/AsViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaodan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Aided Diagnosis and Out-of-Distribution Detection in Glaucoma Screening Using Color Fundus Photography. (arXiv:2202.11944v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11944","description":"<p>Artificial Intelligence for RObust Glaucoma Screening (AIROGS) Challenge is\nheld for developing solutions for glaucoma screening from color fundus\nphotography that are robust to real-world scenarios. This report describes our\nmethod submitted to the AIROGS challenge. Our method employs convolutional\nneural networks to classify input images to \"referable glaucoma\" or \"no\nreferable glaucoma\". In addition, we introduce an inference-time\nout-of-distribution (OOD) detection method to identify ungradable images. Our\nOOD detection is based on an energy-based method combined with activation\nrectification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kondo_S/0/1/0/all/0/1\">Satoshi Kondo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_S/0/1/0/all/0/1\">Satoshi Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirasawa_K/0/1/0/all/0/1\">Kosuke Hirasawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Disentangled Generative Adversarial Network for Zero-Shot Sketch-Based 3D Shape Retrieval. (arXiv:2202.11948v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11948","description":"<p>Sketch-based 3D shape retrieval is a challenging task due to the large domain\ndiscrepancy between sketches and 3D shapes. Since existing methods are trained\nand evaluated on the same categories, they cannot effectively recognize the\ncategories that have not been used during training. In this paper, we propose a\nnovel domain disentangled generative adversarial network (DD-GAN) for zero-shot\nsketch-based 3D retrieval, which can retrieve the unseen categories that are\nnot accessed during training. Specifically, we first generate domain-invariant\nfeatures and domain-specific features by disentangling the learned features of\nsketches and 3D shapes, where the domain-invariant features are used to align\nwith the corresponding word embeddings. Then, we develop a generative\nadversarial network that combines the domainspecific features of the seen\ncategories with the aligned domain-invariant features to synthesize samples,\nwhere the synthesized samples of the unseen categories are generated by using\nthe corresponding word embeddings. Finally, we use the synthesized samples of\nthe unseen categories combined with the real samples of the seen categories to\ntrain the network for retrieval, so that the unseen categories can be\nrecognized. In order to reduce the domain shift between the synthesized domain\nand the real domain, we adopt the transductive setting to reduce the gap\nbetween the distributions of the synthesized unseen categories and real unseen\ncategories. Extensive experiments on the SHREC'13 and SHREC'14 datasets show\nthat our method significantly improves the retrieval performance of the unseen\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zongyan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Le Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMILE: Sequence-to-Sequence Domain Adaption with Minimizing Latent Entropy for Text Image Recognition. (arXiv:2202.11949v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11949","description":"<p>Training recognition models with synthetic images have achieved remarkable\nresults in text recognition. However, recognizing text from real-world images\nstill faces challenges due to the domain shift between synthetic and real-world\ntext images. One of the strategies to eliminate the domain difference without\nmanual annotation is unsupervised domain adaptation (UDA). Due to the\ncharacteristic of sequential labeling tasks, most popular UDA methods cannot be\ndirectly applied to text recognition. To tackle this problem, we proposed a UDA\nmethod with minimizing latent entropy on sequence-to-sequence attention-based\nmodels with classbalanced self-paced learning. Our experiments show that our\nproposed framework achieves better recognition results than the existing\nmethods on most UDA text recognition benchmarks. All codes are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ren Yeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Self-Supervised Learning for Semantic Segmentation. (arXiv:2202.11981v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11981","description":"<p>In this work, we present a fully self-supervised framework for semantic\nsegmentation(FS^4). A fully bootstrapped strategy for semantic segmentation,\nwhich saves efforts for the huge amount of annotation, is crucial for building\ncustomized models from end-to-end for open-world domains. This application is\neagerly needed in realistic scenarios. Even though recent self-supervised\nsemantic segmentation methods have gained great progress, these works however\nheavily depend on the fully-supervised pretrained model and make it impossible\na fully self-supervised pipeline. To solve this problem, we proposed a\nbootstrapped training scheme for semantic segmentation, which fully leveraged\nthe global semantic knowledge for self-supervision with our proposed PGG\nstrategy and CAE module. In particular, we perform pixel clustering and\nassignments for segmentation supervision. Preventing it from clustering a mess,\nwe proposed 1) a pyramid-global-guided (PGG) training strategy to supervise the\nlearning with pyramid image/patch-level pseudo labels, which are generated by\ngrouping the unsupervised features. The stable global and pyramid semantic\npseudo labels can prevent the segmentation from learning too many clutter\nregions or degrading to one background region; 2) in addition, we proposed\ncontext-aware embedding (CAE) module to generate global feature embedding in\nview of its neighbors close both in space and appearance in a non-trivial way.\nWe evaluate our method on the large-scale COCO-Stuff dataset and achieved 7.19\nmIoU improvements on both things and stuff objects\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Q/0/1/0/all/0/1\">Qi Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-QGN: Navigation Map from a Monocular Camera using Quadtree Generating Networks. (arXiv:2202.11982v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11982","description":"<p>Monocular depth estimation has been a popular area of research for several\nyears, especially since self-supervised networks have shown increasingly good\nresults in bridging the gap with supervised and stereo methods. However, these\napproaches focus their interest on dense 3D reconstruction and sometimes on\ntiny details that are superfluous for autonomous navigation. In this paper, we\npropose to address this issue by estimating the navigation map under a quadtree\nrepresentation. The objective is to create an adaptive depth map prediction\nthat only extract details that are essential for the obstacle avoidance. Other\n3D space which leaves large room for navigation will be provided with\napproximate distance. Experiment on KITTI dataset shows that our method can\nsignificantly reduce the number of output information without major loss of\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1\">Daniel Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morel_O/0/1/0/all/0/1\">Olivier Morel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasseur_P/0/1/0/all/0/1\">Pascal Vasseur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demonceaux_C/0/1/0/all/0/1\">C&#xe9;dric Demonceaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIAOTracker: A comprehensive framework for MCMOT with global information and optimizing strategies in VisDrone 2021. (arXiv:2202.11983v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11983","description":"<p>In recent years, algorithms for multiple object tracking tasks have benefited\nfrom great progresses in deep models and video quality. However, in challenging\nscenarios like drone videos, they still suffer from problems, such as small\nobjects, camera movements and view changes. In this paper, we propose a new\nmultiple object tracker, which employs Global Information And some Optimizing\nstrategies, named GIAOTracker. It consists of three stages, i.e., online\ntracking, global link and post-processing. Given detections in every frame, the\nfirst stage generates reliable tracklets using information of camera motion,\nobject motion and object appearance. Then they are associated into trajectories\nby exploiting global clues and refined through four post-processing methods.\nWith the effectiveness of the three stages, GIAOTracker achieves\nstate-of-the-art performance on the VisDrone MOT dataset and wins the 3rd place\nin the VisDrone2021 MOT Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yunhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Junfeng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhihang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junhao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Actor-centric Human-object Interaction Detection. (arXiv:2202.11998v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11998","description":"<p>While Human-Object Interaction(HOI) Detection has achieved tremendous\nadvances in recent, it still remains challenging due to complex interactions\nwith multiple humans and objects occurring in images, which would inevitably\nlead to ambiguities. Most existing methods either generate all human-object\npair candidates and infer their relationships by cropped local features\nsuccessively in a two-stage manner, or directly predict interaction points in a\none-stage procedure. However, the lack of spatial configurations or reasoning\nsteps of two- or one- stage methods respectively limits their performance in\nsuch complex scenes. To avoid this ambiguity, we propose a novel actor-centric\nframework. The main ideas are that when inferring interactions: 1) the\nnon-local features of the entire image guided by actor position are obtained to\nmodel the relationship between the actor and context, and then 2) we use an\nobject branch to generate pixel-wise interaction area prediction, where the\ninteraction area denotes the object central area. Moreover, we also use an\nactor branch to get interaction prediction of the actor and propose a novel\ncomposition strategy based on center-point indexing to generate the final HOI\nprediction. Thanks to the usage of the non-local features and the\npartly-coupled property of the human-objects composition strategy, our proposed\nframework can detect HOI more accurately especially for complex images.\nExtensive experimental results show that our method achieves the\nstate-of-the-art on the challenging V-COCO and HICO-DET benchmarks and is more\nrobust especially in multiple persons and/or objects scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kunlun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Leizhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenhui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Luxin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xu Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rare Gems: Finding Lottery Tickets at Initialization. (arXiv:2202.12002v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12002","description":"<p>It has been widely observed that large neural networks can be pruned to a\nsmall fraction of their original size, with little loss in accuracy, by\ntypically following a time-consuming \"train, prune, re-train\" approach. Frankle\n&amp; Carbin (2018) conjecture that we can avoid this by training lottery tickets,\ni.e., special sparse subnetworks found at initialization, that can be trained\nto high accuracy. However, a subsequent line of work presents concrete evidence\nthat current algorithms for finding trainable networks at initialization, fail\nsimple baseline comparisons, e.g., against training random sparse subnetworks.\nFinding lottery tickets that train to better accuracy compared to simple\nbaselines remains an open problem. In this work, we partially resolve this open\nproblem by discovering rare gems: subnetworks at initialization that attain\nconsiderable accuracy, even before training. Refining these rare gems - \"by\nmeans of fine-tuning\" - beats current baselines and leads to accuracy\ncompetitive or better than magnitude pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1\">Kartik Sreenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grinde_M/0/1/0/all/0/1\">Matthew Grinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagle_A/0/1/0/all/0/1\">Alliot Nagle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Merge Tokens in Vision Transformers. (arXiv:2202.12015v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12015","description":"<p>Transformers are widely applied to solve natural language understanding and\ncomputer vision tasks. While scaling up these architectures leads to improved\nperformance, it often comes at the expense of much higher computational costs.\nIn order for large-scale models to remain practical in real-world systems,\nthere is a need for reducing their computational overhead. In this work, we\npresent the PatchMerger, a simple module that reduces the number of patches or\ntokens the network has to process by merging them between two consecutive\nintermediate layers. We show that the PatchMerger achieves a significant\nspeedup across various model sizes while matching the original performance both\nupstream and downstream after fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Renggli_C/0/1/0/all/0/1\">Cedric Renggli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_A/0/1/0/all/0/1\">Andr&#xe9; Susano Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1\">Joan Puigcerver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1\">Carlos Riquelme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing generalisability of deep learning-based polyp detection and segmentation methods through a computer vision challenge. (arXiv:2202.12031v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12031","description":"<p>Polyps are well-known cancer precursors identified by colonoscopy. However,\nvariability in their size, location, and surface largely affect identification,\nlocalisation, and characterisation. Moreover, colonoscopic surveillance and\nremoval of polyps (referred to as polypectomy ) are highly operator-dependent\nprocedures. There exist a high missed detection rate and incomplete removal of\ncolonic polyps due to their variable nature, the difficulties to delineate the\nabnormality, the high recurrence rates, and the anatomical topography of the\ncolon. There have been several developments in realising automated methods for\nboth detection and segmentation of these polyps using machine learning.\nHowever, the major drawback in most of these methods is their ability to\ngeneralise to out-of-sample unseen datasets that come from different centres,\nmodalities and acquisition systems. To test this hypothesis rigorously we\ncurated a multi-centre and multi-population dataset acquired from multiple\ncolonoscopy systems and challenged teams comprising machine learning experts to\ndevelop robust automated detection and segmentation methods as part of our\ncrowd-sourcing Endoscopic computer vision challenge (EndoCV) 2021. In this\npaper, we analyse the detection results of the four top (among seven) teams and\nthe segmentation results of the five top teams (among 16). Our analyses\ndemonstrate that the top-ranking teams concentrated on accuracy (i.e., accuracy\n&gt; 80% on overall Dice score on different validation sets) over real-time\nperformance required for clinical applicability. We further dissect the methods\nand provide an experiment-based hypothesis that reveals the need for improved\ngeneralisability to tackle diversity present in multi-centre datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghatwary_N/0/1/0/all/0/1\">Noha Ghatwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isik_Polat_E/0/1/0/all/0/1\">Ece Isik-Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballester_M/0/1/0/all/0/1\">Miguel-&#xc1;ngel Gonz&#xe1;lez Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hicks_S/0/1/0/all/0/1\">Steven Hicks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_S/0/1/0/all/0/1\">Sahadev Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Ziyi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_T/0/1/0/all/0/1\">Tianyuan Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">ChengHui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">JiangPeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_D/0/1/0/all/0/1\">Doyeob Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomar_N/0/1/0/all/0/1\">Nikhil Kumar Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haithmi_M/0/1/0/all/0/1\">Mahmood Haithmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Amr Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daul_C/0/1/0/all/0/1\">Christian Daul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salem_O/0/1/0/all/0/1\">Osama E. Salem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamarque_D/0/1/0/all/0/1\">Dominique Lamarque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannizzaro_R/0/1/0/all/0/1\">Renato Cannizzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Realdon_S/0/1/0/all/0/1\">Stefano Realdon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+East_J/0/1/0/all/0/1\">James E. East</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFFDEX 2.0: A Real-Time Facial Expression Analysis Toolkit. (arXiv:2202.12059v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12059","description":"<p>In this paper we introduce AFFDEX 2.0 - a toolkit for analyzing facial\nexpressions in the wild, that is, it is intended for users aiming to; a)\nestimate the 3D head pose, b) detect facial Action Units (AUs), c) recognize\nbasic emotions and 2 new emotional states (sentimentality and confusion), and\nd) detect high-level expressive metrics like blink and attention. AFFDEX 2.0\nmodels are mainly based on Deep Learning, and are trained using a large-scale\nnaturalistic dataset consisting of thousands of participants from different\ndemographic groups. AFFDEX 2.0 is an enhanced version of our previous toolkit\n[1], that is capable of tracking efficiently faces at more challenging\nconditions, detecting more accurately facial expressions, and recognizing new\nemotional states (sentimentality and confusion). AFFDEX 2.0 can process\nmultiple faces in real time, and is working across the Windows and Linux\nplatforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bishay_M/0/1/0/all/0/1\">Mina Bishay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preston_K/0/1/0/all/0/1\">Kenneth Preston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strafuss_M/0/1/0/all/0/1\">Matthew Strafuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Page_G/0/1/0/all/0/1\">Graham Page</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turcot_J/0/1/0/all/0/1\">Jay Turcot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavadati_M/0/1/0/all/0/1\">Mohammad Mavadati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-Based Affordance Detection via Cyclic Bilateral Interaction. (arXiv:2202.12076v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12076","description":"<p>Affordance detection, which refers to perceiving objects with potential\naction possibilities in images, is a challenging task since the possible\naffordance depends on the person's purpose in real-world application scenarios.\nThe existing works mainly extract the inherent human-object dependencies from\nimage/video to accommodate affordance properties that change dynamically. In\nthis paper, we explore to perceive affordance from a vision-language\nperspective and consider the challenging phrase-based affordance detection\nproblem,i.e., given a set of phrases describing the action purposes, all the\nobject regions in a scene with the same affordance should be detected. To this\nend, we propose a cyclic bilateral consistency enhancement network (CBCE-Net)\nto align language and vision features progressively. Specifically, the\npresented CBCE-Net consists of a mutual guided vision-language module that\nupdates the common features of vision and language in a progressive manner, and\na cyclic interaction module (CIM) that facilitates the perception of possible\ninteraction with objects in a cyclic manner. In addition, we extend the public\nPurpose-driven Affordance Dataset (PAD) by annotating affordance categories\nwith short phrases. The contrastive experimental results demonstrate the\nsuperiority of our method over nine typical methods from four relevant fields\nin terms of both objective metrics and visual quality. The related code and\ndataset will be released at \\url{https://github.com/lulsheng/CBCE-Net}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liangsheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data variation-aware medical image segmentation. (arXiv:2202.12099v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12099","description":"<p>Deep learning algorithms have become the golden standard for segmentation of\nmedical imaging data. In most works, the variability and heterogeneity of real\nclinical data is acknowledged to still be a problem. One way to automatically\novercome this is to capture and exploit this variation explicitly. Here, we\npropose an approach that improves on our previous work in this area and explain\nhow it potentially can improve clinical acceptance of (semi-)automatic\nsegmentation methods. In contrast to a standard neural network that produces\none segmentation, we propose to use a multi-pathUnet network that produces\nmultiple segmentation variants, presumably corresponding to the variations that\nreside in the dataset. Different paths of the network are trained on disjoint\ndata subsets. Because a priori it may be unclear what variations exist in the\ndata, the subsets should be automatically determined. This is achieved by\nsearching for the best data partitioning with an evolutionary optimization\nalgorithm. Because each network path can become more specialized when trained\non a more homogeneous data subset, better segmentation quality can be achieved.\nIn practical usage, various automatically produced segmentations can be\npresented to a medical expert, from which the preferred segmentation can be\nselected. In experiments with a real clinical dataset of CT scans with prostate\nsegmentations, our approach provides an improvement of several percentage\npoints in terms of Dice and surface Dice coefficients compared to when all\nnetwork paths are trained on all training data. Noticeably, the largest\nimprovement occurs in the upper part of the prostate that is known to be most\nprone to inter-observer segmentation variation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dushatskiy_A/0/1/0/all/0/1\">Arkadiy Dushatskiy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lowe_G/0/1/0/all/0/1\">Gerry Lowe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFusionMOT: A 3D Multi-Object Tracking Framework Based on Camera-LiDAR Fusion with Deep Association. (arXiv:2202.12100v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12100","description":"<p>In the recent literature, on the one hand, many 3D multi-object tracking\n(MOT) works have focused on tracking accuracy and neglected computation speed,\ncommonly by designing rather complex cost functions and feature extractors. On\nthe other hand, some methods have focused too much on computation speed at the\nexpense of tracking accuracy. In view of these issues, this paper proposes a\nrobust and fast camera-LiDAR fusion-based MOT method that achieves a good\ntrade-off between accuracy and speed. Relying on the characteristics of camera\nand LiDAR sensors, an effective deep association mechanism is designed and\nembedded in the proposed MOT method. This association mechanism realizes\ntracking of an object in a 2D domain when the object is far away and only\ndetected by the camera, and updating of the 2D trajectory with 3D information\nobtained when the object appears in the LiDAR field of view to achieve a smooth\nfusion of 2D and 3D trajectories. Extensive experiments based on the KITTI\ndataset indicate that our proposed method presents obvious advantages over the\nstate-of-the-art MOT methods in terms of both tracking accuracy and processing\nspeed. Our code is made publicly available for the benefit of the community\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chunyun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhankun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Ying Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiawei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-based Network for Deformable Medical Image Registration. (arXiv:2202.12104v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12104","description":"<p>Deformable medical image registration plays an important role in clinical\ndiagnosis and treatment. Recently, the deep learning (DL) based image\nregistration methods have been widely investigated and showed excellent\nperformance in computational speed. However, these methods cannot provide\nenough registration accuracy because of insufficient ability in representing\nboth the global and local features of the moving and fixed images. To address\nthis issue, this paper has proposed the transformer based image registration\nmethod. This method uses the distinctive transformer to extract the global and\nlocal image features for generating the deformation fields, based on which the\nregistered image is produced in an unsupervised way. Our method can improve the\nregistration accuracy effectively by means of self-attention mechanism and\nbi-level information flow. Experimental results on such brain MR image datasets\nas LPBA40 and OASIS-1 demonstrate that compared with several traditional and DL\nbased registration methods, our method provides higher registration accuracy in\nterms of dice values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_W/0/1/0/all/0/1\">Wen Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xuming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Robust Monocular Depth Estimation For Outdoor Environment Via Monochrome And Color Camera Fusion. (arXiv:2202.12108v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12108","description":"<p>Depth estimation plays a important role in SLAM, odometry, and autonomous\ndriving. Especially, monocular depth estimation is profitable technology\nbecause of its low cost, memory, and computation. However, it is not a\nsufficiently predicting depth map due to a camera often failing to get a clean\nimage because of light conditions. To solve this problem, various sensor fusion\nmethod has been proposed. Even though it is a powerful method, sensor fusion\nrequires expensive sensors, additional memory, and high computational\nperformance.\n</p>\n<p>In this paper, we present color image and monochrome image pixel-level fusion\nand stereo matching with partially enhanced correlation coefficient\nmaximization. Our methods not only outperform the state-of-the-art works across\nall metrics but also efficient in terms of cost, memory, and computation. We\nalso validate the effectiveness of our design with an ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyeonsoo Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1\">Yeongmin Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Younkwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slow-Fast Visual Tempo Learning for Video-based Action Recognition. (arXiv:2202.12116v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12116","description":"<p>Action visual tempo characterizes the dynamics and the temporal scale of an\naction, which is helpful to distinguish human actions that share high\nsimilarities in visual dynamics and appearance. Previous methods capture the\nvisual tempo either by sampling raw videos with multiple rates, which requires\na costly multi-layer network to handle each rate, or by hierarchically sampling\nbackbone features, which relies heavily on high-level features that miss\nfine-grained temporal dynamics. In this work, we propose a Temporal Correlation\nModule (TCM), which can be easily embedded into the current action recognition\nbackbones in a plug-in-and-play manner, to extract action visual tempo from\nlow-level backbone features at single-layer remarkably. Specifically, our TCM\ncontains two main components: a Multi-scale Temporal Dynamics Module (MTDM) and\na Temporal Attention Module (TAM). MTDM applies a correlation operation to\nlearn pixel-wise fine-grained temporal dynamics for both fast-tempo and\nslow-tempo. TAM adaptively emphasizes expressive features and suppresses\ninessential ones via analyzing the global information across various tempos.\nExtensive experiments conducted on several action recognition benchmarks, e.g.\nSomething-Something V1 &amp; V2, Kinetics-400, UCF-101, and HMDB-51, have\ndemonstrated that the proposed TCM is effective to promote the performance of\nthe existing video-based action recognition models for a large margin. The\nsource code is publicly released at https://github.com/zphyix/TCM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel unsupervised covid lung lesion segmentation based on the lung tissue identification. (arXiv:2202.12148v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12148","description":"<p>This study aimed to evaluate the performance of a novel unsupervised deep\nlearning-based framework for automated infections lesion segmentation from CT\nimages of Covid patients. In the first step, two residual networks were\nindependently trained to identify the lung tissue for normal and Covid patients\nin a supervised manner. These two models, referred to as DL-Covid and DL-Norm\nfor Covid-19 and normal patients, respectively, generate the voxel-wise\nprobability maps for lung tissue identification. To detect Covid lesions, the\nCT image of the Covid patient is processed by the DL-Covid and DL-Norm models\nto obtain two lung probability maps. Since the DL-Norm model is not familiar\nwith Covid infections within the lung, this model would assign lower\nprobabilities to the lesions than the DL-Covid. Hence, the probability maps of\nthe Covid infections could be generated through the subtraction of the two lung\nprobability maps obtained from the DL-Covid and DL-Norm models. Manual lesion\nsegmentation of 50 Covid-19 CT images was used to assess the accuracy of the\nunsupervised lesion segmentation approach. The Dice coefficients of 0.985 and\n0.978 were achieved for the lung segmentation of normal and Covid patients in\nthe external validation dataset, respectively. Quantitative results of\ninfection segmentation by the proposed unsupervised method showed the Dice\ncoefficient and Jaccard index of 0.67 and 0.60, respectively. Quantitative\nevaluation of the proposed unsupervised approach for Covid-19 infectious lesion\nsegmentation showed relatively satisfactory results. Since this framework does\nnot require any annotated dataset, it could be used to generate very large\ntraining samples for the supervised machine learning algorithms dedicated to\nnoisy and/or weakly annotated datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khah_F/0/1/0/all/0/1\">Faeze Gholamian Khah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mostafapour_S/0/1/0/all/0/1\">Samaneh Mostafapour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shojaerazavi_S/0/1/0/all/0/1\">Seyedjafar Shojaerazavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdi_Goushbolagh_N/0/1/0/all/0/1\">Nouraddin Abdi-Goushbolagh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arabi_H/0/1/0/all/0/1\">Hossein Arabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v1 [cs.CR])","link":"http://arxiv.org/abs/2202.12154","description":"<p>Trojan attacks on deep neural networks are both dangerous and surreptitious.\nOver the past few years, Trojan attacks have advanced from using only a simple\ntrigger and targeting only one class to using many sophisticated triggers and\ntargeting multiple classes. However, Trojan defenses have not caught up with\nthis development. Most defense methods still make out-of-date assumptions about\nTrojan triggers and target classes, thus, can be easily circumvented by modern\nTrojan attacks. In this paper, we advocate general defenses that are effective\nand robust against various Trojan attacks and propose two novel \"filtering\"\ndefenses with these characteristics called Variational Input Filtering (VIF)\nand Adversarial Input Filtering (AIF). VIF and AIF leverage variational\ninference and adversarial training respectively to purify all potential Trojan\ntriggers in the input at run time without making any assumption about their\nnumbers and forms. We further extend \"filtering\" to\n\"filtering-then-contrasting\" - a new defense mechanism that helps avoid the\ndrop in classification accuracy on clean data caused by filtering. Extensive\nexperimental results show that our proposed defenses significantly outperform 4\nwell-known defenses in mitigating 5 different Trojan attacks including the two\nstate-of-the-art which defeat many strong defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1\">Kien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harikumar_H/0/1/0/all/0/1\">Haripriya Harikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susilo_W/0/1/0/all/0/1\">Willy Susilo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring CLEVRness: Blackbox testing of Visual Reasoning Models. (arXiv:2202.12162v1 [cs.LG])","link":"http://arxiv.org/abs/2202.12162","description":"<p>How can we measure the reasoning capabilities of intelligence systems? Visual\nquestion answering provides a convenient framework for testing the model's\nabilities by interrogating the model through questions about the scene.\nHowever, despite scores of various visual QA datasets and architectures, which\nsometimes yield even a super-human performance, the question of whether those\narchitectures can actually reason remains open to debate. To answer this, we\nextend the visual question answering framework and propose the following\nbehavioral test in the form of a two-player game. We consider black-box neural\nmodels of CLEVR. These models are trained on a diagnostic dataset benchmarking\nreasoning. Next, we train an adversarial player that re-configures the scene to\nfool the CLEVR model. We show that CLEVR models, which otherwise could perform\nat a human level, can easily be fooled by our agent. Our results put in doubt\nwhether data-driven approaches can do reasoning without exploiting the numerous\nbiases that are often present in those datasets. Finally, we also propose a\ncontrolled experiment measuring the efficiency of such models to learn and\nperform reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mouselinos_S/0/1/0/all/0/1\">Spyridon Mouselinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in Medical Image Analysis: A Review. (arXiv:2202.12165v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12165","description":"<p>Transformers have dominated the field of natural language processing, and\nrecently impacted the computer vision area. In the field of medical image\nanalysis, Transformers have also been successfully applied to full-stack\nclinical applications, including image synthesis/reconstruction, registration,\nsegmentation, detection, and diagnosis. Our paper presents both a position\npaper and a primer, promoting awareness and application of Transformers in the\nfield of medical image analysis. Specifically, we first overview the core\nconcepts of the attention mechanism built into Transformers and other basic\ncomponents. Second, we give a new taxonomy of various Transformer architectures\ntailored for medical image applications and discuss their limitations. Within\nthis review, we investigate key challenges revolving around the use of\nTransformers in different learning paradigms, improving the model efficiency,\nand their coupling with other techniques. We hope this review can give a\ncomprehensive picture of Transformers to the readers in the field of medical\nimage analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kelei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chen Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zihao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreeSOLO: Learning to Segment Objects without Annotations. (arXiv:2202.12181v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12181","description":"<p>Instance segmentation is a fundamental vision task that aims to recognize and\nsegment each object in an image. However, it requires costly annotations such\nas bounding boxes and segmentation masks for learning. In this work, we propose\na fully unsupervised learning method that learns class-agnostic instance\nsegmentation without any annotations. We present FreeSOLO, a self-supervised\ninstance segmentation framework built on top of the simple instance\nsegmentation method SOLO. Our method also presents a novel localization-aware\npre-training framework, where objects can be discovered from complicated scenes\nin an unsupervised manner. FreeSOLO achieves 9.8% AP_{50} on the challenging\nCOCO dataset, which even outperforms several segmentation proposal methods that\nuse manual annotations. For the first time, we demonstrate unsupervised\nclass-agnostic instance segmentation successfully. FreeSOLO's box localization\nsignificantly outperforms state-of-the-art unsupervised object\ndetection/discovery methods, with about 100% relative improvements in COCO AP.\nFreeSOLO further demonstrates superiority as a strong pre-training method,\noutperforming state-of-the-art self-supervised pre-training methods by +9.8% AP\nwhen fine-tuning instance segmentation with only 5% COCO masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Distilled StyleGAN: Towards Generation from Internet Photos. (arXiv:2202.12211v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12211","description":"<p>StyleGAN is known to produce high-fidelity images, while also offering\nunprecedented semantic editing. However, these fascinating abilities have been\ndemonstrated only on a limited set of datasets, which are usually structurally\naligned and well curated. In this paper, we show how StyleGAN can be adapted to\nwork on raw uncurated images collected from the Internet. Such image\ncollections impose two main challenges to StyleGAN: they contain many outlier\nimages, and are characterized by a multi-modal distribution. Training StyleGAN\non such raw image collections results in degraded image synthesis quality. To\nmeet these challenges, we proposed a StyleGAN-based self-distillation approach,\nwhich consists of two main components: (i) A generative-based self-filtering of\nthe dataset to eliminate outlier images, in order to generate an adequate\ntraining set, and (ii) Perceptual clustering of the generated images to detect\nthe inherent data modalities, which are then employed to improve StyleGAN's\n\"truncation trick\" in the image synthesis process. The presented technique\nenables the generation of high-quality images, while minimizing the loss in\ndiversity of the data. Through qualitative and quantitative evaluation, we\ndemonstrate the power of our approach to new challenging and diverse domains\ncollected from the Internet. New datasets and pre-trained models are available\nat https://self-distilled-stylegan.github.io/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mokady_R/0/1/0/all/0/1\">Ron Mokady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarom_M/0/1/0/all/0/1\">Michal Yarom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tov_O/0/1/0/all/0/1\">Omer Tov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_O/0/1/0/all/0/1\">Oran Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1\">Inbar Mosseri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparative study of in-air trajectories at short and long distances in online handwriting. (arXiv:2202.12237v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12237","description":"<p>Introduction Existing literature about online handwriting analysis to support\npathology diagnosis has taken advantage of in-air trajectories. A similar\nsituation occurred in biometric security applications where the goal is to\nidentify or verify an individual using his signature or handwriting. These\nstudies do not consider the distance of the pen tip to the writing surface.\nThis is due to the fact that current acquisition devices do not provide height\nformation. However, it is quite straightforward to differentiate movements at\ntwo different heights: a) short distance: height lower or equal to 1 cm above a\nsurface of digitizer, the digitizer provides x and y coordinates. b) long\ndistance: height exceeding 1 cm, the only information available is a time stamp\nthat indicates the time that a specific stroke has spent at long distance.\nAlthough short distance has been used in several papers, long distances have\nbeen ignored and will be investigated in this paper. Methods In this paper, we\nwill analyze a large set of databases (BIOSECURID, EMOTHAW, PaHaW,\nOxygen-Therapy and SALT), which contain a total amount of 663 users and 17951\nfiles. We have specifically studied: a) the percentage of time spent\non-surface, in-air at short distance, and in-air at long distance for different\nuser profiles (pathological and healthy users) and different tasks; b) The\npotential use of these signals to improve classification rates. Results and\nconclusions Our experimental results reveal that long-distance movements\nrepresent a very small portion of the total execution time (0.5 % in the case\nof signatures and 10.4% for uppercase words of BIOSECUR-ID, which is the\nlargest database). In addition, significant differences have been found in the\ncomparison of pathological versus control group for letter l in PaHaW database\n(p=0.0157) and crossed pentagons in SALT database (p=0.0122)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Martinez_C/0/1/0/all/0/1\">Carlos Alonso-Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Jiri Mekyska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-line signature verification system with failure to enroll managing. (arXiv:2202.12242v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12242","description":"<p>In this paper we simulate a real biometric verification system based on\non-line signatures. For this purpose we have split the MCYT signature database\nin three subsets: one for classifier training, another for system adjustment\nand a third one for system testing simulating enrollment and verification. This\ncontext corresponds to a real operation, where a new user tries to enroll an\nexisting system and must be automatically guided by the system in order to\ndetect the failure to enroll situations. The main contribution of this work is\nthe management of failure to enroll situations by means of a new proposal,\ncalled intelligent enrollment, which consists of consistency checking in order\nto automatically reject low quality samples. This strategy lets to enhance the\nverification errors up to 22% when leaving out 8% of the users. In this\nsituation 8% of the people cannot be enrolled in the system and must be\nverified by other biometrics or by human abilities. These people are identified\nwith intelligent enrollment and the situation can be thus managed. In addition\nwe also propose a DCT-based feature extractor with threshold coding and\ndiscriminability criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fabregas_J/0/1/0/all/0/1\">Joan Fabregas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMOTHAW: A novel database for emotional state recognition from handwriting. (arXiv:2202.12245v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12245","description":"<p>The detection of negative emotions through daily activities such as\nhandwriting is useful for promoting well-being. The spread of human-machine\ninterfaces such as tablets makes the collection of handwriting samples easier.\nIn this context, we present a first publicly available handwriting database\nwhich relates emotional states to handwriting, that we call EMOTHAW. This\ndatabase includes samples of 129 participants whose emotional states, namely\nanxiety, depression and stress, are assessed by the Depression Anxiety Stress\nScales (DASS) questionnaire. Seven tasks are recorded through a digitizing\ntablet: pentagons and house drawing, words copied in handprint, circles and\nclock drawing, and one sentence copied in cursive writing. Records consist in\npen positions, on-paper and in-air, time stamp, pressure, pen azimuth and\naltitude. We report our analysis on this database. From collected data, we\nfirst compute measurements related to timing and ductus. We compute separate\nmeasurements according to the position of the writing device: on paper or\nin-air. We analyse and classify this set of measurements (referred to as\nfeatures) using a random forest approach. This latter is a machine learning\nmethod [2], based on an ensemble of decision trees, which includes a feature\nranking process. We use this ranking process to identify the features which\nbest reveal a targeted emotional state.\n</p>\n<p>We then build random forest classifiers associated to each emotional state.\nOur results, obtained from cross-validation experiments, show that the targeted\nemotional states can be identified with accuracies ranging from 60% to 71%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Likforman_Sulem_L/0/1/0/all/0/1\">Laurence Likforman-Sulem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esposito_A/0/1/0/all/0/1\">Anna Esposito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Clemen&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordasco_G/0/1/0/all/0/1\">Gennaro Cordasco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLPnet: A new DNN model and Bengali OCR engine for Automatic License Plate Recognition. (arXiv:2202.12250v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12250","description":"<p>The development of the Automatic License Plate Recognition (ALPR) system has\nreceived much attention for the English license plate. However, despite being\nthe sixth largest population around the world, no significant progress can be\ntracked in the Bengali language countries or states for the ALPR system\naddressing their more alarming traffic management with inadequate road-safety\nmeasures. This paper reports a computationally efficient and reasonably\naccurate Automatic License Plate Recognition (ALPR) system for Bengali\ncharacters with a new end-to-end DNN model that we call Bengali License Plate\nNetwork(BLPnet). The cascaded architecture for detecting vehicle regions prior\nto vehicle license plate (VLP) in the model is proposed to eliminate false\npositives resulting in higher detection accuracy of VLP. Besides, a lower set\nof trainable parameters is considered for reducing the computational cost\nmaking the system faster and more compatible for a real-time application. With\na Computational Neural Network (CNN)based new Bengali OCR engine and\nword-mapping process, the model is characters rotation invariant, and can\nreadily extract, detect and output the complete license plate number of a\nvehicle. The model feeding with17 frames per second (fps) on real-time video\nfootage can detect a vehicle with the Mean Squared Error (MSE) of 0.0152, and\nthe mean license plate character recognition accuracy of 95%. While compared to\nthe other models, an improvement of 5% and 20% were recorded for the BLPnetover\nthe prominent YOLO-based ALPR model and the Tesseract model for the\nnumber-plate detection accuracy and time requirement, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onim_M/0/1/0/all/0/1\">Md. Saif Hassan Onim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyeem_H/0/1/0/all/0/1\">Hussain Nyeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Koushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mahmudul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishmam_A/0/1/0/all/0/1\">Abtahi Ishmam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akif_M/0/1/0/all/0/1\">Md. Akiful Hoque Akif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovi_T/0/1/0/all/0/1\">Tareque Bashar Ovi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISDA: Position-Aware Instance Segmentation with Deformable Attention. (arXiv:2202.12251v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12251","description":"<p>Most instance segmentation models are not end-to-end trainable due to either\nthe incorporation of proposal estimation (RPN) as a pre-processing or\nnon-maximum suppression (NMS) as a post-processing. Here we propose a novel\nend-to-end instance segmentation method termed ISDA. It reshapes the task into\npredicting a set of object masks, which are generated via traditional\nconvolution operation with learned position-aware kernels and features of\nobjects. Such kernels and features are learned by leveraging a deformable\nattention network with multi-scale representation. Thanks to the introduced\nset-prediction mechanism, the proposed method is NMS-free. Empirically, ISDA\noutperforms Mask R-CNN (the strong baseline) by 2.6 points on MS-COCO, and\nachieves leading performance compared with recent models. Code will be\navailable soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_K/0/1/0/all/0/1\">Kaining Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Cong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pengfei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Method for Waste Segregation using Convolutional Neural Networks. (arXiv:2202.12258v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12258","description":"<p>Segregation of garbage is a primary concern in many nations across the world.\nEven though we are in the modern era, many people still do not know how to\ndistinguish between organic and recyclable waste. It is because of this that\nthe world is facing a major crisis of waste disposal. In this paper, we try to\nuse deep learning algorithms to help solve this problem of waste\nclassification. The waste is classified into two categories like organic and\nrecyclable. Our proposed model achieves an accuracy of 94.9%. Although the\nother two models also show promising results, the Proposed Model stands out\nwith the greatest accuracy. With the help of deep learning, one of the greatest\nobstacles to efficient waste management can finally be removed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Jash Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamat_S/0/1/0/all/0/1\">Sagar Kamat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from the Pros: Extracting Professional Goalkeeper Technique from Broadcast Footage. (arXiv:2202.12259v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12259","description":"<p>As an amateur goalkeeper playing grassroots soccer, who better to learn from\nthan top professional goalkeepers? In this paper, we harness computer vision\nand machine learning models to appraise the save technique of professionals in\na way those at lower levels can learn from. We train an unsupervised machine\nlearning model using 3D body pose data extracted from broadcast footage to\nlearn professional goalkeeper technique. Then, an \"expected saves\" model is\ndeveloped, from which we can identify the optimal goalkeeper technique in\ndifferent match contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wear_M/0/1/0/all/0/1\">Matthew Wear</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beal_R/0/1/0/all/0/1\">Ryan Beal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthews_T/0/1/0/all/0/1\">Tim Matthews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norman_T/0/1/0/all/0/1\">Tim Norman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchurn_S/0/1/0/all/0/1\">Sarvapali Ramchurn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inflation of test accuracy due to data leakage in deep learning-based classification of OCT images. (arXiv:2202.12267v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12267","description":"<p>In the application of deep learning on optical coherence tomography (OCT)\ndata, it is common to train classification networks using 2D images originating\nfrom volumetric data. Given the micrometer resolution of OCT systems,\nconsecutive images are often very similar in both visible structures and noise.\nThus, an inappropriate data split can result in overlap between the training\nand testing sets, with a large portion of the literature overlooking this\naspect. In this study, the effect of improper dataset splitting on model\nevaluation is demonstrated for two classification tasks using two OCT\nopen-access datasets extensively used in the literature, Kermany's\nophthalmology dataset and AIIMS breast tissue dataset. Our results show that\nthe classification accuracy is inflated by 3.9 to 26 percentage units for\nmodels tested on a dataset with improper splitting, highlighting the\nconsiderable effect of dataset handling on model evaluation. This study intends\nto raise awareness on the importance of dataset splitting for research on deep\nlearning using OCT data and volumetric data in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tampu_I/0/1/0/all/0/1\">Iulian Emil Tampu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_A/0/1/0/all/0/1\">Anders Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haj_Hosseini_N/0/1/0/all/0/1\">Neda Haj-Hosseini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Feature Attribution Methods in the Image Domain. (arXiv:2202.12270v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12270","description":"<p>Feature attribution maps are a popular approach to highlight the most\nimportant pixels in an image for a given prediction of a model. Despite a\nrecent growth in popularity and available methods, little attention is given to\nthe objective evaluation of such attribution maps. Building on previous work in\nthis domain, we investigate existing metrics and propose new variants of\nmetrics for the evaluation of attribution maps. We confirm a recent finding\nthat different attribution metrics seem to measure different underlying\nconcepts of attribution maps, and extend this finding to a larger selection of\nattribution metrics. We also find that metric results on one dataset do not\nnecessarily generalize to other datasets, and methods with desirable\ntheoretical properties such as DeepSHAP do not necessarily outperform\ncomputationally cheaper alternatives. Based on these findings, we propose a\ngeneral benchmarking approach to identify the ideal feature attribution method\nfor a given use case. Implementations of attribution metrics and our\nexperiments are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gevaert_A/0/1/0/all/0/1\">Arne Gevaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_A/0/1/0/all/0/1\">Axel-Jan Rousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_T/0/1/0/all/0/1\">Thijs Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valkenborg_D/0/1/0/all/0/1\">Dirk Valkenborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeys_Y/0/1/0/all/0/1\">Yvan Saeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorizer: A Scalable Interpretable Approach to Context Modeling for Medical Image Segmentation. (arXiv:2202.12295v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12295","description":"<p>Convolutional Neural Networks (CNNs) with U-shaped architectures have\ndominated medical image segmentation, which is crucial for various clinical\npurposes. However, the inherent locality of convolution makes CNNs fail to\nfully exploit global context, essential for better recognition of some\nstructures, e.g., brain lesions. Transformers have recently proved promising\nperformance on vision tasks, including semantic segmentation, mainly due to\ntheir capability of modeling long-range dependencies. Nevertheless, the\nquadratic complexity of attention makes existing Transformer-based models use\nself-attention layers only after somehow reducing the image resolution, which\nlimits the ability to capture global contexts present at higher resolutions.\nTherefore, this work introduces a family of models, dubbed Factorizer, which\nleverages the power of low-rank matrix factorization for constructing an\nend-to-end segmentation model. Specifically, we propose a linearly scalable\napproach to context modeling, formulating Nonnegative Matrix Factorization\n(NMF) as a differentiable layer integrated into a U-shaped architecture. The\nshifted window technique is also utilized in combination with NMF to\neffectively aggregate local information. Factorizers compete favorably with\nCNNs and Transformers in terms of accuracy, scalability, and interpretability,\nachieving state-of-the-art results on the BraTS dataset for brain tumor\nsegmentation, with Dice scores of 79.33%, 83.14%, and 90.16% for enhancing\ntumor, tumor core, and whole tumor, respectively. Highly meaningful NMF\ncomponents give an additional interpretability advantage to Factorizers over\nCNNs and Transformers. Moreover, our ablation studies reveal a distinctive\nfeature of Factorizers that enables a significant speed-up in inference for a\ntrained Factorizer without any extra steps and without sacrificing much\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ashtari_P/0/1/0/all/0/1\">Pooya Ashtari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sima_D/0/1/0/all/0/1\">Diana Sima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lathauwer_L/0/1/0/all/0/1\">Lieven De Lathauwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sappey_Marinierd_D/0/1/0/all/0/1\">Dominique Sappey-Marinierd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_F/0/1/0/all/0/1\">Frederik Maes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huffel_S/0/1/0/all/0/1\">Sabine Van Huffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSGCNet: A Pyramidal Scale and Global Context Guided Network for Dense Object Counting in Remote Sensing Images. (arXiv:2012.03597v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03597","description":"<p>Object counting, which aims to count the accurate number of object instances\nin images, has been attracting more and more attention. However, challenges\nsuch as large scale variation, complex background interference, and non-uniform\ndensity distribution greatly limit the counting accuracy, particularly striking\nin remote sensing imagery. To mitigate the above issues, this paper proposes a\nnovel framework for dense object counting in remote sensing images, which\nincorporates a pyramidal scale module (PSM) and a global context module (GCM),\ndubbed PSGCNet, where PSM is used to adaptively capture multi-scale information\nand GCM is to guide the model to select suitable scales generated from PSM.\nMoreover, a reliable supervision manner improved from Bayesian and Counting\nloss (BCL) is utilized to learn the density probability and then compute the\ncount expectation at each annotation. It can relieve non-uniform density\ndistribution to a certain extent. Extensive experiments on four remote sensing\ncounting datasets demonstrate the effectiveness of the proposed method and the\nsuperiority of it compared with state-of-the-arts. Additionally, experiments\nextended on four commonly used crowd counting datasets further validate the\ngeneralization ability of the model. Code is available at\nhttps://github.com/gaoguangshuai/PSGCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangshuai Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenghui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1\">Qi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation. (arXiv:2012.08512v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.08512","description":"<p>A majority of methods for video frame interpolation compute bidirectional\noptical flow between adjacent frames of a video, followed by a suitable warping\nalgorithm to generate the output frames. However, approaches relying on optical\nflow often fail to model occlusions and complex non-linear motions directly\nfrom the video and introduce additional bottlenecks unsuitable for widespread\ndeployment. We address these limitations with FLAVR, a flexible and efficient\narchitecture that uses 3D space-time convolutions to enable end-to-end learning\nand inference for video frame interpolation. Our method efficiently learns to\nreason about non-linear motions, complex occlusions and temporal abstractions,\nresulting in improved performance on video interpolation, while requiring no\nadditional inputs in the form of optical flow or depth maps. Due to its\nsimplicity, FLAVR can deliver 3x faster inference speed compared to the current\nmost accurate method on multi-frame interpolation without losing interpolation\naccuracy. In addition, we evaluate FLAVR on a wide range of challenging\nsettings and consistently demonstrate superior qualitative and quantitative\nresults compared with prior methods on various popular benchmarks including\nVimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR\nfor video frame interpolation can serve as a useful self-supervised pretext\ntask for action recognition, optical flow estimation, and motion magnification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalluri_T/0/1/0/all/0/1\">Tarun Kalluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Du Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09716","description":"<p>Identifying the status of individual network units is critical for\nunderstanding the mechanism of convolutional neural networks (CNNs). However,\nit is still challenging to reliably give a general indication of unit status,\nespecially for units in different network models. To this end, we propose a\nnovel method for quantitatively clarifying the status of single unit in CNN\nusing algebraic topological tools. Unit status is indicated via the calculation\nof a defined topological-based entropy, called feature entropy, which measures\nthe degree of chaos of the global spatial pattern hidden in the unit for a\ncategory. In this way, feature entropy could provide an accurate indication of\nstatus for units in different networks with diverse situations like\nweight-rescaling operation. Further, we show that feature entropy decreases as\nthe layer goes deeper and shares almost simultaneous trend with loss during\ntraining. We show that by investigating the feature entropy of units on only\ntraining data, it could give discrimination between networks with different\ngeneralization ability from the view of the effectiveness of feature\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Unsupervised Diversity Denoising and Artefact Removal. (arXiv:2104.01374v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.01374","description":"<p>Image denoising and artefact removal are complex inverse problems admitting\nmultiple valid solutions. Unsupervised diversity restoration, that is,\nobtaining a diverse set of possible restorations given a corrupted image, is\nimportant for ambiguity removal in many applications such as microscopy where\npaired data for supervised training are often unobtainable. In real world\napplications, imaging noise and artefacts are typically hard to model, leading\nto unsatisfactory performance of existing unsupervised approaches. This work\npresents an interpretable approach for unsupervised and diverse image\nrestoration. To this end, we introduce a capable architecture called\nHierarchical DivNoising (HDN) based on hierarchical Variational Autoencoder. We\nshow that HDN learns an interpretable multi-scale representation of artefacts\nand we leverage this interpretability to remove imaging artefacts commonly\noccurring in microscopy data. Our method achieves state-of-the-art results on\ntwelve benchmark image denoising datasets while providing access to a whole\ndistribution of sensibly restored solutions. Additionally, we demonstrate on\nthree real microscopy datasets that HDN removes artefacts without supervision,\nbeing the first method capable of doing so while generating multiple plausible\nrestorations all consistent with the given corrupted image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Prakash_M/0/1/0/all/0/1\">Mangal Prakash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delbracio_M/0/1/0/all/0/1\">Mauricio Delbracio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jug_F/0/1/0/all/0/1\">Florian Jug</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error. (arXiv:2105.13343v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.13343","description":"<p>In computer vision, it is standard practice to draw a single sample from the\ndata augmentation procedure for each unique image in the mini-batch. However\nrecent work has suggested drawing multiple samples can achieve higher test\naccuracies. In this work, we provide a detailed empirical evaluation of how the\nnumber of augmentation samples per unique image influences model performance on\nheld out data when training deep ResNets. We demonstrate drawing multiple\nsamples per image consistently enhances the test accuracy achieved for both\nsmall and large batch training. Crucially, this benefit arises even if\ndifferent numbers of augmentations per image perform the same number of\nparameter updates and gradient evaluations (requiring the same total compute).\nAlthough prior work has found variance in the gradient estimate arising from\nsubsampling the dataset has an implicit regularization benefit, our experiments\nsuggest variance which arises from the data augmentation process harms\ngeneralization. We apply these insights to the highly performant NFNet-F5,\nachieving 86.8$\\%$ top-1 w/o extra data on ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1\">Soham De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Samuel L. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid mmWave and Camera System for Long-Range Depth Imaging. (arXiv:2106.07856v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07856","description":"<p>mmWave radars offer excellent depth resolution even at very long ranges owing\nto their high bandwidth. But their angular resolution is at least an\norder-of-magnitude worse than camera and lidar systems. Hence, mmWave radar is\nnot a capable 3-D imaging solution in isolation. We propose Metamoran, a system\nthat combines the complimentary strengths of radar and camera to obtain\naccurate, high resolution depth images over long ranges even in high clutter\nenvironments, all from a single fixed vantage point. Metamoran enables rich\nlong-range depth imaging with applications in security and surveillance,\nroadside safety infrastructure and wide-area mapping. Our approach leverages\nthe high angular resolution from cameras using computer vision techniques,\nincluding image segmentation and monocular depth estimation, to obtain object\nshape. Our core contribution is a method to convert this object shape into an\nRF I/Q equivalent, which we use in a novel radar processing pipeline to help\ndeclutter the scene and capture extremely weak reflections from objects at long\ndistances. We perform a detailed evaluation of Metamoran's depth imaging\ncapabilities in 400 diverse scenes. Our evaluation shows that Metamoran\nestimates the depth of static objects up to 90 m and moving objects up to 305 m\nand with a median error of 28 cm, an improvement of 13$\\times$ compared to a\nnaive radar+camera baseline and 23$\\times$ compared to monocular depth\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhakara_A/0/1/0/all/0/1\">Akarsh Prabhakara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Diana Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munir_S/0/1/0/all/0/1\">Sirajum Munir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankanaryanan_A/0/1/0/all/0/1\">Aswin Sankanaryanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowe_A/0/1/0/all/0/1\">Anthony Rowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Swarun Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13301","description":"<p>Physics perception very often faces the problem that only limited data or\npartial measurements on the scene are available. In this work, we propose a\nstrategy to learn the full state of sloshing liquids from measurements of the\nfree surface. Our approach is based on recurrent neural networks (RNN) that\nproject the limited information available to a reduced-order manifold so as to\nnot only reconstruct the unknown information, but also to be capable of\nperforming fluid reasoning about future scenarios in real time. To obtain\nphysically consistent predictions, we train deep neural networks on the\nreduced-order manifold that, through the employ of inductive biases, ensure the\nfulfillment of the principles of thermodynamics. RNNs learn from history the\nrequired hidden information to correlate the limited information with the\nlatent space where the simulation occurs. Finally, a decoder returns data back\nto the high-dimensional manifold, so as to provide the user with insightful\ninformation in the form of augmented reality. This algorithm is connected to a\ncomputer vision system to test the performance of the proposed methodology with\nreal information, resulting in a system capable of understanding and predicting\nfuture states of the observed fluid in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1\">Beatriz Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">David Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Importance-aware Transferable Adversarial Attacks. (arXiv:2107.14185v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14185","description":"<p>Transferability of adversarial examples is of central importance for\nattacking an unknown model, which facilitates adversarial attacks in more\npractical scenarios, e.g., black-box attacks. Existing transferable attacks\ntend to craft adversarial examples by indiscriminately distorting features to\ndegrade prediction accuracy in a source model without aware of intrinsic\nfeatures of objects in the images. We argue that such brute-force degradation\nwould introduce model-specific local optimum into adversarial examples, thus\nlimiting the transferability. By contrast, we propose the Feature\nImportance-aware Attack (FIA), which disrupts important object-aware features\nthat dominate model decisions consistently. More specifically, we obtain\nfeature importance by introducing the aggregate gradient, which averages the\ngradients with respect to feature maps of the source model, computed on a batch\nof random transforms of the original clean image. The gradients will be highly\ncorrelated to objects of interest, and such correlation presents invariance\nacross different models. Besides, the random transforms will preserve intrinsic\nfeatures of objects and suppress model-specific information. Finally, the\nfeature importance guides to search for adversarial examples towards disrupting\ncritical features, achieving stronger transferability. Extensive experimental\nevaluation demonstrates the effectiveness and superior performance of the\nproposed FIA, i.e., improving the success rate by 9.5% against normally trained\nmodels and 12.8% against defense models as compared to the state-of-the-art\ntransferable attacks. Code is available at: https://github.com/hcguoO0/FIA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengchang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v4 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2109.05539","description":"<p>Brain-inspired computation and information processing alongside compatibility\nwith neuromorphic hardware have made spiking neural networks (SNN) a promising\nmethod for solving learning tasks in machine learning (ML). However, the mere\nuse of spiking neurons is not sufficient for building models that mimic the\nlearning mechanisms in the biological brain; network architecture and learning\nrules are important factors to consider when developing such artificial agents.\nIn this work, inspired by the human visual pathway and the role of dopamine in\nlearning, we propose a reward-modulated locally connected spiking neural\nnetwork, BioLCNet, for visual learning tasks. To extract visual features from\nPoisson-distributed spike trains, we used local filters that are more analogous\nto the biological visual system compared to convolutional filters with weight\nsharing. In the decoding layer, we applied a spike population-based voting\nscheme to determine the decision of the network. We employed\nSpike-timing-dependent plasticity (STDP) for learning the visual features, and\nits reward-modulated variant (R-STDP) for training the decoder based on the\nreward or punishment feedback signal. For evaluation, we first assessed the\nrobustness of our rewarding mechanism to varying target responses in a\nclassical conditioning experiment. Afterwards, we evaluated the performance of\nour network on image classification tasks of MNIST and XOR MNIST datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_E/0/1/0/all/0/1\">Erfan Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1\">Saeed Reza Kheradpisheh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised domain adaptation for cross-modality liver segmentation via joint adversarial learning and self-learning. (arXiv:2109.05664v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05664","description":"<p>Liver segmentation on images acquired using computed tomography (CT) and\nmagnetic resonance imaging (MRI) plays an important role in clinical management\nof liver diseases. Compared to MRI, CT images of liver are more abundant and\nreadily available. However, MRI can provide richer quantitative information of\nthe liver compared to CT. Thus, it is desirable to achieve unsupervised domain\nadaptation for transferring the learned knowledge from the source domain\ncontaining labeled CT images to the target domain containing unlabeled MR\nimages. In this work, we report a novel unsupervised domain adaptation\nframework for cross-modality liver segmentation via joint adversarial learning\nand self-learning. We propose joint semantic-aware and shape-entropy-aware\nadversarial learning with post-situ identification manner to implicitly align\nthe distribution of task-related features extracted from the target domain with\nthose from the source domain. In proposed framework, a network is trained with\nthe above two adversarial losses in an unsupervised manner, and then a mean\ncompleter of pseudo-label generation is employed to produce pseudo-labels to\ntrain the next network (desired model). Additionally, semantic-aware\nadversarial learning and two self-learning methods, including pixel-adaptive\nmask refinement and student-to-partner learning, are proposed to train the\ndesired model. To improve the robustness of the desired model, a low-signal\naugmentation function is proposed to transform MRI images as the input of the\ndesired model to handle hard samples. Using the public data sets, our\nexperiments demonstrated the proposed unsupervised domain adaptation framework\nreached four supervised learning methods with a Dice score 0.912 plus or minus\n0.037 (mean plus or minus standard deviation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Simon Chun-Ho Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weitian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CT-ICP: Real-time Elastic LiDAR Odometry with Loop Closure. (arXiv:2109.12979v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.12979","description":"<p>Multi-beam LiDAR sensors are increasingly used in robotics, particularly with\nautonomous cars for localization and perception tasks, both relying on the\nability to build a precise map of the environment. For this, we propose a new\nreal-time LiDAR-only odometry method called CT-ICP (for Continuous-Time ICP),\ncompleted into a full SLAM with a novel loop detection procedure. The core of\nthis method, is the introduction of the combined continuity in the scan\nmatching, and discontinuity between scans. It allows both the elastic\ndistortion of the scan during the registration for increased precision, and the\nincreased robustness to high frequency motions from the discontinuity.\n</p>\n<p>We build a complete SLAM on top of this odometry, using a fast pure LiDAR\nloop detection based on elevation image 2D matching, providing a pose graph\nwith loop constraints. To show the robustness of the method, we tested it on\nseven datasets: KITTI, KITTI-raw, KITTI-360, KITTI-CARLA, ParisLuco, Newer\nCollege, and NCLT in driving and high-frequency motion scenarios. Both the\nCT-ICP odometry and the loop detection are made available online. CT-ICP is\ncurrently first, among those giving access to a public code, on the KITTI\nodometry leaderboard, with an average Relative Translation Error (RTE) of 0.59%\nand an average time per scan of 60ms on a CPU with a single thread.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dellenbach_P/0/1/0/all/0/1\">Pierre Dellenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1\">Jean-Emmanuel Deschaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquet_B/0/1/0/all/0/1\">Bastien Jacquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1\">Fran&#xe7;ois Goulette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Score-based diffusion models for accelerated MRI. (arXiv:2110.05243v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.05243","description":"<p>Score-based diffusion models provide a powerful way to model images using the\ngradient of the data distribution. Leveraging the learned score function as a\nprior, here we introduce a way to sample data from a conditional distribution\ngiven the measurements, such that the model can be readily used for solving\ninverse problems in imaging, especially for accelerated MRI. In short, we train\na continuous time-dependent score function with denoising score matching. Then,\nat the inference stage, we iterate between numerical SDE solver and data\nconsistency projection step to achieve reconstruction. Our model requires\nmagnitude images only for training, and yet is able to reconstruct\ncomplex-valued data, and even extends to parallel imaging. The proposed method\nis agnostic to sub-sampling patterns, and can be used with any sampling\nschemes. Also, due to its generative nature, our approach can quantify\nuncertainty, which is not possible with standard regression settings. On top of\nall the advantages, our method also has very strong performance, even beating\nthe models trained with full supervision. With extensive experiments, we verify\nthe superiority of our method in terms of quality and practicality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2020 CATARACTS Semantic Segmentation Challenge. (arXiv:2110.10965v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.10965","description":"<p>Surgical scene segmentation is essential for anatomy and instrument\nlocalization which can be further used to assess tissue-instrument interactions\nduring a surgical procedure. In 2017, the Challenge on Automatic Tool\nAnnotation for cataRACT Surgery (CATARACTS) released 50 cataract surgery videos\naccompanied by instrument usage annotations. These annotations included\nframe-level instrument presence information. In 2020, we released pixel-wise\nsemantic annotations for anatomy and instruments for 4670 images sampled from\n25 videos of the CATARACTS training set. The 2020 CATARACTS Semantic\nSegmentation Challenge, which was a sub-challenge of the 2020 MICCAI Endoscopic\nVision (EndoVis) Challenge, presented three sub-tasks to assess participating\nsolutions on anatomical structure and instrument segmentation. Their\nperformance was assessed on a hidden test set of 531 images from 10 videos of\nthe CATARACTS test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luengo_I/0/1/0/all/0/1\">Imanol Luengo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grammatikopoulou_M/0/1/0/all/0/1\">Maria Grammatikopoulou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_R/0/1/0/all/0/1\">Rahim Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walsh_C/0/1/0/all/0/1\">Chris Walsh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nwoye_C/0/1/0/all/0/1\">Chinedu Innocent Nwoye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alapatt_D/0/1/0/all/0/1\">Deepak Alapatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_Z/0/1/0/all/0/1\">Zhen-Liang Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_C/0/1/0/all/0/1\">Chen-Chen Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_G/0/1/0/all/0/1\">Gui-Bin Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_Z/0/1/0/all/0/1\">Zeng-Guang Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ha_H/0/1/0/all/0/1\">Heonjin Ha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiacheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haojie Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1\">Dong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giddwani_B/0/1/0/all/0/1\">Bharat Giddwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hongliang_R/0/1/0/all/0/1\">Ren Hongliang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pissas_T/0/1/0/all/0/1\">Theodoros Pissas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravasio_C/0/1/0/all/0/1\">Claudio Ravasio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huber_M/0/1/0/all/0/1\">Martin Huber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Birch_J/0/1/0/all/0/1\">Jeremy Birch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rio_J/0/1/0/all/0/1\">Joan M.Nunez Do Rio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_L/0/1/0/all/0/1\">Lyndon da Cruz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bergeles_C/0/1/0/all/0/1\">Christos Bergeles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hongyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1\">Fucang Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+KumarTomar_N/0/1/0/all/0/1\">Nikhil KumarTomar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">Pal Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaghela_U/0/1/0/all/0/1\">Uddhav Vaghela</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_J/0/1/0/all/0/1\">Jianyuan Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_H/0/1/0/all/0/1\">Haili Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_F/0/1/0/all/0/1\">Feihong Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Da-Han Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INTERN: A New Learning Paradigm Towards General Vision. (arXiv:2111.08687v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08687","description":"<p>Enormous waves of technological innovations over the past several years,\nmarked by the advances in AI technologies, are profoundly reshaping the\nindustry and the society. However, down the road, a key challenge awaits us,\nthat is, our capability of meeting rapidly-growing scenario-specific demands is\nseverely limited by the cost of acquiring a commensurate amount of training\ndata. This difficult situation is in essence due to limitations of the\nmainstream learning paradigm: we need to train a new model for each new\nscenario, based on a large quantity of well-annotated data and commonly from\nscratch. In tackling this fundamental problem, we move beyond and develop a new\nlearning paradigm named INTERN. By learning with supervisory signals from\nmultiple sources in multiple stages, the model being trained will develop\nstrong generalizability. We evaluate our model on 26 well-known datasets that\ncover four categories of tasks in computer vision. In most cases, our models,\nadapted with only 10% of the training data in the target domain, outperform the\ncounterparts trained with the full set of data, often by a significant margin.\nThis is an important step towards a promising prospect where such a model with\ngeneral vision capability can dramatically reduce our reliance on data, thus\nexpediting the adoption of AI technologies. Furthermore, revolving around our\nnew paradigm, we also introduce a new data system, a new architecture, and a\nnew benchmark, which, together, form a general vision ecosystem to support its\nfuture development in an open and inclusive manner. See project website at\nhttps://opengvlab.shlab.org.cn .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yinan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1\">Jianing Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qinghong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mengya Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gengshi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yichao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenggang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Shuo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lebanon Solar Rooftop Potential Assessment using Buildings Segmentation from Aerial Images. (arXiv:2111.11397v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11397","description":"<p>Estimating the solar rooftop potential of buildings' rooftops at a large\nscale is a fundamental step for every country to utilize its solar power\nefficiently. However, such estimation becomes time-consuming and costly if done\nthrough on-site measurements. This paper uses deep learning-based multi-class\ninstance segmentation to extract buildings' footprints from satellite images.\nHence, we introduce Lebanon's first complete and comprehensive buildings'\nfootprints map. Furthermore, we propose a photovoltaic panels placement\nalgorithm to estimate the solar potential of every rooftop, which results in\nLebanon's first buildings' solar rooftop potential map too. Finally, we report\ntotal and average solar rooftop potential per district and localize regions\ncorresponding to the highest solar rooftop potential yield. Conducted analysis\nreveal solar rooftop potential urban patterns and provide policymakers and key\nstakeholders with tangible insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1\">Hasan Nasrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samhat_A/0/1/0/all/0/1\">Abed Ellatif Samhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faour_G/0/1/0/all/0/1\">Ghaleb Faour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1\">Ali J. Ghandour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Monocular Depth Prediction with Instance Convolutions. (arXiv:2112.01521v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01521","description":"<p>With the advent of deep learning, estimating depth from a single RGB image\nhas recently received a lot of attention, being capable of empowering many\ndifferent applications ranging from path planning for robotics to computational\ncinematography. Nevertheless, while the depth maps are in their entirety fairly\nreliable, the estimates around object discontinuities are still far from\nsatisfactory. This can be contributed to the fact that the convolutional\noperator naturally aggregates features across object discontinuities, resulting\nin smooth transitions rather than clear boundaries. Therefore, in order to\ncircumvent this issue, we propose a novel convolutional operator which is\nexplicitly tailored to avoid feature aggregation of different object parts. In\nparticular, our method is based on estimating per-part depth values by means of\nsuperpixels. The proposed convolutional operator, which we dub \"Instance\nConvolution\", then only considers each object part individually on the basis of\nthe estimated superpixels. Our evaluation with respect to the NYUv2 as well as\nthe iBims dataset clearly demonstrates the superiority of Instance Convolutions\nover the classical convolution at estimating depth around occlusion boundaries,\nwhile producing comparable results elsewhere. Code will be made publicly\navailable upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simsar_E/0/1/0/all/0/1\">Enis Simsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ornek_E/0/1/0/all/0/1\">Evin P&#x131;nar &#xd6;rnek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamo_H/0/1/0/all/0/1\">Helisa Dhamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGM3D: Stereo Guided Monocular 3D Object Detection. (arXiv:2112.01914v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01914","description":"<p>Monocular 3D object detection aims to predict the object location, dimension\nand orientation in 3D space alongside the object category given only a\nmonocular image. It poses a great challenge due to its ill-posed property which\nis critically lack of depth information in the 2D image plane. While there\nexist approaches leveraging off-the-shelve depth estimation or relying on LiDAR\nsensors to mitigate this problem, the dependence on the additional depth model\nor expensive equipment severely limits their scalability to generic 3D\nperception. In this paper, we propose a stereo-guided monocular 3D object\ndetection framework, dubbed SGM3D, adapting the robust 3D features learned from\nstereo inputs to enhance the feature for monocular detection. We innovatively\npresent a multi-granularity domain adaptation (MG-DA) mechanism to exploit the\nnetwork's ability to generate stereo-mimicking features given only on monocular\ncues. Coarse BEV feature-level, as well as the fine anchor-level domain\nadaptation, are both leveraged for guidance in the monocular domain.In\naddition, we introduce an IoU matching-based alignment (IoU-MA) method for\nobject-level domain adaptation between the stereo and monocular predictions to\nalleviate the mismatches while adopting the MG-DA. Extensive experiments\ndemonstrate state-of-the-art results on KITTI and Lyft datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zheyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization. (arXiv:2201.13256v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2201.13256","description":"<p>Plug-and-Play (PnP) methods solve ill-posed inverse problems through\niterative proximal algorithms by replacing a proximal operator by a denoising\noperation. When applied with deep neural network denoisers, these methods have\nshown state-of-the-art visual performance for image restoration problems.\nHowever, their theoretical convergence analysis is still incomplete. Most of\nthe existing convergence results consider nonexpansive denoisers, which is\nnon-realistic, or limit their analysis to strongly convex data-fidelity terms\nin the inverse problem to solve. Recently, it was proposed to train the\ndenoiser as a gradient descent step on a functional parameterized by a deep\nneural network. Using such a denoiser guarantees the convergence of the PnP\nversion of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this\npaper, we show that this gradient denoiser can actually correspond to the\nproximal operator of another scalar function. Given this new result, we exploit\nthe convergence theory of proximal algorithms in the nonconvex setting to\nobtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM\n(Alternating Direction Method of Multipliers). When built on top of a smooth\ngradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target\nstationary points of an explicit functional. These convergence results are\nconfirmed with numerical experiments on deblurring, super-resolution and\ninpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Hurault_S/0/1/0/all/0/1\">Samuel Hurault</a>, <a href=\"http://arxiv.org/find/math/1/au:+Leclaire_A/0/1/0/all/0/1\">Arthur Leclaire</a>, <a href=\"http://arxiv.org/find/math/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISNet: Costless and Implicit Image Segmentation for Deep Classifiers, with Application in COVID-19 Detection. (arXiv:2202.00232v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.00232","description":"<p>This work proposes a novel deep neural network (DNN) architecture, Implicit\nSegmentation Neural Network (ISNet), to solve the task of image segmentation\nfollowed by classification. It substitutes the common pipeline of two DNNs with\na single model. We designed the ISNet for high flexibility and performance: it\nallows virtually any classification neural network architecture to analyze a\ncommon image as if it had been previously segmented. Furthermore, in relation\nto the original classifier, the ISNet does not cause any increment in\ncomputational cost at run-time. We implement an ISNet based on a DenseNet121\nclassifier to solve the task of COVID-19 detection in chest X-rays. The ISNet\nprecisely ignored the image regions outside of the lungs; it achieved 94.5\n+/-4.1% mean accuracy with an external test database, surpassing the\nperformances of state-of-the-art DNNs (U-Net segmenter followed by DenseNet121\nand standalone DenseNet121) by 6 to 7.9%. ISNet presents an accurate, fast, and\nlight methodology to perform classification preceded by segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bassi_P/0/1/0/all/0/1\">Pedro R.A.S. Bassi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalli_A/0/1/0/all/0/1\">Andrea Cavalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04595","description":"<p>Neural image compression have reached or out-performed traditional methods\n(such as JPEG, BPG, WebP). However,their sophisticated network structures with\ncascaded convolution layers bring heavy computational burden for practical\ndeployment. In this paper, we explore the structural sparsity in neural image\ncompression network to obtain real-time acceleration without any specialized\nhardware design or algorithm. We propose a simple plug-in adaptive binary\nchannel masking(ABCM) to judge the importance of each convolution channel and\nintroduce sparsity during training. During inference, the unimportant channels\nare pruned to obtain slimmer network and less computation. We implement our\nmethod into three neural image compression networks with different entropy\nmodels to verify its effectiveness and generalization, the experiment results\nshow that up to 7x computation reduction and 3x acceleration can be achieved\nwith negligible performance drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yin_S/0/1/0/all/0/1\">Shanzhi Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Wen Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Youneng Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yongsheng Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFUGE2 Challenge: Treasure for Multi-Domain Learning in Glaucoma Assessment. (arXiv:2202.08994v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08994","description":"<p>Glaucoma is the second leading cause of blindness and is the leading cause of\nirreversible blindness disease in the world. Early screening for glaucoma in\nthe population is significant. Color fundus photography is the most cost\neffective imaging modality to screen for ocular diseases. Deep learning network\nis often used in color fundus image analysis due to its powful feature\nextraction capability. However, the model training of deep learning method\nneeds a large amount of data, and the distribution of data should be abundant\nfor the robustness of model performance. To promote the research of deep\nlearning in color fundus photography and help researchers further explore the\nclinical application signification of AI technology, we held a REFUGE2\nchallenge. This challenge released 2,000 color fundus images of four models,\nincluding Zeiss, Canon, Kowa and Topcon, which can validate the stabilization\nand generalization of algorithms on multi-domain. Moreover, three sub-tasks\nwere designed in the challenge, including glaucoma classification, cup/optic\ndisc segmentation, and macular fovea localization. These sub-tasks technically\ncover the three main problems of computer vision and clinicly cover the main\nresearchs of glaucoma diagnosis. Over 1,300 international competitors joined\nthe REFUGE2 challenge, 134 teams submitted more than 3,000 valid preliminary\nresults, and 22 teams reached the final. This article summarizes the methods of\nsome of the finalists and analyzes their results. In particular, we observed\nthat the teams using domain adaptation strategies had high and robust\nperformance on the dataset with multi-domain. This indicates that UDA and other\nmulti-domain related researches will be the trend of deep learning field in the\nfuture, and our REFUGE2 datasets will play an important role in these\nresearches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1\">Xingxing Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_J/0/1/0/all/0/1\">Jaemin Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1\">Shuang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Menglu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_C/0/1/0/all/0/1\">Chenglang Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Benjian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shaohua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fumero_F/0/1/0/all/0/1\">Francisco Fumero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sigut_J/0/1/0/all/0/1\">Jose Sigut</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Almubarak_H/0/1/0/all/0/1\">Haidar Almubarak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bazi_Y/0/1/0/all/0/1\">Yakoub Bazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanhao Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yating Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Innani_S/0/1/0/all/0/1\">Shubham Innani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_T/0/1/0/all/0/1\">Tianjiao Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orlando_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Orlando</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiulan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDAR-guided Stereo Matching with a Spatial Consistency Constraint. (arXiv:2202.09953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09953","description":"<p>The complementary fusion of light detection and ranging (LiDAR) data and\nimage data is a promising but challenging task for generating high-precision\nand high-density point clouds. This study proposes an innovative LiDAR-guided\nstereo matching approach called LiDAR-guided stereo matching (LGSM), which\nconsiders the spatial consistency represented by continuous disparity or depth\nchanges in the homogeneous region of an image. The LGSM first detects the\nhomogeneous pixels of each LiDAR projection point based on their color or\nintensity similarity. Next, we propose a riverbed enhancement function to\noptimize the cost volume of the LiDAR projection points and their homogeneous\npixels to improve the matching robustness. Our formulation expands the\nconstraint scopes of sparse LiDAR projection points with the guidance of image\ninformation to optimize the cost volume of pixels as much as possible. We\napplied LGSM to semi-global matching and AD-Census on both simulated and real\ndatasets. When the percentage of LiDAR points in the simulated datasets was\n0.16%, the matching accuracy of our method achieved a subpixel level, while\nthat of the original stereo matching algorithm was 3.4 pixels. The experimental\nresults show that LGSM is suitable for indoor, street, aerial, and satellite\nimage datasets and provides good transferability across semi-global matching\nand AD-Census. Furthermore, the qualitative and quantitative evaluations\ndemonstrate that LGSM is superior to two state-of-the-art optimizing cost\nvolume methods, especially in reducing mismatches in difficult matching areas\nand refining the boundaries of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Siyuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yongxiang Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Teacher Knowledge Distillation for Incremental Implicitly-Refined Classification. (arXiv:2202.11384v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11384","description":"<p>Incremental learning methods can learn new classes continually by distilling\nknowledge from the last model (as a teacher model) to the current model (as a\nstudent model) in the sequentially learning process. However, these methods\ncannot work for Incremental Implicitly-Refined Classification (IIRC), an\nincremental learning extension where the incoming classes could have two\ngranularity levels, a superclass label and a subclass label. This is because\nthe previously learned superclass knowledge may be occupied by the subclass\nknowledge learned sequentially. To solve this problem, we propose a novel\nMulti-Teacher Knowledge Distillation (MTKD) strategy. To preserve the subclass\nknowledge, we use the last model as a general teacher to distill the previous\nknowledge for the student model. To preserve the superclass knowledge, we use\nthe initial model as a superclass teacher to distill the superclass knowledge\nas the initial model contains abundant superclass knowledge. However,\ndistilling knowledge from two teacher models could result in the student model\nmaking some redundant predictions. We further propose a post-processing\nmechanism, called as Top-k prediction restriction to reduce the redundant\npredictions. Our experimental results on IIRC-ImageNet120 and IIRC-CIFAR100\nshow that the proposed method can achieve better classification accuracy\ncompared with existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Longhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zhenyu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuesheng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}